##### Acknowledgements. I would like to thank my supervisor, Richard
Shillcock, for all his help and advice over the years; for his giving me
freedom to work on the topics in this thesis even though they did not
align with his planned research trajectory, and for his perseverance in
handling my endless drafts. Secondly, a huge thanks to Christopher L
Buckley, Alec Tschantz, and Anil Seth for hosting me for a year at the
Sackler Centre and the Evolutionary and Adaptive Systems Group (EASY) at
the University of Sussex. You have all taught me so much about the craft
of research, and without your mentorship and guidance, I would not be
half the researcher I am today. I would also like to thank Conor Heins
for many stimulating conversations around the free energy principle and
related topics. Finally, and above all, I would like to thank my wife,
Mycah Banks, for her unending love and support throughout this entire
process; her aid with proofreading and figure preparation for many
papers, and her sacrifice in putting up with a husband who always has
some more research to do. Without you, none of this would be possible.
Thank you, Mycah.

\standarddeclaration

###### Contents

-    1 Introduction
    -    1.1 Thesis Overview
    -    1.2 Statement of Contributions
        -    1.2.1 Included in Thesis
        -    1.2.2 Not Included in the Thesis
-    2 The Free Energy Principle
    -    2.1 History and Logical Structure
    -    2.2 Formulation
    -    2.3 Markov Blankets
    -    2.4 Variational Inference
    -    2.5 Intrinsic and Extrinsic information geometries
    -    2.6 Self-Organization and Variational Inference
    -    2.7 The Expected Free Energy and Active Inference
    -    2.8 Philosophical Status of the FEP
    -    2.9 Discussion of Assumptions required for the FEP
        -    2.9.1 Assumptions on the Form of the Langevin Dynamics
        -    2.9.2 The Markov Blanket Condition
        -    2.9.3 Assumptions of the free energy Lemma
    -    2.10 Active Inference
        -    2.10.1 Discrete State-space models and Perception
        -    2.10.2 Action Selection and the Expected Free Energy
    -    2.11 Discussion
-    3 Predictive Coding
    -    3.1 Introduction
    -    3.2 Predictive Coding
    -    3.3 Hierarchical predictive coding
        -    3.3.1 Dynamical Predictive coding
    -    3.4 Predictive Coding and Kalman Filtering
        -    3.4.1 The Kalman Filter
        -    3.4.2 Predictive Coding as Kalman Filtering
        -    3.4.3 Results
        -    3.4.4 Discussion
    -    3.5 Relaxed Predictive Coding
        -    3.5.1 Methods
        -    3.5.2 Results
        -    3.5.3 Discussion
    -    3.6 Conclusion
-    4 Scaling Active Inference
    -    4.1 Introduction
        -    4.1.1 Reinforcement Learning
        -    4.1.2 Deep Reinforcement Learning
        -    4.1.3 Model-free vs Model-based
        -    4.1.4 Exploration and Exploitation
        -    4.1.5 Control as Inference
    -    4.2 Deep Active Inference
        -    4.2.1 Model-Free: Active Inference as Variational Policy
            Gradients
        -    4.2.2 Model-based: Reinforcement Learning through Active
            Inference
        -    4.2.3 Related Work
        -    4.2.4 Iterative and Amortised Inference
        -    4.2.5 Control as Hybrid Inference
    -    4.3 Conclusion
-    5 The Mathematical Origins of Exploration
    -    5.1 Introduction
    -    5.2 Origins of the Expected Free Energy
        -    5.2.1 Control as Inference and Active Inference
    -    5.3 Evidence and Divergence Objectives
        -    5.3.1 Control as Inference
        -    5.3.2 KL Control
        -    5.3.3 Active Inference
        -    5.3.4 Action and Perception as Divergence Minimization
    -    5.4 Towards a General Theory of Mean-Field Variational
        Objectives for Control
        -    5.4.1 Encoding Value
        -    5.4.2 General Graphical Models
    -    5.5 Discussion
-    6 Credit Assignment in the Brain
    -    6.1 Introduction
        -    6.1.1 Backpropagation in the Brain
    -    6.2 Predictive Coding Approximates Backprop Along Arbitrary
        Computation Graphs
        -    6.2.1 Methods and Results
        -    6.2.2 RNN and LSTM
    -    6.3 Interim Discussion
    -    6.4 Activation Relaxation
        -    6.4.1 Method and Results
        -    6.4.2 Loosening Constraints
        -    6.4.3 Interim Discussion
    -    6.5 Three-Factor Learning Rules and a Direct Implementation
    -    6.6 Discussion
    -    6.7 Conclusion
-    7 Discussion
    -    7.1 Question 1: Scaling Active Inference
    -    7.2 Question 2: The Mathematical Origins of Exploration
    -    7.3 Question 3: Credit Assignment in the Brain
    -    7.4 Closing Thoughts
-    A Derivation of Kalman Filtering Equations from Bayes’ Rule
-    B Appendix B: Equations of the LSTM cell
-    C Appendix C: Predictive Coding Under the Laplace Approximation

###### List of Figures

-    2.1 The logical flow of the argument of the FEP from the initial
    formulation to the crucial approximate Bayesian inference lemma. We
    begin with a setting of random Langevin stochastic dynamical
    systems, which possess a non-equilibrium-steady state. By applying
    the Ao decomposition, we can understand the dynamics in terms of a
    gradient descent upon the surprisal. Upon the addition of a Markov
    Blanket partition, we can express subsets in terms of their own
    marginal flows via the marginal flow lemma. If we then identify the
    internal states as parametrizing a variational distribution over the
    external states, we can interpret the marginal flow on the surprisal
    as a flow on the variational free energy, under the Laplace
    approximation.
-    2.2 The intuition behind the Markov Blanket partition. The brain
    (or bacillus) consists of internal states @xmath which are separated
    from the outside world (external states @xmath by the blanket states
    @xmath , which can themselves be partitioned into sensory states
    @xmath , representing the sensory epithelia, and which are directly
    influenced by external states, and active states @xmath representing
    the organisms effectors and which are directly influenced by
    internal states, and act on external states. We see that perception
    concerns the minimization of free energy of the internal states ,
    while action concerns the minimization of the expected free energy
    of the active states . Figure originally appeared in
    friston2019particularphysics
-    3.1 MNIST digits in the training set recreated by the network. Top
    row the actual digits, bottom row, the predictive reconstructions.
-    3.2 Unseen MNIST digits in the test set recreated by the network.
    Top row the actual digits, bottom row, the predictive
    reconstructions.
-    3.3 Images of hallucinated digits "dreamt" by the network. These
    were generated by sampling the latent space around the
    representations of some exemplar digits in the latent space, and
    then letting the predictive coding network generate its prediction
    from the chosen latent state.
-    3.4 A PCA clustering plot of the values of test MNIST digits in the
    latent space. Even though the 20 dimensional latent space has been
    reduced down to two, clusters are still visible. For instance, all
    the 1s are clustered in the top left corner. We thus see that
    predictive coding appears to be a powerful and fully unsupervised
    learning algorithm, capable of separating out distinct digits in the
    latent space, despite not being trained with any label information
    at all – and purely on reconstruction.
-    3.5 Test set CIFAR digits reconstructed by the network. The first
    of the two lines is the image and the second is the reconstruction.
    The network is extremely good at reconstructing CIFAR images.
-    3.6 The CIFAR model interpolating between a horse and a cat. Read
    the images left to right top to bottom - like text. The
    interpolation is done by stepping in the latent space from the
    representation of the first image in the direction of the second
    until it is reached.
-    3.7 Prediction errors and prediction for simple toy dynamical
    models. The task of the dynamical predictive coding model is to
    learn to predict a sinewave using only the first two dynamical
    orders – so including position, velocity, and acceleration. The
    model starts from randomly initialized parameters. We see that the
    model very quickly learns to match the incoming sine wave
    observations with only minimal error at the beginning.
-    (a) Incoming sense data - i.e. a sine wave
-    (b) First derivative of the incoming sense data
-    (c) Predicted incoming sense data
-    (d) Prediction temporal derivative of the incoming sense data
-    (e) Prediction error at the first dynamical level
-    (f) Prediction error at the second dynamical level
-    (g) Acitvation of representation units at the first dynamical level
-    (h) Activation of the representation units at the second dynamical
    level
-    3.8 Dynamical models tested on more challenging sawtooth and square
    wave inputs. The model was randomly initialized and only modelled
    the first two dynamical orders (so position, velocity,
    acceleration). Apart from a brief initial period of uncertainty, the
    model rapidly learned to predict these more challenging wave shapes.
-    (a) Incomding sense data - the sawtooth wave
-    (b) First derivative of the incoming sense data
-    (c) Predicted incoming sense data
-    (d) Prediction temporal derivative of the incoming sense data
-    (e) Prediction error at the first dynamical level
-    (f) Prediction error at the second dynamical level
-    (g) Activation of representation units at the first dynamical level
-    (h) Activation of the representation units at the second dynamical
    level
-    3.9 The training graphs of the full construct model. It can
    successfully predict the first three temporal derivatives of a sine
    wave, and also minimise prediction error up to multiple hierarchical
    layers. The full construct model was randomly initialized, and
    learnt both parameters and inferred states purely online – thus
    achieving a ‘double deconvolution’ ( friston2008DEM ) .
-    (a) Incomding sense data - Sine wave
-    (b) First derivative of the sense-data
-    (c) Predicted incoming sense data
-    (d) The models’ prediction of the incomign sense data
-    (e) The models’ prediction of the first derivative of the incoming
    sense-data
-    (f) the models’ prediction of the second derivative of the incoming
    sense-data
-    (g) The prediction error at the first hierarcical layer
-    (h) The prediction error at the second hierarcical layer
-    (i) The prediction error at the first dynamical layer
-    3.10 The true dynamics, control input, and observations generated
    by a random C matrix. These are the source of truth that the
    predictive coding Kalman filter tries to approximate. The
    observations differ substantially from the true dynamics due to the
    random C matrix, which makes the inference problem faced by the
    predictive coding filter much more challenging, since it must
    de-scramble the observations to infer the true dynamics.
-    (a) True Dynamics
-    (b) Control Input
-    (c) Observations
-    3.11 Tracking performance of our gradient filter compared to the
    true values and the analytical Kalman Filter.We show the tracking
    over 2000 timesteps.
-    (a) Position
-    (b) Velocity
-    (c) Acceleration
-    3.12 Here, we zoom in on 100 timestep period to demonstrate
    tracking performance in miniature and the effect of few gradient
    updates. In this case, we plotted the estimates after only 5 steps.
    Even two steps often suffice (with a large learning rate) to provide
    very accurate estimates
-    (a) Position
-    (b) Velocity
-    (c) Acceleration
-    (a) Position for learnt A matrix
-    (b) Velocity for learnt A matrix
-    (c) Filtering performance for adaptively learning just the A
    matrix.
-    3.13 Filtering performance for adaptively learning both the A and B
    matrices in concert (second row). The filtering behaviour of the of
    the randomly initialized filters without adaptive learning is also
    shown. Importantly, with learning the estimated position, velocity,
    and accelerations track their true values precisely while the
    estimates without learning and just randomly initialized A or A and
    B matrices rapidly diverge from the truth.
-    (d) Position: learnt A and B matrices
-    (e) Velocity: learnt A and B matrices
-    (f) Acceleration: learnt A and B matrices
-    3.14 Very poor tracking behaviour with a learnt C matrix. This is
    despite the fact that the Bayesian loss function rapidly decreases
    to a minimum. This shows that the filter can find a prediction-error
    minimizing "solution" which almost arbitrarily departs from reality
    if the C-matrix is randomized. Panel D shows the loss computed by
    the network which rapidly declines, even while the predictions
    rapidly diverge from the truth (Panels a,b,c)
-    (a) Position: learnt C matrix
-    (b) Velocity: learnt C matrix
-    (c) Acceleration: learnt C matrix
-    (d) Loss function over timesteps
-    3.15 The weight transport problem and our solution. On the left is
    the standard predictive coding network architecture. Our diagram
    represents the prediction errors @xmath of one layer receiving
    predictions and transmitted prediction errors to the value neurons
    of the layer above. Prediction errors are transmitted upwards using
    the same weight matrix @xmath as the predictions are transmitted
    downwards. On the right, our solution eschews this biological
    implausibility by proposing a separate set of backwards weight
    @xmath (in red), which are learned separately using an additional
    Hebbian learning rule.
-    3.16 Test accuracy of predictive coding networks with both learnt
    backwards weights, and the ideal weight transposes with both relu
    and tanh activation functions on the MNIST and FashionMNIST
    datasets. Both networks obtain almost identical learning curves,
    thus suggesting that learnt backwards weights allow for a solution
    to the weight-transport problem.
-    (a) MNIST dataset; tanh activation
-    (b) MNIST dataset; relu
-    (c) Fashion dataset; tanh
-    (d) Fashion dataset; relu
-    3.17 Test accuracy of predictive coding networks with and without
    the nonlinear derivative term, using relu and tanh activation
    functions on the MNIST and FashionMNIST datasets. We find that on
    the MNIST dataset performance is similar, while on the FashionMNIST
    dataset and the tanh activation function, the lack of the nonlinear
    derivative appears to slightly hurt performance.
-    (a) MNIST dataset; tanh activation
-    (b) MNIST dataset; relu activation
-    (c) Fashion dataset; tanh activation
-    (d) Fashion dataset; relu activation
-    3.18 The error-connectivity problem and our solution. On the left,
    the biologically implausible one-to-one connectivity between value
    and error nodes required by the standard predictive coding theory.
    On the right, our solution to replace these one to one connections
    by a fully connected connectivity matrix @xmath . By learning @xmath
    with a Hebbian learning rule we are able to achieve comparable
    performance to the one-to-one connections with a fully dispersed
    connectivity matrix.
-    3.19 Test accuracy of predictive coding networks with and without
    learnable error connections for both relu and tanh activation
    functions on the MNIST and FashionMNIST datasets. We see that,
    interestingly, using learnt error weights decreased performance only
    with the tanh but not the relu nonlinearity, and then only slightly
    in the FashionMNIST case.
-    (a) MNIST dataset; tanh activation
-    (b) MNIST dataset; relu activation
-    (c) Fashion dataset; tanh activation
-    (d) Fashion dataset; relu activation
-    3.20 Schematic representations of the architecture across two
    layers of a.) the standard predictive coding architecture and b.)
    The fully relaxed architecture. Importantly, this architecture has
    full connectivity between all nodes and also non-symmetric forward
    and backwards connectivity in all cases. In effect, this
    architecture only maintains a bipartite graph between error and
    value neurons, but no other clear structure
-    3.21 Test accuracy standard and fully relaxed predictive coding
    networks (the combined algorithm), for both relu and tanh activation
    functions on the MNIST and FashionMNIST datasets. We see that,
    interestingly,performance is degraded in all cases and that the relu
    networks are especially affected – with catastrophic declines in
    performance to become almost random. The reasons for this are
    currently unknown and will be investigated in future work.
-    (a) MNIST dataset; tanh activation
-    (b) MNIST dataset; relu activation
-    (c) Fashion dataset; tanh activation
-    (d) Fashion dataset; relu activation
-    4.1 Graphical model for control as inference, with optimality
    variables @xmath . Other than the optimality variables, the
    graphical model takes the form of a Markov Decision Process with
    actions @xmath and states @xmath . The state of a specific timestep
    depends on the action and state of the last time-step. By writing
    out an explicit graphical model like this, we can apply a whole
    field’s worth of inference algorithms on graphical models like this
    to solve control problems.
-    4.2 Comparison of the mean reward obtained by the Active Inference
    agent compared to two reinforcement learning baseline algorithms –
    Actor-Critic and Q learning on the CartPole environment. We
    demonstrate the learning curves over 2000 episodes, averaged over 5
    different seeds. 500 is the maximum possible reward. We see that
    while the vanilla actor critic agent initially learns faster, over a
    long time horizon, the active inference agent outperforms it – and
    both perform better than the vanilla Q learning agent.
-    4.3 Comparison of Active Inference with standard reinforcement
    learning algorithms on the Acrobot environment. Here we see the
    learning curves plotted over five seeds over 20000 episodes. The
    maximum possible reward in this environment was 0, so no agents are
    optimal. We see again that active inference outperforms the other
    two methods consistently.
-    4.4 Comparison of Active Inference with reinforcement learning
    algorithms on the Lunar-Lander environment. Learning curves
    presented over 15000 episodes, averaged over 5 seeds. Here the
    vanilla policy gradient algorithm strongly outperforms the others,
    for unclear reasons, although active inference is still comparable
    with the other standard reinforcement learning algorithms. A score
    of 200 is optimal.
-    4.5 We compare the full Active Inference agent (entropy
    regularization + transition model) with an Active Inference agent
    without the transition model, and without both the entropy term and
    the transition model). We see that while removing the transition
    model appears to have little effect, removing the entropy
    regularisation term substantially impairs performance. This may be
    due to the entropy term aiding in staving off policy collapse.
-    4.6 Comparison of the rewards obtained by the fully ablated Active
    Inference agent with standard reinforcement-learning baselines of
    Q-learning and Actor-Critic on the CartPole environment. Learning
    curves are averaged over 5 seeds. We see that despite being fully
    ablated, the active inference agent continous to perform comparably
    with standard reinforcement learning agents.
-    4.7  (A) Mountain Car: Average return after each episode on the
    sparse-reward Mountain Car task. Our algorithm achieves optimal
    performance in a single trial.  (B) Cup Catch: Average return after
    each episode on the sparse-reward Cup Catch task. Here, results
    amongst algorithms are similar, with all agents reaching asymptotic
    performance in around 20 episodes. (C & D) Half Cheetah: Average
    return after each episode on the well-shaped Half Cheetah
    environment, for the running and flipping tasks, respectively. We
    compare our results to the average performance of SAC after 100
    episodes learning, demonstrating our algorithm can perform
    successfully in environments which do not require directed
    exploration. Each line is the mean of 5 seeds and filled regions
    show +/- standard deviation.
-    4.8 (A & B) Mountain Car state space coverage : We plot the points
    in state-space visited by two agents - one that minimizes the free
    energy of the expected future (FEEF) and one that maximises reward.
    The plots are from 20 episodes and show that the FEEF agent searches
    almost the entirety of state space, while the reward agent is
    confined to a region that can be reached with random actions.  (C)
    Ant Maze Coverage : We plot the percentage of the maze covered after
    35 episodes, comparing the FEEF agent to an agent acting randomly.
    These results are the average of 4 seeds.
-    4.9 Overview of classic RL and control algorithms in our scheme.
    Standard model-free RL corresponds to amortised policies, planning
    algorithms are iterative planning, and control theory infers
    iterative policies. The amortised plans quadrant is empty, perhaps
    suggesting room for novel algorithms. The position of the algorithms
    within the quadrant is not meaningful.
-    4.10 (a - c) : Amortised predictions of @xmath are shown in red,
    where @xmath denote the expected states, shaded areas denote the
    predicted actions variance at each step, and the expected trajectory
    recovered by iterative inference is shown in blue. At the onset of
    learning ( a ), the amortised predictions are highly uncertain, and
    thus have little influence on the final approximate posterior. As
    the amortised model @xmath learns ( b ), the certainty of the
    amortised predictions increase, such that the final posterior
    remains closer to the initial amortised guess. At convergence, ( c
    ), the iterative phase of inference has negligible influence on the
    final distribution, suggesting convergence to a model-free
    algorithm. (d) Here, we compare our algorithm to its constituent
    components – the soft-actor critic (SAC) and an MPC algorithm based
    on the cross-entropy method (CEM). These results demonstrate that
    the hybrid model significantly outperforms both of these methods.
-    5.1 Numerical illustration of optimizing a multimodal desired
    distribution with an Evidence objective (Panel A) vs a Divergence
    Objective (panel B). The desire distribution consisted of the sum of
    two univariate Gaussian distributions, with means of @xmath and
    @xmath and variances of @xmath and @xmath respectively. We then
    optimized an expected future distribution, which also consisted of
    two Gaussians with free means and variances using both an Evidence
    and a Divergence objective. As can be seen, optimizing the Evidence
    Objective results in the agent fitting the predicted future density
    entirely to an extremely sharp peak around the mode of the desired
    distribution. Conversely, optimizing a divergence objective leads to
    a precise match of the predicted and desired distributions (panel B
    shows the two distributions almost precisely on top of one another).
    As a technical note, to be able to see both the evidence and deisre
    distributions on the same scale, for the evidence objective the
    predicted distribution is normalized but the desired distribution is
    not. Code for these simulations can be found at:
    https://github.com/BerenMillidge/origins_information_seeking_exploration.
-    (a) Optimizing with an Evidence Objective
-    (b) Optimizing with a Divergence Objective
-    6.1 Top: Backpropagation on a chain. Backprop proceeds backwards
    sequentially and explicitly computes the gradient at each step on
    the chain. Bottom: Predictive coding on a chain. Predictions, and
    prediction errors are updated in parallel using only local
    information. Importantly, while the original computation graph
    (black lines) must be a DAG, the augmented predictive coding graph
    is cyclic, due to the backwards (red) prediction error connections.
-    6.2 Top: The computation graph of the nonlinear test function
    @xmath . Bottom: graphs of the log mean divergence from the true
    gradient and the divergence for different learning rates.
    Convergence to the exact gradients is exponential and robust to high
    learning rates.
-    6.3 Mean divergence between the true numerical and predictive
    coding backprops over the course of training. In general, the
    divergence appeared to follow a largely random walk pattern, and was
    generally neglible. Importantly, the divergence did not grow over
    time throughout training, implying that errors from slightly
    incorrect gradients did not appear to compound.
-    (a) Network2
-    (b) Conv Layer 2
-    (c) FC Layer 1
-    (d) FC Layer 2
-    6.4 Training and test accuracies of the CNN network on the SVHN and
    CIFAR datasets using the cross-entropy loss. As can be seen
    performance remains very close to backprop, thus demonstrating that
    our predictive coding algorithm can be used with different loss
    functions, not just mean-squared-error.
-    (a) SVHN test accuracy
-    (b) CIFAR training accuracy
-    (c) CIFAR test accuracy
-    6.5 Test accuracy plots for the Predictive Coding and Backprop RNN
    and LSTM on their respective tasks, averaged over 5 seeds.
    Performance is again indistinguishable from backprop.
-    6.6 Training losses for the predictive coding and backprop RNN. As
    expected, they are effectively identical.
-    6.7 Computation graph and backprop learning rules for a single LSTM
    cell. Inputs to the LSTM cell are the current input @xmath and the
    previous embedding @xmath . These are then passed through three
    gates – an input, forget, and output gate, before the output of the
    whole LSTM cell can be computed
-    6.8 The LSTM cell computation graph augmented with error units,
    evincing the connectivity scheme of the predictive coding algorithm.
    The key move is to associate each intermediate node in the
    computation graph with its own prediction error unit
-    6.9 Training losses for the predictive coding and backprop LSTMs
    averaged over 5 seeds. The performance of the two training methods
    is effectively equivalent.
-    6.10 Divergence between predictive coding and the correct backprop
    gradients as a function of sequence length. Crucially, this
    divergence only increases linearly in the sequence length, allowing
    for very accurate gradient computation even with long sequences.
-    6.11 Number of iterations to reach convergence threshold as a
    function of sequence length. Importantly, the number of iterations
    required to converge appears to grow sublinearly with sequence
    length, again implying that convergence is not computationally
    unattainable even with very long sequences.
-    6.12 Training losses for the predictive coding and backprop LSTMs
    averaged over 5 seeds. The performance of the two training methods
    is effectively equivalent.
-    6.13 Train and test accuracy and gradient angle (cosine similarity)
    for AR vs backprop for MNIST and Fashion-MNIST datasets. Importantly
    the training and test accuracies are virtually identical between the
    AR-trained and backprop-trained networks. Additionally, the gradient
    angle between the AR update and backprop is always substantially
    less than the 90 degrees required to allow for learning.
-    6.14 Train and test accuracy and gradient angle (cosine similarity)
    for AR vs backprop for MNIST and Fashion-MNIST datasets.
    Importantly, we see that even with the additional relaxations, the
    AR trained algorithm performs comparably to backprop
-    6.15 Angle between the AR and backprop updates in the learnable
    backwards weights, no nonlinear derivatives, and the combined
    conditions. At all times this angle remains under 90 degrees and is
    steady for all cases apart from the no nonlinear derivatives cases,
    where it appears to increase over time. Interestingly, this does not
    appear to hinder learning performance noticeably, and may simply
    reflect the angle getting increasingly worse as the network
    converges.
-    6.16 Assessing whether the frozen feedforward pass assumption can
    be relaxed. We show the resulting performance (test accuracy)
    against baseline of relaxing this assumption on the MNIST dataset.
    All results averaged over 10 seeds. These results show clearly that
    the frozen feedforward pass assumption can be relaxed for the
    nonlinear derivative and in the weight update nonlinear derivative,
    but not both nonlinear derivatives simultaneously, and definitely
    not using the @xmath term in the weight update
-    (a) MNIST nonlinear derivative relaxation update
-    (b) MNIST nonlinear derivative weight update
-    (c) MNIST both nonlinear derivative
-    (d) MNIST current x weight update
-    (a) Conv backwards weights
-    (b) FC backwards weights
-    (c) Both backwards weights
-    (d) Conv no nonlinear derivative
-    (e) FC no nonlinear derivative
-    (f) Both no nonlinear derivative
-    6.17 Performance (test accuracy), averaged over 10 seeds, on
    CIFAR10 demonstrating the scalability of the learnable backwards
    weights and dropping the nonlinear derivatives in a CNN
    architecture, compared to baseline AR without simplifications.
    Performance is equivalent throughout.
-    (g) Conv backwards weights
-    (h) FC backwards weights
-    (i) Both backwards weights
-    (j) Conv no nonlinear derivative
-    (k) FC no nonlinear derivative
-    (l) Both no nonlinear derivative
-    (a) Conv backwards weights
-    (b) FC backwards weights
-    (c) Both backwards weights
-    (d) Conv no nonlinear derivative
-    (e) FC no nonlinear derivative
-    (f) Both no nonlinear derivative
-    6.18 Potential schematic for a direct implementation of backprop in
    the brain. All that is necessary for this to be plausible is
    three-factor learning rules.

## Chapter 1 Introduction

The Free Energy Principle (FEP) ( friston2006free ; friston2012free ;
friston2019particularphysics ; parr2020Markov ) is an emerging theory in
theoretical neuroscience which aims to tackle an extremely deep and
fundamental question – can one characterise necessary behaviour of any
system that maintains a statistical separation from its environment (
parr2020Markov ; friston2019particularphysics ; bruineberg2020emperor )
? Specifically, it argues that any such system can be seen as performing
an elemental kind of Bayesian inference where the dynamics of the
internal states of such a system can be interpreted as minimizing a
variational free energy functional ( beal2003variational ) ¹ ¹ 1 Hence
the name the ‘ free energy principle’ , and thus performing approximate
(variational) Bayesian inference ( friston2019particularphysics ) . The
FEP is thus effectively a formalization and generalization of the
Ashbyan good regulator principle ( conant1970every ) , where an
intrinsic property of these kinds of systems is that they in some sense
come to embody a Bayesian model of their surroundings, and the perform a
inference using this model ( baltieri2020predictions ) .

The free energy principle therefore provides a close link between the
notions of self-organisation and dissipative structures in
thermodynamics ( prigogine1973theory ; seifert2008stochastic ) , with
cybernetic notions of feedback, regulation, and control (
wiener2019cybernetics ; kalman1960contributions ; johnson2005pid ) , to
more ‘cognitive’ ideas of inference and learning (
schmidhuber1991possibility ; dayan2008decision ; rao1999predictive ) .
Specifically, we see that, in some sense, all of these notions can be
construed as necessary properties and consequences of systems that
self-organize to, and maintain themselves at, a non-equilibrium steady
state. While having developed over time into a very general theory of
self-organising systems, the free energy principle has emerged from
theoretical neuroscience as a way to understand the properties of
biological and cognitive systems, especially the brain (
friston2003learning ; friston2006free ) . As such, the most developed
process theories, which are explicitly inspired by the free energy
principle – predictive coding ( mumford1992computational ;
rao1999predictive ; friston2005theory ) , and active inference (
friston2012active ; friston2015active ; friston2017active ) , each
purport to be theories of inference, action, and learning in the brain.
A core tenet of the free energy principle is that perception, action,
and learning can all be unified under a single inference objective which
minimizes a single objective – the variational free energy. As we shall
see, these different ‘process theories’ arise simply from the choice of
a specific parametrization of the generative model and the variational
density, where certain choices have been found to be useful and also
give potentially biologically plausible inference and learning rules. In
this thesis, we focus on the high level applications of the free energy
principle to neuroscience and to machine learning, and make multiple
contributions to the literature through the application of the free
energy principle to all of perception, action, and learning.
Specifically, in this thesis, we are primarily concerned with scaling up
methods which have emerged in the theoretical neuroscience literature,
to the extremely challenging and complex tasks which can be solved with
modern machine learning methods. The value of scaling up such methods is
twofold. Firstly, FEP inspired process theories often possess
significant biological plausibility, in that they provide a potential
account of what the brain is doing – thus, if they can scale them up to
handle the sort of tasks that the brain must solve, then we can be more
confident that these theories could, in theory, be actually implemented
in the brain. Secondly, the free energy principle and its process
theories contain many insights which can potentially be used to improve
and extend current state of the art methods in machine learning. In this
thesis, we aim to present both kinds of contributions – firstly by
demonstrating that FEP inspired models and process theories can scale,
and secondly, by showcasing how ideas from the FEP can be used to
advance the field of machine learning or neuroscience on its own terms.

### 1.1 Thesis Overview

This thesis is organized into three main parts. We begin with a detailed
introduction and mathematical walkthrough of the Free Energy Principle
(FEP) as presented in its most recent incarnation ( friston2019free ;
parr2020Markov ) (Chapter 2) and then, in the first section of original
work (Chapter 3), we consider applications of the free energy principle
to perception – and make contributions to the FEP process theory of
predictive coding . In the second section (Chapters 4 and 5), we
consider applications of the free energy principle to action , and work
with the process theory of active inference . Finally, in the third
section (Chapter 6), we consider applications of the free energy
principle to learning , and especially focus on what FEP-inspired models
and process theories can tell us about the nature of credit assignment
in the brain. Below is a chapter by chapter breakdown of the work in the
thesis and what I see are the main contributions to both machine
learning and neuroscience in each. Throughout the thesis, since each
chapter covers a fairly distinct topic, I have tried to make each
chapter modular and mostly independent of the others. Each chapter
contains an introduction and mini literature review on the field it
discusses, as well as presenting my original work.

In Chapter 2, I will give a detailed overview of the free energy
principle, starting from first principles, and include a discussion of
the mathematical assumptions and provide some of my opinions on the
philosophical nature of the theory and its potential utility. I will
then give a brief walk-through of discrete state space active inference
as presented in ( friston2015active ; friston2017process ; da2020active
) which will be the focus of the ‘scaling up’ work in Chapter 4.

In Chapter 3, we deal principally with models of perception and
predictive coding. We begin by giving a brief overview and mathematical
walkthrough of predictive coding theory as it is presented in (
friston2005theory ; friston2008hierarchical ; buckley2017free ) . We
then cover in depth two contributions to the theory of predictive
coding. First, we present work where we scale up and empirically test
the performance of large scale predictive coding networks on machine
learning datasets, which had not been tested before in the literature.
We also clarify the relationship between predictive coding and other
known algorithms such as Kalman filtering. Secondly, we discuss relaxing
various relatively un-biologically plausible aspects of the predictive
coding equations, such as the need for symmetric forward and backwards
weights, the necessity of using nonlinear derivatives in the update
rules, and the one-to-one error to value neuron connectivity required by
the standard algorithms. All of these conditions put serious constraints
on the biological plausibility of the algorithm, and here we show that
to some extent they can each be relaxed without harming performance.

Then, in Chapters 4 and 5, I present my work on the applications of the
free energy principle to questions of action selection and control.
Chapter 4 focuses predominantly on scaling up active inference methods
to achieve results comparable to those achieved in the deep
reinforcement learning literature, while Chapter 5 takes a more abstract
and mathematical approach and investigates in depths the mathematical
origin of objective functionals which combine both exploitatory and
exploratory behaviour – an approach which has the potential to finesse
the exploration-exploitation dilemna ( friston2015active ) .

Specifically, in Chapter 4, I will first review the rudiments of
reinforcement learning (RL), and its current incarnation in deep
reinforcement learning, including the two paradigms of model-free and
model-based approaches. I will then present two of my contributions of
merging active inference and deep reinforcement learning under the new
paradigm of deep active inference. I will first discuss deep active
inference in the model-free paradigm, and show how in this case the
active inference equations can naturally be understood as specifying an
actor-critic architecture with a bootstrapped value function, except one
where the value-function becomes the expected-free energy functional,
which provides an intrinsic source of exploratory drive to the algorithm
which can improve performance. I then discuss the similarities and
differences to standard deep reinforcement learning algorithms and
empirically compare the performance of the algorithms on a number of
challenging continuous control tasks from OpenAI gym (
brockman2016openai ) .

Then I will present a second piece of work which applies active
inference instead to the model-based reinforcement learning paradigm. We
will show that in this case, we can use the powerful generative ‘world
models’ ( ha_recurrent_2018 ) of active inference to work as transition
models of the learnt dynamics, and then the use of action selection in
planning. The use of the expected free energy functional again furnishes
an intrinsic exploratory for active inference agents, which we again
show is crucial to effective, goal-directed exploration and that it
empirically improves performance on a suite of continuous control tasks.
We then conceptualize reinforcement learning through the lens of
inference, and understand the distinction between model-free and
model-based reinforcement learning through the lens of iterative and
amortised inference. We then demonstrate how these two types of
inference can be combined , leading to a novel hybrid inference
algorithm which we show attains both the sample efficiency of
model-based reinforcement learning with the higher asymptotic
performance and fast computation time of model-free RL.

Then, in Chapter 5, we move into a more abstract, mathematical domain.
Here we grapple with deep questions underlying the objective functions
of reinforcement learning. Specifically, we wish to understand the
mathematical origin and nature of the expected free energy term which
grants deep active inference agents their superior exploratory
capacities. Having tackled this, we turn to the deeper question of the
mathematical origin of information-seeking exploratory terms within the
inference objective optimized in reinforcement learning methods, and
thus the mathematical origin of exploratory drives. We present a new
dichotomy between evidence and divergence objectives, and demonstrate
how only divergence objectives, which intuitively can be seen as
minimizing the divergence between the predicted and desired futures,
rather than simply maximizing the likelihood of the desired future, are
required to obtain such terms. We then relate this fundamental dichotomy
to a number of objectives prominent in both the cognitive science,
neuroscience, and machine learning literatures. Finally, we further seek
to explore the general possible space of variational objective
functionals for control, and provide a wide-ranging categorisation of
the potential of such functionals within our framework.

Finally, in Chapter 6, we turn to the application of insights and ideas
from the free energy principle to learning . Specifically, we focus on
the vexing question of how to achieve credit assignment in the brain.
This is necessary since, we assume, that most of the statistical
‘parameters’ in the brain -- such as synaptic weights -- start out
initialized fairly randomly during development and thus need to be
trained or learned through interactions with the environment ² ² 2 It is
possible that some pathways, especially low-level subcortical pathways
may be, to some extent, hardwired by evolution. However, it is generally
considered infeasible for the immense number and complexity of the
neocortical circuitry to be hardwired in this way . Understanding how
this learning can take place is a fundamental question within
neuroscience. One approach, which has recently been immensely successful
in machine learning with large artificial neural networks, is the idea
of learning through gradient descent using the backpropagation of error
algorithm. Since backpropagation of error is such a successful algorithm
in artificial neural network – which, although simplified are
nevertheless generally quite a close substrate to biological neural
networks – it is very likely that it would also work to successfully
train biological neural networks, if it could be implemented in a
biologically plausible manner in such networks. The question, then,
becomes whether and how backprop can be implemented in biologically
plausible neural networks. While this is an extremely broad question
which cannot likely be answered in a single thesis, we present two novel
contributions to this question here.

Specifically, in Chapter 6, we first provide a brief review of the
credit assignment problem in the brain, as well as the backpropagation
algorithm (and automatic differentiation in general), for context, and
then present our two contributions to this field. First, we demonstrate
how under certain conditions, predictive coding itself can be utilized
as a biologically plausible method of credit assignment in the brain,
can apply to any arbitrary computation graph, and can be used to train
modern machine learning architectures such as CNNs and LSTMs with
performance comparable to backprop. Secondly, we introduce a novel,
simpler algorithm for credit assignment in the brain, which we call
Activation Relaxation and then discuss their similarities and
differences. We end with a discussion of the current state of credit
assignment algorithms and backpropagation in the brain, and the
importance of this field of research.

Finally, in Chapter 7, we provide a discussion and overview of the work
in our thesis. We will briefly survey what has been achieved, and where
the limitations and directions for future work lie, as well as the
implications of the work of this thesis.

### 1.2 Statement of Contributions

This statement provides a detailed overview of the work undertaken in
this PhD which has resulted in research papers, both the papers included
in this thesis and also those not included. I provide a brief summary of
the key results and narrative of each paper, as well as a detailed
breakdown my contributions. * denotes equal contribution.

#### 1.2.1 Included in Thesis

##### Chapter 3

-   Predictive Coding – a Theoretical and Experimental review (2021).
    Beren Millidge , Alexander Tschantz, Anil Seth, Christopher L
    Buckley. In Preparation

    This paper provides a full review of recent advances in predictive
    coding, as well as the mathematical basis of the theory. It covers
    all of the mathematical, implementational, and neuronal aspects of
    predictive coding theory. As a first author paper, I conceptualised
    the idea, collated the necessary materials for the review, and wrote
    the paper. Alexander Tschantz, Christopher L Buckley, and Anil Seth,
    contributed edits and other editorial suggestions.

-   Neural Kalman Filtering (2021). Beren Millidge , Alexander Tscahtnz,
    Anil Seth, Christopher L Buckley. Arxiv

    This paper reviews the close connection between Kalman filtering and
    linear predictive coding, demonstrates that predictive coding can
    closely approximate the performance of Kalman filtering on filtering
    tasks, and proposes a low-level neural implementation of predictive
    coding which could be implemented in the brain. As a first author
    paper, I conceptualised the idea, worked out the mathematical
    derivations, implemented the model and experiments, and wrote a
    first draft of the paper. Alexander Tschantz, Christopher L Buckley,
    and Anil Seth contributed editorial and narrative suggestions.

-   Relaxed Predictive Coding (2020). Beren Millidge , Alexander
    Tschantz, Anil Seth, Christopher L Buckley. Arxiv

    This paper shows how several biologically implausible aspects of the
    predictive coding algorithm – backwards weight symmetry, nonlinear
    derivatives, and one-to-one error unit connectivity – can be relaxed
    without unduly harming performance on challenging object recognition
    tasks. As a first author paper, I conceptualised the idea,
    implemented the code and experiments, and wrote up a first draft of
    the paper. Alexander Tschantz, Anil Seth, and Christopher L Buckley
    contributed editorial suggestions.

-   Implementing Predictive Processing and Active Inference: Preliminary
    Steps and Results (2019). Beren Millidge , Richard Shillcock.

    This paper provides reference implementations of multi-layer
    predictive coding networks trained for object recognition within a
    machine learning paradigm. As a first author paper, I conceptualised
    the idea, wrote the code and experiments, and wrote up the initial
    draft of the paper. Richard Shillcock contributed edits.

##### Chapter 4

-   Deep Active Inference as Variational Policy Gradients (2019). Beren
    Millidge . Published in the Journal of Mathematical Psychology .

    This paper merges active inference and model-free deep reinforcement
    learning to create a deep active inference agent, very similar to
    actor-critic methods in deep reinforcement learning. The performance
    of deep active inference and deep reinforcement learning is compared
    on a suite of OpenAI Gym continuous control tasks. As a sole author
    paper, I conceptualised the idea, executed it mathematically and in
    code, designed and implemented the experiments and wrote up the
    paper.

-   Reinforcement Learning through Active Inference (2020). Alexander
    Tschantz*, Beren Millidge  * , Anil Seth, Christopher L Buckley.
    Published in ICLR workshop on Bridging AI and Cognitive Science .

    This paper applies active inference to model-based reinforcement
    learning methods for continuous control. We use an ensemble
    transition model parametrised by deep neural networks for
    model-based planning. The expected free energy and free
    energy-of-the-expected future objective functionals provide
    additional exploratory bonuses which allow considerably greater and
    faster performance of the method compared to standard deep
    reinforcement learning baselines. I was joint first author on this
    paper. While the initial idea was primarily conceptualized by
    Alexander Tschantz and Christopher L Buckley, I contributed equally
    to the design and implementation of the algorithm, the experiments,
    and the writing of the paper.

-   Reinforcement Learning as Iterative and Amortised inference (2020).
    Beren Millidge *, Alexander Tschantz*, Anil Seth, Christopher L
    Buckley. Arxiv .

    This short workshop paper derives the key mathematical result of
    understanding model-free and model-based reinforcement learning in
    terms of iterative and amortised inference. It then partitions known
    reinforcement learning algorithms into a quadrant based on two
    orthogonal axes – firstly whether it uses iterative or amortised
    reinforcement learning, and secondly whether we optimize over plans
    or over policies. As a joint first author, I contributed equally
    (with Alexander Tschantz) in the idea, formulation of the dichotomy,
    and the mathematical derivations. I also was the primary author of
    the text of the paper. Alexander Tschantz, Christopher L Buckley,
    and Anil Seth also contributed edits to the paper draft.

-   Control as Hybrid Inference (2020). Alexander Tschantz, Beren
    Millidge , Anil Seth, Christopher L Buckley. Published in ICML
    workshop on the Theoretical Foundations of Reinforcement Learning .

    This paper combines both iterative (model-based) and amortised
    (model-free) reinforcement learning methods to obtain a hybrid
    method which combines both the sample-efficiency of model-based RL,
    with the asymptotic performance and fast computation of model-free
    methods. As second author, I contributed equally to the idea, the
    mathematical derivation, and architectural formulation of the hybrid
    agent. Alexander Tschantz took the lead with implementing the agent
    in code, designing and running, the experiments, and writing up the
    initial draft of the paper. I then contributed paper edits along
    with Anil Seth and Christopher L Buckley.

##### Chapter 5

-   Whence the Expected Free Energy (2020). Beren Millidge , Alexander
    Tschantz, Anil Seth, Christopher L Buckley. Published in Neural
    Computation .

    This paper investigates the mathematical origin of the expected free
    energy functional in active inference, demonstrates its relationship
    to other algorithm, and proposes a novel, more principled objective,
    the free energy of the expected future (FEEF). As a first author, I
    primarily conceptualised the idea and worked out the mathematical
    results. I also wrote up the initial draft and was instrumental in
    handling later edits. Christopher L Buckley also contributed
    significantly to some of the mathematical results. Alexander
    Tschantz, Christopher L Buckley, and Anil Seth, also contributed
    through edits to the main text of the paper.

-   On the relationship of Active Inference and Control as Inference
    (2020). Beren Millidge , Alexander Tschantz, Anil Seth, Christopher
    L Buckley. Published in the IEEE International Workshop on Active
    Inference .

    This paper derives the relationship between active inference
    methods, and the control as inference paradigm which is popular
    within the reinforcement learning community. As first author, I
    conceptualised the idea, derived the primary mathematical results,
    and wrote the initial draft of the paper. Alexander Tschantz, Anil
    Seth, and Christopher L Buckley contributed paper edits.

-   Understanding the Origin of Information-Seeking Exploration in
    Probabilistic Objectives for Control (2021). Beren Millidge ,
    Alexander Tschantz, Anil Seth, Christopher L Buckley. Submitted to
    Arxiv .

    This paper introduces the dichotomy between evidence and divergence
    objectives, demonstrates how divergence objectives are necessary for
    the emergence of information maximizing exploration, and unifies
    many disparate objectives proposed in the machine learning and
    cognitive science communities under this formalism. As a first
    author paper, I conceptualised and derived the mathematical results,
    and wrote up a first draft of the paper. Alexander Tschantz,
    Christopher L Buckley, and Anil Seth contributed paper edits and
    narrative suggestions. Alexander Tschantz contributed heavily to the
    cognitive science and psychophysics sections.

##### Chapter 6

-   Predictive Coding Approximates Backprop along Arbitrary Computation
    Graphs (2020). Beren Millidge , Alexander Tschantz, Anil Seth,
    Christopher L Buckley. Arxiv

    This paper demonstrates that predictive coding can approximate the
    backpropagation of error algorithm along arbitrary computation
    graphs. Predictive coding is used to train state of the art machine
    learning architectures, and obtains identical performance to
    backprop even for deep and complex architectures. As a first author
    paper, I conceptualised the idea, implemented the models and
    experiments in code, and wrote up the initial draft of the paper.
    Alexander Tschantz, Christopher L Buckley, and Anil Seth contributed
    paper edits and narrative suggestions.

-   Activation Relaxation: A Local, Dynamical Approximation to
    Backpropagation in the Brain (2020). Beren Millidge , Alexander
    Tschantz, Anil Seth, Christopher L Buckley. Arxiv

    This paper introduces a novel algorithm for approximating
    backpropagation in a local, biologically plausible way, which we
    call the activation relaxation algorithm. Crucially, this approach
    is significantly simpler than predictive coding, in that it does not
    require a special population of error neurons. Additionally, we show
    that several of the remaining biologically implausible aspects of
    the algorithm – specifically the symmetric backwards weights – can
    also be relaxed, leading to an extremely simple and biologically
    plausible algorithm for credit assignment. As a first author paper,
    I invented the algorithm and performend the mathematical
    derivations. I implemented the model in code, and ran the
    experiments. I wrote up the initial draft of the paper. Alexander
    Tschantz, Christopher L Buckley, and Anil Seth contributed editorial
    suggestions.

-   Investigating the Scalability and Biological Plausibility of the
    Activation Relaxation Algorithm (2020). Beren Millidge , Alexander
    Tschantz, Anil Seth, Christopher L Buckley. Published at NeurIPS
    2020 Workshop: Beyond Backprop .

    This paper extends and empirically tests the Activation Relaxation
    algorithm on more challenging tasks including large-scale CNN
    models. Moreover, it investigates the degree to which the loosening
    the assumptions of the activation relaxation algorithm hinder
    performance. As a first author paper, I conceptualized the core
    ideas to test, implemented the experiment and analyzed the results.
    I wrote up the initial draft of the paper. Alexander Tschantz,
    Christopher L Buckley, and Anil Seth contributed editorial
    suggestions.

#### 1.2.2 Not Included in the Thesis

-   Combining active inference and hierarchical predictive coding – a
    tutorial review and case study (2019). Beren Millidge , Richard
    Shillcock.

    This paper uses hierarchical predictive coding networks as a
    dynamics model for a simple active inference approach which is then
    applied to discrete action reinforcement learning tasks such as the
    cart-pole. As a first author paper, I conceptualised the idea,
    implemented the code and experiments, and wrote up the initial
    draft. Richard Shillcock contributed paper edits and suggestions.

-   A predictive processing account of visual saliency using
    cross-predicting autoencoders (2018). Beren Millidge , Richard
    Shillcock. Psyarxiv .

    This paper demonstrates how applying a cross-modal prediction
    objective in predictive coding, allows for the development of error
    representations which provide a good empirical match to estimates of
    visual saliency in natural image scenes. As a first author paper, I
    contributed equally in conceptualising the idea with my supervisor,
    Richard Shillcock. I implemented the models and experiments, and
    wrote up the initial draft of the paper. Richard Shillcock then
    contributed with editorial suggestions.

-   Exploring infant vocal imitation in Tadarida brasiliensis mexicana
    (2019). Richard Shillcock, Beren Millidge , Andrea Ravignani.
    Published in Neurobiology of Speech and Language .

    This paper introduces a multi-agent model of vocal imitation in bat
    infants, which provides a gradient soundscape which can guide
    mothers to pups in a crowded bat colony. As a second author paper, I
    contributed substantially to the development of the model. I
    implemented the model in code and ran the experiments. I contributed
    substantially to the writing of the paper.

-   Curious inferences: reply to Sun and Firestone on the Dark Room
    Problem (2020). Anil Seth, Beren Millidge , Christopher L Buckley,
    Alexander Tschantz. Published in Trends in Cognitive Science .

    This short response argues against the Dark Room Problem in
    predictive coding by suggesting that the intrinsic exploratory
    drives of the expected free energy and other objectives such as the
    free energy of the expected future suffice to drive the agent away
    from dark-room environments. I was involved in the conceptualisation
    and writing of the piece, although the main impetus behind this
    response lay with Anil Seth.

-   The Acquisition of Culturally Patterned Attention Styles under
    Active Inference (2020). Axel Constant, Alexander Tschantz, Beren
    Millidge , Filipo Criado-Boado, Andy Clark. Arxiv .

    This paper presents an active inference model of culturally
    patterned saccade behaviour trained on archaeological vase patterns.
    It demonstrates that more complex patterns result in more vertically
    oriented saccade behaviour, thus corroborating experimental studies.
    I jointly designed and implemented the active inference model with
    Alexander Tschantz and ran the experiments. I also wrote the first
    draft of the methods section of the paper. Andy Clark, Felipe
    Criado-Boado, and Axel Constant conceptualised the idea and
    experiments.

## Chapter 2 The Free Energy Principle

The free energy principle is a grand theory, arising out of theoretical
neuroscience, with deep ambitions to provide a unified understanding of
the nature of self organisation under the rubric of Bayesian inference (
friston2006free ; friston_free_2019 ; friston2010free ; friston2012free
) . Perhaps the central postulate of this theory is the ‘Free Energy
Lemma’ which states that one can interpret any self organizing system,
of any type and on any scale, as performing a kind of elemental Bayesian
inference upon the external environment that surrounds it (
friston2013life ; friston2012ao ; friston2019particularphysics ) . More
generally than this, it claims to provide a recipe, in terms of a set of
statistical independencies – which we call the ‘Markov Blanket’,
following ( pearl2011Bayesian ) – which define precisely and
mathematically what it means to be a system at all (
friston2019particularphysics ) . Understanding self-organization through
the lens of inference provides an exceptionally powerful perspective for
understanding the nature of self-organizing systems, as it allows one to
immediately grasp the nature of the dynamics which undergird
self-organization, as well as apply the extremely large and powerful
literature on Bayesian inference methods and algorithms to the dynamics
of self-organizing systems ( parr2020modules ; parr2020Markov ;
yedidia2011message ) . Moreover, by framing everything in statistical
terms – in terms of conditional independencies, generative models, and
approximate variational distributions – the free energy principle
provides a novel and powerful vocabulary to talk about such systems, as
well as to ask questions such as ‘what kind of generative model does
this system embody?’ ( baltieri2020predictions ; maturana2012autopoiesis
) which would be impossible to ask and answer without it. Ultimately,
this new statistical and inferential perspective upon dynamics may lead
to important advances or novel insights.

This perspective also has exceptionally close relationships with early
cybernetic views of control and regulation ( wiener2019cybernetics ;
conant1970every ; kalman1960new ) , and philosophically the FEP can be
seen as a mathematical generalization of Ashby’s notion that every good
regulator of a system must become, in effect, a model of the system (
conant1970every ) . The FEP nuances this notion slightly by instead
stating that every system that regulates itself against the external
environment, must in some sense embody a generative model of the
environment, and also that the flow of the internal states of the system
necessarily perform approximate variational inference upon an
approximate posterior distribution over the external states of the
environment, such that, broadly, they track the fluctuations in the
external environment.

The free energy principle originated in theoretical neuroscience, as an
attempt to understand the mathematical properties that a self-organising
living, biotic system, must possess in order to sustain itself against
thermodynamic equilibrium. It was first and especially applied to
understanding the function of the brain ( friston2006free ;
friston2010action ; friston2012history ) , and has been developed into
two main process theories – predictive coding ( rao1999predictive ;
friston2003learning ; friston2005theory ; friston2008hierarchical ) and
active inference ( friston2009reinforcement ; friston2012active ;
friston2015active ; friston2017process ; friston2018deep ; da2020active
) which have been investigated in a wide variety of paradigms, where it
has been used to investigate a wide variety of phenomena from (
friston2014anatomy ; friston2015knowing ; friston2015active ) ,
information foraging and saccades ( parr2017uncertainty ; parr2018active
; parr2019computational ) exploratory behaviour (
schwartenbeck2013exploration ; friston2015active ; friston2017curiosity
; friston2020sophisticated ) , concept learning (
schwartenbeck_computational_2019 ) , and a variety of neuropsychiatric
disorders ( lawson2014aberrant ; adams2012smooth ; mirza2019impulsivity
; cullen2018active ) . These process theories translate the abstract
formulation of the FEP into concrete and practical algorithms by
specifying certain generative models, variational distributions, and
inference procedures, and have been shown to be extremely useful both in
providing powerful and biologically plausible theories of learning and
inference in the brain, and also in developing highly effective
inference algorithms which have advanced the state of the art in machine
learning ( parr2019neuronal ; millidge_deep_2019 ;
tschantz2020reinforcement ; millidge2020relationship ) .

In this chapter, we will provide a relatively self-contained step
through of the key mathematical results of the most recent incarnation
of the free energy principle as presented in (
friston2019particularphysics ; parr2020Markov ) , as well as the details
of the discrete-state-space active inference process theory (
friston2015active ; da2020active ) . While none of the material in this
chapter is original, it is necessary (especially the material on active
inference), to understand what is to come in later chapters. Since this
thesis covers a fairly wide range of topics, each individual thesis
chapter also comes with its own literature review covering the necessary
background for the original material in that chapter.

It is important to note that the material in the first section of this
chapter (walkthrough of the Free Energy principle as described in
friston2019particularphysics ) is not original to me and draws heavily
from an unpublished monograph ( friston2019particularphysics ) , albeit
a monograph which has widely been viewed as the canonical reference
point for the theory. Additionally, many core elements of the theory
presented here have been published elsewhere ( friston2013life ;
friston2020some ; parr2020Markov ; friston2020sentience ) . Notably, a
fair amount of the material covered here is also controversial within
the community and the validity of many required assumptions still
remains to be assessed. Where relevant, in this section, we provide
additional disclaimers highlighting core assumptions and potentially
problematic elements of the mathematical exposition – and additionally
in the discussion section we include an itemized list of all assumptions
as well as critical discussions on each. While some of this material is
somewhat extraneous to the original work covered in later chapters, we
believe that this presentation of the free energy principle gives the
reader valuable context into the broader paradigm of the FEP which has
inspired much of the original work in this thesis. Additionally, by
condensing the logical flow of the FEP, and providing a detailed
critical discussion of the logic and assumptions required, we aim to
provide a broader service to the community by helping to make clear the
current state as well as current controversies and debates at the
cutting-edge of the free energy community.

Finally, it is important to note that the process theories derived from
the FEP – which we will primarily focus on in the rest of the thesis –
do not strictly require the full mathematical structure of the FEP to
hold for their validity. As scientific theories about the real world,
they rise or fall on empirical considerations independently of the
overall mathematical construct of the FEP, and thus while the material
in this chapter is useful contextually, it is not necessary to
understand the work in the chapters that follow. Throughout we have
aimed to make sure that each chapter, by and by large, is ‘modular’, so
that they can be read and understood in isolation. As such, we have
striven to ensure that each chapter contains sufficient background
information within it to let it be understood and evaluated
independently of the others.

### 2.1 History and Logical Structure

Historically, the free energy principle has evolved over the course of
about fifteen years. Its intellectual development can best be seen in
two phases. In the first phase, an intuitive and heuristic treatment
emerged with friston2006free which stated that the imperative to
minimize variational free energy emerged from a necessary imperative of
minimizing the system’s entropy, or log model evidence, which is upper
bounded by variational free energy. This imperative emerges due to the
self-sustaining nature of biological systems such as brains, in that
they maintain a set distribution against the inexorably increasing
entropic nature of thermodynamic reality ( friston2009free ) . In order
to do so, systems must constantly seek to reduce and maintain their
entropy across their state space. Since the VFE is computationally
tractable while the entropy itself is not, it was postulated that neural
systems maintain themselves by implicitly minimizing this proxy rather
than the actual entropy itself ( friston2010free ) .

Later, in the second phase ( friston2013life ) , this heuristic argument
and intuition was related more formally to concepts in stochastic
thermodynamics ( friston2012ao ; friston2012free ) . Specifically, the
framework developed mathematically into a description of stochastic
dynamics (as stochastic differential equations) separated into
‘external, internal, and blanket’ states by a statistical construct
called a Markov Blanket . This blanket makes precise the statistical
independence conditions required to make sense of talking about a
‘system’ as distinct from its ‘environment’. Moreover, by separating the
‘blanket’ into ‘sensory’ and ‘active’ states, one can obtain a
statistical description of the core elements of a
perception-action-loop, a central concept in cybernetics, control
theory, and reinforcement learning. Secondly, the theory developed a
precise notion of what it means to maintain a stable ‘phenotype’ which
is interpreted mathematically as a non-equilibrium steady-state density
(NESS) over the state-space. This steady state is non-equilibrium due to
the presence of ‘solenoidal flows’ which are flows orthogonal to the
gradient of the NESS density. Mathematically, such flows do not increase
or decrease the entropy of the steady-state-density, but do, however, in
contrast to an equilibrium steady state (ESS), provide a clear arrow of
time. Given this, it is claimed, that under certain conditions, one can
draw a relationship between the flow dynamics and the process of
variational Bayesian inference through the minimization of the
variational free energy (VFE)– which measures the discrepancy between an
approximate posterior and generative model – and that the dynamics that
result from this specific kind of flow under a Markov blanket at the
NESS density can be seen as approximating a gradient descent upon the
VFE, thus licensing the interpretation of the system as performing a
basic kind of Bayesian inference or, ‘self-evidencing’ T (
hohwy2008predictive ; clark2015surfing )

While the intuitions and basic logical structure of the theory has
remained roughly constant since friston2013life ; friston2012free , the
mathematical formulation and some of the arguments have been refined in
the most recent friston2019particularphysics monograph and related
papers ( friston2020some ; parr2020Markov ) . These papers have drawn
close connections between the formulation of free energy principle, and
many aspects of physics including the principle of least action in
classical mechanics, and notions of information length and the arrow of
time in stochastic thermodynamics. Additionally, the Particular Physics
monograph ( friston2019particularphysics ) contains a novel
information-geometric gloss on the nature of the Bayesian inference
occurring in the system. Specifically, it argues that the internal
states of the system can be seen as points on an statistical manifold
that parametrize distributions over the external states, and that thus
the internal states can be described using a ‘dual-aspect information
geometry.’ According to this perspective, internal states evolve in both
the ‘intrinsic’ state space of the system’s physical dynamics, while
simultaneously parameterising a manifold of statistical beliefs about
external states - the so-called ‘extrinsic’ information geometry.

While the mathematical depths of the FEP often appears formidably
complex to the uninitiated, the actual logical structure of the theory
is relatively straightforward. First, we want to define what it means to
be ‘a system’ that keeps itself apart from the outside ‘environment’
over a period of time. The FEP answers this question precisely its own
way. We define a ‘system’ as a dynamical system which has a
non-equilibrium steady state (NESS) which it maintains over an
appreciable length of time, and that the dynamics are structured in such
a way that they obey the ‘Markov Blanket Condition’. Specifically,
having a NESS can be intuitively thought of as defining dynamics which
produce something like a system – i.e. a recognizable pattern of states
which persists relatively unchanged for some period of time. For
instance, we can think of the biological systems in such a manner.
Biological organisms maintain relatively steady states, against constant
entropic dissipation, for relatively long (by thermodynamic standards)
periods of time. Of course, from a purely thermodynamical perspective,
in resisting entropy themselves, biological organisms are not countering
the law of thermodynamics. To achieve their steady state requires a
constant influx of energy – hence it is a non-equilibrium steady state
(NESS). From this perspective, we can understand biological organisation
to be the process of creating ‘dissipative structures’ (
prigogine1973theory ; kondepudi2014modern ) which only manage to
maintain themselves at steady state and reduce their own entropy at the
expense of consuming energy and increasing the entropy production rate
of their environment ( prigogine2017non ) . Illustrative physical
examples of similar NESS states are Benard convection cells , and the
Belousov-Zhabotinsky reaction ( zwanzig2001nonequilibrium ) . In
practical terms, we can consider the NESS density to be the ‘phenotype’
of the system. From the perspective of the FEP, we are not usually
concerned with whether a set of dynamics possesses a NESS density, or
how convergence to the NESS density works, instead we take it as an
axiom that we possess a system with a NESS density, and are instead
concerned with the dynamical behaviour of the system at the NESS
density. While this is clearly a special case, nevertheless dynamical
systems at NESS already exhibit rich behaviours to effectively maintain
themselves there, and it is these properties which necessarily any
system which maintains itself at NESS, which are the fundamental object
of study of the FEP.

Secondly, now that we have a system which has a NESS density, and thus
exhibits some stability through time, we also require a statistical way
to separate the ‘system’ from the ‘environment’. The FEP handles this by
stipulating that any system it considers must fulfil a set of criteria
which we call the Markov Blanket conditions. These conditions, deriving
from the idea of Markov blankets in Bayesian networks (
pearl2011Bayesian ; pearl2014probabilistic ) , set forth a set of
conditional independence requirements that allow a system to be
statistically separated from its environment ¹ ¹ 1 Whenever we say
Markov Blanket, following standard use in the literature, we mean the
minimal Markov blanket – i.e. the Markov Blanket which requires the
fewest number of blanket states to achieve the required conditional
independencies. . Specifically, we require that the dynamics of the
system can be partitioned into three sets of states -- ‘internal’ states
which belong to the system of study, ‘external’ states which correspond
to the environment, and ‘blanket states’ which correspond to the
boundary between the system and its environment. Specifically, we
require the internal states to be conditionally independent of the
external states given the blanket states, and vice versa. Thus all
‘influence’ of the environment must travel through the blanket, and
cannot directly interact with the internal states of the system which
are ‘shielded’ behind the blanket ² ² 2 Interestingly, mathematically,
the MB condition and all of the FEP is completely symmetrical between
‘internal’ and ‘external’ states. Thus from the perspective of the
system, the ‘external states’ are its environment, but from the
perspective of the environment, the ‘external states’ are the system.
This means that the environment models and performs inference about the
system just as the system models and performs inference on the
environment. We can thus think of the environment-system interaction as
a duality of inference, where each tries to model and infer the other in
a loop.

Now that we have a system with a NESS density which obeys the Markov
Blanket conditions, so that we can partition it into external, internal,
and blanket states, we then wish to understand the dynamics of the
system at the NESS density, so we can understand the necessary
behaviours of the system to allow the NESS to be maintained. The
derivation of the FEP then uses the Helmholtz (Ao) decomposition (
yuan2017sde ; yuan2011potential ; yuan2012beyond ) to represent the
dynamics as a gradient flow on the log of the NESS density (which is
called the surprisal) with both dissipative (in the direction of the
gradient) and solenoidal (orthogonal to the gradient) components. Now
that we can express the flows of the system in terms of gradients of the
log NESS density, we then invoke the Marginal Flow Lemma to write out
the dynamics of each component of the partitioned dynamics (i.e.
external, internal, and blanket states) solely in terms of a gradient
flow on its own marginal NESS density. This means that we can express,
for instance, the dynamics of the internal states solely in terms of
gradient flows on the marginal NESS density over the internal and
blanket states.

Given this marginal partition, we can analyze and understand each of the
flows in each partition of the system independently. Specifically, to
understand the Ashbyan notion that ‘every good regulator of a system is
a model of the system’ , we wish to understand the relationship between
the flows of the internal and external states, which are statistically
separated from the blanket. Despite this separation, it is possible to
define a mapping between the most likely internal state, given a
specific configuration of the blanket states, and the distribution over
the most likely external state of the system. We can use this mapping to
interpret internal states as parametrizing a variational or approximate
distribution over the external states. This interpretation sets up the
‘dual-aspect’ information geometry of the internal states, since
internal state changes simultaneously represent changes in parameters of
the distribution over internal states (which can potentially be
non-parametric), and changes to the parameters of the variational
distribution over external states. This latter interpretation means that
the internal states parameterise a statistical manifold equipped with a
Fisher information metric (if the variational distribution is in the
exponential family), and in general becomes amenable to the techniques
of information geometry ( amari1995information ; ollivier2017information
) Finally, given that we can interpret the internal states as
paramterising a distribution over external states, we can reconsider the
gradient flow upon the log NESS density with a new light. Specifically,
we can understand the marginal NESS density to represent the implicit
generative model of the system, and the gradient flow dynamics as a
descent upon the free energy, with a perfect Bayes-optimal posterior.
Alternatively, if we invoke an approximate posterior distribution over
external states which is parametrized by the internal states, we can
represent the gradient flow of the internal states as performing an
approximate minimization of the variational free energy (VFE), and thus
the internal states of the system can be interpreted as performing
approximate variational Bayes. This is the key result of the FEP. It
states, simply, that the necessary dynamics of any system that maintains
itself at a non-equilibrium steady state, and possesses a Markov
Blanket, can be interpreted as modelling, and performing approximate
variational inference upon the external states beyond its own Markov
Blanket. It thus generalizes and makes precise Ashby’s notion that every
good regulator must in some sense be a model of the system (
conant1970every ) . Here we see that in order to maintain a
non-equilibrium steady state, to counteract the dissipative forces
inherent in thermodynamics, it is necessary to perform some kind of
inference about the environment beyond the system itself.

### 2.2 Formulation

Here we begin the precise mathematical description of the FEP. We aim to
provide a consistent notation, and more detailed derivations of key
results than are often presented. The presentation in this chapter
mostly follows the order of presentation in friston2019particularphysics
, although many circumstantial topics are omitted to focus on the main
flow of the argument. We begin with the basic mathematical setting and
formulation of the theory. We assume that the dynamics we wish to
describe can be expressed in terms of a Langevin stochastic differential
equation ( jaswinskistochastic ) ,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath is a vector of states of some dimensionality, and f(x) is
an arbitrary nonlinear but differentiable function of the states.
Expressing the dynamics in terms of a Langevin stochastic differential
equation is a very flexible parametrization of the dynamics, and is the
standard form studied in the field of stochastic differential equations,
thus allowing the immediate use of results from that field.
Specifically, here we assume already that this process is not history
dependent. The dynamics only depend on the instantaneous values of the
states. In practice, history dependent systems can be represented in
this fashion, albeit somewhat unintuitively by adding sufficient
statistics of the history to the state itself. @xmath is assumed to be
white (zero autocorrelation) Gaussian noise with zero mean such that
@xmath = @xmath where @xmath is the half the variance of the noise. Zero
autocorrelation means that the covariance between the noise at any two
time instants, even infintesimally close together, is 0 – @xmath . We
assume that this noise is added additively to the dynamics.

This stochastic differential equation can also be represented not in
terms of dynamically changing states, but in terms of a dynamically
changing probability distribution over states. This transformation is
achieved through the Fokker-Planck equation, by which we can derive that
the change in the distribution over states can be written as,

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

Where @xmath is the instantaneous distribution over the states at a
given time @xmath . Here @xmath is the gradient function and simply
denotes the vector of partial derivatives of the function @xmath with
respect to each element of the vector @xmath . @xmath . @xmath
represents the matrix of second partial derivatives of the function.

Next, we presuppose that the dynamics expressed in Equation 2.1 tend
towards a non-equilibrium steady state @xmath where we represent the
steady state distribution as @xmath . Note that this distribution no
longer depends on time, since it is by definition at a steady state. We
use @xmath to make clear that this distribution is at steady state. By
definition a steady state distribution does not change with time, so
that @xmath .

The distinction between an equilibrium steady state and a
non-equilibrium steady state (NESS) distribution is subtle and
important. An equilibrium steady state, mathematically, is one where the
property of detailed balance holds. This means that any transition
between states at equilibrium is just as likely to go in the ‘forwards’
direction as it is to go in the ‘backwards’ direction. In effect, the
dynamics are completely symmetric to time, and thus there is no notion
of an arrow of time in such systems. Conversely, a non-equilibrium
steady state is one where detailed balance does not hold, so there is a
directionality to the dynamics, and thus an arrow of time, even though
the actual distribution over states remains constant. From a
thermodynamic perspective, the equilibrium-steady-state is the
inexorable endpoint of the second law of thermodynamics, since it is the
maximum entropy state. Conversely, a NESS is not a maximum entropy
solution, since the directionality of the dynamics means that there is a
degree of predictability in the system which could in theory be
exploited to produce work. Non-equilibrium steady states can arise in
thermodynamic systems but require an external source of driving energy
as a constant input to the system, which is then dissipated to the
external surroundings and gives the NESS a positive entropy production
rate. To take an intuitive example, we can think about the thermodynamic
equilibrium of a cup of coffee with cream added. The equilibrium steady
state (ESS) is when the coffee and cream have completely diffused into
one another, so that the cream maintains a constant proportion
throughout the entire coffee cup. This will be the inevitable result (by
the second law of thermodynamics) of adding an initially low entropy
highly concentrated cream scoop into the coffee. On the other hand, we
can think of the non-equilibrium steady state (NESS) as to be when the
cream and coffee are equally diffused throughout, but somebody ³ ³ 3 Of
course the analogy fails here since this represents a system with
external driving (the person stirring) whereas the true NESS has no
external driving and as such is just ‘intrinsically being stirred’ with
no stirrer. is constantly stirring the coffee in a specific direction.
Here, we are at steady state because the concentrations of cream and
coffee don’t change over time, but nevertheless there is a
directionality to the dynamics in the direction of the stirring. This
directionality is only maintained due to a constant input of energy ⁴ ⁴
4 It’s important to note that here we are using physical intuition and
concepts like ‘energy’ in a purely metaphorical sense. All results here
apply to arbitrary SDEs which do not necessarily follow the same
constraints as physical systems – i.e. respect conservation of energy to
the system (the stirring) ⁵ ⁵ 5 Interestingly, physical experience with
this analogy would suggest that the solenoidal dynamics leading to NESS
would lead to faster convergence to the NESS density compared to the
strictly dissipative dynamics leading to ESS – effectively, stirring
helps the cream diffuse faster. This insight has been applied to the
design of highly efficient Markov-Chain-Monte-Carlo samplers in machine
learning ( metropolis1953equation ; neal2011MCMC ;
betancourt2013generalizing ) . The flow caused by the stirring is
referred to as the ‘solenoidal flow’ and mathematically is necessarily
orthogonal to the gradient of the steady state distribution. This is
necessary so that the solenoidal flow does not ascend or descent the
gradient of the density, and thus change the steady state distribution
which, as a steady state, by definition cannot change ⁶ ⁶ 6 Importantly,
in this coffee-cream example, we are not claiming that if the stirring
is removed then it will settle into a different steady state
distribution, merely that the steady state with the stirring is
non-equilibrium steady state (NESS) while the steady-state without
stirring is an equilibrium steady state (ESS) due to the lack of
solenoidal flow. Adding solenoidal flow to an ESS always can generate a
NESS while the converse is not true. There are NESSs which can exist
solely in virtue of their solenoidal dynamics without a corresponding
ESS. An example of this would be a spinning top, which remains spinning
solely due to its solenoidal motion. . Biological self organizing
systems are often considered to be ‘dissipative structures’, or
non-equilibrium steady states from the perspective of thermodynamics (
prigogine1973theory ; kondepudi2014modern ) , since they maintain a
relatively steady state over time which requires a constant influx of
energy to maintain.

Given that we presuppose a system with a NESS density, we wish to
understand the dynamics at the NESS density – specifically, how does the
solenoidal flow help prevent the system from relaxing into an
equilibrium-steady-state (ESS)? To understand this, we utilize the
Helmholtz decomposition ( yuan2017sde ; yuan2012beyond ; friston2012ao )
to rewrite the dynamics at the NESS into a form of a dissipative and
solenoidal ascent upon the gradient of the log NESS density,

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

Where @xmath is a dissipative component of the flow which tries to
ascend the log density. It is the amplitude of the random fluctuations
in the original SDE formulation ( jordan1998variational ;
yuan2010constructive ; yuan2011potential ) , which in effect are
constantly trying to ‘smooth out’ the NESS density and increase its
entropy. Conversely, the @xmath represents the solenoidal portion of the
flow which, although orthogonal to the gradient of the log potential,
successfully counteracts the dissipative effects of the @xmath terms to
maintain the dynamics at a steady state. While @xmath and @xmath can in
theory be state-dependent, from here on out we typically assume that
they are not – @xmath ; @xmath , and additionally assume that @xmath is
a diagonal matrix ⁷ ⁷ 7 Technically, we only need to assume a
block-diagonal matrix, but we also typically also assume that the noise
in each state dimension is independent , so there is no
cross-correlation between states in the noise added to the system.

It is straightforward to verify that the Helmholtz decomposition of the
dynamics satisfies the steady state condition @xmath by plugging this
form into the Fokker-Planck equation (Equation 2.2 ),

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (2.4)
  -- -------- -------- -- -------

Where the last line follows because, by definition, the gradient of the
solenoidal flow with respect to the gradient of the log density is 0,
since the solenoidal flow must be orthogonal to the gradient of the
density, which is represented by the solenoidal @xmath matrix being
antisymmetric @xmath .

### 2.3 Markov Blankets

From these preliminaries, we have a set of dynamics of states @xmath ,
which possess a NESS density, and we can express the dynamics at the
NESS density in terms of dissipative @xmath and a solenoidal @xmath
flows on the gradient of the log density. Now, we begin to explore the
statistical structure of these dynamics in terms of a Markov Blanket.
Specifically, we next require that we can partition the states @xmath of
the dynamics into three separate units. External states @xmath ,
internal states @xmath , and blanket states @xmath such that @xmath .
Intuitively, the external states represent the ‘environment’; the
internal states represent the ‘system’ we wish to describe, and the
blanket states represent the statistical barrier between the system and
its environment. For instance, we might wish to describe the dynamical
evolution of a simple biological system such as a bacterium in such a
manner. Here, the internal states would describe the internal cellular
environment of the bacterium – the cytoplasm, the nucleus, the ribosomes
etc. The external states would be the environment outside the bacterium,
while the blanket states would represent the cell membrane, sensory
epithelia, and potentially active instruments such as the flagella which
interact physically with the external environment. The key intuition
behind the FEP is that although all influence between external and
internal states is mediated by the blanket states, simply maintaining
the non-equilibrium steady state against environmental perturbations
requires that the internal states in some sense model and perform
(variational) Bayesian inference on the external states. The Markov
Blanket condition is straightforward. It simply states that the internal
and external states must be independent given the blanket states,

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

While in probabilistic terms this factorisation is straightforward, it
has more complex consequences for the dynamical flow of the system.
Firstly, we additionally decompose the blanket states into sensory
@xmath and active @xmath states such that @xmath and thus, ultimately
@xmath . Sensory states are blanket states that are causal children of
the external states – i.e. the states that the environment acts on
directly. Active states are those blanket states that are not causal
children of the external states. Essentially, external states influence
sensory states, which influence internal states, which influence active
states, which influence external states. The circular causality implicit
in this loop is what allows the Markov Blanket condition to represent
the perception-action loop. For notational purposes, we also define
autonomous states @xmath which consist of active and internal states,
and particular states @xmath which consist of sensory, active, and
internal states.

The next step is to understand what the conditional independence
requirements put forth in Equation 2.5 imply for the dynamics of the
flow. Specifically, we obtain the marginal flow lemma (see
friston2019physics for a full derivation), which states that the
marginal flow of a partition, averaged under its complement, can be
expressed as an Ao-decomposed flow on the gradients of the log of the
marginal distribution. For instance, the flow of the internal states
@xmath , averaged under the complement @xmath of the particular states
@xmath can be expressed as,

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

Importantly, we see that the marginal flow lemma allows us to express
the flow of a subset of states, averaged under their complements, in
terms of independent Helmholtz decompositions on their marginal NESS
densities, if we ignore solenoidal coupling terms (such as @xmath ).
This allows us to investigate in detail the information-theoretic
interactions of one set of states with another, and allows us to gain
intuition and understanding of the core information-theoretic properties
of the perception-action loop. For instance, using the marginal flow
lemma we can express the flow of autonomous (active and internal) @xmath
as,

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where we see that autonomous states follow a gradient descent on the
marginal NESS density of the internal, sensory, and active states, and
attempt to suppress their surprisal or, on average, their entropy. We
can use a series of mathematical ‘inflationary devices’ to express this
surprisal in terms of its interaction with the external states beyond
the blanket.

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (2.8)
  -- -------- -------- -- -------

Thus we can see that the flow of autonomous states acts to minimize the
inaccuracy (maximize accuracy) and minimize the complexity of the
external states with respect to the particular states of the system in
question. Parsed into more intuitive terms, we can thus see that the
flow of ‘system’ states ( @xmath ) aim to maximize the likelihood of the
internal states given the external states – i.e. perform maximum
likelihood inference on themselves (c.f. ‘self evidencing’ (
hohwy2016self ) ) – while simultaneously minimizing the complexity – or
the divergence between the external states given the internal states,
and the ‘prior’ distribution over the external states. In short, by
re-expressing the flow in information-theoretic terms, we can obtain a
decomposition of the entropy term into intuitive and interpretable
sub-components which can help us reason about the kinds of behaviours
these systems must exhibit.

### 2.4 Variational Inference

Variational inference is a technique and method for approximating
intractable integrals in Bayesian statistics ( feynman1998statistical ;
jordan1998introduction ; ghahramani2001propagation ;
jordan1999introduction ; fox2012tutorial ; neal1998view ) . Typically, a
direct application of Bayes-rule to compute posteriors in complicated
systems fails due to the intractability of the log model evidence, which
appears in the denominator of Bayes’ rule. While there exist numerical
or sampling-based methods to precisely compute this integral, they
typically scale poorly with the dimension of the problem – a phenomenon
which is known as the curse of dimensionality ( goodfellow2016deep ) .
Variational techniques originated from methods in statistical physics in
the 1970s and 1980s ( feynman1998statistical ) , and were then taken up
in mainstream statistics and machine learning in the 1990s (
ghahramani2001propagation ; beal2003variational ; jordan1998introduction
) where they have become an influential, often dominant approach for
approximating posteriors and fitting complex high-dimensional Bayesian
models to data ( feynman1998statistical ; jordan1999introduction ;
ghahramani2000graphical ; beal2003variational ; blei2017variational ;
kingma_auto-encoding_2013 ; dayan1995helmholtz ) .

The core idea of variational inference is to approximate an intractable
inference problem with a tractable optimization problem. Thus, instead
of directly computing a posterior distribution @xmath where @xmath is
some set of hypotheses and @xmath is the data, we instead postulate an
approximate or variational distribution @xmath which is often, although
not always, parametrized with some fixed number of parameters @xmath .
We then seek to optimize the parameters @xmath to minimize the
divergence between the approximate and true posterior,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

Unfortunately, this optimization problem is itself intractable since it
contains the intractable posterior as an element. Instead, we minimize a
tractable bound on this quantity called the variational free energy
(VFE) @xmath ,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (2.10)
  -- -------- -------- -- --------

Since the VFE is based on a divergence between the variational
distribution and the generative model @xmath , it is tractable as we
assume we know the generative model that gave rise to the data. By
minimizing the VFE, therefore, we reduce the divergence between the true
and approximate posteriors, and thus improve our estimate of the
posterior.

Secondly, the variational free energy is simultaneously a bound upon the
log model evidence @xmath , a quantity of great important for
model-selection ( geweke2007Bayesian ; friston2018Bayesian ) , and which
is usually intractable to compute due to the implicit integration over
all possible hypotheses (or parameters) @xmath .

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.11)
  -- -------- -------- -- --------

The second line follows due to the non-negativity of the KL divergence.
The VFE is the foundation of the free energy principle as, we shall
show, we can interpret self-organizing systems which maintain themselves
at a non-equilibrium-steady state to be implicitly minimizing the VFE,
and thus performing variational Bayesian inference.

It is important to note here that while the variational free energy
@xmath is not technically a KL divergence, since the two distributions
it involves do not share the same support (one being a posterior and the
other a joint), for notational convenience in this thesis we slightly
abuse the KL notation to represent free energies of one form or another.
Formally, we will use @xmath .

We can gain some intuition for the effects of minimizing the VFE by
decomposing into various constituent terms. Here we showcase two
different decompositions which each give light to certain facets of the
objective function,

  -- -------- -------- -- --------
     @xmath   @xmath      (2.12)
              @xmath      (2.13)
  -- -------- -------- -- --------

Here we see that we can decompose the variational free energy into two
separate decompositions, each consisting of two terms. The first
decomposition splits the VFE into an ‘energy’ term, which effectively
scores the likelihood of the generative model averaged under the
variational distribution, whilst the entropy term encourages the
variational distribution to become maximally entropic. Essentially, this
decomposition can be interpreted as requiring that the variational
distribution maximize the joint probability of the generative model
(energy), while simultaneously remaining as uncertain as possible
(entropy) ⁸ ⁸ 8 Interestingly, this energy, entropy decomposition is
precisely why this information-theoretic quantity is named the
variational free energy . The thermodynamic free energy, a central
quantity in statistical physics, has an identical decomposition into the
energy and the entropy. . The second decomposition – into an ‘accuracy’
and a ‘complexity’ term – speaks more to the role of the VFE in
inference. Here the accuracy term can be interpreted as driving the
variational density to produce a maximum likelihood fit of the data, by
maximizing their likelihood under the variational density. The
complexity term can be seen as a regularizer, which tries to keep the
variational distribution close to the prior distribution, and thus
restrains variational inference from pure maximum-likelihood fitting.

### 2.5 Intrinsic and Extrinsic information geometries

Now, we wish to understand the relationship between the internal states
and the external states, which are separated by the blanket states.
Importantly, the existence of the blanket means that we can define a
mapping between the most likely internal state, given a specific blanket
state, and a distribution over external states ⁹ ⁹ 9 This function is
defined if we assume injectivity between the most likely internal and
blanket states ( parr2020Markov ) . . We define the most likely internal
and external states given a blanket state as,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (2.14)
  -- -------- -------- -- --------

Next, we assume that there is a smooth and differentiable function
@xmath which maps between the most likely internal and external states
given a blanket state,

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

Importantly, we interpret the output of this function – the most likely
external states given the blanket states – as parametrizing the mean
over a full distribution over the external states, as a function of the
internal states @xmath . This allows us to interpret the flow of
internal states as parametrising distributions over the external states.

Crucially, we can say that if any given set of internal states
parametrizes a distribution over external states, then the space of
internal states effectively represent a space of distributions over
external states, parametrized by internal states. This space of
distributions may be, and usually is, curved and non-euclidean in
nature. The field of information geometry has emerged to allow us to
describe and mathematically characterise such spaces correctly (
amari1995information ; caticha2015basics ) . A key result in information
geometry is that the space of parameters of families of exponential
distributions is a non-euclidean space with the Fisher Information as
its metric. A metric is simply a notion of distance for a given space.
For instance, in Euclidean space, the metric is @xmath where @xmath is
the dimensionality of the space. We can represent general coordinate
transformers on spaces with any metric through the use of a metric
tensor @xmath . Essentially, we measure differences between
distributions in terms of the KL divergence, and thus if we want to see
how an infinitesimal change in the parameters of a distribution results
in changes to the distribution itself, we can measure an infinitesimal
change in their KL divergence as a function of the infinitesimal change
in the parameters. i.e.

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

In the case of the space of parameters of exponential distributions, the
metric tensor is the Fisher information, which arises as from the Taylor
expansion of the infinitesimal KL divergence between the two
distributions. We define @xmath . Specifically, since there is only an
infintesimal change, we can Taylor-expand around @xmath to obtain,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.17)
  -- -------- -------- -- --------

‘ Where the first two terms vanish, so we need only handle the second
term,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (2.18)
  -- -------- -------- -- --------

where @xmath is the Fisher information. Since the internal states can be
interpreted as parametrizing distributions over external states, as
parameters, they lie on an information-geometric manifold with a Fisher
information metric. This is the extrinsic information geometry.
Simultaneously, the internal states also parametrize (implicitly) a
second (empirical) distribution over the internal states. This
parametrization gives rise to a second information geometry – the
intrinsic geometry, since it represents the relationship the internal
states have to the distribution over themselves. Specifically, suppose
@xmath define the sufficient statistics of a variational density over
internal states @xmath , and @xmath define the sufficient statistics of
the variational density over external states @xmath , then we can see
that the internal states in fact parametrize two densities and thus
partake in two simultaneous information geometries. First, there is a
metric defined over the space of internal densities,

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

which is called the intrinsic information geometry. And secondly, a
metric defined over the space of external densities, parametrized by
internal states,

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

which is called the extrinsic information geometry. These well-defined
intrinsic and extrinsic information geometries, allow us to interpret
the motion of the internal as also movement on the intrinsic and
extrinsic statistical manifolds. Crucially, enabling us to make
mathematically precise the link between two conceptually distinct ideas
– dynamical motion in space, and variational inference (i.e. Bayesian
belief updating) on parameters of distributions. Using this underlying
information-geometric framework, in the next section we shall go on to
see how we can interpret the dynamics of a non-equilibrium system at
NESS as performing approximate variational Bayesian inference on its
external environment.

### 2.6 Self-Organization and Variational Inference

Here we present the key results of the free energy principle via the
free energy lemma. This says, firstly, that the dynamics of the
autonomous states can be interpreted as minimizing a free energy
functional over the external states, and thus can be construed as
performing a kind of elemental Bayesian (variational) inference.
Specifically, we will first consider the general case in terms of the
‘particular’ free energy, which stipulatively assumes that the system
obtains the correct posterior at every time-point, rendering the
traditional variational bound superfluous, and thus demonstrating that
in a way self-organizing systems maintaining themselves at NESS can be
construed as performing exact Bayesian inference on the generative model
they embody through their NESS density. We thus reach the key statement
of the FEP – that the dynamics of self-organizing systems that maintain
themselves at NESS can be interpreted as performing exact Bayesian
inference on the external states beyond their blanket or, alternatively,
they can be interpreted as approximating approximate (variational)
Bayesian inference.

We then introduce the general case of the variational free energy ,
which is in general a bound upon the log of the NESS density, and we
show in the special case of assuming that the variational distribution
over external states which is parametrized by the internal states can be
approximated by the Laplace approximation, that we can interpret the
flow of autonomous states as directly performing a descent upon the
variational free energy and thus directly performing variational
Bayesian inference. Since we, as the modeller, can specify the
variational distribution in any desired way, then this means that this
interpretation is tenable for an extremely wide range of systems. The
Laplace approximation approximates the variational distribution as a
Gaussian where the variance is a function of the curvature at the mean.
Intuitively, this assumption is that the Gaussian is tightly peaked
around the mean value. This approximation is theoretically
well-justified, due to the underlying Gaussianity of the stochastic
noise in the system, and the likely concentration of the probability
mass around the mean. Moreover, the Gaussian distribution arises
regularly in nature whenever averages over large numbers of independent
events are taken (c.f. the Central Limit Theorem (CLT)), and can thus be
considered a natural modelling choice for distribution of the mode of
the external states given the blanket, which likely is composed of
contributions from a large number of specific external states.

To recall, we can write the flow of autonomous states @xmath in terms of
a gradient descent on the log NESS density of the particular states
@xmath with both dissipative and solenoidal components via the Helmholtz
decomposition.

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

Then we can define the particular free energy as the variational free
energy, where the variational distribution over external states, is
stipulatively defined to be equal to the ‘true’ posterior distribution
over external states given the particular states @xmath ¹⁰ ¹⁰ 10 We
implicitly assume here that the variational distribution can be
stipulated to be of the same family of the true posterior, so that they
can match one another . With this assumption, we can define the
particular free energy using the standard form for the variational free
energy

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (2.22)
  -- -------- -------- -- --------

where the last line follows because the bound is always 0 since we have
defined the variational and true posteriors to be the same. Importantly,
we see that the particular free energy is then equal to the log of the
NESS density over the sensory, internal, and active states. As such, we
can rewrite the dynamics of the autonomous states directly in terms of
the particular free energy,

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

While this may seem like just a mathematical sleight of hand, it
demonstrates how systems which maintain the statistical structure of a
Markov Blanket at equilibrium can in fact be interpreted as performing
variational Bayesian inference with a correct posterior distribution.
If, conversely, we relax this assumption somewhat, so that, as is
typical for variational inference when the class of distributions
represented under the variational density does not include the true
posterior, then we retain an approximate relationship. That is, when
@xmath , we obtain,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

So we can see that in this case, we can interpret the dynamics of the
autonomous states as approximating approximate Bayesian inference. This
is perhaps the most general statement of the FEP – that the dynamics of
a system which maintains the statistical structure of a Markov Blanket
at NESS against external dissipative perturbations, can be interpreted
as performing approximate variational Bayesian inference to optimize a
distribution over the external states of the environment, parametrized
by its own internal states. The distinction between variational and
particular free energy, with the particular free energy always using the
stipulatively correct posterior, while being somewhat a mathematical
trick, is also a useful philosophical distinction to draw. In effect, we
can think of the system as always performing correct Bayesian inference,
simply because the inference is over the system itself, where the
generative model of the system is simply its NESS density. Conversely,
we can see the approximation arising from the approximate variational
distribution as being related to the imperfection of our own
understanding of the system as an exogenous modeller. The system is
perfectly happy using its Bayes-optimal posterior at all times. A
variational distribution distinct from this posterior must be, in some
sense, the creature and creation of the modeller, not of the system, and
as such the approximations to the dynamics that arise from this
approximation is due to the approximations implicit in modelling rather
than in the dynamics of the system per-se. It is also important to note
that while we have used an approximation sign, in reality the
variational free energy is an upper bound upon the log model evidence or
the particular free energy – i.e. @xmath and the approximate dynamics
can be interpreted as driving the system towards the minimization of
this bound, and thus increasing the accuracy of the approximation in a
manner analogous to the similar process inherent in variational
inference.

While in the general case above, the relationship between the dynamics
of the system and variational inference is only approximate, if we are
only interested in the mode of the external states – i.e. the most
likely external state configuration – instead of the full distribution,
then the approximation becomes exact and we can directly see that the
dynamics of the system do perform variational inference upon the mode of
the external states. Here we can see that, in a sense, the
maximum-a-posteriori (MAP) modes for the internal states precisely track
the MAP modes for the external states and thus, under the Laplace
approximation, can be seen as directly performing a minimization of the
variational free energy.

Firstly, recall from previously that we had defined the smooth mapping
between the modes of the external and internal states given the blanket
state, @xmath . By applying the chain rule to this function, it is
straightforward to derive the flow of the external mode with respect to
the internal mode,

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

Then, assuming that the mapping is invertible (requiring that the
internal states and external states have the same dimensionality), or
rather in the general case that it has a Moore-Penrose pseudoinverse, we
can express the dynamics of the internal mode in terms of the dynamics
of the external mode,

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

Similarly, we can derive the expression of the NESS density over the
external mode in terms of the mode of the internal states, which
provides a precise mapping, called the synchronization manifold ,
between the two densities, even though they are in fact separated by the
Markov Blanket,

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

Combining Equation 2.24 and Equation 2.26 and using the fact that the
flow of the external mode, by the marginal flow lemma is, @xmath , we
can express the flow of the internal mode in terms of the marginal NESS
density over the external states, thus understanding how the internal
states probabilistically track changes in their environment,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (2.27)
  -- -------- -------- -- --------

where @xmath . Crucially, this expression allows us to write the flow of
the internal mode as a gradient descent on the NESS density of the
external mode as a function of the internal mode, given the blanket,
with respect to the internal states. Fascinatingly, this relationship
takes the same general form of the Helmholtz decomposition with separate
dissipative @xmath and solenoidal @xmath components which are simply the
original dissipative and solenoidal components with respect to the
internal states modulated by the inverse of the mapping function @xmath
. In effect, this implements a coordinate transform between the
coordinates of the flow of the external states to the coordinates of the
flow of the mode of the external states, as a function of internal
states.

Now we demonstrate how we can interpret this gradient descent on the
NESS density of the mode over external states in terms of a direct
descent on the variational free energy, and thus as directly and exactly
performing variational inference. First, we must define our variational
distribution @xmath which is a distribution over the modes of external
states, given the blanket states, parametrized by the mode of the
internal states. Since we are only interested now in distributions over
the mode of the external states, a reasonable assumption is that it is
approximately Gaussian distributed due to the central limit theorem.
This means that a Laplace approximation, which is a Gaussian
approximation where the covariance is simply a function of the mean,
derived via a second order Taylor-expansion of the density at the mode,
is a good approximation to use here. We thus define the variational
density as,

  -- -------- -- --------
     @xmath      
     @xmath      (2.28)
  -- -------- -- --------

Importantly, if we substitute this definition of @xmath into the
variational free energy and drop constants unrelated to the variational
parameters @xmath , we obtain,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The second line follows since this is the only term where @xmath is
directly utilized. Then, from this definition, we can see that the
variational free energy is actually precisely the gradient term we see
in the expression for the flow of the internal state mode, thus allowing
us to rewrite it as,

  -- -- -- --------
           (2.29)
  -- -- -- --------

After this thicket of mathematics, we thus see a crucial result for the
FEP. That, with a Laplace-encoded variational density, we can see that
the mode of the internal states precisely tracks the mode of the
external states, and the dynamics that allows it to do so are precisely
those of a gradient descent on the variational free energy, thus
enabling an exact interpretation of the flow of the internal states as
performing Bayesian inference on the external states. This proof
demonstrates the fundamentally Ashbyan nature of self-organization at
non-equilibrium steady state, where systems, in order to maintain their
steady state, and thus existence as distinct systems, are necessarily
forced to engage in some degree of modelling or tracking external states
of the environment, in order to counter their dissipative perturbations.
Interestingly, this exact relationship to variational inference only
emerges when considering the modes of the system, not the full
distribution over environmental and internal states as was done
previously, where we only obtained an approximation to variational
inference. Perhaps this is because, in some sense, the system need not
perform inference on full distributions, but only on modes. This perhaps
makes more intuitive sense within the cybernetic Ashbyan paradigm where,
in general, the system is seen as significantly smaller than the
environment, and thus simply cannot be expected to encode a fully
accurate model of the entire environment which, in the extreme case,
includes the entire rest of the universe. Instead, the system simply
models and tracks coarse-grained environmental variables such as the
mode.

### 2.7 The Expected Free Energy and Active Inference

So far, we have only considered the relationship between internal and
external states, and observed that the flow of the internal state can be
considered to be performing a variational gradient descent on the
parameters of the variational density over external states. The internal
state dynamics exactly follow a variational gradient descent if we
assume that the internal states parametrize a Laplacian approximate
posterior, or they approximately follow a variational gradient descent
if we assume a broader class of variational posteriors. From this, we
can interpret the flow of the internal states as performing some kind of
‘perceptual’ inference about the causes of fluctuations in the blanket
states – namely, the external states. But what about the active states?
How do they fit into this picture?

First, we recall from the approximate Bayesian inference lemma that we
can express the flow of the autonomous states (active and internal) in
terms of an approximate gradient descent on the variational free energy
(Equation 2.29 ). By the marginal flow lemma, if we ignore solenoidal
coupling between internal and active states, we can partition this
descent into separate (marginal) descents on the internal and the active
states, allowing us to write the flow of the active states as

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

where @xmath and @xmath are the block matrices corresponding solely to
the interactions between active states in the larger @xmath and @xmath
matrices. Crucially, if we recall the definition of the variational free
energy,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.31)
  -- -------- -------- -- --------

Crucially, the only term in this decomposition that depends on the
active states @xmath is the first inaccuracy term. Thus, we can
straightforwardly write down the flow of the active states as,

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

Where we can intuitively see that the flow of the active states
effectively minimize inaccuracy (or maximize accuracy). In effect, we
can interpret the flow of the active states at the NESS density to try
to ensure that the variational ‘beliefs’ encoded by the blanket and
internal states of the system are as accurate as possible. Since active
states can only influence external states and not internal states, the
way this is achieved is by acting upon the external states to bring them
into alignment with the beliefs represented by the internal states –
hence active inference .

While this provides a good characterisation of the flow of the system at
equilibrium, we are often also interested in the properties of dynamical
systems as the self-organize towards equilibrium. Specifically, we wish
to characterise the nature of the active states during this process of
self-organization, so that we can understand the necessary kinds of
active behaviour any self-organizing system must evince. To begin to
understand the nature of this self-organization we first define another
information theoretic quantity, the Expected Free Energy (EFE) which
serves as an upper-bound on surprisal throughout the entire process of
self-organization, with equality only at the equilibrium itself. Since
we have this upper-bound, we can interpret self-organizing systems away
from equilibrium, by following their surprisal dynamics as approximating
expected free energy minimization, using logic directly analogous to the
approximate Bayesian inference lemma. Conversely, turning this logic
around lets us construct self-organizing systems by defining some
desired NESS density, and then prescribing dynamics which simply
minimize the EFE.

To handle systems away from equilibrium, we define some new terminology.
We define @xmath to be the probability density over the variables of the
system at some time @xmath , which depends on some set of initial
conditions @xmath . To simplify, we average over the external initial
condition and only represent the particular initial condition @xmath .
Next we define the expected free energy @xmath similarly to the
variational free energy, but with the current-time predictive density
taking the place of the approximate variational posterior, and the NESS
density taking the place of the generative model.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.33)
  -- -------- -------- -- --------

We see that the EFE mandates the minimization of both ambiguity (i.e.
avoiding situations which are heavily uncertain) and risk (avoiding
large divergences between the current state density and the equilibrium
state. It is straightforward to see that the EFE is an upper bound on
the expected predictive surprisal at any time-point, by using the fact
that the KL-divergence is always greater than or equal to 0,

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Similarly, it is straightforward to see that at equilibrium, the EFE
simply becomes the surprisal.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.34)
  -- -------- -------- -- --------

Since this is the case, we can understand the EFE as effectively
quantifying the discrepancy between the current predictive density and
the equilibrium. Because of this, we can see that the EFE is necessarily
a Lyapunov function of self-organizing dynamics, and it makes sense to
interpret self-organizing dynamics under a Markov blanket as minimizing
the EFE. Conversely, if one wants to define a set of dynamics that
self-organize to some given attractor @xmath then one simply needs to
define dynamics that minimize the EFE to achieve convergence to the
equilibrium (in the case where there are no local minima).

Taking this converse approach allows us to move from simply providing an
interpretative characterisation of given dynamics in terms of inference,
and move instead to constructing or defining systems, or agents, which
can achieve specific goals. This approach is taken in the literature on
active inference process theories ( friston2012active ;
friston2015active ; friston2017active ; da2020active ) where instead of
simply describing a given stochastic differential equation, we instead
consider the NESS density to be the preferences or desires of the agent
often represented as a Boltzmann distribution over environmental rewards
@xmath and the active states (the agent’s actions) being computed
through a minimization of the EFE, with this minimization either taking
place directly as a gradient descent in continuous time and space (
friston2009reinforcement ) or else as an explicit model-based planning
algorithm as in the discrete-time and discrete-space formulation (
friston2017process ; tschantz2020reinforcement ; millidge_deep_2019 ;
millidge2020relationship ) .

### 2.8 Philosophical Status of the FEP

It is worth stepping back from the mathematical morass, at this point,
to try to define at a high level what kind of theory, philosophically
speaking the FEP is, and what kind of claims it makes. There have been
numerous debates in the literature about whether the FEP is
‘falsifiable’, or whether it is ‘correct’, and whether or not it makes
any specific, empirical claims ( williams2020brain ; andrews2020math ) .
However often debates on this matter are obscured or confused by the
challenging and deep mathematical background required for a full
understanding of the specifics of the FEP. It is clear from the
mathematics that the FEP offers only an ‘interpretation’ of already
extant dynamics. In short, FEP presupposes the existence of the kinds of
dynamics it wishes to make sense of – dynamical systems which organize
themselves into a non-equilibrium steady state, and which maintain the
requisite statistical independency structure of the Markov Blanket
condition. Once these conditions are satisfied, the FEP gives an
interpretation of the dynamical evolution of such a system as performing
a kind of variational Bayesian inference whereby the internal states of
the system (defined by the Markov Blanket partition) can be seen as
inferring or representing external states which are otherwise
statistically isolated behind the Markov Blanket. Crucially, the FEP, in
its most general formulation does not make any specific predictions
about the flow of the system. It offers an interpretation only. While
systems that implement the FEP can be derived, and several process
theories have been explicitly derived from within the FEP framework (
friston2005theory ; friston2015active ) , all such theories necessitate
making specific and ultimately arbitrary modelling choices, such as of
the generative model and variational density. Such choices sit below the
level of abstraction that the mathematical theory of the FEP exists at.
The FEP offers a mathematical interpretation only of certain dynamical
structures.

The FEP is often compared and analogised to the principle of least
action in physics ( lanczos2012variational ) which allows one to
describe many physical processes (although not all) as minimizing the
path integral of a functional called the ‘action’ over a trajectory of
motion ( sussman2015structure ) . This argument is often used to claim,
in my opinion correctly, that the FEP is a mathematical ‘principle’ or
interpretation and therefore cannot be falsified or empirically tested.
In my opinion, however, the principle of least action is, in its
philosophical status, not directly analogous to the FEP. While the
relationship between the path integral of the action and the dynamics
prescribed by the Euler-Lagrange equations is simply a mathematical
truth, the principle of least action itself, as applied to physics
contains a fundamentally empirical and falsifiable claim – that physical
systems in the real world can be well described through its own
mathematical apparatus – that is of dynamics derived from minimizing an
action. This claim is in principle falsifiable. Not all dynamical
systems can be derived from least action principles. If physical systems
predominantly came from the class that cannot be so derived, the
principle of least action in physics would be effectively falsified, and
the mathematical apparatus underlying it would have become nothing more
than an arcane mathematical curiosity. So far as we know, there is no
a-priori reason why much of physics can be so well understood through
action principles, and indeed there are areas of physics – such as
statistical mechanics and thermodynamics, and dissipative
non-conservative systems in general – which cannot (so far) be described
straightforwardly in these terms.

It appears a closer physics analogy to the FEP might be one direction of
Noether’s theorem. Noether’s theorem proves a direct correspondences
between symmetries or invariances in a given system, and conservation
laws. For instance, in physical systems, time-translation symmetry
implies the conservation of energy, and rotational symmetry (of the
underlying euclidean space, not any given object within it) implies the
conservation of angular momentum. The FEP, similarly, can be thought to
show a correspondence between the dynamics of a certain kind of system
(NESS density, Markov Blanket conditions) and the dynamics of
variational Bayesian inference. Interestingly, while the ‘forward’
direction from the NESS density and Markov Blanket conditions is treated
in the FEP, any reverse conditions – i.e. whether the presence of
Bayesian inference dynamics implies any kind of statistical structure
upon the dynamics of the system remains unclear, and this is likely a
fruitful direction for further theoretical work. Noether’s theorem,
unlike the principle of least action, matches more closely than the
principle of least action since it only specifies correspondences
between certain kinds of mathematical objects (symmetries and
conservation laws) just as the FEP only specifies a correspondence
between dynamical flows at NESS of a system with a Markov Blanket, and
the gradient flows on the variational free energy.

While its status as a mathematical principle and interpretation only can
shield the FEP from the possibility of an empirical ‘falsification’,
this does not mean that the theory is not subject to some kind of
implicit intellectual review. Much of the core motivation behind the FEP
has been to try to derive universal properties of the kind of biological
self-organizing systems which give rise to structured behaviour
including relatively ‘high level’ processes such as the
perception-action-loop, explicit perception and inference about the
causes of the external world and, ultimately, prospective inference and
planning. For instance, much of the FEP literature has been focused on
and applied to understanding brain function ( friston2008hierarchical ;
friston2015active ; friston2017process ) . This ambition renders the FEP
open to questions about its ‘applicability’, if not its falsifiability.
The FEP imposes relatively stringent conditions that dynamical systems
must satisfy for the logical steps in the FEP to hold. In the next
section, we present a detailed itemized list and critical discussion of
all the assumptions required. Some especially key assumptions, which
substantially restrict the potential class of systems the FEP can apply
to are:

-   That the system in question can be adequately represented as a
    Langevin equation (i.e. the system is Markov and does not depend on
    history) with additive white Gaussian noise.

-   that the dynamical system as a whole have a well-defined NESS
    density (including over the external states).

-   that the system obey the Markov Blanket conditions, which are, in
    general, relatively restrictive about the kinds of flows that are
    possible, and appear to have become more restrictive in
    friston2020some , which precludes any solenoidal coupling between
    active and sensory states (indeed the didactic treatments of the
    free energy lemma typically require a block-diagonal Q matrix,
    meaning no solenoidal coupling between subsets of states). If this
    assumption is relaxed, then there are additional solenoidal coupling
    terms in the flow of the internal states, so at best one can say
    that gradient ascent upon the surprisal is a component of the flow .

-   That there be an injective mapping between the most-likely internal
    state given the blanket and the mode of the distribution of external
    states given the blanket states, which is additionally smooth and
    differentiable (this is required for the dual-aspect information
    geometry, and thus the identification with Bayesian inference).

These conditions are quite strict about the class of systems that the
FEP can apply to, and it is unclear if ‘real systems’ of the kind that
FEP desires to explain – such as biological self-organization, and
especially brains, can fulfil them. If it turns out that such systems
flagrantly violate the conditions for the FEP, then the FEP cannot be
said to apply to them and thus cannot be of use in understanding them,
even as an interpretatory device. In this case, the FEP would fail the
applicability criterion, and would cease to be particularly useful for
its original goals of neuroscience, even if it remains not technically
falsified and does, in fact, apply to some obscure mathematical class of
dynamical systems. Importantly, many of the assumptions of the FEP, when
interpreted strictly, do not appear to hold in general for complex
biological systems such as brains. For instance, to take extreme but
illustrative examples, it is clear that no biological system is ever in
a true non-equilibrium steady state, since eventually all such organisms
will age and die, and indeed eventually the entire universe will likely
decay to a thermodynamic equilibrium state. Additionally, the Markov
Blanket assumption is directly violated by things such as x-rays (and
indeed gravity) which can directly interact with ‘internal states’ of
the brain, such as neurons, without first passing through the Markov
Blanket of the physical boundaries of the brain and the sensory
epithelium. As such, for a real physical system, we must take the
assumptions of the FEP to be only approximations, which hold locally, or
approximately, but not for all time and with complete perfection. It
remains to be seen, and empirically investigated if possible, the extent
to which the mathematical interpretations and logical statements of the
FEP remain robust to such slight relaxations of its core assumptions.

While the FEP provides a mathematical interpretation of certain kinds of
dynamics in terms of inference, it also, largely, remains to be seen
whether such an interpretation is useful for spurring new ideas,
questions, and developments within the fields the FEP hopes to influence
– such as neuroscience, cognitive science, and dynamical systems theory.
Returning to our anaologies of the least action principle and Noether’s
theorem, while both of these mathematical results only provide
interpretations of known dynamics, by operating at a high level of
abstraction they provide powerful capabilities for generalization. For
instance the principle of least action allows for dynamics to be
derived, via the Euler-Lagrange equations, directly from the high level
specification of the action. For instance, the potentially new or
counterfactual laws of physics can be derived simply by postulating a
given Lagrangian or Hamiltonian and then working through the
mathematical machinery of the principle of least action to derive the
ensuing dynamics. Additionally, by investigating invariances in the
action, one can often understand the kinds of invariances and degrees of
freedom that exist in the actually realized dynamics. Similarly,
Noether’s theorem allows one to play with setting up certain conserved
quantities or symmetries a-priori, and then work out precisely the
consequences that these entail for the dynamics.

It is currently unclear to what extent the FEP offers such powerful
advantages of abstraction and generalization. This is largely due to the
FEP being immature as a field compared to the cornerstones of classical
physics, and the majority of the research effort so far has gone into
making the theory precise rather than deriving consequences and
generalizations from it, but there are some promising initial signs
which have just begun to emerge in the literature of the power the FEP
perspective offers. From a practical perspective, the FEP appears to
offer a number of novel techniques. Firstly, given a desired NESS
density, the free energy lemma provides a straightforward way of
deriving dynamics which will necessarily reach that density, due to the
fact that the variational free energy becomes a Lyapunov function of the
system as a whole. This approach has strong potential links to
Markov-Chain-Monte-Carlo methods in machine learning and statistics,
which aim to approximate an intractable posterior distribution by the
time evolution of a Markov process ( metropolis1953equation ;
neal2011MCMC ; betancourt2017conceptual ; chen2014stochastic ;
brooks2011handbook ) . The FEP provides a new perspective on such
systems as fundamentally performing variational Bayesian inference, and
may in future be used to develop improved algorithms in this domain,
akin to the developments of Hamiltonian ( betancourt2013generalizing )
and Riemannian MCMC ( girolami2011riemann ) methods. For instance, there
is much potential in the idea of solenoidal flow speeding up convergence
to the desired equilibrium density ( ma2015complete ) . Conversely, the
FEP, through the Helmholtz decomposition, may additionally provide tools
for inferring the eventual NESS density given a specific set of dynamics
( ma2015complete ; friston2019particularphysics ) . This would allow,
again, for an analytical or empirical characterisation of the ultimate
fate of a system, and allow for characterising different kinds of
systems purely by their dynamics far from equilibrium.

A second strand of potentially directly useful research which has begun
to arise from the FEP is empirical and statistical methodologies for
defining, computing, and approximating Markov Blankets. This implies the
ability to infer the statistical independency structure of the dynamics
purely either from analytical knowledge of the dynamics or,
alternatively, from purely observed trajectories. There are already two
approaches to achieve this in the literature. One which utilizes graph
theory in the form of the graph Laplacian to infer nodes of the Markov
blanket based on the parents, and children of parents of the largest
eigenstates of the Jacobian ( palacios2017biological ; friston2013life ;
friston2020parcels ) . A second approach directly uses the Hessian of
the dynamics to attempt to read off the conditional independency
requirements it implies ( friston2020parcels ) . These approaches may
have substantial merit and utility in understanding the effective
statistical independency structure of complex dynamical processes,
especially questions regarding functional independence in the brain.
This strand of research heavily relates to the question of abstraction
in dynamical systems – namely, whether complex systems can or cannot be
straightforwardly partitioned into independent subsystems which can then
be abstracted over. For instance, the ideal would be the ability to,
given a complex high-dimensional dynamical system, parse this system
into individual ‘entities’ (separated by Markov blankets) which interact
with each other according to another set of (hopefully simpler)
dynamical rules. This would allow for an automatic procedure to
transform a high dimensional complex system into a simpler,
low-dimensional approximate system more amenable for analysis and,
ultimately understanding ( friston2013life ; parr2020modules ;
friston2007parcels ) .

### 2.9 Discussion of Assumptions required for the FEP

Here we provide a general overview and short discussion of every
assumption required at each stage of the FEP. Ultimately, the overall
picture that emerges is that the FEP requires many assumptions to work,
and it is unlikely that all of them can be fulfilled by the kinds of
complex self-organizing systems that the FEP ultimately ‘wants’ to be
about – such as biological self organization and, ultimately, brains.
However, this is not necessarily overly problematic for the FEP as many
of its assumptions may be approximately, or locally true over small
enough time periods. This is not necessarily a bad thing – almost all of
the sciences ultimately use simplified models to try to understand their
ultimate objects of study in a more tractable way. The FEP is simply
continuing that tradition, but if we do this, we need to make explicit
the key distinction between the model and the reality or, more
memorably, the map and the territory.

The first set of key assumptions that the FEP makes comes through the
definition of the kinds of stochastic dynamical systems that it works
with. Specifically, we make the following assumptions about the form of
the dynamics we deal with,

-   The system as a whole can be modelled as a Langevin SDE of the form
    @xmath

-   The noise @xmath is Gaussian with 0 mean and a covariance matrix
    @xmath .

-   The noise is additive to the dynamics

-   @xmath does not change with time

-   @xmath has no state dependence (no heteroscedastic noise)

-   @xmath is a diagonal matrix (each state dimension has independent
    noise)

-   The dynamics @xmath do not themselves change with time.

We also must make the following assumptions about the system as a whole,

-   The system is ergodic , which means that state and time averages
    coincide or, alternatively, that there must be some probability of
    ultimately reaching every part of the system from every other part.

-   The system possesses a well characterized
    nonequilibrium-steady-state density (NESS), which does not change
    over time

-   Once the system reaches this NESS density it cannot escape it –
    there is no metastability or multiple competing attractors.

These assumptions setup the basic formalism we wish to consider. From
here, we then apply the Ao decomposition to rewrite the dynamics in the
form of a gradient descent on the log of the potential function with
dissipative and solenoidal components @xmath . To be able to implement
this decomposition requires,

-   The dynamics function @xmath be smooth and differentiable

Now, we apply the Markov Blanket conditions at the NESS density,

-   The state space @xmath can be partitioned into a set of four states
    – internal @xmath , external @xmath , active @xmath and sensory
    @xmath which, at the NESS density fulfill the following conditional
    independence relationships: @xmath .

-   We thus require all partitions to be at NESS, including the external
    states . This means that the environment also has to be at steady
    state, not just the system.

-   We often assume no solenoidal coupling between internal and sensory
    states (internal states do not directly act on sensory states – only
    the external states do), nor between active and external states
    (active states drive the external state but are not driven by it).
    Mathematically this corresponds to @xmath , @xmath .

-   We may even require that @xmath be block diagonal – thus allowing
    for no solenoidal coupling between subsets of the Markov Blanket at
    all.

Given the Markov Blanket conditions hold, we can then begin to move
towards the free energy lemma. To begin with, we must first assume,

-   There is a unique argmax @xmath exists for both internal and
    external states for every blanket state @xmath .

-   That there exists a function @xmath which maps from @xmath to @xmath

-   That @xmath is invertible

-   That @xmath is differentiable

-   For the particular free energy, we assume that the variational
    posterior @xmath is equal to the true posterior, and thus that the
    true posterior can be represented by a vector of sufficient
    statistics ( @xmath ).

These assumptions on @xmath are quite restrictive. A more detailed
discussion of what these assumptions require can be found in the next
section of this chapter, where every restriction is listed and discussed
in some depth.

Finally, to reach the free energy lemma, we must make the following
assumptions,

-   The flow of the sufficient statistic of external states @xmath
    follows the same (Ao-decomposition) dynamics as the external states
    themselves

-   The variational distribution @xmath is a Laplace distribution
    (Gaussian) with a fixed covariance @xmath as a function of @xmath .

This first assumption has come under heavy controversy and is discussed
in more detail later. These additional assumptions pertain to the
Laplace approximation, but the final assumption here appears to go
beyond what is typically required by variational Laplace where, since
the conditional distribution is a function of the blanket, one would
expect the conditional covariance to be one too.

#### 2.9.1 Assumptions on the Form of the Langevin Dynamics

The FEP formulation makes reasonably strong assumptions about the nature
of the dynamics that it models – restricting them to the form of
stochastic dynamics which can be written as a Langevin equation with
additive Gaussian noise. While the assumptions on the dynamics function
are not that strong, only requiring differentiability and
time-independence, the restrictions on the noise in the system are quite
severe.

Firstly, it is important to note that using additive white noise, while
a common modelling assumption due to its mathematical simplicity,
nevertheless imposes some restrictions on the kind of systems that can
be modelled – especially as complex self organizing systems typically
evince some kind of colored smooth noise, as well as often power-law
noise distributions which are associated with self-organized criticality
( ovchinnikov2016introduction ) .

However, the further assumptions on the @xmath covariance matrix – that
it is diagonal, state-independent, and time-independent – are also
strong additional restrictions. Specifically, this means that the noise
to every dimension in the system is completely independent of any other
dimension, and that the noise is constant at every point throughout the
state space and throughout time.

##### Ergodicity and the Ao Decomposition

The Ao decomposition requires both that the dynamics possess a
consistent non-equilibrium steady state density (which forms the
potential function) and also that the dynamics are ergodic.
Additionally, this ergodicity assumption is implicitly used in the
Bayesian mechanics, which allows expectations of the surprisal to be
taken and interpreted as entropies, and thus to ultimately derive an
interpretation of the dynamics in terms of accuracy and complexity. In
general, for many biological and self-organizing systems, ergodicity
does not hold and such systems typically exhibit substantial amounts of
path dependence and irreversibility. This means that on a strict
reading, for most systems the FEP desires to model, the ergodicity
assumption does not hold. However, it may still be possible to describe
ergodicity as holding locally in the small region of the state space
around the NESS density and this may be sufficient for an approximate
version of the FEP to hold, although the resistance of the FEP to slight
perturbations of its assumptions remains unclear.

#### 2.9.2 The Markov Blanket Condition

##### Is Information Retained Behind the Blanket? – The Time Synchronous
Markov Blanket Condition

A potentially substantial problem, which has been raised by Martin Biehl
and Nathaniel Virgo, for the FEP is that the Markov Blanket conditions
would appear to very strongly imply that the internal states cannot
store any more information about the external states than the blanket
states. This fact can be derived from a straightforward application of
the data processing inequality. Translated into the terminology of
biological systems like brains, this would mean that the state of the
brain could contain no more information about the environment than the
state of the sensory epithelia and actuators at the current time. In
effect, this would rule out systems obeying the FEP from exhibiting any
sort of long term memory or learning – clearly a very undesirable
side-effect.

In discussions within the community, there have been many attempts to
finesse this apparent difficulty with appeals to notion of nested
temporal scales and the local applicability of the FEP. The intuitive
argument is that if the Markov Blanket conditions rule out information
storage on the macroscale where they apply locally, they may
nevertheless allow for the slow accumulation of information over a
longer timescale. Effectively, if we can imagine that there are two
kinds of variables – ‘fast’ variables which can change over the a given
timescale and ‘slow’ variables which do not. Then, if we can consider
the slow variables fixed over some timescale, then we can consider the
fast variables to reach a NESS density over that timescale, however over
longer timescales, the values of the fast variables can influence the
slow variables leading to them changing over time, and thus inducing a
different NESS density over among the fast variables. The change in the
slow variables can be considered to be learning, and could allow for the
accumulation of information over time. This process of timescale
separation is directly analogous to the classical distinction between
inference (fast) and learning (slow) in machine learning and control
theory, and can also be expressed physically in terms of an adiabatic
reduction ( friston2019particularphysics ) which explicitly separates
out the dynamics of the system into fast and slow eigenmodes. This
construction, however, does require a notion of ‘approximate’ NESS for a
timescale which is long enough for the ‘fast’ variables but also short
enough for the ‘slow’ variables to appear fixed.

##### The Real Constraints on Solenoidal Coupling?

While the Markov blanket conditions only explicitly disallow solenoidal
coupling directly between the internal and external states – @xmath ,
the free energy lemma appears to require a significantly greater
reduction of solenoidal coupling. Specifically, the free energy lemma
requires that, for a straightforward identification of the surprisal
with the free energy, that the form of the dynamics for each marginal
subset of states in the partition take the same form as the dynamics of
the full set of states @xmath . Specifically, this means that all
solenoidal coupling between the subsets must be suppressed, since if
they were not then, by the marginal flow lemma, there would be
additional solenoidal coupling terms in Equation 2.29 , which would
complicate the relation to free energy minimization with additional
solenoidal terms. As such, for the free energy lemma, as currently
presented, we appear to have the extremely strong condition of the
diagonality of @xmath , where each subset in the Markov Blanket is only
allowed solenoidal interactions with itself.

It is important to note that this restriction is significantly stronger
than those required just by the Markov Blanket condition, and indeed is
stronger even than the flow constraints proposed in friston2020some .
While this does not entirely rule out any interactions between different
subsets of the Markov blanket, it does mean that all interactions have
to be mediated through the gradient term, since both the @xmath and
@xmath matrices are assumed to be diagonal. However, it may be that the
additional solenoidal terms in the free energy lemma as a result of
non-diagonal Q are not that deleterious to the theory since as these are
purely solenoidal terms, they are orthogonal to the flow and do not
affect the ultimate minima of the system.

#### 2.9.3 Assumptions of the free energy Lemma

##### The @xmath function

The existence and general properties of the @xmath function have also
recently elicited much discussion and debate within the community.
Specifically, it is not at all clear that this function exists in the
general case, for arbitrary dynamics functions @xmath and conditional
NESS distributions @xmath and @xmath . In later papers it is assumed to
exist under the condition of injectivity between @xmath and @xmath . In
effect, this means that there must be a unique mapping between @xmath
and @xmath for all blanket states – i.e. that for every blanket state,
if the argmax of the internal states is @xmath , then the argmax of the
external states must be @xmath . Additionally, there must be a
corresponding (and separate) external argmax for every internal argmax.
There may, however, be some external argmaxes with no corresponding
internal argmaxes (although the converse condition does not hold). This
requires that the dimensionality of the external states be greater than
or equal to the dimensionality of the internal states – which should
generally hold for most reasonable systems where we can safely assume
that the environment is larger than the system itself. This injectivity
condition also guarantees invertibility in the case that the internal
and external state spaces are of the same dimension. It is also possible
to use the Moore-Penrose pseudoinverse for the case where the external
state space is larger, at the cost of the free energy lemma becoming
approximate instead of exact.

The differentiability of the @xmath function is a more stringent
condition. In many cases this is unlikely to be met, since the argmax
functions which the @xmath function maps between are generally
nondifferentiable. It remains unclear to what extent differentiable
@xmath functions can exist in systems of interest.

##### The flow of the Sufficient Statistics @xmath

An additional important assumption necessary for the free energy lemma,
is that the flow of the sufficient statistics of the external mode
follow the same flow as the external states generally. This assumption
turns out to be crucial to the free energy lemma which relies heavily in
the fact that the flow of the sufficient statistic @xmath can be written
as a gradient descent on the log surprisal – which can then be expressed
in terms of a free energy under the Laplace approximation.

This assumption is also problematic and has been the source of much
discussion within the community. The extent to which this assumption is
justified remains unclear. Specifically, it appears to rule out the use
of arbitrary functions @xmath (to be discussed in the next section) to
parametrize the external sufficient statistic (although not the internal
sufficient statistic). The assumption effectively holds to the extent to
which one can describe the sufficient statistic as equal to some
external state @xmath , which may occur often for the argmax but not
necessarily always. It remains to be seen whether the argmax is in fact
the optimal such function – which is dependent on the blanket, but which
can identify a consistent @xmath to identify with and thus partake in
the same dynamics.

##### Potential and Optimal @xmath Functions

While the didactic treatment of the FEP in (
friston2019particularphysics ; parr2020Markov ) , it is assumed that the
@xmath function relates the argmax of @xmath and of @xmath , this is a
simple assumption and is not particularly required by the theory. It
only requires that there be some function not that it necessarily be an
argmax. This means that we could, in theory use an arbitrary function
@xmath instead of the argmax. Indeed, we might desire to make this
function contain as much information as possible about the true
conditional distribution of the internal states given the external
states, so that when the @xmath function maps this to the sufficient
statistic of the external density it can be seen as performing inference
with the most information possible between the external and internal
states. An additional benefit of defining an arbitrary function for
@xmath instead of using @xmath is that we can make @xmath
differentiable, which alleviates much of the difficulty of making @xmath
differentiable as well.

While this approach brings many benefits, it also has the drawback of
the necessity to choose a suitable function @xmath which introduces
another degree of freedom into the modelling process. One possible
condition is that we could chose the optimal @xmath to be the one that
contains the most information about the internal state or, alternatively
minimizes the KL between the approximate conditional distribution over
the internal states parametrized vy @xmath and the true conditional over
the blanket states. That is, we could define,

  -- -------- --
     @xmath   
  -- -------- --

This would reduce the number of degrees of freedom of @xmath and provide
a valid modelling target, although the actual computability of this
minimization process is potentially a problem, as is whether this
objective is actually optimal. Nevertheless, the use of an arbitrary
@xmath function for the sufficient statistics of the internal states may
yet resolve or ameliorate some of the difficulties with the free energy
lemma, and is an interesting inroad to begin understanding various
relaxations or extensions to the current incarnation of the free energy
principle.

### 2.10 Active Inference

In the previous section, we have covered the very general and abstract
form of the FEP, here we elucidate the central process theory that has
emerged from the FEP literature in theoretical neuroscience – Active
Inference ( friston2012active ; friston2009reinforcement ) . Active
inference is a normative theory of perception, decision-making, and
learning which ties these three core cognitive processes together under
the general paradigm of variational inference via the minimization of
the variational free energy ( friston2015active ; friston2017process ) .
Specifically, it views all of these processes as emerging out of a
central imperative of the system to minimize its free energy over time,
and thus perform inference. Perception can be quite clearly stated as an
inference problem of inferring the hidden states and causes of the world
from sensory observations. Learning too can be interpreted as inference
over the parameters of the generative model, which takes place on a
slower timescale than perceptual inference. Finally, action selection,
decision-making, and planning can be described as inference on policies
over trajectories into the future. While there are numerous methods to
perform this inference, active inference chooses to minimize an expected
free energy functional which encodes goals in terms of a prior over
future states ( da2020active ) .

Active inference has been applied widely and productively in theoretical
neuroscience, and active inference models have been proposed for
planning and navigation ( kaplan2018planning ) , saccadic eye movements
( parr2017uncertainty ; parr2018active ; parr2018anatomy ) and visual
foraging ( parr2019computational ; heins2020deep ) , and general
planning problems in reinforcement learning and machine learning (
ueltzhoffer_deep_2018 ; tschantz2020reinforcement ; tschantz2020control
; millidge_deep_2019 ) . Additionally, by simulating various lesions or
incorrect update rules in the active inference scheme, one may obtain
interesting behavioural anomalies or shortfalls which one can then
analogize to known psychiatric disorders – an approach known as
computational psychiatry ( parr2019computational ; cullen2018active ) .
By drawing correspondences between disorders of behaviour in tractable
artificial systems and the psychiatric symptoms of disorders in humans
or other animals, one may be able to shed new light upon the actual
mechanistic underpinnings of such disorders, which may lead to novel
hypotheses, experimental protocols and, ultimately, treatments.
Computational psychiatric approaches using active inference have
pioneered statistical, mechanistic models of impulsivity (
mirza2019impulsivity ) , visual neglect ( parr2018computational ) ,
autism ( lawson2014aberrant ) , schizophrenia ( adams2012smooth )
substance use disorder and addiction ( schwartenbeck2015optimal ) , and
rumination ( hesp2020sophisticated ) . Finally, the epistemic
imperatives that arise from the minimization of the expected free energy
functional have given rise to a number of simulation studies applying
the approach to exploration tasks ( schwartenbeck2013exploration ;
friston2015active ; friston2017curiosity ) , visual foraging and other
information-seeking saccade behaviour ( heins2020deep ; parr2017active )
, and exploration in complex sparse-reward environments from
reinforcement learning ( tschantz2020reinforcement ) , which will be
significantly expanded upon in later chapters of this thesis.

Broadly, there are two main classes of active inference models in the
literature – continuous-time, continuous-state models, which are an
extension of predictive coding models of brain function (
friston2009reinforcement ; friston2010action ; pio2016active ;
baltieri2017active ; baltieri2019pid ; millidge2019combining ) , and
discrete-time discrete-state-space active inference models which have
been heavily developed in the literature in the past decade, and perhaps
now form the main theory of active inference applied to the brain (
da2020active ; friston2017process ) . All of these models, however,
ultimately are derived from the same mathematical apparatus. As such, an
advantage of active inference for modelling behaviour is that due to its
developed and shared mathematical apparatus, different models are
specified simply through the generative model and variational
distribution, and can be directly compared through Bayesian model
comparison techniques. This can be used to fit active inference models
to empirical behavioural data in a straightforward fashion. Continuous
time, continuous state based models are explained in depth in Chapter 3,
where I discuss my work with these models in the context of predictive
coding. Here, we present an introduction to discrete-state-space active
inference which shall form the basis of my work in Chapters 4 and 5 of
the thesis.

Here we introduce a more standard notation which shall be used for the
rest of the thesis. We consider our agent to be situated in a Partially
Observed Markov Decision Process (POMDP) ( sutton1990integrated ;
kaelbling1996reinforcement ) . The agent is given observations @xmath ,
and must infer the hidden states of the world @xmath that gave rise to
the observations. The agent may additionally possess models with
parameters @xmath , which can also be optimized. Finally, action
selection consists of inferring actions @xmath , or policies @xmath
which are simply sequences of actions in order to achieve some desired
goal. Active inference is based around the fundamental imperative of
minimizing the variational free energy (VFE). We may recall from
Equation 2.12 that the VFE consists of the KL divergence between a
variational distribution @xmath and a generative model @xmath .

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

If we additionally want to infer the parameters @xmath , we can extend
the generative model and variational density to include a distribution
over the parameters – this provides a fully Bayesian treatment of
parameters in contrast to many machine learning schemes which treat them
effectively as point distributions,

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

In order to implement a specific active inference scheme, the key thing
to specify is the nature of the generative model, and the nature of the
variational distribution. These two distributions suffice to completely
specify the model, and with these distributions set, the processes of
learning, inference, and action selection can be handled by the standard
mathematical apparatus of the theory. Specifically, given a specific
variational distribution and a generative model, we can implement
perception as a minimization of the VFE with respect to the variational
distribution with respect to the hidden states, and we can implement
learning as the minimization of the VFE with respect to the parameters

  -- -- -------- -- --------
        @xmath      
        @xmath      (2.37)
  -- -- -------- -- --------

There are then two separate ways to implement action. The most
straightforward approach, which is utilized in the continuous time
version of active inference is to similarly implement action as a
gradient descent on the VFE with respect to action,

  -- -- -------- -- --------
        @xmath      (2.38)
  -- -- -------- -- --------

Where we have made the implicit dependence of the observations, and
hence the VFE on action explicit, which makes such a minimization
non-trivial. A second approach, which is typically used in the
discrete-state-space paradigm is to assume a specific functional form
for the variational posterior over policies – that of a softmax
distribution over the Expected Free Energy (EFE) @xmath (
friston2015active ) ,

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

where @xmath is a softmax function. Effectively, what this states is
that the optimal policy is a softmax distributions over the
path-integrals of the EFE into the future. Effectively, the optimal
distribution over policies is simply one that selects policies in
proportion with the exponentiated EFE resulting from executing that
policy in the future. This means that the policy with the greatest EFE
is most likely, while policies with lesser EFE are exponentially less
likely to be selected based on the difference between their EFE and that
of the best policy.

Discrete state-space active inference therefore optimizes two
complementary objective functions. Optimizing the variational free
energy, which is used for perception and learning, ensures that the
agent learns an accurate world model, and is able to accurately infer
the hidden states of the world from current observations. The second
objective, the expected free energy, is used to score potential plans or
action policies, to allow the agent to make decisions which are adaptive
relative to its goals. To successfully predict and infer with
trajectories in the future requires a highly developed and accurate
world-model, able to make accurate multi-step predictions of the
consequences of action. Such a world model is provided by the
minimization of the VFE in the inference and learning steps. This
separation of inference and action selection into two separate
objectives – the VFE and the EFE – introduces a measure of complexity
into the theory which may or may not be unavoidable. In Chapter 5, we
focus especially on this question and investigate the nature of the EFE,
and whether all facets of inference, learning, and action selection can
be subsumed under a single unified objective.

#### 2.10.1 Discrete State-space models and Perception

The core component of the discrete-state-space model is the discrete
generative models and variational densities it is based upon.
Specifically, we split the generative model into a likelihood and prior
distribution @xmath , and then represent each of these distributions as
a categorical distribution,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (2.40)
  -- -------- -------- -- --------

A categorical distribution is one that simply directly assigns some
probability value to every possible discrete contingency. The parameters
@xmath and @xmath of these distributions are simply these probability
values, which can be represented straightforwardly in terms of matrices.
@xmath is simply a normalized matrix of probabilities representing the
likelihood contingencies – that is, for every hidden state @xmath , what
is the probability of each potential outcome. Similarly, the transition
matrix @xmath is a matrix of probabilities representing the probability
of transitioning from any one hidden state to any other hidden state.
Similarly, we define our variational distribution @xmath to be a
categorical distribution of the probability of each discrete hidden
state as a function of the observed observation @xmath . With our
variational and generative model set, we can explicitly write out and
evaluate the VFE,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (2.41)
  -- -------- -------- -- --------

Where we have simply explicitly written out the variational free energy
in terms of the parameters of the categorical distributions. The
expectation operator @xmath can be computed as a simple dot product
instead of an integral due to the discrete state space. Importantly,
because both our variational density and generative models are
categorical distributions, we can derive an analytical expression for
the minimum of @xmath with respect to the variational parameters @xmath
, allowing for an exact Bayes-optimal single-step update for perception,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (2.42)
  -- -------- -------- -- --------

Learning can be approached similarly, by placing suitable hyperpriors
(typically dirichlet ( schwartenbeck_computational_2019 ) ) upon the
parameters of the @xmath and @xmath matrices and then minimizing the VFE
with respect to these parameters. For more information on how learning
is implemented see da2020active ; friston2017process .

#### 2.10.2 Action Selection and the Expected Free Energy

Action selection is then handled via the variational posterior being
equal to the softmaxed path integral of the EFE through time (Equation
2.39 ). Typically, in small discrete state spaces, this path integral
can be computed exactly, by simply computing the EFE for every single
policy and every single possible trajectory through the state-space.
Unfortunately, this approach scales exponentially in the time horizon,
and is thus not suitable for long, open-ended tasks, although it remains
a highly effective method for simulating short tasks, such as single
trials in a psychophysical, or simple decision-making, paradigm (
friston_active_2015 ; schwartenbeck2015optimal ;
friston2020sophisticated ) . Various methods have been proposed to
handle this exponential complexity. One commonly proposed method is to
simply prune potential trajectories which become too unlikely (i.e. have
too low an EFE) to be worth considering further. Typically, such
methods, however, do not reduce the algorithm to a smaller (polynomial)
complexity class, but instead simply reduce the exponential coefficient
which allows the method to scale to slightly larger tasks but does not
remove the fundamental exponential complexity of the algorithm. Other
approaches involve approximating the path integral with either
bootstrapping value-function methods ( millidge_deep_2019 ) , which take
advantage of the recursive temporal decomposition of the EFE, or
alternatively Monte-Carlo techniques which approximate the EFE through a
random sampling of trajectories, which corresponds to classical
model-predictive control algorithms ( kappen2012optimal ) . An
additional consideration in the EFE is the need to specify a desired or
goal state for the action selection mechanism to achieve. This can be
considered to be a probabilistic description of rewards in reinforcement
learning and psychology, or of utility in economics. Mathematically,
this specification is achieved by defining a biased generative model
@xmath which contains a desired distribution @xmath which encodes the
rewards or utility as effectively priors in the inference procedure.

Since action selection is dependent entirely on the path-integral of the
EFE, the properties of the EFE functional essentially determines the
kind of behaviour that finding trajectories that minimize the EFE will
induce. Here, we showcase two decompositions of the EFE, and discuss its
intrinsic exploratory drive.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (2.43)
  -- -------- -------- -- --------

The first decomposition into risk and ambiguity obtains when the goal
distribution is specified in terms of the hidden states of the world
@xmath . In this case, EFE minimization can be thought of as directly
trying to match the expected states of the world to the desired states,
and thus achieving one’s goals, while simultaneously trying to minimize
the ambiguity of the observations one receives. The second decomposition
into extrinsic and intrinsic value (information gain) occurs when the
goal distribution is specified in terms of the observations @xmath . The
extrinsic value term can be thought of as the expected reward or
expected utility, since it is the average amount of reward expected
under the predicted observation distribution. Most interestingly in this
decomposition, however, is the information gain term which encourages
the agent to maximize the divergence between the variational prior and
the variational posterior. Effectively this term encourages the agent to
seek out information in the environment which will maximally update its
beliefs about the world. In effect, this term encourages a specific kind
of information-seeking exploration, where agents that minimize the EFE
are effectively driven to seek out and integrate resolvable uncertainty
about the world into their world model.

This intriguing property of active inference agents which minimize the
EFE has been extensively investigated in the literature – from simple
tasks such as the T-maze which require deciding whether to gain
information by seeking out a cue or not ( friston2015active ) , to
planning visual saccades in a way which maximizes the information about
the scene gained ( parr2017uncertainty ; heins2020deep ) and to using
directed exploration to develop highly sample efficient and powerful
general reinforcement learning algorithms for sparse-reward environments
( millidge2019deep ) . Moreover, the fact that the EFE naturally gives
rise to an information-seeking exploration term offers a promising and
fascinating avenue for resolving the exploration-exploitation tradeoff
and deriving optimal exploration strategies directly from variational
Bayesian inference algorithms.

### 2.11 Discussion

In this chapter, we have already covered a substantial amount of ground.
We have reviewed the core tenets of the free energy principle, provided
a mathematically detailed walk-through of the core results, and
discussed their philosophical implications and meaning. We have
additionally provided a short review of a core process theory – discrete
state space active inference – which we will fundamentally build upon in
various ways in the rest of this thesis. Chapter 3 will focus on
applying the free energy principle to perception – and will focus on the
implementation and extension of the process theory of predictive coding.
Chapter 4 will focus on merging active inference as presented here with
modern deep reinforcement learning methods to allow active inference
approaches, which are currently bottlenecked due to their discrete
explicit tabular representations and the exponential complexity of the
action selection algorithm, to be extended to challenging machine
learning problems. Chapter 5 will focus especially on the Expected Free
Energy term and will seek to unravel the origin of its information
seeking properties and in the process will reveal deep connections
between active inference and other variational Bayesian approaches to
action such as control as inference, as well as revealing a
substantially richer landscape of potential variational functionals for
control than has been previously realized.

## Chapter 3 Predictive Coding

### 3.1 Introduction

In this chapter, we consider the application of the free energy
principle to perception. Here, we focus entirely on visual perception
and the process theory of predictive coding ( friston2003learning ;
friston2005theory ; bastos2012canonical ; buckley2017free ;
spratling2017review ) . This chapter is organized into four relatively
independent sections which each present a separate piece of work, which
extends or contributes to the theory or practice of predictive coding.
The general theme of our work aims to scale up predictive coding to
reach the levels of performance achieved by machine learning, as well as
to understand the potential biological plausibility of the theory. Both
of these are important for understanding the potential predictive coding
has as a general theory of cortical function since, if it is actually
implemented in the brain, it must meet both bars of extremely high
scalability (since the brain effortlessly handles perception and
inference with extremely detailed and complex inputs, as well as
constructing extremely powerful and general representations), as well as
the biological plausibility necessary to allow the dynamics prescribed
by predictive coding to be implemented by neural circuitry.

We begin with a mathematical introduction to predictive coding and its
dynamics. This is followed with the presentation of our work where we
experiment with implementing larger scale predictive coding networks
than previously in the literature, and validate their performance and
capabilities on benchmark machine learning datasets – thus demonstrating
that predictive coding as a theory can be scaled up to the standards of
modern deep learning. We also experiment with dynamical predictive
coding networks using generalized coordinates (as introduced (
friston2008DEM ) ) as well as combining both hierarchical and dynamical
predictive coding networks, although these implementations are only
tested on relatively small toy tasks and scaling these up to larger and
much more challenging dynamical tasks, such as video prediction, remains
an important avenue for future work.

In the second section, we focus on understanding how predictive coding
can be written as a filtering algorithm – and thus can be applied
productively to fundamentally dynamical instead of static stimuli. We
demonstrate precisely how predictive coding is related to Kalman
filtering – a ubiquitous and extremely successful Bayesian filtering
algorithm ( kalman1960contributions ; kalman1961new ) – and also show
how predictive coding can extend this algorithm to allow for the online
learning of the parameters of the generative model (as well as inference
of the states) – a capability which is not usually achieved with Kalman
filtering alone. We validate the performance of this algorithm on simple
filtering tasks.

Fourthly, we investigate the biological plausibility of predictive
coding, show how the standard model possesses three key implausibilities
– weight transport, nonlinear derivatives, and one-to-one error unit
connectivity, and show how each can be overcome with biologically
plausible additions to the algorithm without causing much of a
degradation in the classification performance of the algorithm.

### 3.2 Predictive Coding

Predictive coding is an influential theory in computational and
cognitive neuroscience, which proposes a potential unifying theory of
cortical function ( friston2003learning ; friston2005theory ;
rao1999predictive ; friston2010free ; clark2013whatever ;
seth2014cybernetic ) – namely that the core function of the brain is
simply to minimize prediction error, where the prediction errors denote
mismatches between predicted input and the input actually received. This
minimization can be achieved in multiple ways: through immediate
inference about the hidden states of the world, which can explain
perception ( beal2003variational ) , through updating a global
world-model to make better predictions, which could explain learning (
friston2003learning ; neal1998view ) , and finally through action to
sample sensory data from the world that conforms to the predictions (
friston2009reinforcement ) , which potentially provides an account of
adaptive behaviour and control. Prediction error minimization can also
be influenced by modulating the precision (or inverse variance) of
sensory signals, which may shed light on the neural implementation of
attention mechanisms ( feldman2010attention ; kanai2015cerebral ) .
Predictive coding boasts an extremely developed and principled
mathematical framework, which formulates it as a variational inference
algorithm ( blei2017variational ; ghahramani2000graphical ;
jordan1998introduction ) , alongside many empirically tested
computational models with close links to machine learning (
beal2003variational ; dayan1995helmholtz ; hinton1994autoencoders ) ,
which address how predictive coding can be used to solve challenging
perceptual inference and learning tasks similar to those faced by the
brain. Moreover, predictive coding also has been translated into
neurobiologically plausible microcircuit process theories (
bastos2012canonical ; shipp2016neural ; shipp2013reflections ) which are
increasingly supported by neurobiological evidence ( walsh2020evaluating
) . Predictive coding as a theory is also supported by a large amount of
empirical evidence and offers a single mechanism that accounts for
diverse perceptual and neurobiological phenomena such as end-stopping (
rao1999predictive ) , bistable perception ( hohwy2008predictive ;
weilnhammer2017predictive ) , repetition suppression (
auksztulewicz2016repetition ) , illusory motions ( lotter2016deep ;
watanabe2018illusory ) , and attentional modulation of neural activity (
feldman2010attention ; kanai2015cerebral ) . As such, and perhaps
uniquely among neuroscientific theories, predictive coding encompasses
all three layers of Marr’s hierarchy by providing a well-characterised
and empirically supported view of ‘what the brain is doing’ at the
computational, algorithmic, and implementational level ( marr1982vision
) .

The core intuition behind predictive coding is that the brain is
composed of a hierarchy of layers, which each make predictions about the
activity of the layers below ( clark2015surfing ;
friston2008hierarchical ) . These descending downward predictions at
each level are compared with the activity and inputs of each layer to
form prediction errors -- which is the information in each layer which
could not be successfully predicted. These prediction errors are then
fed upwards to serve as inputs to higher levels, which can can then be
utilized to reduce their own prediction error. The idea is that, over
time, the hierarchy of layers instantiates a range of predictions at
multiple scales, from the fine details in local variations of sensory
data at low levels, to global invariant properties of the causes of
sensory data (e.g., objects, scenes) at higher or deeper levels. ¹ ¹ 1
This pattern is widely seen in the brain ( hubel1962receptive ;
grill2004human ) and also in deep (convolutional) neural networks (
olah2017feature ) , but it is unclear whether this pattern also holds
for deep predictive coding networks, primarily due to the relatively few
instances of deep convolutional predictive coding networks in the
literature so far. . Predictive coding theory claims that the goal of
the brain as a whole, in some sense, is to minimize these prediction
errors, and in the process of doing so performs both perceptual
inference and learning. Both of these processes can be operationalized
via the minimization of prediction error, first through the optimization
of neuronal firing rates on a fast timescale, and then the optimization
of synaptic weights on a slow timescale ( friston2008hierarchical ) .
Predictive coding proposes that using a simple unsupervised loss
function, such as simply attempting to predict incoming sensory data, is
sufficient to develop complex, general, and hierarchically rich
representations of the world in the brain, an argument which has found
recent support in the impressive successes of modern machine learning
models trained on unsupervised predictive or autoregressive objectives (
radford2019language ; kaplan2020scaling ; brown2020language ) .
Moreover, the fact that, in these machine learning models, errors are
computed at every layer means that each layer only has to focus on
minimizing local errors rather than a global loss. This property
potentially enables predictive coding to learn in a biologically
plausible way using only local and Hebbian learning rules (
whittington2017approximation ; millidge2020predictive ;
friston2003learning ) .

While originating from many varied intellectual currents, including the
speculations of Helmholtz ( helmholtz1866concerning ) , ideas in
information theory ( shannon1948mathematical ) and Barlow’s minimum
redundancy principle ( barlow1961possible ) , as well as ideas from
cybernetics ( wiener2019cybernetics ; seth2014cybernetic ) and early
work on machine learning ( jordan1998introduction ;
hinton1994autoencoders ) , modern predictive coding can be best
described as a variational inference algorithm ( beal2003variational )
on the hidden causes of sensory sensations, under Gaussian and Laplace
assumptions. Variational inference is a method of approximate Bayesian
inference, arising in statistical physics ( feynman1998statistical ) ,
which turns an intractable inference problem into a potentially
tractable optimization problem. In brief, we postulate a variational
density @xmath , under the control of the modeller, and try to minimize
the divergence between this variational density and the true posterior.
Since this divergence is not tractable either (since it contains the
true posterior), instead we optimize a tractable bound on this
divergence known as the variational free energy @xmath which measures
the expected difference between the logs of the variational posterior
and the generative model. To make this concrete, suppose we have
observations (or data) @xmath ; we wish to infer hidden, or latent,
states of the world @xmath , with a generative model @xmath and a
variational density @xmath with parameters @xmath . Then, we can write
the variational free energy @xmath as,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.1)
  -- -------- -------- -- -------

To derive a specific variational inference algorithm – such as
predictive coding – we must explicitly specify the forms of the
variational posterior and the generative model. In the case of
predictive coding, we assume a Gaussian form for the generative model
@xmath where we first partition the generative model into likelihood
@xmath and prior @xmath terms. The mean of the likelihood Gaussian
distribution is assumed to be some function @xmath of the hidden states
@xmath , which can be parametrized with parameters @xmath , while the
mean of the prior Gaussian distribution is set to some arbitrary
function @xmath of the prior mean @xmath . We also assume that the
variational posterior is a dirac-delta (or point mass) distribution
@xmath with a center @xmath ² ² 2 In previous works, predictive coding
has typically been derived by assuming a Gaussian variational posterior
under the Laplace approximation. This approximation effectively allows
you to ignore the variance of the Gaussian and concentrate only on the
mean. This procedure is effectively identical to the dirac-delta
definition made here, and results in the same update scheme. However,
the derivation using the Laplace approximation is much more involved so,
for simplicity, here we use the Dirac delta definition. See Appendix C,
or buckley2017free for a detailed walkthrough of the Laplace derivation
.

Given these definitions of the variational posterior and the generative
model, we can write down the concrete form of the variational free
energy to be optimized. We first decompose the variational free energy
into an ‘ Energy ’ and an ‘ Entropy ’ term

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (3.2)
  -- -------- -------- -- -------

where, since the entropy of the dirac-delta distribution is 0 (it is a
point mass distribution), we can ignore the entropy term and focus
solely on writing out the energy.

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.3)
  -- -------- -------- -- -------

where we define the prediction errors @xmath and @xmath . We thus see
that the energy term, and thus the variational free energy, is simply
the sum of two squared prediction error terms, weighted by their inverse
variances, plus some additional log variance terms.

Finally, to derive the predictive coding update rules, we must make one
additional assumption – that the variational free energy is optimized
using the method of gradient descent such that,

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

Given this, we can derive dynamics for all variables of interest (
@xmath by taking derivatives of the variational free energy @xmath . The
update rules are as follows

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (3.5)
  -- -------- -------- -- -------

Furthermore while it is possible to run the dynamics for the @xmath and
the @xmath simultaneously, it is often better to treat predictive coding
as an EM algorithm ( dempster1977maximum ) and alternate the updates.
Empirically, it is typically best to run the optimization of the @xmath
s, with fixed @xmath until close to convergence, and then run the
dynamics on the @xmath with fixed @xmath for a short while (
friston2005theory ) . This implicitly enforces a separation of
timescales upon the model where the @xmath are seen as dynamical
variables which change quickly while the @xmath are slowly-changing
parameters. For instance, the @xmath s are typically interpreted as
rapidly changing neural firing rates, while the @xmath s are the slowly
changing synaptic weight values ( rao1999predictive ; friston2005theory
) .

Finally, we can see how this derivation of predictive coding maps onto
putative psychological processes of perception and learning. The updates
of the @xmath can be interpreted as a process of perception, since the
@xmath is meant to correspond to the true latent state of the
environment generating the @xmath observations. By contrast, the
dynamics of the @xmath can be thought of as corresponding to learning,
since these @xmath effectively define the mapping between the latent
state @xmath and the observations @xmath .

### 3.3 Hierarchical predictive coding

Thus far, we have only derived a predictive coding scheme with a single
level of latent variables @xmath . However, the expressivity of such a
scheme is limited. The success of deep neural networks in machine
learning have demonstrated that having hierarchical sets of latent
variables is key to allowing methods to learn abstractions and to handle
intrinsically hierarchical dependencies of the sort humans intuitively
perceive ( krizhevsky2012imagenet ; hinton2012neural ) . Predictive
coding can be straightforwardly extended to handle hierarchical dynamics
of arbitrary depth. This is done through postulating multiple layers of
latent variables @xmath and then defining the generative model as
follows,

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where @xmath and the final layer @xmath has an arbitrary prior @xmath
and the latent variable at the bottom of the hierarchy is set to the
observation actually received @xmath . Similarly, we define a separate
variational posterior for each layer @xmath , then the variational free
energy can be written as a sum of the prediction errors at each layer,

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where @xmath = @xmath . Given that the free energy divides nicely into
the sum of layer-wise prediction errors, it comes as no surprise that
the dynamics of the @xmath and the @xmath are similarly separable across
layers.

  -- -------- -------- -- -------
     @xmath   @xmath      (3.8)
  -- -------- -------- -- -------

  -- -------- -------- -- -------
     @xmath   @xmath      (3.9)
  -- -------- -------- -- -------

We see that the dynamics for the variational means @xmath depend only on
the prediction errors at their layer and the prediction errors on the
level below. Intuitively, we can think of the @xmath s as trying to find
a compromise between causing error by deviating from the prediction from
the layer above, and adjusting their own prediction to resolve error at
the layer below. In a neurally-implemented hierarchical predictive
coding network, prediction errors would be the only information
transmitted ‘upwards’ from sensory data towards latent representations,
while predictions would be transmitted ‘downwards’. Crucially for
conceptual readings of predictive coding, this means that sensory data
is not transmitted directly up through the hierarchy, as is assumed in
much of perceptual neuroscience. The dynamics for the @xmath s are also
fairly biologically plausible as they are effectively just the sum of
the precision-weighted prediction errors from the @xmath s own layer and
the layer below, the prediction errors from below being transmitted back
upwards through the synaptic weights @xmath and weighted with the
gradient of the activation function @xmath .

Importantly, the dynamics for the synaptic weights is entirely local,
needing only the prediction error from the layer below and the current
@xmath at the given layer. The dynamics thus becomes a Hebbian rule
between the presynaptic @xmath and postsynaptic @xmath , weighted by the
gradient of the activation function.

Based upon our previous work ( millidge2019implementing ) , we present
empirical evaluations and demonstrations of the expressive power of
hierarchical predictive coding networks on standard machine learning
benchmarks with learnable generative models. While some previous work
has implemented hierarchical predictive coding models and tested them on
‘blind deconvolution’ of simulated ERP data ( friston2008hierarchical ;
friston2005theory ) , we present a key demonstration of predictive
coding networks within a machine learning paradigm, and with a
completely learnt generative model.

First, we tested the potential of predictive coding networks as
autoencoders ( hinton1994autoencoders ) from machine learning. Here, the
goal of the network is simply to reconstruct its input data. In theory,
this can be done trivially be learning the identity mapping, so to make
it difficult we create an information bottleneck ( tishby2000information
) in the hidden layers, such that the input is compressed to a much
smaller latent code, which must then be decompressed to successfully
reconstruct the image. Autoencoders of this type are widely used in
machine learning and a probabilistic variant – variational autoencoders
( kingma_auto-encoding_2013 ) are still state of the art at many image
generation tasks ( child2020very ) . Here we demonstrate that predictive
coding networks can also function as powerful autoencoders. We first
test the potential of predictive coding on the MNIST dataset – a
standard machine learning benchmark dataset of 60,000 28x28 grayscale
handwritten digits. We utilized a three layer predictive coding network,
with an input and output dimensionality of 784 (the size of a flattened
vector of the MNIST digit), and a latent dimensionality of 20, meaning
that the network had to learn to compress a 784 dimensional manifold
into a 20 dimensional latent space. We trained the predictive coding
network according to Equations 3.2 with a batch size of 64 and a
learning rate of 0.01. A sigmoid nonlinearity was used on the input and
latent layers. We trained the networks for 100 epochs. Each epoch
consisted of updating the @xmath s using Equation 3.8 for 100 steps, and
then updating the weights @xmath using Equation 3.9 once. The model was
able to recreate MNIST digits successfully, as shown in the example
reconstructions below:

Additionally, the model was also able to recognize and reconstruct
previously unseen MNIST digits, albeit with slightly lower fidelity.
Nevertheless it is impressive how rapidly and well the network is able
to generalize to completely unseen digits.

Since the model is a generative model, it is also able to generalize
outside the training set to generate, or ‘dream’, completely unseen
digits by sampling from the latent space. Examples are shown below:

Finally, to visualize the latent space, PCA (principal components
analysis) was applied to the learned representations in the 20
dimensional latent space to shrink it down to a two dimensional space.
It is apparent upon inspection of Figure 3.4 that the latent space, even
when shrunk down to two dimensions, does a good job of clustering the
MNIST, putting all the 1s in the top left corner, or all the zeros in
the middle right. This strongly indicates that the predictive coding
model is able to learn the categories of digits despite being trained in
an entirely unsupervised way without any knowledge of the true
identities of the digits.

We additionally tested the network’s capability to reconstruct CIFAR
images, which are 32x32 colour images of natural scnes. Our network was
the same in the MNIST case except that we used a latent dimension of 50.

The hierarchical predictive coding CIFAR models are able to learn to
reconstruct CIFAR images with impressive fidelity given that they were
compressed from a 1024 dimensional image into a 50 dimensional latent
state.

Importantly, due to having learnt a latent space, we are able to
interpolate between images, and thus investigate the properties of the
learnt latent space. To interpolate, we begin with two images @xmath and
@xmath and take the difference vector in the latent space @xmath where
@xmath is the encoder function. Then, we step in the latent space by a
constant @xmath and decode such that @xmath where @xmath is the decoding
function. We stepped in increments of @xmath . Below we can see the
network smoothly interpolating between an image of a horse and a cat. We
see that the predictive coding network appears to naturally learn a
smooth latent space, allowing for generalization to unseen images and
smooth interpolation between classes in the latent space.

#### 3.3.1 Dynamical Predictive coding

While so far, we have only considered modelling just a single static
stimulus @xmath . However, the data the brain receives comes in temporal
sequences @xmath . To model such temporal sequences, it is often useful
to split the latent variables into states , which can vary with time,
and parameters which cannot. In the case of sequences, instead of
minimizing the variational free energy, we must instead minimize the
free action @xmath , which is simply the path integral of the
variational free energy through time:

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (3.10)
  -- -------- -------- -- --------

While there are numerous methods to handle sequence data, one
influential and elegant approach ( friston2008DEM ;
friston2008hierarchical ; friston2010generalised ) is to represent
temporal data in terms of generalized coordinates of motion . These
coordinates represent not just the immediate observation state, but all
the temporal derivatives of the observation. For instance, suppose that
the brain represents beliefs about the position of an object. Under a
generalized coordinate model, it would also represent beliefs about the
velocity (first time derivative), acceleration (second time derivative),
jerk (third time derivative) and so on. All these time derivative
beliefs are concatenated to form a generalized state. The key insight
into this dynamical formulation is, that when written in such a way,
many of the mathematical difficulties in handling sequences disappear,
leaving relatively straightforward and simple variational filtering
algorithms which natively handle smoothly changing sequences. For
instance, we maintain a coherent concept of a stationary state, since we
can define it as one in which none of the time derivatives are changing.
This allows the variational inference procedure to track a moving target
by representing it as a steady state in a moving frame of reference.

Because the generalised coordinates are notationally awkward, we will be
very explicit in the following. We denote the time derivatives of the
generalized coordinate using a @xmath , so @xmath is the belief about
the velocity of the @xmath , just as @xmath is the belief about the
‘position’ about the @xmath . A key point of confusion is that there is
also a ‘real’ velocity of @xmath , which we denote @xmath , which
represents how the belief in @xmath actually changes over time.
Importantly, this is not necessarily the same as the belief in the
velocity: @xmath , except at the equilibrium state, which can be
understood as the path of least action. Intuitively, this makes sense as
at equilibrium (mimimum of the free action, and thus perfect inference),
our belief about the velocity of mu @xmath and the ‘real’ velocity
perfectly match. Away from equilibrium, our inference is not perfect so
they do not necessarily match. We denote the generalized coordinate
representation of a state @xmath as simply a vector of each of the
beliefs about the time derivatives @xmath . We also define the operator
@xmath which maps each element of the generalised coordinate to its time
derivative i.e. @xmath . With this notation, we can define a dynamical
generative model using generalized coordinates. Crucially, we assume
that the noise @xmath in the generative model is not white noise, but is
coloured, so it has non-zero autocorrelation and can be differentiated.
Effectively, coloured noise allows one to model relatively slowly (not
infinitely fast) exogenous forces on the system. For more information on
coloured noise vs white noise see ( friston2008DEM ; yuan2012beyond ) .
With this assumption we can obtain a generative model in generalized
coordinates of motion by simply differentiating the original model.

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
              @xmath   @xmath      (3.11)
  -- -------- -------- -------- -- --------

Where we have applied a local linearisation assumption ( friston2008DEM
) which drops the cross terms in the derivatives. We can write these
generative models more compactly in generalized coordinates.

  -- -------- -------- -- --------
     @xmath   @xmath      (3.12)
  -- -------- -------- -- --------

which, written probabilistically is @xmath . It has been shown (
friston2008DEM ) that the optimal (equilibrium) solution to this free
action is the following stochastic differential equation,

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

Where @xmath is the generalized noise at all orders of motion.
Intuitively, this is because when @xmath then @xmath , or that the
‘real’ change in the variable is precisely equal to the expected change.
This equilibrium is a dynamical equilibrium which moves over time, but
precisely in line with the beliefs @xmath . This allows the system to
track a dynamically moving optimal solution precisely, and the
generalized coordinates let us capture this motion while retaining the
static analytical approach of an equilibrium solution, which would
otherwise necessarily preclude motion. There are multiple options to
turn this result into a variational inference algorithm. Note, the above
equation makes no assumptions about the form of variational density or
the generative model, and thus allows multimodal or nonparametric
distributions to be represented. For instance, the above equation
(Equation 3.13 ) could be integrated numerically by a number of
particles in parallel, thus leading to a generalization of particle
filtering ( friston2008variational ) . Alternatively, a fixed Gaussian
form for the variational density can be assumed, using the Laplace
approximation. In this case, we obtain a very similar algorithm to
predictive coding as before, but using generalized coordinates of
motion. In the latter case, we can write out the free energy as,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.14)
  -- -------- -------- -- --------

Where @xmath and @xmath . Moreover, the generalized precisions @xmath
not only encode the covariance between individual elements of the data
or latent space at each order, but also the correlations between
generalized orders themselves. Since we are using a unimodal (Gaussian)
approximation, instead of integrating the stochastic differential
equations of multiple particles, we instead only need to integrate the
deterministic differential equation of the mode of the free energy,

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

which cashes out in a scheme very similar to standard predictive coding
(compare to Equation 3.8 ), but in generalized coordinates of motion.
The only difference is the @xmath term which links the orders of motion
together. This term can be intuitively understood as providing the
‘prior motion’ while the prediction errors provide ‘the force’ terms. To
make this clearer, let’s take a concrete physical analogy where @xmath
is the position of some object and @xmath is the expected velocity.
Moreover, the object is subject to forces @xmath which instantaneously
affect its position. Now, the total change in position @xmath can be
thought of as first taking the change in position due to the intrinsic
velocity of the object @xmath and adding that on to the extrinsic
changes due to the various exogenous forces.

Things get more complex when we consider a model which has both
dynamical and hierarchical components where there are interactions
between them. This we call a full-construct model following the lead of
this tutorial ( buckley2017free ) . In a full construct model there is a
dynamical hierarchy of levels where it is assumed that each dynamical
order is only able to affect the level below:

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath               (3.16)
  -- -------- -------- -- --------

Similarly, there is simultaneously a hierarchy of levels, where each
level is assumed to be predicted by the level above it:

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath               (3.17)
  -- -------- -------- -- --------

Therefore, each node in the lattice of hierarchical and dynamic
hierarchies is influenced by two separate predictions - the dynamical
prediction going from higher dynamical orders to lower, and the
hierarchical prediction propagating from higher levels of the hierarchy
to lower ones. Thus, a single state of a cause-unit @xmath , where
@xmath is the level of the hierarchy, and @xmath is the dynamical order,
is defined to be:

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

This means that the variational free energy must sum over both dynamical
and hierarchical prediction errors, such that:

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

And that additionally the updates for the representation-units and the
weights must take this into account. The revised update rules are
presented below:

  -- -------- -------- -- --------
     @xmath   @xmath      (3.20)
  -- -------- -------- -- --------

And for the weights the update rule thus becomes:

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

These rules appear somewhat more complicated than the corresponding
rules in the static case. Nevertheless they only incur a linear (in the
order of generalized coordinates considered) additional computational
cost.

Preliminary dynamical and full construct models were implemented and
tested on simple stimuli. The first task the dynamical models were
tested on was predicting a sine wave. This is the perfect toy-task since
sine waves have analytic derivatives to any infinite degree. We used a
dynamical model which represented three orders of generalized motion.
The model was trained to predict a sine wave autoregressively and its’
first two temporal derivatives. The model rapidly learned to predict the
sine wave, as can be seen from the training graphs below. However, there
was a consistent phase-error in the predictions it made, which could
have been caused by the rapid rate of change of the sine-wave
observations.

The dynamical model does not only work with sine waves. The model was
also tested on more jerky waveforms sawtooth waves. In this case a two
layer linear dynamical model was used which learned to predict the
sawtooth wave and its first temporal derivative. The model predicts the
wave very successfully, including the temporal derivative, although
there it is a little less successful. Once again there is a persistent
patterned prediction error, likely caused by the lag time between the
models predictions and the the observations it receives.

We can also train ‘full-construct’ models on dynamical stimuli. Here we
used a model with two hierarchical layers and three dynamical layers and
was trained autoregressively to predict a sine-wave. Training was
‘online’ with a learning rate of @xmath . The generative model
parameters @xmath were updated initialized randomly and updated each
epoch. For every ‘tick’ of the sine wave, the variational parameters
@xmath were updated for 100 steps. Training graphs are shown below:

The top two rows of graphs show the incoming sense data and the first
two temporal derivatives of the sense-data. The next two rows show the
prediction errors over time for various levels of the hierarchy. The
full construct model appears a bit less stable and successful than the
simple dynamical model, likely because it is much more complex and has
many more moving parts. Nevertheless it manages to learn the sine wave
shapes relatively faithfully and does also manage to rapidly reduce the
prediction error over time. Moreover these sorts of tasks do not really
play well to the strength of the full-construct model since the input
data (the sine wave) contains no suitable hierarchical structure for the
higher levels to model. It seems likely that as these models are scaled
up to more challenging tasks, the greater expressivity and power of the
full-construct models will become more apparent. So far, we have only
experimented with ‘full-construct’ models on simple toy tasks such as
sine waves. An interesting avenue for future would work be experimenting
as to whether full construct models could be scaled up to handle
challenging machine learning tasks dealing with sequential data such as
video prediction. While predictive coding networks have been proposed
for this task ( lotter2016deep ) , none to our knowledge have explicitly
utilized generalized coordinates in any capacity. There is, however, a
literature using the predictive coding schemes with generalized
coordinates in nonlinear (chaotic) systems with separation of temporal
scales. This enables the recognition and prediction of things like
birdsong and speech – and indeed their learning of particular songs.
Crucially, these applications rest upon generalised coordinates of
motion, usually up to 4th order motion ( friston2009predictive ;
friston2015duet ; isomura2019Bayesian )

### 3.4 Predictive Coding and Kalman Filtering

A key intuition behind the utility of predictive coding is that it
naturally handles filtering tasks. Filtering tasks require constant
updates of a moving state estimate given sequences of new data.
Effectively, filtering is the task of learning and inferring movements
in the hidden state of the world from changing input sequences – as
opposed to the usual machine learning task of inferring hidden states
(such as labels) from single static inputs. Importantly the core task
faced by much of the brain is fundamentally one of filtering, since the
inputs the brain receives are actually temporally extended sequences,
rather than static flashes. For instance, in vision the task of the
brain is not to categorize static images but rather to infer the state
of, and ultimately interact with, a smoothly changing external world
situated in continuous time. Moreover, it is known that the brain takes
substantial advantage of the additional information given by integrating
sequences over time such as optical flow ( gibson2002theory ) and active
motion to explore different angles on a given scene ( henderson2017gaze
) .

If, as we generally assume throughout the thesis, that the brain is
fundamentally a (Bayesian) inference machine, then the core task of the
brain must be Bayesian Filtering instead of static Bayesian inference (
sarkka2013Bayesian ) . Bayesian filtering is mathematically somewhat
more involved, due to the need to perform inference over sequences
instead of single data-points, but there are a wide variety of
algorithms in the literature which perform Bayesian filtering, often
highly effectively ( kutschireiter2018nonlinear ;
kutschireiter2020hitchhiker ) . Mathematically, we can formalize the
filtering problem as follows ( jaswinskistochastic ; stengel1994optimal
) . We have an estimated state @xmath , and some model of how the world
evolves (the dynamics model): @xmath . We also receive observations
@xmath , and you have some model of how the observations depend on the
estimated state (the observation model): @xmath . The task, then, is to
compute @xmath . In the general nonlinear case, this calculation is
analytically intractable and extremely expensive to compute exactly.
Some form of approximate solution is required. Two forms of
approximation are generally used. The first is to approximate the model
- such as by assuming linearity of the dynamics and observation models.

The second method is to approximate the posterior – usually with a set
of samples (or particles). This approach is taken by the class of
particle filtering algorithms which track the changing posterior by
propagating the particles through the dynamics and then resampling based
upon updated measurement information ( arulampalam2002tutorial ;
gordon1993novel ) . This approach can handle general nonlinear filtering
cases, but suffers strongly from the curse of dimensionality. If the
state-space is high-dimensional the number of particles required for a
good approximation grows rapidly ( doucet2000sequential ) .Moreover,
there has been some fascinating work on implementing particle filtering
methods in neural circuitry ( kutschireiter2015neural ) , as well as
speculation about whether perhaps the brain may utilize particle or
sampling methods for inference instead of variational ones (
sanborn2016Bayesian ) .

Nevertheless, here we focus primarily on approximate variational
approaches to inference. Specifically, we first demonstrate that
predictive coding is naturally a filtering algorithm – perhaps more
naturally than one applied to static datasets. The only change to the
algorithm is simply what is predicted. If predictive coding is set up so
as to predict the next input ( mumford1992computational ;
clark_whatever_2013 ) , which is highly plausible in the brain, then it
can perform variational Bayesian filtering. In this section, we explore
this predictive coding filtering algorithm and show, crucially, that in
the linear case it becomes a variant of Kalman filtering – a fundamental
and ubiquitous algorithm in classical control ( kalman1960contributions
; kalman1960new ) . Moreover, in the nonlinear case, predictive coding
becomes a variant of extended Kalman filtering ( ollivier2019extended )
. The Kalman Filter solves the general filtering problem by making two
simplifying assumptions. The first is that both the dynamics model and
the observation model are linear. The second assumption is that noise
entering the system is white and Gaussian. This makes both the prior and
likelihoods Gaussian. Since the Gaussian distribution is a conjugate
prior to itself, this induces a Gaussian posterior, which can then serve
as the prior in the next timestep. Since both prior and posterior are
Gaussian, filtering can continue recursively for any number of
time-steps without the posterior growing in complexity and becoming
intractable. The Kalman filter is the Bayes-optimal solution provided
that the assumptions of linear models and white Gaussian noise are met (
kalman1960new ) . The Kalman Filter, due to its simplicity and utility
is widely used in engineering, time-series analysis, aeronautics, and
economics ( grewal2010applications ; leondes1970theory ;
schneider1988analytical ; harvey1990forecasting ) .

Since predictive coding possesses several neurophysiologically realistic
process theories ( bastos2012canonical ) , this correspondence provides
an avenue for a biologically plausible implementation of Kalman
filtering in the brain. There is substantial evidence that the brain is
capable of Bayes-optimal integration of noisy measurements, and is
apparently in possession of robust forward models both in perception (
zago2008internal ; simoncelli2009optimal ) and motor control (
munuera2009optimal ; gold2003influence ; todorov2004optimality ) .
de2013Kalman have even shown that a Kalman filter successfully fits
psychomotor data on visually guided saccades and smooth pursuit
movement, although they remain agnostic on how it may be implemented in
the brain. We demonstrate, however, a clear mathematical link of the
relationship between Kalman filtering and predictive coding, allowing us
first to use results from Kalman filtering to understand the performance
of predictive coding algorithms, and second enabling us to utilize the
process theories of predictive coding to understand how the brain may
perform crucial filtering tasks.

First, we reveal the precise relationship between Kalman filtering and
predictive coding – namely that both optimize the same Bayesian
objective, which is convex in the linear case. However, the Kalman
filter solves the optimization problem analytically, thus giving rise to
its algebraic complexities and especially the highly neurobiologically
implausible Kalman gain matrix. Predictive coding, on the other hand,
solves the objective through a process of gradient descent on the
sufficient statistics of the variational distribution, thereby obtaining
biologically plausible Hebbian update rules. Additionally, the fully
Bayesian perspective granted by predictive coding also allows us to
perform learning of the generative model – i.e. learning the
coefficients of the dynamics and likelihood matrices – which allows us
to handle cases where the model of the world is unknown, in contrast to
traditional Kalman filtering which assumes accurate (and linear)
dynamics and observation models of the world. While the close
relationship between Kalman filtering and (linear) predictive coding has
been hinted at before ( friston2005theory ; friston2008hierarchical ) ,
there it is claimed that predictive coding is ‘equivalent’ to Kalman
filtering – which is not the case except insofar as the two algorithms
optimize the same objective. baltieri2020Kalman provide side by side
comparisons of the update rules for predictive coding and Kalman
filtering, but do not go beyond this superficial analysis to uncover the
precise relationship between them.

Secondly, we directly compare the performance of the Kalman filter and
our predictive coding algorithm on a simplified location tracking task –
which the Kalman filter excels at. We show that despite the predictive
coding algorithm performing a gradient descent instead of an analytical
solution, it performs comparably with the Kalman filter and, due to the
convexity of the underlying optimization problem, requires very few
iterations to converge. This rapid convergence is important, since the
brain is heavily time-constrained in its inferences – choices often must
be made fast. Secondly, we demonstrate that the learning rules for the
likelihood and dynamics matrices allow us to perform online tracking
even when the model is completely unknown. We only show that this is the
case for the dynamics, however, as learning does not perform well with
an unknown observation model. We hypothesize that this is due to the
ill-posedness of the resulting optimization problem.

#### 3.4.1 The Kalman Filter

The Kalman Filter is defined upon the following linear state-space ³ ³ 3
For simplicity, the model is presented in discrete time. The continuous
time analogue of the Kalman filter is the Kalman-Bucy filter (
kalman1961new ) . Generalization of this scheme to continuous time is an
avenue for future work.

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (3.22)
  -- -------- -------- -- --------

Where @xmath represents the hidden or internal state at time t. @xmath
is the control - or known inputs to the system - at time t. Matrices
@xmath , @xmath , and @xmath parametrize the linear dynamics or
observation models, and @xmath and @xmath are both zero-mean white noise
Gaussian processes with covariance @xmath and @xmath , respectively.
Since the posterior @xmath is Gaussian, it can be represented by its two
sufficient statistics – the mean @xmath and covariance matrix @xmath .

Kalman filtering proceeds by first ‘projecting’ forward the current
estimates according to the dynamics model. Then these estimates are
‘corrected’ by new sensory data. The Kalman filtering equations are as
follows:
Projection

  -- -------- -- --------
     @xmath      
     @xmath      (3.23)
  -- -------- -- --------

Correction

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      (3.24)
  -- -------- -- --------

Where @xmath and @xmath are the mean and variance of the estimate of the
state @xmath at time t, and @xmath is the Kalman gain matrix. Although
these update rules provide an analytically exact solution to the
filtering problem, the complicated linear algebra expressions,
especially that for the Kalman gain matrix @xmath , make it hard to see
how such equations could be implemented directly in the brain.

Importantly, these Kalman filtering equations can be derived directly
from Bayes’ rule. The mean of the posterior distribution is also the MAP
(maximum-a-posteriori) point, since a Gaussian distribution is unimodal.
Thus, to estimate the new mean, we simply have to estimate,

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (3.25)
  -- -------- -- --------

In the second line, the algebraic form of the Gaussian density is
substituted and we have switched the maximization variable to @xmath due
to the fact that the maximum of a Gaussian is also its mean. We also
minimize the log probability instead of maximizing the probability,
which gets rid of the exponential and the normalizing constant (which
can be computed analytically since the posterior is Gaussian) ⁴ ⁴ 4 The
log transformation is valid under maximization/minimization since the
log function is monotonic. . From this objective, one can simply solve
analytically for the optimal @xmath and @xmath . For a full derivation
see Appendix A.

#### 3.4.2 Predictive Coding as Kalman Filtering

Here we demonstrate the relationship between predictive coding and
Kalman filtering. First, we need to explicitly write out and adapt the
mathematical apparatus of predictive coding to filtering problems. To do
so, we need to perform variational inference over full trajectories
@xmath of observations and hidden states. If we then assume trajectories
are Markov, and are thus licensed to apply a Markov factorization of the
generative model @xmath ⁵ ⁵ 5 Where, to make this expression not a
function of @xmath , we implicitly average over our estimate of @xmath
from the previous timestep: @xmath and a mean-field temporal
factorization of the variational density, so that it is independent
across timesteps @xmath , then the variational free energy of the
trajectory factorizes into independently optimizable free-energies of a
particular timestep,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (3.26)
  -- -------- -------- -- --------

This temporal factorization of the free energy means that the
minimization at each timestep is independent of the others, and so we
only need consider a single minimization of a single timestep to
understand the solution, since all time-steps will be identical in terms
of the solution method. Applying the linear Gaussian assumptions of the
Kalman filter, we can specify our generative model in terms of Gaussian
distributions,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.27)
  -- -------- -------- -- --------

Since we know the posterior is Gaussian, it makes sense to also use a
Gaussian distribution for the variational approximate distribution.
Importantly, for predictive coding we make an additional assumption –
the Laplace Approximation – which characterises the variance of this
Gaussian as an analytic function of the mean, thus defining,

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

where @xmath are the parameters of the variational distribution – in
this case a mean and variance since we have assumed a Gaussian
variational distribution. With the variational distribution and
generative model precisely specified, it is now possible to explicitly
evaluate the variational free energy for a specific time-step,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.29)
  -- -------- -------- -- --------

Where the second term is the entropy of the variational distribution.
Since we are only interested in minimizing with respect to the mean
@xmath and the expression for the entropy of a Gaussian does not depend
on the mean, we can ignore this entropy term in subsequent steps. The
key quantity is the ‘energy’ term @xmath . Since the Laplace
approximation ensures that most of the probability distribution is near
the mode @xmath of the variational distribution, we can well approximate
the expectation using a Taylor expansion to second order around the
mode,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.30)
  -- -------- -------- -- --------

Since the first term vanishes as @xmath and we can neglect the second
term since it only depends on @xmath and not @xmath , then the only term
that matters for the minimization is the first term @xmath . This means
that we can write the overall optimization problem solved by predictive
coding as,

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

which is the same as the MAP optimization problem presented in Equation
3.4.1 . This means that ultimately the variational inference problem
solved by predictive coding and the MAP estimation problem solved by the
Kalman filter are the same although the interpretation of @xmath differs
slightly – from being a parameter of a Gaussian variational distribution
versus simply a variable in the generative model – the actual update
rules involving @xmath are the same in both cases. Now we know that
(linear) predictive coding and Kalman filtering share the same
objective, we can precisely state their differences. While Kalman
filtering analytically solves this objective directly, in predictive
coding, we instead set the dynamics of the parameters to be a gradient
descent on the variational free energy, which reduces to the MAP
objective solved by the Kalman Filter.

For instance, we can derive the dynamics with respect to the variational
parameters @xmath which, in neural process theories, are typically
operationalized as the ‘activation’ units as,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.32)
  -- -------- -------- -- --------

Where @xmath and @xmath .Thus, we can see that the gradient perfectly
recapitulates the standard predictive coding scheme with precision
weighted prediction errors. Similarly, by taking gradients with respect
to the @xmath , @xmath , and @xmath matrices of the generative model, we
obtain familiar looking update rules which consist of Hebbian update
rules between the prediction errors and the presynaptic activations

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.33)
  -- -------- -------- -- --------

And similarly for the @xmath matrix.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.34)
  -- -------- -------- -- --------

And the @xmath observation matrix.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.35)
  -- -------- -------- -- --------

#### 3.4.3 Results

We now compare the analytical Kalman filter with our predictive coding
algorithm on a simple filtering application – that of tracking the
motion of an accelerating body given only noise sensor measurements. The
body is accelerated with an initial high acceleration that rapidly
decays according to an exponential schedule. The filtering algorithm
must infer the position, velocity, and true acceleration of the body
from only a kinematic dynamics model and noisy sensor measurements. The
body is additionally perturbed by white Gaussian noise in all of the
position, velocity and displacement. The control schedule and the true
position, velocity and displacement of the body are shown in Figure 3.10
below.

The analytical Kalman filter was set up as follows. It was provided with
the true kinematic dynamics matrix ( @xmath ) and the true control
matrix ( @xmath ),

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (3.36)
  -- -------- -------- -- --------

The observation matrix C matrix was initialized randomly with
coefficients drawn from a normal distribution with 0 mean and a variance
of 1. This effectively random mapping of sensory states meant that the
filter could not simply obtain the correct estimate directly but had to
disentangle the measurements first. The Q and R matrices of the
analytical Kalman filter were set to constant diagonal matrices, where
the constant was the standard variance of the noise added to the system.

The performance of the analytical Kalman filter which computed updates
using equations 1-3 is compared with that of our neural Kalman filter
using gradient descent dynamics. ⁶ ⁶ 6 The code used for these
simulations is freely available and online at @xmath In this comparison
the A, B, and C matrices are fixed to their correct values and only the
estimated mean is inferred according to Equation 3.4.2 . Comparisons are
provided for a number of different gradient steps. As can be seen in
Figure 3.12 , only a small number (5) of gradient descent steps are
required to obtain performance very closely matching the analytical
result. This is likely due to the convexity of the underlying
optimization problem, and means that using gradient descent for "online"
inference is not prohibitively slow. The simulation also shows the
estimate for too few (2) gradient steps for which results are similar,
but the estimate may be slightly smoother.

Next, we demonstrate the adaptive capabilities of our algorithm. In
Figure 3.13 , we show the performance of our algorithm in predicting the
position, velocity, and acceleration of the body when provided with a
faulty A matrix. Using Equation 3.4.2 , our model learns the A matrix
online via gradient descent. To ensure numerical stability, a very small
learning rate of @xmath must be used. The entries of the A matrix given
to the algorithm were initialized as random Gaussian noise with a mean
of 0 and a standard deviation of 1. The performance of the algorithm
without learning the A matrix is also shown, and estimation performance
is completely degraded without the adaptive learning. The learning
process converges remarkably quickly. It is interesting, moreover, to
compare the matrix coefficients learned through the Hebbian plasticity
to the known true coefficients. Often they do not match the true values,
and yet the network is able to approximate Kalman filtering almost
exactly. Precisely how this works is an area for future exploration. If
a system similar to this is implemented in the brain, then this could
imply that the dynamics model inherent in the synaptic weight matrix
should not necessarily be interpretable.

We also show (second row of Figure 3.13 ) that, perhaps surprisingly,
both the A and B matrix can be learned simultaneously. In the
simulations presented below, either only the A matrix, or both the A and
the B matrix were initialized with random Gaussian coefficients, and the
network learned to obtain accurate estimates of the hidden state in
these cases. ⁷ ⁷ 7 The results of only learning the B matrix were
extremely similar for that of the A matrix. For conciseness, the results
were not included. Interested readers are encouraged to look at the
@xmath file in the online code where these experiments were run.

We also tried adaptively learning the @xmath matrix using Equation 3.4.2
, but all attempts to do so failed. Although the exact reason is
unclear, we hypothesise that an incorrect @xmath matrix corrupts the
observations which provides the only "source of truth" to the system. If
the dynamics are completely unknown but observations are known, then the
true state of the system must be at least approximately near that
implied by the observations, and the dynamics can be inferred from that.
On the other hand, if the dynamics are known, but the observation
mapping is unknown, then the actual state of the system could be on any
of a large number of possible dynamical trajectories, but the exact
specifics of which are underspecified. Thus the network learns a C
matrix which corresponds to some dynamical trajectory, which succeeds in
minimizing the loss function, but which is completely dissimilar to the
actual trajectory the system undergoes. This can be seen by plotting the
loss obtained according to Equation 3.4.1 in Figure 3.14 , which rapidly
decreases, although the estimate diverges from the true values.

#### 3.4.4 Discussion

Here we have elucidated the precise relationship between Kalman
filtering – an optimal linear Bayesian filtering algorithm – and
predictive coding – a neurophysiologically realistic theory of cortical
function. Specifically, that they both optimize the same objective
function – a Bayesian MAP filtering objective – while the Kalman
filtering solves the resulting optimization problem analytically,
predictive coding approaches derive their dynamics from a gradient
descent on the same objective. "This result provides a new perspective
on predictive coding; especially if we make the simplifying assumption
that the precisions are not optimised with respect to variational free
energy – or, in the examples above, we assume the conditional covariance
is zero. This reduces variational inference to a MAP optimisation
problem. This follows due to the Laplace approximation, which
effectively means the variational precision can be derived analytically
from the expectation (from the curvature of the log likelihood at the
expectation) – see ( friston2007free ) for details. Alternatively, we
can just ignore the conditional uncertainty as in the MAP optimisation
perspective.

Our work also demonstrates how straightforwardly predictive coding can
be applied to solve Bayesian filtering problems rather than simply
static Bayesian inference problems. Since the brain is enmeshed in
continuous sensory exchange with a constantly moving world, filtering is
a much more realistic challenge to solve than pure inference on a static
dataset. It thus seems likely that the neural circuitry dedicated to
perception is specialized for solving precisely these sorts of filtering
problems. Moreover, due to predictive coding’s biologically realistic
properties, our results provide a powerful biologically plausible
approach for how the brain might solve such filtering problems.

Nevertheless, there remain several deficiencies of our algorithm (and
predictive coding more generally) in terms of biological plausibility
which it is important to state. Our model assumes full connectivity for
the ‘diffuse’ connectivity required to implement matrix multiplications.
Additionally in other cases it requires one-to-one excitatory
connectivity, both constraints which are not fully upheld in neural
circuitry. Additionally, in one case (that of the "C matrix" between the
populations of neurons representing the estimate and the sensory
prediction errors), we have assumed a complete symmetry of backward and
forward weights, such that the connections which embody the C matrix
downwards also implement the @xmath matrix when traversing upwards. This
is also a constraint not satisfied within the brain. Additionally, our
model can represent negative numbers in states or prediction errors,
which rate-coded neurons cannot. Several of these implausibilities will
be directly addressed in the context of (static) predictive coding later
in this chapter.

We believe, however, that despite some lack of biological plausibility,
our model is useful in that it shows how a standard engineering
algorithm can be derived in a way more amenable to neural computation,
and provides a sketch at how it could be implemented in the brain.
Moreover, we hope to draw attention to Bayesian filtering algorithms and
how they can be implemented neurally, instead of just Bayesian inference
on static posteriors.

Finally, while our algorithm and experiments have only considered the
linear case, it can be straightforwardly extended to the nonlinear case,
where it results in standard nonlinear predictive coding as discussed
previously. Explicitly and empirically comparing the performance of our
algorithm against nonlinear extensions to the Kalman filter such as
extended or unscented ( wan2000unscented ) Kalman filtering are an
important and exciting avenue for future work.

### 3.5 Relaxed Predictive Coding

In the literature predictive coding has been proposed as a general
theory of cortical function ( friston2003learning ; friston2005theory ;
friston2008hierarchical ; kanai2015cerebral ; spratling2008reconciling )
. There is additionally a small literature of process theories which try
to translate the mathematical formalism into purported neural circuitry
( bastos2012canonical ; keller2018predictive ; kanai2015cerebral ) , and
some of the predictions of predictive coding have been extensively
compared and evaluated against neurophysiological data (
walsh2020evaluating ; friston2008hierarchical ; huang2011predictive ;
clark2015surfing ; aitchison2017or ) . Despite the general acceptance of
predictive coding as a biologically plausible algorithm which could in
theory be implemented in the brain, there nevertheless are several
highly implausible aspects of the core algorithm that have been largely
glossed over in the formulations of the process theories, which focused
primarily on macro-scale connectivity constraints instead of the precise
mathematical form of the learning and update rules in the algorithm (
bastos2012canonical ) . Here, we introduce three potentially severe
biological implausibilities which emerge directly from the form of the
predictive coding algorithm and demonstrate empirically how, with some
ingenuity and adaptation of the algorithm, these implausible assumptions
can be ‘relaxed’ without major damage to the empirical performance of
predictive coding networks on object recognition tasks. This work is
based on ( millidge2020relaxing )

Recall, that the core of the predictive coding formalism is three key
relationships. First, the concept of prediction error as the difference
between the activity of the neurons in a layer and the top-down
predictions from higher layers. Second, the update rule for the
activities of a layer, which minimizes both the prediction errors at its
own layer, as well as the layer below. And thirdly, the learning rule
for the weights, as a local Hebbian function of the prediction errors at
their own layer ( friston2005theory ) .

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (3.37)
  -- -------- -------- -- --------

where @xmath represents the partial derivative of the post-activations
with respect to either the @xmath or the @xmath depending upon the
update rule. Equation 3.5 states that prediction errors are computed as
a simple subtraction of the value neurons at a layer and the prediction
from the layer above. The vector @xmath represents the activity of the
value neurons at a specific level @xmath . The vector @xmath is a vector
of the activity of the error neurons at a level l. Predictions are
mapped down from the higher layers through a set of weights, denoted
@xmath which is an @xmath matrix where M is the number of neurons at
level @xmath and N is the number of neurons at level @xmath . @xmath is
a nonlinear activation function applied to the outputs of a neuron and
@xmath is the pointwise derivative of the activation function ⁸ ⁸ 8 Here
we have specialized the predictive coding update rules somewhat from any
arbitrary function @xmath to an elementwise nonlinearity followed by a
multiplication with a weight matrix @xmath . This is because in this
section we consider the application of predictive coding to train
artificial neural networks with this specific type of structure .

Equation 3.8 specifies the update rule for the @xmath at a specific
layer. The update is equal to the sum of the prediction errors projected
up from the layer below, multiplied by the top down predictions and
projected back through the weight matrix and subtracted from the
prediction errors at the current layer. This is a biologically plausible
learning rule as it is a simple sum of multiplication of locally
available information. Note: the the update includes prediction error
terms from both the current layer and the layer below. This equation is
why it is necessary to transmit prediction errors upwards.

Equation 3.9 is the update rule for the weights @xmath . This obeys
Hebbian plasticity since it is simply a multiplication of the two
quantities available at each end of the synaptic connection – the
prediction error of the layer below and the value neurons at the current
layer. The only slight difficulty is the derivative of the nonlinear
activation function of the prediction. While this information is locally
available in principle, it requires a somewhat more complex neural
architecture and it is not certain that the derivatives of activation
functions can be computed straightforwardly by neurons. Luckily, we show
below that this term is not needed for successful operation of the
learning rule.

Importantly, we additionally recall that predictive coding can be
considered to be a variational inference algorithm, as Equations 3.8 and
3.9 can be directly derived as a gradient descent upon the variational
free energy @xmath which (under Gaussian assumptions) takes the form of
a simple sum of squared prediction errors at each layer. We additionally
ignore the precision parameters @xmath in this analysis since in general
their biological plausibility has not been strongly analyzed in the
literature (although there are some speculative suggestions linking them
to either lateral connectivity ( friston2005theory ) or else subcortical
activity in the pulvinar ( kanai2015cerebral ) ).

Specifically, we focus on three important implausibilities. The first is
the problem of weight symmetry, or the required equality of forward and
backward weights. This problem is often called the weight transport
problem in the literature ( lillicrap2016random ; crick1989recent ;
lillicrap2020backpropagation ) . The learning rules in these networks
require information to be sent ‘backwards’ through the network. Since
synaptic connections are generally assumed to be uni-directional, in
practice this means that these backward messages need to be sent through
a second set of backwards connections with the exact same synaptic
weights as the forward connections. Clearly, expecting the brain to have
an identical copy of forward and backward weights is infeasible.
Mathematically, this problem arises from the @xmath term in the dynamics
equation for the @xmath s (Equation 3.8 ), since this weight transpose
uses the forward weight matrix @xmath but instead maps the bottom-up
prediction error to the level above, thus requiring information to be
sent ‘backwards’ or ‘upwards’ through ‘downwards’ connections. In the
brain, this would require information to propagate backwards from the
soma of the post-synaptic neuron, back through the axon and to the soma
of the pre-synaptic cell – a possibility which is considered to be
extremely implausible ( lillicrap2014random ) . Here, we address this
problem in predictive coding networks by using a separate set of
randomly initialized backwards weights trained with a separate Hebbian
learning rule, which also only requires local information. This removes
the necessity of beginning with symmetrical or identical weights and
proposes a biologically plausible method of learning good backward
weights from scratch in an unsupervised fashion. In the brain this would
be implemented as a reciprocal set of ‘backwards’ connections going from
the lower-layers to higher layers, which are definitely present in the
brain ( grill2004human ) .

The weight transport problem is also present in neural implementations
of the backpropagation of error algorithm from machine learning, and
there exists a small literature addressing it within this context. A key
paper ( lillicrap2014random ; lillicrap2016random ) shows that simply
using random backwards weights is sufficient for some degree of
learning. This method is called feedback alignment (FA) since during
training the feedforward weights learn to align themselves with the
random feedback weights so as to transmit useful gradient information. A
variant of this – direct feedback alignment (DFA) ( nokland2016direct )
has been shown that direct forward-backward connectivity is not
necessary for successful learning performance. Instead, all layers can
receive backwards feedback directly from the output layer. It has also
been shown ( liao2016important ) that performance with random weights is
substantially improved if the forward and backward connections share
merely the same sign, which is less of a constraint than the exact
value. One further possibility is to learn the backwards weights with an
independent learning rule. This has been proposed independently in
amit2019deep and akrout2019deep who initialize the backwards weights
randomly, but train them with some learning rule. Our work here differs
primarily in that we show that this learning rule works for predictive
coding networks while they only apply it to deep neural networks learnt
with backprop. Moreover, our Hebbian learning rule can be
straightforwardly derived in a mathematically principled manner as part
of the overarching variational framework of predictive coding.

The second problem is one of backward nonlinear derivatives. In
predictive coding networks (along with backprop), the update and
learning rules require the pointwise derivatives of the activation
function to be computed at each neuron. Mathematically, this is the
@xmath term. For individual biological neurons, while a nonlinear
forward activation function is generally assumed, the ability to compute
the derivative of the activation function is not known to be
straightforward. While in some cases this issue can be ameliorated by a
judicious choice of activation function – for instance the pointwise
derivative of a rectified linear unit is simply 0 or 1, and is a simple
step function of the firing rate – the problem persists in the general
case. Here, we show that, somewhat surprisingly, it is possible to
simply ignore these pointwise derivatives with relatively little impact
on learning performance, despite the update rules now being
mathematically incorrect. This may free the brain of the burden of
having to compute these quantities.

A third issue, specific to frameworks that explicitly represent
prediction errors, is the requirement of one-to-one connections between
activation units and their corresponding error units. While not
impossible, this precise, one-to-one connectivity pattern is likely
difficult for the brain to create and maintain throughout development
and learning. One possibility, as explored by sacramento2018dendritic is
that prediction errors and predictions may be housed in separate
dendritic compartments on a single neuron, thus potentially obviating
this issue (although their scheme relied on another set of one-to-one
connections between pyramidal cells and inhibitory interneurons).
However, the plausibility of this idea in terms of actual dendritic
morphology and neurophysiology is unclear. Instead, here we present a
network-level solution and show that learning can continue unaffected
despite random connectivity patterns between value and error units as
long as these connection weights can also be learned. We propose a
further Hebbian and biologically plausible learning rule to update these
weights which also only requires local information. Finally, we
experiment with combining our solutions to all of these problems
together to produce a fully relaxed predictive coding architecture.
Importantly, this architecture possesses a simple bipartite but
otherwise fully connected connectivity pattern with separate learnable
weight matrices covering every connection, all of which are updated with
local Hebbian learning rules. We show that despite the simplicity of the
resulting relaxed scheme, that it can still be trained to high
classification accuracy comparable with standard predictive coding
networks and ANNs using backpropagation.

#### 3.5.1 Methods

To test the performance of the predictive coding network under various
relaxations, we utilize the canonical MNIST and FashionMNIST (
xiao2017fashion ) benchmark datasets. Since this is a supervised
classification task, we follow the approach of
whittington2017approximation and millidge2020predictive , who utilized a
‘reverse’ predictive coding architecture where the inputs were presented
to the top layer of the network and the labels were predicted at the
bottom. This formulation allows for the straightforward representation
of supervised learning problems in predictive coding. In effect, the
network tries to generate the label from the image. We utilized a
4-layer predictive coding network consisting of 784, 300, 100, and 10
neurons in each layer respectively. We tested both rectified-linear
(relu) and hyperbolic tangent (tanh) activation functions, which are the
most common activation functions used in machine learning. During
training the @xmath s were updated for 100 iterations using Equation 3.8
with both the input and labels held fixed. After the iterations of the
@xmath s, the remaining prediction errors in the network were used to
update the weights according to Equation 3.9 . At test time, a digit
image was presented to the network, and the top-down predictions of the
network were propagated downwards to produce a prediction at the lowest
layer, which was compared to the true label to obtain the test accuracy.

The network was trained and tested on the MNIST and FashionMNIST
datasets. MNIST is a dataset of 60,000 @xmath grayscale images of
handwritten digits from 0 to 9. The goal is to predict the digit from
the image. The FashionMNIST dataset contains images of different types
of clothing, which must be sorted into ten classes. FashionMNIST is a
more challenging classification dataset, while also serving as a drop-in
replacement for MNIST since its data is in exactly the same format. The
input images were flattened into a 784x1 vector before being fed into
the network. Labels were represented as one-hot vectors and were
smoothed using a value of 0.1 for the incorrect labels. The dataset was
split into a training set of 50000 and a test set of 10000 images. All
weight matrices were initialized as draws from a multivariate Gaussian
with a mean of 0 and a variance of 0.05. It is likely, given the large
literature on how to best initialize deep neural networks, that there
exist much better initialization schemes for predictive coding networks
as well, however we did not investigate this here. All results presented
were averaged over 5 random seeds. We plot error bars around the means
as the standard deviation of the seeds.

#### 3.5.2 Results

##### Weight Transport

Mathematically, the weight transport problem is caused by the @xmath
term in Equation 3.9 . In neural circuitry this weight transpose
corresponds to transmitting the message backwards through the same
connections or, alternatively, an identical copy of the backward
weights. We wish to replace this copy of the forward weights with an
alternative, unrelated set of weights @xmath . Unlike FA or DFA methods,
which simply use random backwards weights, we propose to learn the
backwards weights through a simple synaptic plasticity rule.

  -- -------- --
     @xmath   
  -- -------- --

This rule is Hebbian since it is just the multiplication of the
activities of the units at each end of the connection. The backwards
pointwise derivative poses a slight problem in that it is first
multiplied with the errors of the level below. However, as we show
below, pointwise nonlinear derivatives are not actually needed for good
learning performance, so the problem is surmounted. This rule is simply
the transposed version of the original weight update rule (Equation 3.9
). And thus, if the forward and backwards weights are initialized to the
same value, barring numerical error, they will stay the same throughout
training. Importantly, we demonstrate here that this rule allows rapid
and effective learning of the weights even if the forward and backwards
matrices are initialized in a completely independent (and random)
fashion. This means that in the brain the forwards and backwards
connections originate completely independently, and that, moreover, if
we accept the forward weight update rule as plausible, we should accept
the backwards weight update as well, thus leading to no greater demands
of biological plausibility for this backwards weight update.

This procedure allows us to begin with a randomly initialized set of
backwards weights, and then applying the learning rule to these weights
allows us to very quickly recover performance equal to the identical
backwards weights. As shown in Figure 3.16 , performance both with and
without the learnt backwards weights is almost identical for both the
relu and tanh nonlinearities and the MNIST and FashionMNIST datasets,
thus suggesting that this approach of simply learning an independent set
of backwards weights is a highly effective and robust method for
tackling the weight transport problem.

##### Backwards nonlinear derivatives

The second remaining biological implausibility is that of the backwards
nonlinear derivatives. Note that in Equations 3.5 , an @xmath term
regularly appears denoting the pointwise derivative of the nonlinear
activation function. Since these are pointwise derivatives, when the
mathematics is translated to neural circuitry, these derivatives need to
be computed at each individual neuron. It is not clear whether neurons
are capable of easily computing with the derivative of their own
activation function. We apply a straightforward remedy to address this.
We simply experiment with removing the pointwise derivatives from the
update rules. For instance Equation 3.8 would become just @xmath .
Perhaps surprisingly we found that this modification, although not
mathematically correct, did little to impair performance of the model at
classification tasks, except perhaps for the hyperbolic tangent
nonlinearity in the FashionMNIST case.

In effect, by removing the pointwise nonlinear derivatives, we have made
the gradient updates linear in the parameters. Since the real updates
are nonlinear, our update rules are simply the projection of the
nonlinear update rules onto a linear subspace. However, using a similar
argument to that in feedback alignment, we hypothesize that it is likely
that the linear projection of the nonlinear updates are quite close in
angle to the nonlinear updates, so the direction of the linear gradient,
averaged over many batches and update steps, is sufficiently close to
the true gradient as to allow for learning in this model. An alternative
option is to note that the derivatives of the nonlinearity depend
closely upon the value of the function. In the case of the relu
nonlinearity, the derivative is only different from 1 when the activity
is 0 (the neuron does not fire). When there is no firing, there can
still be updates to the dynamics – which depend on the prediction errors
and not on the actual firing rate – and so there is still error when
dropping the derivative term. However, if most activations are greater
than 0, the error should be minimal, which is what we appear to observe.
Similarly, in the hyperbolic tangent nonlinearity, the region of
activation between @xmath and @xmath is broadly linear, and thus we
should expect the dropping of the nonlinear derivative term in this
region to have relatively little effect. The robustness and relatively
little impact on training therefore suggest that the activations of the
predictive coding network largely remain within this stable regime over
the course of training – an intriguing and important finding given that
we made no efforts (such as regularization) to specifically encourage
this outcome. If brains operated in a similar regime, it may mean that
explicit computation of the activity derivatives is unnecessary, which
would make credit assignment substantially easier (if approximate).

##### Error connections

The third and final biological implausibility that we address in this
section is that of the one-to-one connections between value and the
error units at a given layer. This can be seen directly for the
prediction errors in Equation 3.5 , but broken down into individual
components (or neurons).

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

We see that the activity of the error unit vector (i.e. each error
neuron @xmath ) is driven by a one-to-one connection from its matching
value neuron @xmath . By contrast, the top-down predictions have a
diffuse connectivity pattern, where every value neuron @xmath in the
layer above affects each error neuron @xmath through the synaptic weight
@xmath . A one-to-one connectivity structure is a highly precise and
sensitive pattern and it is difficult to see how it could first develop
and then be maintained in the brain throughout the course of an
organisms life. Additionally, while precise connectivity can exist in
theory, there is little evidence neurophysiologically (
bastos2012canonical ; walsh2020evaluating ) for the kind of regular and
repeatable one-to-one connectivity patterns that predictive coding would
require in the brain. Moreover, if predictive coding were implemented
throughout the cortex, this one-to-one connectivity should be highly
visible to neuroscientists. To relax this one-to-one connectivity
constraint, we postulate a diffuse connectivity pattern between them,
mediated by a set of connection weights @xmath . The new equation for
the prediction errors becomes:

  -- -------- -- --------
     @xmath      
     @xmath      (3.39)
  -- -------- -- --------

While using randomly initialized weights @xmath completely destroys
learning performance, it is possible to learn these weights in an online
unsupervised fashion using another Hebbian learning rule. The learning
rule for the error weights @xmath can be derived as a gradient descent
on the variational free energy function, whereby now the prediction
errors include the error weights,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (3.40)
  -- -------- -------- -- --------

This rule is completely linear and Hebbian since it is simply a
multiplication of the activations at the two endpoints of the
connection. We show in Figure 3.19 that using this rule allows
equivalent learning performance to the one-to-one case as the required
weight values are rapidly learnt.

We see that, overall, training performance can be maintained even with
learnt error connections, a perhaps surprising result given how key the
prediction errors are in driving learning. Interestingly, we see a
strong effect of activation function on performance. Performance is
indistinguishable from baseline with a relu activation function but
asymptotes at a slightly lower value than the baseline with tanh.
Investigating the reason for the better performance of the relu
nonlinearity would be an interesting task for future work.

##### Combining Relaxations

It is also possible to combine all of the above relaxations together in
parallel, to create a network architecture which is avoids many of the
major biological plausibility pitfalls of predictive coding. A schematic
representation of this combined architecture compared to the standard
predictive coding architecture is shown below (Figure 3.20 ).

The relaxed architecture no longer has any one-to-one connectivity
patterns, which have been replaced with full connectivity matrices
parametrised by the error connection weights @xmath . Moreover, the
forwards and backwards weights are separated into two separate and
independent weight matrices @xmath and @xmath while the standard
predictive coding model uses the true weight transpose @xmath , which
requires copying the weights. In essence, the fully relaxed architecture
simply consists of two bipartite populations of neurons, which only
synapse onto the other population. Beyond this there is no special
connectivity structure required. Nevertheless, we show that even this
very simple architecture, with only Hebbian learning rules, can still be
trained to perform well at supervised classification.

We tested the classification ability of the fully relaxed architecture
with both hyperbolic tangent and rectified linear activation functions,
and on the MNIST and FashionMNIST datasets, and the results are shown in
Figure 3.21 . Overall we found that another strong effect of activation
function where this time training was unstable and diverged when using
rectified linear units but not when using tanh neurons. We hypothesize
that this could be due to the rectified linear units not having a
saturation point unlike tanh and thus being more prone to exploding
gradients. We found, on the other hand, that while performance of the
fully relaxed network asymptotically tended to be slightly worse than
the standard, it was still very high on both the MNIST and fashion MNIST
datasets, thus showing that even highly relaxed and extremely local
networks with an extremely generic, essentially fully connected,
connectivity pattern can be trained to very high accuracies using this
predictive coding algorithm which only requires Hebbian updates.

#### 3.5.3 Discussion

We have shown that it is possible to surmount three key biological
implausibilities of the predictive coding learning rules, and thus
strengthen the case that predictive coding may be implemented in
cortical circuitry. In the weight transport and error connections case,
the solution has been to propose a separate set of weights which are
themselves learnable by a Hebbian rule. In the backwards nonlinearities
case, it suffices simply to ignore the biologically implausible terms in
the rule. Moreover, we have shown that performance, at least under the
hyperbolic tangent nonlinearity, is still stable and roughly comparable
with the baseline when all the relaxations are combined together, thus
resulting in an extremely local,straightforward, and biologically
plausible architecture. Overall, we believe these results show that
predictive coding offers a surprisingly robust model of learning and
inference in the brain and that it can survive often severe
perturbations to its basic equations. Through this work we have
substantially diminished the constraints a neurophysiologically
realistic process theory of predictive coding must satisfy. In so doing,
it is possible that predictive coding may now fit a greater part of
neurophysiological data, while opening the way to potentially
constructing novel microcircuit designs which implement relaxed forms of
predictive coding. There may also be gains from applying these
heuristics to other biologically plausible approximations to backprop.
We show in the final chapter on biologically plausible credit assignment
in the brain, that many of these techniques also work for other
algorithms, thus hinting at potentially general properties of
perturbational robustness of these neurally inspired learning algorithms
which may be of significant theoretical interest.

An additional theoretical note is the power of the assumption of
variational optimality. Like a Lagrangian in physics, the variational
free energy @xmath enables potentially complex ‘laws of motion’ to be
derived through a simple mathematical apparatus and which can be
extended to lead to otherwise-difficult insights. For instance, the
learning rules for the error-weights @xmath can be derived
straightforwardly in the variational framework as simple gradient
descents on @xmath given an augmented generative model containing the
@xmath term. Regardless of one’s theoretical or ontological commitments
to variational inference in the brain, the mathematical reformulation of
neural activity as encoding solutions to a variational inference problem
allows considerable modelling flexibility and mathematical insight
through which results can be easily derived which would be much harder
to achieve through other means.

Having removed the symmetric backwards weights and the one-to-one error
connectivity scheme, we are left with an essentially bipartite graph.
There are connections between the value and error units of the same
level, and the error units of one level and the value units of the level
above, but crucially there are no direct connections between the value
or error units of one layer and those of the layer above. Stepping out
of the predictive coding framework, we have effectively shown that a
simple bipartite connectivity structure and Hebbian learning rules
suffices to learn complex input-output mappings, and may mathematically
approximate backpropagation. This is a surprising result given that
previously Hebbian learning has not generally been thought to be
sufficient to learn complex representations in the brain (
baldi2016theory ) . This shows that perhaps it is possible for the brain
to go further with clever connectivity patterns and Hebbian learning
than previously thought.

It is also important to note here that while we have resolved several
biological implausibilities of predictive coding, there are still
several other difficulties that must be faced before a direct
implementation of predictive coding is possible given what we currently
know about neural circuitry. A key challenge is the simple problem of
negative prediction errors and activations – in the mathematical
formalism, prediction errors and values are real numbers which can be
both positive and negative, and to any degree of accuracy. In the brain,
however, we assume that these numbers are represented by average firing
rates, which cannot go negative, and additionally have a degree of
accuracy constrained by the intrinsic noise levels of the brain and the
integration windows over which post-synaptic neurons can listen. While
it seems likely that a lack of numerical accuracy is not that
significant for neural networks in general, given recent results in
machine learning demonstrating that only 16 bit floats are necessary at
most ( gupta2015deep ) , the issue of negative numbers is substantial
since negative prediction errors are absolutely necessary for the
functioning of the algorithm. One possibility is that the brain could
maintain a high default firing rate, and treat deviations below this
default as negative. However, the maintenance of a sufficiently high
default firing rate would be energy inefficient, and there is much
evidence that neurons primarily maintain low tonic firing rates (
walsh2020evaluating ) . Another option could be to utilize separate
populations of ‘positive’ and ‘negative’ neurons, perhaps excitatory and
inhibitory neurons, however this would require a precise connectivity
scheme to integrate these two contributions together, which has largely
not yet been worked out in the context of predictive coding.

An additional limitation of our work is that we have only tested the
performance of the relaxations on relatively small networks and using
the relatively simple MNIST and Fashion-MNIST datasets which are simple
enough that even fairly non-scalable methods can work on them. A prime
example of this is the study by bartunov2018assessing who showed that
many of the methods in the literature for alternative biologically
plausible methods for credit assignment, although performing well on
MNIST, generally performed poorly on more challenging datasets such as
CIFAR10, CIFAR100, and ImageNet. A key task must be to investigate the
scaling properties of these relaxations to more challenging tasks and
datasets, as well as different network architectures such as
convolutional neural networks. Some preliminary, but supportive results
come from millidge2020investigating (discussed in chapter 6) where we
show that the learnable backwards weights and dropping the nonlinear
derivatives do in fact scale to larger scale CNN networks.

### 3.6 Conclusion

In this chapter, we have studied the application of the free energy
principle to perception – specifically by investigating and proposing
substantial extensions to the predictive coding process theory (
friston2003learning ; friston2005theory ; friston2008hierarchical ) as
well as testing the performance of large-scale implementations of the
theory. Overall, we believe that our work in this chapter has make
substantial improvements to the theory and practice of predictive
coding.

Specifically, for the first time, we have implemented and tested
predictive coding models on a larger scale than previously – and have
compared them against machine learning approaches on standard machine
learning datasets such as MNIST. We have demonstrated that predictive
coding networks are able to successfully reconstruct digits
successfully, interpolate between them, and separate out different digit
representations in the learnt latent space despite being trained with an
entirely unsupervised objective. Moreover, we have demonstrated that
predictive coding can achieve this in hierarchical and dynamical setups
with randomized initial weights which are then learned, in contrast to
prior work ( friston2008DEM ; friston2008hierarchical ;
friston2005theory ) which primarily focus on the inference capabilities
of predictive coding and provide a-priori the correct generative model.
We also implemented and demonstrated that dynamical, and both
hierarchical and dynamical predictive coding models can function well on
simple toy tasks and can very quickly learn various challenging
wave-forms.

Secondly, we have investigated the use of predictive coding algorithms
for filtering tasks (as opposed to static inference). In filtering, the
aim is to infer an entire trajectory of states given a trajectory of
observations, instead of simply inferring a single hidden state given a
single observation. We make precise, for the first time, the precise
relationship between Kalman filtering – a ubiquitous algorithm in
classical control and filtering theory – and predictive coding which is
that the predictive coding dynamics can be derived as a gradient descent
on the Gaussian maximum-a-posteriori objective, which the Kalman filter
equations can be derived as an analytical solution to. We then
demonstrate the successful filtering capabilities of our predictive
coding filtering algorithm, and demonstrate how the broader variational
approach allows us to successfully also learn the parameters of the
generative model online for filtering tasks – thus performing double
deconvolution where we infer both states and parameters simultaneously.

Finally, we have investigated and improve the biological plausibility of
the predictive coding process theory which, after all, is often
explicitly proposed as a neuroscientific theory of cortical function (
friston2003learning ; bastos2012canonical ) . Here we focus on and
present solutions to three outstanding issues of biological
implausibility with the standard predictive coding dynamics. First, we
address the weight transport problem – the need to transmit activities
backwards – by proposing a set of independent backwards weights which
are initialized randomly, and then can also be learnt with an
independent and biologically plausible Hebbian update rule. Secondly, we
address the problem of nonlinear derivatives by showing that in many
cases these derivatives can be dropped from the update rules with
relatively small performance penalties, and thirdly, we address the
issue of needing precise one-to-one error to value neuron connectivity
by proposing instead fully distributed connectivity between error and
value neurons, but with an additional learnable weight matrix which can
be additionally optimized with another Hebbian rule. We show that these
relaxations can substantially improve the biological plausibility of the
predictive coding algorithm while only causing relatively small
degradations of performance on machine learning benchmark classification
tasks.

Overall, therefore, we believe that in this chapter we have made
significant contributions to the theory and practice of predictive
coding. On the theoretical level, we have demonstrated its relationship
to Kalman filtering, and we have addressed several outstanding
challenges of biological implausibility that the standard theory faces.
On an implementational and practical level, we have empirically
investigated for the first time the performance of predictive coding
networks within the machine learning paradigm on machine learning
benchmark tasks, and especially in cases where the true generative model
is not provided to the network a-priori. Additionally, through our work,
we have substantially scaled up predictive coding approaches to handle
significantly larger and more challenging tasks than previously, and
have made these implementations available to the community through a
number of open-source software projects ⁹ ⁹ 9 See:
https://github.com/BerenMillidge/PredictiveCodingBackprop ,
https://github.com/BerenMillidge/RelaxedPredictiveCoding ,
https://github.com/BerenMillidge/NeuralKalmanFiltering

In the next two chapters, we advance from the problem of perception, to
consider the problem of action selection through the lens of the free
energy principle, and its concomitant process theory active inference .
In some ways this problem is more challenging than pure perception,
since it requires the modelling of entire trajectories of observations,
states, and actions, in order to make the best long term decisions which
will, over time, outperform locally greedy options. In the next Chapter
(Chapter 4), we focus primarily on scaling up existing active inference
models using deep neural networks to match the performance of state of
the art reinforcement learning algorithms. In the chapter after that
(Chapter 5), we aim to provide a deep mathematical investigation and,
ultimately, insight into the nature of objective functionals which
combine both reward-seeking and information-seeking imperatives.

## Chapter 4 Scaling Active Inference

### 4.1 Introduction

In this chapter, we consider the application of the free energy
principle to action selection, or control, problems. While in the
previous chapter on perception, we focused on the process theory of
predictive coding, here we focus on the process theory of active
inference, and are especially inspired by the discrete state-space
active inference theory introduced in Chapter 2. Here, we aim to solve a
key limitation of those methods – their scalability. We propose to do so
by parametrizing the key densities of the generative model and
recognition distribution by deep neural networks, and then utilizing the
tools of deep reinforcement learning to allow active inference agents to
scale to levels comparably achieved by contemporary machine learning.

This chapter comprises multiple sections. At the beginning, we give a
detailed introduction to reinforcement learning (
sutton2018reinforcement ) , and especially deep reinforcement learning,
as well as the control of inference framework ( rawlik2013probabilistic
; levine2018reinforcement ) from reinforcement learning which also
frames the control problem as one of inference. Then we present two
studies where we demonstrate that active inference approaches can scale
up to be comparable with contemporary deep reinforcement methods. We
achieve this scaling by first parametrizing the key distributions in the
active inference model by deep neural networks trained through gradient
descent, and secondly by approximating the (exponential time)
computation of the path integral of the expected free energy either with
an amortized neural network prediction, or else through monte-carlo
trajectory sampling using a continuous action planning algorithm. In
doing so, we create algorithms that are comparable in scalability and
performance to current methods in deep reinforcement learning.
Additionally, we demonstrate that oftentimes these algorithms can
outperform their reinforcement learning counterparts due to the unique
properties and insights active inference brings to the table.

In the second section of this chapter, we focus a little more abstractly
in trying to understand the difference between model-free and
model-based reinforcement learning approaches in terms of inference, and
determine that this difference is primarily due to the difference
between what we call iterative variational inference – where the
parameters of the variational distribution are directly optimized – and
amortized inference – where instead the parameters of a function which
outputs the parameters of the variational distribution are optimized.
Given this distinction, we first use it to present a taxonomy of a wide
range of current reinforcement learning algorithms using a simple
two-dimensional quadrant, and secondly, we derive novel algorithms which
emerge by combining both iterative and amortized inference together – an
approach we call hybrid inference – which results in powerful algorithms
which combine the benefits of each approach, while ameliorating their
respective weaknesses.

#### 4.1.1 Reinforcement Learning

The reinforcement learning, or control, problem is one of the most
fundamental problems in artificial intelligence and in engineering
adaptive systems ( sutton1990integrated ; sutton1998introduction ;
kaelbling1996reinforcement ; dayan1997using ) , and concerns the
computation of optimal action ( wolpert1997computational ;
todorov2008general ) . The problem is simple. We assume that there is
some kind of agent in some kind of environment, and that the agent can
take actions which affect the environment ( sutton1998introduction ) .
Suppose that the agent has some kind of notion of goals or desires that
it wants to achieve – whether these are encoded as a desire
distribution, as an objective function, or as rewards given by the
environment. The control problem is to compute the optimal action
schedule to fulfill the agent’s goals. As might be expected, this
question has huge applications and implications for an extremely wide
range of fields, from machine learning and artificial intelligence
(understanding how to make artificial agents act to achieve their goals)
( sutton1998introduction ; mnih2013playing ; silver2016mastering ;
schrittwieser2019mastering ; schulman2015trust ) to cognitive science
and economics (understanding how humans implement action strategies to
achieve their goals) ( todorov2008general ; wolpert1997computational ;
dayan2008decision ; daw2006cortical ) to biology (how do all sorts of
biological systems act adaptively) ( dayan2009goal ;
mehlhorn2015unpacking ; krebs1978test ; pyke1984optimal ) to control
theory (how to design and program systems which can adaptively regulate
and control their environments) ( kirk2004optimal ; kwakernaak1972linear
; sethi2000optimal ; kalman1960contributions ; johnson2005pid ;
kappen2005path ) .

While this provides an intuitive specification of the control problem,
to make real progress we must make it precise mathematically. First, we
assume that the environment has states which we denote @xmath and that
the agent can emit actions @xmath . Secondly, we assume that there
exists some reward function which emits rewards @xmath dependent on
environmental states @xmath . The only thing the agent has control over
are its actions @xmath which can affect the environment to give it more
rewards. We assume that the agent optimizes over trajectories of states
and actions going into the future, which we denote as @xmath , @xmath ,
and @xmath . For the moment, to retain full generality, we remain
indifferent to whether the agent considers time continuous or discrete,
so that @xmath . Finally, we assume that the environmental dynamics and
the rewards granted can both be stochastic and can thus be
mathematically formalized in terms of probability distributions @xmath
and @xmath . A key advantage of this probabilistic formalism is that it
allows us to represent (and remain agnostic between) intrinsic
stochasticity in the environment, and the agent’s uncertainty about the
environment. If the environment or rewards are in fact deterministic and
known, we can simply set the distributions to be dirac deltas to recover
a deterministic framework. Under this formalism, the objective of the
control problem is to maximize ¹ ¹ 1 Here, following the convention in
reinforcement learning and economics, we are optimistic and we talk
about reward (or equivalently utility) maximization. Control theory, on
the other hand, takes a more depressive interpretation and works in
terms of minimizing costs. Mathematically, these two formulations are
completely equivalent. ,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (4.1)
  -- -------- -------- -- -------

Where we can safely take the log of the reward function since log is a
monotonic function and does not impact the optimum of the optimization
process, but tends to make things nicer numerically. Essentially, what
this states is that the control objective is simply to maximize the
probability or amount of reward expected under the trajectory of
environment states given the agent’s trajectory of actions. From this,
we can see that to first get a handle on the control problem, we need to
understand firstly the environmental dynamics @xmath and the reward or
utility function @xmath . We typically represent these dynamics as
stochastic differential (or difference) equations depending on whether
time is discrete or continuous, as follows,

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

for continuous time, where @xmath is some kind of noise or,

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

for discrete time. One simplifying assumptions we often make is that the
dynamics are Markovian, meaning that the state at time @xmath can be
computed solely in terms of the state at time @xmath and the action at
time @xmath , thus that the dynamics become,

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

This approach simplifies the analysis considerably. An additional
assumption, which is often made, is that the rewards depend only on the
state and actions at the current time @xmath . Under these assumptions
the environment of the control problem can be considered to be a Markov
Decision Process (MDP). It is also sometimes the case that we assume we
do not know the true state of the environment, but are only given access
to partial observations @xmath which may not be Markov, even though the
hidden states are Markov. The observations are related to the states
through a likelihood mapping @xmath . This type of environment is called
a Partially-Observed Markov Decision Process (POMDP) (
kaelbling1996reinforcement ) and is substantially harder to solve
optimally than an MDP due to the need to correctly infer the hidden
states @xmath from the observations @xmath . Nevertheless, the POMDP
model has a great deal of generality since, as the state is hidden, it
can be whatever is necessary to preserve Markovian dynamics, thus
enabling any non-Markovian environment to be written in terms of a
Markovian POMDP.

Early approaches to the control problem tried to use methods in
variational calculus to directly find analytical solutions to the
control problem. Such approaches yielded success in some simple but
important, cases, such as Markov linear Gaussian dynamics and quadratic
costs. These conditions correspond to dynamics which can be specified as
(assuming discrete time),

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (4.5)
  -- -------- -------- -- -------

Where A, B, Q, and R, are known matrices and @xmath is white Gaussian
Wiener noise ( wiener2019cybernetics ) . In this case, an analytical
solution exists in both continuous and discrete time which gives rise to
the linear quadratic regulator ( kirk2004optimal ;
kalman1960contributions ; kalman1960new ) , a centerpiece of modern
control theory which is remarkably effective for controlling even
complex systems, and for which control solutions can be computed very
relatively cheaply and in real time. While the linear dynamics and
quadratic costs conditions are quite restrictive (especially the linear
dynamics), the linear quadratic regulator approach can be extended
somewhat to nonlinear dynamics by simply using a local linearity
approximation at every timestep and applying model-predictive control.
This iterative LQR ( li2004iterative ) algorithm, is quite robust and
can achieve significant feats of nonlinear control, including use in
controlling industrial robotics ( feng2014optimization ) . Other
variational approaches have also been applied and can be quite effective
in many cases. For instance, Pontryagin’s maximimum principle (
kopp1962pontryagin ; kirk2004optimal ) or ‘bang-bang’ control can be
applied productively to find optimal policies in many settings.
Recently, there has been advances using path integral methods and the
Feynman-Kac lemma to find control solutions for certain classes of
nonlinear dynamics ( kappen2005path ; kappen2012optimal ;
williams2017information ) .

Another approach to the control problem, which can work for arbitrary
dynamics, is to simply optimize the control function by gradient descent
with respect to the actions. Such an approach goes by the name of policy
gradients, since given a policy function @xmath parametrised by
parameters @xmath , we can simply compute gradients of the control
problem loss @xmath and optimize the parameters by stochastic gradient
descent. The chief difficulty is to propagate gradients through the
potentially nondifferentiable and unknown expectation under the
environmental dynamics @xmath . Luckily, this is achievable through the
policy gradient theorem ( williams1989learning ; sutton1998introduction
)

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (4.6)
  -- -------- -------- -- -------

Which allows an estimate of the gradient to be computed simply through
averages of environmental dynamics. Importantly, this approach does not
require knowledge of the true environmental dynamics at all (unlike
classical control theory), since we only require samples from the
environment which can be obtained simply through interacting with it.
Intuitively, we can think of this theorem as saying that the gradient of
the control objective is simply the average gradient of the policy,
weighted by the rewards received.

While this method works, the downside is that since the expectation is
effectively computed through Monte-Carlo sampling (and generally
relatively few samples at that), the gradient estimates generally have a
very high variance, which makes learning troublesome and slow. A number
of baseline approaches have been invented to try to deal with this
problem ( sutton2018reinforcement ) and to make it more tractable.
Nevertheless, policy gradient approaches can be scaled up and applied
successfully in challenging tasks, especially continuous control tasks,
by parametrizing the policy @xmath with a deep neural network and
directly applying the policy gradient theorem with some additional
tricks ( schulman2015trust ; schulman2017proximal ) .

Another approach to the control problem in Markov conditions (but
arbitrary dynamics as long as they are Markov) is to use a recursive
solution method pioneered by Richard Bellman ( bellman1952theory ) . He
noticed that optimal solutions to the control problem satisfy an
interesting recursive relationship – that the optimal path to the goal
at a timestep @xmath , must include the optimal path to the goal at a
later timestep @xmath . This property allows you to build up a backwards
recursion where you start at the goal at the end and then work
backwards, constructing the optimal path in a piecewise fashion from the
previous optimal path. The key mathematical quantity, here, is the
cost-to-go, which intuitively is the cost of the optimal trajectory from
the current position to the goal. In modern reinforcement learning
parlance, this cost-to-go is called the optimal value function of a
state, and is conversely the expected reward which would be attained
from a given state assuming the optimal policy is followed. Written out
mathematically, this approach gives rise to the recursive Bellman
equation,

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

which simply states that the optimal value function of a current state,
is the reward of the current state plus the maximum average value
function of the next state. In effect, if iterated backwards from the
end (where the optimal value function is simply @xmath and assumed
known) this recursion allows you to build up the optimal path by working
backwards. If all environmental states and actions are known and finite,
then this algorithm can be run explicitly to compute the optimal
solution in polynomial time (as opposed to the exponential time approach
of just trying all possible paths and picking the best). This is thus a
dynamic programming algorithm which is equivalent to other standard
dynamic programming algorithms in computer science such as Dijkstra’s
algorithm for the shortest paths.

Importantly, the Bellman recursion holds not just for the optimal policy
and value function, but indeed for any policy and value function, this
allows solution methods using this recursion to apply even when the
state and action space is too large to represent explicitly. In this
case, we can write the Bellman recursion as,

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

and, without working backwards from the end, we can simply estimate the
value functions @xmath of a given policy @xmath by moving around in our
environment, computing rewards and storing the state and the next state,
and applying Equation 4.8 . If we do this sufficiently for a given
policy, we can then form an estimate of the global value function @xmath
for all x. With this value function, we can then improve the policy, by
simply defining a new policy that takes @xmath . Somewhat surprisingly,
it has been proven that if the estimated value function is accurate,
then the new policy defined in such a manner is necessarily the same or
better (in terms of average reward obtained from the MDP) than the
previous policy, and that if we iterate the process of sampling new
states to estimate the value function, and then improving the policy,
then we will converge upon the optimal policy @xmath (
sutton2018reinforcement ) . This approach, called policy iteration, is
the cornerstone of classic reinforcement learning algorithms such as
temporal difference learning ( sutton1988learning ) , and SARSA (
sutton1996generalization ; singh1996reinforcement ) .

A closely related approach is called Q-learning ( watkins1992q ) , which
instead of using the value function, instead maintains an estimate of
the state-action value function @xmath , which is called the Q-function
for historical reasons. The Q function satisfies a similar recursive
relationship to the value function,

  -- -------- --
     @xmath   
  -- -------- --

Given an estimate of the Q function, the policy improvement step is
simple. @xmath . The Q-learning algorithm combines both policy
evaluation (estimating the value or Q function) and policy improvement
into one continuous algorithm, which can be simply defined as,

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (4.9)
  -- -------- -------- -- -------

The Q-learning algorithm is extremely popular and effective and has been
central to many major successes of reinforcement learning, from playing
backgammon ( tesauro1994td ) to Atari ( mnih2013playing ;
schrittwieser2019mastering ) . The Q function and value function can be
related straightforwardly by @xmath – i.e. the value function is simply
the Q function averaged over all actions. Another approach is to write
everything instead in terms of an advantage function @xmath which simply
subtracts the action-independent value function baseline from the
Q-value, effectively normalizing it, since the only important thing from
the perspective of Q learning is the relative values of each action.
This approach reduces the gradient variance when trying to estimate the
Q function and often makes the resulting algorithms more stable (
hessel2018rainbow ) .

In classical reinforcement learning we typically represent the Q and
value function explicitly in discrete-state and discrete-action
environments. For instance, with discrete states, the value function
@xmath would simply be a vector of length @xmath where @xmath is the
number of distinct states. The Q function @xmath is simply an @xmath
matrix where @xmath is the action dimension.

Another useful representation is the successor representation (
dayan1997using ) . This approach rewrites the value function in terms of
the instantaneous reward @xmath and a successor matrix @xmath which is a
@xmath matrix which represents the average transition probabilities for
a given policy from state @xmath to state @xmath . This matrix @xmath
can be thought of as the stationary transition distribution of the
Markov chain for a given policy. The value function can be decomposed
into,

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

The successor representation, crucially, by separating out the
policy-dependent component (M) from the reward @xmath allows for
computation of different value functions for a given policy rapidly
under different reward functions. Thus this representation allows for
very flexible changes of behaviour given a change in reward. Of course
the optimal policy also changes under a change of reward function, and
this change cannot be straightforwardly determined solely by the
successor representation.

#### 4.1.2 Deep Reinforcement Learning

While classical reinforcement learning approaches typically represent
the value or Q functions explicitly as vectors and matrices, such
methods only work for relatively small and discrete state and action
spaces, and cannot easily scale to the extremely large state and action
spaces required for playing complex games as well as for complex
continuously valued action spaces such as in robotics, where simply
discretizing the space with a sufficiently fine grid will simply result
in too many states to handle. Therefore, to maintain scalability,
instead of explicitly representing the state and value functions, it
becomes necessary to approximate them with powerful function
approximators. While other approaches using linear features (
baird1995residual ; gordon1995stable ) , or nonlinear basis function
kernels ( doya2000reinforcement ) are possible, recent systems have
overwhelmingly used deep neural networks to approximate the value or Q
functions directly. To make this explicit, instead of representing the
value function @xmath as a vector, we instead represent it as a function
@xmath which takes a state @xmath and maps it to a scalar value. This
function is implemented by a deep neural network with parameters @xmath
. We can then learn these parameters using stochastic gradient descent
on a loss function which is a modified version of the Bellman recursion,

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

which is the squared residual between the value predicted for a state
@xmath by the value network, and the value predicted by the Bellman
recurrence relation. By using this approach, therefore, we utilize the
intrinsic generalization capabilities of deep neural networks to allow
us to estimate value or Q functions for an otherwise intractably large
space. This approach can be straightforwardly extended to Q-learning and
other Bellman based methods. Similarly, applying policy gradients with
deep neural networks is even simpler – we simply parametrise the policy
@xmath with a deep neural network with parameters @xmath and train it
directly using stochastic gradient descent on the policy gradient
objective ( schulman2015trust ; schulman2017proximal ) .

To get this approach working well in practice, however, requires quite a
number of tricks. For instance, the neural networks cannot be trained
simply through continuous interaction with the environment, as this
gives rise to correlated data which leads to overfitting and
catastrophic forgetting within the value network ( mnih2013playing ) .
Thus, instead, it is necessary to make the data fed into the network as
@xmath as possible by using a memory replay buffer, which stores all
experience the agent has encountered over its lifetime and then replays
it at random to be optimized according to Equation 4.8 ( mnih2013playing
) . Similarly, note that in the value network update equation (Equation
4.11 ), the parameters of the value network appears twice – once
computing the value of @xmath and again computing the values of @xmath .
It has been found empirically, that this leads to instabilities in the
optimization process which often destroy learning, since the
optimization process is effectively chasing a set of moving targets. To
resolve this problem, the value estimates of @xmath are often computed
using a ‘frozen’ value network which is not optimized directly, but is a
copy of the value network from some number of iterations past. The
frozen network is then updated to match the current value network every
given number of iterations ( mnih2015human ) . Another issue is that the
value estimates are often skewed extremely positive due to the max
operator in the objective interacting inaccurate value function
estimates. This can be ameliorated empirically by simply training two
(or many) value networks in parallel, and then choosing the smallest
value estimates out of all of them, a technique known as dueling value
networks ( wang2016dueling ) . For a thorough review of tricks and tips
for training deep reinforcement learning agents, we suggest
fujimoto2018addressing ; hessel2018rainbow .

Nevertheless, once all of these instabilities have been addressed, the
result is an extremely powerful and general learning technique which has
been demonstrated empirically to scale up to solve very challenging
tasks such as Atari games ( mnih2015human ; mnih2014neural ) and Go (
silver2016mastering ; silver2017mastering ) , Starcraft II (
vinyals2019grandmaster ) , and ultimately very challenging tasks in
robotics ( nagabandi_neural_2017 ; nagabandi2019deep ; chua_deep_2018 ;
williams2017model ) .

#### 4.1.3 Model-free vs Model-based

It is important to note that all the methods we have discussed so far
require samples from the true environmental dynamics to approximate the
expectation @xmath from the ultimate loss function (Equation 4.1.1 .
While these samples are easy to acquire in the case where interacting
with the environment is cheap and easy, such as when the environment is
a simulation such as a game or an OpenAI gym environment (
brockman2016openai ) , in many real world tasks this is not the case.
For instance, in robotics, interacting with the real environment is
often slow (the real robot has to actually move or do things in physical
space) and costly (this movement requires power and also induces wear
and tear on the robot. In extreme situations, bad policies may actually
result in the robot damaging itself). In this case, it is often better
if we can somehow eschew interacting with the real environment in favour
of a model of the real environment. Having a ‘world model’ (
ha_recurrent_2018 ) of the environment allows the agent to plan and test
different potential courses of action without having to sustain costly
and slow interactions with the real world. The utility of models does
not just extend to the environmental dynamics. It is often the case that
the actual reward function of the agent is unknown. This is not
generally true in reinforcement learning and control theory, which
typically have well specified rewards, but is often true in the case of
biological organisms encountering novel contingencies, where it is not
necessarily known a-priori if a situation is good or bad. Thus, we can
also learn a model of the reward function as well.

Mathematically, we can formalize this property of having a model of the
transition dynamics or reward functions by postulating additional
probability densities which the agent possesses @xmath and @xmath which
represent the agent’s model of the true environmental dynamics @xmath
and true reward function @xmath . Then, using importance sampling, we
can introduce these models into the previous loss function,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (4.12)
  -- -------- -------- -- --------

Here we see that the objective can be partitioned into three terms. The
first, reward maximization, is equivalent to the original control
objective except it utilizes the model environmental dynamics and reward
function instead of the true environmental dynamics and reward function.
The second term, ‘system identification’, encodes the KL divergence
between the true and modelled environmental dynamics, and is minimized
with respect to the parameters of the model of the dynamics. Optimizing
this term encourages the agent to learn an accurate dynamics model of
the world. The third term, ‘reward model identification’ encourages the
reward model to match the true reward function and optimizing this term
with respect to the parameters of the reward model minimizes the
difference between the model reward distribution and the true reward
distribution.

Interestingly, mathematically, the minimization of all of the three
parameters is over all three terms. This leads to, for instance, the
parameters of the reward and dynamics model to be optimized over the
reward maximization term, which effectively encourages the dynamics and
reward models to try to learn positively biased dynamics and reward
models which are encouraged to give out larger rewards. Conversely, the
divergence terms between true and modelled environmental dynamics and
reward function are also being optimized with respect to action, so the
optimisation process also encourages action to make the true dynamics
and true reward function correspond more closely to the modelled ones.
These interactions between the optimizations of the different terms
often have strange and deleterious effects on agent behaviour,
especially learning the dynamics to maximize the reward, which can give
agents a highly dysfunctional ‘optimism bias’ ( levine2018reinforcement
) . As such, in practice, this optimization is often split into three
separate and independent optimizations for each set of parameters
respectively,

  -- -------- -- --------
     @xmath      (4.13)
     @xmath      
     @xmath      (4.14)
  -- -------- -- --------

These three optimization processes correspond to maximizing the reward
(standard reinforcement learning), learning the environmental dynamics
model, and learning the reward function, respectively. In control
theory, the process of learning a dynamics model is called system
identification.

Given good reward and dynamics models, it is then possible to utilize
standard model-free reinforcement learning approaches to estimate Q and
value functions, or estimate policy gradients directly through simulated
samples of the dynamics model. With these it is then possible to learn
policies in the usual way without ever having to interact with the
environment (except to learn the models in the first place). This
approach, first introduced in the Dyna architecture ( sutton1991dyna )
has been studied in the literature for a long time, and is generally
more sample efficient than pure model-free reinforcement learning which
learns directly from environmental transitions, as learning dynamics
models of the world is typically faster than learning reward models,
since the prediction errors in the state of the world is a richer
informational signal than the reward prediction error since the reward
is usually scalar while the world-state is typically very high
dimensional.

Another approach, once you have a model is to skip the model-free
methods and move directly to planning using the model. In the simplest
case, this can be done by sampling different action trajectories,
simulating their consequences and rewards using the dynamics and reward
models, and then simply choosing the action trajectory with the best
estimated rewards. More advanced methods include the cross-entropy
method, which tries to fit a probabilistic (Gaussian) action
distribution to maximize rewards, and the path-integral method, which
fits a Boltzmann distribution of action trajectories over rewards (
kappen2012optimal ; rawlik2013probabilistic ; theodorou2010reinforcement
; williams2017model ; williams2018predictive ) . This kind of
model-based planning is often combined with Model-Predictive-Control,
which is simply where you re-plan fully at every time-step, and has been
used to reach state of the art performance on a wide variety of
reinforcement learning ( nagabandi2019deep ) and robotics tasks (
williams2016aggressive ; williams2017information ) , all while requiring
substantially fewer environmental interactions than model-free
reinforcement learning approaches.

#### 4.1.4 Exploration and Exploitation

An interesting question that arises in the control problem, wherever
there are any unknowns, either of the optimal value function and policy
or the environmental dynamics and reward function, is the
exploration-exploitation tradeoff ( cohen2007should ; dayan2008decision
; sutton1998introduction ; kaelbling1998planning ; mobbs2018foraging ;
berger2014exploration ) . This trade-off arises because in order to
obtain a more accurate estimate, it is usually necessary to explore new
regions of the state-space away from the locally optimal location.
However, by ignoring the locally optimum course of action, you incur an
opportunity cost equal to the difference between the (usually worse)
reward from the exploring compared to the local optimum. Conversely, by
only ever exploiting the local optimum and never exploring, if there are
actually better optima out there, which you have simply not found, you
incur a constant opportunity cost of the distance to the true optimum
every single time you exploit your local optimum. Thus, to find the
truly optimal behaviours, it is necessary to explore widely, and not
just be sucked into whatever the closest local optima you find. However,
exploration has an intrinsic cost associated with it, since assuming you
have a halfway decent local optimum, almost everything you explore will
be worse than that, and thus exploration must be kept to a minimum to
maximize return, even in the long run.

The exploration-exploitation tradeoff is also central to the behaviour
and performance of reinforcement learning agents, especially deep
reinforcement learning agents which cannot explicitly represent every
contingency in the state-space. Empirically, it has been found that
except in extremely simple tasks, or where the reward function is a
smooth gradient to the global optimum, some forms of exploration are
necessary to achieve good performance with deep reinforcement learning
techniques. A large number of heuristic exploration techniques have been
developed in the literature which work well for many tasks. Perhaps the
simplest of these is the @xmath -greedy approach (
sutton1998introduction ) , which simply takes the greedy action @xmath
amount of the time, and takes a random action @xmath percent of the
time, where @xmath is a small number – typically about @xmath . This
method essentially bakes in a certain degree of random exploration into
the method so that it will (eventually) explore all contingencies purely
due to its random actions. In many control tasks this method generates
sufficient exploration to enable good performance. More sophisticated
variants anneal the value of @xmath over time; working on the idea that
at the beginning when little is known you should explore more, and then
later when you already have a pretty good policy you should explore
less. Another approach in algorithms like Q learning is that instead of
simply taking the maximum value, take actions with a probability equal
to their softmaxed Q-values – @xmath where @xmath is a parameter which
controls the spread or entropy of this distribution. When @xmath is
large, then the distribution is highly peaked and tends towards the max.
When @xmath is small, then the distribution tends towards a uniform over
all action possibilities. This method is called Boltzmann exploration (
cesa2017boltzmann ) , since the action distribution is equivalent to the
Boltzmann distribution of statistical mechanics with the @xmath
parameter functioning as an inverse temperature.

Another approach, which we will explore in detail in the next section,
is to add an entropy maximization term to the objective (
levine2018reinforcement ) . Thus, instead of simply maximizing the
reward, the goal is to maximize the reward while keeping the entropy of
the policy as great as possible, and thus making action as random as
possible ² ² 2 This idea is similar to, but distinct from, ideas such as
upper-confidence-bound sampling which assign optimism bonuses to states
in proportion to the inverse degree to which they have been sampled. The
key difference is that maximum entropy approaches provide bonuses to
actions based on the overall entropy of the action distribution while
UCB algorithms provide bonuses to states based on their epistemic
uncertainty. While still a random kind of exploration, this performs
better than @xmath greedy approaches since it explicitly trades off
randomness and reward maximization in the objective, rather than as an
@xmath parameter that needs to be tuned by hand.

While all these exploration methods work well in practice in many
benchmark environments for deep reinforcement learning, they all
fundamentally utilize random exploration – i.e. actions are selected
randomly in order to drive exploration. It is important to note,
however, that this strategy is necessarily very inefficient, since even
with purely random actions, a random walk will explore slowly. Indeed,
these methods tend to perform rather poorly in more challenging large
environments where all useful contingencies cannot realistically be
explored with a random walk in a reasonable amount of time, and tend to
perform especially poorly in sparse reward environments, where rewards
are hard to obtain and often require a pretty good policy to even get
any reward at all ( tschantz2020reinforcement ) . A good example of such
an environment is many games, where you are only rewarded a 1 if you win
the game. However, to even win any games at all requires some skill at
playing ³ ³ 3 This is often addressed in practice by using reward
shaping, where typically either the agent designer or the environment
designer will create a ‘proxy’ reward function for the real one which is
less sparse. For instance, in a game like chess, instead of simply
rewarding winning or losing the game, the agent might get rewards for
taking pieces, or gaining positional advantage. While this approach
works very well in practice, it requires human intervention for every
task the agent tries to accomplish, and is often tricky to design a
proxy reward which successfully leads to the correct ultimate behaviour.
Ultimately, we want agents to be able to interact with the world fully
autonomously, which means that they should not need special
human-designed reward functions to handle any new situation, and so we
do not consider reward shaping further . To address the shortcomings of
random exploration, a significant amount of work has been done on
directed , or information-seeking, exploration. Here, exploratory
actions are not completely random but instead directed at some
exploratory goal – usually to accumulate information or to resolve
uncertainty about the world. This makes sense as in a stationary
environment there is little point in repeatedly exploring bad options
which are known to be bad. The key is to explore where there is
remaining resolvable uncertainty.

A number of different objectives, or ‘intrinsic measures’ (
oudeyer2009intrinsic ) have been proposed to achieve this. These include
prediction error minimization ( pathak2017curiosity ) , ensemble
divergence ( chua_deep_2018 ) , explicit information gain (
shyam_model-based_2019 ; tschantz2020reinforcement ; sun_planning_2011 )
, and empowerment ( klyubin2005empowerment ) . Typically such approaches
postulate a separate ‘exploration objective’ and then either operate in
two phases whereby first the model optimizes the exploration objective,
and then it switches to optimizing the greedy objective (
shyam_model-based_2019 ) , or alternatively, the exploration and greedy
objectives are added together to form a unified objective function which
is minimized throughout ( tschantz2020reinforcement ) . Such approaches
therefore encourage the agent to seek a balance between its exploratory
and reward-maximization imperatives. This has the theoretical advantage
of encouraging the agent to only explore regions of the state-space
which combine both a high expected reward and much resolvable
uncertainty, as opposed to simply resolving uncertainty for its own
sake. In the literature, most of these exploratory objectives are simply
postulated and argued for on intuitive grounds and then empirically
compared. However, with the exception of the entropy maximization
discussed above, which we shall see arises from explicitly considering
control as a variational inference problem, the mathematically
principled origins of these additional exploratory terms, especially
information-gain and empowerment terms remains mysterious. Chapter 5 is
dedicated to deriving the mathematical basis for such objectives.

#### 4.1.5 Control as Inference

Since we have been so interested in understanding brain function through
the lens of Bayesian (variational) inference in this thesis, a natural
question arises as to whether the control problem as discussed
previously can be cast in such an inference framework. It turns out that
this is indeed the case, and is quite straightforward to achieve.
Starting from ( attias2003planning ) and then developed by (
toussaint2006probabilistic ; todorov2008general ;
rawlik2013probabilistic ; kappen2012optimal ; theodorou2010generalized ;
levine2018reinforcement ) , a small line of the literature has worked on
investigating and developing the close connections between the control
problem and Bayesian inference.

While the control objective (Equation 4.1.1 is a probabilistic
objective, it is not yet an inference objective. There is nothing there
to be inferred. The key step is to define dummy variables @xmath which
are binary random variables which simply whether a given trajectory
timestep is optimal or not. @xmath if the timestep is optimal and @xmath
if it is not optimal. Given this dummy variable, the task of inferring
the optimal policy can be written simply as finding the distribution
@xmath . To begin inferring this distribution, it is first necessary to
make one more assumption about the dummy variables @xmath , in order to
operationalize the notion of optimality. We define @xmath such that the
probability of optimality is proportional to the exponentiated reward.
Intuitively, this can be seen as a mathematical trick allowing the
‘log-likelihood of optimality’ to be equal to the reward, thus allowing
us to cast reward maximization as a process of maximum likelihood
estimation.

One way we can find the crucial distribution is simply to directly
compute it via Bayes rule. First, we write out Bayes rule explicitly,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (4.15)
  -- -------- -------- -- --------

Where, in the final line, we assume that the action prior @xmath is
uniform. Now, looking at the two terms we have left, we can intuitively
think of the numerator @xmath as representing the probability of
optimality of all future states, given the current state and action.
However, we have another term for this – the cost-to-go, or the
Q-function. In effect, we obtain the Bellman recursive relationship
directly from Bayes rule. Secondly, the denominator @xmath clearly
corresponds to the value function.

We can from this directly derive recursive relationships among these
terms,

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

We can thus see, that due to our definition of optimality that @xmath ,
that to obtain the correspondence to the value and Q function requires
the log of the optimality probability. We thus have,

  -- -------- -- --------
     @xmath      
     @xmath      (4.17)
  -- -------- -- --------

Where, by taking the log of the integral and exponential, we effectively
have the log-softmax function instead of the max in the Bellman
equations. This corresponds to a ‘soft’ maximum instead of the hard
maximum used in the traditional Bellman recursion. However, other than
that, our new definitions of the value and Q function satisfy the
standard Bellman recursive relationship, and as such can be used to
derive all the traditional reinforcement learning algorithms such as
Q-learning, temporal difference learning, and SARSA (
sutton1996generalization ) .

Another approach is to use variational inference and attempt to
approximate the true posterior distribution given optimality @xmath with
a variational distribution @xmath . To make this approximation accurate,
we thus wish to minimize the divergence between the two distribution.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.18)
  -- -------- -------- -- --------

Where we only need to optimize the Evidence Lower Bound (ELBO) term
since @xmath is constant with respect to the variational density @xmath
. Crucially, we can then split up the ELBO term as follows,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.19)
  -- -------- -------- -- --------

If we then assume that the variational dynamics @xmath are equal to the
true environmental dynamics @xmath (the agent cannot change the dynamics
except through action) then the action divergence term disappears.
Additionally, if we use the fact, defined earlier, that @xmath , then we
obtain an objective which looks substantially more similar to
traditional reinforcement learning objectives except for an additional
action divergence term between the variational action distribution (the
policy) and a prior action distribution.

  -- -------- -------- -- --------
     @xmath   @xmath      (4.20)
  -- -------- -------- -- --------

If we further assume a uniform action prior, then the control as
inference objective reduces to,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (4.21)
  -- -------- -------- -- --------

Which is simply reward maximization while also simultaneously maximizing
the entropy of the policy @xmath . This objective has been utilized in a
number of recent works ( rawlik2013stochastic ;
haarnoja2017reinforcement ; haarnoja2018soft ; abdolmaleki2018maximum )
, and forms the basis of the soft-actor critic architecture (
haarnoja2018soft ) , which simply optimizes a relatively standard
actor-critic architecture on this objective. It has been found to reach
state-of-the-art performance for model-free reinforcement learning on a
wide range of challenging continuous control tasks ( hessel2018rainbow ;
haarnoja2018applications ) . Moreover, the simplicity and robustness of
this algorithm allow it to serve as an influential benchmark for the
field. This CAI objective can be straightforwardly optimized by taking
gradients of @xmath against whatever parameters there are. For instance,
optimizing the control-as-inference objective is identical to standard
policy gradients except that each reward the agent receives has @xmath
subtracted from it. This differentiates control as inference from
previous works ( o2017deep ) , which heuristically used an entropy
regulariser to prevent policy collapse, but computed the entropy
directly outside of the expression for the reward. The
control-as-inference objective is both simpler to implement and more
robust than this method.

Importantly, the control as inference objective, as it is a variational
bound, can be derived directly from, and as a lower bound on the
marginal likelihood of optimality @xmath . The derivation is
straightforward and goes as follows,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (4.22)
  -- -------- -------- -- --------

Under the assumption that the variational and generative dynamics are
the same. Overall, the control as inference approach demonstrates that
it is possible, even relatively straightforward, to derive a range of
reinforcement learning algorithms from a variational approach on an MDP
graphical model augmented with additional optimality variables. Doing so
results in the standard reward maximization objective plus a
regularisation term which tries to keep the learned policy @xmath as
close as possible to some action prior @xmath . If the action prior is
set to be uniform, then this regularising KL divergence simply reduces
to the maximization of the entropy of the action policy. This action
entropy maximization term functions as a powerful regulariser and
implicit exploratory drive, which aims to keep the policy as random as
possible while still maintaining performance. This term is especially
powerful and important for preventing policy collapse, a well-known
phenomenon in policy gradient and actor-critic methods (
fujimoto2018addressing ) , in which the probabilistic policy of an agent
typically collapses to some deterministic policy which may not even be
very good. Once it is in this state, it is very difficult for the agent
to continue to explore to find better policies, since it has minimal
probability of taking other actions. The theoretical benefits of this
action entropy term have been demonstrated empirically in the
literature, where ‘soft’ control as inference approaches have generally
shown to outperform classical reinforcement approaches as well as being
more stable and easy to train. However, it is important to note that
although the action entropy term is effective at maintaining
exploration, it only encourages random exploration, or random walk
behaviour in action-space. To solve sparse reward tasks in a reasonable
amount of time, it is possible that a more intelligent, directed
exploration strategy is needed, which focuses explicitly on minimizing
resolvable uncertainty about the world. Such information-seeking
exploration objectives are a major focus and benefit of active
inference, and understanding their mathematical nature and origins is
the major task of Chapter 5 of this thesis.

### 4.2 Deep Active Inference

Active inference and reinforcement learning both purport to solve the
same fundamental problem – that of adaptive action selection to maximize
some notion of rewards or desires, given uncertainty about the world and
about the optimal policy to take. While active inference arises from the
paradigm of variational Bayesian inference with posterior policy
distributions and complex generative world models ( friston_active_2015
; friston2017process ) , reinforcement learning arises primarily from
the Bellman equation and the recursive properties of optimality (
sutton2018reinforcement ; kaelbling1996reinforcement ) , and utilizes
constructs such as value functions and policy networks to learn adaptive
behaviour even on challenging and complex control tasks.

Despite the very different origins of the two fields, since they are
fundamentally trying to solve the same problem, it seems likely that
there is much each field can learn from the other, since they both
illuminate difference facets of the same reality. Specifically, the
discrete-state-space active inference models introduced previously, in
Chapter 2, suffer from many limitations of scale. Specifically, they
represent the core distributions as discrete categorical variables,
which require a relatively small, discrete and known state-space to
function. Moreover, active inference models typically assume knowledge
of the true generative process (i.e. the likelihood and prior matrices
(A and B)), which often cannot simply be assumed in more realistic
control tasks. ⁴ ⁴ 4 There is some work empirically investigating
learning the A matrix using dirichlet hyperpriors over its values (
schwartenbeck_computational_2019 ) , and the rules for learning the B
matrix are also straightforward. However, most of the active inference
literature eschews these methods in favour of hand-designed likelihood
and transition matrices ( friston_active_2015 ; friston2012active ;
parr2017uncertainty ; friston_deep_2018 ) , and large scale studies of
the effectiveness of these learning algorithms has not been ascertained
at scale. . An additional, and serious obstacle to the scalability of
classical active inference methods is the computation of the policy
prior, which is often taken to be the softmax of the expected free
energy over all policies. This is typically computed explicitly and
exactly in the literature ( da2020active ; friston_active_2015 ) , and
requires an explicit enumeration of every single policy and its
associated trajectory for which the expected free energy can then be
computed. In computational complexity terms, this results in exponential
complexity in both the time horizon and the size of the discrete
state-space, which clearly poses a significant computational scaling
issue even for relatively small state-spaces and time-horizons. It is
largely this obstacle which has prevented the application of truly large
scale active inference models and limited most studies to toy tasks.
There have been several methods in the literature proposed to somewhat
ameliorate the computational expense of the expected free energy,
notably by pruning away policies which have an a-priori likelihood less
than some threshold ( friston2020sophisticated ) . However, although
such approaches enable scaling to slightly larger tasks, they do not
attack the fundamentally exponential complexity of the algorithm, rather
they simply reduce the exponential coefficient.

Indeed, all the scaling limitations of active inference are almost
identical to those of tabular reinforcement learning with explicitly
represented state and action value functions. In reinforcement learning,
this scaling barrier was removed through the use of deep neural networks
as flexible function approximators, to learn, via gradient descent, to
approximate the required constructs by training on a dataset of
environmental interactions ( sutton2018reinforcement ) . We propose a
similar approach may prove equally useful for scaling up active
inference models. In the next two sections we present two studies which
attempt to scale up active inference by using deep neural networks to
flexibly approximate key densities in the active inference equation, as
well as utilize methods from deep reinforcement learning to approximate
the evaluation of the expected free energy over policies, which is
fundamental to action selection in active inference.

In the first study, which is based on the paper ( millidge_deep_2019 ) ,
we are heavily inspired by model-free deep reinforcement learning
algorithms. We represent the transition dynamics, observation
likelihood, and variational action distribution as neural networks
trained to jointly minimize the variational free energy. Similarly, we
utilize a bootstrappped value network to approximate the expected-free
energy value function. We show that this model-free deep active
inference approach can scale to perform equivalently, if not sometimes
superiorly to contemporary deep reinforcement learning approaches.

In the second study, which is based on the paper (
tschantz2020reinforcement ) , which was a joint collaboration with
Alexander Tschantz at the University of Sussex, we utilize a scheme
inspired by model-based active inference for action selection.
Specifically, we use a model-based iterative planner to estimate the
variational action distribution, and estimate the expected free energy
value function based on simulated rollouts within the planner. We focus
more heavily on the exploratory nature of the behaviour furnished by the
expected free energy (and free energy of the expected future – to be
discussed in chapter 4) objectives and demonstrate that optimizing these
objectives leads to empirically better performance, especially on
sparse-reward tasks which require substantial amounts of exploration.

#### 4.2.1 Model-Free: Active Inference as Variational Policy Gradients

##### Derivation

The fundamental idea of active inference is to reformulate the control
problem as a variational inference one, and then use variational methods
to solve it. Specifically, we wish to recast the problem of control into
one of inferring the optimal state and action distribution. The formal
setup we use to describe this is a discrete-time Partially Observed
Markov Decision Process (POMDP) model. In this model, the agent receives
observations @xmath , which are generated by some hidden environmental
state @xmath which satisfies the Markov property. The observations
themselves do not necessarily have to be Markov. The agent can then emit
actions @xmath which can alter the latent state of the environment and
thus generate new observations. We assume that the agent maintains a
desire distribution @xmath over observations such that it most desires
to be experiencing high rewards. Observations, states, and actions are
optimized over full discrete-time trajectories from times @xmath to a
given time horizon @xmath . Given these assumptions, we can write down a
factorization of the environmental POMDP as follows,

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

We then assume that the agent knows the basic POMDP structure and
factorisation properties of the agent (although not necessarily any
details about the precise distributions involved), and maintains an
additional generative distribution over actions which specify its ideal
action generating process. We can thus write the agent’s generative
model as,

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

From now on, since the true environmental generative process is never
known, we do not refer to it, only the generative model of the agent.
Thus, for notational convenience, we denote @xmath simply as @xmath .
The inference problem we wish to solve, is to infer the optimal action
and state distribution given observations. That is, the key idea in
active inference is to infer the distribution,

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

Note that unlike in control as inference approaches which encode reward
directly into the inference process by performing inference on a
graphical model augmented with additional optimality nodes, here we
encode rewards or goals into the model through the action prior, as we
shall see later. In most situations, a direct computation of this
posterior distribution is intractable, so we resort to a variational
approximation. We define the variational distribution @xmath which is
under the control of the agent, and then try to minimize the divergence
between the true and approximate posteriors,

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

This divergence is still intractable since it contains the intractable
posterior, however we can derive a computable bound on this divergence
known as the variational free energy, which we can then optimize,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.27)
  -- -------- -------- -- --------

Now, if we study the expression for the variational free energy @xmath
in some detail, we can see that it can be split up into three
interpretable terms,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.28)
  -- -------- -------- -- --------

If we apply the Markov assumption in the generative model, and assume
that the variational posterior factorises across time such that @xmath ,
then we find that the previous derivation (Equation 4.2.1 ) in terms of
full trajectories simplifies considerably into a sum of individual
timesteps. Thus, we can write,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.29)
  -- -------- -------- -- --------

Examining these terms, we can see several familiar objectives from the
machine learning literature. For instance, the first reconstruction
error term is simply just the log-likelihood of observations expected
under the trajectory belief distribution. This term is commonly
optimized in all sorts of machine learning tasks, of especial interest
here is its use as part of the objective of the variational autoencoder
( kingma_auto-encoding_2013 ) If the likelihood term @xmath can be
thought of as the decoder of a variational autoencoder, then conversely
the @xmath term can be thought of as the encoder. Similarly, the state
divergence term is often used as a regulariser or a method of training a
transition model in model-based reinforcement learning. Indeed, we can
see the transition dynamics term @xmath as encoding a direct model of
the transition dynamics. Thus, it is straightforward to parameterise
these distributions using deep neural networks. The @xmath and @xmath
distributions are parametrised by the encoder and decoder of a
variational autoencoder respectively, which can be trained through a
reconstruction loss on the environmental observations. The @xmath
distribution can be encoded as a deep neural network trained on the
transition dynamics of the environment. With this, we turn to the two
terms in the action divergence. The first, @xmath , can be thought of as
a parametrized policy network, of the kind used in policy gradients or
actor critic methods in reinforcement learning. Specifically, it can be
thought of as a simple mapping between a state and the correct action to
output from this state. So far the inference procedure we have written
has no notion of rewards or goals. To achieve adaptive reward-sensitive
action inference, this must be added somewhere. Following common
practice in the active inference literature, we encode goals or rewards
into the the action prior by assuming that it is equal to the softmax of
the expected free energies of future trajectories @xmath where @xmath is
the expected free energy functional from active inference, @xmath is a
precision parameter which controls the entropy or ‘temperature’ of the
softmax, and @xmath denotes the softmax function. Intuitively, we can
consider this agent trying to optimize the sum over time of its expected
free energy (and thus the extrinsic and intrinsic value components of
the EFE), and then selecting actions with a probability proportional to
the relative value of each choice. Such an action prior effectively
implements a Boltzmann action distribution, which has been empirically
studied in human and animal choice behaviours ( daw2006cortical ) . The
action divergence term them simply tries to minimize the divergence
between the variational action policy @xmath parametrised by a deep
neural network, and the ‘ideal’ action distribution @xmath .

The key difficulty, then, is the computation of the softmaxed expected
free energy, as this is a path integral of the expected free energy of a
trajectory into the future. Unlike in tabular active inference
approaches, we cannot simply enumerate all possible future trajectories
and evaluate them. Instead, we make use of a trick from deep
reinforcement learning, called bootstrapping, which takes advantage of
the recursive nature of the Bellman equation. Here, we first note that
the expected free energy, since it can be simply written as a path
integral through time, obeys a similar recursive relationship,

  -- -------- -- --------
     @xmath      
     @xmath      (4.30)
  -- -------- -- --------

From here, we can expand the expected free energy term using its
standard definition ( friston_active_2015 ) to obtain,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (4.31)
  -- -------- -------- -- --------

Here, we see that the expected free energy can be (approximately)
decomposed into a reward maximization term and also an information gain
term to be maximized. This information gain term, here between posterior
and prior expectations over states, can be seen as an
exploration-inducing ‘intrinsic reward’ inherent to active inference
agents, which furnishes them with greater directed exploration
capabilities than baseline reinforcement learning agents which lack this
additional term and only focus on greedy reward maximization.

Putting this all together, we realize that we can express the path
integral of the expected free energy recursively as,

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

To efficiently approximate this, we can then utilize the bootstrapping
method used in deep Q learning. Here we explicitly train a neural
network to predict the expected free energy value function @xmath .
While it may seem like this requires explicit computation of the path
integral, to produce correct targets for the network, in fact it does
not due to the recursive nature of the expected free energy equation.
Let’s denote the expected free energy value function predicted by the
network as @xmath which takes as inputs only the current state and
observation and uses it to predict the full path integral. The value
network has parameters @xmath . We can then approximate the true EFE
recursive relationship,

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

where we have replaced the recursive computation of the future expected
free energies with the prediction from the value network, which thus
defines the value ‘estimate’ @xmath . Then, to train the value network,
we simply minimize the squared difference between the estimated EFE and
the predicted EFE,

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

This method is referred to as bootstrapping because the real expected
free energy path integral is never computed. Instead the targets are
also constructed from the value network which are then used to train the
value network. While this circular relationship may make it unintuitive
that this method can work, we can understand why it does work
empirically through the fact that at each step of the optimization, some
local information about the EFE is fed into the network through the
(true) computation of @xmath . Thus over time, this local information
builds up and allows the network to converge to the correct value
function. Importantly, in Q learning, there are also several tricks that
have been found to be necessary to ensure a stable convergence.
Principally, using the same value network to update both the targets and
the predictions simultaneously does lead to instabilities and possible
divergence. To ameliorate this, we use a ‘frozen’ copy of the value
network to compute the targets @xmath and hold it fixed while the true
value network is updated for @xmath steps. After the @xmath steps we
replace the frozen value network with another frozen copy of the newly
trained value network. In this way, the targets do not change rapidly
over the course of optimization but nevertheless will slowly converge
towards the correct targets. Empirically, as in Q-learning, we found the
use of a target network a necessity for stable learning of the value
function. Given a trained value network which can output a prediction
for the expected free energy value function, we can then compute the
action prior @xmath for any given state and observation. We have thus
made concrete every single one of the distributions in the variational
free energy functional @xmath , so that it can be explicitly evaluated.
Once it can be evaluated, its gradients can be computed through
automatic differentiation techniques, and we can train all the
parameters of the agent jointly through stochastic gradient descent.

##### Model

At this point, to avoid losing sight of the wider picture, it is
worthwhile to take a step back and look at the model as a whole. We
propose to represent and train the variational posterior @xmath and the
observation likelihood @xmath as a variational autoencoder. We
additionally represent the dynamics distribution @xmath as a deep neural
network transition model. We represent the variational action posterior
@xmath as a deep neural network ‘policy network’ and can estimate the
action prior @xmath using the softmaxed predictions of an expected free
energy value network @xmath . All of these terms are combined according
to Equation 4.2.1 to compute the total variational free energy which
forms the unified loss function of the model. Then, all the parameters
of each network, except for the value network, are optimized according
to a gradient descent on this total loss. The value network, by
contrast, is trained using its own separate bootstrapping loss.

The full deep active inference algorithm is presented below,

Initialize Observation Networks @xmath with parameters @xmath .
Initialize State Transition Network @xmath with parameters @xmath
Initialize policy network @xmath with parameters @xmath Initialize
bootstrapped EFE-network @xmath with parameters @xmath

Receive prior state @xmath

Take prior action @xmath

Receive initial observation @xmath

Receive initial reward @xmath

while @xmath do

@xmath @xmath @xmath Receive observation @xmath Receive reward @xmath
@xmath @xmath @xmath @xmath @xmath @xmath

end while

Algorithm 1 Deep Active Inference

While all the derivations are straightforwardly presented for a full
POMDP model, in our experiments we only utilized simple MDP examples
with observable state. This was because the main difficulty and
contribution of this work is the action selection mechanism, and that
the extension to POMDPs is straightforward by just training a separate
variational autoencoder to estimate the latent state given the
observations. Because of this the two terms @xmath and @xmath are
superfluous and not computed. This leaves the transition model @xmath ,
the policy network @xmath and the value network @xmath . The transition
model is necessary to compute the exploratory information gain term in
the expected free energy.

We represented the policy network, transition model, and value network
each as two-layer fully connected neural networks with relu activation
functions and a hidden size of 200 neurons. All networks, except the
value network, were optimized by minimizing jointly the variational free
energy, through stochastic gradient descent with the ADAM optimizer and
a learning rate of 1e-4. The value network was trained on the
bootstrapping objective (Equation 4.34 ) with the same learning rate and
optimizer. A memory buffer was used to store all the agent’s experience
which was replayed to the agent for training at random in minibatches of
size 64. No preprocessing was done on the input data for any of the
experiments. The models were implemented and automatic differentiation
was performed in the Flux.jl machine learning framework ⁵ ⁵ 5 The code
to reproduce all experiments can be found at:
https://github.com/BerenMillidge/DeepActiveInference. . We used a target
network to stabilize the learning of the value network. The target
network was copied from the value network every 50 episodes. Each
episode consisted of a full iteration through the replay buffer.
Additionally, as is common in reinforcement learning tasks, we utilized
a temporal discount on the reward. Our temporal discount factor was 0.99
for all models.

##### Results

We compared the performance of the active inference agent to two strong
model-free reinforcement learning baselines. Deep-Q learning (
mnih2013playing ; mnih2015human ) , and an actor-critic (
mnih2016asynchronous ) architecture. Deep Q learning parametrises the Q
function using a deep neural network with a similar bootstrapping
objective to the EFE value function that we computed earlier, except
only using reward. For the Q-learning agent we utilized the same
hyperparameters and value network size that we used for the active
inference agent, to enable a fair comparison. Moreover, we implemented
the Q-learning agent with Boltzmann exploration ( cesa2017boltzmann ) ,
which is very similar to the softmax function used to compute the action
prior.

We also compared the deep active inference agent to an actor-critic
architecture. Unlike a Q-learning agent which computes actions directly
by maxing over the Q-function, the actor-critic architecture maintains a
separate ‘actor’ or policy network which is trained on a policy gradient
objective based on value function estimates learnt by the ‘critic’ – a
value function estimator trained through bootstrapping. The policy
network and value network of the actor-critic architectures were trained
using the same hyperparameters and architecture as the active inference
agent, to enable a fair comparison of the methods.

We compared the performance of the active inference agent on three
continuous control environments from the OpenAI Gym ( brockman2016openai
) . These were CartPole, Acrobot, and LunarLander. The cartpole
environment is simple and requires the agent to balance a pole atop a
cart by pushing the cart either to the left or to the right. The
state-space of the cartpole environment is a four dimensional vector,
comprising the cart position angle and velocity, as well as the angle
and velocity of the pole. A reward of +1 is given for each timestep the
episode does not end up to 200 steps. The episode will end early if the
cart is more than 2.4 units from the centre (the cart has left the
screen), or else the pole angle is more than 15 degrees from vertical
(the pole has fallen down). The acrobot environment requires the agent
to learn to swing up and balance a triple jointed pendulum. It has a
state-space of 6 dimensions which represent the angles and velocities of
the joints. The action space is a three dimensional vector corresponding
to the force the agent wishes to exert on each joint. The reward
schedule is -1 for every timestep the pendulum is above the horizontal,
and 0 if it is above horizontal. The acrobot is a challenging task for
exploration, since purely random actions are very unlikely to lead to
any reward. The lunarlander environment requires the agent to learn to
land a simulated spacecraft on a surface within a target region in a
Newtonian physics environment. It has a 8-dimensional state-space and a
four-dimensional action space, with actions corresponding to fire left
engine, fire right engine, fire upwards engine, and extend docking legs.
The agent receives a reward of 100 for landing on the launchpad (located
at (0,0)), and a -0.3 reward for every time step the rocket’s engines
are firing. It does not receive a penalty for simply doing nothing.

We compare the performance of the active inference, Q-learning, and
actor-critic agent, in terms of pure reward obtained in each environment
⁶ ⁶ 6 Note this comparison based purely on reward actually penalizes the
active inference agent to some degree, since it does not simply optimize
the reward, but also by satisfying the epistemic drives furnished by the
EFE objective function . We ran for 15000 episodes over 20 random seeds
for each agent and plotted mean rewards obtained below,

Since the active inference agent possesses several distinct features
beyond the standard actor critic architectures, we performed an ablation
study to understand whence its boost in performance arose.

We see that the key factor in the superior performance of the active
inference agent is the additional action entropy term that is
additionally optimized. This provides additional empirical confirmation
to the success of control-as-inference approaches in the reinforcement
literature which similarly utilize such an entropy regularisation term.
Perhaps surprisingly, we found little effect of the epistemic terms in
the EFE on total performance. We hypothesise that this was for two
reasons. Firstly, the tasks that were tested possessed a relatively
dense reward structure, sufficient to be learned by standard
reinforcement learning agents utilizing only random exploration
strategies, and thus that more advanced and powerful exploration
strategies are likely unnecessary for such tasks. Secondly, the
epistemic action term was the information gain between the prior and
posterior states, which is effectively a measure of the predictive
success of the transition model. Importantly, we found that the
transition model very rapidly converged during training, much faster
than the policy or value network, and thus that throughout most of the
training period, the exploration term was thus negligible.

##### Interim Discussion

In this section, we have demonstrated how active inference approaches
can be straightforwardly scaled up by parametrizing the likelihood,
inference, and transition distributions with deep neural networks, and
then additionally approximating the path integral of the expected free
energy with an amortized value network. This is possible because the
expected free energy satisfies a similar Bellman-like recursion to the
reward in reinforcement learning, because it is factorizable across time
into a sum of independent time-steps. Importantly, we have demonstrated
that by taking this approach, our active inference agent can handle
complex machine learning benchmark tasks just as well as several core
deep reinforcement learning approaches, thus rendering it significantly
more scalable than previous efforts in the literature which have
generally been restricted to small, discrete, and straightforwardly
enumerable state spaces.

Moreover, we have shown that our algorithm is competitive, and in some
cases superior, to standard baseline reinforcement learning agents on a
suite of reinforcement learning benchmark tasks from OpenAI Gym (
brockman2016openai ) . While our active inference agent performed worse
than direct policy gradients on the Lunar-Lander task, we believe this
is due to the inaccuracy of the expected-free energy-value-function
estimation network, since the policy gradient method used direct and
unbiased monte-carlo samples of the reward rather than a bootstrapping
estimator. Since the performance of Active Inference, at least in the
current incarnation, is sensitive to the successful training of the
EFE-network, we believe that improvements here could substantially aid
performance. Moreover, it is also possible to forego or curtail the use
of the bootstrapping estimator and use the generative model to directly
estimate future states and the expected-free energy thereof, at the
expense of greater computational cost. We take this approach of using
the transition model to generate sample rollouts and using these to
compute a Monte-Carlo estimate of the EFE path integral in the next
section, where these estimates are used to inform model-predictive
planning.

An additional advantage of our approach is that due to having the
transition model, it is possible to predict future trajectories and
rewards N steps into the future instead of just the next time-step.
These trajectories can then be sampled from and used to reduce the
variance of the bootstrapping estimator, which should work as long as
the transition model is accurate. This @xmath could perhaps even be
adaptively updated given the current accuracy of the transition model
and the variance of the gradient updates. This is a way of controlling
the bias-variance trade-off in the estimator, since the future samples
should reduce bias while increasing the variance of the estimate, and
also the computational cost for each update.

Another important parameter in active inference is the precision (
feldman2010attention ; kanai2015cerebral ) , which in the
discrete-state-space paradigm corresponds to the inverse temperature
parameter in the softmax and so controls the stochasticity of action
selection ⁷ ⁷ 7 In the continuous predictive coding paradigm the
precision modulates the ‘importance’ of the prediction errors. . In all
simulations reported above we used a fixed precision of 1. However, in
the discrete state-space case, the precision is often explicitly
optimized against the variational free energy, and the same can be done
in our deep active inference algorithm. In fact, the derivatives of the
precision parameter can be computed automatically using automatic
differentiation. Determining the impact of precision optimization on the
performance of these algorithms is a potentially worthwhile avenue for
future work.

While we did not find that using the epistemic reward helped improve
performance on our benchmarks, this could be due to the simplicity of
the tasks we were trying to solve, for which random exploration is
sufficient. In the next section, we demonstrate that the epistemic
affordances engendered by the use of the EFE value function prove
instrumental in attaining high performance in sparse-reward tasks.

The entropy regularization term which emerges directly from the
mathematical formulation of active inference proved to be extremely
important, and was often the factor causing the superior performance of
our active inference agent to the reinforcement learning baselines. This
entropy term is interesting, since it parallels similar developments in
reinforcement learning, which have also found that adding an entropy
term to the standard sum of discounted returns objective improves
performance, policy stability and generalizability (
haarnoja2017reinforcement ; haarnoja2018acquiring ) . This is of even
more interest given that these algorithms can be derived from a similar
variational framework which also casts control as inference (
levine2018reinforcement ) . Later (in Chapter 5), we discuss in
significant detail how such paradigms relate to active inference.
Additionally, many of the differences between active inference and the
standard policy gradients algorithm – such as the expectation over the
action, and the entropy regularization term – have been independently
proposed to improve policy gradient and actor critic methods (
fujimoto2018addressing ) . The fact that these improvements fall
naturally out of the active inference framework could suggest that there
is deeper significance to the probabilistic inference formulation
espoused by active inference. The other key difference between policy
gradients and active inference is the optimization of the policy
probabilities versus the log policy probabilities, and multiplying by
the log of the probabilities of the estimated values, rather than the
estimated values directly. It is currently unclear precisely how
important these differences are to the performance of the algorithm, and
their effect on the numerical stability or conditioning of the
respective algorithms, and this is also an important avenue for future
research. However, the comparable performance of active inference to
actor-critic and policy gradient approaches in our results suggest that
the effect of these differences may be minor.

#### 4.2.2 Model-based: Reinforcement Learning through Active Inference

While the last section focused on scaling active inference using
model-free reinforcement learning methods, here we focus on scaling
active inference in a way inspired by model-based reinforcement learning
methods. Model-based reinforcement learning is perhaps a better fit for
the central ideas in the classical active inference. Tabular active
inference, after all, is a model-based algorithm which explicitly
evaluates and optimizes future plans using explicit models of the
environmental dynamics. Moreover, as tabular active inference explicitly
replans at every time-step, it can be considered to be a
model-predictive control algorithm. In fact, the explicit enumeration
and evaluation of every possible policy can perhaps best be thought of
as a truly exhaustive planning algorithm.

Similar to our previous approach described earlier, we propose to
develop deep active inference methods which utilize deep neural networks
to parametrize key distributions from the active inference framework.
Specifically, we maintain deep neural network representations of the
observation likelihood distribution @xmath as well as the transition
model which parametrises the dynamics model of the environment @xmath .
The major difference is how the action policy @xmath is handled. In the
previous model-free approach, this distribution was represented as an
independent policy neural network which was trained against the action
prior which represented the softmax of the expected free energy value
function in an actor-critic like fashion. Here, we treat the action
posterior as the output of a model-based planning algorithm.

Specifically, and quite elegantly, we can show that under certain
conditions of the generative model of the future, that we can derive the
optimal plan as a softmax over the expected free energy in the future,
(which was merely assumed to be the action prior in the model-free
case). Moreover, we then show that this path integral can be
approximated by monte-carlo sampling in the form of a model-based
planning algorithm which samples and evaluates given potential future
trajectories using the transition and reward models possessed by the
agent.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
                          (4.35)
  -- -------- -------- -- --------

It is important to note that here we do not use the expected free energy
as our objective, unlike in standard active inference. Instead, we use
the recently introduced objective: the free energy of the expected
future (FEEF). This objective maintains the exploratory information gain
terms of the traditional expected free energy while possessing a clear
mathematical origin with strong intuitive grounding. Specifically, the
FEEF can be defined as,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (4.36)
  -- -------- -------- -- --------

Where we can see that the FEEF can be split into approximately three
terms – the likelihood divergence term which measures how much the
expected observations diverge from the desired observations, and which
effectively encodes reward or utility seeking behaviour, and an
information gain term to be maximized which induces exploratory,
uncertainty reducing behaviour. Since the FEEF objective includes both
latent states @xmath and parameters @xmath , we actually obtain two
separate information gain terms, one for the states and one for the
parameters. The core finding and argument of this part of the chapter,
and an example of what the theory of active inference can bring to
contemporary deep reinforcement learning, is that the exploratory
information-seeking terms furnished by active inference objectives such
as the expected free energy, or free energy of the expected future, by
inducing purposeful and goal-directed exploratory behaviour, they can
outperform traditional random exploration on a number of challenging
reinforcement learning tasks, and their advantages become especially
apparent in the case of sparse rewards where random exploration is often
simply insufficient to find any good solutions in a reasonable time.
Moreover, recent work in the literature, which utilizes exploratory
objectives, but in a first exploratory, and then exploitatory phase, we
argue that it is necessary to combine the two objectives to be jointly
optimized. In this way the agent is furnished with a desire for goal
directed exploration . It is not rewarded simply for reducing any
uncertainty, but only uncertainty that also exists in rewarding regions
in the state-space. In this way, agents explore precisely only as much
as needed, thus providing a step towards a practical solution to the
exploration-exploitation tradeoff.

##### Model

As in previous work, we extended active inference by using deep neural
networks to parametrize key densities. Our model utilized a neural
network transition model to model the distribution @xmath . Since the
FEEF objective requires the evaluation of an information gain term over
the parameters (denoted @xmath ) of the transition model, we maintained
an approximate distribution over the parameters @xmath using an ensemble
of transition models with independently initialized parameters, and
trained on different batches from the replay buffer. This ensemble
approach has been found to be widely useful in model-based reinforcement
learning and to offer a superior representation of the true posterior
over the parameters than competing methods such as Bayesian neural
networks. Importantly, utilizing an explicit ensemble of transition
models allows the estimated posterior over the parameters to be
multimodal, as opposed to the unimodal Gaussian assumption implicit in
the Bayesian neural networks approach ( tran2018Bayesian ;
gal2016improving ) . Moreover, an ensemble of models has been found
empirically to help avoid overfitting in low-data regimes, which are
also when the advantages of model-based reinforcement learning are most
apparent. Each element of the transition model ensemble was implemented
as an independent neural network with two hidden layers with 400 neurons
each. The networks used the swish activation function. The transition
networks predicted the difference in the next state (
shyam_model-based_2019 ) instead of the next state, as this has been
found to help capture environmental dynamics more accurately in
practice. Since we are evaluating future simulated rollouts, we cannot
simply rely on environmentally provided ‘true rewards’. While many
methods in the literature assume the existence of a known reward
function which can be queried even for counterfactual or simulated
trajectories ( chua_deep_2018 ; hafner2018learning ) , we do not, as
such a reward oracle is unrealistic in many if not most situations.
Instead, we learnt a reward model based upon previous interactions with
the real environment, and then used the reward model to score proposed
trajectories. The reward model was parametrised by a two layer
multi-layer perceptron network with 400 units in the hidden layer and a
relu activation function. The reward model was trained on a mean-square
error loss between actually observed rewards for a given state, and the
reward predicted by the reward model. Importantly, the FEEF objective
defines the extrinsic reward to be the KL divergence between the
observation likelihood and the desired observation distribution. Since
our model was situated purely in an MDP setting with fully observed
state, the only observation was the reward, and thus the reward model
doubled as the likelihood model. We set the desired reward observations
to be a Gaussian distribution with a variance of 1 centred at the
maximum possible reward for the environment. Since we interpret the
predictions of the reward model as representing the mean of a Gaussian
distribution, we can analytically calculate the KL divergence term. This
allowed us to straightforwardly compute and optimize the reward
maximization part of the FEEF objective. Similarly, to evaluate the
information gain terms of the FEEF objective, we can rewrite it in a
more tractable way,

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (4.37)
  -- -------- -- --------

In effect, the parameter information gain term decomposes into an
entropy of the average state minus the average of the entropy. The
average of the entropy can be computed semi-analytically since each
transition model ensemble models a Gaussian distribution, which has a
known and analytically calculable entropy. Then, the average can simply
be performed by directly averaging the entropies of each member of the
ensemble together. The entropy of the average term is more difficult,
since the average of many different Gaussian distributions is not
necessarily Gaussian. As such we approximate it with a nearest neighbour
entropy approximation ( mirchev_approximate_2018 ) which we found worked
well in practice.

To train the model, we optimized the reward and transition models on
data taken from a replay buffer. They were trained with stochastic
gradient descent on their respective loss functions using a negative log
likelihood loss. We cold-started the training of the agent at the end of
every episode, as we found that this led to more consistent behaviour
and performance. We initialized each episode with a dataset @xmath taken
from an agent with a random policy, to ensure some degree of transition
and reward model accuracy before beginning with the model-based
planning.

To compute actions we used a model-based planner (CEM) (
rubinstein1997optimization ) within the model-predictive control
paradigm. The CEM planning algorithm generates and evaluates the reward
of a large number of action trajectories, then takes the mean and
variance of some elite set of actions (usually the top 10) of the
actions and restarts the evaluation using actions sampled from a
Gaussian distribution with this mean and variance. For every timestep, a
30 step planning horizon was used resulting in a 30-step action plan, of
which the first action was executed. In accordance with model-predictive
control, we replan at each step. For the CEM algorithm, we used 700
candidate action sequences to be evaluated in each iteration, for 7
iterations. To train the transition and reward models we used the ADAM
optimizer with a learning rate of 1e-4 and trained for 100 epochs.

Input: Planning horizon @xmath | Optimisation iterations @xmath | Number
of candidate policies @xmath | Current state @xmath | Likelihood @xmath
| Transition distribution @xmath | Parameter distribution @xmath |
Global prior @xmath Initialize factorized belief over action sequences
@xmath . for @xmath do

Sample @xmath candidate policies from @xmath for @xmath do

@xmath @xmath for @xmath do

@xmath @xmath @xmath

end for

end for

@xmath

end for

return @xmath

Algorithm 2 Inference of @xmath

##### Results

We tested the performance of our algorithm against strong model-free and
model-based baselines on a number of challenging control tasks ⁸ ⁸ 8
Different tasks are utilized in this section compared to the previous
one because here we are dealing with continuous actions while in the
previous section we dealt only with learning discrete-action
controllers. . We utilized first the mountain-car task from OpenAI Gym,
which requires the agent to steer a car on a 1D line to a goal. This
task is difficult because the agent must first move the car away from
the goal up a hill, to build up momentum to be able to get over the
larger hill. This poses a difficult exploration problem which purely
random exploration agents struggle to solve. By contrast, our agent can
solve this task instantaneously, within only a single episode, due to
its goal directed exploration.

Since the mountain car environment possesses only a two dimensional
state space, we can explicitly plot and compare the degree of the space
covered by the active inference agent vs the greedy reward maximizing
reinforcement learning agent.

We thus see that the exploratory drives inherent in the active inference
agent propel it to explore a significantly larger fraction of the
state-space than the reward maximizing agent, and it is this exploration
which allows it to stumble upon the goal and thus rapidly learn to solve
the task. By contrast, the random exploration reward-maximizing agent is
unable to escape the local minimum at its start location by a purely
random walk, since to do so requires too many correct moves in a
sequence than can be generated within a reasonable time. We also tested
our agent on sparse reward versions of two challenging control tasks. In
the first task, Cup Catch, the agent must actuate a cup to catch a ball
thrown at it. The agent receives a reward of 1 if it catches the ball,
and a reward of 0 if it does not. Similarly, in the Half-Cheetah
environment, the agent takes control of the limbs of a running planar
cheetah in a semi-realistic physics simulation. Its goal is to maximize
the velocity at which the cheetah moves forward, ideally by running. We
also experimented with a no-reward environment to test the pure
exploration capabilities of our agent. For this we utilized the ant-maze
environment in which the agent must actuate an ant-like robot in order
to explore as much of a maze as possible. The agent receives no
extrinsic rewards at any point in this task.

Overall, we see that the active inference and reward maximization agent
performs similarly in the cup-catch environment. We hypothesise that
this is because, even with the sparse reward, the reward is easy enough
to obtain even with purely random behaviour. Similarly, on the
half-cheetah benchmark, our model performs significantly better than the
model-free SAC agent, due to the superior sample efficiency of
model-based over model-free methods, however it also does not show much
significant improvement compared to reward maximizing model-based
baselines. However in the ant-maze environment, our agent evinces
significantly more exploration capacity and explores a substantially
larger proportion of the total state space than any other agent, once
again demonstrating its superior exploratory capabilities.

##### Interim Discussion

So far, in this chapter, we have applied an active inference perspective
to reinforcement learning and recast the traditional RL objective into a
more active-inference-inspired one; reformulating the reward
maximization objective as that as minimizing the divergence between
predicted and desired probabilistic futures. From this starting point,
we can derive a novel algorithm that exhibits high performance as well
as robustness, flexibility, and sample efficiency in a number of
environments that are known to be exceptionally challenging for
traditional reinforcement learning methods, while also performing
comparably in environments where standard RL methods do well.

We believe that through these two studies, we have convincingly
demonstrated that active inference approaches can be successfully scaled
to levels equal to contemporary reinforcement learning methods in both
model-free and model-based paradigms. Moreover, we hope to have shown
some ways in which the integration of active inference and reinforcement
learning can provide novel and useful perspectives to inform and inspire
work in the deep reinforcement learning community. We demonstrate that
the exploration-inducing properties of active inference objective
functionals such as the Expected Free Energy and the Free Energy of the
Expected Future are highly beneficial especially in more challenging
tasks with sparse or no rewards, while also performing comparably to
pure reward maximization approaches on dense reward tasks that can be
solved with purely random exploration. Moreover, combining both
exploratory and reward maximizing terms in a single objective function
and jointly optimizing them is crucial to derive algorithms which can
simply learn to solve a task without separate exploratory and
exploitatory phases as in much of the literature (
shyam_model-based_2019 ) , although the idea of inducing exploration by
optimizing an epistemic term ( oudeyer2009intrinsic ;
schmidhuber2007simple ; pathak2017curiosity ) has been applied
previously in reinforcement learning, and that can be competitive
directly with both purely exploratory and purely exploitatory tasks in
the regions where each of these methods excel.

An additional idea, inspired by active inference, which could inform
reinforcement learning perspective is the idea of representing
preferences, instead of scalar reward values, as a distribution over
observations. We believe that this modelling choice could enable greater
flexibility in learning non-scalar, non-monotonic reward functions, as
well as providing a natural, Bayesian framework for handling the case of
unknown, uncertain, or nonstationary reward distributions. We believe
that in many naturalistic settings, especially for biological organisms,
rewards are not simply given a-priori by some known oracle, but are
task-dependent, contingent, often highly uncertain, and nonstationary.
Active inference provides a straightforward Bayesian account to handle
precisely such conditions. Although in this work, and in much of the
literature, we instead take extremely simplifying assumptions such as
@xmath to make active inference as close as possible to reinforcement
learning, future work should instead head in the opposite direction and
try to deliberately explore the regions where active inference
approaches offer greater flexibility than the traditional reinforcement
learning paradigm, and thus demonstrate the advantages of active
inference approaches there.

#### 4.2.3 Related Work

Before our work in deep active inference there was a small amount of
prior work which is important to review. The seminal paper which began
this field is Deep Active Inference ( ueltzhoffer_deep_2018 ) , which
initiated the idea of using deep neural networks to approximate key
densities within the active inference paradigm. This paper uses small
neural networks to parametrise the transition dynamics and likelihood in
active inference, and uses genetic algorithms to directly optimize a
policy module of the expected free energy in a black-box fashion. This
is because, under their problem setup, the expected free energy depends
on the environmental dynamics and is thus nondifferentiable, assuming
the environment itself is unknown. They produced a simple agent which
can learn to solve the mountain car problem from OpenAI gym after many
iterations. This paper was my inspiration to dive deeper into trying to
understand the commonalities and differences between deep active
inference and deep reinforcement learning.

Another piece of work, arising contemporaneously with my own initial
work ( millidge2019combining ; millidge_deep_2019 ) , was that by
catal_Bayesian_2019 . They also parametrised the likelihood and
transition dynamics using deep neural networks, and additionally
explicitly utilized an expected free energy value function. However,
instead of directly solving the sparse-reward challenge implicit in the
mountain-car environment they tested their agent in, they instead
constructed a hand-crafted ‘state-prior’ generated from expert-rollouts
which already directly solved the task, thus providing an effectively
dense reward signal for this sparse reward problem.

Some other related work is that of cullen2018active who also applied
active inference to more complex non-toy environments. They trained an
active inference agent to play a subset of DOOM – the ‘take-cover’
environment in OpenAI Gym. However, they still fundamentally utilised
the discrete-state-space active inference formulation by discretising
the continuous DOOM environment into 8 discrete states using the Harris
Corner detection algorithm, and then applying discrete-state active
inference onto the discrete states.

Just after my initial work came similar work by ( tschantz_scaling_2019
) , who instead applied active inference in a model-based fashion. Their
model parametrises the transition dynamics using a deep neural network,
and then uses model based planning (using the CEM algorithm) to optimize
the expected free energy over time. We jointly extended their model in (
tschantz2020reinforcement ) to investigate explicitly the exploratory
effects of the EFE or FEEF objective, and whether such methods can be
further scaled through learning the transition and reward model.

After our work, several recent approaches have scaled active inference
further. ccatal2020learning , situate deep active inference within a
purely POMDP setting, using a VAE encoder and decoder to parametrize the
likelihood and state-posterior mappings, and then explicitly compute an
action search tree using their transition model to approximate the path
integral over the expected free energy through time. Similarly,
fountas2020deep , also explicitly compute a model-based EFE search tree
in their tasks, while simultaneously approximating the output of the
action planner with a model-free ‘habitual’ policy network.

#### 4.2.4 Iterative and Amortised Inference

Now that we firmly understand the notion of implementing control as an
inference procedure, it is worth recapping a fundamental distinction
between two different types of inference, which are important and
implicit in the literature, but rarely well explained. The crucial
distinction is between what we call iterative and amortised inference.
Iterative inference is the kind that arises directly from a naive
application of Bayes Rule, and was the standard inference approach used
until the rise of deep learning very recently ( wainwright2008graphical
; jordan1998introduction ; beal2003variational ) . Almost all
‘classical’ variational or Bayesian inference methods are iterative.
Amortised inference only became prominent with the advent of the
variational autoencoder ( kingma_auto-encoding_2013 ) , but has since
become the dominant approach, especially within machine learning. The
key distinction is that iterative inference directly optimizes the
parameters of the variational distribution. For instance, suppose we
assume our variational distribution is Gaussian, than iterative
inference tries to optimize the means and variance @xmath of this
Gaussian to fit some datapoint. This is what is implied by the standard
reading of the ELBO or variational free energy equation (
hinton1994autoencoders ; beal2003variational ) .

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

Amortised inference, by contrast, does not directly optimize the
parameters of the variational distribution. Rather, it learns the
parameters of a function that maps data to parameters of the variational
distribution . Effectively, the variational parameters themselves are
never optimized directly, they are simply spit out of the amortised
function @xmath , which is then learned. @xmath . Rather, it is the
parameters of this amortisation function @xmath that are learned.
Importantly, these functions are optimized not just against a single
data-point but across the whole dataset @xmath . Once learned, the
amortisation function @xmath can be quickly used to compute estimated
variational parameters @xmath for any data-point, thus amortising the
cost of inference across the whole dataset. By contrast, iterative
inference must start from scratch from each individual data-point given
and optimize the variational parameters afresh. While it is often
written the same way, to make the notation very explicit, we write the
amortisation objective to be optimized as,

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

The reason that amortised inference has risen to such popularity and
ubiquity lately is due to the fact that the amortisation function @xmath
is straightforward to implement as a deep neural network, where @xmath
are the neural network weights which can be trained by a gradient
descent on the ELBO or variational free energy. For instance, in a
variational autoencoder, @xmath is effectively implemented by the
encoder, which maps the data directly to the variational parameters (the
mean and variance of the Gaussian). The iterative approach, by contrast,
would forego the encoder and run gradient descent directly on the mean
and variance themselves for each data-points. We thus see why amortised
methods are preferred. The amortised method can infer the mean and
variance quickly, in one feedforward pass of the network, while gradient
descent training is split over an entire dataset. By contrast, the
iterative approach would require a gradient descent for every inference
that the network wishes to make. Importantly, however, the variational
parameters found by the amortised methods are in general worse estimates
than those found by iterative inference. This is simply because the
iterative inference method optimizes the parameters afresh with each
data-point, while amortised inference must try to estimate them given a
general function which must work for every datapoint. Thus the
amortisation function @xmath must generalize in a way that iterative
methods do not have to, and thus any generalization error will cause the
amortised method to perform worse. This difference in performance is
called the amortisation gap .

Interestingly, it has recently been shown ( marino2018iterative ) , that
you can gain performance improvements by combining iterative and
amortised inference together. For instance, in a variational
autoencoder, if you first perform the amortised mapping to obtain
initial estimates of the variational parameters @xmath but then run
several iterative descent steps directly on your initial estimates of
the parameters, this can improve the inference accuracy and reduce the
amortisation gap compared to the pure amortisation approach with only a
relatively small computational penalty for each inference.

Given that we know we can understand control problems in terms of
inference, it is also interesting to consider whether the type of
inference applied in control as inference can be best understood as
iterative or amortised inference. Indeed, we argue that this distinction
between iterative and amortised inference maps rather cleanly (although
not perfectly) to the distinction between model-based and model-free
reinforcement learning. Where model-free RL can be thought of as
amortised inference and model-based as iterative inference. The
reasoning here is straightforward but requires some subtly about what
exactly is being inferred.

The key quantity to be inferred in control as inference approaches is
the variational distribution over actions @xmath . Model-free approaches
which try to maintain a constant estimate of the value function, Q
function or advantage function using the Bellman equation can be
understood as amortised inference. This is most explicit in the case of
actor-critic or policy gradient methods which explicitly maintain an
amortised policy @xmath , which is implemented as a deep neural network
with weights @xmath where the weights are not optimized separately for
each data-point, but rather across all data-points. Approaches based
purely on value function learning, such as Q learning, can also be
expressed in such a manner, because here the optimal policy depends in a
straightforward way upon the amortised value function. For standard
deterministic Q learning we have that @xmath , or that the action
distribution is a dirac-delta over the maximum value of the Q function,
which is itself amortised and implemented as a deep neural network. In
soft methods, the delta-max is replaced by a softmax over all action
values, so that actions are selected with probability proportional to
their relative exponentiated magnitudes.

Model-based methods, by contrast, appear to correspond to iterative
inference approaches. The key to understanding this is that it is the
planner which matters and is effectively doing the inference, not
anything to do with the model – i.e. the transition model – in
model-based methods, which is often amortised. We can treat the
varieties of planning algorithms such as CEM and path integral control
as optimizing the actions or action sequences directly over the course
of multiple iterations, and thus corresponds to iterative inference.
Indeed okada_variational_2019 have shown how these standard planning
algorithms can be derived as variational inference algorithms themselves
under certain conditions.

To support these identifications intuitively model-free methods share
the same advantages and disadvantages as amortised inference – that they
are trained across a whole dataset but fast to compute for any
individual instance, and less sample efficient, since the amortisation
function can only be learnt across a wide range of experience to enable
good generalization. Model-based methods are the opposite and share the
properties of iterative inference approaches. They are very sample
efficient and perform well with very small amounts of data (since
planning occurs for each datapoint independently, the only need for data
is in the amortised training of the transition model). However, they are
much more computationally expensive per datapoint, since they must
undertake an iterative planning process for each state, instead of
directly mapping a state to an action, as an amortised policy does.
Interestingly, however, for model-free vs model-based approaches, the
amortisation gap is often the other way around. Currently, model-free
amortised policies generally achieve a higher asymptotic accuracy than
do model-based planners ( hafner2019dream ; shyam_model-based_2019 ;
haarnoja2018soft ) . This is for two reasons. Firstly, there is an
additional distinction which must be made between inferring a single
action, as is done by model free policies, and inferring a whole
sequence of actions (an action plan) which is what is typically done by
model-based planners (although often this whole sequence is discarded
and recomputed every time, an approach which is called model predictive
control). Inferring a full plan is almost always harder than inferring a
single action to take immediately, and this may be the cause of some of
the reverse amortisation gap. An additional and potentially more serious
issue is that current planning methods are generally quite crude and
cannot represent expressive distributions over action plans. For
instance the cross-entropy method can only represent single unimodal
Gaussian plans, and similarly path integral control, the other state of
the art method ( williams2017information ; williams2017model ;
williams2018predictive ; theodorou2010reinforcement ;
theodorou2012relative ) suffers from similar constraints. While there
has been some recent work in improving the expressivity of planning
methods, such as multimodal CEM ( okada2020planet ) , much work remains
to be done here to be able to match the expressive capabilities of deep
neural network policies.

Finally, it is important to note that the above distinction has revealed
an additional orthogonal dimension of whether it is single actions that
are inferred or whole action plans. We thus see that we can plot
reinforcement learning and control methods into a quadrant with two
orthogonal dimensions – whether iterative or amortised inference is
used, and whether full action plans or just single actions are inferred.
We thus see that the standard distinctions of ‘model-free’ vs
‘model-based’ themselves map onto the diagonal of the quadrant.
Model-free reinforcement learning is amortised inference of single
actions, while the standard model-based methods correspond to iterative
inference of full action plans. Importantly, there are several methods
on the off diagonal, such as iLQR ( li2004iterative ) which infers
single actions in an iterative fashion. Understanding and plotting
reinforcement learning methods in such a way reveals the full space of
methods and which areas are potentially underexplored. For instance, we
immediately see that there are very few, if any, methods which utilize
amortised plans, even though learning amortised plans could well be
straightforward and may even be beneficial. This would then be a fertile
area for future work.

#### 4.2.5 Control as Hybrid Inference

##### Introduction

Building on the observation that iterative and amortised inference can
be combined to construct an iterative-amortised inference scheme which
combines the advantages and ameliorates the respective disadvantages of
both iterative and amortised inference – enabling rapid and flexible
inference with a high asymptotic performance, and an adaptive scheme
which can leverage additional computing power only where it is most
needed. In this section, we apply iterative-amortised combination to
reinforcement learning which can be seen as combining model-based and
model-free RL, using the identification previously developed.

Specifically, variants of model-based planning algorithms can be derived
as variational inference algorithms using mirror descent to optimize the
variational free energy in an iterative fashion ( okada_variational_2019
) . Additionally, as discussed previously, model-free reinforcement
learning methods such as Q-learning, policy gradients, and actor-critic
can be cast as optimizing another variational free energy bound, but
rather this time in an amortised fashion. Given this, there are multiple
potential ways to combine model-based and model-free reinforcement
learning approaches. Perhaps one of the simplest approaches, which we
apply in this study, is to use the model-free policy as an
initialization of the model-based planner. Model-based planning
algorithms such as CEM or MPPI, require an initial action distribution
@xmath to begin with, which they they proceed to optimize. Usually this
initial action distribution is set to some simple known distribution
such as a zero-centred normal distribution @xmath with a variance
parameter @xmath which becomes a hyperparameter of the planning
algorithm.

Instead, we propose to initialize the planner with the results of the
amortised model-free policy network @xmath . To do this for a potential
action trajectory requires knowledge of future states to feed into the
model-free policy network. However, conveniently, the model-based
planner also has access to a transition model which is used to generate
these simulated state trajectories, given the actions output by the
policy network.

##### Model and Hyperparameter Details

To make our model concrete, we need to instantiate many distributions
such as the transition models @xmath , the parameterised policy @xmath
and the variational iterative planning algorithm which instantiates
@xmath .

The transition model was instantiated as an ensemble of three layer
multi-layer perceptron networks with a hidden size dimension of size
250, which was trained to output a Gaussian distribution (mean and
variance) over the change in environment state . That is, rather than
explicitly model @xmath , we instead modelled @xmath , which we could
then use to reconstruct the next predicted environmental state as @xmath
. Training the transition model to predict state differences instead of
the states directly is a common trick used in model-based reinforcement
learning which has been found to significantly improve modelling
performance by incorporating explicit information about the derivatives
of the states, which is hard to derive solely from the states
themselves. To obtain a measure of uncertainty over the transition model
parameters @xmath , which can be utilized to drive information-gain
maximizing exploration, we maintained an ensemble of 5 transition models
which were each trained on independently sampled batches of transition
data. Each ensemble possessed independent randomly and uniformly
initialized weights.

For the amortised action policy, we utilized the soft-actor-critic
architecture (SAC) ( haarnoja2018soft ) , with a policy-network which
consisted of a three-layer MLP model with a hidden dimension of 256. We
did not use an adaptive @xmath parameter for the SAC agent but set it to
0.2 throughout.

For the iterative planner, we used the standard and powerful CEM
algorithm ( de2005tutorial ) , with a time-horizon of 7, a number of
iterations of 10, and a trajectory sample size of 500. For each
generated trajectory, to encourage more exploration, we added additional
action noise sampled independently from @xmath .

We maintained a memory buffer of all environmental interactions seen by
the agent, and used various samples to train the transition model and
amortised policy network. We trained the model over 10 epochs which
iterated over the full memory buffer, with a batch size of 50. The full
control as hybrid inference algorithm is defined as follows,

Input: Planning horizon @xmath | Optimisation iterations @xmath | Number
of samples @xmath | Current state @xmath | Transition distribution
@xmath | Amortisation function @xmath Amortised Inference :

@xmath

Extract @xmath from @xmath

Initialise @xmath with parameters @xmath Iterative Inference :

for @xmath do

Sample @xmath action sequences @xmath

Initialise particle weights @xmath

for @xmath do

@xmath

@xmath

end for

end for

Extract @xmath from @xmath

return @xmath

Algorithm 3 Inferring actions via CHI

##### Related Work

There has been a small amount of prior work aiming at combining
model-free and model-based ( li2020robot ; che2018combining ) . For
instance, a strand of research has focused on using a learned transition
model to generate additional simulated data which can then be used to
train a model-free policy ‘offline’. This approach was pioneered with
the Dyna architecture ( sutton1991dyna ) , but has also been extended
and applied in more modern deep reinforcement learning settings (
gu2016continuous ) . Conversely, in ( farshidian2014learning ) and (
nagabandi2018neural ) a model-based planner was used to initialize a
model-free policy – the opposite direction to our model. Our approach
does share similarities with the approach used in AlphaGo (
silver2017mastering ) which used learned amortized policy networks to
generate proposals for the monte-carlo-tree-search (MCTS) used to select
moves in that approach. However, their approach was justified on
heuristic grounds and they did not consider how their approach
corresponds to a mathematically principled combination of iterative and
amortised variational inference. Indeed, it is not yet clear if the MCTS
algorithm can be cast as performing some kind of variational inference
or not.

While we are the first to consider the combination of amortised and
iterative inference in reinforcement learning, and to make the
connection to model-based and model-free methods, there is a line of
work combining the two approaches to inference in the context of
unsupervised generative modelling, typically using variational
autoencoders. ( kim_emi:_2018 ) , employ amortised inference in a VAE to
initialize the set of variational parameters which are then optimized
directly against the ELBO. A similar approach was taken by Marino (
marino2018iterative ) , who showed that by repeatedly encoding the
gradients and optimizing the variational parameters against the ELBO,
which was found empirically to improve performance and help narrow the
amortisation gap.

##### Results

We tested our hybrid agent first on a didactic toy continuous control
task. The goal of this task was to simply explore how the iterative and
amortised control schemes interact. The environment was a simple 2-D
planar environment, where the agent began in the bottom-left corner, and
where the goal was to arrive in the top-right corner. The reward signal
was a smooth gradient field leading to the top-right which was
implemented as @xmath where @xmath and @xmath represent the x and y
coordinates of the goal state. In the centre of the environment there
was an impassable wall except for a small opening through which the
agent could pass. The agent could control its x and y velocity – @xmath
with a maximum velocity of 0.05 and a minimum velocity of -0.05.

The graph shows the evolution of the agent’s iterative and amortised
policies as it learns to complete the task. As can be seen, the
iterative policy starts out highly uncertain, with a high variance. As
the amortised policy is slowly learnt, the variance of the iterative
policy shrinks, and the resulting policy closely matches the amortised
policies. This immediately suggests an adaptive method of saving
computation – when the variance of the iterative policy is small, or the
iterative policy is very close to the amortised policy, rely solely on
the computationally cheap amortised policy only. Conversely, when the
amortised policy or the iterative policy is highly uncertain (as at the
beginning of training), then the computationally expensive
model-predictive-control of the iterative policy should be utilized. In
this way, the agent can attain the impressive sample efficiency and
rapid performance of model-based planning at the beginning of training,
when the amortised policy is poor, but then once the amortised policy
becomes good, the agent can simply rely on that and thus achieve the
high asymptotic performance and relative computational cheapness of
model-free RL.

We also compared the hybrid agent on a challenging continuous control
task – HalfCheetah Run @xmath . This environment requires the agent to
take control of a bipedal simulated cheetah in a planar environment with
semi-realistic physics. The agent’s goal is to move the cheetah’s limbs
in such a way as to maximize the overall velocity of the cheetah, while
simultaneously minimizing the total action applied. The reward function
for the task was @xmath where @xmath denotes the overall velocity of the
cheetah. The hybrid agent was evaluated against strong model-free (SAC)
and model-based (CEM) baselines. As can be seen from Figure 4.10 the
hybrid agent significantly outperforms both baselines and simultaneously
achieves the sample efficiency of model-based methods with superior
performance to the model-free SAC agent.

##### Interim Discussion

Empirically, we find that the hybrid agent performs well, and that the
interaction of the iterative and amortised inference components allow
for a natural adaptive scheme to switch between and apportion
computation in a way that maximizes the computational resources
available to the agent. Moreover, the use of an amortised policy to
initialize the iterative planner cuts significantly down on the
computational expense of the planner and tends to stabilize performance.
Additionally, the use of the iterative planner at the start means that
the agent rapidly discovers highly rewarding trajectories which are then
used to train the SAC agent, and thus provides a powerful source of
implicit exploration for the model-free SAC agent. Interestingly,
however, this highly rewarding data generated by the iterative planner
comes with a cost – it is heavily biased towards positive trajectories,
and thus the SAC agent, as it is not exposed directly to negative
trajectories in the real world, simply does not learn them and thus
learns a highly optimistic value function, which performs poorly when
interacting with the real environment. We call this the data-bias issue
and left unchecked it inhibited the performance and learning of the
algorithm.

To ameliorate the data-bias issue, we instead train the SAC agent from
the simulated rollouts of the iterative planner. These rollouts,
especially in the early stages of iteration, contain many examples of
(predicted) negative trajectories, which thus helps render the dataset
fed to the model-free SAC agent less positively biased. These rollouts
do have their own difficulties – namely that they are fundamentally from
a simulation and thus may be a poor representation of the actual
dynamics of the world (especially when the transition model is poor).
Secondly, the rollouts are still biased to some degree by the operation
of the iterative planner even in the early stages of iteration.
Additionally, this becomes more acute as the model-free policy becomes
better, as it learns to avoid negative contingencies and its action
predictions are then used to initialize the planner, thus creating a
compounding positive bias to the data that is fed into the SAC agent.
Nevertheless, we find empirically that this solution suffices to train a
high performance model-free policy network.

On a more theoretical level, it is important to note that we chose a
relatively straightforward scheme of combination – using the amortised
policy to simply initialize the model-based planner. A more involved,
but slightly more principled method may be to set the action prior of
the iterative planner (which is currently assumed uniform, as is
standard in the control as inference framework) to the amortised policy
@xmath . Using this method, instead of a direct initialization, the
amortised policy would serve as a regularizer on the iterative
model-based planner, ensuring that the resulting iterative policy is
penalized for its divergence from the model-free amortised policy. Such
regularisation methods have been found to be beneficial for the
stability of learning in a number of reinforcement learning algorithms
and especially in policy gradient methods, where methods such as PPO (
schulman2017proximal ) reach state of the art performance.

### 4.3 Conclusion

In this chapter, we have investigated the application of methods derived
from the free energy principle – specifically active inference – to the
general problem of optimal action selection and control. We have focused
especially on a core limitation of current active inference methods:
their scalability. We have demonstrated how many of the key
distributions which arise out of the free energy objective can be
parametrized using deep neural networks, to derive schemes which can
look very similar to contemporary deep reinforcement methods – both
model-free and model-based methods. We show that these methods – which
we call deep active inference approaches – can perform comparably and
often better to their deep reinforcement learning counterparts.

Specifically, in the first study presented in this chapter, we showcase
how active inference can be interpreted through the lens of model-free
reinforcement learning. In this case, we use a learnt action policy
@xmath and parametrize the action prior using an amortized expected-free
energy value network, to approximate the required path integral over the
expected free energy. The resulting algorithm looks very similar to
actor-critic methods in model-free reinforcement learning, but using the
expected free energy instead of the reward. Additionally, we find that
this approach also utilizes additional entropy regularisation terms
which can be shown to substantially improve the stability and the
performance of the resulting algorithm – thus demonstrating how insights
and the mathematical formalism of active inference can also lead to
improvements in reinforcement learning algorithms.

In the second study, we instead approximate the path integral of the
expected free energy, with monte-carlo sampling of trajectories and
ultimately use a model-predictive control planning algorithm to compute
optimal trajectories instead of an amortised policy network. This simple
change is sufficient to move us into the realm of model-based
reinforcement learning. Here, we show that active inference can again
attain the performance of comparable model-based deep reinforcement
learning algorithms, and can be applied to solve challenging continuous
control tasks. Additionally, here the superior exploratory capabilities
of the expected-free energy functional come into play, and we see that
they allow the construction of powerfully exploratory goal-directed ,
information-seeking behaviours, which can solve very challenging sparse
reward tasks, such as the mountain car, with ease. This demonstrates
another way in which insights from active inference can aid the
development of deep reinforcement learning algorithms.

We then turn to a more abstract consideration of the difference between
model-based and model-free reinforcement learning in terms of inference
– an insight which is heavily enabled by the active inference
formulation of action selection as fundamentally an inference problem.
We demonstrate that we can see the distinction between model-free and
model-based as simply that of iterative vs amortized inference, where
iterative variational inference directly optimizes the parameters of the
variational distribution, while amortized inference instead optimizes
the parameters of a mapping function which maps observations directly to
variational parameters. We then show how there is a separate dichotomy
between whether policies or plans are inferred, and that this provides
us with a simple two dimensional quadrant scheme upon which to place all
major reinforcement learning algorithms. It also demonstrates that there
are several areas which are underexplored in the literature – especially
the direct computation of amortized plans.

Finally, we use this insight into the nature of model-based and
model-free reinforcement learning in terms of iterative and amortized
inference to ask how these two approaches can be combined . We show that
this can yield powerful algorithms which can combine both the
sample-efficiency and rapid learning of model-based planning with the
asymptotic performance and computational cheapness of model-free
reinforcement learning. Importantly, investigations into this field of
combined or hybrid reinforcement learning algorithms are only just
beginning, and there are many design choices left to be extensively
investigated in future work.

Overall, crucially, we have shown that the free energy principle and
active inference can be successfully applied and scaled up to handle
large and challenging control tasks and to create algorithms which
perform comparably with state of the art methods in deep reinforcement
learning.

In the next chapter, we extend the intuitions provided here about the
importance of combining exploration and exploitation and turn to a more
abstract and mathematical analysis of what kind of mathematical
procedure gives rise to the combination of exploratory and epistemic
action that characterise such objective functionals as the Expected Free
Energy and the Free Energy of the Expected Future.

## Chapter 5 The Mathematical Origins of Exploration

### 5.1 Introduction

In the previous chapter, we have seen the importance and benefits of
information-seeking as opposed to random exploration for reinforcement
learning tasks. Information-seeking exploration, which explicitly aims
to reduce uncertainty about either the environment or the agent’s model
of the environment, provides a powerful exploration strategy that allows
the rapid and efficient exploration of an environment, as opposed to the
random walk strategy employed by random exploration. Moreover, when
combined in a single loss function with a reward maximizing term, this
combination results in goal-oriented exploration where the agent is only
driven to explore contingencies which are also likely to lead to high
reward. We have seen that this goal-directed, or goal-oriented,
exploration mechanism has performed well in model-based reinforcement
learning tasks including sparse-reward environments which are
challenging for standard reinforcement learning agents. Moreover, this
kind of exploration is almost certainly necessary for biological
organisms in more ecologically valid tasks, where rewards are often very
sparse and environments are typically very large compared to those in
mainstream reinforcement learning benchmarks.

In this chapter, we take a more abstract perspective, and study in depth
the question of the mathematical origin and meaning of such
goal-directed exploration objectives which unite both reward seeking and
information maximizing terms in a single objective. Specifically, we
seek to understand whence they arise, and what the mathematical
formulation which can give rise to them is. While for practical purposes
and engineering applications it is often sufficient to glue different
terms together in an ad-hoc way to construct an objective which gives
rise to some desired behaviour, we wish to probe the deeper theory
underlying such functionals which has so far remained mostly mysterious.
It is the hope that by mathematically understanding the origin and
nature of such objectives, as well as their properties, we can
illuminate a swathe of current methods in reinforcement learning,
cognitive sciences, decision theory, and behavioural economics, as well
as deeply understanding how they interrelate to one another. Moreover,
it seems likely, given the generally productive dialectic between theory
and practice in all of these fields, that by contributing to the
underlying theory of such objectives, we can ultimately contribute to
the design of more powerful objectives and methods than are currently
used in the literature.

To begin, we wish to deeply examine the origin and nature of the
Expected Free Energy (EFE) functional. The EFE is central to the theory
of active inference, where it is proposed that all agents under the free
energy principle, which must seek to minimize the long term path
integral of their surprise must choose policies that minimize the EFE.
The EFE has been widely used in almost all models in discrete-time
active inference ( friston_active_2015 ; friston2017active ;
friston2018deep ; friston2017process ; da2020active ) with the exception
of the later development of the generalized free energy (
friston2015active ; parr2017uncertainty ; parr2017active ) .Despite this
ubiquitous use within the active inference community, the precise
mathematical origin and nature of the EFE functional have remained
unclear. In the literature, the EFE is often justified through a
reductio-ad-absurdum argument ( friston2015active ) which runs as
follows – since (we assume under the FEP) all agents minimize free
energy, then they must think they will minimize free energy in the
future. Since the future is uncertain, instead of the standard
variational free energy (VFE), they must instead minimize their expected
free energy (EFE), else they are not a free-energy minimizing agent
(disproven conclusion) ¹ ¹ 1 Technically this is more of an induction
argument than a reductio-ad-absurdum, but we still refer to it as such
due to its description as a reductio in the literature (
friston2015active ) . Central to this logic is the claim that the EFE is
the ‘natural’ extension of the VFE to account for uncertain futures.

In the first section of this chapter, we investigate this claim in
detail. Specifically, we argue that the EFE is not necessarily the only
way to extend the VFE into the future, and that there are in fact other,
more straightforward extensions, such as an objective we call the free
energy of the future (FEF). We then perform a direct side-by-side
comparison of the EFE and FEF functionals and comment on their
similarities and differences, and discuss their respective bounding
behaviour on the expected free energy. We then discuss how active
inference approaches are related to the control as infrerence framework,
and decide upon two key differences – the objective functional utilized
for action selection, where active inference uses the EFE, and control
as inference uses the FEF, and secondly the encoding of value or goals
into the inference process, where active inference directly encodes
values into the generative model through the use of a biased desire
distribution @xmath , control as inference instead uses independent
optimality variables @xmath ² ² 2 We also denote any distribution
involving a desire distribution with a @xmath and, for instance, refer
to @xmath as a biased generative model ) . Finally, we then introduce a
second objective functional, which we call the free energy of the
Expected Future (FEEF) which combines both an intuitively grounded
starting point with the same exploration seeking term as is present in
the EFE, and which was investigated previously in Chapter 4. We discuss
the nature of different possible objective functionals for control.

In the second half of this chapter, we retreat from the specifics of the
EFE, active inference, and control as inference, to instead define a
general framework for understanding the origin of information seeking
exploration terms in control functionals. We argue that the key
distinction, is that between evidence objectives, which maximize the
likelihood of achieving a desire distribution, with divergence
functionals which try to minimize the divergence between a predicted and
desire functional. Specifically, divergence objectives give rise to
information gain terms while evidence objectives do not. We trace this
capacity to the fact that divergence objectives implicitly maximize the
entropy of the agent’s future, in a close connection to empowerment
objectives, while evidence objectives do not. Finally, we put all this
together into a coherent framework which can be used to understand the
full landscape of variational objective functionals for control tasks.

The material in this chapter is heavily based on three first-author
papers. Whence the expected free energy ( millidge2020whence )
(published in neural computation), On the relationship between active
inference and control as inference (published at the IEEE international
workshop on active inference) ( millidge2020relationship ) , and (
millidge2021understanding ) Understanding the Origin of
Information-Seeking Exploration in Probabilistic Objectives for Control
, Arxiv (to be submitted to Royal Society Interface).

### 5.2 Origins of the Expected Free Energy

Here we investigate the origins of the expected free energy (EFE) term
within active inference. It is often claimed that the reason active
inference agents minimize this term is that free energy minimizing
agents must minimize the variational free energy (VFE) into the future
which, since the future is uncertain, constitutes the expected free
energy. To make this claim precise, we need to understand exactly the
variational free energy ‘into the future’ should consist of. We argue
that it must satisfy two conditions, which are both satisfied by the
variational free energy, and which are crucial for that objective
functions to operate. First, we argue that, like the VFE, the VFE
extended into the future should be a divergence between a variational
approximate posterior and a generative model of future states. Secondly,
we argue that, again like the VFE, any free energy of the future should
additionally be a bound on the log model evidence of future
observations. These conditions are both important precisely because they
define why the variational free energy is useful. Minimizing the
divergence between the posterior and the generative model is useful
since it implicitly makes the variational posterior a good
approximation. Conversely, bounding the log model evidence is useful
since the log-model evidence provides a very general measure of how
‘good’ a specific model is, which can be used for Bayesian
model-comparison or even just to understand the amount of inherent
information in the data. Moreover, the log model evidence has especial
import for methods under the aegis of the free energy principle since
the log model evidence is simply the surprisal @xmath which is the basic
quantity which is minimized throughout the theory.

First, we need to define precisely the mathematical setup of the
problem. We assume that our agent exists in a POMDP environment with
states @xmath , observations @xmath , policies (sequences of actions)
@xmath . The agent maintains a variational distribution over the states
and actions and a generative model over the states, observations, and
policies. Specifically, although we are technically interested in the
functionals over a full trajectory @xmath , in practice the functional
decomposes into a sum of independent functionals for each timestep.
Thus, for understanding the behaviour of agents optimizing the
functional, it suffices to consider only a single timestep of the
functional at @xmath .

We argue that the expected free energy does not fulfil these conditions,
but rather another objective functional does, which we call the free
energy of the future (FEF). We define the FEF to be,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (5.1)
  -- -------- -------- -- -------

which is simply the KL divergence between the approximate posterior
generative model over future states, averaged under the expected future
observations @xmath . This trivially satisifes the first condition,
since it is a KL divergence between the variational posterior, and the
generative model, as is the VFE. Next, we show that this functional is a
bound on the expected log model evidence in the future.

  -- -------- -------- -- -------
     @xmath   @xmath      (5.2)
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.3)
  -- -------- -------- -- -------

Crucially, we can see that this is an upper bound on the log model
evidence, and thus minimizing the FEF will tend to decrease the gap
between the FEF and the expected log-model evidence. This functional
thus exhibits identical behaviour to the VFE. To gain a better
understanding of the key differences between the EFE and the FEF, we can
exhibit them side by side.

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (5.4)
  -- -------- -------- -- -------

While the two formulations may look very similar, the key distinction is
that the FEF optimizes the divergence between the variational posterior
@xmath and the generative model while the EFE minimizes the variational
prior @xmath . While this difference may seem small, we see that it has
a significant impact when it comes to the resulting interpretable terms
from the decomposition of the two functionals,

  -- -------- -------- -- -------
     @xmath   @xmath      (5.5)
     @xmath   @xmath      (5.6)
  -- -------- -------- -- -------

Specifically, we see that while both the FEF and the EFE can be split
into ‘extrinsic’ and ‘intrinsic’ value terms, the intrinsic value term
in the FEF is positive while in the EFE it is negative. Specifically
this means that the FEF tries to minimize exploration and keep the
posterior and prior as close together as possible. This minimizing
information gain term is analogous to the complexity term in the VFE
which functions as a regulariser which attempts to keep the posterior as
close to the prior as possible, while still fitting the data. Here, we
see that the goal of the FEF is to, in effect, maximize reward, while
trying to learn as little about the environment as possible. While this
may seem to be an unfortunate objective, in some small cases it may be
beneficial, especially when in the case of offline reinforcement
learning, where there is no continual interaction with an environment,
only trying to learn an optimal policy from a given dataset of
interactions ( ( levine2018reinforcement ) . In such cases, failures of
generalization and extrapolation can often result in poor results
whenever the learned policy is moved even slightly off the data
manifold, and this kind of conservative regularisation of the learning
process can prove highly beneficial ( levine2020offline ) . By contrast,
the EFE maximizes the information gain term, since it is negative, and
tries to drive the posterior and prior as far apart as possible. This
results in information-seeking exploration.

However, while the EFE has an intuitively better exploratory grounding,
it is not a bound on the log model evidence, as the FEF is. We can show
this straightforwardly by noting that the ‘extrinsic value’ term of the
EFE simply is the log model evidence,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.7)
  -- -------- -------- -- -------

and that thus by the non-negativity of KL divergences, the EFE is a
lower bound on the log model evidence. This bound is in the wrong
direction, since to make it tight, the EFE should be maximized instead
of minimized.

Importantly, in the definition of the EFE there is an approximation step
where we have approximated @xmath with the approximate posterior @xmath
. If we make this approximation explicit, we can write the EFE as,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.8)
  -- -------- -------- -- -------

Where we see that the EFE can be both an upper and lower bound on the
log model evidence depending on whether the information gain term or the
posterior divergence term is larger. We can thus see that the likely
time-course of the EFE is to cycle around the bound over the course of
inference until, potentially, it reaches it. This is because, at the
start of training, when inference is poor, the posterior divergence is
likely greater than the information gain, so the EFE functions as an
upper bound and minimizing it gets us closer to the true log model
evidence. This effect likely quickly fades away as the information gain
term becomes bigger, and here the EFE minimizing agent will
preferentially explore its environment in an information-seeking
fashion, driving the EFE away from the real log model evidence for the
environment. Finally, if there are no residual sources either of
posterior divergence (so that the true and approximate posteriors are in
the same class), or information gain (so that the agent has a perfect
model of the environment, and the environment has no intrinsic
stochasticity which gives rise to aleatoric uncertainty), then both the
posterior divergence and the information gain terms will be 0, and the
EFE will finally converge to exactly the log model evidence. While this
behaviour of the EFE functional may lead to adaptive behaviour, it is
not particularly mathematically principled as an extension to the VFE,
and thus it is not necessarily clear why the EFE should be considered to
be a better extension of the VFE than the FEF.

This derivation also reveals an interesting connection between the EFE
and the FEF. Specifically, it is revealed that the EFE is simply the FEF
minus an additional information gain term, thus effectively comprising
the free energy into the future (FEF), with an additional exploratory
information gain term. This derivation can also be derived
straightforwardly from a direct comparison of the two functionals,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (5.9)
  -- -------- -------- -- -------

We can thus understand the origin of the information gain term in the
EFE – it is simply the FEF into the future minus the information gain
exploration term. This means that, in effect, the exploratory properties
of the EFE are simply present by construction. Is it possible, then, to
derive mathematically or intuitively principled objectives which
maintain the information seeking properties of the EFE?

While this question is definitively answered later in this chapter, here
we present a hint of the solution. We propose a novel objective, which
we call the free energy of the expected future (FEEF), which can be
characterised simply as the divergence between the expected beliefs
about future observations and states @xmath and the desired distribution
@xmath . The FEEF objective can be written as,

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

In effect, this objective can be understood as compelling an agent to
bring a predicted (variational) world and a desired (generative)
distribution into alignment. This objective has a strong intuitive basis
for understanding adaptive action, since we are simply trying to
minimize the difference between our veridical beliefs about the future
and our desires. Since the desire distribution is assumed fixed, the
only way to maximize their alignment is to take action to force the
predicted belief distribution towards the desired distribution. If the
belief distribution is accurate, then this will result in trajectories
that really do take the agent towards its desired distribution.
Crucially, we can then decompose this objective into an extrinsic and
intrinsic information seeking term, just like the EFE.

  -- -------- -------- -- --------
     @xmath   @xmath      
                          (5.11)
  -- -------- -------- -- --------

Here, we see that the epistemic information seeking term is identical to
that of the EFE, and thus we would expect FEEF and EFE minimizing agents
to show similar exploratory behaviour. The key difference between these
objectives lies in the extrinsic value term. While the EFE simply aims
to maximize the likelihood of the desire distribution under the
variational belief distribution, the FEEF explicitly tries to minimize
the KL divergence between them, and thus try to match the two
distributions.

Another way of looking at the same thing is to consider this
straightforward relationship between the EFE and the FEEF,

  -- -------- -- -- --------
     @xmath         
                    (5.12)
  -- -------- -- -- --------

We can thus see, that the FEEF is simply the EFE plus an observation
likelihood entropy term. This term is to be maximized and thus
effectively provides an additional source of random exploration to the
FEEF agent rather than the EFE. In effect, the FEEF agent optimizes the
EFE while trying to keep its observation mapping as random as possible.
Another advantage of the FEEF, is that it is equivalent to the VFE at
the present time. This is because the observation entropy term is
constant since it cannot be affected by future observations, and thus
the expression as a whole reduces to the VFE. This means that the FEEF
can be used as a unified objective for both perception and action, while
the EFE can only be used for control. Due to this, a FEEF agent can have
all distributions trained jointly on the FEEF objective while for an
active inference agent, typically, if the transition and likelihood
matrices are learnt, they are optimized against the VFE, while only
action selection takes place using the EFE. This adds an additional
degree of simplicity and elegance to FEEF-minimzing agents while they
retain the same exploratory behaviours as active inference agents.

#### 5.2.1 Control as Inference and Active Inference

This relationship between the FEF and the EFE sheds light upon the
relationship between active inference and control as inference
approaches to control. While the formulations at an abstract level are
very similar – both attempt to solve the control problem by deriving
inference algorithms which operate on graphical models, and usually
utilize the machinery of variational inference to do so – at a lower
detailed level the theories appear quite different and are presented
with substantially differing motivations and notation. Using our
newfound understanding of variational objective functionals of the
future, such as the EFE and the FEF, here we pin down what exactly the
relationship between control as inference and active inference is.

First, we note that there are many straightforward notational
differences between the theories which can be overcome. One obvious
difference is that active inference is primarily concerned with the
inferring of policies (or action sequences) while control as inference
concerns itself with simply inferring policies, or single actions for a
given timestep. It is important to note, however, that it is possible to
reformulate active inference so that it infers policies, and,
conversely, to reformulate control as inference so that it infers full
action plans. A second distinction is that active inference is typically
formulated for POMDPs while control as inference only for MDPs. It is
straightforward, however, to extend control as inference to the POMDP
setup, which results in the following objective,

  -- -------- -------- -- --------
     @xmath   @xmath      
                          
                          (5.13)
  -- -------- -------- -- --------

Here we have used notation standard in control as inference derivations,
namely @xmath is an amortized state-action policy and @xmath is the
‘optimality variable’. Importantly, this novel extension of
control-as-inference to a POMDP setting leads directly to a
straightforward implementation in terms of deep reinforcement learning,
similar to the approaches in Chapter 4. Specifically, this objective can
either be expressed directly as a sum over trajectories, and thus
optimized by planning algorithms using model-based deep reinforcement
learning or, alternatively, it can be expressed recursively and computed
using a value or Q function approach which lends itself naturally to
model-free deep reinforcement learning approaches. The key extension
would be learning a probabilistic encoder-decoder model, most likely a
VAE, to infer the distributions @xmath and @xmath and then to optimize
the entropy of the VAE decoder in the control objective, in accordance
with this objective function. Empirically investigating the performance
of this method, and the impact of the observation ambiguity term, has
not, to my knowledge, been explored in the literature, and would be an
interesting avenue for further work.

Secondly, we can similarly reformulate the control as inference approach
to infer plans instead of policies. This is done by extending the
generative model to cover whole trajectories instead of single
observations, states, or actions. We then infer a constant random
variable @xmath the policy for the whole trajectory. Written out
explicitly, from this generative model you can derive a variant of the
active inference optimal plan derivation to discover that the optimal
plan under the control as inference is simply the softmax path integral
over the variational free energy (VFE), augmented with optimality
variables, and extended into the future.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
                          (5.14)
  -- -------- -------- -- --------

We can decompose this VFE functional into the future as,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.15)
  -- -------- -------- -- --------

Which we can see is equivalent to the standard control as inference
POMDP objective, except that it is missing the action divergence terms.
The action divergence terms are missing simply because full policies are
inferred instead of individual actions. Conversely, we can rederive
active inference to infer individual actions rather than full policies.
To do so, we simply need to add individual actions into the generative
model and variational density and then crank through the derivation,

  -- -------- -------- -- --------
     @xmath   @xmath      
                          
              @xmath      (5.16)
  -- -------- -------- -- --------

Here we see that the expression to be optimized with respect to the
policy parameters @xmath is simply the standard active inference
objective with an additional action-divergence term. If we assume the
action prior @xmath is uniform, then we regain the well known control as
inference policy entropy term. Now that we have extended the theories to
allow for a direct side-by-side comparison, we can see that the two
major differences lie in the information gain term for active inference
as opposed to the complexity ‘state-divergence’ term for control as
inference, and secondly that the control as inference approach contains
an additional ‘likelihood entropy’ term in its objective which active
inference lacks. We know now that the information gain term in active
inference arises directly from the definition of the EFE functional,
which is not an intrinsic part of active inference and may not be
theoretically justified. Indeed, if we replace the EFE in the active
inference derivation with the FEF, we can obtain an objective identical
to the control as inference approach except that it has no likelihood
entropy term.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.17)
  -- -------- -------- -- --------

This means that, effectively, we can consider control as inference as
active inference with a FEF objective or, conversely, that active
inference is simply control as inference with a nonstandard EFE
objective. The question then remains, why does the control as inference
objective possess an additional likelihood entropy term which active
inference does not, since it is not due to a difference in objective
function. We demonstrate that the difference actually arises due to the
way goals or likelihoods are encoded in the inference procedure.
Specifically, active inference encodes goals or rewards directly into
the generative model, so that active inference agents function with a
biased generative model which blends reward-driven and veridical
perception. By contrast, control as inference, keeps veridical inference
and reward computation entirely separate, and instead includes rewards
into the inference procedure through the use of exogenous optimality
variables. The two methods thus solve subtly different inference
problems due to these distinctions in how they encode rewards. Put
simply, active inference encodes rewards and goals through priors;
control as inference encodes them through conditioning. Phrased
intuitively, we can think of control as inference as solving the
inference problem: Assuming that I have acted optimally in the future,
what actions do I infer I will have taken , while active inference
solves the inference problem: Infer my most likely actions, given that I
strongly believe in the future I will observe highly desirable states .
In sum, control as inference maintains a strict distinction between a
veridical perceptual generative model, which generates objectively
likely future outcomes for a given action sequence, while active
inference maintains a biased generative model which preferentially
predicts desired outcomes. Control as inference conditions this accurate
generative model on observing high reward contingencies, and thus
maintains an adaptive action plan. Active inference, on the other hand,
simply maximizes the likelihood of this biased generative model, thus
also inferring adaptive actions.

While this distinction may seem arcane, it actually speaks to deep
philosophical differences in perspectives between the two theories.
Control as inference arises from the cognitivist and logical views of
artificial intelligence, which maintains that in some sense intelligence
is pure thought, which can then be unbiasedly applied to inferring
action. Control as inference maintains the modularity thesis, which is
widespread throughout artificial intelligence and engineering fields
which is the strict separation of perception and control. Perception
aims to build up an accurate world model, while control uses this world
model to compute the best actions. Active inference, by contrast, has
much more in common with enactivist, embodied, and cybernetical views of
control, which see the agent and environment inextricably enmeshed in a
fundamental feedback process – the perception action loop. Here, it is
unnecessary to maintain a strict separation between perception and
control. Indeed, veridical perception is unnecessary since the ultimate
aim of perception, in this view, is not the unbiased modelling of the
world, but rather to subserve adaptive action.

The mathematical effect of this distinction is that control as inference
maintains two separate likelihoods - a veridical perceptual likelihood
@xmath , and a reward-encoding optimality likelihood @xmath , while
active inference only maintains a biased observational likelihood @xmath
. Due to this, control as inference approaches possess an additional
likelihood entropy term which active inference ones lack.

### 5.3 Evidence and Divergence Objectives

Given that we now see that the control as inference framework, even when
extended to full action policies and partially observable environments,
does not manifest an information-seeking exploration term while the
expected free energy does and, moreover, that conversely the EFE does
not function as a bound on a known quantity like the expected model
evidence which is used in variational inference, we are left with the
question of trying to understand whether and how information-seeking
objectives can be derived in a mathematically principled manner. Here,
we argue that objective functionals for control existing in the
literature can be sensibly split into two separate classes – evidence
objectives, which typically arise from a direct variational inference
approach, and which do not manifest information gain terms, and
divergence objectives which arise out of directly minimizing a KL
divergence between two models, and which do give rise to information
seeking terms. We then show how well known objectives in a variety of
literatures can be understood in our scheme.

First, we need to formalize our mathematical setup. We assume that we
have an environment which, from the agent’s perspective, is an unknown
black box. The agent inputs actions @xmath to the environment and it
outputs observations @xmath in return. The exact process that produces
these observations in the environment is forever unknown. The agent can
assume that the observations in the environment are produced by some
form of POMDP model with hidden latent states @xmath , but this is
merely the agent’s model of the environment. The agent can also
parametrize its model with learnable parameters @xmath , which it
assumes are either constant throughout its entire lifetime, or else
change on a much slower timescale than the latent states @xmath . Next,
to formalize a control, as opposed to just an inference problem, we need
some notion of the reward or goal that the agent wishes to achieve. We
formalize this by specifying that the agent possesses an additional
desire distribution @xmath over the observations it receives. This
distribution encodes the ‘ideal world’ of the agent in the sense that
these are precisely the observations it aims to achieve. To map this to
a standard reinforcement learning setting, with rewards, we can perform
the now familiar trick of defining, @xmath where @xmath is the total
reward achieved across a given observation trajectory. Importantly, this
equivalence to standard reinforcement methodology is only a special
case, and that this formulation in terms of a desire distribution is
more flexible in its specification of the rewards. Specifically, here we
assume no specific form of the desire distribution, nor any
factorization of desires across timesteps, for instance, so the desired
observations in the future can depend on the desired observations at the
present in arbitrarily complex ways. It is also straightforward to
extend the framework to a desired distributions over actions as well, by
defining @xmath . This is useful for instance if we want to penalize the
costs (i.e. energetic for biological organisms or robotic systems) of
action. For instance, to define a quadratic cost in the action
magnitude, we can define @xmath , or a Gaussian distribution in the
action centered around 0. The variance of the Gaussian ( @xmath )
effectively defines the weighting coefficient which scales the size of
the penalty. Here, to keep notation simple, we do not discuss action
penalties and only work with a desired observation distribution @xmath ,
but the extension to actions is entirely straightforward.

The general objective of the control problem is to compute an action
trajectory @xmath which results in the realization of your goals, which
are defined through the desire distribution @xmath . We also assume,
that if the agent actually executes a given action trajectory, it will
receive a real trajectory of observations from the environment according
to some distribution @xmath . This distribution can be approximated
either by sampling real trajectories from the environment, as is
implicitly the case in model-free reinforcement learning methods, or
else the agent can explicitly model this distribution as @xmath , with
parameters @xmath , which may be implemented as a deep neural network,
for example. We call this distribution the predicted distribution, since
in some sense it is what the agent predicts will occur if it executes a
given action trajectory.

With the preliminaries settled, we can formally define the evidence and
divergence objectives. Evidence objectives try to maximize the
likelihood of the desire distribution averaged under the predicted
distribution. Intuitively, we wish to find the action trajectory that
maximizes the expected likelihood of the desire distribution.
Mathematically, we can represent this as,

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

Where here, as is usual, we optimize the log of the desire distribution
instead of the desire distribution itself. Since the log is a monotonic
function, this does not affect the actual optimum of the problem and
since we often factorize the desire distribution into products, the log
will turn that into a sum, which is usually much better behaved
numerically.

Conversely, for a divergence objective, instead of maximizing the
likelihood of the desires under the predicted distribution, we instead
with to directly minimize the divergence of the desire and predicted
distribution. In effect, we want to make the desire distribution and the
predicted distribution match. Mathematically, we can write this as,

  -- -- -- --------
           (5.19)
  -- -- -- --------

Where here we use the KL divergence @xmath . Intuitively, we can think
of the distinction between evidence and divergence objectives being that
the evidence objective seeks to match the predicted distribution to the
mode of the desire distribution – i.e. it focuses all effort onto the
most desired observations, while neglecting the lesser desired
observations. This is why evidence objectives typically arise from
direct maximizing principles such as utility maximization, or
variational inference. Meanwhile, divergence objectives seek to
precisely match the predicted distribution and the desired distribution,
so that if some observation is not the most preferred one, but has some
moderate level of preference, the divergence objective would seek to
have that observation manifest some amount of time in proportion to its
relative preferredness. This means that in general, if the desire
distribution is spread out, then agents will seek to realize a
spread-out distribution of predicted observations. In contrast, under an
evidence objective, even if the desire distribution is broad, the agent
will continue to place all effort to keep the predicted distribution a
peak around the mode of the desire distribution.

Another way we can interpret the difference between the objectives is in
terms of the effect of the objective upon the predicted distribution
@xmath . Specifically, we can think of the divergence objective as a
balance between trying to maximize the likelihood of the desired
distribution under the predicted distribution, and trying to maximize
the entropy of the predicted distribution. We can think of this as the
divergence objective as effectively saying ‘try to maximize your desires
or utility while also keeping the future as broad as possible’ – i.e.
keeping your options open. This can be demonstrated mathematically
through the simple definition of the KL divergence,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Here we see that we can express the divergence objective simply as the
evidence objective plus the maximization of the entropy of the predicted
distribution. In effect, the divergence objective simply includes an
entropy regulariser to the standard evidence objective. Conversely, we
can also express the relationship between the objectives in the other
way. We can think of the evidence objective as simply trying to match
the predicted and desire distribution while simultaneously minimizing
the entropy of the predicted distribution. This is straightforward to
show mathematically,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.20)
  -- -------- -------- -- --------

This formulation gives a straightforward intuition for the
‘mode-seeking’ behaviour of the evidence objective. The evidence
objective seeks to match the predicted and desire distribution, while
also being penalized for the breadth of the predicted distribution. The
best way to resolve this tension is by forming a highly peaked predicted
distribution around the mode of the desire distribution so that it can
cover as much probability mass as possible.

Interestingly, differences between the two formulations generally only
emerge when the desire distribution is broad and complex. Here, the
divergence objective will tend to force the predicted distribution and
desire distribution to match in their complexity, while the evidence
distribution will seek out and focus around its mode. Another intuitive
way of thinking about the distinction is that the divergence objective
implicitly maximizes some sort of future empowerment, by implicitly
trying to keep all future options open, by seeking to make future
observations as entropic as possible. Evidence objectives, by contrast,
seek the opposite. They aim for ‘precise futures’ where the amount of
future variability is as low as possible. It is straightforward to show
that, unlike evidence objectives, divergence objectives can be
immediately decomposed into an ‘extrinsic’ value divergence term, and an
information-seeking exploratory term,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.21)
  -- -------- -------- -- --------

Importantly, the property that divergence objectives give rise to
directed information-seeking exploratory terms, arises directly from the
previously discussed intuition that they attempt to maximize the entropy
of future observations. Such information gain terms arise whenever the
predicted distribution is extended to model additional latent variables
or parameters. The proof of this is straightforward,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.22)
  -- -------- -------- -- --------

Put verbally, this relationship shows that to maximize the entropy of a
distribution, if the distribution can be understood in terms of a latent
set of variables, requires both maximizing the conditional entropy of
the variable given the latent variables, while simultaneously maximizing
the mutual information of the latent variables between the observed and
latent variables. Intuitively, within our context, this means that to
maximize the entropy of future observations, it is necessary to
successfully model the relationship between these future observations
and their latent states, which entails maximizing the mutual information
between the latents and the observations. It is the maximization of this
mutual information which undergirds the exploratory information-seeking
behaviour which is manifested by divergence objectives. Conversely, the
fact that evidence objectives seek to minimize the entropy of the
predicted distribution means that they implicitly seek to minimize the
amount of mutual information between observation and latent variables.
We can think of this as divergence objectives aim to reach a given set
of goals while also learning as much as possible about the environment,
in order to precisely match the two distributions. Evidence objectives,
on the other hand, seek to reach their goals while learning as little as
possible about the environment, and keeping the environment as regular
and predictable as possible.

Now that we have proposed and given considerable intuition for our
dichotomy between evidence and divergence objectives, we look to see
where these objectives appear in the literature, and how they can
explain differences in exploratory behaviour between differing
paradigms.

#### 5.3.1 Control as Inference

It is straightforward to show that the control as inference, and
variational inference objectives are bounds upon the evidence objective.
Put simply, we have that control as inference aims to solve the
inference problem of inferring an action distribution given a desired
set of observations. Specifically, we seek to obtain the distribution
@xmath where we use @xmath to denote a set of hypothetical ‘optimal’
actions which have been conditioned upon. To find this posterior, we use
a variational approximation by defining the variational density @xmath
and optimizing the following variational lower bound,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Which serves as the control as inference objective. It is then
straightforward to show that this objective serves as a bound on an
evidence objective,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.23)
  -- -------- -------- -- --------

And thus we can see that the control as inference framework optimizes a
variational bound on the evidence objective. This straightforwardly
explains why control as inference approaches do not give rise to
directed, information-seeking exploration, but rather instead only
induce random action entropy maximizing exploration terms. This random
exploration, while highly efficient in many contemporary dense-reward
reinforcement learning benchmark tasks, where a random policy often
suffices to cover enough of the state space, it is increasingly
ineffective in extremely high dimensional, and sparse reward
environments.

#### 5.3.2 KL Control

Another control method in the literature that has been applied, and
studied fairly extensively is KL control. Although not as widely used in
reinforcement learning, it is often implicit optimized in control tasks,
and has deep relationships with the beginning of the control as
inference approach, as well as more esoteric path integral methods.
Moreover, it has recently seen renewed application in deep reinforcement
learning approaches such as state-marginal matching of (
lee2019efficient ) . KL control, as the name implies, chooses control to
minimize the KL divergence between the current state and a set of
desired states, leading to the following objective function,

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

Here we have used @xmath instead of @xmath to denote that KL control has
typically only been applied to fully-observed Markovian MDP environments
as opposed to full POMDP dynamics. As such, while the KL control
objective is clearly just the divergence objective, its superior
exploratory capabilities have not been significantly explored in the
literature due to the only applications currently being in fully
observable environments while the information-seeking exploratory terms
require the extension to hidden variable models. Another interesting
point is that the objective in continuous time active inference in
predictive coding models can also be as a KL divergence between a
desired ‘set-point’ and a currently observed point, and is thus an
instance of KL control. However, these models also do not handle latent
variable models, and thus also do not manifest the full exploratory
capabilities of divergence objectives.

#### 5.3.3 Active Inference

Given that we know from previously, that the expected free energy
contains an information gain term, which gives rise to the
information-seeking exploratory behaviour of active inference agents, it
is worth investigating the relationship of the EFE to evidence and
divergence functionals. Recall, from the previous section that the EFE
formed neither an upper nor a lower bound upon the log model evidence,
but instead formed an upper bound when the posterior divergence was
greater than the information gain term, and a lower bound otherwise,
with the goal of eventually converging directly to the log model
evidence in the case that both of these terms become zero. Noting that
the log model evidence simply, as the name suggests, is the evidence,
objective, we can rewrite this in terms of our new understanding as,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.25)
  -- -------- -------- -- --------

so that the EFE does not stand in a straightforward relationship as
bound in any specific direction on the evidence objective. Nevertheless,
since the EFE contains an information gain term to be maximized, as do
divergence objectives, we might expect to obtain a straightforward
relationship between the EFE and the divergence objective. We can write
out the relationship between the divergence objective and the EFE as
follows,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.26)
  -- -------- -------- -- --------

Here we see that the EFE can be expressed in terms of the divergence
objective, an information gain term, and, interestingly, the variational
free energy. In effect, the divergence objective consists of the EFE,
the VFE, and an information gain term. Specifically, the EFE becomes an
upper bound on the divergence objective when the information gain term
is greater than the variational free energy. This is similar to
previously where we saw that the EFE became a bound on the evidence when
the information gain is less than the posterior divergence. It is thus
clear that the EFE objective does not serve as a valid and consistent
bound on either of the divergence or the evidence objectives. Similarly,
we can express the EFE directly in terms of the divergence objective as
follows,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.27)
  -- -------- -------- -- --------

Where we see that the divergence objectives simply is the EFE plus an
information gain term, minus the marginal or predicted entropy term. As
such, even within this framework, the mathematical origin and the
behaviour of the EFE remains unclear since the EFE does not form
consistent bounds on either objective, but instead oscillates above and
below both.

#### 5.3.4 Action and Perception as Divergence Minimization

A recent framework, inspired by active inference and advances in deep
reinforcement learning, which aims to unify perception and action under
a single framework is Action and Perception as Divergence Minimization (
hafner2020action ) (APDM). This framework proposes that both action and
perception can be modelled as an agent trying to mininimize a divergence
functional between two distributions an ‘actual’ distribution @xmath ,
and a target distribution @xmath .

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.28)
  -- -------- -------- -- --------

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.29)
  -- -------- -------- -- --------

By expressing this bound explicitly, we can see how it is an upper bound
on the information gain, since the posterior divergence is always
positive. The tightness of the bound then depends on how closely the
actual and target distributions match. In general, we can use this
approach to write out a full expression for the divergence objective
between two joint distributions over both observations and latent
variables.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.30)
  -- -------- -------- -- --------

In effect, we see that minimizing the divergence between two joint
distributions requires the minimizations of both the likelihood
divergence and the posterior divergence, while also requiring the
maximization of the information between posterior and prior of the first
term in the joint KL.

It is also straightforward to relate this joint divergence to the
divergence objective, which is the divergence between marginals instead
of joints.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.31)
  -- -------- -------- -- --------

Since the posterior divergence is always positive (as a KL divergence),
we observe that the joint divergence is simply an upper bound on the
divergence objective. Since the divergence is minimized, this bound is
in the correct direction, and thus minimizing the joint divergence is a
reasonable proxy for minimizing the marginal divergence objective. By
minimizing the joint, it implicitly encourages agents to minimize both
the marginal divergence as well as the divergence between the predicted
and desired posterior distributions.

While the generic APDM divergence, as just a divergence of joints, is
straightforwardly an upper bound on the divergence objective, we show
that under the common definitions of the actual and target
distributions, the APDM divergence can also be understood as a lower
bound on the evidence objective, thus providing a bridge between the two
objectives. Although the actual and target distributions can be defined
differently depending on the objective you desire to reproduce, one
canonical form of the actual and target distributions, which can
reproduce control as inference as well as variational perceptual
inference is as follows. We define the actual distribution to be the
combination of the ‘real’ data distribution @xmath and also a
variational belief distribution @xmath such that @xmath . Similarly, we
define the target distribution to be the product of the agent’s
veridical generative model @xmath and the exogenous desire distribution
@xmath such that @xmath . This target distribution is valid as long as
the ultimate objective is optimized via gradients of the divergence,
which does not require that the target distribution be normalized. Under
this definition of the actual and target distributions, the APDM
objective becomes,

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

In the case of known observations in the past, we assume that the data
distribution becomes points around the actually observed observations
@xmath while the desire distribution becomes uniform – as there is
little use for control in having desires about the unalterable past.
Under these assumptions, the APDM objective simply becomes the ELBO or
the negative free energy, thus replicating perceptual inference.
However, on inputs in the future, the data distribution becomes a
function of action (since actions can change future observations) and
the desire distribution becomes relevant, thus allowing the minimization
of the APDM functional to underwrite control. To gain a better intuition
for the interplay of perception and control in the APDM functional, we
showcase the following decomposition,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.33)
  -- -------- -------- -- --------

which demonstrates that the APDM objective effectively unifies action
and perception by summing together a perceptual objective (VFE) with the
divergence objective for control. This confirms the previous finding
that the APDM objective forms an upper bound on the divergence objective
since the ELBO, as a KL divergence, is bounded below by 0. We also
observe that this form of the APMD objective is also approximately a
lower bound on the expected evidence objective, thus providing a link
between the two objectives.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.34)
  -- -------- -------- -- --------

Which is approximately equal to the APDM objective under the condition
that the posterior divergence bound between desire posterior @xmath and
true posterior @xmath is small.

### 5.4 Towards a General Theory of Mean-Field Variational Objectives
for Control

Now that we understand the division of objectives for control into two
classes of evidence and divergence functionals, we can start to try to
understand the full possibilities of the space of potential objectives.
Here, in this final section of Chapter 5, we try to present precisely
such a taxonomy. We focus specifically on ‘mean-field’ variational
objective functionals, meaning that we can split the objective into a
number of independent objectives for each time-step of a trajectory
which can be minimized independently. Crucially, such a mean-field
assumption is also made in the traditional reinforcement learning
paradigm, where it is a necessary precondition for the Bellman equation,
and also is standard in the control as inference framework as well.

While the division into divergence and evidence objectives is clearly
important, it cannot be the full story. Recall that one of the key
differences between control as inference and active inference discussed
earlier, was not just the objective of the EFE vs the FEF, but also the
way value was encoded into the inference procedure. Control as inference
encoded value via an additional set of ‘optimality variables’ which were
augmentations to the graphical model, and did not interact with any
previously existing variables. Active inference, by contrast, encoded
value directly through a biased generative model of the observations ³ ³
3 Active inference can also be formulated with biased states, see (
da2020active ) . We call the method used by control as inference, which
does not affect any currently existing variable an exogenous encoding of
value, while the method used by active inference, since it encodes value
directly into the model itself, we call an endogenous encoding.
Exogenous and Endogenous encodings of value provide an orthogonal
dimension of objective variablility on top of the evidence-divergence
dichotomy, since clearly one can have an evidence, or a divergence
objective with both an endogenous or an exogenous value encoding.
Finally, the actual specifics of the generative model used clearly
affects the variational objective irrespective of whether it is an
evidence or divergence objective, or uses an endogenous or exogenous
value encoding. For instance, whether we consider latent states, or
various different types of model parameters in the generative model
leads to a different objective functional.

We thus see that we can break down the landscape of potential mean field
objective functionals for control into three orthogonal dimensions.

-   Whether an evidence or divergence functional is used.

-   Whether value is encoded exogenously, or endogenously.

-   The generative model underlying the objective functional.

Under different values for each of these dimensions, the objective
functional that is specified changes in a straightforward and principled
manner. Thus, our scheme allows the direct derivation of any given
functional, and an understanding of its decompositions, and hence the
behaviour it induces, for any choice of these variables. While we have
covered the effect of choosing an evidence or a divergence functional
previously, we have not yet been explicit about the effect of the other
two dimensions. In this section, we explore the effects of these
additional dimensions of design choice in more detail.

#### 5.4.1 Encoding Value

However, the need to encode goals or desires into the inference
procedure immediately introduces design choices of how exactly this is
to be done. We argue that these design choices can first be split along
two orthogonal axes – firstly, whether goals are encoded exogenously as
an additional input to the inference process, or endogenously through
fundamentally biasing one or more aspects of the inference model. The
second axis of variation is whether goals and desires enter the
inference procedure through the generative model or the variational
distribution. Making different design choices here produces a variety of
different variational algorithms for control.

If goals are encoded through the generative model, then whether the
goals are encoded exogenously or endogenously is the primary distinction
between the formalisms of control-as-inference and active inference. On
the other hand, encodings goals through the variational model instead
leads to novel algorithms which generally have not been much explored in
the literature. Encoding goals exogenously through the variational
distribution leads to a variational bound similar to
control-as-inference but with an extrinsic value term with a
reversed-KL-divergence which thus exhibits mode-seeking rather than
mean-seeking behaviour, which is related to pseudolikelihood methods (
peters2007reinforcement ) in variational reinforcement learning.
Encoding endogenous goals through the variational distribution leads to
a novel class of reverse-active-inference algorithms which minimize a
variational divergence between a biased approximate posterior and a
veridical generative model.

##### Exogenous Value: Maximum-Entropy RL

To encode goals exogenously into the generative model, we must augment
the POMDP graphical model with additional optimality variables @xmath .
The idea here is that the optimality variables are binary bernoulli
variables which mark whether a trajectory is optimal from the current
state where the probability of optimality is often set to the
exponentiated reward @xmath so that @xmath . Adaptive actions are then
inferred by first assuming that the agent has acted optimally into the
future, and then inferring the actions that would be consistent with
that belief – i.e. we wish to infer the posterior @xmath . This
posterior can then be approximated by minimizing the augmented
variational bound:

  -- -------- -------- -- --------
     @xmath   @xmath      (5.35)
  -- -------- -------- -- --------

By splitting apart this bound into its constituent parts, we can
investigate the expected behaviour of agents which act so as to minimize
the bound.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
                          
              @xmath      (5.36)
  -- -------- -------- -- --------

The bound thus splits into four separate and identifiable terms –
extrinsic value, observation ambiguity, action divergence, and state
divergence. The first extrinsic value term corresponds to the expected
external rewards given by the environment. This is due to the definition
of optimality that @xmath so that the extrinsic value is simply the
expected reward of a given state-action pair. By minimizing the negative
expected reward, we wish to maximize the expected reward on a given
time-step. This is identical to the standard reinforcement learning
objective of reward maximization so that if @xmath only contained the
extrinsic value term, it would be exactly equivalent to reinforcement
learning except that the expectation is taken with respect to the
agent’s beliefs over states and actions rather than the true
environmental transition dynamics.

The observation ambiguity term @xmath grants a bonus to agents for
reaching areas of state-space with a high expected likelihood, that is
areas where the state-observation mapping is highly precise. In effect,
if the agent must learn a likelihood mapping, this discourages
exploration by granting bonuses for staying in regions already well
characterised. This term only arises in the POMDP setting due to the
addition of a likelihood term in the generative model. The third term is
the action divergence which is to be minimized, and penalizes the agent
for the divergence between its variational policy @xmath and its prior
policy @xmath . If the prior policy is assumed to be uniform such that
@xmath where @xmath is the cardinality of the action space in discrete
action-spaces, and @xmath in continuous action spaces with a minimum and
maximum action value, then this action divergence term reduces to the
negative expected action entropy @xmath . The final term is the state
divergence, so that the agent tries to minimize the divergence between
its variational posterior over the state and the prior state expected
under the generative model. If the transition model is learnt, this
leads it to prioritising transitions with known dynamics, again causing
the agent to primarily confine itself to regions of the state-space it
has already modelled well. In the MDP setting, without observations,
this term vanishes since the variational posterior @xmath becomes the
variational prior @xmath which is often assumed equal to the generative
prior @xmath . Thus in the case of an MDP with a uniform action prior we
obtain the maximum-entropy RL objective:

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

Which induces agents both to maximize expected rewards while also
maximizing the policy entropy. Intuitively this means that the agent
should try to maximize rewards while acting as randomly as possible
(maximizing entropy). This policy entropy term thus provides a crude
bonus for random exploration and often helps prevents the
commonly-observed phenomenon of ‘policy collapse’ (
fujimoto2018addressing ) whereby reinforcement learning agents will
often rapidly learn to put all probability mass onto a single action,
thus preventing other actions from being taken, hindering exploration
and long-term performance. Interestingly, when extended to the POMDP
case with learnt transition and likelihood models, this formalism gives
rise to additional observation-ambiguity and state-divergence terms
which serve to further disincentivise exploration by penalising moving
too far from the prior predicted state and giving bonuses for highly
predictable likelihoods. Intuitively we can think of these extra terms
as trying to do away with the additional uncertainty induced by the
POMDP setting, so the agent confines itself to the region which is as
close to an MDP as possible.

##### Endogenous Value: Active Inference

While maximum entropy reinforcement methods can be derived by encoding
goals exogenously to the generative model through the use of additional
‘optimality variables’, it is also possible to encode goals endogenously
by directly biasing some aspect of the generative model towards
preferred outcomes. Intuitively, we can think of the difference as
follows. With exogenous optimality variables, we possess a veridical
generative model outputting the likely and unbiased trajectories for a
given series of actions. We then ‘shift’ these trajectories to converge
on the goal by conditioning on the optimality variables, and then infer
the actions consistent with the shifted trajectories. By contrast,
active inference endogenously encodes goals by biasing the model so that
instead of a veridical generative model which generates trajectories
that are then shifted, instead we have a biased generative model which
directly outputs a biased trajectory of observations @xmath converging
on the goal, which can then be used to infer the actions consistent with
this biased trajectory. In this manner, instead of proposing additional
optimality variables, we directly posit a biased generative model @xmath
and optimise the biased variational bound:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (5.38)
  -- -------- -------- -- --------

This variational bound decomposes into three terms as can be seen above.
The first, extrinsic value, is such because it is the biased probability
of observations expected under the variational distribution. If we
assume that the biased generative model is influenced by the external
rewards in the environment, to allow for consistency with reinforcement
learning and the maximum-entropy RL framework such that @xmath then this
first term reduces to minimizing the expected sum of negative rewards,
or maximising expected rewards. The other two action-divergence and
state-divergence terms are equivalent to the terms in the maximum
entropy bound above, except that the active inference bound is lacking
the ‘observation ambiguity’ term. This is because exogenously encoding
goals adds an additional set of optimality variables to the variational
bound, thus maintaining a distinction between veridical observations and
biased optimality variables, while endogenously encoding goals needs to
‘hijack’ at least one degree of freedom in the bound in order to encode
the goals directly. Here the observation ambiguity term has effectively
been hijacked by being biased with reward to instead encode the
extrinsic value.

Interestingly, the choice of encoding goals endogenously also gives an
additional set of choices of how to bias the generative model. The key
choice is between having a biased likelihood, as done above, or a biased
marginal and posterior. This decomposition is shown below:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.39)
  -- -------- -------- -- --------

Here, we assume that @xmath such that the extrinsic value term is again
directly equal to the rewards. An interesting difference is that the
biased state-divergence is now between the variational posterior and the
biased state posterior, which represents the posterior over states that
would be observed given the biased observations. This gives this second
term also the flavour of an extrinsic value as the goal is not only to
maximize rewards but match the veridicial variational distribution to
the biased state distribution induced by the desired observations

Alternatively, it is also possible to consider encoding the bias into
the states @xmath instead of the observations. In this case, we obtain
the following objective functional:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.40)
  -- -------- -------- -- --------

In this case, we have regained the observation-ambiguity term and
instead the state-divergence term has been ‘hijacked’ by the rewards to
become the extrinsic value term, which has become the divergence between
predicted and desired states.

Given that there exist these two design choices of whether to encode
goals exogenously or endogenously and which lead to subtly different
objectives, a natural question to ask is what are the relative
advantages and disadvantages of the two methods? What trade-offs exist
and where might each be useful? In general, the primary practical
difference between the methods is that by endogenously encoding rewards,
the agent loses a degree of freedom, which manifests itself as the loss
of an observation ambiguity term if the goals are encoded through biased
observations, or the loss of the state-divergence term if the goals are
encoded as a desired state distribution. These additional terms tend to
discourage exploration by causing the agent to remain in areas with
known mappings, so that by disabling them endogenous goals tend to
encourage exploration. On the other hand, by keeping to well-known
regions where the POMDP behaves like an MDP, the exogenous goals method
enforces a greater conservative bias towards safety, which may be
especially useful in settings where exploration is costly, satisficing
policies are easy to find, or in policies learnt in an offline or
imitation-learning setting where venturing outside of the training
distribution can have deleterious effects on the policy.

There are also important philosophical and representational differences
between the two. On a representational note, although in the derivations
above both the optimality variable and the biased generative model have
been defined in terms of exponentiated reward, this is not necessarily
the case. Since in the endogenous encoding case, the goals are encoded
directly as prior distributions into the generative model, it is
possible to model complex and potentially nonstationary goals
distributions in this manner. However, due to the optimality variables
being binary, and conditioned upon, this may constrain their
representational power compared to the endogenous method, although in
practice this difference may be negligible as although the variables
themselves are binary, the probability of optimality being one, can be
defined as an arbitrary function of the states, actions or observations.
Thus in practice, there may be little representational difference
between the two methods, except that the endogenous case has a slightly
simpler intuitive justification as directly specifying a desired
distribution over states or observations.

The difference between exogenous and endogenous encodings of value also
has significant philosophical import. Exogenous encodings, by adding
desires on top of an unbiased generative model maintain a clean
distinction between veridical perception and action selection, and
goals. This maintains the core modularity thesis of much work in
artificial intelligence that perception, and action selection should be
kept separate such that first a veridical world-model is constructed
which tries to accurately model the world, then given a set of arbitrary
goals, a general-purpose planner or policy can be utilised or learnt to
enable the agent to achieve these goals. This approach corresponds to
the classical perceive-(value)-plan-act cycle in cognitive science and
maintains separate modules for a goal-agnostic perceptual system, a
goal-agnostic planner or action-selection mechanism, and then a set of
goals which are not intrinsic to the agent but which are constructed or
handed-down from on high. Endogenous encoding methods, by contrast, tend
to blur the boundaries between these sytems, since goals are encoded and
adaptive actions are selected through a process of biased perception and
inference whereby an agent does not first infer a true trajectory,
compares it to its goals, and then tries to match the too, instead it
simply sees a biased trajectory leading to its goals and then acts
consonantly with what it sees. This view has close links to embodied and
enactivist views in philosophy and cognitive science which stress that
rather than distinct modular systems of perception, valuation, and
action there is instead a single combined system or sensorimotor loop
which directly acts on sensorimotor contingencies in an adaptive fashion
( baltieri2018modularity ) .

##### Encoding Value into the Variational Distribution

Previously, we have encoded values either exogenously or endogenously
into the generative model of the agent. In the case of endogenous
encodings, this means that the agent makes biased predictions, rather
than forming biased inferences. However, it is also possible to consider
and investigate what happens if instead values were encoded into the
variational distribution so that the agent’s inference procedure rather
than model is biased. We first consider the case of exogenously encoded
goals. This requires that the variational distribution is augmented with
binary optimality variables, just like the generative model was
previously giving @xmath . From this we can write the relevant free
energy functional:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
                          
              @xmath      (5.41)
  -- -------- -------- -- --------

We see that the resulting functional, under the assumption that @xmath ,
is exactly equivalent to the functional obtained for the exogenously
encoded generative model, up to a sign difference in the extrinsic term
which can be finessed without loss of generality by inverting the sign
of the reward function. We thus see that, if values are exogenously
encoded, it does not matter which distribution they are primarily
encoded through. Intuitively this is because since the veridical
distributions are maintained through endogenous coding, they are
unaffected by the encoding of value into them, thus which one to choose
has no effect overall up to a trivial sign difference which can be
easily finessed through negatively encoding reward.

When rewards are endogenously encoded, however, the resulting
functionals are not equivalent. We first show this with a biased-state
functional which should be compared to Equation 5.4.1 , where we
directly bias the state-inference part of the variational functional so
as to preferentially infer being in desired states from a given
observation @xmath . Through this decomposition we obtain the
functional:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (5.42)
  -- -------- -------- -- --------

which is very similar to the corresponding state extrinsic value
functional in Equation 5.4.1 , except that in the extrinsic value term
the divergence is between the biased variational posterior and a
veridical generative prior, rather than a veridical variational
posterior and a biased generative prior. By having the biases occur on
the left side of the KL, we are essentially minimizing the reverse-KL
compared to when value is encoded into the generative model. This gives
the resulting agents a mode-seeking rather than a mean-seeking
behaviour, since agents optimizing under the reverse KL will suffer a
large penalty if the desires are in regions with a very low veridical
probability. Moreover, there is a further subtle difference in the POMDP
case here since we are contrasting the biased distribution with a prior
rather than a posterior. However, in the MDP case this difference
vanishes, and we obtain the simplified functional:

  -- -------- -------- -- --------
     @xmath               
              @xmath      
                          (5.43)
  -- -------- -------- -- --------

Which is lacking the observation ambiguity term due to being an MDP, and
also the extrinsic value has become the divergence between desired
variational states and predicted generative states. This functional is
exactly equivalent to the alternate formula with values encoded
endogenously into the generative model except that it uses the reverse
KL divergence. Moreover these functionals are deeply related to KL
control with an additional action divergence term, and thus when value
is instead encoded into the variational distribution we have reverse-KL
control which uses the reverse KL divergence and is very closely related
to pseudo-likelihood methods in reinforcement learning (
abdolmaleki2018maximum ; peters2007reinforcement ) .

Overall then we have explored two orthogonal axes of variation for the
problem of how to encode a notion of value, reward, or desires into an
otherwise value-agnostic variational inference procedure in order to be
able to infer adaptive actions. We have shown that the first question of
whether value is to be encoded exogenously or endogenously makes subtle
but significant differences in the resulting functionals. Specifically,
by requiring the utilisation and biasing of one of the variables in the
model to encode value, endogenous encoding tends to lose one degree of
freedon in its functional compared to exogenous encoding. In the POMDP
setting this is typically the observation-ambiguity term if goals are
encoded into observations, or the state-divergence term if goals are
encoded into states. Beyond this, we see that these different means of
encoding value also has significant philosophical importance as to the
nature of perception, action and value. Exogenously encoding goals
supports a modular description of these three functions as independent
systems which are each agnostic with respect to the outputs of the
others, while endogenously encoding goals merges them all together into
a mixed system where action and perception are intrinsically biased by
goals towards adaptive outcomes. Moreover, this dichotomy of exogenous
or endogenous encoding is the primary difference between variational
control frameworks arising from reinforcement learning and from active
inference, and this fact speaks to the difference in underlying
cognitive philosophy between these theories where reinforcement learning
draws heavily from cognitivist and representational traditions in
artificial intelligence which prize principled, independent, and modular
systems, while active inference comes from a heavily embodied and
enactive viewpoint influenced by dynamical systems theory whereby
systems are seen primarily in terms of their situatedness within a
action-perception sensorimotor loop, and there is not necessarily any
clean distinction between phases or subsystems of this loop.

Finally, we have seen that the second axis of variation is whether the
goals are encoded into the generative model or the variational
distribution. The effects of this difference are subtler than for the
exogenous vs endogenous encoding dichotomy. Variational and generative
encodings are equivalent up to a sign difference in the exogenous case
since the encoding does not affect the veridicality of the distribution,
while in the endogenous case this causes subtle differences such that
the generative and variational encodings are typically equivalent up to
the extrinsic value terms which becomes the reverse-KL for the
variational encoding relative to the generative. This leads to a close
connection with pseudolikelihood methods in reinforcement learning (
abdolmaleki2018maximum ) and in fact provides a generalisation of these
methods to the POMDP setting.

#### 5.4.2 General Graphical Models

In all previous work, we have primarily focused on how the algorithms
and functionals differ when placed into a POMDP setting with visible
observations and unobserved (Markovian) hidden states, and actions which
affect the hidden states. However this is fundamentally a modelling
choice. For instance, if state information is perfectly observed, as is
often assumed in RL, then the problem reduces to a simple MDP
formulation. The MDP formulation with exogenous rewards was addressed in
Equation 5.4.1 , however when encoding rewards endogenously, this must
be encoded into the generative or variational distributions, so that
with the FEEF objective functional the extrinsic term becomes simply a
state divergence.

  -- -------- -- -- --------
     @xmath         
                    (5.44)
  -- -------- -- -- --------

Here no information gain is possible since states are directly observed,
thus the only exploration is through random entropy maximizing terms.
Importantly, if the goals were instead encoded into the variational
distribution, the only effect this would have would be to flip the
extrinsic value KL into a reverse-KL and thus induce mode-seeking rather
than mean-seeking distribution matching behaviour. In the MDP setting,
then, we obtain a maximum-entropy KL-control objective.

While it is possible to restrict the generative models only to MDPs, we
can also consider extending them to also explicitly model the prior and
posterior distributions of parameters underlying the variational and
generative distributions. For instance, we have implicitly been
utilising a variational posterior @xmath , a transition model @xmath , a
likelihood model @xmath , an action policy @xmath and an action prior
@xmath . All of these distributions may have parameters, or be
parameterised by a flexible function approximator such as a neural
network, which itself has parameters. These parameters can be be
included in the variational inference procedure by adding a generative
model and variational distribution over the parameters. For instance,
supposing we have some set of parameters @xmath which parametrise the
transition model such that the transition model becomes @xmath , then we
can pose an inference problem and write down a variational free energy
functional which also includes a model over parameters:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
                          (5.45)
  -- -------- -------- -- --------

We thus see that minimizing the variational free energy directly,
requires minimizing the divergence between posterior and prior beliefs
over the parameters, thus implicitly penalising updates which cause
large parameter updates, and thus disincentivising exploration. In
general, by adding additional variables to the variational free energy,
we simply obtain additional divergence terms to be minimized as above,
essentially penalising deviations between the posterior and prior
beliefs for that variable. However, and analogously with the states,
when we utilize a FEEF objective functional, we can obtain a parameter
information gain term as well as a countervailing parameter
approximation error term as shown below:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (5.46)
  -- -------- -------- -- --------

In the general case adding additional variables to the FEEF objective
will create an information gain term in that variable as well as the
counteracting posterior divergence term. Analogously, adding additional
variables to the class of divergence functionals will also create
information gain terms in those variables without the posterior
divergence. The reason for this behaviour can ultimately be derived
directly from the variational marginal entropy by considering augmenting
it with parameters @xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (5.47)
  -- -------- -------- -- --------

Thus we have seen that by implicitly maximizing the marginal entropy, we
in fact are implicitly optimising the information gain for any latent
variables in the model. Similarly, as discussed previously, the primary
difference between exogenous and endogenous encoding of value is that
endogenous encodings lack a degree of freedom that exogenous encodings
can make use of. In general therefore, as we augment the graphical
models these functionals are derived from with additional variables, we
see that the exogenous encoding is roughly equivalent to the endogenous
encoding with an additional degree of freedom. This thus derives two
‘scaling laws’ for our families of functionals as additional sets of
variables are added to the functional – that the exogenous encoding will
approximate the endogenous encoding with an additional degree of
freedom, and that with VFE derived functionals we will obtain divergence
terms to be minimized between the posterior and prior for the variable,
with a FEEF functional we will obtain an information gain term and a
posterior approximation error term, while with a divergence based
functional we will obtain an information gain term only in the new
variable.

### 5.5 Discussion

In this chapter, we have answered and discussed two key questions.
Firstly, we now understand the mathematical origin of
information-seeking exploration terms in variational objectives for
control. While this appears arcane, this is actually a question with
deep implications both philosophically, as well as for applications. On
a philosophical and mathematical note, we have uncovered the principled
mathematical origin of information-maximizing exploration. We see that
it arises from minimizing divergences between a predicted and a desired
distribution, and specifically from the predicted entropy maximization
half of the divergence objective. In effect, we see that it is by trying
to maximize the entropy of future observations, that induces information
seeking exploration in latent space, whenever any latent states or
parameters are added to the generative model. This makes intuitive sense
– if the future is broad and entropic, it contains much information, and
to maximize that breadth entails learning about the world in order to
ensure a precise match everywhere between predicted distribution and a
complex desire distribution. Conversely, we have seen that standard
evidence objectives used in variational inference or control result in
effectively trying to match the predicted and desired distribution while
trying to minimize the entropy of future states or, alternatively, to
make the future maximially predictable as well as in conformity to the
agent’s desires. As such, the agent’s goal is to effectively minimize
the amount of information it receives about the world, learning only as
much as is sufficient for control, and thus not giving rise to any kind
of information-seeking exploratory behaviour. We can thus understand
precisely why standard objectives such as control as inference are
insufficient to obtain information seeking behavioural objectives.
Similarly, our approach allows us to understand and rationalize a number
of approaches in the literature ( sun_planning_2011 ;
oudeyer2009intrinsic ; klyubin2005empowerment ) which add additional
exploratory terms to their reward-maximizing agents, with the heuristic
justification that they should increase exploration. Indeed, many such
approaches are implicitly minimizing a divergence objective without
realizing it. Our advances here make this practice explicit and allows
one to understand the precise mathematical nature of the objective being
optimized. Moreover, this distinction also sheds light on the possible
objective functions used by biological creatures such as humans in
psychological or behavioural economics tasks, for instance, one can
understand the otherwise-puzzling phenomenon of probability matching, as
the inevitable outcome of optimizing a divergence functional, and this
elegantly explains both why it is present and additionally, why it is
beneficial – since optimizing this objective in more complex
environments naturally leads to exploratory information-seeking
behaviour which often will outperform pure reward maximization even on
its own terms.

This approach is also useful for applications, since we can begin by
deriving a variety of methods using divergence objectives and
understanding their exploratory behaviour. While little work has yet
been done on explicitly divergence minimizing agents, we believe that
this will be an important area in the future, as successful protocols
and algorithms for exploration in reinforcement learning will become
increasingly important as the sparsity, dimensionality, and difficulty
of RL benchmark tasks increases.

Secondly, we understand the relationship between control as inference
and active inference. We know that the key difference is simply a
difference in objective function between the two (the EFE vs the FEF),
and secondly a distinction between them is their encodings of value –
that active inference uses an endogenous, while control as inference
uses an exogenous encoding. Moreover, understanding the distinctions
between the two theories, as well as the general and broad distinction
between evidence and divergence objectives, then allows us to raise our
eyes and perceive a much larger vista of the full landscape of potential
objective functionals for control. In the latter half of this chapter,
we have seen that these functionals can vary along two orthogonal
dimensions – whether an evidence or divergence functional is used, as
well as the nature of their encoding of value (endogenous or exogenous)
as well as whether value is encoded into the variational or generative
distribution. Moreover, we have derived a good understanding of the
impact of different definitions of the generative model upon the
objective functionals that result. This allows us a broad and unique
understanding of the possible space of objective functionals, as well as
the design choices which influence which one to choose. Future work in
this area should investigate the actual impacts of different choices on
agent behaviour, both in simple toy environments where the effects can
be easily understood, as well as in more complex and difficult benchmark
tasks where using different objectives may well give rise to algorithms
which are more effective than current agents. As an example, while only
using random exploration, control as inference inspired approaches such
as the soft-actor-critic have given rise to state of the art
performance. We see no reason why more exploratory information-seeking
agents, powered by divergence objectives, should not lead to similar
gains in performance, especially on challenging sparse-reward tasks.
While we present some preliminary results to this effect in Chapter 4,
much more work remains to be done to pin down what gains, if any, are
possible by this approach.

Finally, our approach directly gives us the objective functionals
utilizing different generative models. For instance, if you want to
extend your reinforcement learning algorithms to POMDPs, or POMDPs with
hierarchical levels of latent states, or to explicitly model
distributions over different model parameters, or to explicitly model a
reward distribution or reward model, then our framework provides a
recipe for precisely and immediately deriving the necessary objective to
optimize.

In the next chapter, we move on to consider applications of the free
energy principle to learning, where we focus on deriving novel
algorithms which can perform credit assignment in neural networks in a
biologically plausible fashion.

## Chapter 6 Credit Assignment in the Brain

### 6.1 Introduction

In this chapter we shift gears again and now consider applications of
the free energy principle to the problems of learning in the brain.
Specifically, here we aim to understand the nature of credit assignment
in the brain, and focus on how and whether the backpropagation of error
algorithm which underpins all the recent successes of machine learning
in training deep artificial neural networks, could potentially be
implemented in the brain. In this chapter, we present the fruit of our
work investigating this extremely important question for the case of
rate-coded integrate and fire neurons engaged in a static task (such as
object recognition), where there is only a feedforward pass to be
concerned with and all backpropagation is through space, and not time.
While this setting is considerably simplified from the one the brain
faces in reality, it is also much more tractable and well-understood and
solving the problem in this domain may provide vital clues into the full
solution.

This chapter is split into four relatively independent sections. In the
first section, we provide a general introduction and mini literature
review on backpropagation and previous attempts to derive biologically
plausible algorithms to implement backpropagation in the brain. In the
next two sections, we then present our work deriving new algorithms for
biologically plausible approximations to the backpropagation of error
algorithm.

In the first section, we show that predictive coding – the free energy
process theory from chapter 3 – can, if set-up correctly, exactly
approximate the backpropagation of error algorithm along arbitrary
computation graphs. This result is fascinating since predictive coding
has a long history and well-developed literature on its properties,
performance, and especially its biological plausibility, as well as
possessing several well-developed theoretical neural implementations (
bastos2012canonical ; keller2018predictive ; kanai2015cerebral ) . We
empirically validate this approximation to backprop and showcase that
predictive coding can perform equally to backprop at training complex
machine learning architectures such as CNNs and LSTMs.

Secondly, we develop a novel algorithm – Activation Relaxation (AR) --
which also can asymptotically converge to the required backpropagation
error gradients using only local connectivity -- and which does not
require two separate population of value and error neurons. We
empirically show that this algorithm can train complex machine learning
architectures with performance equal to backprop and, additionally,
demonstrate that the same relaxations shown in Chapter 3 for predictive
coding -- such as using learnable backwards weights to overcome the
weight transport problem, and dropping the nonlinear derivatives also
work for the AR algorithm, thus importantly both substantially improving
the overall biological plausibility of the AR algorithm as well as
demonstrating the generalizability of the results in Chapter 3 to other
algorithms ¹ ¹ 1 For this thesis, the AR algorithm is not directly
related to the FEP, although it was initially inspired by research into
predictive coding which is a process theory of the FEP .

Finally, in the third section, we have included a more speculative
discussion on a potential further algorithm for solving backprop in
rate-coded neurons directly, instead of in an iterative fashion. We
discuss the possible limitations of this algorithm as well as the
required neural circuitry and, for the first time, begin to precisely
understand what the key problems are for the rate-coded sense and also
what a real solution would look like.

#### 6.1.1 Backpropagation in the Brain

Due to the immense success of machine learning approaches based upon
connectionist deep neural networks trained upon the backpropagation of
error algorithm, our paradigms of how the brain functions is also
shifting. Specifically, the paradigm that the brain, or at least the
neocortex, is fundamentally a blank-slate learning machine which uses
general purpose learning algorithms to handle inputs, akin to a deep
neural network is becoming increasingly influential in neuroscience,
partially displacing older views that the brain consists of a series of
separate ‘modules’ ( fodor1983modularity ; pinker2003language ) , each
of which performs a single specialized function using what are
effectively specific and pre-set algorithms, hard-coded over the course
of evolutionary history. While functional specialisation is an extremely
notable characteristic of the brain, it is more widely believed that
this specialisation, especially in the cortex, is due to differences in
input and small inductive biases shaping the nature and output of a very
general learning algorithm which is implemented throughout the cortex,
rather than each functional module possessing its own independent and
isolated suite of algorithms. This view is supported by evidence of a
remarkable uniformity of cortical cytoarchitecture and neuroanatomy,
belied by the heterogeneity observed in subcortical areas (
bear2020neuroscience ) .

While, for a long time, it was empirically unclear whether the
fundamentals of intelligence – such as robust and generalizable
perception, natural language capabilities, and adaptive action planning
– could emerge solely from learning algorithms with relatively few
inductive biases, applied to vast amounts of data, the past decade and
its immense advances in machine learning are suggestive that this may be
possible after all. Modern machine learning, effectively, represents the
culmination and empirical verification of earlier connectionist theory (
rumelhart1986learning ) .

This view places the central focus on learning. Since the
backpropagation of error algorithm has proven so immensely successful in
machine learning, to train an extremely wide variety of architectures to
perform an impressive array of tasks ( krizhevsky2012imagenet ;
goodfellow2014generative ; radford2019language ;
schrittwieser2019mastering ; schmidhuber1999artificial ) , and given
that the brain itself faces an almost identical credit assignment
problem in having to adjust synaptic strengths to allow for learning to
occur, it is a very interesting and important question to ask whether
the learning algorithm implemented in the brain could simply be
backprop. If this question were conclusively answered in the
affirmative, it would represent an enormous conceptual breakthrough in
neuroscience since it would provide, for the first time, a general and
powerful organizing principle for the brain (or at least the cortex) as
a whole, it would allow the importation directly into neuroscience of a
large quantity of results from machine learning. Such an answer would
additionally have deep philosophical implications. It would imply that
there is very little effective difference between current machine
learning methods and the kinds of learning, inference, and planning
algorithms that are implemented in the brain to give rise to undeniably
intelligent and apparently conscious behaviour, and as such would imply
that the current paradigm in machine learning suffices, with more scale
and potentially more expressive architectures, to create fully general
intelligences akin to humans or beyond ( bostrom2017superintelligence )
.

On the other hand, if it were shown that the brain were conclusively not
doing backprop, then this would also be an advance, although a lesser
one, in neuroscience. Such a conclusion would necessarily shed light
upon the actual algorithms utilized by the brain for credit assignment
and learning, which would provide both a general principle for
understanding the function and operation of the brain over time, as well
as undoubtedly provide key insights for machine learning in developing,
improving, and scaling up current methods.

The theory and algorithm for backpropagation of error (Backprop) emerges
in the 1970s ( linnainmaa1970representation ) , and by the 1980s was
widely used for training connectionist neural networks (
rumelhart1985feature ; rumelhart1986learning ; griewank1989automatic )
Already in the 1980s, researchers had proposed that backprop could be
implemented in the brain, and tried to find commonalities between then
contemporary neuroscience and progress in connectionist modelling using
neural networks ( rumelhart1986learning ) . However, a number of
articles argued convincingly that a direct implementation of
backpropagation is biologically implausible ( crick1989recent ) , which
dampened down potential interest in this connection considerably until
the question was re-raised by the successes of machine learning in the
2010s. Before investigating the ways in which backpropagation appears
biologically implausible, and how these issues might be addressed, we
first give a detailed introduction to the backprop algorithm.

First we must decide on some terminology. Neural networks are typically
trained to minimize some loss function @xmath . The credit assignment
problem concerns the computation of the derivatives of the loss with
respect to every parameter of the network @xmath . Backpropagation of
error is an algorithm that solves the credit assignment problem exactly
using the technique of Automatic Differentiation (AD) (
griewank1989automatic ; baydin2017automatic ; van2018automatic ;
paszke2017automatic ) . ² ² 2 There are other methods for computing
derivatives, such as finite differences, but they are less accurate and
more computationally costly than AD and are not generally used in
machine learning . Given these derivatives, the network can be trained
with the stochastic gradient descent algorithm @xmath . However, given
the gradients computed by backprop, other gradient algorithms are
possible including a variety of modified descent procedures such as
Nesterov Momentum ( nesterov27method ) , RMS-prop ( hinton2012neural )
and Adam ( kingma2014adam ) second order methods such as natural
gradients ( amari1995information ) and Gauss-Newton optimization, as
well as stochastic sampling methods such as stochastic langevin dynamics
( welling2011Bayesian ) , and Hamiltonian MCMC ( neal2011MCMC ) .

The fundamental mathematics underlying backprop is the chain rule of
calculus. Essentially, if we have a function – such as a neural network
– which consists of the composition of many differentiable functions,
then we can express the derivative of a complex composite function as a
product of the derivatives of all the component functions with respect
to one another. Suppose we have the forward function and loss function,

  -- -------- -- -------
     @xmath      
     @xmath      (6.1)
  -- -------- -- -------

where @xmath is the prediction outputted by the neural network, @xmath
is the number of layers, @xmath is the activation functions for each
layer and @xmath is the weights or parameters for each layer. @xmath is
the overall loss function, @xmath is the desired target outputs and
@xmath is the loss function. With this forward function, we can compute
the derivative of the loss with respect to any parameter set – for
instance @xmath using the chain rule,

  -- -- -- -------
           (6.2)
  -- -- -- -------

which allows us to express the derivative of a product of all the
derivatives of the intermediate component functions. In general, this is
possible for any function as long as every component function is
differentiable. We can represent any function as a computation graph
which is a graph where each intermediate step in the computation is a
vertex and each component function is an edge. While this example, and
most simple neural network, is simply a chain graph, other more complex
graphs are possible. If there are multiple paths through the graph, the
derivatives of the paths are summed together. As long as every component
function is differentiable, every one of the derivatives in Equation 6.2
can be explicitly computed and evaluated, thus allowing the full
derivative @xmath to be computed explicitly. Importantly, the
computational cost of such an evaluation is generally linear in the
number of component functions – and thus is of approximately the same
complexity as simply evaluating the function in the first place. AD
approaches like this, then, allow for the evaluation of derivatives of
any differentiable computation for a low and constant additional
computational cost – allowing for their widespread use within machine
learning.

There are two approaches in AD to computing the chain of derivatives as
in Equation 6.2 , which are called ‘forward-mode’ and ‘reverse-mode’ AD.
The difference between these methods is effectively whether the product
of derivatives is computed from right to left (forward-mode) or left to
right (reverse mode). Forward-mode effectively accumulates the gradients
starting with the initial jacobian @xmath and then moving leftwards down
the chain, in the same direction as the original function evaluation.
This allows derivative evaluation to take place in parallel with
original function evaluation through the use of ‘dual numbers’ (
griewank1989automatic ) which extend every number with an ‘derivative
part’ analogous to how complex numbers extend real numbers with a
complex part. Since dual-numbers can be evaluated in parallel with the
original function evaluation, the computational cost of forward-mode AD
is of the order of the input dimension and it has a constant memory
cost, since no intermediate products need to be stored in memory.

Reverse-mode AD, conversely, evaluates the chain of derivatives from
left to right. That is, it starts with and accumulates onto the vector
@xmath , which is also called the adjoint or pullback ³ ³ 3 In
continuous time, Equation 6.3 becomes the adjoint ODE . It then iterates
recursively ‘backwards’ through the chain using the following equation,

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

where the sum simply states that if there are multiple potential paths
in the graph, they should be summed together. Since it starts at the
‘end’ and works backwards, reverse-mode AD requires the function to be
evaluated first after which the derivatives can begin to be computed in
a backwards sweep, thus leading reverse-mode AD to use characteristic
forward (function evaluation) and backwards (derivative evaluation)
sweeps. Since all intermediate activities must be stored in memory,
reverse-mode AD has a memory cost linear in the number of component
functions, as well as a computational cost which scales with the
dimension of the output. Since neural networks typically have a scalar
output loss and very high dimensional inputs (such as image pixels),
reverse-mode AD is typically computationally cheaper and is the method
used in practice for training deep networks. Reverse mode AD also has
the advantage that it backpropagates the gradients back to where the
weights are directly, while forward-mode AD does compute the weights,
but they are all bunched up at the end of the graph by the loss, and
therefore would need, in a physical system such as the brain, to be
transmitted back to where the weights were originally as well.

Here, we focus primarily on the implementation of reverse-mode AD in the
brain due to its generally superior computational capabilities (and
avoidance of this weight locality problem for forward mode AD). It is
possible, however, that the brain may use some combination of forward
and reverse mode in practice. One especially appealing method is to use
reverse-mode AD to handle hierarchical networks – i.e. nonlocality in
space, while using forward mode AD to handle recurrent credit assignment
through time. Here forward mode AD has the clear advantage that the
gradients move forward in time at the same rate as the weights
themselves, so that there is no locality problem here. In fact, the
locality problem now afflicts reverse-mode AD which, in this
circumstance, needs to backpropagate gradients backwards through time ,
which is problematic. Here we do not address the temporal credit
assignment problem and focus entirely on backpropagation through space,
where we assume reverse-mode AD is the best approach.

Although reverse-mode AD is a well characterised algorithm, it is not at
all clear whether it can be implementated in the brain. Specifically,
backpropagation has three principal problems which make its apparent
biological plausibility dubious – firstly, backpropagation appears to
require non-local information transfer, since the gradient of the
synaptic weights depends on activity from the rest of the network which
ultimately leads to the final loss outcome. Secondly, if we look at the
specific case of a rate-coded standard integrate and fire model, whereby
the output is a function of the input and the synaptic weights,

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

then the derivative of the loss with respect to the pre-activations
becomes,

  -- -- -- -------
           (6.5)
  -- -- -- -------

which requires both the derivative of the activation function, which may
or may not be easy to compute locally, and also the transpose of the
feedforward weights @xmath . This transpose is problematic since
effectively it requires the backwards pass activations to be sent
‘backwards’ through the forwards weights – a process which is
biologically implausible. This problem is called the ‘weight transport
problem’. Finally, if we look at the update rule for the weights
themselves,

  -- -- -- -------
           (6.6)
  -- -- -- -------

which while it does depend on the pre-synaptic activations @xmath also
depends on the adjoint vector @xmath which is generally non-local.
Interestingly, the update rule specifically does not depend at all on
the post-synaptic activity, in contrast to the widely accepted view of
Hebbian plasticity being implemented in the brain, although it does
depend on the derivative of the post-synaptic activity with respect to
the weights @xmath , which may or may not be difficult to compute. The
issue first of computing the adjoint vector @xmath in a biologically
plausible manner, and then transmitting it to the required synapses,
since it is non-local, thus form the core issue standing in the way of
any biologically plausible implementation of reverse-mode AD. A final
issue relates to the need, in reverse-mode AD, for separate forward and
backwards phases, while the brain presumably needs to operate
continuously in time. It has been suggested that the brain’s rhythmic
oscillations ( buzsaki2006rhythms ) may allow it to ‘multiplex’ forward
and backward passes together, although this intuition has not yet, to my
knowledge, been made precise in the literature.

Due to a general understanding that backpropagation is biologically
implausible, much research has focused on other potentially more
biologically plausible methods by which the brain might learn. A large
amount of attention has focused on Hebbian update rules (
gerstner2002mathematical ) , which only utilize the post and pre
synaptic activities, based on the initial intuitions of Donald Hebb (
hebb1949first ) . A number of variants of Hebbian learning have been
proposed, and the full class of potential algorithms has been
exhaustively analysed in ( baldi2016theory ) . However, a key issue with
Hebbian learning is that, since it can only use local information in the
form of pre and post synaptic activities, it cannot incorporate
information about the distant loss function, and thus allow for the
precise goal-directed learning that backprop is capable of. In effect,
Hebbian learning can only capture and strengthen the local correlations
of firing rates across a network and thus, while it can often be used to
solve tasks in shallow networks with just a single or a few hidden
layers, it fails to scale successfully to deep layers (
lillicrap2019backpropagation ) .

A second approach is to use global neuromodulators as a part of a
‘three-factor’ learning rule which includes contributions from both the
pre-synaptic and post-synaptic activity as well as this ‘third-factor’ (
gershman2018uncertainty ) which is often conceptualised to be dopamine,
in light of the fact that dopaminergic connections from the mid-brain
are well-known to innervate large parts of the cortex ( daw2006cortical
) . These dopaminergic neurons could be signalling some kind of global
reward signal, inducing all neurons to increase their weight when a
positive reward is encountered and decrease it when a negative reward
occurs ( seung2003learning ; roelfsema2005attention ;
lillicrap2020backpropagation ) , in a procedure which is effectively
equivalent to policy gradient methods from reinforcement learning (
williams1989experimental ) . While such approaches can, asymptotically,
learn complex functions in deep neural networks, their key limitation is
that the gradient estimates they compute have extremely high variance,
since the global neuromodulator cannot distinguish whether a particular
weight helped give rise to a reward or not, and thus cannot provide
precise feedback like backprop. Instead it takes a substantial amount of
trials for the random noise provided by the contributions of all the
other neurons in the brain to be averaged out to get at the contribution
of just a single synaptic weight, which leads to slow and unstable
learning in complex tasks with deep networks (
lillicrap2019backpropagation ) . Additionally, this approach implicitly
assumes that the entire brain is optimized end-to-end for reward,
however this seems potentially unlikely given that large parts of the
cortices deal with aspects like sensory stimuli which are distant from
reward and most likely use auxiliary losses such as their own immediate
prediction errors. Nevertheless, if it turns out that backpropagation is
not, in fact, used in the brain, then global neuromodulatory rules like
this may be the second best option. Importantly, even if it is the case
that the brain does backprop, it likely also uses this global
neuromodulatory approach in a modulatory function to bias learning
towards high reward contingencies and perhaps also to adaptively tune
learning rates throughout the cortex so that highly valenced experiences
(either positive or negative) have a strong effect on plasticity
throughout the brain.

Finally, there is also a small but growing literature attempting to
understand how and whether backpropagation can be directly implemented
in the brain – namely whether it is possible to design neural circuits
to work around the key limitations proposed earlier. An important line
of work tackles the weight transport problem. lillicrap2016random
demonstrate that in reality precise copying of the forward and backwards
weights is not necessary, and in fact random fixed backward weights
suffice due to the phenomenon of ‘feedback alignment’ whereby the random
feedback weights effectively force the forward weights to align with the
backward ones to be able to learn. This approach can be improved by
ensuring that the sign of the elements of the feedback weight matrix
matches the sign of the forward weight matrix (potentially a more
biologically plausible constraint than exact copying of values) (
liao2016important ) , or else learning the backwards weights using an
additional plasticity rule ( amit2019deep ; akrout2019deep ;
millidge2020relaxing ) . While the feedback alignment technique does not
typically scale to deep architectures, as the feedback path becomes
increasingly corrupted ( bartunov2018assessing ) , an approach called
direct feedback alignment (DFA) ( nokland2016direct ) , whereby all
layers are directly connected to the output layer through random
backwards weights, has been shown to be able to scale to large deep
architectures ( launay2019principled ) , although not the usual
convolutional neural networks used in vision. While an impressive
result, DFA itself violates known neural connectivity constraints which
feature reciprocal connectivity between regions and not every layer
receiving direct backwards connections from the ‘output’.

Secondly, another line of work has focused on the algorithm of
target-propagation ( bengio2015early ; lee2015difference ) which is
similar to backprop except that instead of providing gradients back to
the weights at each layer it provides targets which can then be
optimized locally. These targets are produced by mapping backwards the
output of the network ‘nudged’ towards the true target through an
inverse mapping at each layer. The intuition, is that we want to find
the targets which, if the lower layers had matched their activations,
would have produced a final output closer to the target. The past year
has seen substantial advances in the theoretical analysis of
target-prop, where it is now recognised to not approximate backprop, but
instead be performing a hybrid form of Gauss-Newton optimization (
bengio2020deriving ; meulemans2020theoretical ) . While promising,
large-scale studies on the stability and scalability of target-prop
learning have not yet been done, although initial results are promising
( bartunov2018assessing ) . Additionally, target-prop does not provide
solutions to the weight transport problems (now complicated by the
additional necessity of learning the backwards inverse weights), and the
necessity of storing and comparing the forward and backward information
across phases. The intuition of using the final loss function to compute
layer-wise targets has also been applied, with variations, in other
works ( ororbia2019biologically ; ororbia2017learning ;
kaiser2020synaptic ) .

Another approach is to use only local information at the synapses, but
use a backwards phase which instead of being purely sequential is
treated as a dynamical systems which undergoes multiple iterations.
Using this approach, while it takes longer than a sequential backwards
pass, also allows using only local information, since the information
about the loss can be ‘leaked’ slowly backwards using the dynamics over
time instead of having to be explicitly transmitted backwards. This
allows the brain to operate using the same dynamical rules at all times,
in general, instead of sequential forwards and backwards transmission of
information. One key example of such a framework is the algorithm of
Equilibrium Propagation ( bengio2017stdp ; scellier2017equilibrium ;
scellier2018extending ; scellier2018generalization ) , which uses two
dynamical phases – a free phase where the dynamics of the system are
allowed to evolve without any influence of the targets, and a clamped
phase in which the output units are held at a value nudged towards the
targets, which destabilizes the free-phase equilibrium and instead sends
the network towards a different clamped equilibrium state. It turns out
that the difference between these two states corresponds closely to the
gradients which would otherwise have been backpropagated through the
network and can thus be used to adjust the synaptic weights (
scellier2017equilibrium ) . Equilibrium propagation has been extensively
tested on small datasets like MNIST and CIFAR, although it is not yet
known how well the method scales. Additional problems with EP are its
necessary use of two distinct backwards phases and, crucially, the
storage of information (the equilibrium in the free-phase, throughout
the entirety of the clamped phase before their subtraction to obtain the
gradients. Another iterative algorithm which has been shown to
approximate backprop is predictive coding ( whittington2017approximation
) .

In this thesis chapter, we make two contributions to the theory of
iterative algorithms for approximating backprop. Firstly, we extend work
by whittington2017approximation , showing that predictive coding can
approximate backprop by making this claim precise and exact, and
extending it to arbitrary computation graphs ( millidge2020predictive )
. Specifically, we show that predictive coding provides a fully general
iterative approach to approximating reverse-mode automatic
differentiation through an identification of the equilibrium prediction
error with the adjoint term. This allows us to define predictive coding
networks which can train any contemporary machine learning architecture
with accuracy equivalent to backprop in a local and biologically
plausible (ish) manner. We demonstrate this capability on CNNs and
LSTMs, thus substantially extending the range and scale of architectures
to which predictive coding has been applied.

Secondly, we propose a novel iterative algorithm – Activation Relaxation
( millidge2020activation ) – which converges precisely to the exact
backprop gradients, while also considerably simplifying the predictive
coding update rules and obviating the need for separate populations of
error and ‘value’ neurons which predictive coding possesses.
Additionally, we demonstrate that certain remaining implausibilities in
the algorithm such as the weight transport problem and the nonlinear
derivatives problem can be ‘relaxed’ while retaining learning
performance almost equivalent to backprop, even on challenging and
large-scale computer vision tasks.

A final issue which undermines many of the proposed learning rules for
both sequential methods like target-prop and iterative ones like EP or
predictive coding, is the necessity of three-factor learning rules with
precise vector feedback, like backprop. It is still fairly unclear
whether such rules can actually be implemented in the brain, although
there has been some work showing that prediction error, or gradient like
quantities could in theory be transmitted backwards through the network
using segregated dendrites ( sacramento2018dendritic ) which may help
maintain separate error representations independently of the firing
rates of the rest of the somatic neuron. While the actual biological
plausibility of this approach is unclear, in the last section of this
chapter, we will speculate that if it is plausible, and the brain can
maintain and update separate error and value representations on single
neurons, or indeed on separate populations but with precise three-factor
learning rules, then that is all that is necessary for a direct
biologically plausible implementation of backprop, especially given
recent research showing that the weight transport problem can be largely
overcome through learning the backwards weights. We present a
theoretical algorithm, extremely similar to target-prop, and with
three-factor learning rules, which precisely corresponds to backprop in
the brain, which is mathematically equivalent to backprop. Thus, if
three-factor learning rules are possible in the brain, either through
segregated dendrites, or else precise interneuron connectivty, then
exact backpropagation can be performed as simply as target-prop or any
other algorithm.

### 6.2 Predictive Coding Approximates Backprop Along Arbitrary
Computation Graphs

Here we demonstrate that predictive coding can approximate
backpropagation on arbitrary computation graphs. As we recall from
chapter 5, predictive coding arises from a variational inference
algorithm on the activations on each layer of the hierarchy, whereby the
predictive coding update rules can be derived as a gradient descent on
the variational free energy @xmath . To showcase how this methodology
extends to arbitrary graphs, we must define the variational inference
problem to be solved on an arbitrary computation graph. First, we must
make the notion of a computation graph explicit.

A computation graph @xmath is a directed acyclic graph (DAG) which can
represent the computational flow of essentially any program or
computable function as a composition of elementary functions. Each edge
@xmath of the graph corresponds to an intermediate step – the
application of an elementary function – while each vertex @xmath is an
intermediate variable computed by applying the functions of the edges to
the values of their originating vertices. @xmath denotes the vector of
activations within a layer and we denote the set of all vertices as
@xmath . Effectively, computation flows ‘forward’ from parent nodes to
all their children through the edge functions until the leaf nodes give
the final output of the program as a whole (see Figure 6.1 and 6.2 (top)
for an example). Given a target @xmath and a loss function @xmath , the
graph’s output can be evaluated and, and if every edge function is
differentiable, automatic differentiation can be performed on the
computation graph.

We can extend predictive coding to arbitrary computation graphs in a
supervised setting by defining the inference problem to be solved as
that of inferring the vertex value @xmath of each node in the graph
given fixed start nodes @xmath (the data), and end nodes @xmath (the
targets). We define a generative model which parametrises the value of
each vertex given the feedforward prediction of its parents, @xmath ⁴ ⁴
4 This includes the prior @xmath ), which simply has no parents. , and a
factorised, variational posterior @xmath , where @xmath denotes the set
of parents and @xmath denotes the set of children of a given node @xmath
. From this, we can define a suitable objective functional, the
variational free energy @xmath (VFE), which acts as an upper bound on
the divergence between the true and variational posteriors.

  -- -------- -------- -------- -------
     @xmath   @xmath            (6.7)
                       @xmath   
  -- -------- -------- -------- -------

Under Gaussian assumptions for the generative model @xmath , and the
variational posterior @xmath , where the ‘predictions’ @xmath are
defined as the feedforward value of the vertex produced by running the
graph forward, and all the precisions, or inverse variances, @xmath are
fixed at the identity, we can write @xmath as simply a sum of prediction
errors (see chapter 5), with the prediction errors defined as @xmath .
Since @xmath is an upper bound on the divergence between true and
approximate posteriors, by minimizing @xmath , we reduce this
divergence, thus improving the quality of the variational posterior and
approximating exact Bayesian inference. Predictive coding minimizes
@xmath by employing the Cauchy method of steepest descent to set the
dynamics of the vertex variables @xmath as a gradient descent directly
on @xmath

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

The dynamics of the parameters of the edge functions @xmath such that
@xmath , can also be derived as a gradient descent on @xmath . In a
neural network model, the parameters correspond to the synaptic weights
of each layer of the neural network. Importantly these dynamics require
only information (the current vertex value, prediction error, and
prediction errors of child vertices) locally available at the vertex.

  -- -------- -------- -- -------
     @xmath   @xmath      (6.9)
  -- -------- -------- -- -------

To run generalized predictive coding on a given computation graph @xmath
, we augment the graph with error units @xmath to obtain an augumented
computation graph @xmath . The predictive coding algorithm then operates
in two phases – a feedforward sweep and a backwards iteration phase. In
the feedforward sweep, the augmented computation graph is run forward to
obtain the set of predictions @xmath , and prediction errors @xmath for
every vertex. To achieve exact equivalence with the backprop gradients
computed on the original computation graph, we initialize @xmath in the
initial feedforward sweep so that the output error computed by the
predictive coding network and the original graph are identical – an
assumption we call the fixed prediction assumption.

In the backwards iteration phase, the vertex activities @xmath and
prediction errors @xmath are updated with Equation 6.8 for all vertices
in parallel until the vertex values converge to a minimum of @xmath .
After convergence the parameters are updated according to Equation 6.9 .
Note we also assume, following whittington2017approximation , that the
predictions at each layer are fixed at the values assigned during the
feedforward pass throughout the optimisation of the @xmath s. This is
the fixed-prediction assumption . In effect, by removing the coupling
between the vertex activities of the parents and the prediction at the
child, this assumption separates the global optimisation problem into a
local one for each vertex. We implement these dynamics with a simple
forward Euler integration scheme so that the update rule for the
vertices became @xmath where @xmath is the step-size parameter.
Importantly, if the edge function linearly combines the activities and
the parameters followed by an elementwise nonlinearity, then both the
update rule for the vertices (Equation 6.8 ) and the parameters
(Equation 6.9 ) become Hebbian. Specifically, the update rules for the
vertices and weights become @xmath and @xmath , respectively.

Dataset: @xmath , Augmented Computation Graph @xmath , inference
learning rate @xmath , weight learning rate @xmath for @xmath do

@xmath

for @xmath do

@xmath

end for

@xmath

while not converged do

for @xmath do

@xmath

@xmath

end for

end while

for @xmath do

@xmath

end for

end for

Algorithm 4 Generalized Predictive Coding

#### 6.2.1 Methods and Results

To demonstrate that this predictive coding scheme approximates the
backpropagation of error algorithm at convergence is fairly
straightforward. The key step is to show that the recursion relationship
of the adjoint term for the equilibrium of the prediction errors in
predictive coding is identical to that in reverse-mode AD, even for
arbitrary computation graphs. To make this clear more intuitively, we
first demonstrate that, at the equilibrium of the dynamics, the
prediction errors @xmath converge to the correct backpropagated
gradients @xmath , and consequently the parameter updates (Equation 6.9
) become precisely those of a backprop trained network.

First, under the fixed prediction assumption, we can directly solve for
the equilibrium of the dynamics by setting the time derivative to 0,

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

If we compare this to the recursive relationship inherent to
reverse-mode AD (Equation 6.3 ), we can see that the prediction errors
satisfy the same recursive relationship. Since this relationship is
recursive, all that is needed for the prediction errors throughout the
graph to converge to the backpropagated derivatives is for the
prediction errors at the final layer to be equal to the output gradient:
@xmath . To see this explicitly, consider a mean-squared-error loss
function ⁵ ⁵ 5 While the mean-squared-error loss function fits most
nicely with the Gaussian generative model, other loss functions can be
used in practice. If the loss function can be represented as a log
probability distribution, then the generative model can be amended to
simply set the output distribution to that distribution. If not, then
there is no fully consistent generative model (although all nodes except
the output remain Gaussian), but the algorithm will still work in
practice. See Figure 6.4 for results for CNNs trained with a
crossentropy loss. . at the output layer @xmath with T as a vector of
targets, and defining @xmath . We then consider the equilibrium value of
the prediction error unit at a penultimate vertex @xmath . By Equation
6.10 , we can see that at equilibrium,

  -- -------- -------- -- --------
     @xmath   @xmath      (6.11)
  -- -------- -------- -- --------

since, @xmath , we can then write,

  -- -------- -------- -- --------
     @xmath   @xmath      (6.12)
  -- -------- -------- -- --------

Thus the prediction errors of the penultimate nodes converge to the
correct backpropagated gradient. Furthermore, recursing through the
graph from children to parents allows the correct gradients to be
computed ⁶ ⁶ 6 Some subtlety is needed here since @xmath may have many
children which each contribute to the loss. However, these different
paths sum together at the node @xmath , thus propagating the correct
gradient backwards. . Thus, by induction, we have shown that the fixed
points of the prediction errors of the global optimization correspond
exactly to the backpropagated gradients. Intuitively, if we imagine the
computation-graph as a chain and the error as ‘tension’ in the chain,
backprop loads all the tension at the end (the output) and then
systematically propagates it backwards. Predictive coding, however,
spreads the tension throughout the entire chain until it reaches an
equilibrium where the amount of tension at each link is precisely the
backpropagated gradient.

By a similar argument, it is apparent that the dynamics of the
parameters @xmath as a gradient descent on @xmath also exactly match the
backpropagated parameter gradients.

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.13)
  -- -------- -------- -- --------

Which follows from the fact that @xmath and that @xmath .

To demonstrate empirically that this approach works, we present a
numerical test in the simple scalar case, where we use predictive coding
to derive the gradients of an arbitrary, highly nonlinear test function
@xmath where @xmath is an arbitrary parameter. For our tests, we set
@xmath to 5 and @xmath to 2. The computation graph for this function is
presented in Figure 6.2 . Although simple, this is a good test of
predictive coding because the function is highly nonlinear, and its
computation graph does not follow a simple layer structure but includes
some branching. An arbitrary target of @xmath was set at the output and
the gradient of the loss @xmath with respect to the input @xmath was
computed by predictive coding. We show (Figure 6.2 ) that the predictive
coding optimisation rapidly converges to the exact numerical gradients
computed by automatic differentiation, and that moreover this
optimization is very robust and can handle even exceptionally high
learning rates (up to 0.5) without divergence.

Secondly, we wish to show empirically that this approach can be used to
train deep neural network architectures, of the kind used in machine
learning, up to high levels of performance equivalent to those trained
with backprop. First, we demonstrate a predictive coding CNN model.
Convolutional neural networks have been a cornerstone of machine
learning since the pioneering demonstrations of their power on ImageNet
by ( krizhevsky2012imagenet ) , and are still widely used as state of
the art models for image processing.

The key concept in a CNN is that of an image convolution, where a small
weight matrix is slid (or convolved) across an image to produce an
output image. Each patch of the output image only depends on a
relatively small patch of the input image. Moreover, the weights of the
filter stay the same during the convolution, so each pixel of the output
image is generated using the same weights. The weight sharing implicit
in the convolution operation enforces translational invariance, since
different image patches are all processed with the same weights.

The forward equations of a convolutional layer for a specific output
pixel

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

Where @xmath is the @xmath th element of the output, @xmath is the
element of the input image and @xmath is an weight element of a feature
map. To set-up a predictive coding CNN, we augment each intermediate
@xmath and @xmath with error units @xmath of the same dimension as the
output of the convolutional layer.

Predictions @xmath are projected forward using the forward equations.
Prediction errors also need to be transmitted backwards for the
architecture to work. To achieve this we must have that prediction
errors are transmitted upwards by a ‘backwards convolution’. We thus
define the backwards prediction errors @xmath as follows:

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

Where @xmath is an error map zero-padded to ensure the correct
convolutional output size. Inference in the predictive coding network
then proceeds by updating the intermediate values of each layer as
follows:

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

The CNN weights can be updated using the simple Hebbian rule of the
multiplication of the pre and post synaptic potentials.

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

In our experiments we used a relatively simple CNN architecture
consisting of one convolutional layer of kernel size 5, and a filter
bank of 6 filters. This was followed by a max-pooling layer with a (2,2)
kernel and a further convolutional layer with a (5,5) kernel and filter
bank of 16 filters. This was then followed by three fully connected
layers of 200, 150, and 10 (or 100 for CIFAR100) output units. Each
convolutional and fully connected layer used the relu activation
function, except the output layer which was linear. Although this
architecture is far smaller than state of the art for convolutional
networks, our primary purpose here is to demonstrate the equivalence of
predictive coding and backprop. Further work could investigate scaling
up predictive coding to more state-of-the-art architectures.

Our datasets consisted of 32x32 RGB images. We normalised the values of
all pixels of each image to lie between 0 and 1, but otherwise performed
no other image preprocessing. We did not use data augmentation of any
kind. We set the weight learning rate for the predictive coding and
backprop networks 0.0001. A minibatch size of 64 was used. These
parameters were chosen without any detailed hyperparameter search and so
are likely suboptimal. The magnitude of the gradient updates was clamped
to lie between -50 and 50 in all of our models. This was done to prevent
divergences, as occasionally occurred in the LSTM networks, likely due
to exploding gradients.

First, we compare the test and training accuracy, as well as training
loss plots for convolutional CNNs trained on CIFAR10. Figure 6.4 shows
convincingly that the accuracy and indeed the training dynamics of
predictive coding and backprop are the same, to all intents and
purposes, thus demonstrating that predictive coding approaches can
approximate backprop to a very high accuracy, using only local learning
rules. To investigate this further, we explicitly plotted the divergence
between the gradient estimates produced by predictive coding and the
analytically correct gradients produced by backprop.

Importantly, we found that the divergence between the true and
predictive coding gradients was extremely small, and remained
approximately constant throughout training suggesting that predictive
coding networks do not suffer from accumulating errors in their gradient
approximation process. Importantly, to achieve this level of convergence
required 100 backwards iterations using an inference learning rate of
0.1. This means that the predictive coding has an approximately 100x
computational overhead compared to backprop – largely rendering it
uncompetitive for direct competition in serial computers. Nevertheless,
this choice of 100 iterations is on the high end of what is necessary,
since we are primarily concerned with showing the asymptotic
equivalence, and in reality the number of iterations required may be
substantially lower. Additionally, predictive coding is a fully parallel
algorithm unlike backprop, which must be implemented sequentially and is
a better fit for the highly parallel neural circuitry.

It is also important to note that while predictive coding ‘naturally’
uses the mean-squared error loss – so that the output error is a
standard prediction error, other loss functions as possible, such as the
widely used cross-entropy loss. Predictive coding can be
straightforwardly extended to cover other loss functions by simply
replacing the final prediction error @xmath with the gradient of the
loss function with respect to the outputs. Here we demonstrate that
predictive coding with a multi-class cross entropy loss also performs
equivalently to the network trained with backprop on the CIFAR and SVHN
datasets.

#### 6.2.2 RNN and LSTM

##### Rnn

We additionally tested a predictive coding RNN and LSTM. To train these
recurrent networks with predictive coding, we simply used the approach
of using predictive coding to approximate backpropagation through time
(BPTT) and applied predictive coding to the unrolled computation graph.
With long sequence lengths, this lead to extremely deep graphs for
predictive coding to train. Crucially, we demonstrate that predictive
coding’s ability to train such graphs is not impaired by their depth,
meaning that predictive coding as a training algorithm has an
exceptional scalability for extremely deep models of the kind
increasingly used in contemporary machine learning ( radford2019language
; he2016deep )

The computation graph on RNNs is relatively straightforward. We consider
only a single layer RNN here although the architecture can be
straightforwardly extended to hierarchically stacked RNNs. An RNN is
similar to a feedforward network except that it possesses an additional
hidden state @xmath which is maintained and updated over time as a
function of both the current input @xmath and the previous hidden state.
The output of the network @xmath is a function of @xmath . By
considering the RNN at a single timestep we obtain the following
equations.

  -- -------- -------- -- --------
     @xmath   @xmath      (6.18)
     @xmath   @xmath      (6.19)
  -- -------- -------- -- --------

Where f and g are elementwise nonlinear activation functions. And @xmath
are weight matrices for each specific input. To predict a sequence the
RNN simply rolls forward the above equations to generate new predictions
and hidden states at each timestep.

It is important to note that this is an additional aspect of biological
implausibility that we do not address in here. BPTT requires updates to
proceed backwards through time from the end of the sequence to the
beginning. Ignoring any biological implausibility with the rules
themselves, this updating sequence is clearly not biologically plausible
as naively it requires maintaining the entire sequence of predictions
and prediction errors perfectly in memory until the end of the sequence,
and waiting until the sequence ends before making any updates. There is
a small literature on trying to produce biologically plausible, or
forward-looking approximations to BPTT which does not require updates to
be propagated back through time ( williams1989learning ;
lillicrap2019backpropagation ; steil2004backpropagation ;
ollivier2015training ; tallec2017unbiased ) . While this is a
fascinating area, we do not address it here. We are solely concerned
with the fact that predictive coding approximates backpropagation on
feedforward computation graphs for which the unrolled RNN graph is a
sufficient substrate.

To learn a predictive coding RNN, we first augment each of the variables
@xmath and @xmath of the original graph with additional error units
@xmath and @xmath . Predictions @xmath are generated according to the
feedforward rules (16). A sequence of true labels @xmath is then
presented to the network, and then inference proceeds by recursively
applying the following rules backwards through time until convergence.

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (6.20)
  -- -------- -------- -- --------

Upon convergence the weights are updated according to the following
rules.

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (6.21)
  -- -------- -------- -- --------

Since the RNN feedforward updates are parameter-linear, these rules are
Hebbian, only requiring the multiplication of pre and post-synaptic
potentials. This means that the predictive coding updates proposed here
are biologically plausible and could in theory be implemented in the
brain. The only biological implausibility remains the BPTT learning
scheme.

Our RNN was trained on a simple character-level name-origin dataset
which can be found here: https://download.pytorch.org/tutorial/data.zip
. The RNN was presented with sequences of characters representing names
and had to predict the national origin of the name – French, Spanish,
Russian, etc. The characters were presented to the network as
one-hot-encoded vectors without any embedding. The output categories
were also presented as a one-hot vector. The RNN has a hidden size of
256 units. A tanh nonlinearity was used between hidden states and the
output layer was linear. The network was trained on randomly selected
name-category pairs from the dataset.

We first present the training and test accuracy for the backprop RNNs,
averaged over five seeds. In general, performance between the
backprop-trained and predictive coding networks was indistinguishable on
this task.

Additionally, The training loss for the predictive coding and backprop
RNNs, averaged over 5 seeds is presented below (Figure 6.6 ).

##### Lstm

Unlike the other two models, the LSTM possesses a complex and branching
internal computation graph, and is thus a good opportunity to make
explicit the predictive coding ‘recipe’ for approximating backprop on
arbitrary computation graphs. The computation graph for a single LSTM
cell is shown (with backprop updates) in Figure 6.8 . Prediction for the
LSTM occurs by simply rolling forward a copy of the LSTM cell for each
timestep. The LSTM cell receives its hidden state @xmath and cell state
@xmath from the previous timestep. During training we compute
derivatives on the unrolled computation graph and receive backwards
derivatives (or prediction errors) from the LSTM cell at time @xmath .
For a full and detailed set of equations specifying the complex LSTM
cell, see Appendix B.

The recipe to convert this computation graph into a predictive coding
algorithm is straightforward. We first rewire the connectivity so that
the predictions are set to the forward functions of their parents. We
then compute the errors between the vertices and the predictions.

During inference, the inputs @xmath , @xmath and the output @xmath are
fixed. The vertices and then the prediction errors are updated. This
recipe is straightforward and can easily be extended to other more
complex machine learning architectures. The full augmented computation
graph, including the vertex update rules, is presented in Figure 6.7 .

For the LSTM we also observed a close correspondence between the
performance (in terms of training and test accuracy) between the
predictive coding and backpropagation networks, thus demonstrating that
predictive coding can converge to the exact backprop gradients even on
exceptionally deep and complex computation graphs such as the LSTM

Importantly, we observed rapid convergence to the exact backprop
gradients even in the case of very deep computation graphs (as is an
unrolled LSTM with a sequence length of 100). Although convergence was
slower than was the case for CNNs or lesser sequence lengths, it was
still straightforward to achieve convergence to the exact numerical
gradients with sufficient iterations.

Below we plot the mean divergence between the predictive coding and true
numerical gradients as a function of sequence length (and hence depth of
graph) for a fixed computational budget of 200 iterations with an
inference learning rate of 0.05. As can be seen, the divergence
increases roughly linearly with sequence length. Importantly, even with
long sequences, the divergence is not especially large, and can be
decreased further by increasing the computational budget. As the
increase is linear, we believe that predictive coding approaches should
be scalable even for backpropagating through very deep and complex
graphs.

We also plot the number of iterations required to reach a given
convergence threshold (here taken to be 0.005) as a function of sequence
length (Figure 6.11 ). We see that the number of iterations required
increases sublinearly with the sequence length, and likely asymptotes at
about 300 iterations. Although this is a lot of iterations, the
sublinear convergence nevertheless shows that the method can scale to
even extremely deep graphs.

Our architecture consisted of a single LSTM layer (more complex
architectures would consist of multiple stacked LSTM layers). The LSTM
was trained on a next-character character-level prediction task. The
dataset was the full works of Shakespeare, downloadable from Tensorflow.
The text was shuffled and split into sequences of 50 characters, which
were fed to the LSTM one character at a time. The LSTM was trained then
to predict the next character, so as to ultimately be able to generate
text. The characters were presented as one-hot-encoded vectors. The LSTM
had a hidden size and a cell-size of 1056 units. A minibatch size of 64
was used and a weight learning rate of 0.0001 was used for both
predictive coding and backprop networks. To achieve sufficient numerical
convergence to the correct gradient, we used 200 variational iterations
with an inference learning rate of 0.1. This rendered the predictive
LSTM approximately 200x as costly as the backprop LSTM to run. A graph
of the LSTM training loss for both predictive coding and backprop LSTMs,
averaged over 5 random seeds, can be found below (Figure 6.12 ).

### 6.3 Interim Discussion

Here we have shown that predictive coding can be applied directly to
arbitrary computation graphs and can rapidly and effectively converge to
the exact gradients required for the backpropagation of error algorithm.
We have demonstrated this on deep and state of the art machine learning
architectures, thus achieving significantly greater scale than previous
works using predictive coding ( millidge2019implementing ;
orchard2019making ; whittington2017approximation ) . Moreover, the
predictive coding learning rule uses only local learning dynamics and
Hebbian weight updates in the case of the usual feedforward neural
networks (although they differ somewhat for the LSTM). Weights are
updated using only local prediction errors.

This approach also is innovative in that it phrases backprop in terms of
variational inference on the values of the nodes in the computation
graph. While it may seem just like a mathematical convenience, it
actually has deep implications. It draws another link between the
processes of optimization and variational inference, in a rather
different manner from that which has largely been explored before.
Instead of conceptualising inference as optimization, as is typically
done in variational inference, we instead conceptualize a core component
of optimization – credit assignment – purely in terms of inference.
While this duality has been considered before for two layer networks (
amari1995information ) , our approach is substantially more powerful and
general, by showcasing that it holds for arbitrary computation graphs.
Additionally, our approach provides an avenue for interesting
generalizations of backprop through the use of precision parameters in
predictive coding. Note that in our analysis, we have implicitly assumed
that all of the precision parameters are set to the identity @xmath , as
they do not feature in the learning and update rules, as they do in
Chapter 3. If we reintroduce precision in this context, we see that it
has the role of modulating gradient magnitudes – effectively
implementing an adaptive learning rate. What this means, intuitively, is
that we can think about precision weighting in this case as enabling an
uncertainty aware backprop , which specifically weights gradients by how
uncertain they are – or by their variance. Effectively, this method,
with learnable precisions, would down-weight highly variable and
uncertain gradients while upweighting those known to be certain. When
applied to the input this would mimic features of attention, by
downweighting noisy or otherwise uncertain inputs and having them play
little role in learning. While such an adaptive modulatory role for
precision may bring learning benefits, this must be explored further, as
the author hopes to do in future work.

Finally, it is worth discussing several drawbacks of the method. The key
one is its computational cost. The networks presented here were trained
with 100 dynamical iterations to converge to the prediction error
equilibrium before each weight update, giving predictive coding an
approximately 100x computational cost compared to backprop. This is
obviously highly significant and renders these approaches unusable for
large scale networks on serial Von-Neumann computers. While the brain
utilizes highly parallel circuitry, and may therefore be more suited to
such an iterative algorithm, there are still issues with requiring a
dynamical iteration for convergence. Specifically, such iterations still
require time and discrete phases so that the system cannot likely simply
operate in continuous time. Moreover, if too many iterations are
required, since the brain must respond to a continually changing world
instead of just single images presented in isolation, it may become
overwhelmed by events and fail computationally, if the input changes
faster than it can dynamically converge to a solution. While not
necessarily as severe as needing dynamical approaches for inference ,
which would entirely hamstring any response, here the dynamics are only
required for learning and weight updates, this may nevertheless prove to
be a substantial drawback of the method. Our method, like most others,
also requires two distinct phases which must either be somehow
coordinated explicitly, or multiplexed in the brain.

Additionally, the fixed-prediction assumption embedded in the model
requires maintaining the stored memory of the feedforward pass values
somewhere in the network throughout the backwards dynamical phase which
is potentially problematic in neural circuitry. Finally, although the
predictive coding learning rules are local, in the sense that they only
require information from the same layer, they still require information
from the prediction error units to be transmitted to the activity units,
where we assume the synaptic weights are located (although in a
segregated dendrite model they could just be in different dendrites (
sacramento2018dendritic ) ).

### 6.4 Activation Relaxation

Here we introduce a second iterative algorithm which approximates the
exact backpropagation gradients asymptotically at the equilibrium of a
dyanmical system. We call this algorithm Activation Relaxation (AR)
because of the nature of the update rules, which iteratively update the
activations of neurons in the iterative phase rather than prediction
errors. Crucially, the update rules proposed by AR are exceedingly
simple and elegant, and do not require additional populations of ‘error
neurons’ as in predictive coding, or multiple backwards phases as in
Equilibirium-prop. AR arises quite straightforwardly by trying to take a
first principles approach to the iterative backprop approximation
schemes.

To establish notation, we consider the simple case of a fully-connected
deep multi-layer perceptron (MLP) composed of @xmath layers of
rate-coded neurons trained in a supervised setting. The firing rates of
these neurons are represented as a single scalar value @xmath , referred
to as the neurons activation, and a vector of all activations at given
layer is denoted as @xmath . The activations of the hierarchically
superordinate layer are a function of the hierarchically subordinate
layers activations @xmath , where @xmath is the set of synaptic weights,
and the product of activation and weights is transformed through a
nonlinear activation function @xmath . The final output @xmath of the
network is compared with the desired targets @xmath , according to some
loss function @xmath . In this work, we take this loss function to be
the mean-squared-error (MSE) @xmath , although the algorithm applies to
any other loss function without loss of generality (see Appendix B). We
denote the gradient of the loss with respect to the output layer as
@xmath . In the case of the MSE loss, the gradient of the output layer
is just the prediction error @xmath .

Firstly, we know that the key quantity we wish to approximate is the
adjoint term @xmath for a given layer @xmath . If we know this adjoint,
and have it present somewhere in the local environment, then the
gradient with respect to the weights can be computed using only locally
available information. In predictive coding, we compute this adjoint
term using the recursive relationship of the prediction errors. Here, we
take a different approach. Instead, we ask what is the simplest possible
dynamical system which can converge to the exact adjoint term at the
equilibrium. After some thought, we emerge at a straightforward leaky
integrator model.

  -- -------- -------- -- --------
     @xmath   @xmath      (6.22)
  -- -------- -------- -- --------

which, at equilibrium, converges to

  -- -------- -------- -- --------
     @xmath   @xmath      (6.23)
  -- -------- -------- -- --------

This update rule includes the very adjoint term we are trying to
compute, however, so these dynamics are not immediately computable. To
make them so, we first split up the adjoint. By the chain rule, we can
write Equation 6.22 as,

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

where @xmath is the value of @xmath computed in the forward pass. Next,
we note that if we use the activations of the neurons at each layer
instead of prediction errors to accumulate the adjoint, we can express
this in terms of the equilibrium activation of the superordinate layer,

  -- -------- -- --------
     @xmath      (6.25)
  -- -------- -- --------

However, to achieve these dynamics exactly in a multilayered network
would require the sequential convergence of layers, as each layer must
converge to equilibrium before the dynamics of the layer below can
operate. This sequential convergence would make the algorithm no better
than the sequential backwards sweep of backprop. However, if we
approximate the equilibrium activations of the layer with the current
activation, this allows us to run all layers in parallel, yielding,

  -- -------- -------- -- --------
     @xmath               
                          (6.26)
              @xmath      (6.27)
  -- -------- -------- -- --------

where @xmath represents the partial derivative of the postsynaptic
activation with respect to the presynaptic activation. Despite this
approximation, we argue that the system nevertheless converges to the
same optimum as Equation 6.23 . Specifically, because we evaluate @xmath
at the feedforward pass value @xmath , this term remains constant
throughout the relaxation phase ⁷ ⁷ 7 The need to keep this term fixed
throughout the relaxation phase does present a potential issue of
biological plausibility. In theory it could be maintained by short-term
synaptic traces, and for some activation functions such as rectified
linear units it is trivial. Moreover, later we show that this term can
be dropped from the equations without apparent ill-effect . Keeping this
term fixed effectively decouples each layer from any bottom-up
influence. If the top-down input is also constant, because it has
already converged so that @xmath , then the dynamics become linear, and
the system is globally stable due to possessing a Jacobian which is
everywhere negative-definite. The top-layer is provided with the
stipulatively correct gradient, so it must converge. Recursing backwards
through each layer, we see that once the top-level has converged, so too
must the penultimate layer, and so through to all layers.

Crucially, Equation 6.4 , which is core to the AR algorithm is extremely
simple and biologically plausible. It only requires that the activations
of a given layer are sensitive to the difference between their own
activity and that of the layer above mapped through the backwards
weights, and modulated by the nonlinear derivative of the postsynaptic
potential. This update rule thus functions as a kind of prediction
error, but one that emerges between layers, rather than being
represented by specific prediction error units at a given layer.

Computationally, the AR algorithm proceeds as follows. First, a standard
forward pass computes the network output, which is compared with the
target to calculate the top-layer error derivative @xmath and thus
update the activation of the penultimate layer. ⁸ ⁸ 8 This top-layer
error is simply a prediction error for the MSE loss, but may be more
complicated and less biologically-plausible for arbitrary loss functions
. Then, the network enters into a relaxation phase where Equation 6.4 is
iterated globally for all layers until convergence for each layer. Upon
convergence, the activations of each layer are precisely equal the
backpropagated derivatives, and are used to update the weights (via
Equation ( 6.4 ).

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.28)
  -- -------- -------- -- --------

Dataset @xmath , parameters @xmath , inference learning rate @xmath ,
weight learning rate @xmath . for @xmath do

for @xmath for each layer do

@xmath

end for

while not converged do

@xmath

@xmath

for @xmath for each layer do

@xmath

@xmath

end for

end while

for @xmath do

@xmath

end for

end for

Algorithm 5 Activation Relaxation

#### 6.4.1 Method and Results

We first demonstrate that our algorithm can train a deep neural network
with equal performance to backprop. For training, we utilised the MNIST
and Fashion-MNIST ( xiao2017online ) datasets. The MNIST dataset
consists of 60000 training and 10000 test 28x28 images of handwritten
digits, while the Fashion-MNIST dataset consists of 60000 training and
10000 test 28x28 images of clothing items. The Fashion-MNIST dataset is
designed to be identical in shape and size to MNIST while being harder
to solve. We used a 4-layer fully-connected multi-layer perceptron (MLP)
with rectified-linear activation functions and a linear output layer.
The layers consisted of 300, 300, 100, and 10 neurons respectively. In
the dynamical relaxation phase, we integrate Equation 6.4 with a simple
first-order Euler integration scheme. @xmath where @xmath was a learning
rate which was set to @xmath . The relaxation phase lasted for 100
iterations, which we found sufficient to closely approximate the
numerical backprop gradients. After the relaxation phase was complete,
the weights were updated using the standard stochastic gradient descent
optimizer, with a learning rate of 0.0005. The weights were initialized
as draws from a Gaussian distribution with a mean of 0 and a variance of
0.05. Hyperparameter values were chosen based on initial intuition and
were not found using a grid-search. The AR algorithm was applied to each
minibatch of 64 digits sequentially. The network was trained with the
mean-squared-error loss.

In Figure 6.13 we show that the training and test performance of the
network trained with activation-relaxation is nearly identical to that
of the network trained with backpropagation, thus demonstrating that our
algorithm can correctly perform credit assignment in deep neural
networks with only local learning rules. We also empirically investigate
the angle between the AR-computed gradient updates and the true
backpropagated updates. The gradient angle @xmath was computed using the
cosine similarity metric @xmath , where @xmath was the AR-computed
gradients and @xmath were the backprop gradients. To handle the fact
that we had gradient matrices while the cosine similarity metric only
applies to vectors, following ( lillicrap2016random ) , we simply
flattened the gradient matrices into vectors before performing the
computation. We see that the updates computed by AR are very close in
angle to the backprop updates (under 10 degrees), although the angle
increases slightly over the course of training. The convergence in
training and test accuracies between the AR and backprop shows that this
slight difference in gradient angle is not enough to impede effective
credit assignment and learning in AR. In Appendix C, we take a step
towards demonstrating the scalability of this algorithm, by showing
preliminary results that indicate that AR, including with the
biologically plausible simplifications introduced below, can scale to
deeper CNN architectures and more challenging classification tasks.

#### 6.4.2 Loosening Constraints

While the AR algorithm as above precisely approximates adjoint term
@xmath central to backprop, using only local learning rules, it still
retains a number of biological implausibilities. The core implausibility
is the weight transport problem, which is still present due to the
weight transpose present in Equation 6.4 . Following our previous work
on relaxed predictive coding (Chapter 3), we demonstrate how the same
remedies can be directly applied to the AR algorithm without
jeopardising learning performance.

To address the weight transport problem, we take inspirations from the
approaches of feedback alignment ( lillicrap2016random ) , and (
millidge2020relaxing ) . We postulate an independent separate set of
backwards weights @xmath , so that the update rule for the activations
becomes,

  -- -------- -- --------
     @xmath      (6.29)
  -- -------- -- --------

Then, following our work on relaxed preditive coding in Chapter 5, we
learn these backwards weights with the following Hebbian update,

  -- -------- -- --------
     @xmath      (6.30)
  -- -------- -- --------

The backwards weights were initialized as draws from a 0 mean, 0.05
variance Gaussian. In Figure 6.14 we show that strong performance is
obtained with the learnt backwards weights. We found that using random
feedback weights without learning (i.e. feedback alignment), typically
converged to a lower accuracy and had a tendency to diverge, which may
be due to a simple Gaussian weight initialization used here.
Nevertheless, when the backwards weights are learnt, we find that the
algorithm is stable and can obtain performance comparable with using the
exact weight transposes (Figure 6.14 ). This is a very strong and
encouraging result. First that this learning rule enables performance
with exact weight transposes is impressive, since it implies that the
Hebbian update rule on the backwards weights works and is highly
effective, even early on in training. Secondly, the generalizability of
this remedy for weight transport, from predictive coding networks, deep
neural networks ( amit2019deep ; akrout2019deep ) with backprop, and now
AR suggests that the backwards weights may be able to be independent and
robustly learned from scratch in the brain, thus largely resolving the
weight transport problem altogether.

We additionally plot the angle between the AR with learnable backwards
weights and the true BP gradients (Figure 6.15 ). The angle starts out
very large (about 70 degrees) since the backwards weights are randomly
initialized but then rapidly decreases to about 30 degrees as the
backwards weights are learnt, which seems empirically to be sufficient
to enable strong learning performance.

An additional potential biological implausibility to address is the
nonlinear derivative problem, which consists of the @xmath term. The
biological plausibility of this term depends heavily upon the activation
function used in the network. For instance, in a relu network, this term
is trivial, being 0 if the postsynaptic output is greater than 0, and 1
if it is. However, other commonly used activation functions like tanh,
sigmoid, and especially softmax are more complex and may be challenging
to compute locally in the brain. Here, we experiment with simply
dropping the nonlinear derivative term from the update rule, which
results in the following dynamics,

  -- -------- -- --------
     @xmath      (6.31)
  -- -------- -- --------

Although the gradients do no longer match backprop, we show in Figure
6.14 that learning performance against the standard model is relatively
unaffected, showing that the influence of the nonlinear derivative is
small. We hypothesise that by removing the nonlinear derivative, we are
effectively projecting the backprop update onto the closest linear
subspace, which is still sufficiently close in angle to the true
gradient that it can support learning. Alternatively, it could be that
in the regime of standard activity values prevailing throughout the
network during training, the nonlinear derivatives generally are close
to 1, and thus have little effect on the update rules in any case. If
this is the case, then given that we made no particular effort to
constrain the activities of the network, it supposes that this property,
if it exists, may be highly beneficial for simplifying the computations
in the brain.

By explicitly plotting the angle (Figure 6.15 ), we see that it always
remains under about 30 degrees, sufficient for learning, although the
angle appears to rise over the course of training, potentially due to
the gradients becoming smaller and more noisy as the network gets closer
to convergence.

Moreover, we can combine these two changes of the algorithm such that
there is both no nonlinear derivative and also learnable backwards
weights. Perhaps surprisingly, when we do this we retain equivalent
performance to the full AR algorithm (see Figure 6.14 ), and therefore a
valid approximation to backprop in an extremely simple and biologically
plausible form. The activation update equation for the fully simplified
algorithm is:

  -- -------- -- --------
     @xmath      (6.32)
  -- -------- -- --------

which requires only locally available information and is mathematically
very simple. In effect, each layer is only updated using its own
activations and the activations of the layer above mapped backwards
through the feedback connections, which are themselves learned through a
local and Hebbian learning rule. This rule maintains high training
performance and a close angle between its updates and the true backprop
updates (Figure 6.15 ), and is, at least in theory, relatively
straightforward to implement in neural or neuromorphic circuitry.

Finally, we note that the AR update rules require the nonlinear
derivative @xmath to be evaluated with the activity @xmath evaluated at
its feedforward pass value @xmath . Additionally, in the weight update,
the value of the activity @xmath needs to be evaluated at its
feedforward pass value

  -- -- -- --------
           (6.33)
  -- -- -- --------

We call this the frozen feedforward pass assumption, and it is very
closely related to the fixed-prediction assumption in predictive coding
or, similarly the requirement in equilibrium-propagation to store all
the intermediate equilibrium values of the free phase. Here we
investigate to what extent this assumption can also be relaxed.

We evaluate whether the nonlinear derivative term can be unfrozen so
that it uses the current value of the activity in a.) the function
derivative @xmath in Equation 6.4 , b.) in the weight update equation
(Equation 6.4 ), and c.) we investigate whether the activation value
itself can be replaced in the weight update equation.

In Figure 6.16 , we see that the frozen feedforward pass assumption can
be relaxed in the case of the nonlinear derivatives for both the AR
update and the weight update equation. However, relaxing it in the case
of the weight update equation destroys performance. This means that
ultimately, a direct implementation of AR in biological circuitry would
require neurons to store the feedforward pass value of their own
activations.

All the experiments so far have been done on the relatively simple and
straightforward MNIST dataset with small MLP models. However, it is also
important to verify the scalability of this method. Here we demonstrate
that AR can be used to train large CNNs on challenging image recognition
datasets (SVHN, CIFAR10, and CIFAR100) and, moreover, that the previous
loosenings of the biologically implausible constraints on the algorithm
still do not appear to degrade performance unduly. This extension to
CNNs is especially important because other biologically plausible
schemes such as feedback alignment ( lillicrap2016random ;
lillicrap2014random ) , and directed feedback alignment (
nokland2016direct ) , have been shown to struggle with the
CNN116architectures ( launay2019principled ) . We tested the
simplifications (dropping nonlinearity or learning backwards weights) on
just the convolutional layers of the network, just the fully-connected
layers of the network, or both together. We found that ultimately
performance was largely maintained even when both convolutional and
fully connected layers in the network used learnable backwards weights
or had their nonlinear derivatives dropped from both the update and
weight equations. These results speak to the scalability and
generalisability of these relaxations, and the general robustness of the
AR algorithm. We implemented the learnable backwards weights of the CNN
by applying Equation 6.4 to the flattened form of the CNN filter kernel
weights.

Our CNN consisted of a convolutional layer followed by a max-pooling
layer, followed by an additional convolutional layers, then two fully
connected layers. The convolutional layers had 32 and 64 filters
respectively, while the FC layers had 64,120, and 10 neurons
respectively. For CIFAR100 there are 100 output classes so the final
layer had 100 neurons. The labels were one-hot-encoded and fed to the
network. All input images were normalized so that their pixel values lay
in the range @xmath but no other preprocessing was undertaken. We used
hyperbolic tangent activations functions at every layer except the final
layer which was linear. The network was trained on a mean-square-error
loss function.

We see that the simplifications also scale to the CNN for the SVHN
dataset, although, interestingly, performance is degraded on this
dataset when both convolutional and FC nonlinearities are dropped.
However, since this does not occur in the other, more challenging, CIFAR
datasets, we take this result to be an anomaly.

#### 6.4.3 Interim Discussion

In sum the AR algorithm uses only simple learning rules to
asymptotically approximate the adjoint terms of backprop over the course
of multiple dynamical iterations. We have demonstrated that the AR
algorithm can apply to arbitrary computation graphs, as can predictive
coding, and can be used to train deep CNN models on challenging object
recognition tasks with a performance equivalent to backprop. Moreover,
AR eschews much of the complexity of competing schemes such as
predictive coding, by not requiring two separate populations of value
and error neurons, and equilibrium propagation by not needing two
separate backwards phases – a free phase and a clamped phase.
Additionally, we have demonstrated that some of the remaining biological
implausibilities of AR, such as the weight transport and backwards
nonlinearities problems, can be successfully ameliorated through the
right extensions to the algorithm such as learnable backwards weights
with minimal effect on overall performance. Other constraints, such as
the necessity to use the feedforward pass activities in the dynamics
instead of the current activities cannot be relaxed without
catastrophically damaging overall performance.

Like predictive coding, a limitation of this method is its intrinsically
iterative nature. This iteration scheme means that it is at least
several times more costly than standard backpropagation of error – for
instance, in these experiments we used 100 iterations to reach exact
convergence to the backpropagated gradients, although this is not
strictly necessary for good performance. While some of this
computational cost may be ameliorated by the intrinsic parallelism of
neural circuitry, there is nevertheless a timing issue if convergence is
required to be complete before the next sensory datum arrives, and
issues of gradient interference if it is not. A speculative solution to
this could be synchronization mediated by the brain’s alpha/beta or
gamma band frequencies in the cortex, where one dynamical phase
iterating to convergence would correspond to one wavelength of the band.
Such an identification is highly speculative however, and the
computational function of such oscillations in the brain are still
largely mysterious and highly controversial ( buzsaki2006rhythms ) .

One additional and important drawback of the AR algorithm is that it
requires keeping a memory of the feedforward pass activations throughout
the backwards dynamical phase. This is because the activations swap
their purpose from representing feedforward pass values in the forward
phase, to representing gradients in the backwards phase. Taken
literally, this requires that the learning rules become non-local in
time, although in practice the feedforward pass activations are just
stored. While this is the most substantial drawback to the biological
plausibility of the algorithm, it is important to note that this
drawback is also shared with every other iterative algorithm in the
literature. Predictive coding similarly requires the fixed-prediction
assumption, which is effectively the same, and equilibrium-propagation
requires that all the activations at the equilibrium of the free phase
are stored while the clamped phase converges. We suggest that this
necessity for storage in iterative algorithms to approximate backprop is
universal and it arises for a simple reason. Namely, that the
backpropagated gradients themselves fundamentally only depend upon the
feedforward pass values, since we are backpropagating only on the
feedforward pass and not through the dynamics themselves. Since the
gradients ultimately depend only on the feedforward pass, any dynamical
scheme to approximate them must ‘remember’ what these values are,
somehow. In theory, it may be possible to design algorithms such that
this memory is implicit and thus not explicitly necessary, but no such
algorithms have, to my knowledge, been found so far.

This issue of memory is also closely related to the core computational
properties of reverse-mode AD. Specifically, that it requires storage in
memory of all intermediate activations in the forward pass. This means
that the memory issue also closely applies to sequential backward
algorithms such as target-propagation and, indeed, backprop itself. The
fundamental fact is that backwards pass can only take place after the
forward pass is complete, and that it requires knowledge of forward pass
activities. This fact cannot be ignored or cleverly wished away by any
algorithm but must simply be addressed. If the brain is performing
reverse-mode AD, then it simply must have some way to store, implicitly
or explicitly, the feedforward pass values. The question then becomes
how can this be done in the brain? Some possibilities are multiplexing
using the brain’s own intrinsic rhythms ( buzsaki2006rhythms ) , or some
kind of special parallel class or neurons to maintain activity
explicitly ( o1999biologically ) , or else storage at the synaptic level
through mechanisms like eligibility traces ( bellec2020solution ) .
While we here remain ambiguous on the means by which such storage is
achieved, we have reached a point of conceptual clarity in knowing that
there must be storage . The only question is how.

### 6.5 Three-Factor Learning Rules and a Direct Implementation

After having gone through several iterative algorithms for approximating
the backpropagation of error algorithm, it is worth taking a step back
to understand what has been shown and what is truly necessary. Here, we
argue that in fact, despite strong claims that backprop is biologically
implausible, in fact an actual implementation of backprop in the brain
could be surprisingly simple and biologically plausible. Moreover, that
many of the algorithms, including the iterative algorithms previously,
may simply be complicating matters. The material in this section is
speculative and early stage, and will be investigated further in future
work.

First, we begin by stating, somewhat boldly, that in general the weight
transport problem is solved. That is, there is now fairly strong
evidence in the literature ( lillicrap2016random ; amit2019deep ;
akrout2019deep ; millidge2020relaxing ) firstly that a precise equality
of forward and backwards weights is not necessary for good learning
performance, and secondly, that competitive performance with backprop
can be maintained, even for deep networks, through learnable backwards
weights which update with a fairly straightforward Hebbian learning
rule. If we assume that the weight transport problem is solved, then
there is only the locality issues remaining with a direct implementation
of backprop.

Firstly, we note that if we are implementing reverse-mode AD, and we
find a way to compute the adjoint @xmath locally at a layer then we
actual weight update @xmath requires only local information. This means
that almost the entire challenge of backprop is computing the adjoint
term locally.

Secondly, we notice that the standard forward pass of an artificial
neural network @xmath does not actually correspond to how it would work
in the brain since here the activation function (which is typically
assumed to be applied through the threshold for making an action
potential in the cell soma) is applied after the weights, while in the
brain the synaptic weights are on the dendrites of the post-synaptic
neuron, and thus occur after the action potential. This means that
instead we should use the more biologically plausible forward pass as,

  -- -------- -- --------
     @xmath      (6.34)
  -- -------- -- --------

Where the order of the weights and the activation function are switched.
Specifically, this means that the activation function only applies to
the presynaptic activity and not the weights. This approach considerably
simplifies the requisite gradients, so we get the following expressions
for the adjoint and the weight update

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (6.35)
  -- -------- -------- -- --------

Specifically, the adjoint recursion is simply the adjoint of the layer
above, multiplied through the backwards weights and the derivative of
the activation function of the pre-synaptic activity. The weight update
is substantially simpler since, because the forward pass is linear in
the weights, it is simply a multiplication of the adjoint with the
presynaptic activity which is essentially Hebbian except with the
adjoint replacing the post-synaptic term. Interestingly, this applies
that in this case of backprop, the post-synaptic term should have no
effect on the synaptic weights, which is a very strong and
counterintuitive empirical prediction of backprop.

While it may seem that the switch in the order of the weights and the
activation function might have some serious impact on the expressive
power of the network, we argue that it likely does not. In fact, the two
formulations are equivalent in deep neural networks except for the
beginning and ending layers. To see this, we can simply explicitly write
out the expression for the function computed by a 4 layer neural
network,

  -- -------- -- --------
     @xmath      (6.36)
  -- -------- -- --------

For the vast majority of layers, these expressions are the same up to a
re-bracketing. The only difference is the last layer has no activation
function using the more neural forward pass (but often in neural
networks we use a linear last layer anyway), and that the input is first
passed to an activation function activation function where this is not
common in standard ANNs – but this function could always be set to the
identity to make the equivalence exact. In general, the effect of these
differences will be minor for deep networks.

Now we note the crucial point, that the only biological implausibilities
in these learning rules is the necessity to have the adjoint value
present at the synapse for the weight update rule, since it is just the
multiplication of the adjoint and the presynaptic activation. The
recursive computation of the adjoint itself (Equation 6.3 is relatively
plausible, since the only difficulty is the nonlinear derivative term
@xmath which now is only the derivative of the activation function
applied to the pre-synaptic input. Importantly, for spiking neurons,
this derivative is trivial as is essentially consists of a spike when
the neuron fires and not when it doesn’t. As such, this rule effectively
says that the adjoint should only be updated when there is presynaptic
firing. Putting this all together, we can imagine implementing this in
the brain in a fairly direct forward-backward scheme as in Figure 6.18

Specifically, if we assume that the weight transport problem can be
solved with independently learnable backwards weights, then the
recursion for the adjoint becomes simply,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.37)
  -- -------- -------- -- --------

where the second line is the recursion if we simply ignore the nonlinear
derivative term, as we have also found does not hinder learning in
practice ( millidge2020relaxing ; millidge2020investigating ;
millidge2020activation ; ororbia2019biologically ) . This recursion is
extremely simple and is just the adjoint mapped through the backwards
weights to the layer below. Thus, we can imagine keeping separate
forward and backward passes whereby the adjoint is represented by
interneurons @xmath . Here, the update rules simply become,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (6.38)
  -- -------- -------- -- --------

The simplicity of these update rules implies that the only potential
biological implausibility is in the weight update 6.4 where we have
transmitted the value of the adjoint ‘interneurons’ to the synapses of
the post-synaptic neuron, and used them to update those weights. Through
a careful analysis, we have revealed this question to be the ultimate
crux of whether backprop in the brain is plausible or not. Importantly,
this ability to use the adjoints as part of a ‘local’ learning rule is
crucial to every purported ‘biologically-plausible’ method in the
literature – from predictive coding, to target-propagation, to
equilibrium-prop and AR.

Whether or not the adjoint can be transmitted to the synaptic weights in
the brain is currently a controversial and unresolved question, and to
my knowledge, has not been studied directly. While it may seem fairly
obscure, this analysis suggests that this question is absolutely crucial
to our understanding of whether the brain can directly implement
backprop or not. An important possibility is that of segregated
dendrites ( sacramento2018dendritic ) . Having separate dendritic
compartments would, presumably, straightforwardly enable the broadcast
of the adjoint values to the synaptic weights, since they would be
located in the dendritic tree of the same neuron. The key question would
then become whether information transmitted to the dendrites could
remain segregated. That is, could both Equation 6.34 and Equation 6.5 be
implemented using the same neuron. If this is the case then it would
suggest an incredibly simple biological implementation for
backpropagation, needing only a single type of neuron which would both
send forward connections and reciprocally receive backwards connections
from neurons in the layer above. Such a simple architecture would lend
great support to the idea that the brain can indeed implement backprop,
and perhaps that backpropagation is so straightforward that it may even
function as a computational primitive in neural circuitry.

### 6.6 Discussion

Overall, in this chapter, we have shown that predictive coding, as an
approximation to the backpropagation of error algorithm, can be extended
to arbitrary computation graphs and we have applied predictive coding to
large-scale machine learning architectures such as CNNs and LSTMs and
demonstrated that they perform comparably to backprop trained networks.
Moreover, we have posited a novel iterative algorithm – Activation
Relaxation – that also converges to the exact backprop gradients, does
not require two separate populations of predictions and prediction error
units, and uses extremely simple and elegant learning and update rules.
We have shown that AR also scales to large-scale CNN neural network
models and is competitive with backprop trained networks at scale.
Finally, taking experience from our previous work in this field, we have
re-analyzesd the problem of backpropagation in the brain from first
principles and discovered, somewhat surprisingly, that if we assume that
the weight transport problem is solved, the only major issue of
biological implausibility is whether recursively computed adjoints can
be ‘transferred’ onto synapses to be able to form part of the synaptic
weight updates. If they can, and it seems likely that this is possible
through a mechanism of segregated dendrites or, alternatively,
backpropagating action potentials ( stuart1997action ) , then we can be
fairly certain that propagation in the brain is at least theoretically
achievable. This is a remarkable turn-around from the consensus only
five years ago that it was completely biologically implausible, and
speaks to the rapid development and advances in this field.

Additionally, while this thesis chapter has presented a substantial
extension to an existing algorithm (predictive coding), and an entirely
novel algorithm for credit assignment in the brain (activation
relaxation), we also wish to highlight the conceptual contributions we
have made while thinking about these issues deeply. In my opinion, these
are perhaps the most important sections of this work. Namely, firstly,
the issue of memory and time in any implementation or approximation to
reverse-mode AD. Specifically, that any biologically plausible
algorithm, whether sequential or iterative, due to the very nature of
reverse-mode AD must store the values of the feedforward pass throughout
the backwards sweep or phase, either implicitly or explicitly. While the
rationale for this seems obvious in retrospect, it was not clear
beforehand, and is still not at all clear in the literature. Indeed, the
dependence of almost all of these biologically plausible algorithms on
the memory of forward pass values is generally obfuscated or presented
as a minor hindrance, when in fact it is an absolutely irrevocable fact
of the comptuation these algorithms are trying to render biologically
plausible. Secondly, and perhaps most importantly, we have reached the
crux of the issue of whether backpropagation in the brain is plausible –
namely whether adjoints, which must remain separate from the
post-synaptic activation – can modulate synaptic weight updates. If they
can, then very biologically plausible and elegant schemes exist for a
direct implementation of backpropagation in the brain (see Equations 6.5
, 6.4 ). If it is not possible, then it seems likely, given that the
adjoint equation is absolutely fundamental to reverse-mode AD, that
backpropagation in the brain is not plausible or, at least, is
explicitly achievable except via some roundabout method. Moreover, if
the transport of the adjoint onto the synaptic weight terminals is
possible, then it must be supported by some kind of dedicated
neurophysiological mechanism, which can and must be studied in detail if
we are to understand the explicit details of this key aspect of credit
assignment in the brain. Nevertheless, I now believe that the key
mathematical and conceptual issues in the question of whether the brain
can do backpropagation (in this rate-coded static model) have been
largely worked out and depend now solely on details of neurophysiology.

The crucial caveat to this response is that we have only worked out
credit assignment in an incredibly simplified model of what occurs in
the brain – namely with rate-coded integrate and fire neurons – on
static inputs. Both of these assumptions, however, are false in the
brain. Firstly, neurons are spiking networks which may communicate using
precise spike timings to convey information. Understanding how to
perform credit assignment in such spiking networks is still a young and
open field, although there has been much recent progress (
schiess2016somato ; zenke2018superspike ; kaiser2020synaptic ;
neftci2019surrogate ) . Moreover, and crucially, the key problem the
brain faces is not just backpropagation through space (i.e. layers), but
backpropagation through time. The brain must be able to assign credit
correctly to temporally distant events from the synaptic weight values
that, ultimately caused them. While a mathematical formulation of
reverse-mode AD can be directly formulated by simply performing backprop
on a computation graph ‘unrolled through time’, in practice this means
that in the backwards phase that time must run backwards or,
alternatively that the network must store not only the feedforward pass,
but its entire history , which is definitely biologically implausible.
The key question is thus how to implement backpropagation through time
in a biologically plausible manner. There has been much recent progress
in this field, also combined with spiking networks such as (
zenke2018superspike ; bellec2020solution ) . However, the innovative
approach engendered by Eligibility Propagation (E-prop) only applies to
single layer recurrent networks, leaving open the question of how to
marry backpropagation through space and backpropagation through time.

An additional interesting consideration is that throughout, and
generally in the literature, only reverse-mode AD is considered to be a
contender for the credit assignment algorithm implemented in the brain.
This is due to the historical use and generally better computational
properties of reverse-mode for artificial neural networks (
griewank1989automatic ; baydin2017automatic ) and has thus become the
dominant paradigm ( goodfellow2016deep ; rumelhart1985feature ;
silver2017mastering ) . However, this is not necessarily the case in
highly parallel architectures like the brain, for which additional
forward computation cost may be effectively negligible due to the degree
of parallelization. Forward-mode AD can be implemented through dual
numbers – which directly pair activations with their derivatives, and
which it is interesting to think about how this could relate to neural
activity and synapses. Moreover, a key computational advantage of
forward-mode AD is that it imposes no memory cost, since it is entirely
online and requires no storage of intermediate activations, thus
entirely obviating the memory issues inherent in implementations of
reverse-mode AD. The disadvantage, however, with forward-mode AD in the
brain is that it dislocates the derivative computation from the physical
location of the synapses. The computations and derivatives ‘move
forwards’ up through higher layers and levels of processing while the
synapses remain firmly put. it is necessary, then, whenever the
computations and derivatives reach the end of the process to transmit
the fully computed derivatives back to where they originated. Precisely
working out this process has, to my knowledge, not yet been done, but it
may result in a practicable algorithm. One important case where
forward-mode AD makes sense is in backpropagation through time since, as
the computation moves forward in time, so do the synapses themselves.
Thus, the correct derivatives are always locally available precisely
when they are needed. Forward-mode AD through time is known as
real-time-recurrent learning (RTRL) ( williams1989experimental ) , and
is potentially a good algorithm for the brain to solve recurrence,
although it is extremely computationally expensive, rendering it
uncompetitive with reverse-mode BPTT for training large neural networks.
Moreover, by taking various sparse approximations to RTRL, it is
possible to reduce the computational cost at the cost of somewhat
reduced learning performance. Algorithms such as eligibility-prop
essentially try to make RTRL updates biologically plausible, with some
success.

### 6.7 Conclusion

In this chapter, we have proposed two novel biologically plausible
algorithms for credit assignment in the brain. Firstly, we demonstrate
that predictive coding, under the fixed prediction assumption, and
set-up in a ‘reverse mode’ naturally computed the gradients required for
the backpropagation of error algorithm, as its dynamics satisfy the same
recursive structure of the adjoint equation and thus, the fixed points
of the prediction errors, upon convergence, equal the backpropagated
error gradients which can then be used to perform backprop. We have
extensively empirically validated this correspondence and used it to
train large-scale and complex machine learning architectures such as
CNNs and LSTMs with performance equal to those trained by backprop.

Secondly, we have utilized the insights gained by our work with
predictive coding to derive a new, and much simpler algorithm which we
call Activation Relaxation (AR). Here, instead of using separate
prediction error neurons, we simply update the activation of the value
neurons themselves to become equal to the backpropagated errors during
the backwards iteration phase. While this eschews the fixed feedforward
pass assumption required for predictive coding, it introduces a similar
requirement of storing the initial feedforward pass value throughout the
backwards iteration phase, so that they can then be used during the
weight updates. We also empirically validate this correspondence and
demonstrate that AR can be used to train machine learning architectures
with the same performance as backpropagation. Importantly, we also
investigate the potential for applying the same biologically plausible
relaxations to the AR algorithm as we applied to predictive coding in
Chapter 3, and show that the relaxations perform just as well in this
new setting – speaking to robustness and generalizability of these
relaxations. Overall, we believe the AR algorithm is simpler, more
elegant, and more biologically plausible than competing iterative
backprop schemes such as predictive coding and equilibrium-propagation.
However, it suffers from the limitations inherent in all iterative
approaches – the necessity to somehow store the feedforward pass values
throughout the backwards pass. A clear understanding of this limitation,
then opens the way for future work to try to remedy it or propose a
different method entirely for solving backpropagation in the brain.

Finally, we have included some current (and unpublished) speculations on
the potential solution to backpropagation in the brain for simple
feedforward networks of rate-coded integrate and fire neurons, and have
constructed a relatively direct method of implementing backpropagation
with only a few moving components. Importantly, this construction relies
heavily first on its nonstandard definition of the forward pass, using
@xmath – or the weights after the activation function, rather than
inside of it – which is non-standard for artificial neural networks, but
is actually more biologically plausible, and secondly on the solution to
the weight transport problem to allow for learnable backward weights.
With these issues circumvented, we believe that biologically plausible
backpropagation for rate-coded integrate and fire neurons actually turns
out to be relatively straightforward. The key next move for future work,
now that this base of understanding is established, is to start to
attack the substantially harder problem of biologically plausible
implementations of backpropagation through time, as well as with spiking
neuron models.

Overall, in this chapter, we believe that we have made several clear
contributions towards understanding the biological plausibility of
backpropagation in the brain – firstly by providing and empirically
validating two new iterative algorithms (predictive coding and
activation relaxation) and secondly by coming to a much clearer
understanding of what exactly the remaining stumbling blocks to a
biological implementation are.

## Chapter 7 Discussion

The computer scientist and mathematician Richard Hamming in his
insightful essay You and Your Research describes how he would pose the
following question to his colleagues at Bell Labs – ‘What is the most
important question in your field, and why aren’t you working on it?’ .
As might be expected, this made him unpopular with many of his
colleagues. However, it speaks an important truth – the absolute and
overriding importance of posing and working on the right questions. An
important question that is impossible is futile. A tractable but
unimportant question is useless. Throughout my PhD, I have endeavoured
to orient by Hamming’s maxim; to find the most important yet solvable
question within my field and to try to answer it. This thesis, then, can
be seen as a concatenation of three questions of progressively (in my
opinion) increasing scope and importance.

The first question is local to the active inference community, but is
very important within it. Namely, can active inference be combined with
contemporary deep reinforcement learning methods, and thus be scaled to
the kind of tasks that can be handled by contemporary deep reinforcement
learning? Conversely, does the theory of active inference itself contain
any insights which can be useful for machine learning theorists and
practitioners? I believe that through my work ( millidge_deep_2019 ;
millidge2019combining ; tschantz2020reinforcement ;
millidge2020relationship ; tschantz2020control ) and others (
tschantz_scaling_2019 ; fountas2020deep ; ueltzhoffer_deep_2018 ;
ccatal2020learning ) , both sides of this question have been
definitively answered in the affirmative. Active inference can be
straightforwardly scaled up using artificial neural networks and the
techniques of deep reinforcement learning while, conversely, active
inference has many interesting properties and ideas which could be of
use to the deep reinforcement learning community. In this thesis, we
have explored several of these ideas and primarily focused on how deep
active inference and deep reinforcement learning can be merged. Now that
this has been answered, future work should focus on the converse – how
deep active inference differs from deep reinforcement learning and the
extent to which it can inform and lead to novel and performant
algorithms in deep reinforcement learning.

The second question is more broadly targeted to the reinforcement
learning and cognitive science communities, and concerns the
mathematical origins of exploration . This question is central to a
number of related disciplines such as reinforcement learning (
sutton2018reinforcement ) , decision theory ( daw2006cortical ) ,
control theory ( kalman1960contributions ) , and behavioural economics (
tversky1974judgment ) , which all share the same fundamental object of
study – adaptive decision-making under uncertainty. Where there is
uncertainty so that the true dynamics of the environment and/or the
value of each possible contingency are not known, then the optimal
policy cannot straightforwardly be computed, and agents are necessarily
faced with the exploration-exploitation trade-off. This trade-off arises
because new information can generally only be obtained by trying new
courses of action or venturing into new regions of the state-space.
However, to explore new regions necessarily has an opportunity cost of
not doing what you thought to be the current best option, which could
instead have been exploited . Given that to succeed at complex tasks, it
is almost always necessary to explore, it is important to figure out how
to explore in the most efficient manner. Specifically, we wish to design
algorithms that can acquire the information necessary for success as
rapidly as possible while incurring the minimum opportunity cost. In the
literature, it has been discovered that a very good heuristic for doing
this is simply to optimize a combination of the greedy reward
maximization objective exploit with an additional information gain
exploration term explore ( shyam_model-based_2019 ;
schmidhuber2007simple ; tschantz2020reinforcement ) . Specifically, the
reward maximization part of the objective ensures that agents do not
spend large amounts of time exploring informative but barren regions,
while the information gain terms help the agent not to get stuck in
locally greedy, but globally poor optima. While this objective works
well in practice, its mathematical origin and nature remains obscure. In
the literature, this approach is often described intuitively as simply
adding an additional exploratory term to the loss function. While random
entropy-maximizing exploration can be derived straightforwardly from
variational inference approaches to action ( levine2018reinforcement ) ,
the mathematical origin and commitments of specifically optimizing
information-seeking exploration terms has remained mysterious. This is
the second question we set out to answer in this thesis –
mathematically, from what sort of fundamental objectives do
information-gain exploration terms arise, and how can we characterise
the possible space of such objectives?

In Chapter 5, we answer this question. We show that information gain
maximizing exploration arises from minimizing the divergence between two
distributions – a predicted or expected distribution over likely states,
given actions, and a desired distribution over states, which encodes the
goals of the agent. This differs crucially from evidence objectives,
which are typically used in control as inference schemes (
levine2018reinforcement ; rawlik2013stochastic ) , which only seek to
maximize the likelihood of the desired states, rather than explicitly
match the two distributions. This finding has important implications for
a wide range of fields. Specifically, we argue that any kind of
information-maximizing exploratory behaviour can be seen as implicitly
aiming for a matching of two distributions rather than a likelihood
maximization. This, for instance, can explain several phenomenon, such
as the probability matching behaviour that is regularly observed in
human participants in cognitive science and behavioural economics tasks
( vulkan2000economist ; daw2006cortical ; west2003probability ;
shanks2002re ) , which are puzzling under the presumption of evidence
maximization ( tversky1974judgment ; gaissmaier2008smart ) . Moreover,
by understanding the origin of information-seeking behaviour as emerging
directly from divergence objectives, it provides us with a greater and
deeper understanding of what agents which optimize these
information-seeking terms are actually doing, while the ensuing
mathematical understanding may allow us to manipulate these terms more
confidently into more easily computable or tractable versions which
could aid implementations directly.

The third question is one with the greatest scope and importance. This
question is how can credit assignment be implemented in the brain? And,
specifically, whether and how (if it does) the brain can implement the
backpropagation of error algorithm. The solution to such a question
would be of great importance to neuroscience, since it would provide a
unifying view and mechanistic, algorithmic explanation of at least part
of cortical function. Moreover, it would explain at a detailed level one
of the core functionalities of the brain, and the one that underpins
almost all adaptive behaviour. Credit assignment is crucial to any kind
of long-range learning of the kind that must be occurring in the brain.
It is crucial for everything from learning the best way to form and
interpret sensory representations, to action selection operations to,
potentially long term memory and complex cognitive processing. While the
brain undoubtedly performs a substantial amount of top-down contextual
feedback processing as well as various kinds of homeostatic plasticity,
which both remain poorly understood, we also know from the stunning
success of machine learning in the last decade that simple feedforward
passes on large neural networks trained with the backpropagation of
error algorithm can accomplish tasks such as visual object recognition (
krizhevsky2012imagenet ; child2020very ) , generating realistic images
from text inputs ( ( radford2021learning ) , human-passable natural
language generation ( radford2019language ) , and playing at a
superhuman level games such as Go ( silver2017mastering ) , Atari (
mnih2015human ; mnih2013playing ; schrittwieser2019mastering ) , and
Starcraft ( vinyals2019grandmaster ) , which ten years ago were thought
to be extremely challenging, if not impossible for computers to
accomplish. Credit assignment, then, must be one of the core operations
in the brain, and if we can understand this then it is possible we may
obtain a grasp on how known computational algorithms are implemented in
the brain which allows us to grapple tractably with its immense
complexity.

While I, and the field as a whole, have taken steps towards addressing
and answering this question, we are still a long way from a viable
solution for the brain. Nevertheless, I feel that, in general, with the
profusion of algorithms addressing issues such as the weight transport
problem ( lillicrap2016random ; akrout2019deep ; nokland2016direct ) ,
and well as addressing issues of locality ( ororbia2019biologically ;
whittington2017approximation ; scellier2016towards ) , the field is
close to a good solution for the case of rate-coded neurons on a
temporally static graph. However, a solution under these constraints is
fundamentally only an abstraction of a much messier reality, where
neurons in the brain are not rate-coded but spiking, and must also
achieve credit assignment not just across space (layers) but across time
( lillicrap2020backpropagation ) . While there are some approaches which
grapple with these additional, and harder problems ( zenke2018superspike
; bellec2020solution ) , there are not many and we are far from a viable
global solution to this problem. I suspect it is into these new domains
that future research should primarily be directed, and where important
advances will be made.

### 7.1 Question 1: Scaling Active Inference

It turns out the active inference approaches can be quite
straightforwardly merged with those used in deep reinforcement learning
and can thus straightforwardly be ‘scaled up’ to achieve performance
comparable with the state of the art. Moreover, different choices lead
directly to different schools of model-free or model-based reinforcement
learning. Specifically, active inference fundamentally operates on
several core probabilistic distributions and objectives. The key
distributions are the likelihood distribution @xmath , and the
transition distribution @xmath . While standard discrete-state-space
active inference approaches parametrize these explicitly with
categorical distributions, and optimization of the variational free
energy exactly using analytical solutions resulting in a fixed-point
iteration algorithm, deep reinforcement learning algorithms instead
amortize these distributions with artificial neural networks, and
instead optimize the variational free energy with respect to the
amortised parameters (the weights of the artificial neural networks). In
pure inference terms this can be seen as an E-M algorithm, with a
trivial E-step (amortised inference as a forward pass through the
networks), and then an iterative M-step which corresponds to a gradient
step of stochastic gradient descent on the weights of the neural
networks.

The next translation is to identify the value function in deep
reinforcement learning with the path integral of the expected free
energy over time in active inference. Therefore the derivation of the
optimal policy under active inference @xmath can be seen as a design
choice in active inference to perform what is effectively Thompson
sampling over the softmaxed value function in reinforcement learning, a
choice which is often, but not necessarily made in comparable
reinforcement learning algorithms ( osband2015bootstrapped ) . Finally,
the sole remaining question remains how to compute or approximate the
path integral of the expected free energy, or the value function. While
the discrete state-space active inference literature typically only
deals with short time horizons and small state-spaces where this
integral can be exhaustively computed ( da2020active ) , or else by
simply enumerating and pruning unlikely paths ( friston2020sophisticated
) , the statespaces and time horizons in deep reinforcement learning
problems are typically large enough that this approach becomes
infeasible.

The sole remaining question remains how to approximate this path
integral, and here we can augment active inference with methods well
used in the deep reinforcement learning community. The approach taken by
model-free reinforcement learning is to utilize the iterative and
recursive nature of the Bellman equation to maintain and update at all
times a bootstrapped estimate of the value function (
kaelbling1996reinforcement ; mnih2013playing ) . This approach was
pioneered through the temporal-difference ( sutton1988learning ) , and
Q-learning algorithms ( watkins1992q ) , and gives rise to the
model-free family of reinforcement learning algorithms. Translating this
into the terms of active inference is quite straightforward. Since the
expected free energy objective can be factorised into separate
independent contributions for each timestep, the path integral satisfies
a similar recursive Bellman-like equation. This equivalence has been
recently used to prove the similarities between active inference and
reinforcement learning ( da2020relationship ) . This approach allows us
to straightforwardly define Q-learning and actor-critic like active
inference approaches, as was pioneered in my paper ( millidge2019deep )
. One minor distinction is that the computation of the expected free
energy contains an information gain term which necessitates a model of
the states or dynamics of the world, which would not be necessary when
using standard reinforcement learning approaches, but this information
gain yields superior exploratory capabilities and ultimately
performance.

While, in millidge_deep_2019 we explicitly included action within the
generative model in the agent, so that the action prior @xmath becomes
the path integral of the expected free energy, and the variational
policy posterior @xmath becomes the independently trained policy, thus
recapitulating and providing a variational inference gloss on standard
actor-critic algorithms, this is fundamentally a design choice. If we
instead ignore the policy prior @xmath and define the variational policy
posterior @xmath directly in terms of the value function, we obtain an
algorithm very similar to soft-Q-learning from deep reinforcement
learning ( haarnoja2018soft ) , except it optimizes a value functional
of the expected free energy instead of the reward.

Conversely, we can take the approach used in deep reinforcement learning
to approximate the value function at every timestep through samples of
model rollouts. This is straightforward because the value function is
just fundamentally the expected value of the reward across all possible
trajectories under a given policy. Using model-based rollouts to
approximate this is simply taking a monte-carlo approximation of an
expectation where the real environmental dynamics are approximated by
the model’s transition dynamics. By using importance sampling on this
objective, we can unsurprisingly see that the goodness of this
approximation depends crucially on the match between the true and
modelled dynamics.

If we are equipped with a model of the transition dynamics of the world
@xmath , we can approximate the path integral of the expected free
energy over time in a similar way. By simulating rollouts through the
transition model under a given policy, and then averaging together the
path integral of the expected free energy across rollouts, we form a
monte carlo estimate of the expected free energy value function. This
can then be used to directly compute the posterior distribution over
policies, or else can be fed into an iterative planning algorithm such
as path integral control or CEM which can then be used to obtain an
action plan. Using model-predictive control (replanning at every step),
then allows for the creation of flexible plans for any given situation.
Due to utilizing a transition model and simulated rollouts to estimate
the local value function, instead of bootstrapping from previous
experience, this model-based approach is substantially more sample
efficient than the model-free alternative. In the
tschantz2020reinforcement paper, we took this approach and demonstrated
performance comparable to or superior to standard model-based
benchmarks. Additionally, as before, the exploratory properties of the
expected free energy functional lead to improved performance.

Given that we thus know that active inference can relatively
straightforwardly be mapped to existing algorithms in deep reinforcement
learning, we now turn to the other face of the question – whether deep
reinforcement learning can learn anything from active inference. We
again argue in the affirmative. Namely that active inference, through
the expected free energy functional, provides superior exploratory
capabilities of active inference agent which, in challenging
sparse-reward tasks are necessary to obtain intelligent behaviour where
random exploration is simply not sufficient. While in reinforcement
learning this is a small literature on training agents with additional
exploratory loss functions ( pathak2017curiosity ;
shyam_model-based_2019 ; still2012information ; chua_deep_2018 ;
nagabandi2019deep ; klyubin2005empowerment ) , active inference provides
a mathematically principled and unified way of looking at this, rather
than simply postulating ad-hoc additional loss functions (
oudeyer2009intrinsic ) . Moreover, active inference also provides a
theory of reinforcement learning deeply grounded in variational
inference, and can thus, for instance, be straightforwardly extended to
POMDP models in a way that is nontrivial for standard reinforcement
learning algorithms. Finally, by proposing a unified objective of the
expected free energy, active inference allows, in principle, all
hyperparameters of the algorithm to be optimized directly by gradient
descents against this objective, thus theoretically obviating the need
for expensive hyperparamter sweeps and tuning.

Although the close connection between deep active inference and deep
reinforcement learning is now understood, and we know it is possible to
scale up active inference to the level of deep reinforcement learning,
there still remains much work to be done actually realizing this
connection and constructing deep active inference agents, using the
insights of active inference, which can compete head-to-head with the
state of the art in deep reinforcement learning and, potentially, exceed
it. I believe that especially as the field moves towards facing more
challenging environments with sparse rewards, the exploratory drives
implicitly embedded within active inference agents will become
increasingly important and impactful, since many current environments
possess straightforward dense rewards which provide a continuous reward
gradient from any initial condition to a successful final policy. In
such environments purely random exploration suffices for learning
effective policies. However, such environments are not in general
representative of the kind of environments that face biological
organisms in the real world, and thus to model their behaviour
additional exploratory instincts appear to be required.

Furthermore, on a general note, while current work has been focused on
trying to maximize the commonalities between deep active inference and
deep reinforcement learning, as the goal has been to establish the
connection and derive proof of principle scaled up active inference
models, later work should go the other way, and try to retain the
scalability of deep reinforcement learning while maximizing on what is
unique about active inference.

One intriguing possibility in this direction is to experiment with more
complex distributions of rewards. While active inference can be
formulating in a reward maximizing way, the real object active inference
handles is the biased generative model @xmath , or the desire
distribution @xmath . While this can be defined to be equal to reward
maximization by simply defining the desire distribution to be a
Boltzmann distribution over the realized rewards @xmath (
friston2012active ) , this is not the only way to do it. Indeed, more
complex and potentially multimodal reward distributions could be defined
and optimized directly in the algorithm. In theory this could lead to
more flexible behaviour or, alternatively, being able to model more
complex, context-sensitive or contingent rewards naturally within the
framework. There has been very little work done in this direction, and
it remains an exciting avenue for future work.

Another interesting direction is to explicitly model the generative
processes producing action within an inference framework. For instance,
search algorithms such as Monte-Carlo-Tree-Search have been vital in the
success in key reinforcement learning tasks such as playing Go and Chess
( silver2017mastering ) , and can theoretically be written in a
probabilistic generative model. Doing this would then allow them to be
combined productively with all the standard tools of Bayesian and
variational inference and could potentially lead to substantially more
flexible algorithms for action selection which would provide extremely
powerful inductive biases over standard MLP policy modules which would
allow fast and very effective learning. In a similar vein,
continuous-action planning algorithms, which are currently rather
primitive (such as CEM ( rubinstein1997optimization ) and path integral
control ( kappen2007introduction ) ) and can only explicitly model
unimodal policies, are generally quite ineffective.
okada_variational_2019 has shown how many of these algorithms can be
directly modelled as part of a generative model and directly used in
variational inference, and they use this result to derive more effective
algorithms such as multimodal CEM ( okada2020planet ) . Further
extending this work may lead to the derivation of highly effective and
efficient planning algorithms for continuous control, which are
currently sorely lacking and which would lead to a substantial
improvement in the abilities of model-based continuous control.

Another interesting avenue, which has begun to be explored in the
literature ( friston2020sophisticated ) , which may lead to more nuanced
and effective forms of exploration, is to explicitly model, in model
rollouts, the change in its own beliefs the agent expects to encounter.
If this is explicitly modelled, then the agent can design exploration
strategies specifically to test hypotheses and explore different
strategies. In short, this kind of meta self-knowledge of the likely
changes of one’s own beliefs are vital for the kind of scientific and
experimental thinking that often characterises humans’ phenomenological
experience of the planning process. Such agents would be able to
intelligently consider and compute the value of information both now and
the expected value of information in the future under their expected
future beliefs. Basic models of this have been explored using the
discrete-state-space paradigm ( friston2020sophisticated ;
hesp2020sophisticated ) , however figuring out how to implement this
within the deep reinforcement learning paradigm in a computationally
tractable and efficient way, as well as to test its performance on tasks
which require such nuanced exploration strategies remains a serious
challenge and a worthy research project.

Finally the perspective of active inference – that action and control
are merely inference problems over a graphical model which includes
action variables – naturally lends itself to an understanding of
different kinds of inference – specifically amortised and iterative
inference ( millidge2020reinforcement ; kim2018semi ;
marino2018iterative ) . Understanding how these different types of
action can be combined and merged together, to inherit the strengths of
both and ameliorate the weaknesses of each other, is ultimately going to
be very important in designing algorithms which can initially learn
rapidly from data and then slowly converge to a high asymptotic
performance. There is also strong, but circumstantial evidence, that a
system like this, which combines iterative and amortised inference,
takes place in the brain. For instance, whenever you start learning a
new skill, it takes a lot of thought and explicit mental planning, but
you can learn quickly without needing an extremely large number of
interactions with the environment. However, as you continue to practice,
slowly your skills become habitual. They do not need mental effort and
can occur almost automatically, allowing you to focus on other things
while they are occurring. This distinction between conscious, effortful
action and unconscious, effortless habit is precisely the distinction
between iterative and amortised inference. The benefits of having such a
hybrid system are obvious. It is quick to learn new skills since it can
explicitly plan with them, while once a skill has been practiced many
times it can be offloaded onto a computationally cheap habit system.
This eliminates the necessity, in current model-based reinforcement
learning systems, to undertake expensive model-predictive control and
planning on every single timestep, even when the system has practiced a
given contingency many many times, and also where computing the best
action is actually extremely straightforward.

While we have undertaken some preliminary work in this direction, as is
reviewed in this thesis, really the combination of the two to design
systems with explicit planning and habit systems is just beginning and
there are very many questions which remain unanswered. For instance,
what is the best way to train the habitual system – should it be trained
to mimic the decisions of the explicit planner, or be completely
independently trained on the reward, or both (i.e. by using the output
of the planner as a regulariser of some kind)? Should the habit system
and explicit planner optimize separate reward functions (for instance,
should the planner be more exploratory and the habit system just
optimize rewards?)? Should the habit system be used by the planner in
any manner – for instance habit value functions could be used to endstop
the model rollouts of the planner to provide better local value function
estimates? Should the habit policy be used to initialize the planner?
How should these systems interact to produce actual output actions –
should the output be the sum of both systems? or should there be some
gating mechanism which selects one or the other to produce the output?
If there is such a gating mechanism, how does it work and how should it
compute? How do we know when to turn off the planner and just use the
habit system, if ever? The answer to all of these questions is a
fascinating combination of engineering practice and machine learning
theory, and the end result of getting these questions right will be
flexible, robust, adaptable and sample-efficient systems which also have
a high asymptotic performance with low latency and computational cost
when the habit is established. Such systems could also be used to model
similar computations that occur in biological brains and may shed light
into the design choices available for such system and, ultimately, their
neural implementations.

### 7.2 Question 2: The Mathematical Origins of Exploration

In Chapter 5, we delved deeply into the mathematical origins of
exploratory behaviour. We saw that to obtain information-seeking
exploration as a core part of the objective functional, in addition to
reward maximization crucially entails minimizing a divergence objective
instead of an evidence objective. We then related this new dichotomy
between divergence and evidence objectives to a wide range of currently
used objectives within the reinforcement learning and theoretical
neuroscience communities. The importance of this result, really, lies
not in the relationship to existing methods, but what it tells us about
the deep foundation of exploration. Put simply, we see that extrinsic
exploratory drives emerge from trying to match rather than maximize.
Matching tries to maintain the complexity of the inputs, so that given a
complex desire distribution, agents are driven to stabilize a similarly
complex future. Conversely, maximizing implicitly tries to simplify the
inputs, ideally a maximizing agent would collapse all future inputs to a
dirac delta around the future reward. It is only the extent to which
there is uncertainty in the world, or in the reward function, or a lack
of controllability in the world which prevents this full maximization.
This difference is what gives rise to intrinsic exploratory behaviour in
the divergence minimization case, and to effectively anti-exploratory,
information-minimizing behaviour in the evidence, reward, or utility
maximizing case. We additionally see that reward maximizing agents do
not have, and cannot have, any intrinsic exploratory drives, since the
very nature of their objective compels them to minimize information
gain. Information and learning, to them, is a cost which must be borne,
and not a reward to be pursued for its own sake.

This additional information gain term is very important, because it
drives agents which optimize it to explore and seek out new
contingencies, and to find and update upon resolvable uncertainty in
their world. This means that agents which have an expressive desire
distribution, will tend to learn faster and better world models, as well
as pursue more exploratory policies, which in the long run lead to
higher performance than purely reward-maximizing agents, even when
judged on rewards alone. This has been investigated by ourselves in
Chapter 4, as well as under many other approaches in the literature.
What is most interesting here is that our understanding of these
objectives as divergence objectives provides a precise mathematical
characterisation of what these objectives are implicitly doing.

It is important to note that it is possible to derive some form of
information gain exploration directly from reward maximization – in the
form of explicitly computing and calculating with the value of
information ( still2012information ; schmidhuber2007simple ;
osband2019deep ; tishby2011information ) , where we can operationalize
the value of information purely in terms of reward as the additional
amount of reward expected given better policies as the result of
obtaining and integrating the information into your world and policy
models. While this value of information computation is theoretically
optimal given a reward maximization objective, in practice it is
intractable to compute exactly, and there has been little investigation
in the literature as to direct approximations of this term. However,
there are some heuristic approaches which seek to approximate it,
although it is not clear how well. For instance, osband2019deep argue
that using ‘optimistic value functions’ which automatically up-weight
unknown contingencies can lead to a good approximation of the value of
information, since in practice, the agent will automatically explore
until it has diminished its optimistic bias to the extent that the bias
for all contingencies is lower than the current best option. This
approach, under the names of the upper-confidence bound is widely used
in the multi-armed bandit literature ( garivier2011upper ) and
additionally has been used to great effect in Monte-Carlo Tree Search
algorithms ( kocsis2006bandit ) , and has been investigated to some
degree within deep reinforcement learning ( silver2017mastering ) .
Another approach, known as Thompson sampling ( russo2016information ) ,
explicitly computes or approximates the posterior distribution over
actions given the current history of observations and rewards, and then
achieves some degree of exploration by sampling from this posterior –
the idea being that in uncertain regions, the posterior distribution
should be fairly uniform, and thus provides effectively random
exploration, while when the posterior is sharp then it is likely that
the true optimum has been found and thus exploration is unnecessary and
costly.

An important challenge is that inference based approaches such as
control as inference do not naturally compute any analogue of the value
of information. This is because, ultimately, these approaches take a
mean-field factorisation across time and split their objective up across
time-steps. This means that information can only flow through time
through the transition model, and thus the agent cannot model any kind
of learning in the future, where information it may or may not discover
in the future leads it to change its model, leading it to perform better
(or worse) in the future. Due to this limitation, which is ultimately
applied for reasons of computational tractability, approaches like
control as inference do not compute any kind of value of information
across time-steps. This limitation also applies to the
information-seeking methods we discuss which arise from divergence
functionals. If these functionals are also mean-field factorized, then
agents only seek to maximize the information-gain in the current
time-step. Extending these methods by relaxing the temporal mean-field
assumptions will likely yield more effective and nuanced forms of
exploration, which can induce consistent exploratory behaviour across
multiple timesteps and thus handle more complex contingencies. However,
designing effective and mathematically tractable algorithms which can do
this largely remains an avenue for future work.

However, this information-maximizing exploration with divergence
functionals is not done explicitly to gain any kind of future reward.
Instead, agents optimizing divergence objectives treat optimizing
information gain as an intrinsic good. This is what allows an
information gain objective to arise even though the objective satisfies
the same mean field assumptions as the control as inference objective.
However, this means that to some extent divergence minimizing agents
will continue to explore beyond the point which is strictly necessary
for reward maximization. This means that, in effect, from the
perspective of a purely reward maximizing agent, divergence minimization
is just an additional exploratory heuristic by which to approximate the
value of information terms within a mean-field formulation. However, in
general, it has been empirically found that the information seeking
exploration in a mean-field fashion, while not strictly the value of
information, gives a good approximation in general and will lead to good
performance especially in high dimensional, sparse environments.
However, because it explicitly trades off a reward maximizing and an
information-seeking objective, it will tend to over-explore relative to
pure reward-maximizing value of information computation, by exploring
regions with much resolvable uncertainty but relatively little reward,
and will continue exploring even when it is likely (but not certain)
that the optimal solution has been found. However, as long as all
uncertainty in the environment is resolvable, then the divergence
objective will eventually converge to the reward maximization objective
since the information gain term will eventually become negligible once
the agent possesses a very good and accurate world model.

An interesting direction for future work will lie in relaxing the mean
field assumptions which currently underpin all of these functionals.
While it is likely that a full relaxation will be intractable, there are
many intermediate relaxations which have been proposed in the general
variational inference literature which could prove highly fruitful
within the control task. For instance, the Bethe free energy and related
objectives ( yedidia2001generalized ; pearl2014probabilistic ;
schwobel2018active ) , allow for temporal pairwise correlations to be
explicitly considered. Moreover, there is a highly general family of
‘region graph’ approximations ( yedidia2005constructing ;
yedidia2011message ) which have been developed within the variational
inference literature, and which allow for more complex interactions to
be modelled within a relatively tractable computational framework and
which have been found to improve inference performance. If there is a
way to compute successively better Bayesian approximations to things
like the value of information, or to relax the temporal mean-field
approximations made in contemporary divergence and evidence objectives
for control, it will likely come from a thorough mathematical and
experimental investigation of these more advanced and accurate
approximation techniques. Understanding how the information gain
functionals from divergence objectives function and change as the
temporal mean-field approximation is relaxed is especially interesting,
since preliminary investigations, even within the mean field paradigm,
show that when expressly writing out the objective in terms of entire
trajectories, terms similar to empowerment ( klyubin2005empowerment )
and filtering information gain (backwards in time) result.

Related to the relaxation of the temporal mean-field approximation,
there also needs to be much work allowing agents to explicitly model
changes to their own beliefs in the future. This is necessary to truly
compute realistic information-seeking objectives when utilizing
multi-step planning algorithms, since currently all information gain is
computed with respect to the agent’s current beliefs, which will not
necessarily hold in the future if and when it actually eventually
reaches this information. Such a process would enable an agent to
understand how various kinds of information would change its own beliefs
and policies, and thus be able to plan for multiple sequential
‘realizations’. Human planning is certainly capable of such
introspective capabilities, where we can, for instance, decide to seek
out and investigate certain phenomenon in order to be able to understand
and better seek out information about another task, and so on. Some
fascinating recent work has begun to explore these sorts of
metacognitive abilities within the active inference framework (
friston2020sophisticated ) , but only within a discrete state space and
nonscalable task. The true issue with such approaches will be their
inherent computational difficulty, since on a naive approach they will
require the agent to simulate its own belief updates, requiring it to
store and introspect upon a copy of its own models and inference
procedures. However, since such metacognitive abilities are likely
crucial to effective long range planning, an important strand of future
work will be designing approximations and objectives which can
accomplish this in a computationally tractable manner.

Additionally, the divergence vs evidence framework presents a new class
of objectives which are, in theory, distinct from the usual paradigm of
reward or utility maximization. Here, instead we simply seek to minimize
divergence to a complex reward or desire distribution. In theory, the
idea of having a complex, multimodal and potentially non-scalar desire
distribution instead of a simply scalar reward function to maximize is
that in theory it allows for more complex notions of goals or desires to
be implemented and optimized by agents. Specifically, it allows for
vector-valued , and multimodal goals which are not well handled within
the standard reward-maximization framework, but which can be
straightforwardly handled within our formulation of a desire
distribution by both evidence and divergence objectives. While current
work has mostly focused on demonstrating the equivalence of
reward-maximization, and the desire distribution under certain
conditions (the desire distribution being a Boltzmann distribution of
the reward), and thus showing that the probabilistic case is a strict
generalization of the scalar deterministic case, the real interest of
the probabilistic representation is precisely how it can differ from
simple reward-maximization. Much work remains to be done to understand
the possibilities for more flexible and expressive goal or value
representation which is unlocked by this more general formalism, and how
it can be leveraged to design artificial systems which perform more
capably in practice.

Finally, the idea of divergence minimization also has extremely close
links with Markov-Chain-Monte-Carlo inference procedures, where it has
recently been realized that much of this paradigm can be expressed
within a simple framework of stochastic dynamical systems theory,
whereby all the various samplers can be interpreted as implementing a
certain stochastic differential equation which explicitly performs a
gradient descent on a divergence objective ( ma2015complete ) , with
different algorithms in the literature simply specifying different noise
terms and solenoidal flows ( yuan2017sde ) . Beyond this, the idea of
divergence minimization can also intriguingly be linked to recent
advances in stochastic non-equilibrium thermodynamics (
seifert2012stochastic ) , which has developed ways to translate classic
thermodynamic notions of entropy and entropy production from properties
of large ensembles to properties of individual statistical trajectories
( esposito2010three1 ) . A crucial result in this new formalism of
stochastic dynamics is that any system with positive entropy production
can be construed as minimizing a divergence between its current state
and its ultimate steady state density ( esposito2010three1 ) – and can
thus be interpreted as performing a form of direct divergence
minimization – thus potentially implying that this objective may in some
sense be a more natural one for systems and agents to perform than pure
evidence maximization, and secondly that the laws of thermodynamics
themselves may implicitly require information-maximizing behaviour from
disspative non-equilibrium systems. While these links currently remain
speculative, and further investigation will likely discover greater
nuance and require some qualification of these claims, in my opinion
there is significant potential here to link discoveries in stochastic
thermodynamics to help us build a fully general picture of the necessary
nature of exploratory behaviours in systems evincing the classic
action-perception loop.

### 7.3 Question 3: Credit Assignment in the Brain

By showing that predictive coding can approximate backpropagation along
arbitrary computation graphs, instead of just MLPs, we have specifically
turned predictive coding from a direct model of brain function or
perception, into a learning algorithm which can be applied to arbitrary
architectures. This dual perspective, where predictive coding is both a
generic learning algorithm, as well as specifically a model of learning
and perceptual inference in the brain is most interesting and, as far as
I am aware, is unique to predictive coding. Specifically, casting
predictive coding as a learning algorithm makes clear several
interesting correspondences between inference procedures and learning.
Specifically, that we can derive a learning algorithm on a computational
graph by trying to infer the values of the nodes in the graph.
Predictive coding additionally provides for a straightforward extension
to backprop in the form of precisions, which allow for the learnable up
or down weighting of certain gradient signals depending on the intrinsic
noise of their generating process. Such a system, while not particularly
useful in the standard machine learning paradigm of independent,
identically distributed datasets, may prove extremely important for
learning with more ecologically valid sensory streams which contain
various amounts of noise and distractor information which should not be
learnt from. This perspective of learning and credit assignment as a
kind of inference also immediately lends itself to further applications
and extensions beyond just precision. For instance, the effect of
different generative models other than Gaussian remain to be determined,
as well as potentially different optimization procedures and variational
functionals to be optimized. In general, this perspective allows the
highly developed machinery of inference in graphical models (
pearl2014probabilistic ; ghahramani2001propagation ; beal2003variational
; yedidia2011message ) to be deployed to improve credit assignment and
optimization algorithms. This area, I believe, is an exciting and
potentially highly impactful one for future work since the impact of
improving either credit assignment or optimization processes, which are
at the heart of all of modern machine learning, will necessarily be
substantial.

Furthermore, by demonstrating that many of the biological
implausibilities in the predictive coding scheme can be relaxed (
millidge2020relaxing ) , we, for the first time, demonstrate a
biologically plausible local approximation to backprop which does not
suffer from either issues of locality or issues of weight transport.
Furthermore, we develop a novel and much simplified algorithm –
Activation Relaxation – which possesses relatively straightforward,
local, and elegant update rules when compared with predictive coding
which succeeds in approximating the backpropagation of error algorithm
to arbitrary accuracy given enough iteration steps. Moreover, we have
shown that the same relaxations which work with predictive coding, also
work with the activation relaxation algorithm, thus demonstrating the
robustness and efficacy both of the AR algorithm and of the methods of
relaxation utilized. Crucially, if we look at the final, relaxed AR
update rule (Equation 6.32 , we see that the change in activities for a
layer only requires the current activation values of the layer above,
mapped through the backwards weights which are learnt independently of
the forward weights, be subtracted from the current activation of the
layer. This is sufficient, over a number of iterations, to allow the
activations of each layer of the network to converge to the gradients of
backprop. Then, once this is achieved, the weights can be updated.

While these algorithms have considerable advantages – they exactly
approximate backprop given enough iterations, they require only
biologically plausible local update rules, and they are simple and
potentially straightforward to implement in neural circuitry, they also
have substantial disadvantages. I believe these disadvantages are worth
discussing in some depth since they provide a precise specification of
the areas for improvement in current algorithms. The fundamental
disadvantage of iterative schemes like this is their iterative nature.
Specifically, they require separate phases of operation – a forward
phase which is equivalent to a feedforward pass through the network, and
a backwards iterative phase of multiple dynamical iterations. A
significant issue is that it is unclear whether such phases can
realistically exist within the brain. While there is evidence for
different oscillatory frequency bands in the brain ( buzsaki2006rhythms
) , and even in superficial vs deep cortical layers ( bastos2020layer ;
bastos2015visual ) , it is unclear whether these rhythms do, or can,
coordinate separate feedforward and iterative phases. Such a scheme, if
implemented in the brain, would form a kind of clock, only allowing
feedforward information to be processed in short bursts in between the
iterative phases. While not impossible, this seems at odds with our
current understanding of the brain where feedforward and feedback inputs
are combined together in real time. Another very straightforward problem
is simply the number of iterations these schemes require. The brain
cannot wait for tens or hundreds of iterations until convergence, even
under generous assumptions about rhythmic activity implementing the
phases, while these schemes often require a substantial amount of
iterations to converge to nearly exactly the backprop gradients. While
this could be ameliorated somewhat with high learning rates, and
settling for less than exact convergence to the backprop gradients, it
has not yet been extensively investigated whether the algorithms are
even stable under such conditions. Understanding and optimizing the
number of iterations and various parameters like the learning rate is
still an open area of research. Unlike backprop with deep neural
networks, where all hyperparameters have been effectively extensively
tuned in the literature over a decade of experimentation, good
hyperparameter settings for these alternative algorithms have barely
been explored at all, and their empirical limits of performance at scale
have largely yet to be determined. A final serious difficulty with such
approaches is that to match backprop they require some level of
nonlocality in time, where information from the feedforward pass is
stored and then utilized throughout or at the end of the backwards pass.
This storage of information is fundamentally necessary because the
backprop gradients depend only on the state of the network in the
feedforward pass. If the state changes due to the iterative algorithm,
then the old information from the forward pass must be stored somehow to
maintain convergence to backprop on the forward pass. This manifests
itself in predictive coding as the fixed-prediction assumption, which
explicitly assumes that the values of the forward pass are stored. In
the AR algorithm, it manifests in the necessity to store the original
value of the activity neurons to use to update the weights after the
backwards pass is complete. Due to this storage, the actual update
equations become nonlocal in time. This shortcoming could potentially be
addressed in two ways. Firstly, it might be possible to store the
information from the forward pass, for instance in local recurrent units
which are insulated from the activity changes in the dynamical
iterations, or secondly, if the number of iterations is short enough,
such information could be persisted simply through multi-step recurrent
connectivity. Nevertheless, even if this could be done, the circuitry to
align and ensure the correct time of arrival of all the necessary
signals could be quite complex.

In this field, it is important to reflect deeply upon the role and
utility of simplified models of neural dynamics. Almost all work in this
area operates with very simple models of rate-coded integrate and fire
neurons, typically often in a temporally static ‘instantaneous’
computation graph. Biological plausibility within this model, while a
somewhat vague concept, is often defined by conditions such as only
local connectivity, or that connectivity must be additionally Hebbian,
and that neural connectivity cannot be too precise, and that information
cannot be directly transmitted backwards along axons. However, this
definition necessarily ignores some important aspects of reality.
Obviously the brain uses spiking neurons and must also achieve credit
assignment through time, but additionally there are even questions about
the simplification of neural architecture. For instance, typically such
models implicitly assume a multilayer perceptron (MLP) style
architecture as just a stack of fully connected layers, however each
region of the cortex has an intricate 6-layer structure, and it is not
clear whether each such layer in the cortex should be considered as a
single layer of the MLP, or only the cortical region. If the former,
then the different properties and neurophysiology of each cortical layer
is not modelled. If the latter, then it is not at all clear to what
extent the entire cortical region can be modelled as a simple fully
connected layer. An additional interesting neurophysiological fact which
is rarely explicitly modelled is the predominance of cortical columns in
the visual cortical regions. These columns do not at all correspond to
fully connected layers and it is not yet fully clear what their
computational role is. A straightforward hypothesis is that they are the
brain’s way of implementing a local receptive field operation, like
convolution in convolutional neural networks, but without the advantage
of shared weights across space which is core to the generalization
capabilities of the CNN ( hawkins2007intelligence ) .

The question remains, however, how applicable are such simplified
rate-coded models to the full complexity of credit assignment in spiking
networks through time. The hope, ideally, is that by and by large
important computational primitives and algorithms which have been
developed for biologically plausible credit assignment in rate-coded
neural networks remain functional, or only require minor adaptation, to
work in a spiking context. This is a strong possibility, but it could
also be completely false, and that the brain implements an algorithm
which relies heavily on the unique properties of spiking networks for
its credit assignment capabilities. Ultimately, before we fully
understand the mechanisms of credit assignment in the brain, it will be
hard to fully assess the degree to which work on rate-coded models will
generalize. There has been some preliminary successes though. For
instance, the surrogate gradient technique shows that with only minor
tweaking (defining a surrogate gradient to avoid the nondifferentiable
threshold spiking function), backprop through time (BPTT) can be
straightforwardly used to train spiking neural networks for complex
tasks ( zenke2018superspike ; neftci2019surrogate ) . However, these
methods currently utilize the biologically implausible BPTT algorithm
and it is unclear to what extent biologically plausible alternatives to
BPTT can be straightforwardly applied in such a manner. To answer such a
research question would be an important and timely research agenda which
would substantially advance our understanding of the generalizability of
such models.

Fundamentally, the core challenge is that of time. Most models (with
some exceptions ( bellec2020solution ; schiess2016somato ) ) focus only
on backpropagation and credit assignment through space (i.e. layers of a
neural network architecture) and not through time. However, time is an
inextricable component of the computation in the brain. Fundamentally,
perception in the brain is not about handling static @xmath datasets,
but rather filtering on constantly changing sensory streams of
information. Moreover, credit assignment through time is in some sense a
substantially harder problem, in that the key information necessary at
each step disappears, while when backproapgating through space the
information is always present somewhere in the graph. In the case of
BPTT, all the intermediate activations at each timestep are stored and
then replayed in backwards sequence once the sequence has ended.
However, such an acausal solution is clearly not suitable for
computation in the brain. There are alternatives to BPTT which do not
require explicitly repeating computations backwards through time, but
allow for online integration of gradients. The key such algorithm is the
RTRL algorithm which maintains a Jacobian of values at each timestep and
iteratively updates them at every timestep. In algorithmic terms, RTRL
corresponds to forward-model automatic differentiation while BPTT
corresponds to reverse-mode. Unfortunately, however, RTRL is
substantially more expensive to implement on digital computers, scaling
with @xmath where N is the number of parameters, rather than @xmath for
BPTT, where T is the time horizon. One key advantage of RTRL however is
that unlike BPTT it is not bound by sequence length. While BPTT must
choose some point to stop and then backpropagate all the gradients and
then truncate gradients from after time T, RTRL can operate on sequences
with indefinite length without issues, although credit is slowly diluted
away through time. However, RTRL is also likely not neurally plausible
and it is not clear how to implement RTRL in systems with multiple
recurrent layers interacting without a blowup in the number of learning
rules required. My personal hunch is that extending current models of
local, biologically plausible credit assignment to spiking neural
networks will not be especially challenging, although they might not be
the method that the brain uses. However, I believe that credit
assignment through time is a fundamentally different and more
challenging problem than credit assignment through space, and that
ultimately, to solve the problem of credit assignment in the brain we
must grapple head on with the problem of local credit assignment through
time.

There are already some methods existing in the literature which
accomplish this, specifically eligibility-prop ( bellec2020solution )
proposes eligibility traces to compute the RTRL algorithm in a local
fashion. However the method currently only works for a single recurrent
layer while the brain is deep both in time and in space, and that this
depth requires new algorithms, or the combination of existing algorithms
for backprop in space with those for backprop in time. It is also
unclear to what extent the brain computes the full credit assignment
mandated by RTRL or else some sparse approximation therein, perhaps
aided by the natural sparsity properties of spiking neural networks.

An additional consideration when thinking about credit assignment in the
brain is the question of feedforward vs feedback processing (
kriegeskorte2015deep ) . While the BPTT is only designed for
backpropagating through immediately recurrent feedforward neural
networks, the brain actually contains a multitude of long-range
recurrent loops mediated by top-down feedback connectivity (
felleman1991distributed ; grill2004human ) . Understanding the
properties of these and whether they are used for credit assignment, or
whether the brain computes credit backwards through these top-down
connections is also crucial for understanding the full picture of credit
assignment in the brain. An additional, and almost entirely unanswered
question is the role of long term memory in credit assignment. Current
methods, including BPTT, typically use some temporal cut-off to stop
considering gradient information beyond some time-horizon, and a key
flaw in naive recurrent architectures is that information is slowly lost
over time ( ollivier2015training ; hochreiter1997long ) . The key
innovation in LSTM units is that they contain an explicit
forget/remember gate which allows for memories to be stored potentially
indefinitely ( hochreiter1997long ) . There are additionally various
modifications for recurrent RNNs which help ameliorate this problem as
well ( ollivier2015training ) . Since (we assume) the brain is equipped
with a fairly standard recurrent architecture, the architectural or
learning-rule modifications that enable it to avoid this problem and to
successfully store and utilize even long term credit assignment remains
to be explored and is a very important and fundamental question in
understanding credit assignment and learning in the brain. One
hypothesis is that synapses in the brain may store a temporal hierarchy
of eligibility traces which retain information over progressively longer
timescales, thus allowing them to act on information which has been
accumulated even in the relatively distant past. However, figuring out
the actual specific operation of such a system remains an open research
challenge.

A further interesting question, raised by the recent successes of
instantaneous feedforward architectures such as transformers in
processing sequential data such as natural language text over recurrent
architectures such as LSTMs is to what extent recurrence is actually a
useful computational primitive for handling sequence data as opposed to
one the brain may be forced into due to its inherent computational
constraints and need for online processing instead of batch processing
at the end of a sequence, as in transformers. A key advantage of
attention, as implemented in transformers ( vaswani2017attention ) , is
that it enables arbitrary time-to-time modulation, rather than in
recurrent architectures where the immediate past inputs are combined
with the present ones to predict the future. In this way, transformers
can handle data in a fundamentally acausal manner, with accompanying
computational advantages. It remains to be seen whether attention like
mechanisms can be implemented in a recurrent way, whether recurrent
architectures can be successfully scaled to the level that transformers
have done, or whether they remain on an inferior scaling curve (
kaplan2020scaling ) , and the precise mechanism by which similar
sequence computations are implemented in the brain. The key lesson of
attention is that recurrence is not the only way to handle intrinsically
sequential inputs. It remains to be seen whether other non-recurrent
mechanisms are implemented in the brain.

Importantly, although this line of work has focused on determining
whether the backpropagation of error algorithm can be implemented in the
brain, which is inspired by the impressive success of modern machine
learning, which is based on this algorithm, it is not entirely certain
that the brain achieves credit assignment through backpropagation at
all. There are several alternative methods which are worth discussing in
some detail. For instance, it is also possible that the brain may be
implementing some more advanced kind of learning algorithm than
stochastic gradient descent. meulemans2020theoretical showed that target
propagation implements an approximate version of a second order gradient
descent scheme – known as Gauss-Newton optimization. Similarly there are
other optimization methods, such as conjugate gradients, or coordinate
ascent, or fixed-point iteration, as well as a variety of probabilistic
message passing schemes which do not require explicit gradients to be
computed ( yedidia2011message ; parr2019neuronal ) . Moreover, it is
possible that the brain, while needing to compute gradients, does not
compute by the backpropagation of error algorithm. For instance, it is
possible to compute gradients by finite differences, and although these
finite differences perform strictly worse than automatic differentiation
techniques computationally, they are very simple to implement in
practice. For instance, the brain could easily contain circuits which
could compute time derivatives of a constantly varying temporal stream.
Then, once we have the time derivatives of two variables, it is possible
to directly compute their gradient by @xmath . Such a circuit would only
need local temporal finite differences as well as a divisive feedback
connection to combine the two time derivatives, well within the
possibilities afforded by known neurophysiological constraints. A
further option would be that the brain simply may not compute with
derivatives at all, all the while optimizing some objective function.
There are many ‘black box’ optimization methods which do not require
gradients of the objective. For instance, genetic algorithms (
salimans2017evolution ) , or other brute-force-esque algorithms may
instead be implemented, and indeed it has been shown that in some cases
genetic algorithms, when sufficiently scaled are able to compete with
backprop on some optimization tasks ( salimans2017evolution ;
such2017deep ) . Another possibility, is that the brain could learn
solely by global rewards broadcast to all neurons, where learning
effectively takes place via the policy gradient theorem (
roelfsema2005attention ) . There is some circumstantial
neurophysiological evidence in favour of this – specifically the well
known role of dopamine as a spur to synaptic plasticity ( dayan2009goal
; dayan2008decision ) , as well as the fact that many cortical pyramidal
cells receive global dopaminergic inputs from subcortical regions. There
have also been a variety of models ( roelfsema2005attention ;
pozzi2018biologically ) proposed of this kind of global reward-driven
learning. However, this type of learning suffers from a severe intrinsic
flaw that the gradients it estimates for each parameter have extremely
high variance. This is because each neuron is only provided with a
global reward signal, which is ultimately caused by the interaction of
an extremely large number of other neurons. Thus, averaging out all the
noise introduced by all the other neurons in the brain requires a very
large statistical sample, thus effectively leading to extremely high
variance gradients and slow learning, which scales increasingly poorly
with network size. This is precisely why backprop is such a useful
algorithm, since it provides precise feedback to each neuron about the
loss, thus meaning that the only source of noise is minibatch noise
rather than intrinsic noise due to the activities of other neurons. This
means that backprop computed gradients have much lower variance and can
lead to much faster learning. Nevertheless, it remains inarguable that
pyramidal cells are generally innervated by dopaminergic inputs and that
dopamine, which is released when there is a reward prediction error in
the basal ganglia ( schultz1998predictive ; schultz1998reward ;
dayan2009goal ) can strongly modulate learning. This may imply that
there are effectively two learning systems on top of one another in the
brain. The first, potentially older, is the global slow dopaminergic
system, while the second, entirely cortically based uses precise
vector-feedback with some backpropagation like algorithm. The global
reward signals, then can potentially modulate various aspects of
learning so that contingencies with high reward prediction error are
especially salient and may induce larger changes than those without (
daw2006cortical ; roelfsema2005attention ) .

Finally, it is always possible that we have been misled by the
contemporary successes of machine learning, and that the core
formulation where we perceive the brain implicitly or explicitly
optimizing some objective is simply wrong, and that the brain cannot be
described in such a way at all. While this is a very general formulation
of perception and learning, with support from both machine learning and
statistics and control theory in engineering, it nevertheless could not
be a productive framework in which to think about the function of the
brain. Such a scenario would arise, for instance, if the brain was
merely a grab-bag of various heuristics and reflexes, implicitly tuned
over the course of evolution, without any serious potential for
learning. While this may be true of the brains of some simple animals,
it is clearly not for humans and other creatures with complex, learnt
cognition. Nevertheless, if this is somehow the case, the question will
then become what is a productive mathematical framework in which to
think about what the brain is doing, if not in the language of
probabilistic models and objective functions. It may be that the answer
to this question, if it must be posed, is more interesting and
productive than discovering that the brain was simply doing backprop all
along. Nevertheless, it is extremely unclear at present which of these
possibilities is true. My opinion, given the mathematical elegance and
generality, as well as the empirical successes of the objective function
viewpoint, is that it is a valuable and most likely correct framework
for understanding the operation of the brain. However, it is always
worth noting that this is purely a speculative hunch, and there remains
little non-circumstantial evidence one way or another.

### 7.4 Closing Thoughts

This thesis is titled ‘ Applications of the Free Energy Principle for
Machine Learning and Neuroscience ’, and throughout the thesis we have
endeavoured to demonstrate how to adapt and extend methods from the free
energy principle and its primary process theories – active inference and
predictive coding – to make advances in deep reinforcement learning, and
in neuroscientific theories of perception and credit assignment in the
brain. Interestingly, the pattern of progress in this thesis is that, in
parallel, between the neuroscience and the machine learning, is that
first we simply try to adapt and extend the methods of the free energy
principle to test their capabilities against other existing methods from
the core literatures of these subjects, and then strive to show how
process theories like active inference and predictive coding can extend
and advance upon the current state of the art. Then, this process
inevitably reveals new and interesting questions by itself, which are
somewhat separate from the free energy principle and its process
theories. These questions are firstly the mathematical origin of
exploration (which emerges from considering the nature of the expected
free energy objective in active inference), and secondly credit
assignment in the brain, which emerges from considering how predictive
coding relates to backpropgation of error. In the second set of chapters
(5 and 6) we have then tried to address these further questions, using
methods inspired by the free energy principle, but not necessarily
directly deriving from it. We believe that in many ways this reflects
the theoretical fertility and utility of the FEP as an abstract
principle, that it allows the postulation and exploration of deeper
questions than would be possible without it. Indeed, this theoretical
fertility may be one of the primary means of judging the utility of the
FEP since, as we discussed in Chapter 2, the FEP is technically
non-falsifiable and can be considered more of a mathematical principle,
or perspective, rather than a theory. If this is the case, then the work
in this thesis provides support to the contention that the FEP provides
a useful and fruitful perspective to understanding a variety of
questions both machine learning and neuroscience.

The phenomenological process of intellectual understanding is an
interesting one. At first you appear to spend ages groping around in the
dark, hitting various unknown objects in your way, but slowly gathering
a picture of the obstacles in your path. Then, at long last, you
eventually stumble your way to a light-switch and the whole vista is
revealed. What were once unknown lurking obstacles are transformed into
precisely specified, and tractable, objects, whose relation to one
another can be seen at first glance. In the light, everything is at once
clearer, but also smaller . Questions and dilemmas which seems huge and
irreducibly complex are revealed to be straightforward, even trivial, in
the light of understanding. So much so that it is often easy to forget,
looking back, how these questions appeared when the answer was unknown.
From a personal perspective, and one I have hoped to share in this
thesis, I believe I have managed to obtain and present a clear
understanding of two topics which were not clear in the literature
before my PhD – whether active inference can be successfully scaled up
to compete with modern reinforcement learning methods (and the
relationships between the two theories), and the mathematical origins of
the exploration term in the expected free energy, and its relationship
to more standard functionals such as the variational free energy for
perception. I have additionally contributed to the theory and
implementation of predictive coding models, as well as algorithms for
biologically plausible backprop in the brain. I feel, however, that
while I have uncovered certain key aspects and facets of the problem of
credit assignment in the brain, which were previously shrouded, I have
not yet reached the point where all mystery falls away and the light
pours in. Similarly, I hope that the work in this thesis has helped you,
the reader, understand some things at least a little more clearly.

To conclude, we offer some ideas of for the future development of the
ideas worked out in this thesis. We believe it is clear, we have shown,
that active inference can be productively related and merged with the
large and extremely powerful set of deep reinforcement learning agents
to enable general and flexible learning based algorithms which can
succeed in challenging tasks, even those requiring a considerably degree
of exploration. Further progress in this field, from the perspective of
active inference, should move beyond merely scaling up, and instead
focus on the unique insights and ideas that active inference brings to
the table. Examples of this include its distinct and exploratory
objective functionals such as the Expected Free Energy, or the free
energy of the Expected Future. Another potentially interesting avenue
lies in active inference’s more flexible consideration of reward as a
specific prior distribution , rather than a scalar value. This could
enable more flexible behaviour through better reward speicfication, and
include the ability to learn flexible reward distributions on the fly,
effectively performing reward shaping in a semi-autonomous manner. A
final avenue for exploration is the refinement of the specific
generative models used in control agents. A key tenet of active
inference is the idea of deriving powerful behaviours from inference on
detailed and flexible generative models of the dynamics of the world.
While in this thesis, we have focused primarily on simply approximating
and parametrizing the distributions in the generative model with deep
neural networks, a more refined approach might be to investigate whether
the world can be factorized in a tractable manner, and design generative
models to explicitly exploit these factorizations to allow for more
tractable and efficient inference – effectively giving the agent just
the right inductive biases about the world to subserve its control
objectives. Secondly, for the question of credit assignment in the
brain, I believe that the key advances will come in understanding the
temporal component of credit assignment, and this requires deeply
understanding the fundamental computations of the brain as performing
dynamical algorithms such as filtering and smoothing rather than merely
static Bayesian inference. This line of research would focus attention
both on how the brain can achieve backpropagation through time as well
as through space. There are also extremely interesting links here with
predictive coding, where dynamical approaches using generalized
coordinates remain underexplored, and it may be that by further
developing the explicitly Bayesian approaches to filtering provided by
the dynamical formulations of predictive coding, we can better
characterise and understand the temporal nature of the brain’s
computations. The work in this thesis relating predictive coding and
Kalman filtering is only the beginning, there needs to be much work done
scaling and precisely characterising the performance of dynamical
predictive coding algorithms, understanding whether such recurrent
predictive coding networks can be utilized to perform some form of
temporal credit assignment, as static predictive coding networks can.

## Appendix A Derivation of Kalman Filtering Equations from Bayes’ Rule

In this appendix we derive the Kalman filtering equations directly from
Bayes rule. The first step is to derive the projected covariance,

  -- -------- -------- -- -------
     @xmath   @xmath      (A.1)
              @xmath      (A.2)
              @xmath      (A.3)
              @xmath      (A.4)
              @xmath      (A.5)
  -- -------- -------- -- -------

Step 11 uses the fact that matrices A,B are constant so come out of the
expectation operator, and that it is assumed that covariances between
the state, the noise, and the control – @xmath , @xmath , @xmath – are
0. Step 13 uses the fact that @xmath and that @xmath .

Next we optimize the following loss function, derived from Bayes’ rule
above (equation 5).

  -- -------- -------- -- -------
     @xmath   @xmath      (A.6)
  -- -------- -------- -- -------

To obtain the Kalman estimate for @xmath we simply take derivatives of
the loss, set it to zero and solve analytically.

  -- -------- -------- -- --------
     @xmath   @xmath      (A.7)
              @xmath      (A.8)
              @xmath      (A.9)
     @xmath   @xmath      (A.10)
              @xmath      (A.11)
              @xmath      (A.12)
              @xmath      (A.13)
              @xmath      (A.14)
              @xmath      (A.15)
              @xmath      (A.16)
              @xmath      (A.17)
              @xmath      (A.18)
              @xmath      (A.19)
  -- -------- -------- -- --------

Where @xmath and is the Kalman gain and @xmath is the projected mean.

The first few steps rearrange the loss function into a convenient form
and then derive an expression for @xmath directly. Step 22 applies the
Woodbury matrix inversion lemma to the @xmath term. The next step
rewrites the formula in terms of the Kalman gain matrix K and multiplies
it through. The other major manipulation is the multiplication of the
last term of equation 23 by @xmath which is valid since @xmath .

This derives the optimal posterior mean as the analytical solution to
the optimization problem. Deriving the optimal covariance is
straightforward and done as follows,

  -- -------- -------- -- --------
     @xmath   @xmath      (A.20)
              @xmath      (A.21)
              @xmath      (A.22)
              @xmath      (A.23)
              @xmath      (A.24)
              @xmath      (A.25)
              @xmath      (A.26)
              @xmath      (A.27)
  -- -------- -------- -- --------

Which is the Kalman update equation for the optimal variance. The second
line follows on the assumption that @xmath . On equation 32 the
definition of the Kalman gain is substituted back in and the two @xmath
terms cancel.

## Appendix B Appendix B: Equations of the LSTM cell

The equations that specify the computation graph of the LSTM cell are as
follows.

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The recipe to convert this computation graph into a predictive coding
algorithm is straightforward. We first rewire the connectivity so that
the predictions are set to the forward functions of their parents. We
then compute the errors between the vertices and the predictions.

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

## Appendix C Appendix C: Predictive Coding Under the Laplace
Approximation

In the main derivation in Chapter 3 of the variational free energy
@xmath , we used the assumption that the variational density is a dirac
delta function: @xmath . However, the majority of derivations, including
the original derivations in ( friston2005theory ) instead applied the
Laplace approximation to the variational distribution @xmath . This
approximation defines @xmath to be a Gaussian distribution with a
variance which is a function of the mean @xmath : @xmath . Notationally,
it is important to distinguish between the generative model @xmath , and
the variational distribution @xmath . Here we use the lower-case @xmath
to denote the parameter of the variational distribution. The lower-case
is not meant to imply it is necessarily scalar. As we shall see, the
optimal @xmath will be become the inverse-Hessian of the free energy at
the mode.

Intuitively, this is because the curvature at the mode of a Gaussian
distribution gives a good indication of the variance of the Gaussian,
since a Gaussian with high curvature at the mode (i.e. the mean) will be
highly peaked and thus have a small variance, while a Gaussian with low
curvature will be broad, and thus have a large variance. While our
derivation using a dirac-delta approximation and the standard derivation
using a Laplace approximation obviously differ, they ultimately arrive
at the same expression for the variational free energy @xmath . This is
because both approximations effectively remove the variational variances
from consideration and only use the variational mean in practice. Under
the Laplace approximation, the variational coveriance has an analytical
optimal form and thus does not need to be optimized, and plays no real
role in the optimization process for the @xmath s either. We chose to
present our derivation using dirac-deltas in the interests of
simplicity, since as we shall see the Laplace approximation derivation
is somewhat more involved.

To begin, we return to the standard energy function in multilayer case,
this time under the assuption the Laplace approximation.

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (C.1)
  -- -------- -------- -- -------

Where we have used the analytical result that the entropy of a gaussian
distribution @xmath . Then, we apply a Taylor expansion around @xmath to
each element in the sum,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (C.2)
  -- -------- -------- -- -------

Where in the second line, we have used the fact that @xmath and that
@xmath , which is that the expected squared residual simply is the
variance. We also drop the expectation around the first term, since as a
function only of @xmath and @xmath , it is no longer a function of
@xmath which is the variable the expectation is under. We can then
differentiate this expression with respect to @xmath and solve for 0 to
obtain the optimal variance.

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (C.3)
  -- -------- -------- -- -------

Given this analytical result, there is no point optimizing @xmath with
respect to the variational variances @xmath , so our objective simply
becomes,

  -- -------- -- -------
     @xmath      (C.4)
  -- -------- -- -------

which is exactly the same result as obtained through the dirac delta
approximation.