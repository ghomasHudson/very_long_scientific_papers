### Chapter \thechapter Introduction

The ability to use and comprehend language as a mean of communication is
a unique characteristic of human intelligence. Natural language is
fundamentally non-stationary: the way we produce utterances is modulated
by a myriad of factors that are ever changing based on our
interlocutors, social context, emotions, or communication medium used.
Our capacity to operate in and adapt to such a dynamic environment is
another defining feature of intelligence. Indeed from their youngest
age, infants are seldom placed in static situations, yet they are still
able to learn emerging patterns from a constant flow of information (see
e.g. Dupoux ( 2018 ) ). Hence, it is a natural goal for artificial
intelligence research to develop agents that are capable of acquiring
natural language understanding and generation abilities in the face of a
changing environment.

The development of computer systems which can understand and produce
natural language is realized through Natural Language Processing (NLP) .
In recent years, neural-network based machine learning models have been
the source of rapid progress in NLP , and now represent the
state-of-the-art in a large number of real-world applications (such as
machine translation (Wu et al., 2016 ) , named entity recognition
(Lample et al., 2016 ) or speech recognition (Xiong et al., 2018 ) ),
reaching human parity on a variety of benchmarks (Wang et al., 2020a ;
Rajpurkar et al., 2020 ) ¹ ¹ 1 Meaning that they outperform human
annotated baselines, we make no claim that these systems have achieved
any kind of human-like intelligence. . Yet, these models exhibit a
pattern of failures when confronted with changes to their training
environment:

-   They are highly sensitive to domain shift : they perform poorly when
    confronted with text from a different domain, for example text
    concerning different topics (Gururangan et al., 2020 ) ,
    demographics (Blodgett et al., 2016 ; Amodei et al., 2016 ; Hovy &
    Søgaard, 2015 ) , and even using different data collection processes
    (Gururangan et al., 2018 ) . In particular, these models often
    perform poorly when evaluated on sub-populations, ² ² 2 Domains that
    exist, but are under-represented in their training data (Sagawa
    et al., 2020 ) . and they can latch on to spurious correlations
    (McCoy et al., 2019 ) .

-   They are vulnerable to specific word or character-level
    perturbations (Papernot et al., 2016 ; Ebrahimi et al., 2018b ;
    Belinkov & Bisk, 2018b ) . This phenomenon poses security concerns,
    as this opens up trained models to various types of attacks by
    nefarious agents: e.g. bypassing toxicity detection models (Hosseini
    et al., 2017 ) .

-   When they are trained on new tasks or domains, they rapidly forget
    previously acquired knowledge, a phenomenon known as catastrophic
    forgetting (McCloskey & Cohen, 1989 ; Kirkpatrick et al., 2017 ) .
    This makes adaptation to new data distributions an arduous and
    unreliable process necessitating significant amounts of domain
    expertise and regularization (Miceli Barone et al., 2017 ) .

These shortcomings are not only at odds with the natural behaviour
observed in intelligent agents mentioned above, but represents a
liability for practical users and designers of such systems. Indeed,
machine learning models trained on static datasets can fail
spectacularly in the face of real-world situations, sometimes with dire
consequences (for example, this mis-translation of “good morning” into
“attack them” due to dialectic variations; Hern ( 2017 ) ). More
insidiously, the representation disparity between training datasets and
the real world can, for example, unfairly affect and further
disenfranchise minority groups (Hashimoto et al., 2018 ; Dixon et al.,
2018 ) .

The principal desired characteristic of the machine learning models used
in NLP is the ability to learn and generalize from a finite number of
training examples. The most common learning paradigm, empirical risk
minimization, relies on the assumption that this training data is
sampled from a fixed distribution, and generalization is construed as
the ability to perform well on unseen samples from this same
distribution. The failure cases described above occur when the model
operates in a different distribution than the one it was trained on, in
which case standard generalization guarantees break down. In this
context, the discrepancy between a machine learning model’s learning and
operating environment can be described as distributional shift . The
research presented in this thesis focuses on what we believe to be the
three major axes of progress towards addressing distributional shift in
the context of NLP : Evaluation , Robustness and Adaptation .

We start with evaluation because distributional shift is particularly
difficult to characterize and measure, especially in regard to natural
language. This is partly due to the absence of a canonical metric
structure of the data. In other words, it is not clear how to
efficiently measure the semantic similarity between two sentences and as
a result there is no straight-forward way to measure the discrepancy
between two samples, let alone two distributions. Therefore, as a
natural first step in addressing distributional shift, we propose a new
benchmark (Chapter I ) and evaluation metric (Chapter I ) to evaluate
domain shift and robustness to adversarial perturbations respectively.
With these tools in hand, we set out to construct robust models that are
trained to be less sensitive to distributional shift, even in the
absence of explicit information on the nature of shift. This is done by
leveraging the diversity of data within the training distribution to
ensure uniform performance over a variety of domains present in the
training data (sub-populations). Specifically we formulate a parametric
version of the distributionally robust optimization framework which
allows for training models that are more robust to sub-population shift
(Chapters II and II ). Finally, learning in a static environment is
fundamentally sub-optimal: we cannot expect our models to perform well
on each and every possible future setting, and we must be able to adapt
them to any new scenario we encounter. Consequently, we research
mechanisms by which we are able to fine-tune a trained model on new
evidence, without forgetting previously acquired knowledge (Chapter III
).

The detailed outline of our contributions in this thesis is presented
below:

-    Chapter Learning Neural Models for Natural Language Processing in
    the Face of Distributional Shift lays out a description of the
    distributional shift phenomenon and discusses previous and current
    work in this general area.

-    Part I: Measuring and Evaluating Robustness

    -   In Chapter I , we introduce a benchmark for evaluating machine
        translation models against distributional shift. Specifically,
        the proposed dataset — dubbed MTNT — provides a testbed for
        translation of noisy social media text, a challenging domain for
        common state-of-the-art models trained on news and parliamentary
        proceedings. This work was published as Michel & Neubig ( 2018a
        ) at EMNLP 2018 ³ ³ 3
        https://www.aclweb.org/anthology/events/emnlp-2018/ , Li et al.
        ( 2019 ) at WMT 2019 ⁴ ⁴ 4
        https://www.statmt.org/wmt19/robustness.html and (Specia et al.,
        2020 ) at WMT 2020. ⁵ ⁵ 5
        https://www.statmt.org/wmt20/robustness.html

    -   In Chapter I , we examine the difficulty of evaluating
        adversarial perturbations on NLP models, with an emphasis on
        sequence-to-sequence models. We propose an evaluation framework
        centered around the idea of evaluating the perturbations’ effect
        on the semantics of the input sentences. The content of this
        chapter originally appeared as Michel et al. ( 2019b ) at NAACL
        2019. ⁶ ⁶ 6 https://www.aclweb.org/anthology/events/naacl-2019/

-    Part II: Making Robust Models

    -    Chapter II describes an approach for training models that are
        robust to distributional shift, using a parametric version of
        the distributionally robust optimization (DRO) framework.
        Specifically, we train a model to minimize its expected loss
        under the distribution defined by another generative model (the
        “adversary”), which is itself trained to represent the
        worst-case distribution for the model. In experiments, we find
        that this approach (called Parametric DRO ) yields models that
        are more robust compared to other baselines in the DRO
        literature. This chapter was previously published at ICLR 2021 ⁷
        ⁷ 7 https://iclr.cc/Conferences/2021 as Michel et al. ( 2021 ) .

    -    Chapter II presents an improvement over the P-DRO approach
        whereby a parametric re-weighting of the training data is
        learned. With appropriate constraints, and a simple but crucial
        minibatch-level renormalization trick, we find that this results
        in a simple DRO approach which reliably outperforms other
        baselines and necessitates little hyper-parameter tuning. The
        work presented in this chapter is currently undergoing peer
        review.

-    Part III: Adaptation

    -   In Chapter III , we develop a technique for mitigating the
        catastrophic forgetting issue that arises when adapting a model
        to a new domain or a new task. The approach, inspired by ideas
        in information geometry, is demonstrated to drastically reduce
        forgetting compared to regular gradient descent, and enjoys
        complementarity with common continual-learning baselines. This
        chapter contains work that was released as a preprint.

### Chapter \thechapter Background and Literature Review

From a broad perspective, the goal of machine learning is to produce a
model that is able to perform a given task, such as predicting an output
variable @xmath from an input, @xmath ( e.g. sentiment given a movie
review, or French translation given an English sentence…) given finite
amounts of training data (in our example, @xmath pairs). In this
chapter, we formalize this setting for the rest of this thesis.

#### 1 Terminology and Definitions

In machine learning, we are confronted with learning to perform a task
given a finite number of examples. In the context of this thesis, a task
@xmath will be referring to a triplet containing two measurable spaces
@xmath and @xmath , as well as a probability distribution @xmath with
support on the Cartesian product @xmath . @xmath represents the space of
all possible inputs (for example the space of all @xmath images, or the
space of all English sentences), while @xmath stands for the space of
all admissible outputs (for example the set @xmath in sentiment
classification, or all sentences of a target language in translation).
Finally, @xmath will represent the true distribution of the data in
nature (for example it will assign high probability to a pair of English
and French sentences that are translations of each other).

In general, learning a task will consist of training a model to
approximate the conditional distribution @xmath , based only on a finite
number of training examples @xmath . Specifically, we will focus on
parametric models, i.e. models that are fully determined by a finite,
constant (with respect to the size of the training data) number of
parameters described by a vector @xmath that can take its values in
@xmath , the set of all possible parameters (in practice @xmath or a
subset thereof, where @xmath is the total number of parameters).

Traditionally, given a loss function @xmath measuring the discrepancy
between model @xmath ’s prediction on @xmath and the reference output
@xmath , the modeler will estimate parameters @xmath that better fit the
distribution @xmath data by minimizing the expected risk

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

Note that we abuse notation and write @xmath for @xmath to distinguish
between the dependency over @xmath , the model parameters, and @xmath ,
the model’s inputs. As implied earlier, one cannot compute the total
expectation @xmath and we must resort to minimizing the empirical risk
computed over a finite training set @xmath

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

#### 2 Distributional Shift

##### 2.1 What is Distributional Shift

In the empirical risk minimization ( ERM )paradigm, machine learning
models are trained to minimize their expected loss under a single,
static distribution. However, this assumption is not always valid in
practical applications. For example, a machine translation system
trained on a majority of news article might come to be used to translate
social media comments. While the overall task is can arguably be
considered the same at a high level from the point of view of a human,
there are significant differences in the data distribution (for
instance, social media comments may contain more informal language).

In the most general sense, distributional shift (or distribution shift)
is simply an instance of the test distribution @xmath being different
from the training data distribution @xmath . Of course, this definition
is too broad to be realistic, let alone useful in practice. ⁸ ⁸ 8
Consider, in a binary classification task, setting @xmath . Only the
distribution changes, yet the two tasks are directly at odds with each
other. Precisely defining and categorizing what characterizes an
acceptable shift is still very much an active area of research.

##### 2.2 Categorizing Distributional Shift

While we will not attempt to formulate a complete taxonomy of
distributional shifts, we will paint a broad strokes picture of how they
are categorized in the literature. We will distinguish two main schools
of thought for distinguishing distributional shift: a “mechanistic” and
a “causal” approach.

###### 2.2.1 Mechanistic Categorization

The first approach is to distinguish shifts by describing, in
mathematical terms, the mechanics of how the change manifests in the
data generation process. The appeal of this categorization is that it
allows for a more principled, theoretical approach to tackling
distributional shift. A summary of the examples described in this
section can be found in Table 1 .

The most well known instance is covariate shift (Shimodaira, 2000 ;
Sugiyama & Müller, 2005 ; Storkey, 2009 ; Bickel et al., 2009 ) , where
only the distribution over the input space @xmath (sometimes called the
space of “covariates”) changes, and the conditional distribution @xmath
is assumed to stay the same. Covariate shift is a rather intuitive
assumption, especially in such cases where there is a clear causal
relationship between the covariates @xmath and the targets @xmath . It
holds an important place within the distribution shift literature
because its particular characteristics justify the use of specific
algorithms, some of which enjoy appealing properties. For instance, it
can be tackled by means of re-weighting with the covariate likelihood
ratio @xmath (Shimodaira, 2000 ) , and the covariate shift assumption
can yield tighter guarantees (Duchi et al., 2019 ) .

Another, diametrically opposed type of shift is label-shift (or prior
probability shift in Storkey ( 2009 ) ), where only the marginal on
@xmath changes, and the reverse conditional @xmath stays fixed. This is
most relevant when the causal relationship between @xmath and @xmath is
reversed, for example when diagnosing an illness ( @xmath ) from its
symptoms ( @xmath ). Specific methods have been developed to tackle this
scenario (Zhang et al., 2013 ; Lipton et al., 2018 ) , although their
application is generally less common in the NLP literature. Finally, a
comparatively less well studied occurrence is concept-shift , in which
the marginal @xmath is assumed fixed, but the conditional changes to
another distribution @xmath (Quiñonero-Candela et al., 2009 ) . This can
result from a mismatch between annotators: for instance in machine
translation, different professional annotators may use slightly
different translation conventions. Widmer & Kubat ( 1996 ) postulates
that this is due to the presence of some “unknown context”, such as the
two translators having had a different education, which leads to them
adhering to two mildly different definitions of the translation problem.
A concrete example of this phenomenon in toxicity detection (in the
English language) is the “n-word” problem. This racial slur and other
historically charged terms such as “queer” are often used disparagingly
towards certain underrepresented minorities. However some of them were
re-appropriated by minority speakers within their respective communities
(Rahman, 2012 ) . Without information about the speaker, it can be
difficult to determine the toxicity of an utterance containing such
words, even for human annotators (Sap et al., 2019 ) .

Another, more elaborate example of a “mechanistically defined” category
of distribution shift is what Koh et al. ( 2020 ) calls “sub-population
shift”. In sub-population shift, it is assumed that the training
distribution @xmath can be decomposed as a mixture of @xmath different
domains @xmath , @xmath . This can be the case by design: for example
the training data for the WMT machine translation competition ⁹ ⁹ 9
http://statmt.org/wmt21/ is aggregated from many different machine
translation corpora, or the MultiNLI textual entailment dataset
(Williams et al., 2018 ) was created specifically to contain data from
different domains. But in many more cases the training data may be an
aggregate of various domains, unbeknownst to the dataset’s creator: for
instance text datasets often contain data from different dialects ( e.g.
American vs. British English, French vs. Canadian French, etc…). The
benchmarks aginst which the methods described Part II of this thesis
will be evaluated are all examples of sub-population shifts.

###### 2.2.2 Causal Categorization

An alternate view is to distinguish the distribution shifts through the
real world phenomena that cause them. This more empirical approach has
the merit of describing realistic cases of distribution shift which may
not clearly (or provably) fit into the mathematically defined categories
presented in the previous section, but are prevalent enough to warrant
the development of bespoke solutions.

A predominant cause of distributional shift is change over time (Kelly
et al., 1999 ; Hand, 2006 ) . For instance, Bloomfield ( 1933 )
documents nine categories of lexical semantic drift that words may
undergo over time ¹⁰ ¹⁰ 10 The various causes of this drift are
elaborated upon in later studies (Blank & Koch, 2013 ; Grzega &
Schoener, 2007 ) . While not laying out a precise typology, McCulloch (
2020 ) also touches on a variety of semantic shift that occurred more
recently (and at a comparatively faster pace) with the advent of the
internet. The effect of these changes in meaning on word embedding
models such as Word2Vec ( Mikolov et al. ( 2013 ) , a staple of the NLP
pipeline at the time) were summarized in Kutuzov et al. ( 2018 ) .
Identifying time as the underlying cause for this phenomenon has the
benefit of suggesting solutions tailored to the problem at hand: for
example, one could use dynamic word-embedding models (Rudolph & Blei,
2018 ) to predict upcoming drift, and use this knowledge to augment the
training data. In a more recent example, Lazaridou et al. ( 2021 )
highlights the specific importance of ‘‘dynamic evaluation’’ ¹¹ ¹¹ 11
The process of continuously training a language model at test time.
(Mikolov et al., 2010 ) for mitigating loss in performance due to time
shift in language models.

In other examples, distributional shift can be described in terms of
data collection: a prime example is what Storkey ( 2009 ) calls biased
sampling . Subtle design choices at the time of the creation of the
training dataset (such as data source, pre-processing, or annotation
scheme) can introduce biases into the training distribution. A recent
illustration of biased sampling is the case of crowd-sourced natural
language inference ¹² ¹² 12 Natural language inference is another name
for the task of recognizing textual entailment. In its most common
formulation, the model is given two sentences, the premise and the
hypothesis, and is asked to determine whether the first entails or
contradict the second (or whether there is no clear entailment
relationship). datasets (Gururangan et al., 2018 ) : annotators tasked
with crafting a contradictory hypothesis from a premise would frequently
resort to introducing negation markers (in English: “not”, “never”,
etc…). This introduced a spurious correlation between the presence of
such negative words and the “contradiction” label. When confronted with
test data that doesn’t conform to this pattern ( e.g. cases of
entailment where the hypothesis does include a negation marker), models
trained with empirical risk minimization (ERM) break down (Sagawa
et al., 2020 ) .

###### 2.2.3 Local and Global Shifts

Aside from the two types of categorization described above, we believe
that it is worth making another distinction between what we will call
“local” and “global” shifts, purely by virtue of the fact that they are
both the subject of considerable, yet distinct areas of the literature.
We use the term local to refer to cases where the shifted distribution
@xmath can be obtained by applying a transformation (deterministic or
non-deterministic) to data points in @xmath . Of course, this isn’t a
very clearly defined category since any distribution can be transported
into another one via an optimal transport map. In this case, we refer
specifically to shifts that are best explicitly defined by such “local”
transformations. This category encompasses phenomena such as character
or word-level noise (of the kind described in Belinkov & Bisk ( 2018b )
for instance) or adversarial perturbations (Papernot et al., 2016 ;
Ebrahimi et al., 2018b , a ; Cheng et al., 2018a ) , small modifications
to the input of a machine learning model that cause dramatic changes to
its output.

The notion of global shift will then be used to refer to those type of
distributional shift that cannot easily be modeled as local shifts. We
make this distinction because, as will be discussed in Chapter I , local
shift can — and should — be addressed specifically.

#### 3 Robustness to Distributional Shift

##### 3.1 Distributionally Robust Optimization

There is a rather large body of literature devoted to training models
that are robust to distributional shift, under the name of
distributionally robust optimization ( DRO ). In DRO , models are
trained not to minimize their expected risk under any one distribution
@xmath , but rather their worst case risk under a pre-determined family
of distributions @xmath , called the “uncertainty set”

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

The DRO loss in Equation 3 provides an upper bound on the expected loss
of the model under any distribution in the uncertainty set @xmath ,
which motivates the use the solution of the min-max problem as a robust
model. However this objective is only useful insofar that @xmath (1)
covers test distributions of interest (corresponding to different
domains, demographics, etc.) and (2) is not overly pessimistic. To
fulfil this second condition, there should exist some model @xmath that
achieves low loss simultaneously on the test distribution as well as
@xmath . This often requires that @xmath only contain distributions that
are “close” to the training distribution under some divergence metric,
or that are restricted to covariate shifts.

In some cases, it is conceivable that @xmath is available at training
time. For instance, we might have access to some amount of data in
@xmath domains of interest, in which case the DRO loss can be written as
a maximum over the finite set @xmath . This setting is sometimes
referred to as Group- DRO and has been studied in Oren et al. ( 2019 );
Sagawa et al. ( 2020 ); Zhou et al. ( 2021 ) .

In the absence of explicit information about the domains of interest, it
is up to the practitioner to carefully define this uncertainty set.
There is substantial existing work on nonparametric formulations of DRO
, where @xmath is expressed as a divergence ball centered at the
training distribution. Popular examples in the literature are:

-   @xmath -divergence balls , also called @xmath -divergences or Rényi
    divergences (Cover, 1999 ; Van Erven & Harremos, 2014 ) are a family
    of divergence metrics of the form @xmath where @xmath is a convex
    function mapping 1 to 0, and @xmath is assumed to be absolutely
    continuous to @xmath ( @xmath ). Well-known representatives include
    the Kullback-Leibler (KL) divergence ( @xmath ), Pearson’s @xmath
    divergence ( @xmath ) or the total variation distance ( @xmath ).
    They have seen extensive use in the DRO literature (Ben-Tal et al.,
    2013 ; Hu & Hong, 2013 ; Faury et al., 2020 ) .

-    Wasserstein/IPM balls . The 1-Wasserstein distance or earth mover’s
    distance between two probability distributions @xmath , @xmath on a
    metric space @xmath is defined as @xmath where @xmath is the set of
    all joint distributions on @xmath with marginals @xmath and @xmath ,
    and @xmath is @xmath ’s canonical metric. Contrary to @xmath
    -divergences, the Wasserstein metric makes no assumptions of
    absolute continuity, and it allows practitioners to design problem
    specific uncertainty sets by choosing the most relevant distance
    metric @xmath . This has made it a popular choice for DRO problems
    where such metrics are well defined ( e.g. for real valued inputs),
    particularly in the context of adversarial robustness (Gao &
    Kleywegt, 2016 ; Esfahani & Kuhn, 2018 ; Sinha et al., 2018 ) .
    Integral probability metrics (IPMs) are another common class of
    divergences between probability distributions to which (under some
    conditions, see Theorem 11.8.2 in Dudley ( 2018 ) ) the Wasserstein
    distance belongs. They take the form @xmath where @xmath is a set of
    real valued bounded measurable functions on @xmath . Although
    Wasserstein-DRO is are by far more common in the literature, there
    has been some recent work on general IPM bounded uncertainty sets
    (Husain, 2020 ) .

-    Moment constraints consists in uncertainty sets where admissible
    distribution have to satisfy some (generally linear or quadratic)
    constraint on their moments, e.g. @xmath for some pre-specified
    values @xmath and @xmath . They are perhaps one of the oldest
    formulations of DRO , with roots in operations research (Scarf, 1957
    ; Dupačová, 1987 ; Delage & Ye, 2010 ) and with some exceptions
    (Nguyen et al., 2020 ) , they have seen relatively little use in the
    modern machine learning context.

-    CVaR constraints (or @xmath -coverage). Originating from the
    financial risk management literature (Rockafellar et al., 2000 ) ,
    the conditional value at risk (CVaR) is concerned with guaranteeing
    good performance under all distributions @xmath that @xmath -covered
    by @xmath , meaning that for all @xmath , @xmath with @xmath . This
    formalizes in the language of probability distributions the idea
    that @xmath is a subset that covers only a fraction @xmath of the
    training data. CVaR-constrained uncertainty sets underpin a variety
    of recently proposed approaches for robust machine learning, even if
    they are not always explicitly acknowledged as such (Fan et al.,
    2017 ; Curi et al., 2020 ; Levy et al., 2020 ) .

All these nonparametric approaches are appealing as they require very
little domain-specific knowledge, have well-understood theory (Duchi &
Namkoong, 2018 ) , and optimization procedures ( e.g. Hu & Hong ( 2013 )
for KL constraints and Levy et al. ( 2020 ) for @xmath and CVaR
constraints). We refer to (Rahimian & Mehrotra, 2019 ) for a more
in-depth overview.

Unfortunately, nonparametric DRO algorithms suffer from being overly
pessimistic. Their uncertainty sets tend to include distributions that
are exceedingly difficult to learn, or not representative of real-world
distribution shifts. Furthermore, they often cannot enforce even basic
constraints such as covariate shift (Duchi et al., 2020 ; Hu et al.,
2018 ) . Group-structured DRO uncertainty sets (Sagawa et al., 2020 )
overcome some of these challenges, but as alluded to before, they
require significant domain expertise to pre-specify target
sub-populations that a model should be robust to.

##### 3.2 Measuring Robustness

Over the years, various works have attempted to come up with specific
measures of model robustness in order to provide out-of-distribution
generalization guarantees. Unfortunately, common divergence metrics such
as the KL divergence ( @xmath ; Kullback & Leibler ( 1951 ) ) or the
total variation distance ( @xmath ; Saks ( 1937 ) ) generally yield
vacuous bounds.

Ben-David et al. ( 2010 ) (following a line of work initiated in Kifer
et al. ( 2004 ) and Ben-David et al. ( 2007 ) ) introduced a new
distance metric called the @xmath -divergence, specifically geared
towards yielding out-of-distribution generalization bounds. The central
insight of the @xmath -divergence is to induce a distance that is
specific to the family of models being trained (the titular @xmath ). A
key feature of the @xmath -divergence is that it can be approximated
with another metric, the proxy @xmath -distance. In broad strokes, the
proxy @xmath -distance is proportional to how easy it is to discriminate
between the two domains @xmath and @xmath using classifiers from @xmath
. More precisely, given two datasets @xmath and @xmath sampled from
@xmath and @xmath respectively, it reads:

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

with

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

In other words, @xmath is the best error-rate (within @xmath ) for
distinguishing @xmath from @xmath . Naturally the exact minimum is in
general intractable, but it can be estimated by training a classifier on
finite amounts of training data sampled from @xmath and @xmath , making
it more conducive to applied research.

Although the @xmath -divergence literature has inspired some work in
‘‘unsupervised domain adaptation’’ ¹³ ¹³ 13 Training models to be robust
to domain shift given unlabeled samples from the target domain ( Ganin &
Lempitsky ( 2015 ) and the follow-up literature on domain adversarial
neural networks), it has seen relatively little use in NLP , with some
notable exceptions (Blitzer & Pereira, 2007 ; Rai et al., 2010 ; Glorot
et al., 2011 ) . More recent empirical comparisons (Elsahar & Gallé,
2019 ; Kashyap et al., 2021 ) have shown the proxy @xmath -Distance to
be competitive when it comes to predicting performance drop in the face
of domain shift, possibly heralding a resurgence of the metric. We refer
the interested reader to Kashyap et al. ( 2021 ) for a complete taxonomy
and a more exhaustive overview of domain divergences (written with an
NLP audience in mind).

With regards to adversarial perturbation specifically, there is a
considerable amount of work devoted to providing certifiable robustness
guarantees (see Katz et al. ( 2017 ); Sinha et al. ( 2018 ); Wong &
Kolter ( 2018 ); Raghunathan et al. ( 2018 ) inter alia ). Often, these
guarantees rely on ideas related to Lipschitz continuity bounds:
limiting the change in a model’s output as a function of small
perturbations in its inputs. These ideas are difficult to transpose to
NLP as they rely on some notion of a “small change” in the input, which
is usually described in terms of @xmath norm for continuous data such as
images, but is harder to formalize in natural languages. Jia et al. (
2019 ) circumvent this issue by adapting an approach from the computer
vision literature (Interval Bound Propagation (IBP); Dvijotham et al. (
2018 ) ) to specific neural architectures to yield robustness guarantees
to word-level perturbations at the embedding level. Overall, guaranteed
adversarial robustness in NLP is still an active area of research, with
recent work extending these ideas to more modern architectures (Shi
et al., 2020 ) for example.

## Part I Measuring and Evaluating Robustness

### Chapter \thechapter MTNT: A Testbed for Machine Translation of Noisy
Text

#### 4 Introduction

After this overview of the distributional shift problem, in this chapter
we present our first contribution: a benchmark dataset for Machine
Translation of Noisy Text ( MTNT ), consisting of noisy user-generated
content such as the following comment:

  #nlproc is actualy f*ing hARD tbh []

This handcrafted sentence showcases several types of noise that are
commonly seen on social media: abbreviations (“#nlproc”), typographical
errors (“actualy”), obfuscated profanities (“f*ing”), inconsistent
capitalization (“hARD”), Internet slang (“tbh” for “to be honest”) and
emojis ( [] ). Although machine translation has achieved significant
quality improvements over the past few years due to the advent of Neural
Machine Translation (NMT) Kalchbrenner & Blunsom ( 2013 ); Sutskever
et al. ( 2014 ); Bahdanau et al. ( 2014 ); Wu et al. ( 2016 ) , systems
are still not robust to noisy input like this Belinkov & Bisk ( 2018a );
Khayrallah & Koehn ( 2018 ) . For example, Google Translate ¹⁴ ¹⁴ 14
translate.google.com as of May 2018 translates the above example into
French as:

  #nlproc est en train de f * ing dur hb

which translates back into English as “#nlproc is in the process of [f *
ing] hard hb”. This shows that noisy input can lead to erroneous
translations that can be misinterpreted or even offensive.

Noise in social media text is a known issue that has been investigated
in a variety of previous work Eisenstein ( 2013 ); Baldwin et al. ( 2013
) . Most recently, Belinkov & Bisk ( 2018a ) have focused on the
difficulties that character based NMT models have translating text with
character level noise within individual words (from scrambling to
simulated human errors such as typos or spelling/conjugation errors).
This is a good first step towards noise-robust NMT systems, but as we
demonstrate in Section 5 , word-by-word replacement or scrambling of
characters doesn’t cover all the idiosyncrasies of language on the
Internet.

At this point, despite the obvious utility of creating noise-robust MT
systems, and the scientific challenges contained therein, there is
currently a bottleneck in that there is no standard open benchmark for
researchers and developers of Machine Translation (MT) systems to test
the robustness of their models to these and other phenomena found in
noisy text on the Internet. In this chapter, we introduce Machine
Translation of Noisy Text (MTNT) , a new, realistic dataset aimed at
testing robustness of MT systems to these phenomena. The dataset
contains naturally created noisy source sentences with professionally
sourced translations both in a pair of typologically close languages
(English and French) and distant languages (English and Japanese). We
collect noisy comments from the Reddit ¹⁵ ¹⁵ 15 www.reddit.com online
discussion website (Section 6 ) in English, French and Japanese, and
commissioned professional translators to translate to and from English,
resulting in approximately 1000 test samples and from 6k to 36k training
samples in four language pairs ( English-French ( en-fr ) ,
French-English ( fr-en ) , English-Japanese ( en-ja ) and
Japanese-English ( ja-en ) ). In addition, we release additional small
monolingual corpora in those 3 languages to both provide data for
semi-supervised adaptation approaches as well as noisy Language Modeling
(LM) experiments. We test standard translation models (Section 8 ) on
our data to understand their failure cases and to provide baselines for
future work. ¹⁶ ¹⁶ 16 Additional language modeling experiments can be
found in Appendix 9 Finally, we describe the 2019 and 2020 WMT
Robustness shared task in which MTNT was used as a benchmark for robust
machine translation (Section 10 ). The data is publicly available at
https://pmichel31415.github.io/mtnt/ .

#### 5 Noise and Input Variations in Language on the Internet

##### 5.1 Examples from Social Media Text

The term “noise” can encompass a variety of phenomena in natural
language, with variations across languages ( e.g. what is a typo in
logographic writing systems?) and type of content Baldwin et al. ( 2013
) . To give the reader an idea of the challenges posed to MT and NLP
systems operating on this kind of text, we provide a non-exhaustive list
of types of noise and more generally input variations that deviate from
standard MT training data we’ve encountered in Reddit comments:

-    Spelling/typographical errors : “across” @xmath “accross”,
    “receive” @xmath “recieve”, “could have” @xmath “could of”, “temps”
    @xmath “tant”, “除く” @xmath “覗く”

-    Word omission/insertion/repetition : “je n’aime pas” @xmath “j’aime
    pas”,“je pense” @xmath “moi je pense”

-    Grammatical errors : “a ton of” @xmath “a tons of”, “There are
    fewer people” @xmath “There are less people”

-    Spoken language : “want to” @xmath “wanna”, “I am” @xmath “I’m”,
    “je ne sais pas” @xmath “chais pas”, “何を笑っているの” @xmath
    “何わろてんねん”,

-    Internet slang : “to be honest” @xmath “tbh”, “shaking my head”
    @xmath “smh”, “mort de rire” @xmath “mdr”, “笑” @xmath “w”/“草”

-    Proper nouns (with or without correct capitalization): “Reddit”
    @xmath “reddit”

-    Dialects : African American Vernacular English, Scottish,
    Provençal, Québécois, Kansai, Tohoku…

-    Code switching : “This is so cute” @xmath “This is so kawaii”,
    “C’est trop conventionel” @xmath “C’est trop mainstream”,
    “現在捏造中…” @xmath “Now 捏造ing…”

-    Jargon : on Reddit: “upvote”, “downvote”, “sub”, “gild”

-    Emojis and other unicode characters : [] , [] , [] , [] , [] , [] ,
    []

-    Profanities/slurs (sometimes masked) “f*ck”, “m*rde” …

##### 5.2 Is Translating Noisy Text just another Adaptation Problem?

To a certain extent, translating noisy text is a type of adaptation ,
which has been studied extensively in the context of both Statistical
Machine Translation (SMT) and NMT Axelrod et al. ( 2011 ); Li et al. (
2010 ); Luong & Manning ( 2015 ); Chu et al. ( 2017 ); Miceli Barone
et al. ( 2017 ); Wang et al. ( 2017 ); Michel & Neubig ( 2018b ) .
However, it presents many differences with previous domain adaptation
problems, where the main goal is to adapt from a particular topic or
style. In the case of noisy text, it will not only be the case that a
particular word will be translated in a different way than it is in the
general domain (e.g. as in the case of “sub”), but also that there will
be increased lexical variation (e.g. due to spelling or typographical
errors), and also inconsistency in grammar (e.g. due to omissions of
critical words or mis-usage). The sum of these differences warrants that
noisy MT be treated as a separate instance than domain adaptation, and
our experimental analysis in 8.4 demonstrates that even after performing
adaptation, MT systems still make a large number of noise-related
errors.

#### 6 Collection Procedure

We first collect noisy sentences in our three languages of interest,
English, French and Japanese. We refer to Figure 1 for an overview of
the data collection and translation process.

We choose Reddit as a source of data because (1) its content is likely
to exhibit noise, (2) some of its sub-communities are entirely run in
different languages, in particular, English, French and Japanese, and
(3) Reddit is a popular source of data in curated and publicly
distributed NLP datasets Tan et al. ( 2016 ) . We collect data using the
public Reddit API. ¹⁷ ¹⁷ 17 In particular, we use this implementation:
praw.readthedocs.io/en/latest , and our complete code is available at
http://www.cs.cmu.edu/~pmichel1/mtnt/ .

Note that the data collection and translation is performed at the
comment level. We split the parallel data into sentences as a last step.

##### 6.1 Data Sources

For each language, we select a set of communities (“subreddits”) that we
know contain many comments in that language:

  English:  

    Since an overwhelming majority of the discussions on Reddit are
    conducted in English, we don’t restrict our collection to any
    community in particular.

  French:  

     /r/france , /r/quebec and /r/rance . The first two are among the
    biggest French speaking communities on Reddit. The third is a
    humor/sarcasm based offspring of /r/france .

  Japanese:  

     /r/newsokur , /r/bakanewsjp , /r/newsokuvip , /r/lowlevelaware and
    /r/steamr . Those are the biggest Japanese speaking communities,
    with over 2,000 subscribers at the time of collection.

We collect comments made during the 03/27/2018-03/29/3018 time period
for English, 09/2018-03/2018 for French and 11/2017-03/2018 for
Japanese. The large difference in collection time is due to the variance
in comment throughput and relative amount of noise between the
languages.

##### 6.2 Contrast Corpora

Not all comments found on Reddit exhibit noise as described in Section 5
. Because we would like to focus our data collection on noisy comments,
we devise criteria that allow us to distinguish potentially noisy
comments from clean ones. Specifically, we compile a contrast corpus
composed of clean text that we can compare to, and find potentially
noisy text that differs greatly from the contrast corpus. Given that our
final goal is MT robust to noise, we prefer that these contrast corpora
consist of the same type of data that is often used to train NMT models.
We select different datasets for each language:

  English:  

    The English side of the preprocessed parallel training data provided
    for the German-English WMT 2017 News translation task, ¹⁸ ¹⁸ 18
    http://www.statmt.org/wmt17/translation-task.html as provided on the
    website. This amounts to @xmath million sentences.

  French:  

    The entirety of the French side of the parallel training data
    provided for the English-French WMT 2015 translation task. ¹⁹ ¹⁹ 19
    http://www.statmt.org/wmt15/translation-task.html This amounts to
    @xmath million sentences.

  Japanese:  

    We aggregate three small/medium sized MT datasets: KFTT Neubig (
    2011 ) , JESC Pryzant et al. ( 2017 ) and TED talks Cettolo et al. (
    2012 ) , amounting to @xmath million sentences.

##### 6.3 Identifying Noisy Comments

We now describe the procedure used to identify comments containing
noise.

###### Pre-filtering

First, we perform three pre-processing to discard comments that do not
represent natural noisy text in the language of interest:

1.  Comments containing a URL, as detected by a regular expression.

2.  Comments where the author’s username contains “bot” or
    “AutoModerator”. This mostly removes automated comments from bots.

3.  Comments in another language: we run langid.py ²⁰ ²⁰ 20
    https://github.com/saffsd/langid.py Lui & Baldwin ( 2012 ) and
    discard comments where @xmath for any language other than the one we
    are interested in.

This removes cases that are less interesting, i.e. those that could be
solved by rule-based pattern matching or are not natural text created by
regular users in the target language. Our third criterion in particular
discards comments that are blatantly in another language while still
allowing comments that exhibit code-switching or that contain proper
nouns or typos that might skew the language identification. In
preliminary experiments, we noticed that these criteria 14.47, 6.53 and
7.09 % of the collected comments satisfied the above criteria
respectively.

###### Normalization

After this first pass of filtering, we pre-process the comments before
running them through our noise detection procedure. We first strip
Markdown ²¹ ²¹ 21 https://daringfireball.net/projects/markdown syntax
from the comments. For English and French, we normalize the punctuation,
lowercase and tokenize the comments using the Moses tokenizer. For
Japanese, we simply lowercase the alphabetical characters in the
comments. Note that this normalization is done for the purpose of noise
detection only. The collected comments are released without any kind of
preprocessing. We apply the same normalization procedure to the contrast
corpora.

###### Unknown words

In the case of French and English, a clear indication of noise is the
presence of out-of-vocabulary words (OOV) : we record all lowercased
words encountered in our reference corpus described in Section 6.2 and
only keep comments that contain at least one OOV . Since we did not use
word segmentation for the Japanese reference corpus, we found this
method not to be very effective to select Japanese comments and
therefore skipped this step.

###### Language model scores

The final step of our noise detection procedure consists of selecting
those comments with a low probability under a language model trained on
the reference monolingual corpus. This approach mirrors the one used in
Moore & Lewis ( 2010 ) and Axelrod et al. ( 2011 ) to select data
similar to a specific domain using language model perplexity as a
metric. We search for comments that have a low probability under a
sub-word language model for more flexibility in the face of OOV words.
We segment the contrast corpora with Byte-Pair Encoding (BPE) using the
sentencepiece ²² ²² 22 https://github.com/google/sentencepiece
implementation. We set the vocabulary sizes to @xmath , @xmath and
@xmath for English, French and Japanese respectively. We then use a
5-gram Kneser-Ney smoothed language model trained using kenLM ²³ ²³ 23
https://kheafield.com/code/kenlm/ Heafield et al. ( 2013 ) to calculate
the log probability, normalized by the number of tokens for every
sentence in the reference corpus. Given a reddit comment, we compute the
normalized log probability of each of its lines under our subword
language model. If for any line this score is below the 1st percentile
of scores in the reference corpus, the comment is labeled as noisy and
saved.

##### 6.4 Creating the Parallel Corpora

Once enough data has been collected, we isolate @xmath comments in each
language by the following procedure:

-   Remove all duplicates. In particular, this handles comments that
    might have been scraped twice or automatic comments from bots.

-   To further weed out outliers (comments that are too noisy, e.g.
    ASCII art, wrong language…or not noisy enough), we discard comments
    that are on either end of the distribution of normalized LM scores
    within the set of collected comments. We only keep comments whose
    normalized score is within the 5-70 percentile for English (resp.
    5-60 for French and 10-70 for Japanese). These numbers are chosen by
    manually inspecting the data.

-   Choose @xmath samples at random.

We then concatenate the title of the thread where the comment was found
to the text and send everything to an external vendor for manual
translations. Upon reception of the translations, we noticed a certain
amount of variation in the quality of translations, likely because
translating social media text, with all its nuances, is difficult even
for humans. In order to ensure the highest quality in the translations,
we manually filter the data to segment the comments into sentences and
weed out poor translations for our test data. We thereby retain around
@xmath sentence pairs in each direction for the final test set.

We gather the samples that weren’t selected for the test sets to be used
for training or fine-tuning models on noisy data. We automatically split
comments into sentences with a regular expression detecting sentence
delimiters, and then align the source and target sentences. Should this
alignment fail ( i.e. the source comment contains a different number of
sentences than the target comment after automatic splitting), we revert
back to providing the whole comment without splitting. For the training
data, we do not verify the correctness of translations as closely as for
the test data. Finally, we isolate @xmath samples in each direction to
serve as validation data.

Information about the size of the data can be found in Table 2(c) , 2(a)
and 2(b) for the test, training and validation sets respectively. We
tokenize the English and French data with the Moses Koehn et al. ( 2007
) tokenizer and the Japanese data with Kytea Neubig et al. ( 2011 )
before counting the number of tokens in each dataset.

##### 6.5 Monolingual Corpora

After the creation of the parallel train and test sets, a large number
of unused comments remain in each language, which we provide as
monolingual corpora. This additional data has two purposes: first, it
serves as a resource for in-domain training using semi-supervised
methods relying on monolingual data (e.g. Cheng et al. ( 2016 ); Zhang &
Zong ( 2016 ) ). Second, it provides a language modeling dataset for
noisy text in three languages.

We select @xmath comments at random in each dataset to form a validation
set to be used to tune hyper-parameters, and provide the rest as
training data. The data is provided with one comment per line. Newlines
within individual comments are replaced with spaces. Table 2(d) contains
information on the size of the datasets. As with the parallel MT data,
we provide the number of tokens after tokenization with the Moses
tokenizer for English and French and Kytea for Japanese.

#### 7 Dataset Analysis

In this section, we investigate the proposed data to understand how
different categories of noise are represented and to show that our test
sets contain more noise overall than established MT benchmarks.

##### 7.1 Quantifying Noisy Phenomena

We run a series of tests to count the number of occurrences of some of
the types of noise described in Section 5 . Specifically we pass our
data through spell checkers to count spelling and grammar errors. Due to
some of these tests being impractical to run on a large scale, we limit
our analysis to the test sets of MTNT .

We use slightly different procedures depending on the tools available
for each language. We test for spelling and grammar errors in English
data using Grammarly ²⁴ ²⁴ 24 https://www.grammarly.com/ , an online
resource for English spell-checking. Due to the unavailability of an
equivalent of Grammarly in French and Japanese, we test for spelling and
grammar error using the integrated spell-checker in Microsoft Word 2013
²⁵ ²⁵ 25 https://products.office.com/en-us/microsoft-word-2013 . Note
that Word seems to count proper nouns as spelling errors, giving higher
numbers of spelling errors across the board in French as compared to
English.

For all languages, we also count the number of profanities and emojis
using custom-made lists and regular expressions ²⁶ ²⁶ 26 available with
our code at https://github.com/pmichel31415/mtnt . In order to compare
results across datasets of different sizes, we report all counts per
@xmath words.

The results are recorded in the last row of each section in Table 3 . In
particular, for the languages with a segmental writing system, English
and French, spelling errors are the dominant type of noise, followed by
grammar error. Unsurprisingly, the former are much less present in
Japanese.

##### 7.2 Comparison to Existing Machine Translation Test Sets

Table 3 also provide a comparison with the relevant side of established
MT test sets. For English and French, we compare our data to
newstest2014 ²⁷ ²⁷ 27 http://www.statmt.org/wmt15/dev-v2.tgz and
newsdiscusstest2015 ²⁸ ²⁸ 28 http://www.statmt.org/wmt15/test.tgz test
sets. For Japanese, we compare with the test sets of the datasets
described in Section 6.2 .

Overall, MTNT contains more noise in all metrics but one (there are more
profanities in JESC, a Japanese subtitle corpus). This confirms that
MTNT indeed provides a more appropriate benchmark for translation of
noisy or non-standard text.

Compared to synthetically created noisy test sets Belinkov & Bisk (
2018a ) MTNT contains less systematic spelling errors and more varied
types of noise ( e.g. emojis and profanities) and is thereby more
representative of naturally occurring noise.

#### 8 Machine Translation Experiments

We evaluate standard NMT models on our proposed dataset to assess its
difficulty. Our goal is not to train state-of-the art models but rather
to test standard off-the-shelf NMT systems on our data, and elucidate
what features of the data make it difficult.

##### 8.1 Model Description

All our models are implemented in DyNet Neubig et al. ( 2017 ) with the
XNMT toolkit Neubig et al. ( 2018 ) . We use approximately the same
setting for all language pairs: the encoder is a bidirectional LSTM with
2 layers, the attention mechanism is a multi layered perceptron and the
decoder is a 2 layered LSTM. The embedding dimension is 512, all other
dimensions are 1024. We tie the target word embeddings and the output
projection weights Press & Wolf ( 2017 ) . We train with Adam Kingma &
Ba ( 2014 ) with XNMT’s default hyper-parameters, as well as dropout
(with probability @xmath ). We used BPE subwords to handle OOV words.
Full configuration details as well as code to reproduce the baselines is
available at https://github.com/pmichel31415/mtnt .

##### 8.2 Training Data

We train our models on standard MT datasets:

-   en @xmath fr: Our training data consists in the europarl-v7 ²⁹ ²⁹ 29
    http://www.statmt.org/europarl/ and news-commentary-v10 ³⁰ ³⁰ 30
    http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz corpora,
    totaling @xmath samples, @xmath French tokens and @xmath English
    tokens (non-tokenized). We use the newsdiscussdev2015 ²⁷ dev set
    from WMT15 as validation data and evaluate the model on the
    newsdiscusstest2015 ²⁸ and newstest2014 ²⁷ test sets.

-   en @xmath ja: We concatenate the respective train, validation and
    test sets of the three corpora mentioned in 6.2 . In particular we
    detokenize the Japanese part of each dataset to make sure that any
    tokenization we perform will be uniform (in practice we remove ASCII
    spaces). This amounts to @xmath training samples ( @xmath English
    tokens without tokenization). We concatenate the dev sets associated
    with these corpora to serve as validation data and evaluate on each
    respective test set separately.

##### 8.3 Results

We use sacreBLEU ³¹ ³¹ 31 https://github.com/mjpost/sacreBLEU , a
standardized BLEU score evaluation script proposed by Post ( 2018b ) ,
for BLEU evaluation of our benchmark dataset. It takes in detokenized
references and hypotheses and performs its own tokenization before
computing BLEU score. We specify the intl tokenization option. In the
case of Japanese text, we run both hypothesis and reference through
KyTea before computing BLEU score. We strongly encourage that evaluation
be performed in the same manner in subsequent work, and will provide
both scripts and an evaluation web site in order to facilitate
reproducibility.

Table 4 lists the BLEU scores for our models on the relevant test sets
in the two language pairs, including the results on MTNT .

##### 8.4 Analysis of the MT outputs

To better understand the types of errors made by our model, we count the
n-grams that are over- and under- generated with respect to the
reference translation. Specifically, we compare the count ratios of all
1- to 3-grams in the output and in the reference and look for the ones
with the highest (over-generated) and lowest (under-generated) ratio.

We find that in English, the model under-generates the contracted form
of the negative (“do not”/“don’t”) or of auxiliaries (“That is”/“I’m”).
Similarly, in French, our model over generates “de votre” (where “votre”
is the formal 2nd person plural for “your”) and “n’ai pas” which
showcases the “ne […] pas” negation, often dropped in spoken language.
Conversely, the informal second person “tu” is under-generated, as is
the informal and spoken contraction of “cela”, “ça”. In Japanese, the
model under-generates, among others, the informal personal pronoun 俺
(“ore”) or the casual form だ (“da”) of the verb です (“desu”, to be).
In ja-en the results are difficult to interpret as the model seems to
produce incoherent outputs ( e.g. “no, no, no…”) when the NMT system
encounters sentences it has not seen before. The full list of n-grams
with the top 5 and bottom 5 count ratios in each language pair is
displayed in Table 6 .

##### 8.5 Fine-Tuning

Finally, we test a simple domain adaptation method by fine-tuning our
models on the training data described in Section 6.4 . We perform one
epoch of training with vanilla SGD with a learning rate of @xmath and a
batch size of @xmath . We do not use the validation data at all. As
evidenced by the results in the last row of Table 4 , this drives BLEU
score up by 3.17 to 7.96 points depending on the language pair. However
large this increase might be, our model still breaks on very noisy
sentences. Table 5 shows three examples in fr-en . Although our model
somewhat improves after fine-tuning, the translations remain inadequate
in all cases. In the third case, our model downright fails to produce a
coherent output. This shows that despite improving BLEU score, naive
domain adaptation by fine-tuning doesn’t solve the problem of
translating noisy text.

#### 9 Language Modeling Experiments

In addition to our MT experiments, we report character-level language
modeling results on the monolingual part of our dataset. We use the data
described in Section 6.5 as training and validation sets. We evaluate
the trained model on the source side of our en-fr , fr-en and ja-en test
sets for English, French and Japanese respectively.

We report results for two models: a Kneser-Ney smoothed 6-gram model
(implemented with KenLM) and an implementation of the AWD-LSTM proposed
in Merity et al. ( 2018 ) ³² ³² 32
https://github.com/salesforce/awd-lstm-lm . We report the
Bit-Per-Character (bpc) counts in table 7 . We intend these results to
serve as a baseline for future work in language modeling of noisy text
in either of those three languages.

#### 10 MTNT in Action: the 2019 and 2020 WMT Robustness Shared Tasks

A year after MTNT was originally published, the dataset was featured in
the Robustness shared task ³³ ³³ 33
http://www.statmt.org/wmt19/robustness.html at the 2019 conference on
machine translation (WMT). Participants were allowed to use large
amounts of out-of-domain data from the main WMT translation task ³⁴ ³⁴
34 http://www.statmt.org/wmt15/translation-task.html or from the three
en-ja corpora described in Section 8 , as well as the MTNT dataset
itself as a small in-domain corpus. Submitted systems were evaluated on
new, blind test sets collected using the same procedure as MTNT i.e.
scraped from Reddit, filtered out for noisy comments using a sub-word
language modeling criterion and translated by professionals.

The shared task attracted 23 submissions from 11 teams. Methods used by
competing teams included variants of data cleaning (removal of noisy
training samples), the use of placeholders (to account for special
characters that could be copied such as emojis), data augmentation
strategies (such as back-translation or the addition of filtered data
from external sources), domain-aware training (via the addition of
domain tags), fine-tuning and ensembling. Table 8 showcases an example
where specific handling of casing allowed some systems to outperform
others on samples containing ALL CAPS (a common phenomenon in
user-generated content on social media). A more detailed description of
the submissions, as well as an analysis of the results, can be found in
the findings paper Li et al. ( 2019 ) .

The shared task was renewed in 2020, with a broader focus on general
domain robustness ³⁵ ³⁵ 35 http://www.statmt.org/wmt20/robustness.html .
In this iteration, participants were asked to train models in one of two
language pairs (English-German and English Japanese) using only the data
available for the WMT20 news translation task (Barrault et al., 2020 ) .
The evaluation data was aggregated from three different domains:
Wikipedia comments with toxic content ³⁶ ³⁶ 36
https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge ,
speech recognition transcripts (Wang et al., 2020b ) , and a new test
set of reddit comments, collected following again the same procedure as
MTNT . Evaluation proceeded in two distinct phases. In the first
“zero-shot” phase, submitted systems were simply evaluated against the
unseen test sets, thus testing the domain robustness of models trained
using only out-of-domain data. In the second “few-shot” phase,
participants were given a single week to fine-tune their systems using
limited amounts of training data from each domain (including the MTNT
training data). Each system output was evaluated using both automatic
metrics (BLEU) and human judgements. In the case of the latter,
evaluators were not only asked to rate the machine-generated
translations on a numerical scale of 1 to 5, but also to indicate the
presence of “catastrophic errors” belonging to pre-defined categories
such as “introduction of toxicity”, “mis-translation of named entities”
or “change in units/time/date/numbers”

This edition of the task received 59 submissions by 11 participating
teams from both industry and academic institutions. By and large, most
teams used similar approaches as the previous year, specifically tagged
back-translations, ensembling or fine-tuning on filtered data. In
addition, some teams used adaptor modules (Bapna & Firat, 2019 ) for
more efficient fine-tuning, with some level of success. The human
evaluation of the results uncovered a predominance of named entity
mis-translations among catastrophic errors, followed by sentiment
reversal and introduction of toxicity. We refer to the findings paper
(Specia et al., 2020 ) for a full description of the data collection
process, evaluation procedure and participating systems.

###### Acknowledgement

The work discussed in this section was carried out in two different
papers, Li et al. ( 2019 ) and Specia et al. ( 2020 ) . The present
author was not first author on either of these papers. His contributions
were, for Li et al. ( 2019 ) to help with the general organization of
the task, including establishing the goal of the task, planning,
handling submissions and in particular to be responsible for collecting
the blind test sets. For Specia et al. ( 2020 ) , his contributions were
more focused on data collection for the reddit test set.

#### 11 Related work

Handling noisy text has received growing attention among various
language processing tasks due to the abundance of user generated content
on popular social media platforms Crystal ( 2001 ); Herring ( 2003 );
Danet & Herring ( 2007 ) . These contents are considered as noisy when
compared to news corpora which have been the main data source for
language tasks Baldwin et al. ( 2013 ); Eisenstein ( 2013 ) . They pose
several unique challenges because they contain a larger variety of
linguistic phenomena that are absent in the news domain and that lead to
degraded quality when applying an model to out-of-domain data (Ritter
et al., 2011 ; Luong & Manning, 2015 ) . Additionally, they are live
examples of the Cmabrigde Uinervtisy (Cambridge University) effect,
where state-of-the-art models become brittle while human’s language
processing capability is more robust (Sakaguchi et al., 2017 ; Belinkov
& Bisk, 2018a ) .

Efforts to address these challenges have been focused on creating
in-domain datasets and annotations (Owoputi et al., 2013 ; Kong et al.,
2014 ; Blodgett et al., 2017 ) , and domain adaptation training (Luong &
Manning, 2015 ) . In MT , improvements were obtained for SMT (Formiga &
Fonollosa, 2012 ) . However, the specific challenges for neural machine
translation have not been studied until recently Belinkov & Bisk ( 2018a
); Sperber et al. ( 2017 ); Cheng et al. ( 2018b ) . The first provides
empirical evidence of non-trivial quality degradation when source
sentences contain natural noise or synthetic noise within words, and the
last two explore data augmentation and adversarial approaches of adding
noise efficiently to training data to improve robustness.

Our work also contributes to recent advances in evaluating neural
machine translation quality with regard to specific linguistic
phenomena, such as manually annotated test sentences for English to
French translation, in order to identify errors due to specific
linguistic divergences between the two languages (Isabelle et al., 2017
) , or automatically generated test sets to evaluate typical errors in
English to German translation (Sennrich, 2017 ) . Our contribution
distinguishes itself from this previous work and other similar
initiatives (Peterson, 2011 ) by providing an open test set consisting
of naturally occurring text exhibiting a wide range of phenomena related
to noisy input text from contemporaneous social media.

#### 12 Discussion

In this chapter, we proposed a new dataset to test MT models for
robustness to the types of noise encountered in natural language on the
Internet. We contribute parallel training and test data in both
directions for two language pairs, English @xmath French and English
@xmath Japanese, as well as monolingual data in those three languages.
We show that this dataset contains more noise than existing MT test sets
and poses a challenge to models trained on standard MT corpora. We
further demonstrate that these challenges cannot be overcome by a simple
domain adaptation approach alone. After its publication, MTNT served as
the basis for two shared tasks at the WMT conference in machine
translation, fostering research on models and evaluation metrics for
this specific problem (Li et al., 2019 ; Specia et al., 2020 ) . In
addition, it was used independently in a variety of subsequent work,
either as a benchmark for e.g. data augmentation techniques (Karpukhin
et al., 2019 ; Vaibhav et al., 2019 ) or as an exemplar of domain shift
in machine translation (Michel et al., 2019a ) .

### Chapter \thechapter On Evaluation of Adversarial Perturbations for
Sequence-to-Sequence Models \chaptermark

On Evaluation of Adversarial Perturbations

#### 13 Introduction

While the previous chapter’s focus was on evaluating models under
distributional shift to the social media domain (a type of shift which
we referred to as a “global” shift in Chapter Learning Neural Models for
Natural Language Processing in the Face of Distributional Shift ), in
this chapter we take a closer look at a different type of distributional
shift: adversarial perturbations. At a high level, attacking a machine
learning model with adversarial perturbations is the process of making
changes to its input to maximize an adversarial goal, such as
mis-classification Szegedy et al. ( 2013 ) or mis-translation Zhao
et al. ( 2018 ) . These attacks provide insight into the vulnerabilities
of machine learning models and their brittleness to samples outside the
training distribution. Lack of robustness to these attacks poses
security concerns to safety-critical applications, e.g. self-driving
cars Bojarski et al. ( 2016 ) .

Adversarial attacks were first defined and investigated for computer
vision systems ( Szegedy et al. ( 2013 ); Goodfellow et al. ( 2014b );
Moosavi-Dezfooli et al. ( 2016 ) inter alia), where the input space is
continuous, making minuscule perturbations largely imperceptible to the
human eye. In discrete spaces such as natural language sentences, the
situation is more problematic; even a flip of a single word or character
is generally perceptible by a human reader. Thus, most of the
mathematical framework in previous work is not directly applicable to
discrete text data. Moreover, there is no canonical distance metric for
textual data like the @xmath norm in real-valued vector spaces such as
images, and evaluating the level of semantic similarity between two
sentences is a field of research of its own Cer et al. ( 2017 ) . This
elicits a natural question: what does the term “adversarial
perturbation” mean in the context of NLP ?

We propose a simple but natural criterion for adversarial examples in
NLP , particularly untargeted ³⁷ ³⁷ 37 Here we use the term untargeted
in the same sense as Ebrahimi et al. ( 2018a ) : an attack whose goal is
simply to decrease performance with respect to a reference translation.
attacks on sequence-to-sequence (seq2seq) models: adversarial examples
should be meaning-preserving on the source side, but meaning-destroying
on the target side . The focus on explicitly evaluating meaning
preservation is in contrast to previous work on adversarial examples for
seq2seq models Belinkov & Bisk ( 2018b ); Zhao et al. ( 2018 ); Cheng
et al. ( 2018a ); Ebrahimi et al. ( 2018a ) . Nonetheless, this feature
is extremely important; given two sentences with equivalent meaning, we
would expect a good model to produce two outputs with equivalent
meaning. In other words, any meaning-preserving perturbation that
results in the model output changing drastically highlights a fault of
the model.

A first technical contribution of this chapter is to lay out a method
for formalizing this concept of meaning-preserving perturbations
(Section 14 ). This makes it possible to evaluate the effectiveness of
adversarial attacks or defenses either using gold-standard human
evaluation, or approximations that can be calculated without human
intervention. We further propose a simple method of imbuing
gradient-based word substitution attacks (Section 15.1 ) with simple
constraints aimed at increasing the chance that the meaning is preserved
(Section 15.2 ).

Our experiments are designed to answer several questions about meaning
preservation in seq2seq models. First, we evaluate our proposed
“source-meaning-preserving, target-meaning-destroying” criterion for
adversarial examples using both manual and automatic evaluation (Section
16.2 ) and find that a less widely used evaluation metric (chrF)
provides significantly better correlation with human judgments than the
more widely used BLEU and METEOR metrics. We proceed to perform an
evaluation of adversarial example generation techniques, finding that
chrF does help to distinguish between perturbations that are more
meaning-preserving across a variety of languages and models (Section
16.3 ). Finally, we apply existing methods for adversarial training to
the adversarial examples with these constraints and show that making
adversarial inputs more semantically similar to the source is beneficial
for robustness to adversarial attacks and does not decrease test
performance on the original data distribution (Section 17 ). A toolkit
implementing our evaluation framework is released at
https://github.com/pmichel31415/teapot-nlp .

#### 14 A Framework for Evaluating Adversarial Attacks

In this section, we present a simple procedure for evaluating
adversarial attacks on seq2seq models. We will use the following
notation: @xmath and @xmath refer to the source and target sentence
respectively. We denote @xmath ’s translation by model @xmath as @xmath
. Finally, @xmath and @xmath represent an adversarially perturbed
version of @xmath and its translation by @xmath , respectively. The
nature of @xmath and the procedure for obtaining @xmath from @xmath are
irrelevant to the discussion below.

##### 14.1 The Adversarial Trade-off

The goal of adversarial perturbations is to produce failure cases for
the model @xmath . Hence, the evaluation must include some measure of
the target similarity between @xmath and @xmath , which we will denote
@xmath . However, if no distinction is being made between perturbations
that preserve the meaning and those that don’t, a sentence like “he’s
very friendly ” is considered a valid adversarial perturbation of “he’s
very adversarial ”, even though its meaning is the opposite. Hence, it
is crucial, when evaluating adversarial attacks on MT models, that the
discrepancy between the original and adversarial input sentence be
quantified in a way that is sensitive to meaning. Let us denote such a
source similarity score @xmath .

Based on these functions, we define the target relative score decrease
as:

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

The choice to report the relative decrease in @xmath makes scores
comparable across different models or languages ³⁸ ³⁸ 38 Note that we do
not allow negative @xmath to keep all scores between 0 and 1. . For
instance, for languages that are comparatively easy to translate ( e.g.
French-English), @xmath will be higher in general, and so will the gap
between @xmath and @xmath . However this does not necessarily mean that
attacks on this language pair are more effective than attacks on a
“difficult” language pair ( e.g. Czech-English) where @xmath is usually
smaller.

We recommend that both @xmath and @xmath be reported when presenting
adversarial attack results. However, in some cases where a single number
is needed, we suggest reporting the attack’s success @xmath . The
interpretation is simple: @xmath , which means that the attack has
destroyed the target meaning ( @xmath ) more than it has destroyed the
source meaning ( @xmath ).

Importantly, this framework can be extended beyond strictly
meaning-preserving attacks. For example, for targeted keyword
introduction attacks Cheng et al. ( 2018a ); Ebrahimi et al. ( 2018a ) ,
the same evaluation framework can be used if @xmath (resp. @xmath ) is
modified to account for the presence (resp. absence) of the keyword (or
its translation in the source). Similarly this can be extended to other
tasks by adapting @xmath ( e.g. for classification one would use the
zero-one loss, and adapt the success threshold).

##### 14.2 Similarity Metrics

Throughout Section 14.1 , we have not given an exact description of the
semantic similarity scores @xmath and @xmath . Indeed, automatically
evaluating the semantic similarity between two sentences is an open area
of research and it makes sense to decouple the definition of adversarial
examples from the specific method used to measure this similarity. In
this section, we will discuss manual and automatic metrics that may be
used to calculate it.

###### 14.2.1 Human Judgment

Judgment by speakers of the language of interest is the de facto gold
standard metric for semantic similarity. Specific criteria such as
adequacy/fluency Ma & Cieri ( 2006 ) , acceptability Goto et al. ( 2013
) , and 6-level semantic similarity Cer et al. ( 2017 ) have been used
in evaluations of MT and sentence embedding methods. In the context of
adversarial attacks, we propose the following 6-level evaluation scheme,
which is motivated by previous measures, but designed to be (1)
symmetric, like Cer et al. ( 2017 ) , (2) and largely considers meaning
preservation but at the very low and high levels considers fluency of
the output ³⁹ ³⁹ 39 This is important to rule out nonsensical sentences
and distinguish between clean and “noisy” paraphrases ( e.g. typos,
non-native speech…). We did not give annotators additional instruction
specific to typos. , like Goto et al. ( 2013 ) :

How would you rate the similarity between the meaning of these two
sentences? [itemsep=-4pt] The meaning is completely different or one of
the sentences is meaningless The topic is the same but the meaning is
different Some key information is different The key information is the
same but the details differ Meaning is essentially equal but some
expressions are unnatural Meaning is essentially equal and the two
sentences are well-formed English ⁴⁰ ⁴⁰ 40 Or the language of interest.

###### 14.2.2 Automatic Metrics

Unfortunately, human evaluation is expensive, slow and sometimes
difficult to obtain, for example in the case of low-resource languages.
This makes automatic metrics that do not require human intervention
appealing for experimental research. This section describes 3 evaluation
metrics commonly used as alternatives to human evaluation, in particular
to evaluate translation models. ⁴¹ ⁴¹ 41 Note that other metrics of
similarity are certainly applicable within the overall framework of
Section 14.2.1 , but we limit our examination in this chapter to the
three noted here.

BLEU: Papineni et al. ( 2002 ) is an automatic metric based on n-gram
precision coupled with a penalty for shorter sentences. It relies on
exact word-level matches and therefore cannot detect synonyms or
morphological variations.

METEOR: Denkowski & Lavie ( 2014 ) first estimates alignment between the
two sentences and then computes unigram F-score (biased towards recall)
weighted by a penalty for longer sentences. Importantly, METEOR uses
stemming, synonymy and paraphrasing information to perform alignments.
On the downside, it requires language specific resources.

chrF: Popović ( 2015 ) is based on the character @xmath -gram F-score.
In particular we will use the chrF2 score (based on the F2-score —
recall is given more importance), following the recommendations from
Popović ( 2016 ) . By operating on a sub-word level, it can reflect the
semantic similarity between different morphological inflections of one
word (for instance), without requiring language-specific knowledge which
makes it a good one-size-fits-all alternative.

Because multiple possible alternatives exist, it is important to know
which is the best stand-in for human evaluation. To elucidate this, we
will compare these metrics to human judgment in terms of Pearson
correlation coefficient on outputs resulting from a variety of attacks
in Section 16.2 .

#### 15 Gradient-Based Adversarial Attacks

In this section, we overview the adversarial attacks we will be
considering in the rest of this chapter.

##### 15.1 Attack Paradigm

We perform gradient-based attacks that replace one word in the sentence
so as to maximize an adversarial loss function @xmath , similar to the
substitution attacks proposed in Ebrahimi et al. ( 2018b ) .

###### 15.1.1 General Approach

Precisely, for a word-based translation model @xmath ⁴² ⁴² 42 Note that
this formulation is also valid for character-based models (see Ebrahimi
et al. ( 2018a ) ) and subword-based models. For subword-based models,
additional difficulty would be introduced due to changes to the input
resulting in different subword segmentations. This poses an interesting
challenge that is beyond the scope of the current work. , and given an
input sentence @xmath , we find the position @xmath and word @xmath
satisfying the following optimization problem:

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

where @xmath is a differentiable function which represents our
adversarial objective. Using the first order approximation of @xmath
around the original word vectors @xmath ⁴³ ⁴³ 43 More generally we will
use the bold @xmath when talking about the embedding vector of word
@xmath , this can be derived to be equivalent to optimizing

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

The above optimization problem can be solved by brute-force in @xmath
space complexity, whereas the time complexity is bottlenecked by a
@xmath times @xmath matrix multiplication, which is not more
computationally expensive than computing logits during the forward pass
of the model. Overall, this naive approach is sufficiently fast to be
conducive to adversarial training. We also found that the attacks
benefited from normalizing the gradient by taking its sign.

Extending this approach to finding the optimal perturbations for more
than 1 substitution would require exhaustively searching over all
possible combinations. However, previous work Ebrahimi et al. ( 2018a )
suggests that greedy search is a good enough approximation.

###### 15.1.2 The Adversarial Loss @xmath

We want to find an adversarial input @xmath such that, assuming that the
model has produced the correct output @xmath up to step @xmath during
decoding, the probability that the model makes an error at the next step
@xmath is maximized.

In the log-semiring, this translates into the following loss function:

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

##### 15.2 Enforcing Semantically Similar Adversarial Inputs

In contrast to previous methods, which don’t consider meaning
preservation, we propose simple modifications of the approach presented
in Section 15.1 to create adversarial perturbations at the word level
that are more likely to preserve meaning. The basic idea is to restrict
the possible word substitutions to similar words. We compare two sets of
constraints:

kNN: This constraint enforces that the word be replaced only with one of
its 10 nearest neighbors in the source embedding space. This has two
effects: first, the replacement will be likely semantically related to
the original word (if words close in the embedding space are indeed
semantically related, as hinted by Table 9 ). Second, it ensures that
the replacement’s word vector is close enough to the original word
vector that the first order assumption is more likely to be satisfied.

CharSwap: This constraint requires that the substituted words must be
obtained by swapping characters. Word internal character swaps have been
shown to not affect human readers greatly McCusker et al. ( 1981 ) ,
hence making them likely to be meaning-preserving. Moreover we add the
additional constraint that the substitution must not be in the
vocabulary, which will likely be particularly meaning-destroying on the
target side for the word-based models we test here. In such cases where
word-internal character swaps are not possible or can’t produce OOV
words, we resort to the naive strategy of repeating the last character
of the word. The exact procedure used to produce this kind of
perturbations is described in Listing 15.2 . Note that for a word-based
model, every OOV will look the same (a special <unk> token), however the
choice of OOV will still have an influence on the output of the model
because we use unk-replacement.

⬇

1

1 def make_oov (

2 word ,

3 vocab ,

4 max_scrambling ,

5 ):

6 """Modify a word to make it OOV

7 (while keeping the meaning)"""

8 # If the word has >3 letters

9 # try scrambling them

10 L = len ( word )

11 if L > 3:

12 # For a fixed number of steps

13 for _ in range ( max_scrambling ):

14 # Swap two adjacent letters

15 # in the middle of the word

16 pos = random . randint (1, L - 3)

17 word = word [: pos ]

18 word += word [ pos +1] + word [ pos ]

19 word += word [ pos +2:]

20 # If we got an OOV already just

21 # return it

22 if word not in vocab :

23 return word

24 # If nothing worked, or the word is

25 # too short for scrambling, just

26 # repeat the last letter ad nauseam

27 char = word [-1]

28 while word in vocab :

29 word = word + char

30 return word

\setstretch

In contrast, we refer the base attack without constraints as
Unconstrained hereforth. Table 9 gives qualitative examples of the kind
of perturbations generated under the different constraints.

For subword-based models, we apply the same procedures at the
subword-level on the original segmentation. We then de-segment and
re-segment the resulting sentence (because changes at the subword or
character levels are likely to change the segmentation of the resulting
sentence).

#### 16 Experiments

Our experiments serve two purposes. First, we examine our proposed
framework of evaluating adversarial attacks (Section 14 ), and also
elucidate which automatic metrics correlate better with human judgment
for the purpose of evaluating adversarial attacks (Section 16.2 ).
Second, we use this evaluation framework to compare various adversarial
attacks and demonstrate that adversarial attacks that are explicitly
constrained to preserve meaning receive better assessment scores
(Section 16.3 ).

##### 16.1 Experimental setting

Data: Following previous work on adversarial examples for seq2seq models
Belinkov & Bisk ( 2018b ); Ebrahimi et al. ( 2018a ) , we perform all
experiments on the IWSLT2016 dataset Cettolo et al. ( 2016 ) in the
{French,German,Czech} @xmath English directions ( fr-en , de-en and
cs-en ). We compile all previous IWSLT test sets before 2015 as
validation data, and keep the 2015 and 2016 test sets as test data. The
data is tokenized with the Moses tokenizer Koehn et al. ( 2007 ) . The
exact data statistics can be found in Table 10 .

MT Models: We perform experiments with two common NMT models. The first
is an LSTM based encoder-decoder architecture with attention (Luong
et al., 2015 ) . It uses 2-layer encoders and decoders, and dot-product
attention. We set the word embedding dimension to 300 and all others to
500. The second model is a self-attentional Transformer Vaswani et al. (
2017 ) , with 6 1024-dimensional encoder and decoder layers and 512
dimensional word embeddings. Both the models are trained with Adam
Kingma & Ba ( 2014 ) , dropout Srivastava et al. ( 2014 ) of probability
0.3 and label smoothing Szegedy et al. ( 2016 ) with value 0.1. We
experiment with both word based models (vocabulary size fixed at 40k)
and subword based models (BPE Sennrich et al. ( 2016 ) with 30k
operations). For word-based models, we perform <unk> replacement,
replacing <unk> tokens in the translated sentences with the source words
with the highest attention value during inference. The full experimental
setup and source code are available at
https://github.com/pmichel31415/translate/tree/paul/pytorch_translate/research/adversarial/experiments
.

Automatic Metric Implementations: To evaluate both sentence and corpus
level BLEU score, we first de-tokenize the output and use sacreBLEU ⁴⁴
⁴⁴ 44 https://github.com/mjpost/sacreBLEU Post ( 2018b ) with its
internal intl tokenization, to keep BLEU scores agnostic to
tokenization. We compute METEOR using the official implementation ⁴⁵ ⁴⁵
45 http://www.cs.cmu.edu/~alavie/METEOR/ . ChrF is reported with the
sacreBLEU implementation on detokenized text with default parameters. A
toolkit implementing the evaluation framework described in Section 14.1
for these metrics is released at
https://github.com/pmichel31415/teapot-nlp .

##### 16.2 Correlation of Automatic Metrics with Human Judgment

We first examine which of the automatic metrics listed in Section 14.2
correlates most with human judgment for our adversarial attacks. For
this experiment, we restrict the scope to the case of the LSTM model on
fr-en . For the French side, we randomly select 900 sentence pairs
@xmath from the validation set, 300 for each of the Unconstrained, kNN
and CharSwap constraints. To vary the level of perturbation, the 300
pairs contain an equal amount of perturbed input obtained by
substituting 1, 2 and 3 words. On the English side, we select 900 pairs
of reference translations and translations of adversarial input @xmath
with the same distribution of attacks as the source side, as well as 300
@xmath pairs (to include translations from original inputs). This
amounts to 1,200 sentence pairs in the target side.

These sentences are sent to English and French speaking annotators to be
rated according to the guidelines described in Section 14.2.1 . Each
sample (a pair of sentences) is rated by two independent evaluators. If
the two ratings differ, the sample is sent to a third rater (an auditor
and subject matter expert) who makes the final decision.

Finally, we compare the human results to each automatic metric with
Pearson’s correlation coefficient. The correlations are reported in
Table 11 . As evidenced by the results, chrF exhibits higher correlation
with human judgment, followed by METEOR and BLEU. This is true both on
the source side ( @xmath vs @xmath ) and in the target side ( @xmath vs
@xmath ). We evaluate the statistical significance of this result using
a paired bootstrap test for @xmath . Notably we find that chrF is
significantly better than METEOR in French but not in English. This is
not too unexpected because METEOR has access to more language-dependent
resources in English (specifically synonym information) and thereby can
make more informed matches of these synonymous words and phrases.
Moreover the French source side contains more “character-level” errors
(from CharSwap attacks) which are not picked-up well by word-based
metrics like BLEU and METEOR.

We provide a breakdown of the correlation coefficients of automatic
metrics with human judgment for source-side meaning-preservation, both
in terms of number of perturbed words (Table 12 ) and constraint (Table
13 ). While those coefficients are computed on a much smaller sample
size, and their differences are not all statistically significant with
@xmath , they exhibit the same trend as the results from Table 11 (BLEU
@xmath METEOR @xmath chrF). In particular Table 12 shows that the good
correlation of chrF with human judgment is not only due to the ability
to distinguish between different number of edits.

Thus, in the following, we report attack results both in terms of chrF
in the source ( @xmath ) and relative decrease in chrF (RDchrF) in the
target ( @xmath ).

##### 16.3 Attack Results

We can now compare attacks under the three constraints Unconstrained,
kNN and CharSwap and draw conclusions on their capacity to preserve
meaning in the source and destroy it in the target. Attacks are
conducted on the validation set using the approach described in Section
15.1 with 3 substitutions (this means that each adversarial input is at
edit distance at most 3 from the original input). Results (on a scale of
0 to 100 for readability) are reported in Table 14 for both word- and
subword- based LSTM and Transformer models. To give a better idea of how
the different variables (language pair, model, attack) affect
performance, we give a graphical representation of these same results in
Figure 2 for the word-based models. The rest of this section discusses
the implication of these results.

Source chrF Highlights the Effect of Adding Constraints: Comparing the
kNN and CharSwap rows to Unconstrained in the “source” sections of Table
14 clearly shows that constrained attacks have a positive effect on
meaning preservation. Beyond validating our assumptions from Section
15.2 , this shows that source chrF is useful to carry out the comparison
in the first place ⁴⁶ ⁴⁶ 46 It can be argued that using chrF gives an
advantage to CharSwap over kNN for source preservation (as opposed to
METEOR for example). We find that this is the case for Czech and German
(source METEOR is higher for kNN) but not French. Moreover we find (see
e.g. Table 12 ) that chrF correlates better with human judgement even
for kNN. . To give a point of reference, results from the manual
evaluation carried out in Section 16.2 show that that @xmath of the
French sentence pairs to which humans gave a score of 4 or 5 in semantic
similarity have a chrF @xmath .

Different Architectures are not Equal in the Face of Adversity:
Inspection of the target-side results yields several interesting
observations. First, the high RDchrF of CharSwap for word-based model is
yet another indication of their known shortcomings when presented with
words out of their training vocabulary, even with <unk> -replacement.
Second, and perhaps more interestingly, Transformer models appear to be
less robust to small embedding perturbations (kNN attacks) compared to
LSTMs. Although the exploration of the exact reasons for this phenomenon
is beyond the scope of this work, this is a good example that RDchrF can
shed light on the different behavior of different architectures when
confronted with adversarial input. Overall, we find that the CharSwap
constraint is the only one that consistently produces attacks with
@xmath average success (as defined in Section 14.1 ) according to Table
14 . Table 15 contains two qualitative examples of this attack on the
LSTM model in fr-en .

#### 17 Adversarial Training with Meaning-Preserving Attacks

##### 17.1 Adversarial Training

Adversarial training Goodfellow et al. ( 2014b ) augments the training
data with adversarial examples. Formally, in place of the negative log
likelihood (NLL) objective on a sample @xmath , @xmath , the loss
function is replaced with an interpolation of the NLL of the original
sample @xmath and an adversarial sample @xmath :

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

Ebrahimi et al. ( 2018a ) suggest that while adversarial training
improves robustness to adversarial attacks, it can be detrimental to
test performance on non-adversarial input. We investigate whether this
is still the case when adversarial attacks are largely
meaning-preserving.

In our experiments, we generate @xmath by applying 3 perturbations on
the fly at each training step. To maintain training speed we do not
solve Equation ( 7 ) iteratively but in one shot by replacing the argmax
by top-3. Although this is less exact than iterating, this makes
adversarial training time less than @xmath slower than normal training.
We perform adversarial training with perturbations without constraints
(Unconstrained-adv) and with the CharSwap constraint (CharSwap-adv). All
experiments are conducted with the word-based LSTM model.

##### 17.2 Results

Test performance on non-adversarial input is reported in Table 16 . In
keeping with the rest of the paper, we primarily report chrF results,
but also show the standard BLEU as well.

We observe that when @xmath , i.e. the model only sees the perturbed
input during training ⁴⁷ ⁴⁷ 47 This setting is reminiscent of word
dropout Iyyer et al. ( 2015 ) . , the Unconstrained-adv model suffers a
drop in test performance, whereas CharSwap-adv’s performance is on par
with the original. This is likely attributable to the spurious training
samples @xmath where @xmath is not an acceptable translation of @xmath
introduced by the lack of constraint. This effect disappears when @xmath
because the model sees the original samples as well.

Not unexpectedly, Table 17 indicates that CharSwap-adv is more robust to
CharSwap constrained attacks for both values of @xmath , with @xmath
giving the best results. On the other hand, Unconstrained-adv is
similarly or more vulnerable to these attacks than the baseline. Hence,
we can safely conclude that adversarial training with CharSwap attacks
improves robustness while not impacting test performance as much as
unconstrained attacks.

#### 18 Related work

Following seminal work on adversarial attacks by Szegedy et al. ( 2013 )
, Goodfellow et al. ( 2014b ) introduced gradient-based attacks and
adversarial training. Since then, a variety of attack Moosavi-Dezfooli
et al. ( 2016 ) and defense Cissé et al. ( 2017 ); Kolter & Wong ( 2017
) mechanisms have been proposed. Adversarial examples for NLP
specifically have seen attacks on sentiment Papernot et al. ( 2016 );
Samanta & Mehta ( 2017 ); Ebrahimi et al. ( 2018b ) , malware Grosse
et al. ( 2016 ) , gender Reddy & Knight ( 2016 ) or toxicity Hosseini
et al. ( 2017 ) classification to cite a few.

In MT , methods have been proposed to attack word-based Zhao et al. (
2018 ); Cheng et al. ( 2018a ) and character-based Belinkov & Bisk (
2018b ); Ebrahimi et al. ( 2018a ) models. However these works side-step
the question of meaning preservation in the source: they mostly focus on
target side evaluation. Finally there is work centered around
meaning-preserving adversarial attacks for NLP via paraphrase generation
Iyyer et al. ( 2018 ) or rule-based approaches Jia & Liang ( 2017 );
Ribeiro et al. ( 2018 ); Naik et al. ( 2018 ); Alzantot et al. ( 2018 )
. However the proposed attacks are highly engineered and focused on
English.

#### 19 Conclusion

This chapter highlights the importance of performing meaning-preserving
adversarial perturbations for NLP models (with a focus on seq2seq ). We
proposed a general evaluation framework for adversarial perturbations
and compared various automatic metrics as proxies for human judgment to
instantiate this framework. We then confirmed that, in the context of MT
, “naive” attacks do not preserve meaning in general, and proposed
alternatives to remedy this issue. Finally, we have shown that a careful
choice of more meaning-preserving perturbations is beneficial for
adversarial training.

Since the original publication of the content in this chapter in 2019, a
number of works have followed-up, either by adapting the evaluation
framework to other tasks, e.g. semantic parsing in Huang et al. ( 2021 )
, or by building upon it for designing more imperceptible adversarial
perturbations, for instance using the proposed evaluation metrics as
rewards for reinforcement learning based perturbation generation (Zou
et al., 2020 ) , or pushing further the concept of indistinguishable
perturbations with encoding specific character substitutions (Boucher
et al., 2021 ) .

## Part II Making Robust Models

### Chapter \thechapter Modeling the Second Player in Distributionally
Robust Optimization

#### 20 Introduction

Up to this point, we have principally been concerned with evaluating the
effect that various types of distributional shifts have on existing
models. In this chapter and the next, we now tackle the problem of
training models that are more robust against distributional shifts.

The tendency of machine learning models to exhibit drops in accuracy
when confronted with data from domains that are absent from or
under-represented in their training data often arises from the objective
function of ERM . Recall that in ERM , the parameters @xmath of the
model are learned by minimizing the expectation of a loss function
@xmath under a data distribution @xmath (or, specifically in practice,
an associated empirical data distribution @xmath )

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

When the model encounters data sampled from a different distribution
@xmath , performance can suffer significantly. As described in Chapter
Learning Neural Models for Natural Language Processing in the Face of
Distributional Shift , distributionally robust optimization ( DRO
)provides a natural solution to this issue by replacing the expected
risk under a single distribution @xmath with the worst expected risk
over a pre-determined family of distributions @xmath (the “uncertainty
set”)

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

If @xmath contains @xmath , the DRO objective upper bounds the expected
risk under @xmath . However, a priori knowledge of possible test
distributions is not always available or easy to acquire. For example,
training a model to be robust to some demographic attributes ( @xmath )
requires collecting and annotating data with the necessary information,
an expensive and ethically fraught endeavour. In the absence of such
information, one has to resort to defining the uncertainty set
analytically, drawing on one’s intuition of what constitutes a possible
test distribution given the observed training distribution, such as
using moment constraints (Delage & Ye, 2010 ; Nguyen et al., 2020 ) ,
@xmath -divergence (Ben-Tal et al., 2013 ; Hu & Hong, 2013 ; Faury
et al., 2020 ) , Wasserstein/IPM (Sinha et al., 2018 ; Husain, 2020 )
balls, or coarse-grained mixture models (Oren et al., 2019 ; Hu et al.,
2018 ) . However, the need for keeping the inner supremum in Eq. ( 12 )
tractable limits the possible choices.

In this chapter, we propose that the uncertainty set be instead defined
as a family of parametric generative models. The resulting DRO objective
(§ 21 ) is a differentiable game with two players: the original model
@xmath and a model of its worst-case distribution @xmath , the titular
“second player” which we hereafter refer to as the adversary . Using
this formulation — which we call Parametric DRO (P-DRO) — allows for
more flexibility in the choice of the adversary’s architecture (and so
the uncertainty set). Unfortunately, finding a solution of this game via
direct application of simultaneous gradient descent (Singh et al., 2000
) is difficult (Balduzzi et al., 2018 ) . In particular, direct gradient
descent on the uncertainty set suffers from instability due to the large
variance of the gradients (Greensmith et al., 2004 ) , and
hyper-parameter selection is not straightforward.

To address these challenges, we make two main contributions (§ 22 ):
first, we propose a new relaxation of the DRO game’s inner maximization
problem (with KL constraints). The resulting objective is more amenable
to simultaneous gradient update than the original zero-sum game and
significantly improves training stability, while still yielding useful
adversaries. Second, we develop a principled approach for selecting
hyper-parameters: we leverage the learned adversaries to decide which of
any two given models trained with P-DRO is more robust than the other.

We do an in-depth set of experiments analyzing the effect of our
proposed changes on both a toy task as well as a more realistic, yet
still synthetic sentiment classification task (§ 23 ). Finally, we show
that in the more realistic setting of toxicity detection, P-DRO yields
models that are more robust to changes in demographic groups, even
though these groups are unknown at training time, opening up
applications in combatting dataset bias (§ 24 ). Code to reproduce our
experiments can be found at https://github.com/pmichel31415/P-DRO .

#### 21 Parameterizing the Uncertainty Set

Consider a model parameterized by @xmath . Minimizing the DRO objective
described in Eq. ( 12 ) over the uncertainty set @xmath turns the
optimization problem into the min-max (or zero-sum) game

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

The first player controls the parameters @xmath , whilst the second
player controls the worst-case distribution @xmath . In the absence of
explicit information on groups of interest (such as demographics,
domain, etc.), an adequate choice of the uncertainty set @xmath is
critical to the success of DRO . This is in fact very much an active
area of research ( Sinha et al. ( 2018 ); Duchi & Namkoong ( 2018 );
Oren et al. ( 2019 ) , see Rahimian & Mehrotra ( 2019 ) for a survey).
@xmath must be sufficiently large to contain test distributions of
interest, but if it is too large it may contain “adversarial”
distributions on which no model can perform well. Moreover, the design
of @xmath is also circumscribed by the necessity of keeping the min-max
problem tractable, particularly in the context of stochastic
optimization. In Hu & Hong ( 2013 ) and Duchi et al. ( 2016 ) for
example, the choice of @xmath -divergence balls allows the use of
duality arguments to reformulate ( 13 ) as a more manageable min-min
problem. Others, like Hu et al. ( 2018 ) or Oren et al. ( 2019 ) ,
propose using mixture models, the simplicity of which enables them to
solve the inner maximization problem efficiently.

Instead, we propose to explicitly model the second player in the DRO
game as a parametric model @xmath of the data. Of course, not all
parameterizations @xmath of a given generative model represent useful
distributions, and we require that the adversary stay “close” to the
underlying true data distribution @xmath . As a measure of distance
between @xmath and @xmath , we choose the KL (Kullback & Leibler, 1951 )
divergence due to its wide acceptance in the machine learning community,
as well as its appealing properties in the context of DRO . ⁴⁸ ⁴⁸ 48 For
instance: @xmath implies that @xmath stays within the support of @xmath
The KL upper bound, @xmath , is left as a parameter to be decided by the
experimenter. We refer to the resulting DRO formulation as Parametric
DRO

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

#### 22 Optimizing P-DRO

The min-max problem in Eq. ( 14 ) belongs to a class of games called
“differentiable games” (another famous representative being generative
adversarial networks (Goodfellow et al., 2014a ) ). We can search for a
solution of this game with simultaneous gradient descent (Singh et al.,
2000 ) , i.e. by simultaneously updating @xmath and @xmath with @xmath
and @xmath respectively. Unfortunately, in general, there is no
theoretical guarantee that simultaneous gradient descent will converge
to a Nash equilibrium ⁴⁹ ⁴⁹ 49 Nash equilibria (Osborne & Rubinstein,
1994 ) can be thought of the game theoretic analog of global minima in
optimization. (Balduzzi et al., 2018 ) , nor that any such equilibrium
even exists if the objective is non-convex in @xmath (or non-concave in
@xmath ). The success of GANs and the follow-up literature (Wang et al.,
2019c ) serves as an encouraging example that gradient based methods can
yield useful solutions despite the pessimistic theoretical results. In
this section, we discuss difficulties that arise when optimizing @xmath
and @xmath jointly, and propose modifications of the objective to
address them.

##### 22.1 Training the Model @xmath

We could train the model @xmath by taking negative gradient steps on
@xmath . This gradient can be estimated by sampling examples from @xmath
and averaging the gradient of their losses. Unfortunately, this
objective requires that @xmath is well-behaved at all iterations, as it
is the only source of supervision for @xmath . If @xmath is initialized
incorrectly or begins producing unrealistic @xmath , the quality of
@xmath degrades as it begins to learn a predictor on invalid training
examples from @xmath As an alternative, we opt to compute the gradients
for @xmath with importance sampling, i.e. rewriting @xmath as @xmath ,
which ensures that all @xmath samples will be derived from the training
set itself. Unfortunately, the true density @xmath is unknown to us. As
an approximation, we replace @xmath with the likelihood ratio between
@xmath and the maximum likelihood estimate of @xmath , @xmath . This
changes the min-max problem to

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

This becomes a simple expected loss objective, which we can estimate by
sampling from the empirical distribution @xmath . In experiments, we
find that with this formulation we are able to train robust @xmath even
when @xmath is only a mediocre generative model (see Appendix 25.2 ). To
further stabilize training at the beginning of the optimization process,
we initialize @xmath with @xmath , making the objective exactly the same
as ERM for the first gradient step.

##### 22.2 Training the Adversary @xmath

According to Eq. ( 15 ) the adversary @xmath must maximize @xmath within
a KL ball of fixed radius. This is challenging for several reasons:
first, enforcing the bound is intractable for complex families of
adversaries where e.g. projecting onto the KL ball is another difficult
optimization problem of its own. Second, maximizing the expectation with
respect to the parameters of the distribution @xmath is prone to
instability due to large gradient variance (Greensmith et al., 2004 ) .

###### Lagrangian Relaxation

To address the first difficulty, we loosen the strict KL constraint and
instead consider the Lagrangian relaxation @xmath

  -- -------- -------- -- ------
     @xmath   @xmath      (16)
  -- -------- -------- -- ------

We fix the Lagrangian multiplier @xmath as treat it as a “temperature”
hyper-parameter. With some reorganization (which we develop in Appendix
A ), we can show that

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

Where @xmath and @xmath is a constant in @xmath . In other words,
maximizing @xmath in @xmath is equivalent to minimizing the KL
divergence between @xmath and @xmath . One difficulty with this
objective is that @xmath depends upon the unknown probability density
@xmath . We avoid this problem by treating the density ratio @xmath as a
constant, which is closely related to assumptions that have been used
successfully in past formulations of DRO (Oren et al., 2019 ) .
Empirically, we find that incorporating @xmath as a surrogate for @xmath
is a serviceable approximation, as demonstrated in Section 23 .

###### Reversing the KL

Minimizing the KL divergence in this direction is difficult for several
reasons. First, it entails optimizing an expectation in @xmath over
@xmath , which is difficult due to the large variance of the gradients
(Greensmith et al., 2004 ) . Second, computing this KL necessitates
access to the true theoretical density @xmath in order to compute @xmath
in the argument of the expectation, but this quantity is unknown in
practice. ⁵⁰ ⁵⁰ 50 Note that substituting the empirical distribution
@xmath for @xmath poses issues here, because @xmath is not absolutely
continuous with respect to @xmath . To sidestep these issues, we elect
to minimize the reverse direction @xmath instead. Due to the KL
divergence being non-symmetric, this is a rather crude approximation ⁵¹
⁵¹ 51 For instance, the optimum of the reverse KL doesn’t necessarily
match that of the forward KL within the parametric confusion set @xmath
, the implications of which are discussed in Norouzi et al. ( 2016 ) .
However, we find that this approach dramatically stabilizes the gradient
dynamics while still yielding good adversaries, as observed empirically
in Section 23.7 . Discarding the entropy term (constant in @xmath ), the
resulting problem is equivalent to minimizing

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

in @xmath , where @xmath is the normalizer of @xmath . In this case, we
can estimate this expectation by substituting the empirical distribution
@xmath for @xmath in the expectation.

###### Computing the Normalizer

Approximating the inverse normalizer @xmath in a minibatch yields a
biased estimator. On the other hand, computing @xmath over the entire
training data at each step is prohibitive since it requires computing
the loss of every single example. As a middle ground, we keep a running
normalizer @xmath computed from the average of the normalizers over a
fixed number @xmath of consecutive minibatches. In other words, if
@xmath and @xmath denote the minibatch and adversary parameters at step
@xmath respectively, the normalizer at step @xmath will be

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

If @xmath is too low, there is a risk of under-estimating the
normalizer, especially if the distribution of weights contains
infrequent high weight samples. On the other hand, if @xmath is too high
there is a risk of using “stale” weights in the normalizer. In
experiments, we treat @xmath as a hyper-parameter.

##### 22.3 Optimal Stopping

When should one stop training a model with P-DRO ? In ERM it is
customary to stop training after the empirical risk — periodically
evaluated on a held out validation dataset — stops decreasing. This is
particularly important to prevent over-fitting to the training data.
However, it is not an appropriate criterion for P-DRO , since the model
is not trained to minimize empirical risk in the first place. A more
pertinent choice is to compare the robust validation losses

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

However, finding the inner supremum for each of the @xmath evaluation
checkpoints @xmath is expensive as it requires solving @xmath
independent optimization problems. Instead, we leverage the existence of
adversaries @xmath associated with each model @xmath , as well as the
initial adversary @xmath and take the maximum over the @xmath
adversaries @xmath . Since our relaxation of the P-DRO objective loosens
the KL constraint, we need weed out adversaries which might violate it.
Specifically, we estimate the @xmath on the validation set, using @xmath
as a stand-in for @xmath , and reject all adversaries for which the
result is greater than a threshold, which we set to @xmath based on
preliminary experiments detailed in Appendix 25.1 . ⁵² ⁵² 52 To simplify
notation, this additional constraint is implicit in the rest of this
section. We refer to this stopping criterion as Minmax .

Computing the full min-max necessitates keeping track of @xmath models
and @xmath adversaries, which is ponderous when the model is large. As a
solution, we propose an approximation, Greedy-Minmax , in which we only
keep one best model @xmath . At each evaluation step @xmath , we compare
@xmath to @xmath , and update @xmath to whichever achieves lower robust
validation loss over the @xmath adversaries @xmath .

By keeping track of only one additional model, and using the weights
@xmath of individual examples in @xmath as sufficient statistics for
computing the loss against each adversary, Greedy-Minmax can be achieved
with space complexity @xmath , which is much more efficient than the
@xmath of Minmax.

##### 22.4 Hyper-parameter Selection

Our proposed P-DRO method relies on 3 different hyper-parameters (in
addition to the model’s hyper-parameters): the adversary learning rate
@xmath , the temperature @xmath and the size of the re-normalizing
window @xmath . As a consequence, we need a reliable criterion for
deciding which of two configurations is better. This model comparison
bears many similarities with the stopping problem described above.
Therefore, we resort to a similar solution: given two models @xmath ,
@xmath trained with P-DRO , and their respective adversaries @xmath ,
@xmath (for instance, the adversaries associated with @xmath and @xmath
at periodic checkpoints during training), we select the best model
following

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

#### 23 Experimental Analysis of P-Dro

Before moving on to a real world scenario in Section 24 , we first
demonstrate that P-DRO is able to learn robust models in a synthetic NLP
task, and perform ablation studies to examine the importance of the
various modifications described in Section 22 .

##### 23.1 Experimental Setting

For analysis purposes, we design a simple NLP task amenable to DRO . We
specifically choose NLP as a domain due to the striking success of
language models as generative models of textual data (Sundermeyer
et al., 2012 ; Radford et al., 2018 ) , which can be used to model the
uncertainty set. We base our task off of the binary version of the
Stanford Sentiment Treebank dataset (SST-2; Socher et al. ( 2013 ) ),
which we modify to introduce spurious correlation. Specifically, we
introduce a distractor token to some sentences. The distractor we use
consists of prepending “so , ” to the sentence (“i hated this movie”
@xmath “so , I hated this movie”), which doesn’t change the underlying
sentiment. The resulting samples can be categorized in 4 “groups”
depending on their label (positive or negative) and the presence or
absence of the distractor. In particular, we add this distractor to 95%
of the negative reviews and 5% of the positive reviews in the training
and validation set, so that the presence of the distractor strongly
correlates with negative sentiment (a similar construction is proposed
in (Utama et al., 2020 ) ). In the test data, we modify 50% of all
sentences for each class equitably to ensure that there is enough data
in each group, but we report “average” test accuracy by re-weighting the
group accuracies to mimick the training distribution. We call this
modified task BiasedSST .

##### 23.2 Model Settings

In all experiments, we split the text into sub-word tokens using the
tokenizer described in (Devlin et al., 2018 ) . For the classifier, we
train a simple one layer BiLSTM model with embedding/hidden dimension
300. During training, we sample minibatches that contain at most @xmath
sentences or @xmath tokens, whichever is greater, in order to prevent
GPU memory overflow in case of long sentences. We train all models with
Adam (Kingma & Ba, 2014 ) with an initial learning rate of @xmath ,
which we decay linearly at each step until the end of training. We
validate the models every epoch. For BERT, we start from the
bert-base-uncased checkpoint.

##### 23.3 Adversary Settings

In all experiments, we use an auto-regressive transformer model based on
the successful GPT-2 language model (Radford et al., 2019 ) architecture
but with 6 layers, a dimension of 512 and 8 attention heads (we
experiment with a smaller, LSTM based adversary in Section 25.2 ). In
order to model the input output pair @xmath , we pre-pend a special
label-specific token to sentences before running them through the
language model. We train the adversary with vanilla stochastic gradient
descent, which we found more stable in experiments.

To initialize the adversary (to obtain @xmath ), we first pre-train the
model on a generic, relatively large language modeling dataset,
WikiText-103 (Merity et al., 2017 ) . We also use a batch size of 64
samples or 2500 tokens, and train with Adam for 10 epochs, with a fixed
learning rate of @xmath . Then, we fine-tune this model on each dataset,
this time minimizing the negative log-likelihood of the @xmath pair,
using the same hyper-parameters but a smaller learning rate ( @xmath ).
We find that, due to the small to medium size of the datasets under
consideration, this LM pretraining step helped achieve lower error on
the generative modeling task.

##### 23.4 Baselines

In addition to ERM, we also compare three other approaches. First, to
appreciate how well the model could perform if the groups were known at
training time, we train with Group-DRO on the oracle groups using an
exponentiated-gradients based online algorithm ( Oracle DRO ; Sagawa
et al. ( 2020 ) ). Second, we implement Topic DRO (Oren et al., 2019 ) ,
a method for DRO on NLP where the uncertainty set is determined by
mixtures of a topic model. Finally, we compare to non-parametric DRO
with a KL constrained uncertainty set (Hu & Hong, 2013 ; Hu et al., 2018
) , which we adapt to fit our online mini-batch training setting (
NonParam ). Below, we outline experimental details for the latter two.

###### 23.4.1 Topic DRO

To train the topic model for Topic DRO, we first pre-process the text by
removing all punctuation, urls and user mentions (for twitter data).
Importantly, we remove stop-words for our toxicity experiments but not
for our BiasedSST experiment. This is because the distractor token we
use (‘‘so’’) belongs to most English stop words lists, and removing it
would completely prevent the topic model from picking up on the groups
of interest. We then estimate the parameters of the model with Gensim ⁵³
⁵³ 53 https://radimrehurek.com/gensim/ and use similar settings as Oren
et al. ( 2019 ) ( @xmath , @xmath ), setting the number of topics to 10.

For both Oracle-DRO and Topic DRO, we use the algorithm proposed in
Sagawa et al. ( 2020 ) to estimate the worst-case group (either oracle
group or topic in Topic DRO) online during training. We perform
grid-search over @xmath to find the best learning rate for the group
weights update. For Oracle DRO , the best model is simply selected by
robust validation accuracy. For Topic DRO, unless specified otherwise,
we select the model with the lowest worst-case error over all topics.

###### 23.4.2 NonParam

In the KL-constrained non-parametric setting, the min-max problem reads

  -- -------- -- ------
     @xmath      (22)
  -- -------- -- ------

Here, @xmath is the desired radius of the KL ball, and is treated as a
hyper-parameter. The solution of the inner maximum has an analytical
solution of the form @xmath (see Hu & Hong ( 2013 ); Hu et al. ( 2018 )
for details) with @xmath and @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Note that both computing @xmath and @xmath require taking expectations
over @xmath . In our setting, where @xmath is the output of a large
neural network, we cannot afford to take this expectation over the
entire training data at each step. Instead, we fall back to taking the
average over each mini-batch. We find @xmath with binary search in
@xmath space within the @xmath interval and clip to the lowest or
highest value should the result lie outside the search interval.

In all experiments, we try 4 different values for @xmath : @xmath ,
@xmath , @xmath and @xmath . Unless indicated otherwise, we perform
early stopping and hyper-parameter selection using our Minmax criterion
using the non-parametric weights as adversaries on the validation data.

##### 23.5 P-DRO can Learn Robust Models

We train 7 models with P-DRO on BiasedSST using different
hyper-parameters for the adversary. We start from configuration @xmath ,
@xmath , @xmath , and for each hyper-parameter we run a configuration
with a smaller and a higher value, keeping all other hyper-parameters
the same. We train for 50 epochs and select the best model using the
strategies described in Section 22 .

We report the worst-case (“robust”) accuracy over all groups on the test
set, as well the average accuracy in Table 18 (we report the mean and
standard deviation over 5 runs). We find that both Topic DRO, NonParam
and P-DRO are more robust than ERM , but the latter outperforms the
former two close to 30 and 7 points respectively, achieving @xmath of
Oracle DRO’s robust accuracy, while not leveraging any information on
the oracle groups.

##### 23.6 Optimal Stopping and Hyper-parameter Selection Ablation

To understand the importance of the optimal stopping and hyper-parameter
selection strategy described in Section 22.3 , we perform an ablation on
the BiasedSST dataset comparing 4 strategies:

-    Average : models are selected based on their average zero-one loss
    ( i.e. error rate) on the unmodified validation set. This is the
    baseline stopping criterion.

-    Minmax : selection based on the adversaries (as described in
    Section 22.3 ), with and without the KL constraint , as well as its
    variant Greedy-Minmax for stopping.

-    Oracle : in this setting the groups are known (in the validation
    set), and models are selected based on their error rate on the worst
    performing group. This is the optimal criterion for the group-DRO
    setting we are considering.

To compare stopping criterions experiments, we only consider one set of
hyper-parameters: @xmath , @xmath and @xmath . From the robust
validation accuracies reported in Table 19(a) , we first observe that
Average stopping results in a robust accuracy of 0, highlighting the
necessity for a suitable stopping criterion. We find that Minmax,
especially with a KL constraint, is a much better strategy, recovering
@xmath of the performance achievable with Oracle stopping. Notably, the
Greedy-Minmax variant which we use in practice reaches very close
results ( @xmath point difference) despite its requiring to keep track
of only 2 out of the 50 model checkpoints at any time.

To understand the effectiveness of the Minmax strategy for selecting
hyper-parameters. We take the models trained in Section 23.5 , but
select the best hyper-parameters using the different strategies
described above. Results, shown in Table 19(b) , confirm that Minmax
(with the KL constraint) is a better choice than Average for selecting
hyper-parameters, even though the improvement is not as striking as for
stopping.

##### 23.7 Importance of @xmath

Finally, we investigate the importance of modifying the adversary’s
objective as described in Section 22.2 . For this experiment, we devise
a simpler toy task on which directly training the constrained DRO
objective is possible. Specifically, we consider the two-dimensional
binary classification problem pictured in Figure 4 . The training data
consists of 10,000 points partitioned in two normally distributed
“domains” with a 1:50 sampling ratio and different classification
boundaries. We train a logistic regression model, which cannot perfectly
fit the training data and must trade-off between accuracy on each
domain. For the sake of simplicity, we only model the input variables
@xmath ⁵⁴ ⁵⁴ 54 In other words, we set @xmath , where @xmath , is the
true conditional which will be canceled out in the ratio @xmath . as
isotropic normal distributions with fixed variance: the adversaries’
parameter @xmath represents the location of the Gaussian (we fix the
variance to the empirical variance of the data).

We compare 3 different versions of P-DRO : first, naive simultaneous
gradient descent on the zero-sum game, without any constraint on the
adversary ( bare P-DRO ), then the same, but with an approximation of
the explicit KL constraint between @xmath and @xmath ( +KL constraint .
Even in this simplest setting, the exact KL between @xmath (a gaussian)
and @xmath (a mixture of gaussians) does not have an analytical
expression (Hershey & Olsen, 2007 ) . Instead, we fall back on enforcing
the KL constraint between @xmath and @xmath , both isotropic gaussians
with the same standard deviation. Let @xmath and @xmath denote their
respective mean, and @xmath their standard deviation. In this context,
their KL divergence reduces to:

  -- -------- --
     @xmath   
  -- -------- --

In other words, the KL divergence is equivalent to the euclidean
distance between the distributions’ means. We use this fact to project
@xmath (in the KL sense) onto @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Finally we report results using our relaxation and the KL reversal
described in Section 22.2 ( + @xmath ). For each setting, we report the
average and robust accuracy with mean and standard deviation over 10
runs. For the KL constraint and the relaxation, we report the best
results among 4 values of the KL bound @xmath and the temperature @xmath
respectively.

In Table 20 , we observe that bare P-DRO is too unstable and
systematically diverges. The addition of a KL constraint mitigates this
behaviour, but the zero-sum objective is still unstable, as evidenced by
the high standard deviations. Finally, we find that the addition of
@xmath stabilizes the training process greatly, leading to consistently
high robust accuracy.

#### 24 P-DRO in Practice: Case Study of Toxicity Detection

In this section, we demonstrate the effectiveness of P-DRO in the more
realistic setting of toxicity detection, the task of recognizing various
forms of toxic language (eg. hate speech or offensive language).
Identifying online abuse on the internet is a crucial challenge, and has
garnered much interest in the NLP community (Schmidt & Wiegand, 2017 ;
Fortuna & Nunes, 2018 ) . However, recent work (Sap et al., 2019 ) has
shown that there is strong correlation between toxic labels and the
presence of certain markers of dialects of English spoken by minority
groups. This correlation is in turn amplified by hate speech classifiers
trained on such data, leading to biased prediction.

Our results on BiasedSST suggest that P-DRO can provide one solution to
preventing models from absorbing spurious correlations present in their
training data, even in the absence of protected attributes (such as
language variety here).

##### 24.1 Experimental Setting

Following Sap et al. ( 2019 ) and Xia et al. ( 2020 ) , we perform
experiments on two datasets: DWMW17 (Davidson et al., 2017 ) , a corpus
of 25K tweets classified in three categories: hate speech ( @xmath ),
offensive ( @xmath ) and neither ( @xmath ), and FDCL18 (Founta et al.,
2018 ) , a 100k sized dataset, also collected from Twitter and annotated
with an additional spam label, with the following breakdown by
categories: hateful ( @xmath ), abusive ( @xmath ), normal ( @xmath )
and spam ( @xmath ).

The released version of these datasets does not contain information on
the dialect of each user. In order to be able to evaluate our models,
and to train an Oracle DRO baseline, we follow Sap et al. ( 2019 ) and
use annotations provided by the dialect classifier described in Blodgett
et al. ( 2016 ) to label each example as one of four English varieties:
White-aligned, African American, Hispanic, and Other. Note that, as
these are automatically obtained labels, the groups may not exactly
correspond to the actual racial sociolects, however Sap et al. ( 2019 )
does report that they correlate highly with self-reported race, and they
serve as a useful proxy in the absence of manual annotation.

We formulate the group-DRO problem by separating each dataset into
independent groups identified by both language variety and label, for a
total of 12 and 16 groups for DWMW17 and FDCL18 respectively. Some of
these groups are severely under-represented in the test set. In order to
make our robust accuracy results reliable yet still representative of
the under-represented groups, we combine groups that contain less than
100 samples into a single group to compute robust test accuracies.

On DWMW17 , we train the same BiLSTM model as described in Section 23.6
. To illustrate the applicability of P-DRO to other model architectures,
we pick BERT (Devlin et al., 2018 ) , a large scale pre-trained model as
a classifier on FDCL18 . In both cases, we adopt the Transformer
architecture described in Section 23.6 as the adversary. We train the
adversary with a temperature of @xmath and a normalizing window @xmath .
To demonstrate the efficacy of automatic hyper-parameter selection in
the P-DRO setting, we delegate the choice of the adversary’s learning
rate @xmath to grid-search, training 3 models with @xmath and selecting
the best using the Minmax criterion described in Section 22.4 . We also
report numbers for Oracle DRO and Topic DRO. Results are averaged over 5
runs, each with a different random seed.

##### 24.2 Can P-Dro Produce more Robust Models?

Table 21(a) reports the robust test accuracies of all models on both
tasks. Importantly, except for Oracle DRO, none of the methods compared
here necessitate any knowledge of the groups, neither in the training
nor validation data. We observe that in both settings P-DRO is able to
achieve higher robust accuracy than ERM , Topic DRO and NonParam.

This suggests P-DRO as a useful option in case no group information
whatsoever is available. However, in practice, it may be feasible to
annotate at least a small amount of data with group information. To
emulate this scenario, we perform the same experiment, but assume that
group annotations are available on the validation data, which we use to
determine optimal stopping and hyper-parameters. Results for this
setting are reported in Table 21(b) . We find that, while the use of
robust validation accuracy yields more robust models even for ERM
(especially on FDCL18 ), P-DRO is still the best alternative that
doesn’t require group annotation on the training data.

#### 25 Additional Experiments

##### 25.1 Minmax Validation KL Threshold

The Monte-Carlo estimate of @xmath on the validation set is @xmath .
Similarly to Section 22 , we approximate the (unknown) likelihood ratio
@xmath with @xmath .

We want to reject all adversaries where this approximated KL is greater
than some threshold, @xmath , but how do we choose a good value for
@xmath ? Consider an adversary which selects a fraction of the
validation data of size @xmath for some @xmath . In such a case, the
likelihood ratio is @xmath on this subset and 0 everywhere else, and the
resulting KL estimate will be @xmath . In other words, choosing a
threshold of @xmath means allowing the adversary to potentially select
any subset of size at least @xmath of the original data. Our heuristic
choice, @xmath , corresponds to allowing subsets of size at least @xmath
of @xmath .

Of course, this is only a heuristic because the adversary can reweight
the validation set non-uniformly. To assess the effect of @xmath on
Greedy-Minmax, we compute the average robust validation error of the
selected model across 5 runs for 3 different values of the adversary’s
learning rate. Results on BiasedSST, depicted in Figure 5 , show that
adversaries with higher learning rate are more sensitive to the choice
of threshold, but all values of @xmath between @xmath and @xmath seem to
work for these settings.

##### 25.2 P-Dro Experiments with an LSTM Adversary

We replicate the experiments BiasedSST experiments in Section 23 , but
this time using a smaller generative model, which is unlikely to
generate good samples. Specifically, we use a one layer LSTM model
(Hochreiter & Schmidhuber, 1997 ) with embedding and hidden dimension
256. We only perform grid-search over @xmath and select the best with
Minmax.

Once pre-trained on the BiasedSST dataset, this model achieves a
perplexity of @xmath , more than 4 times worse than the transformer
model we use in other experiments ( @xmath ). However, as evidenced by
its robust accuracy displayed in Table 22 , P-DRO is still able to learn
a robust model. We take this as evidence that the re-weighting
introduced in Section 22 helps stabilize training even when @xmath is
not a perfect model of the data.

##### 25.3 Influence of hyper-parameters on P-Dro

We study the influence of the 3 hyper-parameters @xmath (temperature),
@xmath (size of the renormalization window) and @xmath (learning rate of
the adversary) on the performance of P-DRO . All experiments are run on
the BiasedSST dataset, and the analysis proceeds as follows: we start
from configuration @xmath , @xmath and @xmath and vary each of the
hyper-parameters independently. We report two numbers for each
configuration: robust accuracy of the best model (1) using Greedy-Minmax
stopping and (2) using Oracle stopping. The latter is useful to
disentangle the effect of the stopping criterion.

As seen in the results shown in Table 23 , we find that @xmath has the
least effect on robust accuracies. While the renormalization window
parameter @xmath has some effect on optimal stopping, the best robust
accuracy achieved by the model (with oracle stopping) varies little. We
observe the adversary’s learning rate @xmath to be the most sensitive
hyper-parameter, which is why we restrict our grid-search to @xmath in
Section 24 .

#### 26 Conclusion

In this chapter, we have shown that there is promise in using parametric
families of neural generative models for defining the uncertainty set in
distributionally robust optimization. While we only perform experiments
on NLP tasks, this approach can, in theory, be applied in any modality.
However, the reliance of P-DRO on good quality generative models limits
its adoption in applications where good quality generative models are
unavailable, or when such model cannot produce exact densities
efficiently. In the following chapter, we will address these issues by
parameterizing the likelihood ratio @xmath directly, an alternative
formulation which poses different implementation challenges.

### Chapter \thechapter Distributionally Robust Models with Parametric
Likelihood Ratios

In chapter II we proposed P-DRO , a promising approach where the
uncertainty set is defined by a parametric family of generative models,
which allows for more flexibility in defining the uncertainty set.
Despite P-DRO ’s demonstrated ability to train models that are robust to
distributional shift, it suffers from several drawbacks: first, it
necessitates training an auxiliary generative model of the data. This
can be difficult for several reasons. First, this limits the
applicability of the method to domains with generative models that allow
for exact probability computations. Moreover, even when such generative
models are available, they are often more computationally demanding than
their discriminative counterparts. In language models for instance,
probabilities for sequences of text are obtained by iteratively
producing conditional probabilities over all tokens in the vocabulary.
This additional step results in considerable computational overhead
compared to discriminative models. Second, it is challenging to use in
practice due to its reliance on a number of hyperparameters and
approximations to the objective function.

In this chapter, we propose a new approach for DRO , called R-PDRO,
based on a key modification of the P-DRO algorithm: instead of modeling
the worst-case distributions directly, we parametrize the likelihood
ratio between the training distribution and the worst-case distribution.
This removes the dependency on an unwieldy generative model, making the
method useful for more applications. While likelihood-ratio formulations
of DRO have been tried in prior work (Sagawa et al., 2020 ) , we show
that they are particularly effective for parametric, neural-network
based adversaries. Our approach relies on two simple ideas: a mini-batch
level normalization strategy to enforce likelihood ratio constraints and
a KL divergence penalty to limit the scope of the uncertainty set.
R-PDRO consistently achieves equal or better robust sub-population
accuracy compared to P-DRO and other baselines on a variety of standard
benchmarks in image and text classification. In addition, we find it is
both faster than P-DRO and depends on fewer hyper-parameters. Additional
ablation and analysis experiments demonstrate that our minibatch
normalization strategy is necessary for high performance, and that the
parametric adversaries enabled by R-PDRO are substantially more
resistant to label noise compared to traditional non-parametric
approaches to DRO.

#### 27 Parametric Likelihood Ratio

##### 27.1 DRO as a Likelihood Ratio Optimization Problem

In the situation that all distributions in @xmath are absolutely
continuous with respect to @xmath (i.e. for all @xmath , @xmath only if
@xmath ) the inner maximum in Equation 3 can be rewritten purely as a
function of the likelihood ratio @xmath

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

Such absolute continuity assumptions are standard in @xmath -divergence
and group DRO methods, which both rely upon re-weighting the training
distributions. In fact, the KL divergence constraint in P-DRO
presupposes absolute continuity.

This suggests that the inner maximum can be re-written as an
optimization problem on functions @xmath within the uncertainty set
@xmath

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

This reparametrization of the problem will allow us to replace the
parametric family of generative models with a parametric family over
probability ratios.

##### 27.2 Ratio-based P-Dro

The likelihood-ratio formulation described above is appealing for P-DRO
because it enables the use of discriminative style neural architectures
for parameterizing the ratio @xmath , which opens up many more options
for defining the parametric uncertainty set. Specifically, we can set
the adversary to be any parametric function @xmath verifying @xmath .
The key insight that we use to realize our proposed method is that we do
not need to limit the choice of adversaries to those that implicitly
satisfy this normalization condition ( i.e. generative models). Instead,
we can pick any adversary and treat normalization as an additional
constraint (the “normalization constraint”).

Note that in this case, the KL constraint takes the simple form @xmath .
The final min-max problem, which we dub ratio-based P-DRO (R-PDRO), is
as follows:

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

As in P-DRO , we can look for equilibria of this differentiable min-max
game by performing simultaneous gradient updates (Singh et al., 2000 )
to @xmath and @xmath in directions @xmath and @xmath respectively.
Although finding global equilibria is not guaranteed in high dimensional
non-convex settings (Balduzzi et al., 2018 ) , empirical evidence from
Chapter II suggests that models trained in this manner still reach
useful solutions.

In experiments, we adopt an exponential parameterization @xmath where
@xmath is the output of any parametric model with values in @xmath .
Similarly to P-DRO , we do not explicitly enforce the KL constraint (due
to the difficulty of projecting onto the KL ball), and instead relax it
in the form of a KL term @xmath added to the loss function. The
regularization strength @xmath is treated as a hyper-parameter.

##### 27.3 Enforcing the normalization constraint

In addition to the KL constraint, R-PDRO necessitates that @xmath
satisfies a normalization constraint @xmath to ensure that @xmath is a
proper probability distribution over @xmath . If that were not the case,
the adversary @xmath could artificially increase the weighted
expectation @xmath by assigning a total weight greater than 1 to the
entire dataset.

Existing methods for ratio based DRO such as Sagawa et al. ( 2020 )
achieve this by either projecting @xmath onto the simplex @xmath after
each gradient update on @xmath , or by directly parametrizing the ratio
as @xmath . Unfortunately, these solutions are computationally
infeasible in practical scenarios with large datasets. Indeed, they
necessitate computing the entire expectation over @xmath , which can be
costly when each @xmath is the output of a neural model.

We propose two simple, yet effective solutions for addressing this issue
in the context of minibatch training where we can only compute @xmath
for small number of samples @xmath at each step of training.

###### Self-normalization

is inspired by the idea of “self-normalization” developed for globally
normalized structured prediction models (Andreas et al., 2015 ; Goyal
et al., 2019 ) . It consists in adding a relaxation of the normalization
constraint to the objective. Specifically, following Goyal et al. ( 2019
) we add a squared penalty on the log normalizer at the minibatch level.
Ignoring the KL penalty, this regularized objective takes the following
form:

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

The hyper-parameter @xmath controls the regularization strength.
Intuitively, this penalizes adversary that assign too much (or too
little) total weight to the minibatch. However, the choice of an optimal
@xmath adds an additional degree of freedom to R-PDRO, which suggests
our second option as a simpler alternative.

###### Batch-level normalization

consists of using the normalized ratio at the minibatch level by setting

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

for each sample @xmath in the minibatch. An obvious downside of this
approach is that the weight of each sample now depends on the minibatch
it was sampled from. This can be problematic for small batch sizes: as
an extreme example, for a batch size of 1, this normalization scheme
assigns the same weight of 1 to every sample, making the objective
equivalent to ERM.

However, minibatch approximations have proven effective for other forms
of DRO (Hu et al., 2018 ; Levy et al., 2020 ) and there is some evidence
that they can yield accurate estimates for higher batch sizes (Cortes
et al., 2010 ) . In practice we find that this approach yields good
results for commonly used batch sizes, is generally more stable than the
self-normalization penalty, and does not introduce any additional
hyper-parameters. In most of our experiments, we adopt this approach
unless specified otherwise. In that case, the R-PDRO objective on a
minibatch becomes

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

The KL term serves to penalize ratios that deviate too much from @xmath
. ⁵⁵ ⁵⁵ 55 Note that the penalty is subtracted because we are maximizing
with respect to @xmath The only hyper-parameter that needs to be tuned
is the KL regularization strength @xmath .

#### 28 Experiments

##### 28.1 Datasets

We perform experiments on four datasets: two text classification
datasets used in chapter II , Biased SST and FDCL18 and two image
classification datasets from Sagawa et al. ( 2020 ) , Waterbirds and
CelebA. Specific details for each dataset follow these previous works,
as described below:

###### Biased SST

is based on the SST-2 sentiment classification dataset (Radford et al.,
2018 ) , but modified to introduce spurious correlation between a
distractor token (“So,”) and positive labels in around 95% of the
dataset. In this setting models trained with ERM can very easily learn
this spurious correlation, which hurts performance on the small
sub-population that doesn’t suffer from this bias.

###### Fdcl18

A toxicity detection dataset of tweets labeled as hateful ( @xmath ),
abusive ( @xmath ), normal ( @xmath ) and spam ( @xmath ). The group-
DRO problem is formulated by partitioning the evaluation data along
labels as dialectal annotation obtained automatically with an off-the
shelf classifier (Blodgett et al., 2016 ; Sap et al., 2019 ) . In
particular these dialects align closely with self-reported race, and Sap
et al. ( 2019 ) found that machine learning models trained on such
toxicity detection datasets tend to exhibit bias towards certain labels
depending on the demographics of the tweets’ authors, particularly with
minorities. In order to report more reliable accuracy numbers, all
groups containing less than 100 samples are aggregated when computing
test accuracies.

###### Waterbirds

An image classification dataset where the task is to predict “land bird”
or “water bird” with the confounding factor of the background; most
water (resp. land) bird pictures have water (resp. land) on the
background.

###### CelebA

A popular face recognition dataset originally published by Liu et al. (
2015 ) . The group- DRO problem is formulated as a task of predicting
the hair color (“Blond” or “Dark”) across groups formed by the
combination of the label and the (binary) gender of the subject. Due to
the spurious correlation between blond hair/female and dark hair/male,
models trained with ERM tend to achieve lower accuracies on
underrepresented groups such as “blond-haired male” which totals only
0.85% of the training data.

##### 28.2 Experimental Setting

On BiasedSST and FDCL18 we follow the experimental setting of chapter II
and train a BiLSTM and BERT-base model Devlin et al. ( 2018 )
respectively. On the image classification datasets we train Resnet-50
architectures (He et al., 2016 ) pre-trained on ImageNet (Deng et al.,
2009 ) as in Sagawa et al. ( 2020 ) .

Since the adversary in R-PDRO can be any discriminative architecture, we
opt for the natural solution of using a similar architecture for this
model. For instance on Biased SST, we take @xmath as the raw logit
output by a BiLSTM architecture identical to that of the classifier
(without the final softmax layer). We do the same for the other
datasets, with the exception of FDCL18 where we use a smaller
DistillBERT model (Sanh et al., 2019 ) for efficiency. We use the same
learning rate and optimizer for both model and adversary and only vary
the KL penalty weight @xmath . We perform optimal stopping using the
Minmax criterion proposed in chapter II : every epoch @xmath , we
determine the best model by explicitly solving a greedy approximation of
the min-max game between all previously checkpointed adversaries and
models on the validation dataset @xmath .

  -- -------- -- ------
     @xmath      (29)
  -- -------- -- ------

A similar strategy is applied for hyper-parameter selection.
Importantly, we substitute the 0-1 loss for @xmath in Equation 29 as we
found in preliminary experiments on BiasedSST that it consistently
produced better results.

We compare our results to 5 different methods experimented with in
chapter II :

-    ERM : standard training to minimize the average loss.

-    NonParam : Non-parametric DRO with a KL constrained uncertainty set
    (Hu & Hong, 2013 ; Hu et al., 2018 ) . In this particular case, the
    inner maximization problem has an analytical solution of the form
    @xmath where @xmath is chosen so that the KL divergence with the
    training distribution is smaller than a pre-defined bound @xmath .
    As a result, examples with high losses are up-weighted.

-    Topic-DRO : a variation on Topic CVaR (Oren et al., 2019 ) using
    the online algorithm from Sagawa et al. ( 2020 ) to minimize the
    worst case loss on a collection of pseudo domains automatically
    generated via topic modeling. ⁵⁶ ⁵⁶ 56 This baseline was
    inaccurately referred to as “Topic CVaR” in chapter II We use this
    baseline for the text datasets only (Biased SST and FDCL18 ).

-    P-DRO : the parametric DRO approach proposed in chapter II . For
    image datasets, preliminary experiments using auto-regressive models
    for image modeling (Van Oord et al., 2016 ) proved to be
    prohibitively slow. Therefore, we only report P-DRO on text datasets
    as in chapter II .

-    Oracle DRO : an online algorithm for minimizing the worst-case loss
    on the true uncertainty set (Sagawa et al., 2020 ) . Contrary to all
    other methods, Oracle DRO presupposes that we know the groups of
    interest at training time. As such, it is not directly comparable
    and serves to provide an upper bound of the robust accuracy that can
    be achieved.

Across all experiments, we report results with mean and standard
deviation across 5 reruns with different seeds.

###### 28.2.1 Dataset-specific Hyper-parameters

All hyper-parameters listed below are constant across all methods:

###### Text Datasets

The input data is tokenized using the bert-base-uncased sub-word
tokenizer from Devlin et al. ( 2018 ) . We train both classifier and
adversary with Adam (Kingma & Ba, 2014 ) using a learning rate of @xmath
, linearly decay the learning rate to 0 at each step. We train with
batches of size 64 (or containing up to 2500 tokens, whichever is lower)
for 50 and 20 epochs for BiasedSST and FDCL18 respectively, evaluating
model on the validation data every epoch.

###### Image Datasets

On both datasets, images are rescaled to @xmath pixels and pixel values
are normalized to have mean 0 and variance 1 across all 3 color channels
on the training data. At training time, we augment the data by randomly
cropping or flipping the images horizontally. We train using regular
stochastic gradient descent using a constant learning rate of @xmath and
a batch size of 32. We train for 75 and 13 epochs on Waterbirds and
CelebA respectively (those numbers were chosen to match the number of
steps trained to Sagawa et al. ( 2020 ) despite the smaller batch size),
and validate every 100 (for Waterbirds) and 1000 (for CelebA) training
steps.

###### 28.2.2 Method Specific Hyper-parameters

For NonParam we follow the adaptation of Hu et al. ( 2018 ) used in
chapter II and choose the optimal temperature @xmath based on minibatch
level estimates of the KL divergence. We treat the KL bound @xmath as a
hyper-parameter. We adapt the Minmax stopping criterion of P-DRO to the
non-parametric adversaries as we found it yielded more robust models
than those selected with average validation accuracy. We sweep over
@xmath

For R-PDRO we perform optimal stopping using the Minmax criterion with a
KL threshold of @xmath in all experiments, to match the value
recommended for P-DRO . Specifically, we estimate the KL divergence of
checkpointed adversaries @xmath on the validation data as follows:

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

and reject adversaries for which this quantity exceeds @xmath .

##### 28.3 Results

For all methods, we report the worst accuracy over all groups in the
test set (the “robust accuracy”). Models that are robust to
sub-population shift will have higher robust accuracies. Since there is
often a trade-off between being robust to distribution shift and
performing well on the training distribution, we also report the
standard accuracy on the original test set (the “average accuracy”)

As shown in Table 24 , R-PDRO works particularly well on the two text
classification tasks, beating all baselines by a large margin on both
BiasedSST and FDCL18 . In fact, its robust accuracy almost matches that
of Oracle DRO on FDCL18 , despite the fact that the former doesn’t use
an any group information at training time. Compared to P-DRO , we find
that results are not only better, but also more consistent as evidenced
by the lower standard deviation.

Results are also generally good on image datasets (see Table 25 ). On
Waterbirds both the NonParam baseline and R-PDRO perform much better
than ERM, with a slight edge for R-PDRO (although the latter exhibits a
higher variance across reruns). On CelebA, R-PDRO largely outperforms
the baselines.

#### 29 Analysis and Ablations

We perform a series of analysis experiments and ablation studies to
better (1) identify how the parametric representation of the ratio
provides improvements over non-parametric alternatives and (2)
understand the importance of the various choices made with regards to
the renormalization scheme described in Section 27 . Throughout this
section, experiments are performed on the BiasedSST dataset.

##### 29.1 Why are Parametric Adversaries Better? The Case of Label
Noise

Our experimental results in Section 28 shows that parametric approaches
such as P-DRO and R-PDRO consistently outperform their non-parametric
counterparts. A possible explanation for this phenomenon is that for
non-parametric approaches, the optimal weights generally only depends on
the loss of the model. This can be problematic because the
non-parametric worst-case distribution will indiscriminately up-weight
noisy samples that have high loss. On the other hand, we hypothesize
that it is more difficult for the parametric adversary to “fit to the
noise” and that it tends to focus more on systematic patterns of
failures of the model.

To corroborate this hypothesis, we perform experiments by adding
increasing amounts of noise to the Biased SST. Specifically, for each
example in the training set we replace the original label with a random
label with probability @xmath . ⁵⁷ ⁵⁷ 57 @xmath corresponds to the
original dataset. We then train models on these increasingly noisy
datasets using both a parametric (R-PDRO) and non-parametric (NonParam)
approach. To simplify experiments we only run one hyper-parameter
configuration for each ( @xmath and @xmath for R-PDRO and NonParam
respectively) and report the test accuracies of the model with the
highest robust accuracy on the validation set. Results are averaged over
5 runs with different random seeds.

As showcased in Figure 6 , while R-PDRO’s robust accuracy seems to
suffer under the effect of noise, we find that its average accuracy is a
lot more resilient to the introduction of label noise compared to
NonParam, losing around @xmath points when @xmath ( @xmath ), versus
@xmath for NonParam ( @xmath ). This further supports our hypothesis
that non-parametric adversaries tend to fit to these noisy examples,
which decreases the overall quality of the resulting classifier.

##### 29.2 Optimization with Simultaneous Gradient Updates Plays a
Crucial Role

Despite the aforementionned results, it remains unclear why R-PDRO
learns re-weightings that are less sensitive to label noise or difficult
examples compared to R-PDRO. Indeed, since the non-parametric adversary
is the optimal solution of the inner minimization problem in Eq. 24 , it
stands to reason that (large, sometimes over-parameterized) parametric
adversaries from R-PDRO would converge to the same solutions as
NonParam.

Our hypothesis is that the simultaneous updates to both model and
adversary parameters prevent the parametric adversary from converging
towards such non-parametric solutions, and provides some implicit
regularization against up-weighting examples that are noisy or too
difficult.

To verify this hypothesis, we conduct a toy experiment where we allow
the adversary to take additional gradient steps in-between each update
to the classifier. At the limit, this would allow the adversary to find
an optimum of the inner maximization problem at each step of training
the classifier (which for large enough adversaries, might come close to
the non-parametric solution).

For computational efficiency, these experiments are performed on the toy
setting described in Chapter II , Section 23.7 : a linear model is
trained on a binary classification problem with two domains, one of
which is severely under-represented. For our adversary, we experiment
with a linear adversary, as well as larger multilayer perceptrons with
one hidden layer and 2 (MLP-2) and 4 (MLP-4) hidden units. In Figure 7 ,
we report the average robust accuracy (across 5 reruns) for classifiers
trained with R-PDRO when the adversary is allowed to take more steps
than the classifier.

We observe that R-PDRO’s robust accuracy suffers from giving the
adversary too much time to catch up with the classifier: as the number
of updates to the adversary increases, robust accuracy decreases. This
effect is amplified in larger adversaries ( e.g. MLP-4).

This experiment underlines the importance of the simultaneous gradient
updates, which prevent large, over-parameterized adversaries from
converging to the sub-optimal non-parametric solution.

##### 29.3 Batch-level Normalization vs Self-normalization

In Section 27.3 , we discussed two alternatives for enforcing the
normalization constraint on the ratios ( @xmath ): a
regularization-based approach (“self-normalization”) and batch level
renormalization. In Figure 7(a) , we show the effect of
self-normalization with different values of the regularization weight
@xmath for a fixed value of the KL penalty @xmath . We find that
batch-level normalization achieves a slightly better robust accuracy
than the best self-normalization configuration.

In chapter II , the Minmax stopping criterion was identified as a major
contributing factor to the performance of parametric DRO approaches. To
understand how it affects each normalization strategy, we perform the
same experiment as above but this time without the Minmax criterion,
selecting instead the model with the highest robust accuracy on the
validation set (“Oracle stopping”), which provides an upper bound to the
robust accuracy that can be achieved. Results in Figure 7(b) show that
although accuracies are generally closer, we again find that batch-level
normalization matches the best self-normalization penalty. This
indicates that batch-level normalization not only performs as well as
self-normalization (without the need for tuning the additional
hyper-parameter @xmath ), but also that it interacts better with the
Minmax stopping criterion, making it a preferable alternative overall.

##### 29.4 Effect of Batch Size on Re-normalization

As pointed out in Section 27.3 , a potential downside of the
minibatch-level normalization approach is that the effective weight of
each sample then depends on the other samples within the minibatch. For
example, consider an adversary that assigns high weight to only 5% of
the training data. With a small enough batch size, it is likely that
some batches may not contain any example of the high weight
subpopulation, in which case minibatch level renormalization will
overestimate the weight of the sample in the minibatch.

To assess the severity of this issue, we run R-PDRO on Biased SST with
@xmath and vary the batch size in @xmath . Each configuration is run 3
times, and we report average and standard deviation of the robust and
average test accuracies in Table 26 . Results suggest that while robust
accuracy indeed deteriorates for lower batch sizes (4 and 8), results
are consistently good for batch sizes upwards of 16, a reasonable number
considering that larger batch sizes are often preferred in the
literature (Popel & Bojar, 2018 ; Goyal et al., 2017 ) .

#### 30 Conclusion

In this chapter we have proposed a parametric, likelihood ratio based
approach to distributionally robust optimization of machine learning
models. With the proposed method, we can use any type of parametric
function estimator to define the uncertainty set of the DRO min-max
game. We showed that with a careful renormalization strategy, the
proposed method (R-PDRO) can be used to train robust models. It depends
on very few hyper-parameters and consistently performs well on a number
of benchmarks, making it an appealing off-the-shelf option. Finally we
have shown that such parametric approaches are more resilient to the
presence of noise in the training data when compared to their
non-parametric alternatives.

The main downside of R-PDRO is the computational overhead of jointly
training a second neural model. An interesting direction for future work
is to improve its efficiency through parallel computation or by sharing
parameters between the classifier and the adversary.

## Part III Adaptation

### Chapter \thechapter Regularizing Trajectories to Mitigate
Catastrophic Forgetting \chaptermark

Regularizing Trajectories

  It is good to have an end to journey toward; but it is the journey
  that matters, in the end.

  Ursula K. Le Guin

#### 31 Introduction

The methods presented in previous chapters are able to train models that
are more robust to distributional shifts because they can identify
sub-population of the training data corresponding to potential domains
where the model performs poorly. However, there are cases where the
model might be confronted with a completely unseen test distribution, or
even with a new task. In such cases where the test environment cannot be
anticipated, an alternative solution is to adapt the model, using the
limited amounts of data that are available in the target domain or task.

However, continual learning of multiple tasks is hampered by
catastrophic forgetting (McCloskey & Cohen, 1989 ; Ratcliff, 1990 ) ,
the tendency of previously acquired knowledge to be overwritten when
learning a new task.

Modern techniques to mitigate catastrophic forgetting can be roughly
categorized into 3 lines of work (see Parisi et al. ( 2019 ) for a
comprehensive overview): 1. regularization-based approaches, where
forgetting is mitigated by the addition of a penalty term in the
learning objective ( Kirkpatrick et al. ( 2017 ); Chaudhry et al. (
2018a ) , inter alia ), 2. dynamic architectures approaches, which
incrementally increase the model’s capacity to accomodate the new tasks
(Rusu et al., 2016 ) , and 3. memory-based approaches, which retain data
from learned tasks for later reuse (Lopez-Paz & Ranzato, 2017 ; Chaudhry
et al., 2018b , 2019 ) . Among these, regularization-based approaches
are particularly appealing because they do not increase the model size
and do not require access to past data. This is particularly relevant to
real-world scenarios where keeping data from previous training tasks may
be impractical because of infrastructural or privacy-related reasons.
Moreover, they are of independent intellectual interest because of their
biological inspiration rooted in the idea of synaptic consolidation
(Kirkpatrick et al., 2017 ) .

In these regularization-based approaches, a good regularizer will ensure
that, when learning a new task, gradient descent will ultimately
converge to parameters that yield good results on the new task while
preserving performance on previously learned tasks. Critically, this is
predicated upon successful optimization of the regularized objective, a
fact that has been largely taken for granted in previous work.
Non-convexity of the loss function, along with noise in the data (due to
small or biased datasets) or in the gradients (due to stochastic
gradient descent), can yield optimization trajectories — and ultimately
convergence points — that are highly non-deterministic, even for the
same starting parameters. As we demonstrate in this chapter, this can
cause unintended catastrophic forgetting along the optimization path.
This is illustrated in a toy setting in Figure 9 : a two parameter model
is trained to perform task @xmath (an arbitrary bi-modal loss function)
after having learned task @xmath (a logistic regression task). Standard
finetuning, even in the presence of a regularized objective (EWC;
Kirkpatrick et al. ( 2017 ) ), quickly changes the loss of @xmath and
tends to converge to a solution with high @xmath loss.

We propose to remedy this issue by regularizing not the objective
function but the optimization trajectory itself , specifically by
preconditioning gradient descent with the empirical Fisher information
of previously learned tasks (§ 33 ). This yields what we refer to as a
co-natural gradient, an update rule inspired by the natural gradient
(Amari, 1997 ) , but taking the Fisher information of previous tasks as
a natural Riemannian metric ⁵⁸ ⁵⁸ 58 Informally, the reader can think of
a Riemannian metric as a function that assigns an inner product @xmath
to each point @xmath in the space, thus inducing a localized notion of
distance and curvature. of the parameter space, instead of the Fisher
information of the task being optimized for. When we introduce our
proposed co-natural gradient for the toy example of Figure 9 , the
learning trajectory follows a path that changes the loss on @xmath much
more slowly, and tends to converges to the optimum that incurs the
lowest performance degradation on @xmath .

We test the validity of our approach in a continual learning scenario (§
34 ). We show that the co-natural gradient consistently reduces
forgetting in a variety of existing continual learning approaches by a
large factor, and greatly improves performance over simple finetuning,
without modification to the training objective. As an additional
contribution, we assemble a new collection of 13 tasks for evaluating
continual learning on text classification.

We further investigate the special case of transfer learning in a
two-task, low-resource scenario. In this specific case, control over the
optimization trajectory is particularly useful because the optimizer has
to rely on early stopping to prevent overfitting to the meager amount of
training data in the target task. We show that the co-natural gradient
yields the best trade-offs between source and target domain performance
over a variety of hyper-parameters (§ 36 ).

The work presented in this chapter bears many similarities to previous
related works on Bayesian formulations of continual learning which used
similar inverse-Fisher conditioning to reduce forgetting: NVCL (Tseran
et al., 2018 ) and GNG (Chen et al., 2018 ) . However, we our work makes
significant contributions over these previous approaches

First, these prior works were developed in the framework of Bayesian
continual learning. While the central idea of using the Fisher to
modulate the update is similar, we adopt a broader approach and
successfully apply this technique to a variety of other techniques
outside the Bayesian scaffolding. Second, we introduce a damping
coefficient, which allows control over the regularization effect of the
co-natural gradient, as evidenced by our analysis in Appendix A.4
specifically, and more generally by the results in Section 4 and 5,
where our results are obtained by performing grid-search over this
additional hyper-parameter. Third, we provide empirical evidence of the
co-natural gradients’ effectiveness in a variety of experimental
settings from relatively small, toy-like models and datasets ( e.g.
Split CIFAR or Omniglot) to more realistic scenarios ( e.g.
MiniImageNet, BERT on text classification and our MT experiments). In
comparison, the aforementioned previous work only test small 2 layers
MLP models on MNIST and FashionMNIST. We believe that this establishes
our work as a strong, independent contribution.

#### 32 Background and Notations

We first give a brief overview of the continual learning paradigm and
existing approaches for overcoming catastrophic forgetting.

##### 32.1 Notation

Let us define a task as a triplet containing an input space @xmath and
an output space @xmath , both measurable spaces, as well as a
distribution @xmath over @xmath . In general, learning a task will
consist of training a model to approximate the conditional distribution
@xmath induced by @xmath .

Consider a probabilistic model @xmath parametrized by @xmath where
@xmath is the size of the model, trained to perform a source task @xmath
to some level of performance, yielding parameters @xmath . In the most
simple instance of continual learning, we are tasked with learning a
second target task @xmath . In general in a multitask setting, it is not
the case that the input or output spaces are the same. The discrepancy
between input/output space can be addressed in various ways, e.g. by
adding a minimal number of task-specific parameters (for example,
different softmax layers for different label sets). To simplify
exposition, we set these more specific considerations aside for now, and
assume that @xmath and @xmath .

At any given point during training for task @xmath , our objective will
be to minimize the loss function @xmath – generally the expected
negative log-likelihood @xmath . Typically, this will be performed by
iteratively adding incremental update vectors @xmath to the parameters
@xmath .

##### 32.2 Existing Approaches for Continual Learning

In this chapter, we focus on those models that have a fixed architecture
over the course of continual learning. The study of continual learning
for models of fixed capacity can be split into two distinct (but often
overlapping) streams of work:

Regularization-based approaches introduce a penalty in the loss function
@xmath , often quadratic, pulling the weights @xmath back towards @xmath
:

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

where @xmath is a matrix, generally diagonal, that encodes the
respective importance of each parameter with respect to task @xmath ,
and @xmath is a regularization strength hyper-parameter. Various choices
have been proposed for @xmath ; the diagonal empirical Fisher
information matrix (Kirkpatrick et al., 2017 ) , or path-integral based
importance measures (Zenke et al., 2017 ; Chaudhry et al., 2018a ) .
More elaborate regularizers have been proposed based on e.g. a Bayesian
formulation of continual learning (Nguyen et al., 2017 ; Ahn et al.,
2019 ) or a distillation term (Li & Hoiem, 2016 ; Dhar et al., 2019 ) .
The main advantage of these approaches is that they do not rely on
having access to training data of previous tasks.

Memory-based approaches store data from previously seen tasks for re-use
in continual learning, either as a form of constraint, by e.g. ensuring
that training on the new task doesn’t increase the loss on previous
tasks (Lopez-Paz & Ranzato, 2017 ; Chaudhry et al., 2018b ) , or for
replay i.e. by retraining on instances from previous tasks (Rebuffi
et al., 2017 ; Chaudhry et al., 2019 ; Aljundi et al., 2019b , a ) .
Various techniques have been proposed for the selection of samples to
store in the memory (Chaudhry et al., 2019 ; Aljundi et al., 2019b ) or
for retrieval of the samples to be used for replay Aljundi et al. (
2019a ) .

All of these methods rely on stochastic gradient descent to optimize
their regularized objective or to perform experience replay, with the
notable exception of GEM (Lopez-Paz & Ranzato, 2017 ; Chaudhry et al.,
2018b ) , where the gradients are projected onto the orthogonal
complement of previous task’s gradients. However, this method has been
shown to perform poorly in comparison with simple replay (Chaudhry
et al., 2019 ) , and it still necessitates access to data from previous
tasks.

#### 33 Regularizing the Trajectory

After briefly recalling how the usual update is obtained in gradient
descent, we derive a new, co-natural update designed to better preserve
the distribution induced by the model over previous tasks.

##### 33.1 Warm up: the Standard Gradient Descent Update

At point @xmath in the parameter space, gradient descent finds the
optimal update @xmath that is (1) small and (2) locally minimizes the
difference in loss @xmath ( @xmath at the first order). Traditionally
this can be formulated as minimizing the Lagrangian:

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

with Lagrangian multiplier @xmath . Minimizing @xmath for @xmath yields
the well-known optimal update @xmath :

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

where @xmath corresponds to the learning rate (see Appendix Appendix for
the full derivation).

##### 33.2 KL Regularization of Trajectories

The @xmath term in @xmath implicitly expresses the underlying assumption
that the best measure of distance between parameters @xmath and @xmath
is the Euclidean distance. In a continual learning setting however, the
quantity we are most interested in preserving is the probability
distribution that @xmath models on the source task @xmath :

  -- -------- -- ------
     @xmath      (34)
  -- -------- -- ------

Therefore, a more natural distance between @xmath and @xmath is the
Kullback-Leibler divergence @xmath (Kullback & Leibler, 1951 ) . For
preventing catastrophic forgetting along the optimization path, we
incorporate incorporate this KL term into the Lagrangian @xmath itself:

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

Doing so means that the optimization trajectory will tend to follow the
direction that changes the distribution of the model the least. Notably,
this is not a function of the previous objective @xmath , so knowledge
of the original training objective is not necessary during continual
learning (which is typically the case in path-integral based
regularization methods (Zenke et al., 2017 ) or experience replay
(Chaudhry et al., 2019 ) ).

##### 33.3 Co-natural Gradient Optimization

Presuming that @xmath is small, we can perform a second order Taylor
approximation of the function @xmath around 0. Considering that both the
zeroeth and first order terms are null because @xmath is a global
minimizer of @xmath , this reduces the Lagrangian to a quadratic
optimization problem (we refer the reader to Pascanu & Bengio ( 2013 )
for a more detailed derivation.):

  -- -------- -- ------
     @xmath      (36)
  -- -------- -- ------

where @xmath is the Hessian of the KL divergence around @xmath . A
crucial, well-known property of this matrix is that it coincides with
the Fisher information matrix ⁵⁹ ⁵⁹ 59 Hence our use of the letter
@xmath to designate the Hessian @xmath (the expectation being taken over
the model’s distribution @xmath ; see Appendix Appendix for details).
This is appealing from a computational perspective because the Fisher
can be computed by means of first order derivatives only.

Minimizing for @xmath yields the following optimal update:

  -- -------- -- ------
     @xmath      (37)
  -- -------- -- ------

where coefficients @xmath and @xmath are folded into two
hyper-parameters: the learning rate @xmath and a damping coefficient
@xmath (the step-by-step derivation can be found in Appendix Appendix ).
In practice, especially with low damping coefficients, it is common to
obtain updates that are too large (typically when some parameters have
no effect on the KL divergence). To address this, we re-normalize @xmath
to have the same norm as the original gradient, @xmath . Moreover, to
improve numerical stability in the case of degenerate Fisher matrices,
we add a very small damping term of @xmath to all experiments.

For computational reasons, we will make 3 key practical approximations
to the Fisher:

1.  @xmath : we maintain the Fisher computed at @xmath , instead of
    recomputing @xmath at every step of training. This relieves us of
    the computational burden of updating the Fisher for every new value
    of @xmath . This approximation (shared by previous work, e.g.
    Kirkpatrick et al. ( 2017 ) ; Chaudhry et al. ( 2018a ) ) is only
    valid insofar as @xmath and @xmath are close. Empirically we observe
    that this still leads to good results.

2.  @xmath is diagonal: this is a common approximation in practice with
    two appealing properties. First, this makes it realistic to store
    the @xmath diagonal Fisher coefficients in memory. Second, this
    trivializes the inverse operation (simply invert the diagonal
    elements).

3.  Empirical Fisher: this common approximation replaces the expectation
    under the model’s distribution by the expected log-likelihood of the
    true distribution: @xmath (mind the subscript). This is particularly
    useful in tasks with a large or unbounded number of classes ( e.g.
    structured prediction), where summing over all possible outputs is
    intractable. We can then compute the diagonal of the empirical
    Fisher using Monte Carlo sampling: @xmath with @xmath sampled from
    @xmath (we use @xmath for all experiments).

This formulation bears many similarities with the natural gradient from
Amari ( 1997 ) , which also uses the KL divergence as a metric for
choosing the optimal update @xmath in gradient descent. There is a
however a crucial difference, both in execution and purpose: where the
natural gradient uses knowledge of the curvature of the KL divergence of
@xmath to speed-up convergence, our proposed method leverages the
curvature of the KL divergence on @xmath to slow-down divergence from
@xmath . To highlight the resemblance and complementarity between these
two concepts, we refer to the new update as the co-natural gradient.

##### 33.4 Beyond Two Tasks

In a continual learning scenario, we are confronted with a large number
of tasks @xmath presented in sequential order. When learning @xmath , we
can change the Lagrangian @xmath from 35 to incorporate the constraints
for all previous tasks @xmath :

  -- -------- -- ------
     @xmath      (38)
  -- -------- -- ------

This in turn changes the Fisher in Eq. 38 to @xmath . The choice of the
coefficients @xmath is crucial. Setting all @xmath to the same value,
i.e. assigning the same importance to all tasks is suboptimal for a few
reasons. First and foremost, it is unreasonable to expect of a model
with finite capacity to remember an unbounded number of tasks (as tasks
“fill-up” the model capacity, @xmath is likely to become more
“homogeneous”). Second, as training progresses and @xmath changes, our
approximation that @xmath is less and less likely to hold.

We address this issue in the same fashion as Schwarz et al. ( 2018 ) ,
by keeping a rolling exponential average of the Fisher matrices:

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

In this case, previous tasks are gracefully forgotten at an exponential
rate controlled by @xmath . We account for the damping @xmath term in
Eq. 37 by setting @xmath . In preliminary experiments, we have found
@xmath to yield consistently good results, and use this value in all
presented experiments.

#### 34 Continual Learning Experiment Setting

##### 34.1 Setup, Evaluation and Baselines

To examine our hypothesis that controlling the optimization trajectory
with the co-natural gradient reduces catastrophic forgetting, we follow
the experimental procedure from Chaudhry et al. ( 2019 ) : given a
collection of tasks, we create a “validation set” of 3 tasks used to
select the best hyper-parameters, and keep the remaining tasks for
evaluation. This split is chosen at random and kept the same across all
experiments. In most settings, the nature and possibly the number of
classes changes from task to task. We account for this by training a
separate “task head” for each task: an affine transform projecting the
features onto the number of classes, followed by a softmax layer. We
apply continual learning only to the remaining, “feature-extraction”
part of the model.

We report results using two common metrics for continual learning:
average accuracy , the accuracy at the end of training averaged over all
tasks, and forgetting . Forgetting is defined in Chaudhry et al. ( 2018a
) as the difference in performance on a task between the current model
and the best performing model on this task. Formally if @xmath
represents the accuracy on task @xmath at step @xmath of training, the
forgetting @xmath at step @xmath is defined as @xmath . Low forgetting
means that the model tend to keep the same level of performance on a
task it has previously learned.

We implement the proposed co-natural update rule on top of 3 baselines:

-    Finetuning : Simply train the model on the task at hand, without
    any form of regularization.

-    EWC : Proposed by Kirkpatrick et al. ( 2017 ) , it is a simple but
    effective quadratic regularization approach. While neither the most
    recent nor sophisticate regularization technique, it is a natural
    baseline for us to compare to in that it also consists in a
    Fisher-based penalty — albeit in the loss function instead of the
    optimization dynamics. We also use the rolling Fisher described in
    Section 33.4 , making our EWC baseline equivalent to the superior
    online EWC introduced by Schwarz et al. ( 2018 ) .

-    ER : Experience replay with a fixed sized episodic memory proposed
    by Chaudhry et al. ( 2019 ) . While not directly comparable to EWC
    in that it presupposes access to data from previous tasks, ER is a
    simple approach that boasts the best performances on a variety of
    benchmarks (Chaudhry et al., 2019 ) . In all experiments, we use a
    memory of size 1,000 populated with reservoir sampling.

Training proceeds as follows: we perform exhaustive search on all
possible hyper-parameter configurations using the validation tasks.
Every configuration is reran 5 times (3 for text classification) with a
different random seed and assigned a score based on the average task
score at the end of training (averaged over reruns). We then evaluate
the best hyper-parameters by continual training on the evaluation tasks.
Results are reported over 5 random restarts, and we control for
statistical significance using a paired t-test (we pair together runs
with the same task ordering). For all experiments we train with plain
stochastic gradient descent.

##### 34.2 Datasets

We use the four following task suites in our experiments:

###### 34.2.1 Split CIFAR

The CIFAR100 dataset, split into 20 independent 5-way classification
tasks with 2500 training examples and 500 test examples each. This
split, performed at random, is kept the same across all experiments,
only the order of these tasks is changed. Following Chaudhry et al. (
2018b ) , we use a smaller version of the ResNet architecture (He
et al., 2016 ) train on each task for 1 epoch with batch size 10.

###### 34.2.2 Omniglot

The Omniglot dataset (Lake et al., 2015 ) consists of 50 independent
character recognition datasets on different alphabet. We adopt the
setting of Schwarz et al. ( 2018 ) and consider each alphabet as a
separate task, and split each task such that every character is present
12, 4 and 4 times in the training, validation and test set respectively
(out of the 20 images for each character). ⁶⁰ ⁶⁰ 60 Note that this is a
different setting than the usual meta-learning scenario that Omniglot is
used for.

On this dataset we use the same small CNN architecture as Schwarz et al.
( 2018 ) . We augment the training data by randomly shifting and
rotating images and train on each task for 2500 steps (120 to 417 epochs
depending on the alphabet) with batch size 32. We ignore the validation
data and simply evaluate on the test set at the end of training.

###### 34.2.3 Split MiniImageNet

The MiniImageNet dataset (a subset of the popular ImageNet (Deng et al.,
2009 ) dataset ⁶¹ ⁶¹ 61 Similarly to Omniglot, MiniImageNet was
originally intended as a meta-learning benchmark and therefore its
standard train/validation/test split consists of disjoint classes. We
perform a custom transversal split so that the dataset can be used as a
standard 100-way classification task. The accuracies reported here are
not to be compared with the meta-learning literature. ; Vinyals et al. (
2016 ) ). We split the dataset into 20 disjoint 5-way classification
tasks, similarly to Split CIFAR, and use the same smaller ResNet. We
train on each task for 500 steps with batch size 32 ( @xmath epochs). We
ignore the validation data and simply evaluate on the test set at the
end of training.

###### 34.2.4 Text Classification

The most recent work on continual learning of language tasks
(de Masson d’Autume et al., 2019 ) relies on a relatively small set of
tasks (5 classification tasks from Zhang et al. ( 2015 ) ). This small
number of tasks is not amenable to our experimental setup described
above, where 3 tasks are reserved for validation. Therefore we assemble
a larger collection of text classification datasets from three sources:
the GLUE benchmark (Wang et al., 2019b ) , the SuperGLUE benchmark (Wang
et al., 2019a ) and the text classification datasets used in
de Masson d’Autume et al. ( 2019 ) . To keep things relatively simple,
we only keep tasks that 1. are single sentence or sentence pair
classification tasks and 2. have more than 1000 training examples.
Specifically, we use the following tasks from each dataset (reported
with the training data size):

-    GLUE

    -   CoLA (Warstadt et al., 2018 ) : 8.6K

    -   MultiNLI (Williams et al., 2018 ) : 392.7K

    -   MRPC (Dolan & Brockett, 2005 ) : 3.7K

    -   QNLI (Rajpurkar et al., 2016 ) : 108.4K

    -   QQP (Iyer et al., 2017 ) : 363.8K

    -   RTE (Dagan et al., 2005 ; Haim et al., 2006 ; Giampiccolo
        et al., 2007 ; Bentivogli et al., 2009 ) : 2.5K

    -   SST-2 (Socher et al., 2013 ) : 67.3K

-    SuperGLUE

    -   BoolQ (Clark et al., 2019 ) : 9.4K

-   de Masson d’Autume et al. ( 2019 )

    -   AG News (Gulli, 2005 ) (115.0K)

    -   Amazon Reviews Full (McAuley & Leskovec, 2013 ) (115.0K)

    -   DBPedia (Lehmann et al., 2015 ) (115.0K)

    -   Yahoo Answers (Zhang et al., 2015 ) (115.0K)

    -   Yelp Reviews Full (Yelp, 2015 ) (115.0K)

Note that the RTE dataset from SuperGLUE also satisfies our task type
and dataset size constraint, however according to Wang et al. ( 2019a )
it is an exact duplicate of GLUE’s RTE dataset, therefore we don’t
include it. de Masson d’Autume et al. ( 2019 ) performed some mild
preprocessing and subsampling of the datasets in Zhang et al. ( 2015 ) ,
however they did not release the final data. We perform a similar
preprocessing step: for datasets where each sample consists in several
pieces of text (typically review title and body in the Amazon and Yelp
datasets), we concatenate all into one utterance, joined with a period
and a space (“ . ”). We subsample the datasets to 115K training samples,
5K validation samples and 7.6K test samples.

We randomly select three out of these 13 tasks to serve as the
validation split upon which to perform hyper-parameter search, namely:
BoolQ , MRPC and SST-2 . Since test sets are not available for the GLUE
and SuperGLUE benchmark, we compute the final scores on the validation
datasets. Note that in all our experiments, validation data is not used
during training, therefore there is no leakage of the ultimate
evaluation dataset in the training procedure

Instructions to download and code to preprocess the data will be made
available at
https://github.com/pmichel31415/translate/tree/paul/pytorch_translate/research/adversarial/experiments
to facilitate reproduction of our results and future work on continual
learning for text classification. Following recent practice in text
classification, we use a large model that has already been pre-trained
in an unsupervised fashion (specifically bert-base-uncased ⁶² ⁶² 62 We
use the open-source implementation from Wolf et al. ( 2019 ) with the
default configuration. from Devlin et al. ( 2018 ) ), and fine-tune the
model on the supervised classification tasks. We train on each task for
1000 steps with batch size 16 (from 7 to @xmath epochs due to the large
variance in dataset sizes).

##### 34.3 Grid-search Parameters

For each all image-related tasks, we perform grid-search over the
following parameter values:

-   Learning rate (all methods): 0.1, 0.03, 0.01

-   EWC regularization strength (EWC, Co-natural EWC): 0.5, 1, 5

-   Fisher damping coefficient (Co-natural finetuning, Co-natural EWC,
    Co-natural ER): 0, 1, 0.1

For text classification with BERT specifically, preliminary experiments
showed that all methods benefitted from lower learning rate as well as
lower regularization (likely due to the fact that optimization is
starting from a pretrained model). We therefore use the following
values:

-   Learning rate (all methods): 0.05, 0.01, 0.005

-   EWC regularization strength (EWC, Co-natural EWC): 0.5, 0.1, 0.05

-   Fisher damping coefficient (Co-natural finetuning, Co-natural EWC,
    Co-natural ER): 1, 10, 100

For ER, we simply set the replay batch size to the same value as
standard training (10 and 32 for Split CIFAR and Omniglot respectively).
Note that whenever applicable, we re-normalize the diagonal Fisher so
that the sum of its weights is equal to the number of parameters in the
model. This is so that the hyper-parameter choice is less dependent on
the size of the model. In particular this means that the magnitude of
each diagonal element is much bigger, which is why we do grid-search
over smaller regularization parameters for EWC than is common in the
literature.

#### 35 Continual Learning Results

##### 35.1 The Co-natural Gradient Helps Prevent Forgetting

The upper half of Tables 27(a) , 27(b) , 27(c) and 27(d) reports the
average accuracy of all the tasks at the end of training (higher is
better). We observe that the co-natural gradient always improves greatly
over simple finetuning, and occasionally over EWC and ER. We note that
simple co-natural finetuning (without any other form of regularization)
sometimes matches or exceeds the performance of EWC even though it
requires strictly fewer resources (there is no need to store the
previous parameters as in EWC, or data in ER).

Even more appreciable is the effect of the co-natural trajectories on
forgetting, as shown in the lower half of Table 27 . As evidenced by the
results in the lowest rows, using the co-natural gradient on top of
finetuning and ER systematically results in large drops in forgetting
across all datasets. With EWC, the conclusion is more nuanced. While the
co-natural gradient never increases forgetting, the effect is not always
statistically significant.

To get a qualitative assessment of the learning trajectories that yield
such results, we visualize the accuracy curves of 10 out of the 47
evaluation tasks of Omniglot in Figure 10 . We observe that previous
approaches do poorly at keeping stable levels of performance over a long
period of time (especially for tasks learned early in training), a
problem that is largely alleviated by the co-natural preconditioning.
This seems to come at the cost of more intransigence (Chaudhry et al.,
2018a ) , i.e. some of the later tasks are not being learnt properly. In
models of fixed capacity, there is a natural trade-off between
intransigence and forgetting (see also the “stability-plasticity”
dilemma in neuroscience Grossberg ( 1982 ) ). Our results position the
co-natural gradient as a strong low-forgetting/moderate intransigence
basis for future work.

##### 35.2 Sensitivity to Damping

The main hyper-parameter for the co-natural gradient is the damping
parameter @xmath from Eq. 37 . In our previous experiments, the value of
@xmath is chosen according to grid search on a small number of tasks.
While this is a realistic setting for practical applications, in this
section we perform a smaller, targeted experiment to examine the effect
of @xmath on catastrophic forgetting.

We focus on the 17 evaluation tasks of the Split CIFAR dataset and set
the learning rate to 0.1. We observe how much the model “forgets” about
the first task it has observed after training on all others.
Specifically, we train on the first task, compute its Fisher @xmath ,
and then proceed to train on the 16 remaining tasks with the co-natural
gradient using @xmath (in particular we do not regularize for the other
tasks). We observe how the value of alpha affects the final forgetting
of the first task at the end of training, as well as the model’s ability
to learn new tasks (sometimes referred to as “intransigence” in the
literature Chaudhry et al. ( 2018a ) ). As a measure of the latter, we
report the maximum accuracy achieved by the model averaged over the 16
remaining tasks.

We evaluate values of @xmath from @xmath to @xmath (following a
geometric progression), as well as the two extremal values @xmath
(‘‘pure’’ co-natural gradient ⁶³ ⁶³ 63 As mentioned in Section 33 , we
do actually add a small damping term @xmath to all experiments. ) and
@xmath (simple finetuning). All results are averaged over 10 random
restarts (with different model initialization and task order).

We observe in Figure 10(a) that forgetting monotonically increases with
the damping coefficient. Similarly, Figure 10(b) shows how increasing
@xmath results in lower intransigence. While these two observations are
expected, it is interesting to note the existence of a “sweet spot”
around @xmath where damped co-natural gradient is significantly less
intransigent than the un-damped co-natural gradient while simultaneously
not being significantly less robust to forgetting (all hypotheses are
tested for with @xmath ).

#### 36 Low-Resource Adaptation Experiments

In this section we take a closer look at the specific case of adapting a
model from a single task to another, when we only have access to a
minimal amount of data in the target task. In this case, controlling the
learning trajectory is particularly important because the model is being
trained on an unreliable sample of the true distribution of the target
task, and we have to rely on early-stopping to prevent overfitting. We
show that using the co-natural gradient during adaptation helps both at
preserving source task performance and reach higher overall target task
performance.

##### 36.1 Experimental Setting

We perform experiments on two different scenarios:

###### Image classification

We take MiniImagenet as a source task and CUB (a 200-way birds species
classification dataset; Welinder et al. ( 2010 ) ) as a target task. To
guarantee a strong base model despite the small size of MiniImageNet, we
start off from a ResNet18 model (He et al., 2016 ) pretrained on the
full ImageNet, which we retrofit to MiniImageNet by replacing the last
fully connected layer with a separate linear layer regressed over the
MiniImageNet training data. To simulate a low-resource setting, we
sub-sample the CUB training set to 200 images ( @xmath per class).
Scores for these tasks are reported in terms of accuracy.

###### Machine translation

We consider adaptation of an English to French model trained on WMT15 (a
dataset of parallel sentences crawled from parliamentary proceedings,
news commentary and web page crawls; Bojar et al. ( 2015 ) ) to MTNT (a
dataset of Reddit comments; Michel & Neubig ( 2018a ) ). Our model is a
Transformer (Vaswani et al., 2017 ) pretrained on WMT15. Similarly to
CUB, we simulate a low-resource setting by taking a sub-sample of 1000
sentence pairs as a training set. Scores for these two datasets are
reported in terms of BLEU score (Papineni et al., 2002 ) . ⁶⁴ ⁶⁴ 64 We
use sacrebleu (Post, 2018a ) with -tok intl as recommended by Michel &
Neubig ( 2018a ) .

Here we do not allow any access to data in the source task when training
on the target task. We compare four methods Finetuning (our baseline),
Co-natural finetuning , EWC (which has been proven effective for domain
adaptation, see Thompson et al. ( 2019 ) ) and Co-natural EWC .

Given that different methods might lead to different trade-offs between
source and target task performance, with some variation depending on the
hyper-parameters ( e.g. learning rate, regularization strength…), we
take inspiration from Thompson et al. ( 2019 ) and graphically report
results for all hyper-parameter configuration of each method on the 2
dimensional space defined by the score on source and target tasks ⁶⁵ ⁶⁵
65 For CUB in particular we report the average accuracy of every
configuration over 5 runs, each with a different 200-sized random subset
of the data. . Additionally, we highlight the Pareto frontier of each
method i.e. the set of configurations that are not strictly worse than
any other configuration for the same model.

##### 36.2 Results

The adaptation results for both scenarios are reported in Figure 12 . We
find that in both cases, the co-natural gradient not only helps
preserving the source task performance, but to some extent it also
allows the model to reach better performance on the target task as well.
We take this to corroborate our starting hypothesis: while introducing a
regularizer does help, controlling the optimization dynamics actively
helps counteract overfitting to the very small amount of training data,
because the co-natural pre-conditioning makes it harder for stochastic
gradient descent to push the model towards directions that would also
hurt the source task.

#### 37 Conclusion

We have presented the co-natural gradient, a technique that regularizes
the optimization trajectory of models trained in a continual setting. We
have shown that the co-natural gradient stands on its own as an
efficient approach for overcoming catastrophic forgetting, and that it
effectively complements and stabilizes other existing techniques at a
minimal cost. We believe that the co-natural gradient — and more
generally, trajectory regularization — can serve as a solid bedrock for
building agents that learn without forgetting.

### Chapter \thechapter Conclusion

In this thesis, we have explored ways to mitigate the difficulty of
learning neural models for natural language processing when confronted
with changes in the data distribution. We presented an overview of the
problem and the current state of the literature, and made contributions
along three core axes: evaluation, robustness and adaptation.

Because benchmarks and evaluation metrics are our compass towards
successfully addressing distributional shift, in Part I we first
developed a new dataset ( MTNT , Chapter I ) for evaluating the ability
of machine translation models (usually trained on news articles or
parliamentary proceedings) to handle a shift to the social media domain.
This confirmed that common neural architectures suffer significant
performance drops when confronted with text data from social media
sources. In Chapter I , we tackled the distinct, but equally important
issue of evaluating adversarial perturbation on textual inputs, with a
focus on sequence-to-sequence models. Specifically, we proposed an
evaluation framework which we validated against human evaluations, and
showed its benefits for the purpose of carrying out effective
adversarial training strategies. Collecting datasets and designing
evaluation metrics is a time-consuming, often thankless and ultimately
sisyphean endeavour: new models inevitably “overfit” to the available
benchmarks on which they are evaluated, which necessitates creating new
datasets, new metrics etc …But as our work illustrates, every
contribution helps: for example, our MTNT dataset helped assess the
robustness of modern MT models, and was used as the basis for two
subsequent robust machine translation shared task, helping to drive
innovation in that area.

In Part II we focused on the central problem of training models that are
robust to distributional shifts. Our approach took inspiration from
existing work in distributionally robust optimization , wherein models
are trained to minimize their expected risk against the most difficult
data distribution within a pre-determined family (the “uncertainty
set”). In our work we confronted the difficulty of defining this
uncertainty set appropriately for practical applications. Our approach
(called Parametric DRO ) revolved around modeling the uncertainty set as
a parametric family of generative models (Chapter II ) or likelihood
ratios (Chapter II ) to strike a better balance between the extreme
pessimism or the limited expressivity of existing alternatives in the
literature. In P-DRO , a robust classifier learns to minimize its
expected loss under a parametric adversary which is itself trained to
model the sub-population on which the classifier performs the worst. We
empirically demonstrated that both variants could be used to derive
models that were significantly more robust to sub-population shifts
compared with relevant baselines. While the work presented in these two
chapters is more recent, we hope that it can serve as a strong
foundation towards more distributionally robust models in NLP .

Finally, due to the inherent limitations of our DRO approach, which
anticipates distributional shifts using only a static dataset, in Part
III we turned to the problem of adapting existing models to new data
distributions or tasks. We used ideas from information geometry to
derive a gradient update rule (dubbed the “co-natural gradient”) for
mitigating the catastrophic forgetting effect, where adapting neural
models to new tasks or domains causes significant drops in performance
in previously learned tasks (Chapter III ). The effectiveness of this
method was validated on a diverse selection of continual-learning
benchmarks, as well as a machine translation domain adaptation problem
using our previously proposed MTNT dataset as a test bed.

Beyond the conclusions directly derived from our contributions described
above, we identify two higher level takeaways from our time pursuing
research on the specific topic of distributional shift. The first is
that proper evaluation is of paramount importance . This idea has
permeated every chapter of this thesis. Of course, this was the central
motivation for the entirety of Part I , but we have also made a
conscious effort to conduct fair and consistent evaluation in chapters
II , II and III . This is particularly important in the specialised
sub-areas that are distributionally robust optimization and continual
learning, because due to their cross-sectional nature, they tend to
attract practitioners with diverse research backgrounds. This often
results in inconsistent evaluation standards, which can can hamper
progress (Gulrajani & Lopez-Paz, 2020 ; Farquhar & Gal, 2018 ) .

Our second key takeaway is that, for practical applications of machine
learning (such as NLP ), research progress should be guided by, but not
limited to what is predicted by theory . This is especially relevant in
the context of deep learning, where theory lags behind the state of the
art (Nagarajan & Kolter, 2019 ) ⁶⁶ ⁶⁶ 66 And of course, advancing theory
is a worthwhile and crucial pursuit in and of itself. . In our work on
Parametric DRO in Part II , we experimented with highly
non-convex-concave differentiable games which were not guaranteed to
even converge. Furthermore, we used a number of (sometimes extreme)
approximations to make our methods useable in practical NLP scenarios.
Despite this, we found that they outperformed more principled, but
perhaps less flexible baselines from the non-parametric DRO literature.
This is not to say that research should proceed by blindly throwing
ideas on the metaphorical wall and “seeing what sticks” (an inefficient,
not to mention computationally unsustainable process). But there is a
balance to strike, and we can achieve significant progress by taking
inspiration from more conservative lines of work and porting them over
to more applied research areas, without being restrained by mathematical
rigour.

##### Future Work

During our investigations throughout the course of producing the work
described in this thesis, we have come to identify open questions which
we believe would provide fertile ground for future research. In the next
few paragraphs, we elaborate on these possible future directions towards
close the gap between human and machine learning in NLP in the face of
distributional shift

In our own work, we have mostly been concerned with “interpolative”
versions of distributional robustness, wherein we assume that our
training data contains enough diversity for us to simulate potential
distributional shifts. This setting has its uses, as demonstrated by our
positive results in a variety of realistic benchmarks in Part 3 . But
there is value in considering a more difficult scenario where we might
have to extrapolate and guess at what unseen domains might look like.
While there is some existing work in generating difficult or
out-of-domain examples in computer vision (Yue et al., 2019 ; Zhou
et al., 2020 ) , this remains an under-explored area of the literature,
especially when it comes to natural language processing. Leveraging the
generative capabilities of strong parametric adversaries trained with
P-DRO to hallucinate unseen, yet difficult examples could be a viable
path to progress in this challenging setting.

With respect to the adaptation aspect, we believe that it is time for
the community to move beyond the episodic setting commonly considered in
continual learning and start tackling the “streaming” setting where the
test (or training!) distribution is liable to change continuously over
time. This scenario is more representative of how we as humans interact
with our non-stationary environment, and it poses exciting new
challenges, not only terms of the learning algorithms needed in such
environments, but also with regards to how these same algorithms should
be evaluated. How do “episodic” continual learning algorithm such as the
co-natural gradient (Chapter III ) fare against continuous distribution
shift? And can the variety of data distributions observed over time in
turn guide the construction of more accurate uncertainty sets in DRO ?
These are only some of the many open questions in this so far relatively
uncharted research area. While there has been preliminary work on this
general topic very recently (Lazaridou et al., 2021 ; Hombaiah et al.,
2021 ) , there is still a long road ahead, as it were.

Finally, as large models pre-trained with a language modeling objective
have become ever more prominent in NLP research over the past few years
(Radford et al., 2019 ; Dai et al., 2019 ; Raffel et al., 2019 ) , the
distinction between domain and task has become blurrier. For example, in
Radford et al. ( 2019 ) and Raffel et al. ( 2019 ) , multiple tasks are
reformulated as a single, unified language modeling or question
answering task. We hypothesize that this trend is only going to increase
because, as its name indicates, natural language is a natural
communication protocol for us to interact with and query our models.
With this in mind, we posit that creating models that are both robust
and able to adapt to shift in the data distribution is a critical
requirement for developing artificial agents that are capable of
tackling a variety of tasks in diverse environments. We hope that the
work presented in this thesis towards training models that are better
equipped to deal with distributional shift provides a useful step in
this direction.
