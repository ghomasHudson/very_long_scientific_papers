# Chapter 1 Introduction

The study of quantum many-body phenomena is of pivotal interest in
modern physics. Important areas of research, such as the
characterization of exotic phases of quantum matter and of quantum phase
transitions (Sachdev, 1999 ) , or even the possible realization (Kitaev,
2003 ; Nayak et al., 2008 ) of a quantum computer, rely on our
understanding of collective phenomena in quantum many-body systems.
Theoretical progress in these research areas is hindered both by a
general lack of analytical results as well as the inadequacy of
perturbation theory in the strongly interacting regime where the
interesting physics often lies. As a result, development of numerical
approaches to probe such systems has become a flourishing research
industry. However, numerical methods are limited by staggering
computational costs.

The number of parameters required to describe a generic quantum
many-body wavefunction on a lattice grows exponentially with the number
of sites in the lattice. An immediate consequence is that exact
diagonalization can only be applied to small systems. In particular,
thermodynamic properties often remain inaccessible by this method.
Conventional alternatives include Quantum Monte Carlo sampling
techniques. These are well established numerical methods that have been
used extensively in several areas of Mathematics and Physics. Of
interest here is that these techniques have been applied (Prokofev
et al., 1998 ; Evertz, 2003 ; Syljuasen and Sandvik, 2002 ; Sandvik,
2005 ) successfully to several quantum lattice models. On the other
hand, Quantum Monte Carlo techniques suffer from the notorious sign
problem (Loh and Gubernatis, 1990 ; Henelius and Sandvik, 2000 ) that
hinders their application to certain systems of immense interest.
Notable examples include systems of frustrated magnets and of
interacting fermions that are relevant in the context of quantum
magnetism and high temperature superconductivity (Anderson, 1987 ) .

In recent years, new approaches based on tensor networks have caught
widespread attention. Such approaches can be regarded as generalizations
of the density matrix renormalization group (DMRG) method (White, 1992 ,
1993 ; Schollwock, 2005a ; McCulloch, 2008 ) , which is highly
successful for one-dimensional systems. The potential of tensor network
algorithms relies on the fact that, as DMRG, they can address systems of
frustrated spins and interacting fermions but, unlike DMRG, they can
also be applied to two dimensional systems, both of large size and of
infinite size. The main impediment of such methods comes from the fact
that simulation costs increase rapidly with the amount of entanglement
in the system. Consequently, tensor networks can only represent states
with a limited amount of entanglement. On the other hand, exploitation
of symmetries has remained largely unexplored for a broad class of
tensor networks algorithms.

Symmetries, of fundamental importance in physics, require special
treatment in numerical studies. Unless explicitly preserved at the
algorithmic level, they are bound to be destroyed by the accumulation of
small errors, in which case significant features of the system might be
concealed. On the other hand, when properly handled, the presence of a
symmetry can be exploited to reduce simulation costs.

The goal of this thesis is to extend the tensor network formalism to the
presence of symmetries. We develop a generic framework that can be
applied to adapt any given tensor network representation and algorithm
to both numerically protect symmetries and exploit them for
computational gain.

### 1.1 Tensor network states and algorithms

Tensor networks are an efficient parameterization of low energy quantum
many-body states of lattice models. The degrees of freedom of the model
are arranged on a lattice @xmath made of @xmath sites where each site is
described by a Hilbert space of dimension @xmath . As a result, the
Hilbert space dimension of @xmath grows exponentially with the number of
sites @xmath . Thus, a generic quantum many-body state on the lattice is
parameterized by exponentially many parameters. On the other hand, the
dynamics of the system are typically governed by a local Hamiltonian
@xmath , that is, @xmath decomposes as the sum of terms involving only a
small number of sites, and whose strength decays with the distance
between the sites. The locality of the dynamics often implies that only
a relatively small amount of entanglement is present in the ground
state. In such circumstances, tensor networks offer a good description
of the ground state. Moreover, the description is efficient, in that the
total number of parameters encoded into the tensor networks grows
roughly linearly with @xmath .

Examples of tensor network states for one dimensional systems include
the matrix product state (Fannes et al., 1992 ; Ostlund and Rommer, 1995
; Perez-Garcia et al., 2007 ) (MPS), which results naturally from both
Wilson’s numerical renormalization group (Wilson, 1975 ) and White’s
DMRG and is also used as a basis for simulation of time evolution, e.g.
with the time evolving block decimation (TEBD) (Vidal, 2003 , 2004 ,
2007a ) algorithm and variations thereof, often collectively referred to
as time-dependent DMRG (Vidal, 2003 , 2004 ; Daley et al., 2004 ; White
and Feiguin, 2004 ; Schollwock, 2005b ; Vidal, 2007a ) ; the tree tensor
network (Shi et al., 2006 ) (TTN), which follows from coarse-graining
schemes where the spins are blocked hierarchically; and the multi-scale
entanglement renormalization ansatz (Vidal, 2007b , 2008 ; Evenbly and
Vidal, 2009a ; Giovannetti et al., 2008 ; Pfeifer et al., 2009 ; Vidal,
2010 ) (MERA), which results from a renormalization group procedure
known as entanglement renormalization (Vidal, 2007b , 2010 ) . For two
dimensional lattices there are generalizations of these three tensor
network states, namely projected entangled pair states (Verstraete and
Cirac, 2004 ; Sierra and Martin-Delgado, 1998 ; Nishino and Okunishi,
1998 ; Nishio et al., 2004 ; Murg et al., 2007 ; Jordan et al., 2008 ;
Gu et al., 2008 ; Jiang et al., 2008 ; Xie et al., 2009 ; Murg et al.,
2009 ) (PEPS), 2D TTN, (Tagliacozzo et al., 2009 ; Murg et al., 2010 )
and 2D MERA (Evenbly and Vidal, 2010a , b ; Aguado and Vidal, 2008 ;
Cincio et al., 2008 ; Evenbly and Vidal, 2009b ; Konig et al., 2009 )
respectively. As variational ansätze, PEPS and 2D MERA are particularly
interesting since they can be used to address large two-dimensional
lattices, including systems of frustrated spins (Murg et al., 2009 ;
Evenbly and Vidal, 2010a ) and interacting fermions, (Corboz and Vidal,
2009 ; Kraus et al., 2010 ; Pineda et al., 2010 ; Corboz et al., 2010a ;
Barthel et al., 2009 ; Shi et al., 2009 ; Corboz et al., 2010b ; Pizorn
and Verstraete, 2010 ; Gu et al., 2010 ) where Monte Carlo techniques
fail due to the sign problem.

Some popular tensor networks are summarized in table 1.1 .

### 1.2 Symmetries

The presence of symmetries is a universal trait of physical theories.
Symmetry has become one of the most powerful tools of theoretical
physics, as it has become evident that practically all laws of nature
originate in symmetries. The importance of symmetries in physical
theories was firmly grounded by the famous Noether’s theorem , a
rigorous result that links the presence of a symmetry to the
conservation of a physical quantity.

In this thesis we will be concerned with symmetries exhibited in quantum
lattice models. The many-body Hamiltonian @xmath may be invariant under
certain transformations, which form a group @xmath of symmetries
(Cornwell, 1997 ) . Under the action of the symmetry transformation, the
Hilbert space of the theory is divided into symmetry sectors labeled by
quantum numbers or conserved charges. The symmetry group @xmath may be
Abelian or non-Abelian , depending on whether or not the total effect of
applying two symmetry transformations depends on the order in which the
transformations are applied. The symmetry sectors associated with an
Abelian symmetry correspond to one-dimensional invariant subspaces. In
contrast, the dimension of the symmetry sectors associated with a
non-Abelian symmetry may be larger than one.

On the lattice, one can distinguish between space symmetries, which
correspond to some permutation of the sites of the lattice, and internal
symmetries, which act on the vector space of each site. An example of
space symmetry is invariance under translations by some unit cell, which
leads to conservation of quasi-momentum. An example of internal symmetry
is SU(2) invariance, e.g. spin isotropy in a quantum spin model. An
internal symmetry can in turn be global , if it transforms the space of
each of the lattice sites according to the same transformation (e.g. a
spin independent rotation); or local , if each lattice site is
transformed according to a different transformation (e.g. a
spin-dependent rotation), as it is in the case of lattice gauge models.
A global internal SU(2) symmetry gives rise to conservation of total
spin. Table 1.2 lists some examples of Abelian and non-Abelian physical
symmetries.

By targeting a specific symmetry sector during a calculation,
computational costs can often be significantly reduced while explicitly
preserving the symmetry. It is therefore not surprising that symmetries
play an important role in numerical approaches.

### 1.3 Incorporating symmetries into tensor network algorithms

Exploiting symmetries has been of great interest in numerical
approaches, since it allows selection of a specific charge sector within
the kinematic Hilbert space, and leads to significant reduction of
computational costs.

In the context of tensor network algorithms, benefits of exploiting the
symmetry have been extensively demonstrated especially in the context of
MPS. Both space and internal symmetries, Abelian and non-Abelian, have
been thoroughly incorporated into DMRG code and have been exploited to
obtain computational gains (Ostlund and Rommer, 1995 ; White, 1992 ;
Schollwock, 2005b ; Ramasesha et al., 1996 ; Sierra and Nishino, 1997 ;
Tatsuaki, 2000 ; McCulloch and Gulacsi, 2002 ; Bergkvist et al., 2006 ;
Pittel and Sandulescu, 2006 ; McCulloch, 2007 ; Perez-Garcia et al.,
2008 ; Sanz et al., 2009 ) .

Symmetries have also been used in more recent proposals to simulate time
evolution with MPS (Vidal, 2004 ; Daley et al., 2004 ; White and
Feiguin, 2004 ; Schollwock, 2005b ; Vidal, 2007a ; Daley et al., 2005 ;
Danshita et al., 2007 ; Muth et al., 2010 ; Mishmash et al., 2009 ;
Singh et al., 2010a ; Cai et al., 2010 ) .

Figure 1.1 is demonstrative of the colossal computational gain that has
been obtained by exploiting the symmetry in the context of the MPS. (In
Fig. 6.14 we show an analogous comparison for exploiting symmetries in
the context of the MERA.)

However, when considering symmetries, it is important to notice that an
MPS is a trivalent tensor network. That is, in an MPS each tensor has at
most three indices. The Clebsch-Gordan coefficients (Cornwell, 1997 )
(or coupling coefficients) of a symmetry group are also trivalent, and
this makes incorporating the symmetry into an MPS by considering
symmetric tensors particularly simple. In contrast, tensor network
states with a more elaborate network of tensors, such as MERA or PEPS,
consist of tensors having a larger number of indices. In this case a
more general formalism is required in order to exploit the symmetry.

In this thesis we will describe how to incorporate a global internal
symmetry, given by a compact and reducible group @xmath , into tensor
network algorithms. We will develop a generic strategy that is
independent of the details of the underlying tensor network. We will do
this by imposing the symmetry constraints at the level of individual
tensors that constitute the tensor network. We will then also describe
how symmetric tensors are manipulated such that the symmetry is both
preserved and exploited for computational gain. Having built a framework
of symmetric tensors, we will adapt an arbitrary tensor network to the
presence of symmetry by using symmetric tensors as building blocks for
the tensor network. The resulting tensor network represents a class of
quantum many-body wavefunctions that are invariant (or more generally
covariant) under the symmetry transformation. Algorithms based on such
symmetric tensor networks will also be adapted to the presence of
symmetry. This will be achieved by expressing each step of an algorithm
in terms of symmetric manipulations of the tensors.

As a concrete illustration, we will extensively describe the
implementation of U(1) and SU(2) symmetries into the MPS and MERA. With
these implementations at hand, we will demonstrate the colossal benefits
of incorporating the symmetry into tensor network algorithms. These
include addressing specific symmetry sectors of the Hilbert space,
compactification of the tensor network representation and computational
speedup in numerical simulations. For example, in a lattice spin model
endowed with spin isotropy the ground state is constrained to the spin
zero or singlet sector of the Hilbert space. Therefore, in a numerical
probing for the state, it is sufficient to restrict attention to the
singlet subspace. This can, in turn, potentially result in a substantial
reduction of computational costs.

### 1.4 Plan of the thesis

This thesis is comprised of three published papers corresponding to
chapters 2,3 and 4 and additional chapters 5 and 6. The material in
chapters 5 and 6 was under preparation for publication at the time of
submitting this thesis. The following is a brief summary of all the
chapters.

In Chapter 2 we dive straight into the core of the problem. We describe
the implementation of a non-Abelian symmetry for the case of the
simplest tensor network: the MPS. This will serve to illustrate the key
points that are required to be considered when implementing symmetries
into tensor network algorithms. In addition, this chapter also
demonstrates the benefits of exploiting symmetries in the case of MPS
algorithms. We adapt the infinite time evolving block decimation (iTEBD)
algorithm to the presence of a global SU(2) symmetry. This is of
interest in its own right since the iTEBD algorithm has been immensely
successful in simulations of infinite 1D quantum many-body systems. This
is also the first implementation of a non-Abelian symmetry into the
iTEBD algorithm and has resulted in a significant enhancement of this
algorithm.

In Chapter 3 we go beyond the class of MPS algorithms. We describe the
general strategy to incorporate a wide spectrum of symmetries into more
complex tensor network states and algorithms. We consider tensor
networks made of symmetric tensors, that is, tensors that are invariant
under the action of the symmetry. We develop a formalism to characterize
and manipulate symmetric tensors.

In Chapter 4 we implement the general formalism for the case of an
Abelian symmetry. The implementation of an Abelian symmetry is
simplified by the fact that symmetric tensors are easier to
characterize. In a basis labeled by the charges of the symmetry, a
symmetric tensor has a sparse block structure. We explain how this block
structure can be exploited for computational gain in a practical
implementation of the symmetry. The benefits of exploiting the symmetry
are numerically demonstrated by exploiting U(1) symmetry in the context
of the MERA.

In Chapters 5 and 6 we describe the implementation of a non-Abelian
symmetry. The details of implementing the symmetry are more involved,
since the structural tensors are highly non-trivial. However, the
computational gain that results from exploiting a non-Abelian symmetry
is significantly larger than that obtained by exploiting an Abelian
symmetry. Moreover, the practical scheme presented to implement a
non-Abelian symmetry can be readily extended to incorporate more exotic
symmetry constraints such as those corresponding to the presence of
anyonic degrees of freedom. The benefits of exploiting a non-Abelian
symmetry are numerically demonstrated by means of our implementation of
SU(2) symmetry in the context of the MERA.

Finally, in Chapter 7 we draw conclusions and discuss potential
applications and future directions of this work.

Note on References: In addition to the references listed at the end of
the thesis, chapter wise references appear at the end of chapters 2, 3
and 4 that correspond to published papers.

## Chapter 2 Exploiting symmetries in MPS algorithms: An Example

We kick start the main discussion of the thesis by describing how to
incorporate an SU(2) symmetry into a specific MPS algorithm: the iTEBD
algorithm. The iTEBD algorithm has been immensely successful in
simulations of infinite 1D quantum many-body systems.

We follow a straightforward implementation of the symmetry into the
algorithm. We consider an MPS that is made of trivalent SU(2) invariant
tensors. A trivalent SU(2) invariant tensor decomposes into two pieces.
One piece contains the degrees of freedom whereas the other corresponds
to the Clebsch-Gordan coefficients of SU(2). We describe how the iTEBD
algorithm can be enhanced by exploiting this decomposition of the MPS
tensors. The resulting symmetric algorithm is obtained to be 300 times
faster (See Fig. 1.1 ). We use the symmetric algorithm for a numerical
study of a critical quantum spin chain.

This chapter serves to illustrate the main conceptual ingredients that
are required to incorporate symmetries into tensor network algorithms.
In the next chapter, we will generalize these ingredients by going
beyond the specific details of the SU(2) symmetry group and the MPS
representation.

See pages 2-13 of su2mps.pdf

## Chapter 3 Tensor networks and symmetries: Theoretical formalism

In this chapter we develop a generic theoretical formalism to
incorporate a symmetry into tensor network algorithms. We consider a
wide class of symmetries that are described by a compact and reducible
group @xmath that is multiplicity free, that is, the tensor product of
two charges of the group does not contain multiple copies of a charge.
Our strategy revolves around tensors that are invariant under the action
of the symmetry. As a result, we formulate a framework of symmetric
tensors.

A symmetric tensor transforms covariantly (or remains invariant) under
the action of the symmetry. In a basis labeled by the symmetry charges,
the tensor decomposes into a set of degeneracy tensors and structural
tensors . While the degeneracy tensors contain the degrees of freedom,
the components of the structural tensors are generalizations of the
coupling coefficients of the group, and are determined completely by the
symmetry. Moreover, any symmetry preserving manipulation of the tensor
can be performed in parts. For instance, a permutation of the indices of
a symmetric tensor breaks into the permutation of the corresponding
degeneracy indices and the permutation of the corresponding structural
indices. Therefore, this canonical decomposition of a symmetric tensor
allows for both a compact description of the tensor and a computational
speedup in numerical manipulations of it.

We also point out a numerical connection to the formalism of spin
networks ( Penrose , 1971 ; Major , 1999 ) . A spin network is a
mathematical object that appears, for example, in Loop Quantum Gravity
(Rovelli, 1998 ) , where it is used (Rovelli and Smolin, 1995 ) to
facilitate a description of quantum spacetime. In our formalism, a
tensor network made of symmetric tensors decomposes into a linear
superposition of spin networks. Also, manipulating a symmetric tensor
network requires evaluating a spin network. Thus, our work highlights
the importance of spin networks in the context of tensor network
algorithms, thus setting the stage for cross-fertilization between these
two areas of research.

See pages 1-4 of symTNpaper.pdf

### 3.1 Errata

The following equations appear erroneously in the publication. They are
to be corrected as follows.

The tensors @xmath and @xmath that appear in Eqs.11 and 12 do not carry
degeneracy indices and spin indices corresponding to the coupled charges
@xmath and @xmath , since these indices are summed over in the
description.

In Eq. 11 the components of @xmath and @xmath read as @xmath and @xmath
respectively and the sum is only over different values of charge @xmath
. The corrected equation reads,

  -- -------- --
     @xmath   
  -- -------- --

In Eq. 12 the components of @xmath and @xmath read as @xmath and @xmath
respectively and the sum is only over different values of charge @xmath
. The corrected equation reads,

  -- -------- --
     @xmath   
  -- -------- --

A similar correction holds for Eq. 15 which is a generalization of
Eqs.11 and 12. The sum is only over different values of the intermediate
charges @xmath . The corrected equation reads,

  -- -------- --
     @xmath   
  -- -------- --

## Chapter 4 Implementation of Abelian symmetries

In this chapter we specialize the general formalism to the case of
Abelian symmetries. Abelian symmetries appear frequently in the context
of lattice models with particles (bosons or fermions) as well as those
with spins. In the former, they include particle number conservation and
parity conservation, whereas in the latter they appear as conservation
of spin projection.

The analysis of exploiting an Abelian symmetry is made simple by the
fact that the structural tensors in this case are trivial. On the other
hand, an implementation of an Abelian symmetry serves to expose the
practical difficulties that are encountered when incorporating
symmetries into complicated tensor networks. We pay special attention to
such implementation level concerns. Certain operations in the algorithm
depend only on the symmetry and not on the components of the tensors
involved. We exploit this fact to precompute the output of such
operations and store their result in memory. This is particularly
advantageous in an iterative algorithm where tensor components are
updated or optimized by repeating a set of computations. The runtime
cost of the iterative algorithm can be significantly reduced by reusing
the precomputed results from memory. By making use of precomputation we
obtained a substantial computational gain from exploiting the symmetry
in our MATLAB implementation. However, this was achieved at the expense
of storing potentially large amounts of precomputed data.

The discussion is conducted in the specific context of U(1) symmetry
associated, for example, with conservation of particle number or of spin
projection. We describe how to implement elementary tensor manipulations
such as permutation and reshape of indices in a U(1) symmetric way. We
also present a concrete implementation of the U(1) symmetry in the
context of the MERA. We consider a MERA that is made of U(1) symmetric
tensors. Then using the U(1) MERA we demonstrate the benefits of
including symmetries into tensor networks.

See pages 1-22 of u1.pdf

## Chapter 5 Implementation of non-Abelian symmetries \@slowromancapi@

In this and the following chapter we will address the implementation of
global non-Abelian symmetries into tensor network algorithms. We
consider the specific context of an internal SU(2) symmetry, that gives
rise to spin isotropy. This is an extremely important symmetry that
appears amply in lattice spin models.

In this chapter we will focus on the conceptual aspects of incorporating
the symmetry. The theoretic formalism developed in Chapter 3 will be
adapted to the specific case of SU(2) symmetry. We consider tensors that
are invariant under the action of SU(2). The structural tensors, that
are part of the canonical decomposition, are highly non-trivial (and are
given in terms of the Clebsch-Gordan coefficients). However, the key
advantage of the canonical decomposition is that it allows tensor
manipulations, such as reshape or permutation of indices, to be broken
into an independent manipulation of degeneracy tensors and of structural
tensors.

In the context of numerical simulations the canonical decomposition
leads to a computational gain. Computational cost is incurred only when
manipulating degeneracy tensors. One the other hand, structural tensors
are manipulated algebraically by exploiting properties of the
Clebsch-Gordan coefficients. Manipulations of the structural tensors
reduce to evaluating a spin network. This process involves integrating
over the degrees of freedom associated with spin projection, which are
therefore suppressed in the outcome of the tensor manipulation, as is
expected in a spin isotropic description.

In Chapter 6 we will describe a specific implementation scheme of
elementary manipulations of SU(2)-invariant tensors. We will address
several concerns that are of importance in the practical implementations
of the symmetry.

### 5.1 Preparatory Review: Tensor network formalism

In this section we review the basic formalism of tensors and tensor
networks. Even though we do not make any explicit reference to symmetry
here, our formalism is directed towards SU(2)-invariant tensors.

We begin by recalling the basic notion of a tensor. A tensor @xmath is a
multidimensional array of complex numbers @xmath . The rank of a tensor
is the number @xmath of indices. The size of an index @xmath , denoted
@xmath , is the number of values that the index takes, @xmath . The size
of a tensor @xmath , denoted @xmath , is the number of complex numbers
it contains, namely, @xmath .

#### 5.1.1 Tensors as linear maps

For the purpose of this thesis we regard a rank- @xmath tensor as a
linear map. To this end, we first equip each index @xmath , of the
tensor with a direction: ‘in’ or ‘out’, that is, either incoming into
the tensor or outgoing from the tensor respectively. We denote by @xmath
the directions associated with the indices of tensor @xmath , namely,
@xmath ‘in’ if @xmath is an incoming index and @xmath ‘out’ if @xmath is
outgoing.

Let us also use index @xmath of the tensor to label a basis @xmath of a
complex vector space @xmath of dimension @xmath . Then a rank-one (
@xmath ) tensor with an outgoing index @xmath represents a vector in
@xmath , a rank-two ( @xmath ) tensor @xmath with one incoming index
@xmath and one outgoing index @xmath represents a matrix and so on.

A tensor can be unambiguously regarded as a linear map from a vector
space to complex numbers @xmath . For instance, a vector can be regarded
as a linear map from @xmath to @xmath , a matrix @xmath can be regarded
as a linear map from @xmath to @xmath where @xmath is the dual of vector
space @xmath etc. More generally, we can use a rank- @xmath tensor
@xmath to define a linear map from the tensor product of @xmath vector
spaces to @xmath in the following way. Define a set @xmath , of @xmath
spaces where

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where the @xmath is the dual of vector space @xmath . Then tensor @xmath
can be regarded as a linear map from the product space @xmath to @xmath
,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

We will find this viewpoint of tensors useful for subsequent
generalization to SU(2)-invariant tensors.

It is convenient to use a graphical representation of tensors, as
illustrated in Fig. 1, where a tensor is depicted as a “blob” (or by a
shape e.g., circle, square etc.) and each of its indices is represented
by a line emerging perpendicular from the boundary of the blob. In order
to specify which index corresponds to which emerging line, we follow the
prescription that the lines corresponding to indices @xmath emerge in
counterclockwise order. The first index corresponds to the line emerging
closest to a mark (black dot) inside the boundary of the blob (or the
first line encountered while proceeding counterclockwise from nine
o’clock in case the tensor is depicted as a circle without a mark). The
direction of an index is depicted by attaching an arrow to the line
corresponding to the index. We follow a convention that all arrows in a
diagram point downwards.

#### 5.1.2 Elementary manipulations of a tensor

A tensor can be transformed into another tensor in several elementary
ways. These include, reversing the direction of one or several of its
indices, permuting its indices, and/or reshaping its indices.

Reversing the direction of an index corresponds to mapping the vector
space that is associated with the index to its dual. For example, in

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

if index @xmath is associated to a vector space @xmath , then index
@xmath that is obtained by reversing the direction of @xmath is
associated with the dual space @xmath . Since all arrows in a diagram
point downwards, reversing the direction of an index @xmath is depicted
[Fig. 5.2 (i)] by ‘bending’ the line corresponding to @xmath upwards if
it is an outgoing index or downwards if it is an incoming index. Since
tensor @xmath is components wise equal to tensor @xmath arrows appear to
be irrelevant in the absence of the symmetry. However, arrows will play
an important role when we consider SU(2)-invariant tensors since they
specify how the group acts on each index of a given tensor.

A permutation of indices corresponds to creating a new tensor @xmath
from @xmath by simply changing the order in which the indices appear,
e.g.

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

Permutation of indices is depicted by intercrossing indices, as
illustrated in Fig. 5.2 (ii). Note that when the permutation involves
the first index of the tensor the mark, that indicates the first index,
is also shifted to a new location within the blob. It is useful to note
that an arbitrary permutation of the indices can be broken into a
sequence of swaps of adjacent indices wherein the position of two
indices are interchanged at a time.

Last but not the least, a tensor @xmath can be reshaped into a new
tensor @xmath by ‘fusing’ two adjacent indices into a single index
and/or ‘splitting’ an index into two indices. For instance, in

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

tensor @xmath is obtained from tensor @xmath by fusing indices @xmath
and @xmath together into a single index @xmath of size @xmath that runs
over all pairs of values of @xmath and @xmath , i.e. @xmath , whereas in

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

tensor @xmath is recovered from @xmath by splitting index @xmath of
@xmath back into indices @xmath and @xmath . Reshape of the indices is
depicted as shown in Fig. 5.2 (iii).

#### 5.1.3 Multiplication of two tensors

Given two matrices @xmath and @xmath with components @xmath and @xmath ,
we can multiply them together to obtain a new matrix @xmath , @xmath ,
with components

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

by summing over or contracting index @xmath . The multiplication of
matrices @xmath and @xmath is represented graphically by connecting
together the emerging lines of @xmath and @xmath corresponding to the
contracted index, as shown in Fig. 5.3 (i).

Matrix multiplication can be generalized to tensors, such that, an
incoming index of one tensor is identified and contracted with an
outgoing index of another. For instance, given tensor @xmath with
components @xmath and directions @xmath , and tensor @xmath with
components @xmath and directions @xmath , we can define a tensor @xmath
with components @xmath that are given by,

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

Note that each of the indices @xmath and @xmath , that are contracted,
is incoming into one tensor and outgoing from the other. The
multiplication is represented graphically by connecting together the
lines emerging from @xmath and @xmath corresponding to each of these
indices, as shown in Fig. 5.3 (ii).

Multiplication of two tensors can be broken down into a sequence of
elementary steps by transforming the tensors into matrices, multiplying
the matrices together, and then transforming the resulting matrix back
into a tensor. Next we describe these steps for the contraction given in
Eq. ( 5.8 ). They are illustrated in Fig. 5.4 .

1.   Reverse and Permute the indices of tensor @xmath in such a way that
    the indices @xmath and @xmath that are contracted appear in the last
    positions as outgoing indices and in a given order, e.g. @xmath ,
    and the remaining indices @xmath and @xmath appear in the first
    positions as incoming indices; similarly reverse and permute the
    indices of @xmath so that the indices @xmath and @xmath appear in
    the first positions as incoming indices and in the same order,
    @xmath , and the remaining indices @xmath and @xmath appear in the
    last positions as outgoing indices,

      -- -------- -------- -- -------
         @xmath   @xmath      
         @xmath   @xmath      (5.9)
      -- -------- -------- -- -------

2.   Reshape tensor @xmath into a matrix @xmath by fusing into a single
    index @xmath all the indices that are not contracted, @xmath , and
    into a single index @xmath all indices that are contracted, @xmath ;
    similarly reshape tensor @xmath into a matrix @xmath with indices
    @xmath and @xmath (indices @xmath and @xmath are required to be
    fused according to the same fusion sequence in the two tensors. A
    possible fusion sequence may involve, for example, first fusing
    @xmath and @xmath and then fusing the resulting index with @xmath ),

      -- -------- -------- -- --------
         @xmath   @xmath      
         @xmath   @xmath      (5.10)
      -- -------- -------- -- --------

3.   Multiply matrices @xmath and @xmath to obtain a matrix @xmath with
    components

      -- -------- -- --------
         @xmath      (5.11)
      -- -------- -- --------

4.   Reshape matrix @xmath into a tensor @xmath by splitting indices
    @xmath and @xmath ,

      -- -------- -- --------
         @xmath      (5.12)
      -- -------- -- --------

5.   Reverse and Permute indices of tensor @xmath in the order in which
    they appear in @xmath ,

      -- -------- -- --------
         @xmath      (5.13)
      -- -------- -- --------

The contraction of Eq. ( 5.8 ) can be implemented at once, without
breaking the multiplication down into elementary steps. However, it is
often more convenient to compose the above elementary steps since, for
instance, in this way one can use existing linear algebra libraries for
matrix multiplication. In addition, it can be seen that the leading
computational cost in multiplying two large tensors is not changed when
decomposing the contraction in the above steps.

#### 5.1.4 Factorization of a tensor

A matrix @xmath can be factorized into the product of two (or more)
matrices in one of several canonical forms. For instance, the singular
value decomposition

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

factorizes @xmath into the product of two unitary matrices @xmath and
@xmath , and a diagonal matrix @xmath with non-negative diagonal
elements @xmath known as the singular values of @xmath [Fig. 5.5 (i)].

On the other hand, the eigenvalue or spectral decomposition of a square
matrix @xmath is of the form

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where @xmath is an invertible matrix whose columns encode the
eigenvectors @xmath of @xmath ,

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

@xmath is the inverse of @xmath , and @xmath is a diagonal matrix, with
the eigenvalues @xmath on its diagonal. Other useful factorizations
include the LU decomposition, the QR decomposition, etc. We refer to any
such decomposition generically as a matrix factorization .

A tensor @xmath with more than two indices can be converted into a
matrix in several ways by specifying how to join its indices into two
subsets. After specifying how tensor @xmath is to be regarded as a
matrix, we can factorize @xmath according to any of the above matrix
factorizations, as illustrated in Fig. 5.5 (ii) for a singular value
decomposition. This requires first reversing directions, permuting and
reshaping the indices of @xmath to form a matrix, then decomposing the
latter, and finally restoring the open indices of the resulting matrices
into their original form by undoing the reshapes, permutations and
reversal of directions.

#### 5.1.5 Tensor networks and their manipulation

A tensor network @xmath is a set of tensors whose indices are connected
according to a network pattern, e.g. Fig. 5.6 .

Given a tensor network @xmath , a single tensor @xmath can be obtained
by contracting all the indices that connect the tensors in @xmath [Fig.
5.6 (ii)]. Here, the indices of tensor @xmath correspond to the open
indices of the tensor network @xmath . We then say that the tensor
network @xmath is a tensor network decomposition of @xmath . One way to
obtain @xmath from @xmath is through a sequence of contractions
involving two tensors at a time [Fig. 5.6 (iii)]. Notice how a tensor
that is obtained by contracting a region of a tensor network is
conveniently depicted by a blob or shape that covers that region.

From a tensor network decomposition @xmath for a tensor @xmath , another
tensor network decomposition for the same tensor @xmath can be obtained
in many ways. One possibility is to replace two tensors in @xmath with
the tensor resulting from contracting them together, as is done in each
step of Fig. 5.6 (iii). Another way is to replace a tensor in @xmath
with a decomposition of that tensor (e.g. with a singular value
decomposition). In this thesis, we will be concerned with manipulations
of a tensor network that, as in the case of multiplying two tensors or
decomposing a tensor, can be broken down into a sequence of operations
from the following list:

1.  Reversal of direction of indices of a tensor, Eq. ( 5.3 ).

2.  Permutation of the indices of a tensor, Eq. ( 5.4 ).

3.  Reshape of the indices of a tensor, Eqs. ( 5.5 )-( 5.6 ).

4.  Multiplication of two matrices, Eq. ( 5.7 ).

5.  Factorization of a matrix (e.g. singular value decomposition Eq. (
    5.14 ) or spectral decomposition Eq. ( 5.15 ).

These operations constitute a set @xmath of primitive operations for
tensor network manipulations (or, at least, for the type of
manipulations we will be concerned with). In Sec. 5.6 (and again in
Chapter 6) we discuss how this set @xmath of primitive operations can be
generalized to tensors that are invariant under the action of the group
SU(2).

Next we review basic background material concerning the representation
theory of the group SU(2) without reference to tensor network states and
algorithms. This review is distributed over Sections 5.2 , 5.3 and 5.4 .
We refer the reader to (Cornwell, 1997 ) and Chapters 3 and 4 of
(Sakurai, 1994 ) for additional supporting material for these sections.

### 5.2 Representations of the group SU(2)

In this section we consider the action of SU(2) on a vector space that
is an irreducible representation of the group, and then more generally
on a vector space @xmath that is a reducible representation, namely,
@xmath decomposes as a direct sum of (possibly degenerate) irreducible
representations. We also characterize vectors belonging to @xmath and
linear operators acting on @xmath that are invariant under the action of
SU(2).

Let @xmath be a finite dimensional vector space on which SU(2) acts
unitarily by means of transformations @xmath ,

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

Here @xmath parameterizes the group elements and @xmath ; @xmath and
@xmath are traceless hermitian operators that are said to generate the
representation @xmath of SU(2). These operators close the lie algebra
su(2), namely,

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

where @xmath is the Levi-Civita symbol. The operators @xmath and @xmath
are associated, for example, with the projection of angular momentum or
spin along the three spatial directions @xmath and @xmath ,
respectively.

It follows that,

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

#### 5.2.1 Irreducible representations

Let vector space @xmath transform as an irreducible representation (or
irrep) of SU(2) with spin @xmath . Here @xmath can take values @xmath
and @xmath has dimension @xmath . We choose an orthonormal basis @xmath
, the spin basis , in @xmath that is a simultaneous eigenbasis of the
operators @xmath and @xmath ,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.20)
  -- -------- -------- -- --------

Here @xmath is the magnitude of the spin projection along the @xmath
direction and can assume values in the range @xmath . In this basis, the
action of the operators @xmath and @xmath on the space @xmath is
conveniently described in terms of the raising operator @xmath and the
lowering operator @xmath as

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

The operator @xmath can be written as

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

where @xmath is the Identity acting on the irrep @xmath .

Example 1: Consider that vector space @xmath is a spin @xmath irrep of
SU(2). Then @xmath has dimension one, @xmath . The operators @xmath are
trivial, @xmath .

Example 2: Consider a two-dimensional vector space @xmath that
transforms as an irrep @xmath . Then the orthogonal vectors (in column
vector notation)

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

form a basis of @xmath . In this basis the operators @xmath and @xmath
read as

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

In terms of Pauli matrices @xmath we have

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

Example 3: Also consider a three-dimensional vector space @xmath that
transforms as an irrep @xmath . The orthogonal vectors

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

form a basis of @xmath . In this basis, operators @xmath and @xmath read
as

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.27)
  -- -------- -------- -- --------

#### 5.2.2 Reducible representations

More generally, SU(2) can act on the vector space @xmath reducibly, in
that, @xmath may decompose as the direct sum of irreps of SU(2),

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

Here space @xmath accommodates a spin @xmath irrep of SU(2) and @xmath
is the number of times @xmath occurs in @xmath . The decomposition can
also be re-written in terms of a @xmath dimensional space @xmath . We
say that irrep @xmath is @xmath -fold degenerate and that @xmath is the
degeneracy space. The total dimension of space @xmath is given by @xmath
.

Let @xmath label an orthonormal basis @xmath in the space @xmath . Then
a natural choice of basis of the space @xmath is the set of orthonormal
vectors @xmath , where @xmath assumes various values that occur in the
direct sum decomposition, Eq. ( 5.28 ).

In this basis the action of SU(2) on @xmath is given by

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

as generated by the operators

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

Here @xmath is a @xmath Identity and operators @xmath generate the
irreducible represention @xmath on space @xmath .

The operator @xmath takes the form

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

Example 4: Let vector space @xmath transform as an irrep @xmath with a
finite degeneracy @xmath . The space @xmath decomposes as @xmath where
@xmath is a three-dimensional degeneracy space and @xmath corresponds to
the space of Example 1.

The total dimension of space @xmath is @xmath . The vectors

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

form a basis of @xmath . A basis of @xmath can be obtained as the
product of the the basis ( 5.32 ) of @xmath and the basis ( 5.23 ) of
@xmath . In this basis of the operators @xmath take the form of Eq. (
5.30 ). For instance,

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

Similarly, operators @xmath and @xmath read as

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.34)
  -- -------- -------- -- --------

The operator @xmath reads

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

Example 5: Consider a five-dimensional Hilbert space @xmath that
decomposes into two different irreps @xmath and @xmath with degeneracy
dimensions @xmath and @xmath so that irrep @xmath is two-fold
degenerate. The space @xmath decomposes as @xmath , where @xmath is the
two-dimensional degeneracy space of irrep @xmath and @xmath is the
one-dimensional degeneracy space of irrep @xmath .

The orthogonal vectors

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (5.36)
  -- -------- -------- -- --------

form a basis of @xmath . In this basis, the operators @xmath take the
form

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

where @xmath and @xmath are the generators of irrep @xmath (Examples 1)
and irrep @xmath (Examples 3) respectively. Operators @xmath and @xmath
read as

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (5.38)
  -- -------- -------- -- --------

#### 5.2.3 SU(2)-invariant states and operators

We are interested in states and operators that have a simple
transformation rule under the action of SU(2).

A pure state @xmath with a well defined spin @xmath belongs to the
subspace @xmath . In the spin basis @xmath the state @xmath can be
expanded as

  -- -------- -- --------
     @xmath      (5.39)
  -- -------- -- --------

Under the action of SU(2), @xmath transforms to another pure state
@xmath , @xmath , within the same subspace @xmath . The components
@xmath of @xmath are related to those of @xmath as

  -- -------- -- --------
     @xmath      (5.40)
  -- -------- -- --------

In case of vanishing spin @xmath , the state @xmath transforms trivially
under the action of SU(2), @xmath . In this case Eq. ( 5.40 ) reduces
to,

  -- -------- -- --------
     @xmath      (5.41)
  -- -------- -- --------

That is, @xmath remains invariant under the action of SU(2).
Equivalently, it is annihilated by the action of generators,

  -- -------- -- --------
     @xmath      (5.42)
  -- -------- -- --------

An SU(2)-invariant state @xmath can be expanded in the basis @xmath of
the spin @xmath subspace, @xmath ,

  -- -------- -- --------
     @xmath      (5.43)
  -- -------- -- --------

where we have used @xmath as a shorthand for @xmath .

A linear operator @xmath is SU(2)-invariant if it commutes with the
action of the group,

  -- -------- -- --------
     @xmath      (5.44)
  -- -------- -- --------

or equivalently, if it commutes with the generators @xmath ,

  -- -------- -- --------
     @xmath      (5.45)
  -- -------- -- --------

Notice that the operator @xmath is SU(2)-invariant, Eq. ( 5.19 ).

An SU(2)-invariant operator @xmath decomposes as

  -- -------- -- --------
     @xmath      
     @xmath      (5.46)
  -- -------- -- --------

where @xmath is a @xmath matrix that acts on the degeneracy space @xmath
. This decomposition implies, for instance, that operator @xmath
transforms states with a well defined spin @xmath [such as @xmath of
Eq. ( 5.39 )] into states with the same spin @xmath . Thus,
SU(2)-invariant operators conserve spin.

Example 2 revisited: A generic state @xmath has the form,

  -- -------- -- --------
     @xmath      (5.47)
  -- -------- -- --------

and is an eigenstate of @xmath with eigenvalue @xmath .

According to Schur’s lemma, an SU(2)-invariant operator @xmath acting on
@xmath must be proportional to the Identity,

  -- -------- -- --------
     @xmath      (5.48)
  -- -------- -- --------

Example 4 revisited: A generic state @xmath in the vector space @xmath
of Example 3 has the form

  -- -- -- --------
           (5.49)
  -- -- -- --------

Similar to the previous example, @xmath is an eigenstate of @xmath with
eigenvalue @xmath ..

An SU(2)-invariant operator @xmath must be of the form

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.50)
  -- -------- -------- -- --------

where we have used @xmath as a shorthand for @xmath . Notice that @xmath
in Eq. ( 5.35 ) has this form.

Example 5 revisited: A generic state @xmath has the form

  -- -------- -- --------
     @xmath      (5.51)
  -- -------- -- --------

(For simplicity we have omitted explicit labels in the subscripts.) In
contrast to the previous two examples, a generic state @xmath is not an
eigenstate of @xmath , that is, @xmath is generally not a state with a
well defined spin @xmath .
An SU(2)-invariant vector @xmath has the form

  -- -------- -- --------
     @xmath      (5.52)
  -- -------- -- --------

with non-trivial components only in the spin @xmath subspace. Notice
that this state is annihilated by the action of the operators @xmath
[Eq. ( 5.38 )] in accordance with Eq. ( 5.42 ).

A state with a well defined spin @xmath must be of the form

  -- -------- -- --------
     @xmath      (5.53)
  -- -------- -- --------

with non-trivial components only in the spin @xmath subspace.

An SU(2)-invariant operator @xmath e.g. @xmath in Eq. ( 5.38 ) has the
form

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.54)
  -- -------- -------- -- --------

where @xmath .

Notice that the SU(2)-invariant vector in Eq. ( 5.52 ) and the
SU(2)-invariant matrices in Eqs. ( 5.48 ),( 5.50 ) and ( 5.54 ) have a
sparse structure. In particular, the non-trivial components of an
SU(2)-invariant matrix @xmath are organized into blocks @xmath . This
block structure can be exploited for computational gain. An
SU(2)-invariant matrix can be stored compactly by storing the degeneracy
blocks @xmath , while matrix multiplication and matrix factorizations
can be performed block-wise [Sec. 5.6 ] resulting in a significant
speedup [Fig. 5.26 ] for these operations.

### 5.3 Tensor product of representations

So far we have described the action of SU(2) on a single vector space.
Let us now consider the action of SU(2) on a space @xmath that is a
tensor product of @xmath vector spaces,

  -- -------- -- --------
     @xmath      (5.55)
  -- -------- -- --------

where each vector space @xmath transforms as a finite dimensional
representation of SU(2) as generated by spin operators @xmath . We
consider the action of SU(2) on the space @xmath that is generated by
the total spin operators,

  -- -------- -- --------
     @xmath      (5.56)
  -- -------- -- --------

(each term in the sum acts as @xmath on site @xmath and the Identity on
the remaining sites) and which corresponds to the unitary
transformations,

  -- -------- -- --------
     @xmath      (5.57)
  -- -------- -- --------

As a example consider two vector spaces @xmath and @xmath on which the
action of SU(2) is generated by spin operators @xmath and @xmath
respectively. We can then consider the action of the group on the
product space @xmath as generated by the total spin operators @xmath
that are given by

  -- -------- -- --------
     @xmath      (5.58)
  -- -------- -- --------

Similarly, we can consider the action of SU(2) on the product of three
vector spaces, @xmath and @xmath , that is generated by spin operators
@xmath ,

  -- -------- -- --------
     @xmath      (5.59)
  -- -------- -- --------

where @xmath and @xmath are the spin operators that act on the three
vector spaces.

A basis of @xmath can be obtained in terms of the spin basis of each
vector space in the product, Eq. 5.55 . However, it is convenient to
introduce a coupled basis: the simultaneous eigenbasis of the total spin
operators @xmath and @xmath . In the coupled basis SU(2)-invariant
states @xmath ,

  -- -------- -- --------
     @xmath      (5.60)
  -- -------- -- --------

and SU(2)-invariant operators @xmath ,

  -- -------- -- --------
     @xmath      (5.61)
  -- -------- -- --------

have a sparse structure, namely, @xmath has non-trivial components only
in the spin zero sector of @xmath while @xmath is block-diagonal [Eq. (
5.46 )].

In the remainder of the section we focus on the tensor product of two
representations. We first discuss the case where the two sites transform
as irreducible representations and then the more general case of
reducible representations. The tensor product of several representations
can then be analyzed by considering a sequence of pairwise products.

#### 5.3.1 Tensor product of two irreducible representations

Let vector spaces @xmath and @xmath transform as irreps @xmath and
@xmath respectively. The space @xmath is, in general, reducible and
decomposes as

  -- -------- -- --------
     @xmath      (5.62)
  -- -------- -- --------

where the total spin @xmath assumes all values in the range

  -- -------- -- --------
     @xmath      (5.63)
  -- -------- -- --------

Let @xmath and @xmath denote the spin basis of the respective vector
spaces. Then the vectors

  -- -------- -- --------
     @xmath      (5.64)
  -- -------- -- --------

form a basis of @xmath . Introduce a coupled basis @xmath that fulfills

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.65)
  -- -------- -------- -- --------

The coupled basis is related to the product basis ( 5.64 ) by means of
the transformation

  -- -------- -- --------
     @xmath      (5.66)
  -- -------- -- --------

Here

  -- -------- -- --------
     @xmath      (5.67)
  -- -------- -- --------

are the Clebsch-Gordan coefficients , which vanish unless @xmath and
@xmath are compatible, that is, @xmath and @xmath fulfill

  -- -------- -- --------
     @xmath      (5.68)
  -- -------- -- --------

and @xmath and @xmath fulfill

  -- -------- -- --------
     @xmath      (5.69)
  -- -------- -- --------

The product basis can be expressed in terms of the coupled basis as

  -- -------- -- --------
     @xmath      (5.70)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (5.71)
  -- -------- -- --------

We graphically represent tensors @xmath and @xmath differently from
usual tensors, as shown in Fig. 5.7 (i).

Tensor @xmath is depicted by means of two incoming lines and one
outgoing line that emerge from a point. The outgoing line corresponds to
the spin index @xmath . The incoming lines that are encountered first
and second when proceeding clockwise from the outgoing line correspond
to the spin indices @xmath and @xmath respectively.

Analogously, tensor @xmath is depicted by means of one incoming line and
two outgoing lines that emerge from a point. The spin index @xmath
corresponds to the incoming line in this case while spin indices @xmath
and @xmath correspond to the outgoing lines in the order in which they
are encountered when proceeding counterclockwise from the incoming line.

Tensor @xmath and tensor @xmath fulfill the orthogonality identities,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.72)
  -- -------- -------- -- --------

The special graphical representations for these tensors allows one to
depict the above identities in an intuitive way, as shown in Fig. 5.7
.(ii)-(iii).

Example 6: Let both vector spaces @xmath and @xmath transform as a spin
@xmath irrep, @xmath (Example 2). The space @xmath decomposes into a
direct sum of irreps,

  -- -------- -------- -- --------
     @xmath   @xmath      (5.73)
  -- -------- -------- -- --------

A basis can be introduced in @xmath in terms of the spin basis [Eq. (
5.23 )] of @xmath and of @xmath . The coupled basis of @xmath ,

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

is related to the product basis by means of the Clebsch-Gordan
coefficients,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.74)
     @xmath   @xmath      (5.75)
     @xmath   @xmath      
              @xmath      (5.76)
     @xmath   @xmath      (5.77)
  -- -------- -------- -- --------

For completeness, we list below the numerical value of the
Clebsch-Gordan coefficients that appear in Eqs. ( 5.74 )-( 5.77 ),

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

#### 5.3.2 Tensor product of two reducible representations

Let us now consider that vector spaces @xmath and @xmath transform
reducibly under the action of SU(2). We have

  -- -------- -- --------
     @xmath      (5.78)
  -- -------- -- --------

The space @xmath decomposes into a direct sum of irreps,

  -- -------- -- --------
     @xmath      (5.79)
  -- -------- -- --------

where the total spin @xmath takes all values that are compatible with
any pair of irreps @xmath and @xmath .

Let @xmath and @xmath denote the spin basis of @xmath and @xmath
respectively. We introduce a coupled basis @xmath that fulfills,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.80)
  -- -------- -------- -- --------

and which is related to the product basis,

  -- -------- --
     @xmath   
  -- -------- --

by means of a transformation,

  -- -------- -- --------
     @xmath      (5.81)
  -- -------- -- --------

The components @xmath can be expressed in terms of the Clebsch-Gordan
coefficients as

  -- -------- -- --------
     @xmath      (5.82)
  -- -------- -- --------

Let us explain how this expression is obtained. From the definition,
Eq. ( 5.81 ), we have

  -- -------- -- --------
     @xmath      (5.83)
  -- -------- -- --------

According to the direct sum decomposition, Eq. ( 5.79 ), each vector
@xmath belongs to the subspace @xmath where it factorizes as

  -- -------- -- --------
     @xmath      (5.84)
  -- -------- -- --------

Similarly, we can factorize vectors @xmath and @xmath . The expression
Eq. ( 5.82 ) can then be obtained by substituting these factorizations
into Eq. ( 5.83 ) and re-arranging the terms as shown below,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath .

Here @xmath is a one to one map that relates vectors @xmath to the
vectors @xmath . It can be regarded as a rank- @xmath tensor such that
each component of @xmath is either a zero or a one. We have,

  -- -------- -- --------
     @xmath      (5.85)
  -- -------- -- --------

The product basis can, in turn, be expressed in terms of the coupled
basis,

  -- -------- -- --------
     @xmath      (5.86)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (5.87)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (5.88)
  -- -------- -- --------

We refer to the tensors @xmath and @xmath as the fusing tensor and the
splitting tensor respectively, since they will play an instrumental role
in fusing and splitting indices of an SU(2)-invariant tensor. The
special graphical representation of these tensors and their
decomposition into @xmath and @xmath tensors is shown in Fig. 5.8 .
Tensors @xmath and @xmath are graphically represented by means of a
circle enclosing an arrow head and three lines emerging from the circle
corresponding to the three indices of the tensors. The three lines in
the diagrams of @xmath and @xmath correspond to the degeneracy indices
@xmath and @xmath by using the same assignment rules that were
introduced for tensors @xmath and @xmath respectively. Other features of
the graphical representation include an arrow head that is placed within
the circle to indicate the direction of the fusion and a small
rectangle, placed on the line carrying the coupled spins, that
represents a permutation of basis elements.

We notice that tensor @xmath can be decomposed into two pieces. The
first piece (depicted as the circle enclosing an arrow head) expresses a
basis @xmath of @xmath as the direct product of the basis @xmath of
@xmath and the basis @xmath of @xmath . Note that this procedure does
not always lead to the set @xmath being ordered such that states
corresponding to the same total spin @xmath are adjacent to each other
within the set. However, we require that the basis associated to an
index be maintained as such (this ensures, for example, that an
SU(2)-invariant matrix is block diagonal when expressed in such a
basis). This ordering is achieved by means of the second piece (depicted
as the small rectangle): a permutation of basis states @xmath that
reorganizes them according to their total spin @xmath , so that they are
identified in an one-to-one correspondence with the coupled states
@xmath . In particular, this description of the tensors @xmath and
@xmath can be exploited to multiply together several such tensors, such
as in Fig. 5.12 (iv), in a fast way.

By construction, a resolution of Identity can be obtained in terms of
tensor @xmath and tensor @xmath , as shown in Fig. 5.8 (ii)-(iii).

Example 7: Let vector spaces @xmath and @xmath correspond to the vector
space of Example 4, that is,

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (5.89)
  -- -------- -------- -- --------

The space @xmath decomposes as

  -- -------- -- --------
     @xmath      (5.90)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (5.91)
  -- -------- -- --------

and

  -- -------- -------- -- --------
     @xmath   @xmath      (5.92)
     @xmath   @xmath      (5.93)
  -- -------- -------- -- --------

Recall that the basis of the l.h.s. and r.h.s. of Eq. ( 5.91 ) are
related by the transformations Eqs. ( 5.74 )-( 5.77 ). Let us now
consider how the basis of the l.h.s. and r.h.s. of Eq. ( 5.92 ) and of
Eq. ( 5.93 ) are related. In Eq. ( 5.92 ), for instance, the vectors

  -- -------- --
     @xmath   
  -- -------- --

are related to the vectors

  -- -------- --
     @xmath   
  -- -------- --

in straightforward way by associating the vectors in a one to one
fashion in the order in which they appear in the respective basis. For
example, the change of basis maps the vector

  -- -------- --
     @xmath   
  -- -------- --

and the vector

  -- -------- --
     @xmath   
  -- -------- --

and so on. The basis of the l.h.s. and r.h.s. of Eq. ( 5.93 ) are
related in a similar way. This one to one mapping can be encoded into
@xmath by setting the numerical value of the following components equal
to one,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Example 8: As another example of the change of basis @xmath , consider
that @xmath and @xmath correspond to the vector spaces of Example 3 and
Example 5 respectively. That is,

  -- -------- -------- -- --------
     @xmath               
     @xmath   @xmath      (5.94)
  -- -------- -------- -- --------

The space @xmath decomposes as

  -- -------- -- --------
     @xmath      (5.95)
  -- -------- -- --------

where

  -- -------- -------- -- --------
     @xmath   @xmath      (5.96)
     @xmath   @xmath      (5.97)
     @xmath   @xmath      (5.98)
  -- -------- -------- -- --------

The transformation that relates the bases of l.h.s. and r.h.s. of Eq. (
5.96 ) and of Eq. 5.98 ) is straightforward, we set

  -- -------- --
     @xmath   
  -- -------- --

The basis of the l.h.s. and r.h.s. of Eq. ( 5.97 ) can be related by
mapping the three vectors

  -- -------- --
     @xmath   
  -- -------- --

in a one to one manner, to the two vectors

  -- -------- --
     @xmath   
  -- -------- --

and the vector

  -- -------- --
     @xmath   
  -- -------- --

This is encoded into @xmath by setting

  -- -------- --
     @xmath   
  -- -------- --

### 5.4 Review: Fusion trees

When considering the tensor product of more than two representations one
can obtain several coupled bases of the product space. These correspond
to taking the product of the vector spaces according to different
sequences of pairwise products. The spaces are linearly ordered in a
given way and we only consider a pairwise product of ‘adjacent’ spaces
in this linear ordering. For example, when considering the tensor
product of three representations,

  -- -------- -- --------
     @xmath      (5.99)
  -- -------- -- --------

one can consider either the pairwise products

  -- -------- -------- -- ---------
     @xmath   @xmath      
     @xmath   @xmath      (5.100)
  -- -------- -------- -- ---------

or the pairwise products

  -- -------- -------- -- ---------
     @xmath   @xmath      
     @xmath   @xmath      (5.101)
  -- -------- -------- -- ---------

Considering the tensor product one way or the other leads to two
different coupled bases in @xmath .

More generally, there exist several choices of a coupled basis in the
tensor product of @xmath representations, Eqs. ( 5.55 )-( 5.57 ). A
useful way to specify a particular sequence of pairwise products is by
means of a fusion tree .

A fusion tree, denoted @xmath , is a directed trivalent tree such that
each node of the tree represents the tensor product of two incoming
spaces into the outgoing space. The tree has a total of @xmath open
links which correspond to the @xmath vector spaces @xmath and the
product space @xmath . The internal links correspond to the intermediate
product spaces that appear in a sequence of pairwise products. Figure
5.10 illustrates two different fusions trees @xmath and @xmath that
correspond to two different ways of considering the tensor product of
three and of four representations. The sequence of fusions proceeds from
top to bottom.

A fusion tree can also be specified as a list of fusions. For example,
the fusion trees @xmath and @xmath depicted in Fig. 5.10 (i) can be
specified as

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Fusion trees play an important role in our discussion. In the present
context, a fusion tree characterizes a coupled basis of a tensor product
space. In Sec. 5.5 fusion trees are also used to characterize different
canonical decompositions of an SU(2)-invariant tensor.

In the remainder of the section we consider coupled bases that are
labeled by different fusion trees. We define the unitary transformation
that relates two such coupled bases. This transformation is important
since it also relates two different canonical decompositions of an
SU(2)-invariant tensor, as discussed in Sec. 5.5 . For purpose of
illustration, we first characterize the coupled basis in the simple case
of the tensor product of three representations before proceeding to the
generic case of @xmath representations.

#### 5.4.1 Tensor product of three irreps

Let vector spaces @xmath and @xmath transform as irreps @xmath and
@xmath respectively. The space @xmath is in general reducible, and may
contain several copies of an irrep @xmath . Let us first consider the
sequence ( 5.100 ) of tensor products corresponding to a fusion tree
@xmath . The vector spaces @xmath and @xmath decompose as

  -- -------- -- ---------
     @xmath      (5.102)
  -- -------- -- ---------

and

  -- -------- -- ---------
     @xmath      (5.103)
  -- -------- -- ---------

Notice that we can use the values of @xmath that appear on the r.h.s. of
Eq. ( 5.102 ) to label different copies of @xmath that appear on the
r.h.s. of Eq. ( 5.103 ). Thus, a coupled basis of @xmath can be labeled
as @xmath .

Let @xmath denote the transformation from the product basis
@xmath to the coupled basis @xmath . This change of basis can be
expressed in terms of Clebsch-Gordan coefficients as

  -- -------- -- ---------
     @xmath      (5.104)
  -- -------- -- ---------

where @xmath relates the basis @xmath to the intermediate basis @xmath ,
and @xmath relates this intermediate basis to the coupled basis @xmath .

Alternatively, we can first consider the tensor product @xmath
(corresponding to the sequence ( 5.101 ) of tensor products
characterized by another fusion tree @xmath ),

  -- -------- -- ---------
     @xmath      (5.105)
  -- -------- -- ---------

and use irrep @xmath to label another coupled basis @xmath of @xmath .
Denote by @xmath the change of basis to this new coupled basis. In terms
of Clebsch-Gordan coefficients we have

  -- -------- -- ---------
     @xmath      (5.106)
  -- -------- -- ---------

The two coupled bases @xmath and @xmath are related by a transformation
that is given by a rank- @xmath tensor @xmath with components @xmath ,

  -- -------- -- ---------
     @xmath      (5.107)
  -- -------- -- ---------

Here @xmath are the recoupling coefficients ( Cornwell , 1997 ) of
SU(2). By using Eqs. ( 5.104 ) and ( 5.106 ), the recoupling
coefficients can be explicitly expressed in terms of Clebsch-Gordan
coefficients,

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      
              @xmath      (5.108)
  -- -------- -------- -- ---------

where @xmath . Notice that, since the @xmath ’s are summed over, the
recoupling coefficients depend only on the @xmath ’s. Also recall that
the recoupling coefficients are proportional to the 6-j symbols of the
group,

  -- -------- -- ---------
     @xmath      (5.109)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (5.110)
  -- -------- -- ---------

#### 5.4.2 Tensor product of three reducible representations

Consider the action of SU(2) on the space @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are reducible representations of SU(2). It
induces a decomposition

  -- -------- -- ---------
     @xmath      (5.111)
  -- -------- -- ---------

where @xmath takes all values that are compatible with any @xmath and
@xmath .

Extending the argument for irreps, we can relate the coupled basis of
@xmath to the product basis by first considering the sequence ( 5.100 )
of tensor products and using two @xmath tensors

  -- -------- -- ---------
     @xmath      (5.112)
  -- -------- -- ---------

to relate at each step the coupled basis with the product basis.
Alternatively, we can consider the sequence ( 5.101 ) of tensor products
and use the different set of fusing tensors

  -- -------- -- ---------
     @xmath      (5.113)
  -- -------- -- ---------

to relate the product basis to the coupled basis at each step. The
respective change of basis transformation for the two cases is depicted
in Fig. 5.12 (i).

The two coupled bases, so obtained, are related by means of a matrix
@xmath that decomposes, according to Schur’s Lemma [Eq. ( 5.46 )] as

  -- -------- -- ---------
     @xmath      (5.114)
  -- -------- -- ---------

where the components of @xmath can be expressed in terms of recoupling
coefficients. This decomposition can be derived as follows.

The matrix @xmath is obtained by contracting the tensor network made of
tensors @xmath and tensors @xmath that is shown in Fig. 5.12 (ii). This
contraction can be performed piecewise [Fig. 5.12 (iii)]. For fixed
values of @xmath ’s on all links the tensor network factorizes into two
pieces since each constituent tensor @xmath and tensor @xmath factorizes
into a @xmath and a @xmath tensor. The tensor network made of @xmath
tensors equates [Fig 5.11 (ii)] the Identity times the recoupling
coefficient @xmath . The matrix @xmath in Eq. ( 5.114 ) is then defined
as

  -- -------- -- ---------
     @xmath      (5.115)
  -- -------- -- ---------

where @xmath denotes the matrix that is obtained by contracting together
the @xmath tensors. Here the sum is over all values of @xmath and @xmath
that are compatible with a given value of @xmath .

#### 5.4.3 Tensor product of @xmath irreps

In a similar way, we can consider the tensor product of four irreps;
different choices of a coupled basis, corresponding to different fusion
trees, are related by the @xmath symbols and so on.

More generally, let us consider the tensor product of @xmath
representations, Eq. ( 5.55 ), where each space @xmath ( @xmath )
transforms as an irrep @xmath . A coupled basis can be labeled by a
fusion tree @xmath and the set of intermediate irreps @xmath that are
assigned to the internal links of @xmath . We denote by

  -- -------- -- ---------
     @xmath      (5.116)
  -- -------- -- ---------

such a basis. By attaching the appropriate Clebsch-Gordan tensor @xmath
to each node of @xmath and contracting the resulting tree tensor network
we can obtain tensors

  -- -------- -- ---------
     @xmath      (5.117)
  -- -------- -- ---------

that mediate the change from the product basis to this coupled basis.

Another coupled basis @xmath corresponding to a different fusion tree
@xmath is related to the basis ( 5.116 ) by the transformation

  -- -------- -- ---------
     @xmath      (5.118)
  -- -------- -- ---------

where the coefficients @xmath can be expressed in terms of the
recoupling coefficients [Eq. ( 5.107 )].

As an example, consider two different ways of coupling four spins @xmath
and @xmath according to the fusion trees @xmath and @xmath that are
shown in Fig. 5.10 (ii). The two coupled bases are related by
coefficients @xmath that are defined according to the equality depicted
in Fig. 5.13 . Note that the tensor network made of Clebsch-Gordan
tensors, shown in Fig. 5.13 , is an instance of a spin network . In this
case, the spin network has two open links and can therefore be regarded
as an SU(2)-invariant operator. The equality in the figure then simply
depicts that the spin network is proportional to the Identity. The
numerical value of the coefficient @xmath can be calculated without
contracting the spin network, but by instead following a procedure
called evaluating a spin network. Section 5.7 illustrates with simple
examples the procedure to evaluate a spin network corresponding to a
generic coefficient @xmath . For instance, it is shown that @xmath can
be expressed in terms of two recoupling coefficients,

  -- -------- -- ---------
     @xmath      (5.119)
  -- -------- -- ---------

#### 5.4.4 Tensor product of @xmath reducible representations

Finally, consider the tensor product of @xmath reducible
representations. A coupled basis, labeled by a given fusion tree, is
related to the product basis by means of a transformation that is
obtained by attaching a tensor @xmath to each node of the fusion tree,
and contracting the resulting tree tensor network.

Two different choices of a coupled basis, corresponding to two different
fusion trees @xmath and @xmath , are related by a matrix @xmath . The
matrix @xmath is a generalization of the matrix with the same name that
appears Eq. ( 5.114 ). This matrix is obtained by contracting a tensor
network [e.g. Fig. 5.14 ] made of tensors @xmath and tensors @xmath .

For a fixed value of the total spin @xmath , matrix @xmath decomposes in
terms of degeneracy matrices @xmath . The components of the latter can
be expressed in terms of recoupling coefficients by generalizing Eq. (
5.115 ). We obtain,

  -- -------- -- ---------
     @xmath      (5.120)
  -- -------- -- ---------

where the sum runs over all spin labels but excluding @xmath .

### 5.5 Block structure of SU(2)-invariant tensors

In this section we consider tensors that are invariant under the action
of the symmetry. We explain how such tensors decompose into a compact
canonical form which exploits their symmetry. The canonical form can be
understood as a block structure in the tensor components. In Sec. 5.6 we
then adapt the set @xmath of primitive tensor network manipulations to
work in this form. With the formalism of SU(2)-invariant tensors at hand
we then consider tensor network decompositions made of SU(2)-invariant
tensors in Sec. 6.1 .

#### 5.5.1 SU(2)-invariant tensors

Consider a rank- @xmath tensor @xmath with indices @xmath and directions
@xmath . Each index @xmath is associated with a vector space @xmath on
which SU(2) acts by means of transformations @xmath .

Also consider the action of SU(2) on the space @xmath given by

  -- -------- -- ---------
     @xmath      (5.121)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (5.122)
  -- -------- -- ---------

( @xmath denotes the complex conjugate of @xmath .) That is, @xmath acts
differently depending on whether index @xmath is an incoming or outgoing
index. We then say that tensor @xmath is SU(2) invariant if it is
invariant under the transformation of Eq. ( 5.121 ). In components we
have

  -- -------- -- ---------
     @xmath      (5.123)
  -- -------- -- ---------

for all @xmath .

In the remainder of this section we explore the consequences of the
constraints in Eq. ( 5.123 ). The main result is as follows. By writing
each index @xmath of the tensor in a spin basis, @xmath , the tensor is
revealed to have a block structure, namely, the non-trivial components
are organized into blocks that are supported on orthogonal subspaces.
For a given value of spin @xmath , the index @xmath splits into a
degeneracy index @xmath and a spin index @xmath . An SU(2)-invariant
tensor @xmath decomposes into a set of degeneracy tensors , denoted by
@xmath and carrying all the degeneracy indices, and a set of structural
tensors, denoted @xmath , carrying all the spin indices. The degeneracy
tensors contain all the degrees of freedom and correspond to the
‘blocks’ alluded above. On the other hand, the structural tensors are
completely determined by the symmetry since they can be factorized into
a trivalent tree tensor network made of Clebsch-Gordan coefficients.
Examples of structural tensors include Eq. ( 5.117 ), however, a
structural tensor may not generally decompose according to a fusion
tree. We refer to the decomposition @xmath as the canonical
decomposition or the canonical form of tensor @xmath . The main benefit
of the canonical form lies in the fact that @xmath can be specified
compactly by means of only the degeneracy tensors.

In the ensuing discussion we describe the canonical decomposition of
SU(2)-invariant tensors on a case by case basis. We explicitly describe
the canonical form of SU(2)-invariant tensors with one to three indices.
The canonical form in these cases is unique up to overall numerical
factors. On the other hand, an SU(2)-invariant tensor with four or more
indices can be decomposed in several equivalent ways. We illustrate this
with examples without resorting to a complete theoretical
characterization of the canonical form in all cases. A more rigorous
characterization is developed Chapter 6 where we consider a special
canonical form of SU(2)-invariant tensors, namely, tree decompositions.
A tree decomposition corresponds to decomposing both the degeneracy
tensors and the structural tensors according to a fusion tree. We find
this decomposition more convenient from an implementation point of view.
In Chapter 6, we also describe how to construct a tree decomposition for
any SU(2)-invariant tensor, how two different tree decompositions of the
same tensor are related to one another, and how primitive tensor
manipulations are adapted to tree decompositions.

#### 5.5.2 One index

An SU(2)-invariant tensor @xmath with an outgoing index @xmath fulfills
the constraint [Fig. 5.15 (i)]

  -- -------- -- ---------
     @xmath      (5.124)
  -- -------- -- ---------

where @xmath is the representation of SU(2) on the vector space
associated to index @xmath .

Let us now write index @xmath in the spin basis @xmath . Then we have

  -- -------- -- ---------
     @xmath      (5.125)
  -- -------- -- ---------

where @xmath , shorthand for @xmath , encodes the non-trivial components
of @xmath . Since the only relevant irrep on the one index is @xmath the
structural tensors are trivial. Therefore, tensor @xmath can be stored
compactly as @xmath .

On the other hand, an SU(2)-invariant tensor @xmath with an incoming
index @xmath fulfills

  -- -------- -- ---------
     @xmath      (5.126)
  -- -------- -- ---------

or equivalently

  -- -------- -- ---------
     @xmath      (5.127)
  -- -------- -- ---------

where @xmath and @xmath are the complex conjugate and adjoint of @xmath
respectively. The canonical form of @xmath is the same as that stated as
Eq. ( 5.125 ).

#### 5.5.3 Two indices

An SU(2)-invariant matrix @xmath , possibly rectangular, with indices
@xmath and @xmath fulfills [Fig. 5.16 (i)]

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (5.128)
  -- -------- -------- -- ---------

where @xmath and @xmath are the representations of SU(2) on the vector
space associated to index @xmath and @xmath respectively. Schur’s Lemma
establishes that the matrix @xmath decomposes as

  -- -------- -- ---------
     @xmath      (5.129)
  -- -------- -- ---------

which can also be written in a block-diagonal form,

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (5.130)
  -- -------- -------- -- ---------

Here the sum is over all values @xmath of spin @xmath that are equal to
a value of spin @xmath .

A rank- @xmath SU(2)-invariant tensor @xmath with both incoming indices
@xmath and @xmath is associated with fusing spins @xmath and @xmath into
a total spin 0. It fulfills [Fig. 5.16 (ii)]

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (5.131)
  -- -------- -------- -- ---------

and decomposes as

  -- -------- -- ---------
     @xmath      (5.132)
  -- -------- -- ---------

Similarly, a rank- @xmath SU(2)-invariant tensor with both outgoing
indices @xmath and @xmath fulfills [Fig. 5.16 (iii)]

  -- -------- -- ---------
     @xmath      (5.133)
  -- -------- -- ---------

and decomposes as

  -- -------- -- ---------
     @xmath      (5.134)
  -- -------- -- ---------

Both incoming (or both outgoing) spins @xmath and @xmath are compatible
with the total spin 0 only for values @xmath of spin @xmath such that
@xmath and for values @xmath of @xmath such that @xmath . Therefore, we
can recast the canonical decompositions of Eqs. ( 5.132 )-( 5.134 ) in a
block-diagonal form,

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (5.135)
  -- -------- -------- -- ---------

where @xmath is a @xmath reverse diagonal matrix with diagonal
components

  -- -------- -- ---------
     @xmath      (5.136)
  -- -------- -- ---------

To summarize, the canonical form of a rank- @xmath SU(2)-invariant
tensor reads

  -- -------- -- ---------
     @xmath      (5.137)
  -- -------- -- ---------

Here @xmath contains the degrees of freedom of @xmath that are not fixed
by the symmetry, namely, @xmath transforms trivially under the action of
the SU(2), Eq. ( 5.29 ). On the other hand @xmath is determined by the
symmetry according to the directions @xmath of indices @xmath and @xmath
,

  -- -------- -------- -- ---------
     @xmath   @xmath      (5.138)
              @xmath      (5.139)
  -- -------- -------- -- ---------

Thus, a rank- @xmath SU(2)-invariant tensor @xmath can be stored
compactly as

  -- -------- -- ---------
     @xmath      (5.140)
  -- -------- -- ---------

Example 10: Consider a rank- @xmath SU(2)-invariant tensor @xmath with
both outgoing indices and with each index associated to the vector space
@xmath of Example 8, @xmath . Tensor @xmath has the canonical form

  -- -------- -- ---------
     @xmath      (5.141)
  -- -------- -- ---------

where

  -- -------- -------- -- ---------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (5.142)
  -- -------- -------- -- ---------

The total number of complex coefficients contained in tensor @xmath is
@xmath . However, the tensor can be stored compactly as

  -- -------- --
     @xmath   
  -- -------- --

where the total number of complex coefficients that are contained in
tensors @xmath and @xmath is

  -- -------- -- ---------
     @xmath      (5.143)
  -- -------- -- ---------

Therefore, by exploiting the symmetry the number of coefficients that
need to be stored is twenty times smaller.

#### 5.5.4 Three indices

Consider a rank- @xmath SU(2)-invariant tensor @xmath with incoming
indices @xmath and @xmath and outgoing index @xmath . It fulfills [Fig.
5.17 (i)]

  -- -------- -------- -- ---------
     @xmath   @xmath      
                          (5.144)
  -- -------- -------- -- ---------

where @xmath and @xmath are the representations of SU(2) on indices
@xmath and @xmath respectively. The Wigner-Eckart theorem establishes
that @xmath decomposes as

  -- -------- -- ---------
     @xmath      (5.145)
  -- -------- -- ---------

That is, for compatible values of the spins @xmath and @xmath , tensor
@xmath factorizes into tensor @xmath containing degrees of freedom and a
Clebsch-Gordan tensor that mediates the fusion of spins @xmath and
@xmath into spin @xmath .

An SU(2)-invariant tensor @xmath with another combination of incoming
and outgoing indices has a canonical decomposition that differs in the
Clebsch-Gordan coefficients. For example, if @xmath is an
SU(2)-invariant tensor with incoming indices @xmath and outgoing indices
@xmath and @xmath then it fulfills

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (5.146)
  -- -------- -------- -- ---------

and decomposes as

  -- -------- -- ---------
     @xmath      (5.147)
  -- -------- -- ---------

More generally, a rank- @xmath SU(2)-invariant tensor with any
combination of incoming and outgoing indices decomposes as

  -- -------- -- ---------
     @xmath      
     @xmath      (5.148)
  -- -------- -- ---------

The block structure can be made more explicit by recasting Eq. ( 5.148 )
as

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (5.149)
  -- -------- -------- -- ---------

where we use the direct sum symbol @xmath to denote that the different
tensors @xmath are supported on orthonormal subspaces of the tensor
product of the spaces associated with indices @xmath and @xmath , and
where the direct sum runs over all compatible values of @xmath and
@xmath . The components @xmath are determined by the directions @xmath
of the indices,

  -- -------- -------- -- ---------
     @xmath   @xmath      (5.150)
     @xmath   @xmath      (5.151)
     @xmath   @xmath      (5.152)
     @xmath   @xmath      (5.153)
     @xmath   @xmath      (5.154)
     @xmath   @xmath      (5.155)
     @xmath   @xmath      (5.156)
     @xmath   @xmath      (5.157)
  -- -------- -------- -- ---------

where @xmath .

To summarize, a rank- @xmath SU(2)-invariant tensor @xmath can be stored
in the most compact way as

  -- -------- -- ---------
     @xmath      (5.158)
  -- -------- -- ---------

where the indices @xmath and @xmath are specified in the spin basis,

  -- -------- -- ---------
     @xmath      (5.159)
  -- -------- -- ---------

Example 11: Consider a rank- @xmath SU(2)-invariant tensor @xmath such
that each index, @xmath and @xmath , is associated to the vector space
@xmath of Example 8. Tensor @xmath can be stored by storing the
degeneracy tensors,

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

corresponding to all compatible values of @xmath and @xmath .

The total number of complex coefficients that are contained in the
degeneracy tensors is @xmath , whereas @xmath components; the reduction
in the number of coefficients is seventy-five times, much greater than
that computed for rank- @xmath tensors in Example 10. In general, the
sparsity of SU(2)-invariant tensors increases with increasing number of
indices.

#### 5.5.5 Four indices

A rank- @xmath SU(2)-invariant tensor may be decomposed in several ways
in terms of degeneracy tensors and structural tensors in correspondence
with the existence of different fusion trees for four spins.

Consider a rank- @xmath SU(2)-invariant tensor @xmath with incoming
indices @xmath and @xmath and outgoing index @xmath . It fulfills

  -- -------- -- ---------
     @xmath      (5.160)
  -- -------- -- ---------

where @xmath and @xmath are the representations of SU(2) on the indices
@xmath and @xmath respectively.

Tensor @xmath decomposes as

  -- -------- -------- -- ---------
     @xmath   @xmath      (5.161)
  -- -------- -------- -- ---------

where the sum is over all values of the intermediate spin @xmath .
The coefficients @xmath [Eq. ( 5.104 )] mediate the fusion of the spins
@xmath and @xmath into a total spin @xmath according to a fusion tree,
for example, first fusing @xmath and @xmath and then fusing the
resulting spin with @xmath .

Alternatively, tensor @xmath can be decomposed as

  -- -------- -------- -- ---------
     @xmath   @xmath      (5.162)
  -- -------- -------- -- ---------

in terms of different structural coefficients @xmath [Eq. ( 5.106 )]
that are associated with fusing the spins according to a different
fusion tree, namely, fusing spin @xmath with the spin obtained by first
fusing @xmath and @xmath .

Since Eqs. ( 5.161 ) and ( 5.162 ) represent the same tensor @xmath ,
the tensors @xmath and @xmath are related by

  -- -------- -- ---------
     @xmath      (5.163)
  -- -------- -- ---------

where @xmath are the recoupling coefficients [Eq. 5.107 ].

#### 5.5.6 @xmath indices

Finally, consider a rank- @xmath SU(2)-invariant tensor @xmath with all
outgoing indices and which fulfills Eq. ( 5.123 ). By writing each index
in a spin basis, @xmath , tensor @xmath can be decomposed as

  -- -------- -- ---------
     @xmath      (5.164)
  -- -------- -- ---------

Here tensor @xmath [Eq. ( 5.117 )] is a transformation characterized by
a fusion tree @xmath whose internal links are decorated by the spins
@xmath . Another canonical form of the tensor @xmath ,

  -- -------- -- ---------
     @xmath      (5.165)
  -- -------- -- ---------

comprises of different degeneracy tensors @xmath and different
structural tensors @xmath where the latter is a transformation
characterized by another fusion tree @xmath .

The two canonical forms, Eq. ( 5.164 ) and ( 5.165 ), are related as

  -- -------- -- ---------
     @xmath      (5.166)
  -- -------- -- ---------

where the coefficients @xmath are those which appear in Eq. ( 5.118 ).

Thus tensor @xmath can be compactly stored as

  -- -------- -- ---------
     @xmath      (5.167)
  -- -------- -- ---------

For an arbitrary combination of incoming and outgoing indices of the
tensor, the canonical decomposition is characterized by intermediate
spins @xmath that are assigned to the links of a trivalent tree that is
more general than the fusion tree. Furthermore, two canonical
decompositions are related by means of more generic spin networks than
those considered (See Section 5.7 ) for evaluating the coefficients
@xmath . A rigorous result is presented in Chapter 6 where we describe
the generic transformation that relates two tree decompositions of an
SU(2)-invariant tensor.

### 5.6 Manipulations of SU(2)-invariant tensors

In this section we consider manipulations of SU(2)-invariant tensors
that belong to the set @xmath [Sec. 5.1.5 ] of primitives: reversal of
indices, permutation of indices, reshaping of indices and matrix
operations (matrix multiplication and matrix factorizations). We will
adapt these manipulations to the presence of the symmetry by
implementing them in such a way that the canonical form is maintained.

Our approach will be to describe the basic transformations that are
instrumental in implementing the symmetric version of these
manipulations and demonstrate their use with simple examples. A more
rigorous treatment of adapting the primitive tensor manipulations for
SU(2)-invariant tensors is presented in Chapter 6. The basic
transformations are symmetry preserving and can be described by means of
specical SU(2)-invariant tensors. Consequently, a symmetric manipulation
decomposes into the manipulation of the degeneracy tensors and the
manipulation of the structural tensors. Computational cost is incurred
only by the manipulation of degeneracy tensors. On the other hand, the
manipulation of structural tensors can be performed algebraically by
applying relevant properties of Clebsch-Gordan coefficients. This fact
is responsible for obtaining computational speedup from exploiting the
symmetry.

#### 5.6.1 Reversal of indices

An index @xmath of an SU(2)-invariant tensor can be reversed by means of
the ‘cup’ and ‘cap’ transformations. The cup transformation is given by
a rank- @xmath SU(2)-invariant tensor @xmath with both incoming indices.
It can be used to reverse an outgoing index. Analogously, the cap
transformation is given by a rank- @xmath SU(2)-invariant tensor @xmath
with both outgoing indices and can be used to reverse an incoming index.

In the canonical form, the cup and cap tensors read as block-diagonal
matrices,

  -- -------- -------- -- ---------
     @xmath   @xmath      (5.168)
     @xmath   @xmath      (5.169)
  -- -------- -------- -- ---------

where @xmath and where

  -- -------- -------- -- ---------
     @xmath   @xmath      (5.170)
     @xmath   @xmath      (5.171)
  -- -------- -------- -- ---------

Here @xmath is the tensor defined in Eq. ( 5.136 ). By definition, the
cup transformation inverts the action of the cap transformation and
vice-versa,

  -- -------- -- ---------
     @xmath      (5.172)
  -- -------- -- ---------

Reversal of index @xmath of tensor @xmath can be decomposed into the
reversal of the degeneracy index @xmath of the degeneracy tensors and
reversal of the spin index @xmath of the structural tensors. Reversal of
the degeneracy index is trivial since the cup and cap transformations
act as the Identity @xmath on it whereas the reversal of the spin index
is mediated by transformations @xmath and @xmath .

Figure 5.19 (i)-(ii) introduces a graphical representation of the cup
and cap tensors. The cup tensor is depicted as a small circle with two
incoming lines (forming a ‘cup’) whereas a cap tensor is depicted as a
small circle with two outgoing lines (forming a ‘cap’) ( Baez, ;
Biamonte et al., 2010 ) .

Next, we illustrate how an outgoing index of a tensor can be reversed by
means of the cup transformation. A cap transformation can be used to
reverse an incoming index in an analogous way.

Example 12: Consider a rank- @xmath SU(2)-invariant tensor @xmath with
outgoing indices
@xmath and @xmath and which is given in the canonical form,

  -- -------- -- ---------
     @xmath      (5.173)
  -- -------- -- ---------

where @xmath assumes all values of @xmath that are equal to a value of
@xmath . Consider reversing index @xmath of @xmath as shown in Fig. 5.20
(i). The resulting tensor (or matrix) @xmath is obtained by multiplying
tensor @xmath with a cup by contracting @xmath . We follow the
convention that multiplying with a cup corresponds to bending index
@xmath upwards from the left in the graphical representation. The same
index can be bent upwards from the right by multiplying with the
transpose of the cup.

The resulting matrix @xmath has the canonical form

  -- -------- -- ---------
     @xmath      (5.174)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (5.175)
  -- -------- -- ---------

In order to explain this expression consider Fig. 5.20 (ii) where the
reversal is depicted as it is performed on the canonical form of @xmath
. Reversal of the spin index equates to replacing the shaded region by a
straight line. This corresponds to applying the following algebraic
identity

  -- -------- -- ---------
     @xmath      (5.176)
  -- -------- -- ---------

The factor @xmath is absorbed into the degeneracy tensor @xmath , Eq. (
5.175 ), to obtain the final canonical form.

Example 13: Consider a rank- @xmath SU(2)-invariant tensor @xmath which
is given in the canonical form,

  -- -------- -- ---------
     @xmath      (5.177)
  -- -------- -- ---------

Consider reversing index @xmath of tensor @xmath as shown in Fig. 5.20
(iii) by multiplying @xmath with a cup such that @xmath is contracted.
The canonical form of @xmath reads

  -- -------- -- ---------
     @xmath      (5.178)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (5.179)
  -- -------- -- ---------

The recoupling coefficient @xmath appears due to the reversal of the
spin index @xmath , as shown in Fig. 5.20 (iv). The Clebsch-Gordan
tensor and the cup within the shaded region are replaced with another
Clebsch-Gordan tensor and a recoupling coefficient. This is achieved by
applying a resolution of Identity on spins @xmath and @xmath and
simplifying the resulting diagram by applying the equality shown in Fig.
5.11 (ii).

The procedure of reversing the spin index illustrated in Example 13 can
be applied to reverse a spin index of a generic rank- @xmath
SU(2)-invariant tensor. Recall that a structural tensor is maintained as
a trivalent tree of Clebsch-Gordan tensors. Reversal of the spin index
corresponds to multiplying a cup with a Clebsch-Gordan tensor within
this tree. Then, as in Example 13, we proceed by replacing the
Clebsch-Gordan tensor and the cup by another Clebsch-Gordan tensor and a
recoupling coefficient. The recoupling coefficient is absorbed into the
degeneracy tensor to obtain the canonical form of the resulting tensor.
In this way we can reverse an index of a generic rank- @xmath
SU(2)-invariant tensor.

#### 5.6.2 Permutation of indices

Let us focus on the swap of two adjacent indices of an SU(2)-invariant
tensor. As mentioned in Sec. 5.1.2 an arbitrary permutation of indices
can be applied as a sequence of a number of such swaps.

Consider the swap e.g. Eq. ( 5.4 ) of two adjacent indices of a rank-
@xmath SU(2)-invariant tensor @xmath that is given in the canonical
form,

  -- -------- --
     @xmath   
  -- -------- --

Then tensor @xmath that is obtained as result of swapping indices @xmath
and @xmath has the canonical form

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- -- ---------
     @xmath      (5.180)
  -- -------- -- ---------

Here @xmath is a rank- @xmath SU(2)-invariant tensor with components
@xmath ,

  -- -------- -- ---------
     @xmath      (5.181)
  -- -------- -- ---------

which mediates the swap of the spin indices @xmath and @xmath that fuse
into index @xmath , see Fig. 5.21 . That is,

  -- -------- -- ---------
     @xmath      (5.182)
  -- -------- -- ---------

(The same tensor @xmath also relates, in a similar way, tensor @xmath
and tensor @xmath .)

When swapping two adjacent indices of a generic rank- @xmath tensor the
degeneracy tensors @xmath and @xmath are also related directly by the
swap tensor @xmath , such as in Eq. ( 5.180 ), if we work in a canonical
form in which the indices that are swapped belong to the same node of
the trivalent tree that characterizes the canonical form.

Notice how the canonical form of an SU(2)-invariant tensor facilitates a
computational speedup for permutation of indices: permuting indices of
the tensor is reduced to permuting indices of the much smaller
degeneracy tensors. Figure 5.25 illustrates the computational speedup
corresponding to a permutation of indices performed using our reference
implementation MATLAB. In this implementation permutation of several
indices is performed without necessarily breaking the permutation into
swaps, see Sec. 6.0.4 in Chapter 6.

One can also consider manipulations that involve both reversing indices
and permuting them. In this context it is useful to note that these
manipulations “commute” with one another, as illustrated in Fig. 5.22 .

#### 5.6.3 Reshape of indices

The transformation that implements the reshape of indices of an
SU(2)-invariant tensor depends on the directions of the indices. We
analyze three distinct cases. First, we consider fusion of two outgoing
indices into an outgoing index and splitting of an outgoing index into
two outgoing indices. Second, we consider the analogous reshape of
incoming indices. And third, we consider the fusion of an incoming index
with an outgoing index.

Let us consider the fusion e.g. Eq. ( 5.5 ) of two outgoing indices of
an SU(2)-invariant tensor @xmath . In order to obtain the reshaped
tensor @xmath in a canonical form it is required that the fused index be
maintained in the spin basis. However, the direct product of indices
@xmath may result in an index that does not label a spin basis.
Therefore, we fuse indices by multiplying @xmath with the fusing tensor
@xmath such that indices @xmath and @xmath are contracted [Fig. ( 5.23
(i))],

  -- -------- -- ---------
     @xmath      (5.183)
  -- -------- -- ---------

or in the canonical form [Fig. 5.23 (ii)] ,

  -- -------- -- ---------
     @xmath      (5.184)
  -- -------- -- ---------

Notice that the fusion of the spin indices, here, is straightforward. We
proceed by multiplying with tensor @xmath and replacing the resulting
‘loop’ in the figure with a straight line [Fig. 5.7 (ii)]. The fusion of
two adjacent indices of a generic rank- @xmath SU(2)-invariant tensor
follows a straightforward generalization of Eq. ( 5.186 ). By working in
a canonical form that is characterized by a trivalent tree in which the
two indices belong to the same node, the fusion of the spin indices
involves a simple loop elimination, similar to the one illustrated in
Fig. 5.23 (ii).

The original tensor @xmath can be recovered from @xmath by splitting the
index @xmath back into indices @xmath and @xmath . This is achieved by
multiplying tensor @xmath with the splitting tensor @xmath such that
@xmath is contracted [Fig. 5.23 (iii)],

  -- -------- -- ---------
     @xmath      (5.185)
  -- -------- -- ---------

or in the canonical form [Fig. 5.23 (iv)],

  -- -------- -- ---------
     @xmath      (5.186)
  -- -------- -- ---------

Notice that the sum in Eq. ( 5.186 ) implies that a reshaped tensor
@xmath involves a linear combination of several tensors @xmath . Thus,
performing the fusion in the canonical form requires more work than
reshaping regular indices which is a simple rearrangement of the tensor
components. As a result, fusing indices of SU(2)-invariant tensors can
be more expensive than fusing indices of regular tensors, as illustrated
in Fig. 5.25 for a reshaping done in MATLAB.

Next, let us consider fusing two incoming indices into a single incoming
index. This can be done in one of two equivalent ways. The first
involves multiplying the tensor with a splitting tensor by contracting
the two incoming indices. Equivalently, if one prefers to use the fusing
tensor , one can reverse the two indices, multiply with the fusing
tensor, and finally reverse the fused index. The two approaches are
depicted in Fig. 5.24 (i). The fused index can be split back into the
original indices by reverting the fusion. In the first approach this is
done by multiplying with a fusing tensor while in the second approach
this is done by multiplying with a splitting tensor and then reversing
the two indices [Fig. 5.24 (ii)].

Finally, consider the fusion of an incoming index with an outgoing index
to produce, say, an outgoing index. This can be achieved by reversing
the incoming index and then fusing the indices by means of a fusing
tensor. The fused index should be split in a consistent manner by
reverting this fusion procedure.

#### 5.6.4 Multiplication of two matrices

Let @xmath and @xmath be two SU(2)-invariant matrices given in the
canonical form

  -- -------- -- ---------
     @xmath      (5.187)
  -- -------- -- ---------

Then the SU(2)-invariant matrix @xmath obtained by multiplying together
matrices @xmath and @xmath has the canonical form

  -- -------- -- ---------
     @xmath      (5.188)
  -- -------- -- ---------

where @xmath is obtained by multiplying matrices @xmath and @xmath ,

  -- -------- -- ---------
     @xmath      (5.189)
  -- -------- -- ---------

Clearly, computational gain is obtained as a result of performing the
multiplication @xmath block-wise. This is illustrated by the following
example.

Example 13 : (Computational gain from blockwise multiplication) Consider
vector space @xmath that decomposes as @xmath where @xmath assumes
values @xmath and let @xmath . The dimension of the space @xmath is
@xmath where @xmath .

Consider an SU(2)-invariant matrix @xmath . Since there are @xmath
blocks @xmath and each block has size @xmath , the SU(2)-invariant
matrix @xmath contains @xmath coefficients. For comparison, a regular
matrix of the same size contains @xmath coefficients, a number greater
by a factor of @xmath .

Let us now consider multiplying two such matrices. We use an algorithm
that requires @xmath computational time to multiply two matrices of size
@xmath . The cost of performing @xmath multiplications of @xmath blocks
in Eq. 5.189 scales as @xmath . In contrast the cost of multiplying two
regular matrices of the same size scales as @xmath , requiring @xmath
times more computational time. Figure 5.26 shows a comparison of the
computation times when multiplying two matrices for both SU(2)-invariant
and regular matrices.

#### 5.6.5 Factorization of a matrix

The factorization of an SU(2)-invariant matrix @xmath can also benefit
from the block-diagonal structure. Consider, for instance, the singular
value decomposition (SVD), @xmath , where @xmath and @xmath are unitary
matrices and @xmath is a diagonal matrix with non-negative components.
If @xmath has the canonical form

  -- -------- -- ---------
     @xmath      (5.190)
  -- -------- -- ---------

we can obtain the SU(2)-invariant matrices

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

by performing SVD of each degeneracy matrix @xmath independently,

  -- -------- -- ---------
     @xmath      (5.191)
  -- -------- -- ---------

A different factorization of @xmath , such as spectral decomposition or
polar decomposition, can be obtained by the analogous factorization of
the blocks @xmath . The computational savings are analogous to those
described in Example 13 for the multiplication of matrices. Figure 5.26
shows a comparison of computation times required to perform a singular
value decomposition on SU(2)-invariant and regular matrices using
MATLAB.

### 5.7 Supplement: Examples of evaluating a spin network

Let us consider a spin network @xmath that is constructed by means of
two fusion trees @xmath and @xmath in the following way. First obtain a
tree tensor network @xmath by attaching a tensor @xmath to each node the
fusion tree @xmath . A tensor @xmath mediates the fusion of the incoming
spins into the outgoing spin. Next, obtain the splitting tree that is
dual to @xmath . A splitting tree is obtained by reversing the direction
of all links of a fusion tree. In the graphical representation this
corresponds to a horizontal reflection of the fusion tree. Then obtain a
tree tensor network @xmath by attaching to each node of the splitting
tree a tensor @xmath that mediates the splitting of the incoming spin
into outgoing spins. The spin network @xmath is obtained by connecting
the open links of the two tree tensor networks: @xmath and @xmath .

Since the spin network @xmath has two open links it can be contracted to
obtain an SU(2)-invariant matrix, which according to Schur’s lemma is
proportional to the Identity. An important property of @xmath is that
this proportionality factor can be evaluated algebraically without
contracting the spin network. This property can be exploited to suppress
the potentially high cost of contracting spin networks in numerical
simulations.

The spin network @xmath can be evaluated in terms of the values of basic
spin networks that are shown in Fig. 5.7 (ii) and Fig. 5.11 (ii). The
first step of the evaluation procedure generally involves expressing the
spin network as a composition of these basic spin networks. This can be
achieved by applying, possibly several times, a resolution of Identity,
Fig. 5.7 (i), on appropriate links of the spin network. Then one
proceeds by recursively applying the equalities in Fig. 5.7 (ii) and
Fig. 5.11 (ii) to regions of the spin network, eventually replacing the
spin network with a straight line and an overall numerical factor.
Figure 5.27 (i) illustrates these steps for the simple case of
evaluating the spin network of Fig. 5.13 in terms of two recoupling
coefficients.

We can also consider spin networks that have intercrossing lines such as
those which appear when applying permuting indices of an SU(2)-invariant
tensor. Such a spin network can be evaluated in terms of recoupling
coefficients and swap factors, as illustrated in Fig. 5.27 (ii).

## Chapter 6 Implementation of non-Abelian symmetries \@slowromancapii@

In this chapter we will describe a specific scheme for implementing
non-Abelian symmetries. Our implementation is based on a special
canonical form, the tree decomposition , of symmetric tensors.

A tree decomposition of a symmetric tensor corresponds to representing
and storing the tensor as a tree tensor network made of two parts: (i) a
symmetric vector and (ii) possibly several splitting tensors @xmath . We
describe how to implement the tensor manipulations within the set @xmath
of primitive operations based on tree decompositions. In a tree
decomposition, reshape and permutation of indices take a very simple
form. In order to obtain the vector of the output tensor, one needs to
simply multiply the vector of the initial tree decomposition with a
matrix @xmath [Fig 5.13 (ii)] that depends only on the permutation or
reshape.

This approach offers several advantages. Reshapes and permutations can
be performed without breaking them into pairwise fusions and swaps, as
was described in the previous chapter. More importantly, one can
precompute (that is, compute before running the algorithm) the matrices
@xmath since these do not depend on the actual components of the tensor
being manipulated. This is of special advantage in the case of iterative
algorithms, where by pre-computing these matrices one also eliminates
the cost of evaluating spin networks at runtime, thus substantially
reducing the computational costs.

#### 6.0.1 Tree decompositions of SU(2)-invariant tensors

Consider a rank- @xmath SU(2)-invariant tensor @xmath with indices
@xmath and directions @xmath . Let us apply the following
transformations on tensor @xmath to obtain a vector. First reverse all
incoming indices of @xmath to obtain another tensor @xmath . Then fuse
the indices of @xmath according to a given fusion tree @xmath to obtain
an SU(2)-invariant vector @xmath . This gives rise to a decomposition of
tensor @xmath in terms of the vector @xmath , a set of splitting tensors
that revert the fusion sequence @xmath , and a set of cup tensors that
reverse the split indices that are identified with the incoming indices
of @xmath . We refer to such a decomposition as a tree decomposition of
@xmath and denote it as @xmath . It is completely specified by the
following list of elements:

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

Here the fusion tree @xmath determines the splitting tensors that are
part of the decomposition while the directions @xmath indicate the
presence or absence of a cup tensor on the open indices of the tree
decomposition.

A tree decomposition of a rank- @xmath SU(2)-invariant tensor is shown
in Fig. 6.1 . The tree decomposition in the diagram can be specified as,

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

In the graphical representation of a tree decomposition @xmath the
vector @xmath appears at the top of the tree, the ‘body’ of the tree
comprises of splitting tensors that are connected according to the
fusion tree @xmath and the indices @xmath are associated, from left to
right, to the open lines at the bottom of the tree. Some open indices
are bent upwards by attaching cup tensors. A value @xmath indicates a
cup tensor is attached to index @xmath while @xmath indicates its
absence. We additionally denote by @xmath that the transpose of a cup
tensor is attached to index @xmath . In the graphical representation,
the values @xmath or @xmath correspond to bending the index @xmath
upwards from the left or from the right respectively.

The vector @xmath is obtained by applying an resolution of Identity,
denoted @xmath , on tensor @xmath , as shown on the r.h.s. of Fig. 6.1 .
The resolution of Identity @xmath is given by a tensor network made of a
set of fusing tensors, that fuse the indices of @xmath according to the
fusion tree @xmath , and the corresponding set of splitting tensors that
inverts the fusion. The vector @xmath is obtained by contracting @xmath
with the fusing tensors.

We emphasize that the cup tensors are stored as part of the tree
decomposition without consuming them into the tree. This is done to
simplify reshape and permutation of indices of a tree decomposition
since these operations can be performed without noticing the cup
tensors. For instance, in order to permute the open indices of a tree
decomposition one may proceed by detaching any cup tensors from the
tree, permuting the indices and re-attaching the cup tensors to the
updated tree, a direct application of the commutation property depicted
in Fig. 5.22 . On the other hand, manipulations that involve summing
over an index that is attached to a cup tensor are an exception. For
example, when multiplying two SU(2)-invariant matrices, each given as a
tree decomposition, the cup tensor has to be properly considered to
obtain the resultant matrix. This is discussed in Sec. 6.0.6 .

#### 6.0.2 Mapping between tree decompositions

The same tensor @xmath may be expressed in different tree decompositions
corresponding to different choices of the fusion tree. Two different
fusion trees @xmath and @xmath lead to two different tree decompositions
@xmath and @xmath of the same tensor @xmath . As an example we show two
different but equivalent tree decompositions of a rank- @xmath tensor in
Fig. 6.2 (i). The two decompositions

  -- -------- --
     @xmath   
  -- -------- --

are obtained by applying on the tensor the resolutions of Identity
@xmath and @xmath respectively that are separately depicted in Fig. 6.2
(ii).

Suppose now that we have a tensor @xmath in a tree decomposition @xmath
and we wish to transform it into another tree decomposition @xmath . We
find it convenient to obtain the vector @xmath in steps, as shown in
Fig. 6.3 . First detach all cup tensors from @xmath and apply the
resolution of Identity @xmath on the open indices of the tree. Then
contract the splitting tensors in @xmath and the fusing tensors in
@xmath to obtain a matrix @xmath . The new vector @xmath can be obtained
by multiplying @xmath with the matrix @xmath ,

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

Thus, the matrix @xmath can be used to map from one tree decomposition
of an SU(2)-invariant tensor into another tree decomposition of the same
tensor. Recall that the components of @xmath can be expressed in terms
of recoupling coefficients [Fig. 5.14 , Eq. ( 5.120 )].

To summarize the above procedure we define a template routine NEWTREE
that takes as input a tree decomposition @xmath and a fusion tree @xmath
and returns the tree decomposition @xmath of the tensor. The routine
reads

  -- -------- -- -------
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.4)
  -- -------- -- -------

(where @xmath denotes ‘stored as’.)

Recall that the matrix @xmath is sparse [Eq. ( 5.115 )]. It can be shown
that the matrix-vector multiplication, Eq. ( 6.3 ), can be performed
with a cost that is @xmath by means of sparse multiplication.

Next we describe how manipulations in the set @xmath of primitive tensor
manipulations [Sec. 5.1.5 ] are performed on tree decompositions.
Consider an SU(2)-invariant tensor @xmath that has been given as a tree
decomposition @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath denote the SU(2)-invariant tensor that is obtained from
tensor @xmath as a result of a manipulation in @xmath . Also, let @xmath
denote a tree decomposition of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We will describe how the components of vector @xmath are determined
systematically in terms of components of the vector @xmath .

#### 6.0.3 Reversal of indices

Reversal of an index of a tree decomposition is trivial since the cup
tensors are stored as part of the tree decomposition. It corresponds to
attaching a cup (or its transpose) to the corresponding open index of
the tree in case the index is outgoing or detaching the cup from the
index in case it is incoming. This simple procedure is summarized in the
following template routine which describes reversal of possibly several
indices of tensor @xmath according to new directions @xmath provided as
input. No computation is involved in the procedure. Only information
pertaining to the directions of indices is updated,

  -- -------- -- -------
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
                 
     @xmath      
     @xmath      (6.5)
  -- -------- -- -------

#### 6.0.4 Permutation of indices

The procedure to permute indices of a tree decomposition is illustrated
in Fig. 6.4 (i). We consider a rank- @xmath SU(2)-invariant tensor with
indices @xmath and apply a permutation p ,

  -- -------- --
     @xmath   
  -- -------- --

The permutation is depicted by intercrossing index @xmath and index
@xmath of the tree. This crossing can be ‘absorbed’ into the tree by
applying the resolution of Identity @xmath on the tree. In order to
determine the vector @xmath we contract the splitting tensors in @xmath
and the fusing tensors in @xmath to obtain a matrix @xmath , which is
then multiplied with the initial vector @xmath ,

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

Clearly, the above procedure can be employed to apply any permutation p
on the indices of a rank- @xmath SU(2)-invariant tensor. Matrix @xmath
is a generalization of matrix @xmath [e.g. Fig. 5.14 ] in that it
additionally includes a permutation of indices. The latter can then be
seen as a special instance of @xmath with a trivial permutation of
indices. The components of the degeneracy part @xmath of matrix @xmath
are given by Eq. ( 5.120 ) where coefficients @xmath can be expressed in
terms of recoupling coefficients and swap factors (see Sec. 5.7 ).

Finally, note that the cup tensors do not play any role in the
permutation since reversal of indices commutes with a permutation of
them [Fig. 5.22 ]. In practice, all cup tensors can be detached from the
tree before applying the permutation and then re-attached to the updated
tree.

The procedure to permute indices of a tree decomposition is summarized
in the following template routine:

  -- -------- -- -------
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.7)
  -- -------- -- -------

#### 6.0.5 Reshape of indices

Consider fusing a pair of adjacent indices @xmath and @xmath of the tree
decomposition @xmath . Let us suppose that indices @xmath and @xmath do
not carry cup tensors and also that they belong to the same node in
@xmath . Indices @xmath and @xmath can be fused into an index @xmath by
applying the tensor @xmath and using the equality shown in Fig. 5.9 (i)
to immediately obtain the tree decomposition @xmath . This is
illustrated in Fig. 6.5 (i). Note that the final vector @xmath is the
same as the initial vector @xmath . The updated fusion tree @xmath can
be obtained from @xmath by deleting the node @xmath from @xmath . We
denote this as,

  -- -------- --
     @xmath   
  -- -------- --

The original tree decomposition may be recovered from @xmath by
splitting index @xmath back into indices @xmath and @xmath . This
operation is again straightforward since it does not involve a
computation of vector components, as illustrated in Fig. 6.5 (ii). The
original fusion tree @xmath is recovered by concatenating a node to
@xmath ,

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

Now let us consider fusing indices @xmath and @xmath that do not belong
to the same node of @xmath . In this case one can first map @xmath into
another tree decomposition @xmath in which indices @xmath and @xmath
belong to the same node and then proceed with the fusion on the tree
@xmath as described above. This can be done by applying the procedure
NEWTREE ( 6.4 ) with inputs @xmath and the desired fusion tree.

Consider the template routine FUSE that fuses indices according to a set
of disjoint fusion trees @xmath where each fusion tree specifies fusion
of a subset of adjacent indices @xmath :

  -- -------- -- -------
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.9)
  -- -------- -- -------

Notice that here we essentially apply the total fusion at once by
concatenating the input fusion trees @xmath into a single tree and then
applying the procedure NEWTREE. Consequently, the fusion is carried out
by means of a single matrix-vector multiplication. Furthermore, the
computational cost incurred by the procedure is dominated by the cost of
this step. As mentioned previously, this cost is @xmath .

Also consider the following routine to split indices @xmath of a tree
decomposition @xmath (typically the output of FUSE) by reversing the
fusion sequence encoded in fusion trees @xmath :

  -- -------- -- --------
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.10)
  -- -------- -- --------

Note that no computation of vector components is involved in this
procedure.

Let us now describe how to reshape indices that may carry cup tensors.
First consider the fusion of indices each of which carries a cup tensor.
We proceed by detaching the cup tensors from the indices, applying the
procedure FUSE and finally attaching a cup tensor to each of the fused
indices. Analogously, an index that carries a cup tensor may be split
into two indices, by detaching the cup tensor, applying the procedure
SPLIT and attaching a cup tensor to each of two indices so obtained.

Finally, consider the fusion of an index that carries a cup tensor with
an index that does not carry a cup tensor. The fusion proceeds by
detaching the cup tensor and then applying the procedure FUSE on the
indices. The fusion is to be reversed in a consistent manner by first
applying the procedure SPLIT on the fused index and then attaching a cup
tensor to the originally incoming index.

#### 6.0.6 Matrix multiplication and factorizations

Let us consider how matrix operations are performed on tree
decompositions. Two SU(2)-invariant matrices, each given as a tree
decomposition, may be multiplied together by first obtaining the
matrices in a block-diagonal form (from the respective tree
decompositions), performing a block-wise multiplication (Sec. 5.6.4 )
and recasting the resulting block-diagonal matrix into a tree
decomposition.

An SU(2)-invariant matrix may be factorized e.g. singular value
decomposed in a similar way. One proceeds by obtaining the matrix in a
block-diagonal form, performing block-wise factorization (Sec. 5.6.5 ),
and recasting each of the factor block-diagonal matrices into a tree
decomposition.

In the remainder of the section we explain how a block-diagonal form is
obtained from a tree decomposition and vice-versa.

##### 6.0.6.1 Block-diagonal matrix from the tree decomposition

Consider a tree decomposition @xmath of an SU(2)-invariant matrix @xmath
. The decomposition @xmath comprises of a vector @xmath , a splitting
tensor @xmath and a cup tensor @xmath . We wish to obtain, from @xmath ,
the corresponding block diagonal matrix,

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

This can be achieved by multiplying together the vector, the splitting
tensor and the cup tensor. We perform this multiplication in two simple
steps as shown in Fig. 6.6 . We first multiply @xmath with @xmath to
obtain an intermediate SU(2)-invariant tensor @xmath that takes the
canonical form,

  -- -------- -- --------
     @xmath      (6.12)
  -- -------- -- --------

where the components @xmath of @xmath are given by

  -- -------- -- --------
     @xmath      (6.13)
  -- -------- -- --------

We then multiply (algebraically) tensor @xmath with the cup tensor to
obtain the block-diagonal matrix @xmath to obtain

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

##### 6.0.6.2 Tree decomposition from the block-diagonal matrix

The tree decomposition @xmath can be obtained from the block-diagonal
form ( 6.11 ) in a straightforward manner by reverting the previous
procedure. We first multiply @xmath with a cap tensor to obtain tensor
@xmath . Once again, the outcome of this multiplication follows
algebraically. We obtain

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

We then fuse the indices of @xmath to obtain a vector @xmath ,

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

The tree decomposition @xmath comprises of vector @xmath , the splitting
tensor @xmath that reverts the fusion in Eq. ( 6.16 ) and a cup tensor.

#### 6.0.7 Multiplication of two tensors

We can now consider the multiplication of two SU(2)-invariant tensors by
breaking it into a sequence of five elementary steps consisting of
reversals, permutes, reshapes and matrix multiplication, as was
exemplified in Sec. 5.1.3 and Fig. 5.4 . Here the elementary steps are
performed on the tree decompositions of tensors. Figure 6.7 illustrates
the five steps of the tensor multiplication, Eq. ( 5.8 ), as adapted to
tree decompositions. For simplicity, we have not shown the tree
decompositions in the figure and the circle that depicts a tensor can be
imagined to mask the tree decomposition of the tensor.

#### 6.0.8 Discussion on computational performance

The core of obtaining computational gain from exploiting the symmetry
lies in block-wise matrix operations [Fig. 5.26 ] while permutation and
reshape of indices are applied mainly to obtain block-diagonal matrices
from tensors. As has been illustrated in Fig. 5.25 the cost of
reshaping, for instance, SU(2)-invariant tensors can be significantly
larger than that incurred in reshaping regular tensors, and in some case
can lead to a severe degradation of the overall gain obtained by
exploiting the symmetry. Let us analyze the cost associated to reshape
and permutation of indices of an SU(2)-invariant tensor.

We have described that by working on tree decompositions reshape and
permutation of indices equates to multiplying a matrix @xmath with a
vector. The computation of @xmath may be costly since it generally
involve evaluating many spin networks, see Fig. 6.8 . Consequently, the
cost of reshaping or permuting tensors with a large number of indices
may become significant. This is more so the case for iterative
algorithms where a fixed set of manipulations repeat many times. For
example, one may optimize tensors iteratively in a variational algorithm
such that the components of the tensor are updated in the current
iteration and used as an input to the subsequent iteration. Note that
each iteration involves evaluating a large number of spin networks,
albeit the same spin networks are evaluated in each iteration. This fact
can be exploited to pre-compute the transformations @xmath once, say in
the first iteration of the algorithm, and storing them in memory for
reuse in subsequent iterations. By precomputation of these matrices the
cost of evaluating many spin networks is suppressed from the runtime
costs.

In our MATLAB implementation the use of such a precomputation scheme
resulted in a significant speed-up of simulations at the cost of storing
additional amounts of precomputed data. In the passing we also remark
that since all computations have been reduced to matrix operations,
computational performance can also be potentially enhanced by
parallelizing and vectorizing the underlying matrix operations.

### 6.1 Tensor networks with SU(2) symmetry: A practical demonstration

Consider a lattice @xmath made of @xmath sites where each site @xmath is
described by a vector space @xmath that transforms as a finite
dimensional representation of SU(2). The vector space @xmath of the
lattice is given as

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

Consider a state @xmath that is invariant, Eq. 5.60 , under the action
of SU(2) on the vector space @xmath . We describe @xmath by means of a
tensor network made of SU(2)-invariant tensors.

It readily follows that the tensor obtained by contracting such a tensor
network is SU(2)-invariant, as illustrated in Fig. 6.9 .

On the one hand, by storing each constituent tensor of the tensor
network in a canonical form we can ensure a compact tensor network
description of @xmath . On the other, computational speedup can be
obtained by exploiting the sparse canonical form of the tensors when
performing manipulations of individual tensors in a tensor network
algorithm.

In the remainder of the section we illustrate the implementation of
SU(2) symmetry in tensor network algorithms with practical examples. We
do so in the context of the Multi-scale Entanglement Renormalization
Ansatz, or MERA, and present numerical results from our reference
implementation of SU(2) symmetry in MATLAB.

#### 6.1.1 Multi-scale entanglement renormalization ansatz

Figure 6.10 shows a MERA that represent states @xmath of a lattice
@xmath made of @xmath sites. Recall that the MERA is made of layers of
isometric tensors, known as disentanglers @xmath and isometries @xmath ,
that implement a coarse-graining transformation. In this particular
scheme, isometries map three sites into one and the coarse-graining
transformation reduces the @xmath sites of @xmath into two sites using
two layers of tensors. A collection of states on these two sites is then
encoded in a top tensor @xmath , whose upper index @xmath is used to
label @xmath states @xmath . This particular arrangement of tensors
corresponds to the 3:1 MERA described in (Evenbly and Vidal, 2009a ) .
We will consider a MERA analogous to that of Fig. 6.10 but with @xmath
layers of disentanglers and isometries, which we will use to describe
states on a lattice @xmath made of @xmath sites.

We will use the MERA as a variational ansatz for ground states and
excited states of quantum spin models described by a local Hamiltonian
@xmath . In order to find an approximation to the ground state of @xmath
, we set @xmath and optimize the tensors in the MERA so as to minimize
the expectation value

  -- -------- -- --------
     @xmath      (6.18)
  -- -------- -- --------

where @xmath is the pure state represented by the MERA. In order to find
an approximation to the @xmath eigenstates of @xmath with lowest
energies, we optimize the tensors in the MERA so as to minimize the
expectation value

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

The optimization is carried out using the MERA algorithm described in
(Evenbly and Vidal, 2009a ) , which requires contracting tensor networks
(by sequentially multiplying pairs of tensors) and performing singular
value decompositions.

#### 6.1.2 MERA with SU(2) symmetry

An SU(2)-invariant version of the MERA, or SU(2) MERA for short, is
obtained by simply considering SU(2)-invariant versions of all of the
isometric tensors, namely the disentanglers @xmath , isometries @xmath ,
and the top tensor @xmath . This requires assigning a spin operator to
each index of the MERA. We can characterize the spin operator by two
vectors, @xmath and @xmath : a list of the different values the spin
takes and the degeneracy associated with each such spin, respectively.
For instance, an index characterized by @xmath and @xmath is associated
to a vector space @xmath that decomposes as @xmath with @xmath and
@xmath .

Let us explain how a spin operator is assigned to each link of the MERA.
Each open index of the first layer of disentanglers corresponds to one
site of @xmath . The spin operator on any such index is therefore given
by the quantum spin model under consideration. For example, a lattice
with a spin- @xmath associated to each site corresponds to assigning
spin- @xmath operators [Eq. ( 5.24 )] to each of the open indices.

For the open index of the tensor @xmath at the very top the MERA, the
assignment of spins will depend on spin sector @xmath that one is
interested in. For instance, in order to find an approximation to the
ground state and first seven excited states of the quantum spin model
within the spin sector @xmath , we choose @xmath and @xmath .

For each of the remaining indices of the MERA, the assignment of the
pair @xmath needs careful consideration and a final choice may only be
possible after numerically testing several options and selecting the one
which produces the lowest expectation value of the energy.

For demonstrative purposes, we will use the SU(2) MERA as a variational
ansatz to obtain the ground state and excited states of the spin- @xmath
antiferromagnetic quantum Heisenberg chain that is given by,

  -- -------- -- --------
     @xmath      (6.20)
  -- -------- -- --------

where

  -- -------- -------- -- --------
     @xmath   @xmath      (6.21)
  -- -------- -------- -- --------

@xmath and @xmath are the spin- @xmath operators [Eq. ( 5.24 )]. The
model has a global SU(2) symmetry, since the Hamiltonian commutes with
the spin operators acting on the lattice @xmath . This follows from the
fact that each local term @xmath in the Hamiltonian commutes with the
two site spin operators,

  -- -------- -- --------
     @xmath      (6.22)
  -- -------- -- --------

Each spin- @xmath degree of freedom of the Heisenberg chain is described
by a vector space @xmath that is spanned by two orthonormal states
[Eq. ( 5.23 )],

  -- -------- --
     @xmath   
  -- -------- --

For computational convenience, we will consider a lattice @xmath where
each site contains two spins. Therefore each site of @xmath is described
by a space @xmath , where @xmath and @xmath , also discussed in Example
6. This corresponds to the assignment @xmath and @xmath at the open legs
at the bottom of the MERA. Thus, a lattice @xmath made of @xmath sites
corresponds to a chain of @xmath spins.

Table 6.1 lists some of the spin and degeneracy dimensions assignment
(for the internal links of the MERA) that we have used in the numerical
computations for @xmath (or 108 spins). For a given value of @xmath and
@xmath the corresponding dimension @xmath can be obtained as,

  -- -------- -- --------
     @xmath      (6.23)
  -- -------- -- --------

Figure 6.11 shows the error in the ground state energy of the Heisenberg
chain as a function of the bond dimension @xmath , for the assignments
of @xmath and @xmath that are listed in Table 6.1 . For the choice of
spin assignments listed in the table the error is seen to decay
polynomially with @xmath , indicating increasingly accurate
approximations to the ground state.

#### 6.1.3 Advantages of exploiting the symmetry

We now discuss some of the advantages of using the SU(2) MERA.

##### 6.1.3.1 Selection of spin sector

An important advantage of the SU(2) MERA is that it exactly preserves
the SU(2) symmetry. In other words, the states resulting from a
numerical optimization are exact eigenvectors of the total spin operator
@xmath . In addition, the total spin @xmath can be pre-selected at the
onset of optimization by specifying it in the open index of the top
tensor @xmath .

Figure 6.12 shows the low energy spectrum of the Heisenberg model @xmath
for a periodic system of @xmath sites (or @xmath spins), including the
ground state and several excited states in the spin sectors @xmath . The
states have been organized according to spin projection @xmath . We see
that states with different spin projections @xmath (for a given @xmath )
are obtained to be exactly degenerate, as implied by the symmetry.

Similar computations can be performed with the regular MERA. However,
the regular MERA cannot guarantee that the states obtained in this way
are exact eigenvectors of @xmath . Instead the resulting states are
likely to have total spin fluctuations. This is shown in inset of Fig.
6.12 , which corresponds to the zoom in of the region in the plot that
is enclosed within the box. The inset shows (black asterix points) the
corresponding energies obtained for the enclosed two-fold degenerate
@xmath states using the regular MERA. We see that the states
corresponding to different values of @xmath are obtained with different
energies.

Also note that by using the SU(2) MERA, the three sectors @xmath and
@xmath can be addressed with independent computations. This implies, for
instance, that finding the gap between the first singlet ( @xmath ) and
the first @xmath state, can be addressed with two independent
computations by respectively setting @xmath and @xmath on the open index
of the top tensor @xmath . However, in order to capture the first @xmath
state using the regular MERA, we would need to consider at least @xmath
(at a larger computational cost and possibly lower accuracy), since this
state has only the @xmath ^(th) lowest energy overall.

##### 6.1.3.2 Reduction in memory and computational costs

The use of SU(2)-invariant tensors in the MERA also results in a
reduction of computational costs. We compared the memory and
computational costs associated with using the regular MERA and the SU(2)
MERA. We also found it instructive to compare the analogous costs
associated with a MERA that is made of tensors that remain invariant
under only a subgroup U(1) of the symmetry group. This entails
introducing the spin projection operators @xmath on the links of the
MERA and imposing the invariance of constituent tensors under the action
of these operators. For such a U(1)-MERA, imposing such constraints
corresponds to conservation of the total spin projection @xmath , while
the total spin may fluctuate. (The explicit construction of the
U(1)-MERA was discussed in Chapter 4).

Figure 6.13 shows a comparison of the total number of complex
coefficients that are required to be stored for @xmath sites
(corresponding to 108 spins) in the three cases: regular MERA, U(1) MERA
and the SU(2) MERA. U(1)-invariant tensors (see Chapter 4) have a block
structure in the eigenbasis of @xmath operators on each index of the
tensor, and therefore they incur a smaller memory cost in comparison to
regular tensors. For example, it can be seen that for the same memory
required to store a regular MERA with @xmath , one can instead consider
storing a U(1)-MERA with @xmath . On the other hand, SU(2)-invariant
tensors are substantially more sparse. When written in the canonical
form, SU(2)-invariant tensors are not only block-sparse but each block,
in turn, decomposes into a degeneracy part and a structural part such
that the structural part need not be stored in memory. With the same
amount of memory that is required to store, for example, a @xmath
regular MERA, one can already store a @xmath SU(2) MERA.

In Fig. 6.14 we show an analogous comparison of the computational
performance in the three cases. We plot the computational time required
for one iteration of the energy minimization algorithm of (Evenbly and
Vidal, 2009a ) (during which all tensors in the MERA are updated once),
as a function of the total bond dimension @xmath for the cases of
regular MERA, U(1) MERA and SU(2) MERA. We see that for sufficiently
large @xmath , using SU(2)-invariant tensors leads to a shorter time per
iteration of the optimization algorithm. In the case of symmetric
tensors we considered pre-computation of repeated operations, see Sec.
6.0.8 .

## Chapter 7 Conclusions and Outlook

In this thesis we have described how to incorporate global internal
symmetries into tensor network states and algorithms.

On the theoretical side we developed a framework to characterize and
manipulate symmetric tensors. Any given tensor network can be adapted to
the presence of a symmetry by imposing the constituent tensors to be
symmetric. Symmetric tensors are very sparse objects. Their judicious
use and careful manipulation can lead to an enormous computational gain
in numerical simulations. This has been extensively demonstrated in this
thesis by means of our reference MATLAB implementation.

On the implementation side, we have described a practical scheme for
protecting and exploiting the symmetry in numerical simulations. We
proposed the use of tree decompositions of a symmetric tensor. Several
advantages of this scheme were discussed, not excluding the overall
simplicity and elegance of the method. We hope to have provided a
suitable implementation framework for researchers who are familiar with
the theoretical aspects of incorporating the symmetries into tensor
network algorithm but nonetheless find the practical implementation
challenging.

In implementing symmetries we have gone beyond the case of MPS, which
being a trivalent tensor network is simpler to handle. We described the
construction of the U(1) and SU(2) symmetric versions of the MERA. Our
Abelian implementation led to computational gains measuring up to an
increase of ten to twenty times. The analogous gain from the non-Abelian
implementation was much larger, measuring up to an increase of forty to
fifty times. These gains may be used either to reduce overall
computation time or to permit substantial increases in the MERA bond
dimension @xmath , and consequently in the accuracy of the results
obtained. Therefore the exploitation of symmetries, especially
non-Abelian symmetries, can be an invaluable tool for numerically
challenging systems. This is more so the case in two dimensional lattice
models where simulation costs are much more severe. An example of a
potential application is to a system of interacting fermions that
appears in the context of high temperature conductivity. Here even
though symmetries are present in the model, they have not been throughly
exploited in the context of tensor network algorithms.

Although we have given special attention to specific symmetry groups,
U(1) and SU(2), the formalism presented in this thesis may equally well
be applied to any reducible compact non-Abelian group that is
multiplicity free. In particular, one can consider composite symmetries
such as SU(2) @xmath U(1), corresponding to spin isotropy and particle
number conservation and SU(2) @xmath SU(2) corresponding to conservation
of spin and isospin etc. Such a symmetry is characterized by a set of
charges @xmath ; when fusing two such sets of charges @xmath and @xmath
, each charge @xmath is combined with its counterpart @xmath according
to the relevant fusion rule. Once again, this behaviour may be encoded
into a single fusing tensor @xmath .

Our implementation scheme can also be readily extended to incorporate
more general symmetry constraints such as those associated with
conservation of total fermionic and anyonic charge. One proceeds by
defining the following tensors for the relevant charges,

-   the fusing tensor @xmath that encodes the fusion of two charges,

-   the recoupling coefficients @xmath that relate various ways of
    fusing three charges, and

-   the swap tensor @xmath that encodes the swap behaviour of two
    charges.

Note that within our specific implementation framework, one may instead
just define the linear maps @xmath that mediate tensor manipulations for
the respective charges.

As an example, consider fermionic constraints where the relevant charge,
@xmath , is the parity of fermion particle number. Charge @xmath takes
two values, @xmath and @xmath corresponding to even or odd number of
fermions. The fusing tensor @xmath encodes the fusion rules that specify
how charges @xmath and @xmath fuse together to obtain a charge @xmath .
These correspond to the fusion rules for the group @xmath , given as,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The recoupling coefficients @xmath , associated with the fusion of three
charges @xmath and @xmath are simple in this case owing to the Abelian
fusion rules. They take a value @xmath for all values of intermediate
charges @xmath and @xmath that appear when fusing the three charges one
way or the other.

The final ingredient is the tensor @xmath , which in this case is
defined as,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

In a similar way, one can encode the corresponding fusion rules for
anyonic charges into the fusing tensor @xmath . In the case of anyons,
the recoupling coefficients @xmath are obtained as solutions to the
pentagon equations whereas the tensors @xmath are replaced with the
anyonic braid operators that are obtained as solutions to the hexagon
equations , see (Trebst et al., 2008 ; Feiguin et al., 2007 ; Pfeifer
et al., 2010 ) . Thus, having defined these tensors for the relevant
charges, the formalism and the implementation framework presented in
this thesis can be readily adapted to incorporate the constraints
corresponding to the presence of fermionic or anyonic charges.

In a different context, preservation of symmetry can be crucial even
without demanding a computational gain. Recently, a novel classification
of symmetric phases in 1D gapped spin systems was undertaken in (Chen
et al., 2011 ) . In the absence of any symmetry, in this classification
all states are equivalent to trivial product states. However, by
preserving certain symmetries many phases were reported to exist with a
different symmetry protected topological order . As an alternative, such
a classification could potentially be addressed with the symmetric
version of the MERA, since the MERA is adept at characterizing fixed
points of the renormalization group flow which correspond to different
phases.

Symmetries are a fundamental aspect of nature. Nearly all physical
phenomenon can be explained by the presence or the absence of a
symmetry. In numerical methods, the preservation of symmetries may well
be a necessary requirement for simulating certain aspects of the system.
As a computational aid, symmetries will play a crucial role in pushing
forward the frontiers of tensor network algorithms in the coming years.