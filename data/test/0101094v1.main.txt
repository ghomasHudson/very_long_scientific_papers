# Chapter 1 Introduction

The cement for the subjects this manuscript deals with is the Monte
Carlo method of numerical integration. Therefore, the first section is
endowed with an introduction to its aspects relevant for the second
section, which digresses on the main contents of this thesis.

\minitoc

### 1.1 Numerical integration

Numerical integration is an approximation of the solution to an
integration problem. An integration problem consists of the task to
integrate a function @xmath , the integrand. Sometimes, the integral can
be calculated analytically, but in most cases, this is not possible. Let
us, for simplicity, assume that the problem can be reduced to that of
the calculation of an integral on the @xmath -dimensional hypercube
@xmath . We denote the Lebesgue integral of the integrand @xmath by

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

With numerical integration, this integral is estimated by a weighted
average of @xmath over a finite sample of @xmath points @xmath , that
is, by

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

where the numbers @xmath are the weights coming with the particular
method. Such a method is determined by the choice of the sample and the
weights.

In principle, the only restriction on a method to be acceptable is that,
with the estimate of the integral, it should give an estimate of the
expected error on the result. And of course, this expected error should
not be too large. If a certain method cannot give an error estimate, it
is useless. In practice, there is another restriction on a method to be
acceptable, namely that the computational complexity it introduces
should not be too large. It should be possible to do the computation
within reasonable time. The computational complexity is due to the
generation of the sample, evaluating the weights and evaluating the
function values. Naturally, one expects that a result will become more
accurate if larger samples are used, because then more information about
the integrand is used. But if the evaluation of the function values is
very expensive (time consuming), then one would like to use small
samples, and indeed, there are methods that need smaller samples than
other methods with the same accuracy. For these methods, however, the
generation of the samples is more expensive.

In the case of @xmath , there are many acceptable and efficient methods.
In most of them, the sample is chosen to be distributed evenly over
@xmath , i.e., all the distances between neighbors are the same and the
whole of @xmath is covered. Different weights can be chosen, depending
on the smoothness of the integrand. These methods give an expected error
that decreases with the number of points as @xmath , where @xmath , with
the general rule that @xmath is larger for methods that can be applied
to smoother integrands (cf. [ 2 ] ).

Conceptually, it is a small step to extrapolate these one-dimensional
methods to more dimensions: the sample is taken to be the Cartesian
product in the coordinates of the one-dimensional samples, and the
weights are the products over the coordinates of the one-dimensional
weights. Computationally, however, is it a large step, for the expected
error decreases with @xmath as @xmath . So to get an expected error that
is of the “one-dimensional order” with @xmath points, you need @xmath
points. This small disaster is often called the “curse of
dimensionality”.

A closer look at the choice of the samples reveals the cause of the
curse. In one dimension, the even distribution of the points is the most
uniform distribution possible. This makes the methods applicable to
large classes of functions, because in the choice of the sample no
knowledge about the integrand is assumed. As a result of this, the
behavior with @xmath of the expected error factorizes. In more
dimensions, however, a Cartesian product of these one-dimensional
distributions is not at all ‘uniform’. The distances between neighbors
in different directions are not the same anymore. Therefore, these
methods can only be efficient for integrands that have the same kind of
Cartesian symmetry. The error estimate, however, includes no knowledge
about the integrand, and as a result of this, increases rapidly with the
number of dimensions.

#### 1.1.1 Monte Carlo integration

A popular remedy to the curse of dimensionality is the so called Monte
Carlo (MC) method of numerical integration [ 16 ] . It is based on the
belief that the points of the sample will be distributed fairly over
@xmath if they are chosen at random. To be more precise, the points are
chosen at random, independently and uniformly distributed, and the
estimate of the integral of a function @xmath is given by the unweighted
average

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

With this choice of the samples, the estimate of the integral becomes a
random variable, and probability theory can be applied to do statements
about it (some relevant topics are reviewed in Section 2.1 ). For
example, the expectation value of @xmath is given by

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

So the expectation value of the estimate of the integral is equal to the
integral itself. The variance of @xmath is equal to

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

where @xmath just denotes pointwise multiplication of @xmath with
itself. This means that, if @xmath is square integrable so that @xmath
and @xmath exist, then we can apply the Chebyshev inequality, with the
result that for large @xmath , the estimate @xmath converges to @xmath
with an expected error given by @xmath . This is a very important
result, for it states that the Monte Carlo method works in any dimension
with the same rate of convergence, given by the @xmath -rule. The only
restriction is that @xmath has to be square integrable. If this is not
the case, the Monte Carlo estimate of an integral cannot be trusted.

##### 1.1.1.1 Error estimation

In practice, one of course does not know @xmath , so that it has to be
estimated. This makes Monte Carlo integration a matter of statistics. A
good estimator for the squared error is given by

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

which satisfies @xmath . To get more confidence in the result, an
estimate of the squared error on the estimated squared error can be
calculated with

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

which satisfies @xmath . Notice that @xmath has to exist in order to
credit any value to @xmath . One could, in principle, go on calculating
higher errors on errors, but their significance becomes less and less,
if they converge at all.

##### 1.1.1.2 Sample generation

Another question is how to obtain the random points. Monte Carlo
integration is preferably done with the help of a computer, and there
exist algorithms that produce sequences of numbers between @xmath and
@xmath that are ‘as good as random’. They are called pseudo-random
number generators (c.f. [ 3 ] ). Since they are implemented on a
computer, the algorithms are deterministic, and the numbers they produce
cannot be truly random. The sequences, however, ‘look random’ and are
certainly suitable for the use in Monte Carlo integration. Another
drawback is that, because computers represent real numbers by a finite
number of bits, the algorithms necessarily have a period, that is, they
can produce only a finite number of numbers, and if they are recycled,
they cannot be considered random anymore. Fortunately, modern random
number generators such as have very large periods, up to @xmath .

Finaly, the finiteness of a computer can cause problems when calculating
a Lebesgue integral. In the foregoing, we stated that the Monte Carlo
method is alway applicable if the integrand @xmath is square integrable.
For a computer, however, this is not enough. Consider the function

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

which has Lebesgue integrals @xmath . A computer represents numbers with
finite strings of bits, i.e., the numbers are always rational, so that a
Monte Carlo estimate will always give @xmath . Fortunately, this kind of
pathological cases do not appear often in physical applications.

#### 1.1.2 Importance sampling

The original problem is usually not that of the integration of a
function on a hypercube. In general, it is the problem of integrating a
function @xmath on a more complicated manifold @xmath . As we have seen
before, the problem has to be reduced to that of integrating a function
@xmath on a hypercube @xmath in order to apply the MC method. This is
done with a map @xmath , and sometimes, an invertible map can be found
in which cases we simply have

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

where @xmath is the determinant of the Jacobian matrix of @xmath , so
that @xmath . In general however, this is not the case, and a suitable
mapping @xmath and a function @xmath have to be determined such that

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

where @xmath is the Dirac delta-distribution on @xmath (cf. [ 5 ] ). The
integral of @xmath over @xmath is then given by

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

If @xmath is invertible, then @xmath . We just used the word “suitable”
in connection with the determination of @xmath , and therefore in
connection with the determination with the function @xmath , which is
not unique. Importance sampling is the effort to choose @xmath such that
@xmath is as small as possible, so that the expected error is as small
as possible. The optimal choice would be such that it is zero, but this
would mean that @xmath for all @xmath and that the integration problem
is solved analytically. In practice, @xmath should be chosen as flat as
possible.

#### 1.1.3 Quasi Monte Carlo integration

The Monte Carlo method is very robust, but the @xmath rate of
convergence can be considered rather slow: to get one more significant
digit in the result, 100 times more sample points are needed. The Quasi
Monte Carlo (QMC) method tries to improve this behavior, by using
samples the points of which are distributed more uniformly over the
integration region than independent random points that are distributed
uniformly in the integration region (cf. [ 4 ] ).

The previous sentence seems a bit paradoxical, but notice the difference
between ‘uniformly over’ and ‘uniformly in’. The latter is meant in the
probabilistic sense: a random point is distributed following a
distribution in an integration region, which can be the uniform
distribution. The former is meant for a set of points: the points can be
distributed uniformly over the integration region. In this case, the
word ‘uniformly’ does not really have a meaning yet, and has to be
defined, which is done by introducing measures of rates of uniformity.
They are called discrepancies , and return a number @xmath for a sample,
or point set , @xmath . The idea is then that, the higher the number
@xmath , the less uniformly the points are distributed.

The task in QMC integration is to find low-discrepancy point sets. The
integral of a function @xmath is then estimated again by the unweighted
average @xmath over the point set. That this approach can indeed improve
the convergence of the error is, for example, shown by the Koksma-Hlawka
inequality, which states that

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

where @xmath is the so called star discrepancy of @xmath , and @xmath is
the variation of @xmath in the sense of Hardy and Krause. It is a
complicated function of @xmath that is, however, independent of the
point set. This inequality states that the error, made by estimating the
integral by an unweighted average over the function values at the points
of the point set, decreases with the number of points @xmath at least as
quickly as the star discrepancy of the point set.

### 1.2 Contents of this thesis

The main contents start in Chapter 3 , and can be divided into two
subjects: the calculation of discrepancy distributions, and phase space
integration with the emphasize on a special case of importance sampling.
Chapter 2 reviews some topics from probability theory and formalism of
Feynman diagrams.

#### 1.2.1 Calculation of discrepancy distributions

As discussed before, the relatively slow convergence of the MC method
has inspired a search for other point sets whose discrepancy is lower
than that expected for truly random points. The low-discrepancy point
sets and low-discrepancy sequences have developed into a veritable
industry, and sequences with, asymptotically, very low discrepancy are
now available, especially for problems for which the dimension of the
integration region is very large [ 17 ] . For point sets that are
extracted as the first @xmath elements of such a sequence, though, one
is usually still compelled to compute the discrepancy numerically, and
compare it to the expectation for random points in order to show that
the point set is indeed ‘better than random’. This implies, however,
that one has to know, for a given discrepancy, its expectation value for
truly random points, or preferably even its probability density
(cf. Section 2.1.6 ).

In Chapter 3 , we introduce the formalism of the so-called quadratic
discrepancies , and derive a formula for the generating function of
their probability distribution. Furthermore, we give Feynman rules to
calculate the generating function perturbatively using Feynman diagrams,
with @xmath as expansion parameter. Chapter 4 digresses on the question
whether the asymptotic series obtained is correct, and concludes
affirmative for two examples of discrepancies, with great confidence in
the general case.

In [ 24 , 25 , 26 ] the problem of calculating the probability
distribution of quadratic discrepancies under truly random point sets
has been solved for large classes of discrepancies. Although computable,
the resulting distributions are typically not very illuminating. The
exception is usually the case where the number of dimensions of the
integration problem becomes very large, in which case a normal
distribution often arises [ 23 , 27 ] . In Chapter 5 , we investigate
this phenomenon in more detail, and we shall describe the conditions
under which this ‘law of large dimensions’ applies.

Throughout the discussion of Chapter 5 , only the asymptotic limit of
very large @xmath is considered, which implies that no statements can be
done on how the number of points has to approach infinity with respect
to the number of dimensions, as was for instance done in [ 27 ] . This
problem is tackled in Chapter 6 , in which the diagrammatic expansions
of the generating function is given and calculated to low order for a
few examples. For the Lego discrepancy , which is equivalent with a
@xmath -statistic for @xmath data points distributed over a number of
@xmath bins, cases in which @xmath as well as @xmath become large are
considered, leading to surprising results. Also the Fourier diaphony ,
for which a limit is derived in [ 27 ] , is handled, leading to a
stronger limit.

#### 1.2.2 Phase space integration

A typical example in which the MC method is the only option is in the
problem of phase space integration . It occurs in particle physics,
where the connection between the model of the particles and the
experiments with the particles is made with the help of transition
probabilities (cf. [ 8 ] ). These give the probability to get, under
certain conditions, a transition from one certain state of particles
(the initial state ) to another certain state of particles (the final
state ). On one side, these probabilities can be determined
statistically, by performing an experiment several times, starting with
the same initial state every time, and by counting the number of times
certain final states occur. The probabilities can also be calculated
from the model, and then two outcomes can be compared to evaluate the
model.

Phase space is the space of all possible momentum configurations of the
final-state particles, and particle models predict probability densities
on it. Because of the need of very high statistics for acceptable
precision, it is usually difficult to determine them experimentally. A
solution to this experimental problem is the creation of a mathematical
problem: averaging transition probabilities over phase space. In the
analysis of the experimental data this just means that final states,
that differ only in momentum configuration, are considered equivalent.
In the analysis of the model, this means that an integration of the
probability density over phase space has to be performed.

The actual quantity that physicists deal with is not the transition
probability, but the cross section . If the number of initial particles
is two, then it is the transition rate per unit of time, normalized with
respect to the flux of the initial particles, i.e., the density of the
initial particles times their relative velocity. The differential cross
section @xmath of a proces from a two particle initial state to a
certain final state is given by

  -- -------- -- --------
     @xmath      (1.13)
  -- -------- -- --------

In this expression, @xmath represents the final state degrees of freedom
that have to be integrated or summed in order to get the desired cross
section @xmath . This includes the final-state momenta. The
delta-distribution represents momentum conservation between the initial
and the final states, and @xmath is the relative velocity of the initial
particles. The characteristics of the particular proces are contained in
@xmath , the transition amplitude or matrix element , and has to be
calculated using the particle model in the formalism of quantum field
theory. It determines the function that has to be intergrated over phase
space.

Besides momentum conservation, there are other restrictions the momenta
of the particles have to satisfy, independent of the amplitude.
Algorithms that generate random momenta, satisfying these restrictions,
are called phase space generators , and in Chapter 7 RAMBO is described,
which generates momenta distibuted uniformly in phase space. This
chapter also deals with some techniques that are useful for MC
integration in general.

For certain particle processes, the squared amplitude can have
complicated peak structures, that make it hard to be integrated if the
momenta are generated such that they are distributed uniformly in phase
space. This is in particular true if it concernes processes in which the
strong interaction is involved, for which the integrand contains peak
structures that are governed by the so-called antenna pole structure .
In Chapter 8 , the algorithm SARGE is introduced that generates random
momenta, satisfying the restrictions that are independent of the
amplitude, and such that they are distributed following a density that
containes the antenna pole structure. It improves the MC integration
process through importance sampling.

## Chapter 2 Probability, measures and diagrams

Since this thesis is meant to be read by both theoretical physicists and
mathematicians, this chapter elaborates on some subjects that are
probably not everyday routine to the one or the other. This concerns
probability theory, including a (very) short introduction to
martingales, and Feynman diagrams. The hasty reader is advised to read
at least Section 2.1.6 and Section 2.1.7 .

\minitoc

### 2.1 Some probability theory

We start this section ‘at level zero’ with respect to the probability
theory, but expect the reader to be familiar with a bit of set theory,
logic, measure theory, complex analysis and so on. For more details, we
refer to [ 11 ] , [ 12 ] and [ 13 ] .

#### 2.1.1 Probability space

A probability space consists of a triple @xmath , where @xmath is a set,
@xmath a @xmath -field of subsets of @xmath , and @xmath a probability
measure defined on @xmath . The collection @xmath of subsets is called a
@xmath -field if

1.  @xmath ;

2.  @xmath ;

3.  @xmath ,

where, in the last property, the number of sets @xmath has to be
countable. The probability measure @xmath is a function @xmath with
@xmath . We will only consider probability spaces for which @xmath is a
Lebesgue measurable subset of @xmath , @xmath and for which @xmath is
given by

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath stands for the Lebesgue measure on @xmath and @xmath is a
function @xmath with @xmath . @xmath is called the probability density
or probability distribution , although the latter name is more
appropriate for the set of doubles @xmath .

A simple example of a probability distribution is the uniform
distribution in @xmath , for which @xmath . This is often extended to
more dimensions, say @xmath , by taking the Cartesian product of
independent one-dimensional variables, that is, @xmath , where @xmath
and @xmath for all @xmath . We say that @xmath is distributed uniformly
in @xmath .

#### 2.1.2 Random variables

A random variable @xmath is a function on @xmath . It is an object about
which statements @xmath can be made. These statements are then ‘valued’
with a number between @xmath and @xmath by the probability measure.
Probability theory concerns itself with the calculation of these
numbers; their interpretation depends on the user. It can be a “rate of
belief” (the Bayesian interpretation) or a ratio of outcomes in the
limit of an infinite number of repetitions of experiments (the
frequentist interpretation). In Monte Carlo integration, for example,
the latter applies.

Let @xmath denote a statement @xmath about @xmath , and let @xmath be
the subspace of @xmath for which @xmath is true, then we denote

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

An important operator in the theory of probability is the expectation
value @xmath . It is the average of @xmath over @xmath , weighted with
@xmath :

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

Especially expectation values of powers of @xmath are often considered,
and they are called the moments of the probability distribution of
@xmath . This name anticipates the fact that a random variable has its
own probability distribution, which is simply defined through

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where @xmath . From now on, we will assume that @xmath is real, and
introduce the cumulative probability distribution or distribution
function

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

@xmath is a monotonously increasing function @xmath . Its derivative is
the probability density @xmath , and we have

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

Discontinuities in @xmath are represented by Dirac delta-distributions
in @xmath . An interesting observation is, furthermore, that if @xmath
is distributed following @xmath , then the random variable @xmath is
distributed uniformly in @xmath , since @xmath .

We proceed with a translation of confidence levels, given by @xmath ,
into expectation values. This is done by the Chebyshev inequality ,
which states that, for a given number @xmath ,

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

Its proof is simple. We have

  -- -------- --
     @xmath   
  -- -------- --

where the inequality holds because @xmath under the integrals. The final
expression is smaller than the integral over the whole of @xmath , which
is equal to the r.h.s. of Eq. ( 2.7 ). An example of its use is an
estimate of the probability that a variable @xmath will differ an amount
@xmath from its expectation value @xmath . The Chebyshev inequality
tells us that

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

is called the variance of @xmath , and its square root @xmath is called
the standard deviation . So if we take @xmath , then we see that the
probability of @xmath to be larger than @xmath is smaller than @xmath .

It is common not to consider the random variable itself, but
standardized variable which is given by

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

It has its expectation value equal to zero and its variance equal to
one.

#### 2.1.3 Generating functions

If @xmath is real, its probability density can be calculated as follows.
Let @xmath denote the Heaviside step-function. It can, for example, be
represented by the integral in the complex plane

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

where the contour @xmath is along the line @xmath , and @xmath is
positive and small. If @xmath , then the integration contour can be
closed to the right and the pole in @xmath contributes with a residue
that is equal to @xmath . An extra minus sign comes from the orientation
of the contour. If @xmath , then the contour can be closed to the left,
giving zero. The probability distribution function @xmath is then given
by

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

The integral over @xmath just gives the expectation value of @xmath ,
which is called the moment generating function

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

It carries this name, because its derivatives in @xmath give the moments
@xmath of @xmath . In literature, the characteristic function is often
used, which is just given by @xmath . The final result is that @xmath is
given by

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

We can translate this into a formula for the probability density @xmath
by differentiation with respect to @xmath . The result is that

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

i.e., it is the inverse Laplace transform of the moment generating
function of @xmath . Notice that the generating function satisfies
@xmath , because the probability density @xmath is properly normalized:
@xmath .

Another generating function that is often used, the cumulant generating
function @xmath , is simply given by @xmath . The first cumulant is
equal to @xmath itself, and the second is the variance @xmath .

The generating function of the standardized variable can be expressed in
terms of the original generating function @xmath through

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

#### 2.1.4 Convergence of random variables and distributions

Sequences @xmath of random variables are often considered in
probabilistic analyses, and in particular their limiting behavior.
Therefore, notions of convergence are needed, and we distinguish various
types. First there is convergence in probability , and we write

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

With the Chebyshev inequality, we see that the requirement for
convergence in probability is satisfied if there is a @xmath such that
@xmath for all @xmath . This observation suggests to introduce
convergence in @xmath mean , and we write

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

The case of @xmath can be considered special, and leads to almost sure
convergence :

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

where @xmath with @xmath . To compare these notions of convergence, we
note that (cf. [ 11 ] )

  -- -------- -------- -- --------
     @xmath   @xmath      (2.20)
     @xmath   @xmath      (2.21)
  -- -------- -------- -- --------

Finally, there is convergence in distribution or convergence in law ,
and we write

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

where the latter denotes weak convergence of the distributions @xmath of
the variables @xmath :

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

Notice that, in general, the moments of the variables @xmath are not
bounded functions. The generating functions @xmath , however, are
bounded for imaginary @xmath . We actually have, (cf. [ 12 ] )

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

The notion of weak convergence is also used in connection with
distribution functions, and we write

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

Distribution functions are right-continuous and satisfy @xmath and
@xmath . Because @xmath does not have to be a distribution function in
case of weak convergence, it is useful to define complete convergence ,
and we write

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

We note that weak convergence of a distribution is not necessarily
equivalent with weak convergence of the density, but that (cf. [ 11 ] )

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

We end this section with the remark that @xmath implies @xmath (cf. [ 11
] ), so that

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

#### 2.1.5 Martingales

With a sequence of random variables @xmath should come a sequence of
@xmath -fields @xmath . A sequence @xmath is called a martingale if

1.  @xmath is measurable with respect to @xmath ;

2.  @xmath ;

3.  @xmath with probability one for all @xmath .

The idea is that @xmath depends on a number of @xmath variables @xmath
that take their values in @xmath . In @xmath , the first @xmath
variables have to be taken fixed, and only the average over the
remaining @xmath variables has to be taken. This average can then be
considered to depend on the first @xmath variables again, and this
dependence should be the same as the one of @xmath .

A martingale is called zero-mean if @xmath for all @xmath . Furthermore,
it is called square-integrable if @xmath exists for all @xmath . A
double sequence @xmath is called a martingale array, if @xmath is a
martingale for each @xmath . The variables @xmath are called the
martingale differences. These are the ingredients needed for the
powerful (c.f. [ 13 ] )

##### 2.1.5.1 Central Limit Theorem:

Let @xmath be a zero-mean, square-integrable martingale array with
differences @xmath , and suppose that

  -- -------- -- --------
     @xmath      (2.29)
     @xmath      (2.30)
     @xmath      (2.31)
     @xmath      (2.32)
  -- -------- -- --------

Then @xmath , where @xmath is a normal variable.

A normal variable has a Gaussian distribution with zero mean and unit
variance, given by a density @xmath and generating function @xmath .

##### 2.1.5.2 Adaptive Monte Carlo integration

We apply this theorem to adaptive Monte Carlo integration, as a small
exercise. It concerns the problem of calculating the Lebesgue integral
@xmath of a function @xmath on an integration region @xmath . Let @xmath
be a sequence of positive functions, where @xmath depends on @xmath
variables @xmath that take their values in @xmath . We denote such a set
of @xmath variables by @xmath . Assume that @xmath for all values of
@xmath , so that

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

is a probability density in @xmath . Let us also introduce the functions

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

In adaptive Monte Carlo integration, one generates a random point @xmath
in @xmath following a density @xmath , and with this point a density
@xmath is constructed to generate @xmath , so that @xmath can be
constructed to generate @xmath and so on. Then, one tries to estimate
the integral @xmath with

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

The expectation value and the variance of @xmath can easily be
calculated, with the result that @xmath and @xmath , where

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

Monte Carlo integration is based on the observation that if @xmath
exists for every @xmath , so that @xmath is a finite number, the
Chebyshev inequality gives

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

which suggests to use @xmath as an estimator of @xmath , and to
interpret @xmath as the square of the expected integration error.

We shall prove ¹ ¹ 1 This is a correction of the erroneous proof in the
original thesis. now, that @xmath converges to @xmath with Gaussian
confidence levels. Except of the existence of @xmath , we shall need
some more requirements, but first let us introduce the variables

  -- -------- --
     @xmath   
  -- -------- --

Because we define the variables @xmath explicitly as the sum of the
differences @xmath , we are clearly dealing with a martingale array
(with @xmath ) satisfying ( 2.32 ). It obviously is zero mean, and it is
square integrable by the requirement that @xmath exists for all @xmath .
For the proof, we shall furthermore need the requirements that

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

The first one is satisfied if @xmath exists for all @xmath , which can
be translated in the demand that @xmath and @xmath exist for all @xmath
. The second one puts a restriction on how strong the dependencies
between the variables may be. This demand is, for example, satisfied if
for every @xmath there are numbers @xmath such that

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

and that satisfy @xmath . This is, for example, the case if @xmath . We
prove along the line of argument as presented in [ 27 ] , that the first
three requirements of the theorem are satisfied. First we observe that
the martingale is constructed such that

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

so that, for requirement ( 2.31 ), we have @xmath . For ( 2.29 ) we use
the Chebyshev inequality to find that

  -- -------- --
     @xmath   
  -- -------- --

which goes to zero for all @xmath by ( 2.38 ). Requirement ( 2.30 ) goes
the same way:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used ( 2.40 ) again. The final expression goes to zero for all
@xmath by ( 2.38 ). The result is that, because the variables @xmath
converge to a Gaussian variable with zero-mean and variance one, the
random variables

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

converge to a Gaussian variable with mean @xmath and variance @xmath .
Note that for non-adaptive Monte Carlo integration, for which the
densities @xmath are equal to a fixed density @xmath for all @xmath ,
the Lévy Central Limit Theorem applies (cf. [ 11 ] ) and only the
existence of @xmath is needed.

#### 2.1.6 Hypothesis testing, qualification and discrepancies

Probability theory is extensively used in the field of statistics.
Statisticians try to derive probability distributions from empirical
data, which are believed to be distributed following an existing, but
unknown, distribution. In this section, some statistical procedures and
their relevance to the main subjects of this thesis are discussed.

##### 2.1.6.1 Hypothesis testing

One way to test a model of a physical system is by deriving from this
model the probability distribution according to which certain data from
the system are supposed to be distributed. Then a test has to be
developed, which measures the deviation between the probability
distribution from the model, and the empirical distribution of the data.
Because the actual probability distribution that the data seem to be
drawn from is not known, this procedure belongs to the field of
statistics, and it goes under the name of hypothesis testing .

Let @xmath be a sample of physical data, @xmath the probability density
derived from the model (the hypothesis), and @xmath the statistical
test. In order for the test to be suitable, it should be developed such
that, if @xmath is distributed following @xmath , then

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

The idea is then that, for a finite number of data, @xmath also has to
be small if @xmath is distributed following @xmath . If @xmath happens
to be to large, the hypothesis has to be rejected. The question is now:
what is small or large ? In order to answer this question, the
probability distribution of @xmath under @xmath has to be calculated.
The probability distribution function @xmath is given by

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

where @xmath is the space the data @xmath can take their values in.

The generic shape of @xmath and its derivative, the probability density,
are depicted in Fig. 2.1 . They tell us what the probability would be to
find certain values for @xmath if @xmath would be distributed following
the hypothesis, i.e., they give the confidence levels . For example, we
can read off the first graph that the probability for @xmath to be
larger than @xmath is about @xmath . This means that it is not very
probable to find a value for @xmath this large, so that this number can
be considered large.

##### 2.1.6.2 Qualification of samples

Instead of for hypothesis testing, a test @xmath can also be used to
qualify a sample of data @xmath . Suppose that there is a notion of good
and bad samples, and that this notion is translated into the test @xmath
: if @xmath is small, then @xmath is good , and if @xmath is large, then
@xmath is bad . A first question can be whether the test makes sense,
and an answer can again be given by the probability distribution.
Suppose that the information available about the source of the data
leads to a probability density @xmath according to which the data seem
to be randomly distributed. If the probability density of @xmath looks
like the one in Fig. 2.1 , i.e., if it goes to zero for small values of
@xmath , then the test makes sense. It means that the test is capable of
distinguishing between good samples and the kind of samples that occur
most often. The next question is then: what are small values of @xmath ?
The answer is that values are small if it is improbable to find them.

##### 2.1.6.3 Qualification of algorithms and discrepancy

It can also be the case that the data come from an (expensive) algorithm
that was specially designed to produce good samples (for example
integration points for numerical integration), and the question is
whether the algorithm makes sense. Suppose there is another (cheap)
algorithm that produces data distributed with density @xmath . The
probability distribution of @xmath determines the notion of smallness
for the values of @xmath again, and the expensive algorithm only makes
sense if it produces samples with low values of @xmath that are
improbable to find. In the mentioned case of numerical integration, good
samples are the point sets that are distributed uniformly over the
integration space, and the tests are called discrepancies (Section 3.1
).

Discrepancies have the structure of tests that measure the deviation
between the empirical distribution of the point set, and the uniform
distribution in the integration space. Algorithms to generate point sets
following the uniform distribution (cf. [ 3 ] ) can be considered
‘cheap’ compared to the special algorithms developed for numerical
integration (cf. [ 17 ] ). This seems paradoxical, since numerical
integration asks for point sets that are distributed over the
integration space as uniformly as possible (Section 1.1.3 ). The clue is
that (random) point sets, generated following the uniform distribution,
are not necessarily those that are distributed over the integration
space as uniformly as possible. A simple example is one-dimensional
space. For a given number of points, the most uniform distribution
possible clearly is the one for which all distances between the points
are the same. However, if the points are distributed randomly following
the uniform distribution, this situation will never occur.

The example above gives a simple algorithm to generate good samples in
one-dimensional space. Algorithms become ‘expensive’ if they have to be
generated in more-dimensional spaces.

#### 2.1.7 Calculation of probability distributions

In statistics, probability distributions are used, and in probability
theory, they are calculated. Part of this thesis deals with their
calculation for discrepancies. The way this will be done is by
calculating the generating function. The probability density can then be
found using the inverse Laplace transform (Eq. ( 2.15 )), which can, if
necessary, be calculated through a numerical integral over a contour in
the complex plane.

The distribution is often calculated in certain limits, such as an
infinite number of random variables or degrees of freedom. This is, in
most cases, done because it simplifies the calculation. These limits
can, however, often be considered as the limiting cases in certain
stochastic processes: in Monte Carlo integration, for example, the limit
of an infinite number of integration points can be interpreted as the
limit of an infinite run-time for a computer.

If the generating function is considered, these limits correspond with
weak convergence. However, if the generating function is calculated
through all moments of the distribution, this corresponds with a
stronger convergence: if @xmath is a generating function and

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

but the opposite does not have to be true. The moments might even go to
infinity, while the generating function converges to an analytic
function, and we will encounter an explicit example in which this
happens.

### 2.2 Feynman diagrams and Gaussian measures

Part of this thesis deals with the calculation of probability
distributions of measures of non-uniformity of point sets @xmath in an
integration space. The measures that are considered can be written in
terms of two-point functions as @xmath . Consequently, the calculation
of the moments of the distributions involves the calculation of multiple
convolutions of these two-point functions, and Feynman diagrams can be
of help.

#### 2.2.1 Feynman diagrams

Feynman diagrams are drawings obtained by connecting vertices following
some rules. Let us illustrate this with an example, in which three
vertices are connected to a diagram:

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

The vertices have a number of legs and, in this case, there are two
kinds of legs. The rule to get from these vertices to the particular
diagram could be that legs of the same kind have to be connected. One
rule that will always apply to cases we consider is that all legs have
to be connected to other legs . Notice that, with this rule and the one
that connected legs have to be of the same kind, the diagram drawn above
is not the only possible one. Also

  -- -- -- --------
           (2.46)
  -- -- -- --------

is a permitted diagram. The vertices in the diagrams are connected by
lines . The previous two diagrams we also call connected as a whole,
because one can walk from any vertex to any other vertex over lines. An
example of a disconnected diagram can be obtained with twice as many
vertices:

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

This is a possible diagram if the previous rules are applied.

If there are also rules how to assign a number to a diagram, these,
together with the rules how to construct the diagrams, are called the
Feynman rules . The Feynman rules make the diagrams of practical use.
Certain calculations can be reduced to the assignment of numbers to a
set of diagrams, which then have to be added to finish the whole
calculation. We call such a number the contribution of the diagram, but
this word shall often be omitted, and we will refer to ‘sums of
diagrams’ instead of ‘sums of contributions of diagrams’. In the
following sections, we will give some examples, but first we derive

##### 2.2.1.1 A few general relations

Let @xmath be elements of a commutative algebra over @xmath , and assume
that there is an operation @xmath which is linear over @xmath . Suppose
that every @xmath represents a type of vertex, and that

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

can be interpreted as the sum of all possible diagrams with @xmath
vertices of type @xmath , @xmath vertices of type @xmath and so on.
Then, the sum of all possible diagrams is given by

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

where the second sum is over all partitions @xmath of @xmath . Using the
combinatorial rule of Eq. ( 2.91 ), which is derived in Appendix 2.2.4 ,
and the linearity of @xmath , we find that this is equal to

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

Now, we show that the sum of all possible connected diagrams is given by
@xmath . Define

  -- -------- -- --------
     @xmath      (2.52)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

@xmath contains all diagrams with @xmath vertices of type @xmath . The
sum of all diagrams for which these @xmath vertices are contained in the
same connected piece is denoted @xmath , so that

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

where the sum in the r.h.s. is over all partitions @xmath of @xmath .
Using Eq. ( 2.91 ) again, we find that

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

is the sum of all diagrams for which all vertices of the kind @xmath are
contained in the same connected piece. Because we can take any kind of
vertex for @xmath , the sum of all diagrams has to be given by the
exponential of the sum of all connected diagrams, and we find that

  -- -------- -- --------
     @xmath      (2.57)
  -- -------- -- --------

#### 2.2.2 Gaussian measures

An example of the use of Feynman diagrams is in calculations with
Gaussian measures. We refer to [ 5 ] for more details about the
formalism used.

We are going to look at measures on spaces @xmath of real bounded
functions on a subset @xmath of @xmath , where @xmath . The Lebesgue
measure on @xmath we just denote @xmath . @xmath can also be a finite
set, in which case the Lebesgue integral becomes a finite sum. A measure
on @xmath will be denoted @xmath , and for, not necessarily linear,
functionals @xmath on @xmath , we denote

  -- -------- -- --------
     @xmath      (2.58)
  -- -------- -- --------

The space of continuous linear functionals on @xmath is denoted @xmath ,
and a typical member is the Dirac measure @xmath , which is for every
@xmath defined by

  -- -------- -- --------
     @xmath      (2.59)
  -- -------- -- --------

Furthermore, we introduce the so called @xmath -point functions, which
are given by

  -- -------- -- --------
     @xmath      (2.60)
  -- -------- -- --------

Notice that they are symmetric in their arguments. We will always assume
that @xmath is normalized, so that @xmath . For linear functionals, we
will use the notation

  -- -------- -- --------
     @xmath      (2.61)
  -- -------- -- --------

although ‘ @xmath ’ cannot always be seen as a function value. For
example, @xmath does not exist. If we combine this notation with the
notation of Eq. ( 2.60 ), we can write

  -- -------- -- --------
     @xmath      (2.62)
  -- -------- -- --------

The Fourier transform of a measure @xmath on @xmath is the function on
@xmath given by

  -- -------- -- --------
     @xmath      (2.63)
  -- -------- -- --------

and @xmath is Gaussian if there is a quadratic form @xmath on @xmath
such that the Fourier transform is given by

  -- -------- -- --------
     @xmath      (2.64)
  -- -------- -- --------

@xmath can be written in terms of the two-point function, for take
@xmath , where @xmath is a real variable, and differentiate Eq. ( 2.64 )
twice with respect to @xmath before putting it to zero. Then it is easy
to see that

  -- -------- -- --------
     @xmath      (2.65)
  -- -------- -- --------

With this result, we can express the @xmath -point functions in terms of
the two-point function. If we take @xmath , where @xmath , @xmath are
real variables, and differentiate Eq. ( 2.64 ) once with respect of each
of these variables before putting them to zero, we find that, for odd
@xmath , @xmath , and for even @xmath that

  -- -------- -- --------
     @xmath      (2.66)
  -- -------- -- --------

where the sum is over all pairs @xmath for which @xmath .

##### 2.2.2.1 Diagrams

The previous formula suggests to interpret @xmath as a line that
connects the arguments @xmath and @xmath , so that the r.h.s. consists
of all possible ways to connect the arguments @xmath in pairs with
lines. If there is a prescription to identify a number of @xmath
arguments, then they can represent a vertex. The number of arguments in
a vertex, the number of legs, we call the order of the vertex.

A typical case in which arguments are identified is when integrals of
the following type are calculated. Let @xmath be a sequence of
functionals acting on @xmath as

  -- -------- -- --------
     @xmath      (2.67)
  -- -------- -- --------

Let, furthermore, @xmath be a set of integers larger than zero, and
denote @xmath . The integrals we want to consider are given by

  -- -------- -- --------
     @xmath      (2.68)
  -- -------- -- --------

where we use the notation @xmath . The set of arguments that are the
integration variables of the same @xmath can be considered identical. As
a result, the whole integral is given by the sum of all possible
diagrams with the @xmath vertices of the orders @xmath . The
contribution of a diagram is obtained by convoluting the two-point
functions with the functionals @xmath in the vertices.

The question we want to answer now is, given the Gaussian measure @xmath
and the functionals @xmath , what the sum of all possible diagrams is.
Because @xmath is symmetric, it suffices to consider the integrals

  -- -------- -- --------
     @xmath      (2.69)
  -- -------- -- --------

The diagrams that contribute have @xmath vertices of order @xmath ,
@xmath vertices of order @xmath and so on. In the set of diagrams that
contribute to this integral, there are many diagrams that look exactly
the same because they only differ in the exchange of integration
variables of the same vertex, or in the exchange of vertices of the same
order. We do not want to count them separately, and therefor include in
the contribution of a diagram the number of ways it can be obtained. We
turn this number into a symmetry factor , by considering

  -- -------- -- --------
     @xmath      (2.70)
  -- -------- -- --------

instead of ( 2.69 ). As a consequence, every vertex of order @xmath
accounts for a factor @xmath , and the set of vertices of order @xmath
accounts for a factor @xmath . The contribution of a diagram is then
given by the number obtained calculating the convolutions of the @xmath
’s, represented by the vertices, with the @xmath ’s, represented by the
lines, multiplied with the symmetry factor. This factor is the number of
ways the diagram can be obtained, considering all vertices and all legs
of vertices distinct, divided by @xmath , where @xmath is the number of
vertices of order @xmath . We can use the results of Section 2.2.1 now
and find that

  -- -------- -- --------
     @xmath      (2.71)
  -- -------- -- --------

and that this equal to the exponential of the sum of all connected
diagrams.

#### 2.2.3 Falling powers, diagrams and Grassmann variables

Another, small, example of the use of Feynman diagrams is in the
representation of the numbers

  -- -------- -- --------
     @xmath      (2.72)
  -- -------- -- --------

It can be derived from the relation

  -- -------- -- --------
     @xmath      (2.73)
  -- -------- -- --------

where the second sum on the r.h.s. is over all permutations of @xmath .
This relation is derived in Appendix 2.2.4 . It allows for following
diagrammatic interpretation.

Consider ‘arrowed’ vertices of order two, that is, vertices of order two
with distinct legs: one incoming and one outgoing. They can be connected
with the rule that outgoing legs may only be connected to incoming legs
and vice versa. The legs are connected with an ‘arrowed’ line,
representing a @xmath , and the vertices represent the convolution
@xmath of the two lines arriving at and starting from that vertex. Up to
an overall minus sign, the r.h.s. of Eq. ( 2.73 ) is equal to the sum of
all possible diagrams with @xmath distinct ‘arrowed’ vertices, with the
extra rule that every closed loop gives a factor @xmath . The overall
minus sign is equal to @xmath . For example,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.74)
  -- -------- -------- -- --------

It is useful to consider diagrams that look exactly the same as one
diagram again. In the example above, this applies to the last two
diagrams, and to the second, the third and the fourth diagram. The extra
number the contribution of a diagram has to be multiplied with is turned
into a symmetry factor by considering @xmath instead of @xmath .

##### 2.2.3.1 Grassmann variables

The numbers @xmath can also be written in another way. We introduce
@xmath Grassmann variables @xmath and @xmath , @xmath . They all
anti-commute with each other and commute with complex numbers:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.75)
     @xmath   @xmath      (2.76)
  -- -------- -------- -- --------

These variables are nilpotent, i.e., @xmath for all @xmath . Products of
even numbers of these variables commute with all other combinations.
Furthermore, we introduce the ‘integral’ of these variables, which maps
sums of products of them onto @xmath . It is linear over @xmath and
defined by the relations

  -- -------- -------- -- --------
     @xmath   @xmath      (2.77)
     @xmath   @xmath      (2.78)
  -- -------- -------- -- --------

Notice that the first integral is also zero if @xmath , because then
there has to be a pair @xmath with @xmath in the product of @xmath ’s,
so that @xmath . The same holds if @xmath . A useful calculation of such
an integral is

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (2.79)
  -- -------- -------- -- --------

Another useful relation is the following. If @xmath is a complex number,
then

  -- -- -- --------
           (2.80)
  -- -- -- --------

since only the term with @xmath is non-zero. Let us now introduce the
‘measure’ @xmath , defined by

  -- -------- -- --------
     @xmath      (2.81)
  -- -------- -- --------

Using the result of the previous calculations, we find

  -- -- -- --------
           (2.82)
  -- -- -- --------

since only the term with @xmath contributes. If we combine the two
representations of the numbers @xmath , we can draw the conclusion that

  -- -------- -- --------
     @xmath      (2.83)
  -- -------- -- --------

#### 2.2.4 Gaussian measures and Grassmann variables

As a final application, we are going to combine the previous two
examples. Let @xmath be a Gaussian measure on a space of real bounded
functions on a subset @xmath of @xmath , let @xmath and @xmath , @xmath
be a set of Grassmann variables, and let us denote

  -- -------- -- --------
     @xmath      (2.84)
  -- -------- -- --------

If @xmath is a sequence of functionals acting on @xmath as

  -- -------- -- --------
     @xmath      (2.85)
  -- -------- -- --------

then

  -- -------- -- --------
     @xmath      (2.86)
  -- -------- -- --------

can be calculated using diagrams with @xmath vertices of type @xmath ,
@xmath vertices of type @xmath and so on, as described in Section 2.2.2
. If we apply the results of Section 2.2.3 , we see that

  -- -------- -- --------
     @xmath      (2.87)
  -- -------- -- --------

can be calculated by attaching an incoming and an outgoing ‘arrowed’ leg
to each type of vertex, and using the Feynman rules of Section 2.2.3 .
Furthermore, we see that

  -- -------- -- --------
     @xmath      (2.88)
  -- -------- -- --------

Each @xmath represents a vertex of the kind @xmath , with attached to it
an incoming and an outgoing ‘arrowed’ leg. Now, we can apply the
relations of Section 2.2.1 to arrive at the result that

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (2.89)
  -- -------- -------- -- --------

and that the sum of the connected diagrams is equal to @xmath .

Appendix 2A:  Some combinatorial relations
Consider a sequence @xmath of functions of integer arguments that are
completely symmetric in those arguments. We want to establish a relation
of the kind

  -- -------- -- --------
     @xmath      (2.90)
  -- -------- -- --------

where the second sum on the r.h.s. is over all partitions @xmath of
@xmath . Put like this, the relation is obviously incorrect, since on
the l.h.s. all permutations @xmath are counted separately, whereas on
the r.h.s., only @xmath is counted. At first instance, it seems natural
to correct for this by including a factor @xmath on the l.h.s.. This is,
however, too crude, because permutations of equal @xmath ’s are not
counted separately. This can again be cured by including a factor @xmath
on the r.h.s., and we arrive at

  -- -------- -- --------
     @xmath      (2.91)
  -- -------- -- --------

Note that @xmath is equal to @xmath or @xmath .

Consider the @xmath matrix @xmath , depending on @xmath integer
variables @xmath that run from @xmath to @xmath . The matrix is defined
by

  -- -------- -- --------
     @xmath      (2.92)
  -- -------- -- --------

Every diagonal element of this matrix is equal to @xmath for every
configuration @xmath , because @xmath if @xmath . Now consider a
configuration @xmath for which all @xmath ’s are not equal, except of
one pair @xmath with @xmath . Then @xmath , and we see that row @xmath
and row @xmath are the same, so that @xmath . It is easy to see that
this will always be the case if there are pairs @xmath with @xmath . The
number of configurations @xmath for which all @xmath ’s are not equal is
precisely @xmath , so that we can write down the following identity

  -- -------- -- --------
     @xmath      (2.93)
  -- -------- -- --------

where the second sum on the r.h.s. is over all permutations of @xmath .
We just used the formula @xmath to arrive at this result.

## Chapter 3 The formalism of quadratic discrepancies

Discrepancies are measures of non-uniformity of point sets in subsets of
@xmath -dimensional Euclidean space. They are interesting in connection
with numerical integration, because the integration error can be
estimated in terms of the discrepancy of the point set used (Section
1.1.3 ). Their definition will be given in the first section of this
chapter. An interesting feature of discrepancies is their probability
distribution (Section 2.1.6 ), and large part of this chapter concerns
with techniques, borrowed from quantum field theory, to calculate them
for the so called quadratic discrepancies. In the last section, some
examples of quadratic discrepancies are given.

\minitoc

### 3.1 Definition of discrepancy

The only subspace of @xmath , @xmath that will be considered is the
@xmath -dimensional unit hypercube @xmath since, in practice, an
integration problem can always be reduced to one on @xmath . A point set
@xmath consists of @xmath points @xmath , @xmath . The coordinates of
the points will be labeled with an upper index as @xmath , @xmath . For
an arbitrary subset @xmath of @xmath , we define the characteristic
function @xmath such that

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

The integral of a function @xmath , Lebesgue integrable on @xmath , we
denote by

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

so that the Lebesgue measure of a region @xmath is given by @xmath . For
every point set @xmath , we introduce the estimate @xmath of @xmath
using @xmath by

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

#### 3.1.1 The original definition

Naturally, a discrepancy of the point set is defined with respect to a
certain family @xmath of measurable subsets of @xmath as follows

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

It is the largest absolute error one makes if one tries to estimate the
measure of every subset @xmath by counting the number of points from
@xmath in the subset. The idea is that, if a point set is suitable for
estimating the measures of all subsets well, so that @xmath is small,
then the point set must be distributed very “uniformly” over @xmath . In
order to arrive at a natural notion of uniformity, the family @xmath of
subsets has to be chosen sensibly. In principle, for every finite point
set a subset of @xmath can be found, such that the discrepancy takes its
maximum value, which is @xmath . A first restriction on the subsets one
can for example take is that they have to be convex, i.e., for every
@xmath in @xmath and every @xmath also @xmath is in @xmath .

This restriction still leaves many possible choices for the subsets,
leading to different discrepancies (cf. [ 7 ] ). An important example is
the so called star discrepancy , denoted by @xmath , for which the
family @xmath consists of all subsets

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

It consists of all hyper-rectangles spanned by the origin and points
@xmath . For this discrepancy, various theorems are derived (cf. [ 7 ]
), such as Koksma-Hlawka’s inequality (Eq. ( 1.12 )). In one dimension,
@xmath is equal to the statistic of the Kolmogorov-Smirnov test for the
hypothesis that the points are distributed randomly following the
uniform distribution (cf. [ 3 ] ).

In order to proceed in a direction that leads to a definition of
discrepancy that we will use, we introduce the @xmath - discrepancy . If
we denote @xmath , then

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

It is the average over @xmath of the @xmath power of the error made by
estimating the measures of the subsets using @xmath . This definition
assures the limit @xmath . Furthermore, it satisfies the bounds

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where @xmath is independent of the point set [ 7 ] . For us, the case of
@xmath is in particular interesting. The expression for @xmath can be
evaluated further, with the result that

  -- -- -- -------
           (3.8)
  -- -- -- -------

where

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

and

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

In this case of @xmath , the discrepancy is called quadratic , and is
completely determined by the two-point function @xmath .

#### 3.1.2 Quadratic discrepancies

The quadratic discrepancy invites generalizations. The number @xmath can
be interpreted as a correlation between the points @xmath and @xmath ,
and the discrepancy is a function of the sum over all correlations in
the point set. Various quadratic discrepancies can be defined by
choosing different, and sensible, two-point correlation functions. The
two-point functions can, however, also be interpreted differently,
leading to another approach to quadratic discrepancies. This approach is
based on the insight by H. Woźniakowski [ 18 ] , that @xmath can be
written as an average case complexity . We will demonstrate this here by
constructing the probability measure with respect to which the
discrepancy can be written as an average. For more details about the
formalism, we refer to [ 5 ] .

Consider the Hilbert space @xmath of (equivalence classes of almost
everywhere equal) real quadratically integrable functions on @xmath . We
denote the inner product and the norm on @xmath by

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

Let us, as before, denote @xmath and let @xmath be the “primitivation”
operator, defined by

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

@xmath is a continuous linear map from @xmath to the space @xmath of
continuous functions that vanish if any coordinate @xmath . It even is a
Hilbert-Schmidt operator: if @xmath is a basis of @xmath , then @xmath .

The dual space @xmath , i.e. the space of all continuous linear
functionals on @xmath , consists of all bounded measures on @xmath . For
such a measure @xmath , we will use the notation

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

although “ @xmath ” cannot always be seen as a function value. The
transposed @xmath , which acts on @xmath through the definition @xmath ,
is then simply given by

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

and @xmath is a bounded function. Notice that, because @xmath is
isomorphic to its dual, we can make the straightforward identification
@xmath , where @xmath is the adjoint of @xmath . There is a unique
Gaussian probability measure @xmath on @xmath which has Fourier
transform

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

It is going to serve as the probability measure mentioned before. In
literature, it is known as the Wiener sheet measure . By taking @xmath ,
where @xmath are real variables and @xmath denotes the Dirac measure
@xmath , and differentiating the above equation twice with respect to
@xmath , it is easy to see that @xmath has two point function

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

So the two-point function with which the discrepancy is defined, is the
two-point function of the Gaussian measure @xmath on @xmath . Using this
equation and Eq. ( 3.8 ) and Eq. ( 3.9 ), it is easy to see that

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

So we can identify the square of the discrepancy as the average case
complexity, defined as the squared integration error averaged over
@xmath .

The particular choice of @xmath led to the Wiener sheet measure. In
principle, any Hilbert-Schmidt operator can be used, leading to another
Gaussian measure @xmath . We want to apply this generalization. For
further analysis, it will therefore appear to be convenient to use the
square, as it stands on the l.h.s., as definition for quadratic
discrepancy. Furthermore, the definition of discrepancy is such, that it
goes to zero, if the number of points in a uniformly distributed point
set goes to infinity. This is immediately clear from inequality ( 1.12
), the l.h.s. of which goes to zero if @xmath consists of independent
random points and @xmath goes to infinity. In fact, Monte Carlo
integration tells us that it goes to zero as @xmath . Therefore, it
seems natural to use @xmath times the square of the original definition
of the quadratic discrepancy, especially since we want to calculate
probability distributions of discrepancies for large @xmath . This
multiplication with the factor @xmath is equivalent with considering
@xmath times the average of @xmath random variables (with zero mean)
when applying the central limit theorem in probabilistic analyses.

##### 3.1.2.1 Definition quadratic discrepancy

We conclude this section with the definition of discrepancy we will
further use. Given a Hilbert-Schmidt operator @xmath on @xmath , there
is a Gaussian measure @xmath on @xmath with Fourier transform

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

In the case of the Wiener sheet measure, it even is a measure on a space
of continuous functions, but for the general case this is not necessary.
The operator @xmath should only be such, that it maps @xmath
continuously on a space @xmath of continuous functions, so that there is
a number @xmath such that @xmath for any @xmath . In Appendix 3.4 , we
show that in that case the Dirac measure can be properly defined under
the measure @xmath , which we will need. We shall omit the label @xmath
at the integral symbol from now on.

We define the discrepancy of a point sets @xmath in @xmath as the
quadratic integration error, made by using @xmath , averaged under
@xmath over @xmath :

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

From now on, we will omit the argument @xmath when we denote the
discrepancy. Using this definition, the discrepancy can again be written
in terms of two-point functions. With the Hilbert-Schmidt operator comes
a two-point function

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

Because the combination @xmath appears in the average, it is useful to
introduce the notation

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

and the reduced two-point function

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

which can be written in terms of @xmath like in Eq. ( 3.9 ). It has the
important feature that it integrates to zero with respect to each of its
arguments. The discrepancy is given by

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

i.e., as a sum over two-point correlations between the points of @xmath
. The correlation function @xmath is determined by the operator @xmath
in this formulation.

### 3.2 The generating function

When @xmath consists of uniformly distributed random points, then the
discrepancy @xmath is a random variable with a certain probability
density. In [ 23 , 24 , 25 ] , the generating function

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

has been used to calculate it. We will also concentrate on the
calculation of @xmath . Given @xmath , the probability density can then
be calculated by the inverse Laplace transform (Section 2.1.3 ).

It will turn out that it is far to complicated to calculate @xmath
analytically. For large number @xmath of points, however, a series
expansion in @xmath can be made which can be calculated term by term. We
intend to calculate the generating function from an explicit expression
in terms of @xmath , which we will now derive.

#### 3.2.1 The generating function as an average over functions

First, we introduce the following bounded measure on @xmath , which
consists of a sum of Dirac measures, centered around the points of the
point set, minus one:

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

The integration error of a function @xmath and the discrepancy can be
written in terms of @xmath :

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

Using this expression for the discrepancy and the relation of Eq. ( 3.18
), we can write

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

If now the definition of @xmath is used, and the integrals over @xmath
are performed on the l.h.s. and the r.h.s., we arrive at

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

where we denote

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

#### 3.2.2 Gauge freedom

For the calculation of the generating function of the probability
density of the discrepancy, there exists a freedom in the choice of the
operator @xmath with which the measure is defined, as we will show now.
Let @xmath act on @xmath such that @xmath is a Hilbert-Schmidt operator
on @xmath that maps @xmath continuously on a space of continuous
functions. For each functional @xmath on @xmath there is a functional
@xmath which maps @xmath onto @xmath . We use this to define the measure
@xmath by

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

so that its Fourier transform is given by

  -- -- -- --------
           (3.31)
  -- -- -- --------

and its two-point function is given by

  -- -------- -- --------
     @xmath      (3.32)
  -- -------- -- --------

If @xmath is such that @xmath for all @xmath , then

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

and we call this property the gauge freedom . It leads to a freedom in
the choice of the operator @xmath with which the measure is defined, and
we call these choices the gauges . Most gauge transformations @xmath we
consider are global translations that are characterized by a functional
@xmath , and are given by @xmath for all @xmath . They trivially satisfy
@xmath .

An example of a gauge transformation that satisfies the criteria is
simply given by @xmath . It results in the Landau gauge , for which all
@xmath satisfy @xmath . This is, actually, the natural gauge to choose,
because it restricts the analysis to functions that integrate to zero,
so that the integration error becomes equal to the average of the
function over the point set. The existence of the gauge freedom
originates from the fact that the integration error is the same for
integrands that differ only by a constant. The two-point function is
equal to the reduced two-point function in the Landau gauge: @xmath .

From now on, we will omit the label ‘ @xmath ’ in the notation of the
measures and the two-point functions.

#### 3.2.3 The path integral, perturbation theory and instantons

The connection of the foregoing with Euclidean quantum field theory is
made via the path integral formulation. The emphasis is put on the use
of perturbation theory.

##### 3.2.3.1 The path integral

We want to express the generating function, as given by Eq. ( 3.28 ), in
terms of a Euclidean path integral (cf. [ 9 ] ). We have to introduce
the free action , which is a quadratic functional

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

and we arrive at the path integral formulation of the measure @xmath by
making the identification

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

It is a formal expression, where @xmath represents the product over the
whole of @xmath of the “infinitesimal volume elements” @xmath . This is,
of course, ill-defined, and it gets even worse since the set of
functions @xmath , for which @xmath is finite, in general has measure
zero.

One thing we want to be more precise about is the fact that, in first
instance, @xmath is only well-defined on the image @xmath of @xmath
under @xmath , while this set has measure zero. The members of the
subsets of @xmath that do not have measure zero, however, usually do
satisfy the boundary conditions imposed by @xmath . We assume that these
boundary conditions can be expressed by a finite number of linear
equations

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

Then, the action should be extended as follows:

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

where @xmath for all labels @xmath . The “infinitesimal volume element”
@xmath gets a factor @xmath for every @xmath in order for the measure to
stay normalized to one. The extra terms in the action assure that the
measure is zero if a function does not satisfy the boundary conditions.
Notice that the action is still quadratic in @xmath .

If we apply all this to the expression of Eq. ( 3.28 ) for the
generating function, we find that

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

with an action @xmath given by

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

##### 3.2.3.2 Perturbation theory and instantons

In the action of Eq. ( 3.39 ), @xmath appears to be a natural expansion
parameter if @xmath is large. An expansion around @xmath will
automatically result in an expansion of the generating function around
@xmath . Furthermore, it corresponds to an expansion of the action
around @xmath . An expansion of the action to evaluate the generating
function, however, only makes sense when it is an expansion around a
minimum, so that it represents a saddle point approximation of the path
integral. Therefore, a straightforward expansion such as just proposed,
is only correct if it is an expansion around the minimum of the action,
that is, if the trivial solution @xmath gives the only minimum of the
action. General extrema of the action are given by solutions of the
field equation

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

where @xmath represents the self-adjoint operator @xmath including the
boundary conditions @xmath and possible boundary conditions coming from
the fact that @xmath is not necessarily self-adjoint ¹ ¹ 1 For example,
if @xmath and @xmath , then @xmath , so that the extra boundary
condition is @xmath . . Depending on the value of @xmath , non-trivial
solutions may also exist. At this point it can be said that, because
@xmath is real, non-trivial solutions only exist if @xmath is real and
non-zero so that @xmath . In the analysis of the solutions we therefore
can do a scaling @xmath so that the action for these solutions is given
by

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

These non-trivial solutions we call instantons (cf. [ 10 ] ), although
this may not be a rigorously correct nomenclature, in the field
theoretical sense, for all situations we will encounter. Notice that
instantons under different gauges only differ by a constant: if two
gauges are connected by a global translation @xmath , and @xmath is an
instanton in the @xmath -gauge, then @xmath is an instanton in the
@xmath -gauge. The values of @xmath for which they appear and the value
of the action are gauge invariant, as can be concluded from Eq. ( 3.40 )
and Eq. ( 3.41 ).

If @xmath becomes large, then the contribution of an instanton to the
path integral will behave as @xmath , where @xmath does not depend on
@xmath (Notice that @xmath does not depend on @xmath because the field
equation for these rescaled functions does not depend on @xmath .). The
@xmath -like behavior of the instanton contribution makes it invisible
in the perturbative expansion around @xmath . If @xmath is larger than
zero, this will not be a problem, because the contribution will be very
small. If, however, @xmath is equal to zero, then the contribution will
be more substantial, and it will even explode if @xmath is negative.
Notice that, to be able to do make a perturbation series around @xmath ,
the action has to be zero for this solution, for else the terms would
all become zero or would explode for large @xmath .

The escape from this possible disaster is given by the fact that @xmath
has to be real and larger than zero for instantons to exist, and we want
to integrate @xmath along the imaginary @xmath -axis (Section 2.1.3 ).
Also in the end, when we want to close the integration contour in the
complex @xmath -plane to the right, we will not meet the problem,
because the function we want to integrate is an expansion in @xmath
around @xmath that can be integrated term by term. Problems might only
occur when instantons exist for values of @xmath that are arbitrarily
close to @xmath . We will confirm for a few cases that this does not
happen.

#### 3.2.4 Feynman rules to calculate the @xmath corrections

We just suggested a straight-forward expansion in @xmath of @xmath to
calculate @xmath perturbatively. This way, however, the calculation of
the perturbation series becomes very cumbersome, and the reason for this
is the following. We want to use the fact that an expansion in @xmath
corresponds to an expansion around @xmath of the part of the action that
is non-quadratic in @xmath . The subsequent terms in the expansions are
therefore proportional to moments of a Gaussian measure, and can be
calculated using diagrams (Section 2.2.2 ). These diagrams, the Feynman
diagrams , consist of lines representing two-point functions and
vertices representing convolutions of two-point functions. Because the
action is non-local, i.e. it cannot be written as a single integral over
a Lagrangian density because of the logarithm in Eq. ( 3.39 ), the total
path integral, thus the total sum of all diagrams, cannot be seen as the
exponential of all connected diagrams, and it is this that makes the
calculations difficult.

In order to circumvent this obstacle, we first of all use the Landau
gauge, so that @xmath for all @xmath . Secondly, we introduce @xmath
Grassmann variables @xmath and @xmath , @xmath , as in Section 2.2.3 ,
so that we can write

  -- -------- -- --------
     @xmath      (3.42)
  -- -------- -- --------

If we now define

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

we can write

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

which has exactly the form of the r.h.s. of Eq. ( 2.89 ). Because the
functionals @xmath are also of the kind of Eq. ( 2.85 ), we can use the
statement of Eq. ( 2.89 ), that @xmath is equal to the sum of the
contributions of all Feynman diagrams that can be constructed with the
vertices

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

and with the rules that all incoming legs have to be connected to
outgoing legs and vice versa, and all dashed legs have to be connected
to dashed legs. The lines in the obtained diagrams stand for propagators
:

  -- -------- -- --------
     @xmath      (3.47)
     @xmath      (3.48)
  -- -------- -- --------

and to calculate the contribution of a diagram, boson propagators have
to be convoluted in the vertices as @xmath , fermion propagators as
@xmath , and then these convolutions have to be multiplied. To get the
final result for a diagram, a factor @xmath has to be included for every
vertex of order @xmath , and the symmetry factor has to be included.

The contribution of the fermionic part can easily be determined, for
every fermion loop only gives a factor @xmath . The main problem is to
calculate the bosonic part. Furthermore, only the connected diagrams
have to be calculated, since the sum of their contributions is equal to

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

Because every vertex carries a power of @xmath that is equal to its
order, the expansion in @xmath is an expansion in the complexity of the
diagrams, which can be systematically evaluated.

#### 3.2.5 Gaussian measures on a countable basis

Because @xmath is a Hilbert-Schmidt operator on @xmath , @xmath is a
self adjoint compact operator on @xmath , and there exists an
orthonormal basis @xmath of @xmath , consisting of eigenvectors of
@xmath . If we denote the eigenvalues by @xmath , then the eigenvalue
equation is given by

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

As the notation suggests, they are positive since @xmath . Notice that,
because @xmath is Hilbert-Schmidt, @xmath , and this leads immediately
to the spectral decomposition of @xmath , which is simply given by

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

In principle, the basis and the eigenvalues can be used as an
alternative definition of a quadratic discrepancy. They naturally
introduce the spectral decomposition of a two-point function and a
reduced two-point function. The reasonable requirement of the existence
of @xmath leads to

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

which is satisfied if @xmath comes from a Hilbert-Schmidt operator. If
we denote the expansion of a function @xmath by

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

then the probability measure @xmath can be written as

  -- -------- -- --------
     @xmath      (3.54)
  -- -------- -- --------

The basis functions will often be refered to as modes , originating from
an example of a quadratic discrepancy (the Fourier diaphony), for which
the basis is the Fourier basis without the constant mode.

With different gauges come different bases and strengths. We call a
gauge in which the basis is orthonormal a Feynman gauge. If the Landau
gauge is used, in which @xmath , then the basis functions have to
integrate to zero:

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

where the label indicates the Landau gauge. These functions are the
solutions of the eigenvalue equation

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

It will not always be possible to find the Landau basis. In terms of a
basis that is not in the Landau gauge, @xmath is given by

  -- -------- -- --------
     @xmath      (3.57)
  -- -------- -- --------

### 3.3 Examples

Some explicit, and well known, examples of quadratic discrepancies are
introduced, and cast in the formalism of this chapter.

#### 3.3.1 The @xmath-discrepancy

In our definition, the @xmath -discrepancy is @xmath times the square of
the case of @xmath in Eq. ( 3.6 ). The operator @xmath and the two-point
function @xmath are given by

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

where @xmath . The boundary conditions imposed by @xmath are given by
@xmath if at least one of the coordinates @xmath . The basis functions
can now be found by solving the eigenvalue equation @xmath . The
equation factorizes for the different coordinates, and is most easily
solved by differentiating twice on the l.h.s. and the r.h.s.. The
one-dimensional solutions, that satisfy the boundary conditions, are

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

The set @xmath clearly is orthonormal, and it is the basis in the
one-dimensional case. For @xmath , the basis and the strengths are given
by all possible products

  -- -------- -- --------
     @xmath      (3.60)
  -- -------- -- --------

where @xmath and @xmath for @xmath . The reduced two-point function is
given by

  -- -------- -- --------
     @xmath      (3.61)
  -- -------- -- --------

In one dimension, the eigenfunctions and eigenvalues are

  -- -------- -- --------
     @xmath      (3.62)
  -- -------- -- --------

For @xmath , it is difficult to find all solutions to the eigenvalue
equation, and we will address this problem in Section 5.2.2 .

#### 3.3.2 The Cramér-von Mises goodness-of-fit test

The @xmath -discrepancy is equivalent with the statistic of the
Cramér-von Mises goodness-of-fit test, which tests the hypotheses that
@xmath data @xmath are distributed independently following a cumulative
distribution function @xmath (cf. [ 3 , 19 ] ). Consider, for
simplicity, the one-dimensional case, so that @xmath , and denote @xmath
and @xmath . The statistic is given by

  -- -- -- --------
           (3.63)
  -- -- -- --------

where we put the extra factor @xmath again, just as in the case of the
discrepancies. Because @xmath is a cumulative distribution function, its
inverse @xmath is uniquely defined, and we can re-write the statistic as

  -- -------- -- --------
     @xmath      (3.64)
  -- -------- -- --------

where we denote @xmath . But @xmath , so that @xmath is equal to the
@xmath -discrepancy of the points @xmath . The interpretation of the
statistic is slightly different from the @xmath -discrepancy, but the
probability distribution is exactly the same.

#### 3.3.3 The Fourier diaphony

For the Fourier diaphony, @xmath should impose periodic boundary
conditions. In one dimension, a simple Hilbert-Schmidt operator that
achieves this is given by

  -- -------- -- --------
     @xmath      (3.65)
  -- -------- -- --------

where @xmath . The term of a half in the integration kernel assures that
@xmath integrates to zero, so that the discrepancy is formulated in the
Landau gauge from the start. The choice of the factors seems odd, but
will appear to be the natural choice for the extension to more
dimensions. The two-point function is given by

  -- -------- -- --------
     @xmath      (3.66)
  -- -------- -- --------

Notice that the two-point function only depends on @xmath and therefore
is translation invariant, i.e., @xmath for all @xmath . As a result of
this, all information about @xmath is contained in the function @xmath ,
and we have

  -- -------- -- --------
     @xmath      (3.67)
  -- -------- -- --------

The factor @xmath was chosen such, that @xmath . The set @xmath of
solutions of the eigenvalue equation @xmath is just the Fourier basis on
@xmath without the constant mode:

  -- -------- -- --------
     @xmath      (3.68)
  -- -------- -- --------

with eigenvalues

  -- -------- -- --------
     @xmath      (3.69)
  -- -------- -- --------

The function @xmath is not a member of the basis because of the Landau
gauge. Only functions that integrate to zero are present.

In @xmath dimensions, the operator @xmath is extended as follows. Let
@xmath denote coordinate wise subtraction, then

  -- -------- -- --------
     @xmath      (3.70)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (3.71)
  -- -------- -- --------

The @xmath -dimensional integration kernel is obtained from the
one-dimensional one by adding the constant mode and taking the product
over the coordinates. The extra term of @xmath assures that the constant
mode in @xmath dimensions disappears again. The new factor assures that
the @xmath -dimensional two-point function is equal to one in the
origin:

  -- -------- -- --------
     @xmath      (3.72)
  -- -------- -- --------

The basis in @xmath -dimensions consists of all products over
coordinates of the one-dimensional basis including the constant mode
@xmath . The only product that does not appear is, of course, @xmath .
The eigenvalue coming with @xmath is determined by the choice of @xmath
, and equal to @xmath . The eigenvalues in @xmath dimensions are just
the properly normalized products of the one-dimensional ones. If we
denote @xmath and introduce

  -- -------- -- --------
     @xmath      (3.73)
  -- -------- -- --------

then

  -- -------- -- --------
     @xmath      (3.74)
  -- -------- -- --------

The Fourier diaphony is often written in terms of the complex Fourier
basis of @xmath . Then, it attains the form

  -- -------- -- --------
     @xmath      (3.75)
  -- -------- -- --------

where @xmath , and the first sum is over all @xmath except the constant
mode @xmath . Introduced as in this section, the diaphony is again
@xmath times the square of the definition as given in, for example, [ 20
] .

#### 3.3.4 The Lego discrepancy and the @xmath-statistic

For the Lego discrepancy, the image @xmath is a finite dimensional
vector space. It is obtained by dividing @xmath into @xmath disjoint
‘bins’ @xmath with @xmath , and taking

  -- -------- -- --------
     @xmath      (3.76)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (3.77)
  -- -------- -- --------

and, in first instance, the strengths @xmath are not specified. @xmath
maps @xmath onto the space of functions that are defined with a
precision up to the size of the bins @xmath . Notice that @xmath where
@xmath is the Kronecker delta symbol, and that @xmath , @xmath . The
two-point function is given by

  -- -------- -- --------
     @xmath      (3.78)
  -- -------- -- --------

Clearly, this model is dimension-independent, in the sense that the only
information on the dimension of @xmath is that contained in the value of
@xmath : if the dissection of @xmath into bins is of the hyper-cubic
type with @xmath bins along each axis, then we shall have @xmath . Also,
a general area-preserving mapping of @xmath onto itself will leave the
discrepancy invariant: it will lead to a distortion (and possibly a
dissection) of the various bins, but this influences neither @xmath nor
(by definition) @xmath . Owing to the finiteness of @xmath , a finite
point set can, in fact, have zero discrepancy in this case, namely if
every bin @xmath contains precisely @xmath points (assuming this number
to be integer for every @xmath ).

Because @xmath is @xmath -dimensional, it is easiest to formulate
everything in @xmath . We define

  -- -------- -- --------
     @xmath      (3.79)
  -- -------- -- --------

and divide @xmath into equivalence classes by the prescription that
@xmath if @xmath . This space is @xmath , and it is isomorphic to @xmath
with inner product @xmath . The operator @xmath restricted to @xmath is
given by

  -- -------- -- --------
     @xmath      (3.80)
  -- -------- -- --------

Notice that @xmath is self adjoint. The Gaussian measure @xmath can now
be defined rigorously in terms of a finite-dimensional path integral. If
@xmath is a functional on @xmath , then

  -- -------- -- --------
     @xmath      (3.81)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (3.82)
  -- -------- -- --------

The two-point function and the reduced two-point function can be written
in terms of matrices as

  -- -------- -- --------
     @xmath      (3.83)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (3.84)
  -- -------- -- --------

In the path integral formulation of the generating function, @xmath
occurs, and the series expansion of @xmath and the properties of the
characteristic functions tell us that @xmath , so that the generating
function is given by

  -- -------- -- --------
     @xmath      (3.85)
  -- -------- -- --------

The discrepancy itself can be written as

  -- -------- -- --------
     @xmath      (3.86)
  -- -------- -- --------

is the number of points in bin @xmath .

##### 3.3.4.1 The @xmath-statistic

We did not yet specify the strengths @xmath , but we will in particular
look at the choice for which @xmath for all @xmath . In this case,
@xmath consists of functions in which the largest fluctuations appear
over the smallest intervals. Although not a priori attractive in many
cases, this choice is actually quite appropriate for, e.g. particle
physics where cross sections display precisely this kind of behavior.
The reduced two-point function attains the simple form @xmath and the
discrepancy becomes

  -- -------- -- --------
     @xmath      (3.87)
  -- -------- -- --------

which is nothing but the @xmath -statistic for @xmath data points
distributed over @xmath bins with expected number of points @xmath (cf.
[ 3 ] ).

### 3.4 Appendices

Appendix 3A
Let @xmath be the Hilbert space of (equivalence classes of almost
everywhere equal) real quadratically integrable functions on @xmath ,
with inner product and norm

  -- -------- -- --------
     @xmath      (3.88)
  -- -------- -- --------

A Hilbert space is self-dual, i.e. there is an isomorphism between
@xmath and its dual space @xmath of continuous linear functions @xmath .
It induces an invertible mapping @xmath such that @xmath for all @xmath
, and we write @xmath .

Let @xmath be a Hilbert-Schmidt operator on @xmath , and @xmath its
transposed which acts on @xmath through the definition @xmath . It is
easy to see that @xmath is a Hilbert-Schmidt operator on @xmath and that
@xmath exists for every @xmath . Furthermore, it is well known (cf. [ 5
] ) that there exists a Gaussian measure @xmath on @xmath with Fourier
transform

  -- -------- -- --------
     @xmath      (3.89)
  -- -------- -- --------

By inserting @xmath where @xmath is a real variable, and differentiating
the above equation twice with respect to @xmath before putting it to
zero, one obtains the relation

  -- -------- -- --------
     @xmath      (3.90)
  -- -------- -- --------

With @xmath , a Hilbert space @xmath can be defined, where the norm is
given by

  -- -------- -- --------
     @xmath      (3.91)
  -- -------- -- --------

It is clear that a mapping @xmath can directly be defined on the whole
of @xmath , but we need more: we want to apply it to Dirac-measures,
which @xmath does not contain. Consider therefore the Hilbert space
@xmath , which is the completion of @xmath under the norm

  -- -------- -- --------
     @xmath      (3.92)
  -- -------- -- --------

@xmath can be interpreted as a continuous mapping from @xmath to @xmath
, with @xmath . Furthermore, @xmath can be extended to the whole of
@xmath since it is an isometry:

  -- -- -- --------
           (3.93)
  -- -- -- --------

Now, suppose that @xmath maps @xmath continuously onto a space @xmath of
continuous functions, such that

  -- -- -- --------
           (3.94)
  -- -- -- --------

The dual space @xmath of @xmath consists of bounded measures on @xmath ,
and containes the Dirac-measures. Then we have for every @xmath and
every @xmath :

  -- -- -- --------
           (3.95)
  -- -- -- --------

so that @xmath maps @xmath continuously onto @xmath . Therefore, @xmath
, and @xmath can be applied to @xmath .

Appendix 3B
For the proof that ( 3.94 ) holds in case of the Fourier diaphony, we
use that there is obviously a number @xmath such that

  -- -------- -- --------
     @xmath      (3.96)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (3.97)
  -- -------- -- --------

Now, we can apply the Cauchy-Schwarz inequality, with the result that
for all @xmath

  -- -------- -- --------
     @xmath      (3.98)
  -- -------- -- --------

## Chapter 4 Instantons for discrepancies

It is mentioned in Section 3.2.3 that an expansion of the path integral
representation of the generating function of quadratic discrepancies
around the trivial solution of the field equation is only correct, if
this solution gives the minimum of the action. Furthermore, it is
suggested that the non-trivial solutions, called instantons , might
spoil the perturbation expansion if they exist for real values of the
order parameter @xmath of the generating function that are arbitrarily
close to @xmath . In this chapter, we take a closer look at the issue
for the Lego discrepancy and the @xmath -discrepancy in one dimension,
and show that instantons exist but do not threaten the perturbative
expansion.

For the the @xmath -case, a method had to be developed to analyze the
singularity structures of the solutions of implicit function equations
with numerical help of a computer which is presented in Section 4.4 .

\minitoc

### 4.1 An alternative derivation of the path integral formulation

We start with an alternative derivation of the representation of the
generating function as a path integral. For the Lego discrepancy, this
goes as follows. We consider the case for which @xmath for all @xmath ,
so that the discrepancy is just the @xmath -statistic

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

is the number of points @xmath in bin @xmath . If the points @xmath are
truly randomly distributed, the variables @xmath are distributed
according to a multinomial distribution, so that the generating function
is given by

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where the summation is over all configurations @xmath which satisfy
@xmath . Notice that @xmath for every @xmath , so that the generating
function is not defined if @xmath for the values of @xmath with @xmath .
Using Gaussian integration rules and the generalized binomial theorem,
it is easy to see that Eq. ( 4.2 ) can be written as

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

with @xmath . By writing the @xmath -th power as a power of @xmath and
substituting @xmath , the path integral of Eq. ( 3.85 ) is obtained.

For the @xmath -discrepancy in one dimension, @xmath with the boundary
condition that @xmath (Section 3.3.1 ). The action is therefore given by

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath . We show now that there is a naïve continuum limit with
this result. We use the fact that the discrepancy can be defined as the
naïve continuum limit of

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

@xmath is the discretized version of the @xmath -discrepancy, obtained
when in Eq. ( 3.6 ) the average over a finite number of points @xmath ,
@xmath is taken, instead of the average over the whole of @xmath .
Notice that a whole class of ‘discrete’ discrepancies can be written as
Eq. ( 4.5 ), by choosing different expressions for the @xmath and the
@xmath . Just like the Lego discrepancy, such a discrepancy can be
written in terms of variables @xmath that count the number of points
@xmath in bin @xmath :

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

In the case of the @xmath -type discrepancy, the matrix @xmath is given
by @xmath . The generating function is again given as the expectation
value under the multinomial distribution. If we assume that the matrix
@xmath is invertible and positive definite, as it is for the @xmath
-type discrepancy, use the Gaussian integration rules and the
generalized binomial theorem and do the appropriate coordinate
transformations, we find

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

with

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

For the @xmath -type discrepancy the inverse @xmath of the matrix @xmath
is easy to find and we get

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

so that a naïve continuum limit clearly produces Eq. ( 4.4 ).

### 4.2 Instantons for the Lego discrepancy

We start this section with a repetition of the statement that
non-trivial instanton solutions only exist if @xmath (Section 3.2.3 ).
In order to investigate the instantons, we analyze the action in terms
of the variables @xmath , that is, we consider the integral @xmath ,
with

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

The sum is over @xmath . We are interested in the minima of @xmath . The
‘perturbative’ minimum @xmath , @xmath corresponds to @xmath , @xmath ,
and general extrema of @xmath are situated at points @xmath which are
solutions of the equations

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

If @xmath is positive, @xmath , and therefore @xmath , has to be
positive for every @xmath . The result is that the @xmath can take at
most two values in one solution @xmath (Fig. 4.1 ).

If they all take the same value, this value is @xmath , and we get the
perturbative solution. If they take two values, one of them, @xmath , is
larger that @xmath and the other, @xmath , is smaller than @xmath . With
these results, and the fact that Eq. ( 4.13 ) implies that

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

we see that there are no solutions but the perturbative one if @xmath ,
where @xmath .

In the next section, the other extremal points will be analyzed and it
will appear that minima occur with @xmath . This means that, in the
limit of @xmath , the integral of @xmath is not defined; there is a
‘wall’ in the complex @xmath plane along the positive real side of the
imaginary axis, to the right of which the generating function is not
defined. That this is not an artifact of our approach, can be seen in
the expression of the generating function given by Eq. ( 4.2 ). It is
shown there that the generating function is not defined if @xmath for
any one of the @xmath .

We know (Section 6.2 ) that, at the perturbative level, the generating
function has a singularity at @xmath , but the instanton contributions
cannot correspond with it, because they will appear already for @xmath .
However, in order to calculate the probability density @xmath with the
Laplace transform using the perturbative expression of @xmath , we can
just calculate the contribution of the singularity at @xmath , for that
is the contribution to the perturbative expansion of @xmath .

#### 4.2.1 The wall

To expose the nature of the extrema of @xmath , we have to investigate
the eigenvalues @xmath of the second derivative matrix @xmath of @xmath
in the extremal points. This matrix is given by

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

To show that @xmath becomes negative, we only use its minima, and these
correspond with extremal points in which all eigenvalues of @xmath are
positive. According to Appendix 4.6 , we are therefore only interested
in cases where the degeneracy of negative @xmath is one, for else @xmath
would be a solution. We further are only interested in cases where there
is only one negative @xmath , for if there where more, say @xmath and
@xmath with @xmath , then there would be a solution @xmath . So we see
that the only extremal points we are interested in have all co-ordinates
@xmath equal, or have one @xmath and the others equal to @xmath . If
they are all equal, then they have to be equal to @xmath , and for the
extremal point to be a minimum @xmath has to be smaller than @xmath .
This is the perturbative minimum. Whether the other extremal points are
minima depends on whether @xmath is positive in these points. The
determinant can be written as

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

Now we notice that all extremal points can be labeled with a parameter
@xmath by defining

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

We see that @xmath is a continuous and differentiable function of @xmath
and we have that @xmath . This parameterization induces a
parameterization of @xmath , and with the help of Eq. ( 4.14 ) we see
that

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

So we see that the sign of @xmath is the same as the sign of @xmath : if
an extremal point is a minimum, then @xmath .

The minimal value that @xmath can take to represent a solution is @xmath
, which corresponds to @xmath and @xmath . It is easy to see that @xmath
if @xmath and @xmath , where @xmath is the value of the weight belonging
to the co-ordinate with the value @xmath . This means that if @xmath
starts from @xmath and increases, then it will represent solutions with
@xmath , which are local maxima. We know that, if @xmath , then @xmath ,
@xmath and @xmath , so that @xmath has to become larger than @xmath at
some point. The first point where @xmath becomes equal to @xmath again
we call @xmath (Fig. 4.2 ), so

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

Also the function @xmath itself can be written in terms of @xmath in the
extremal points. We use that

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

and that @xmath if @xmath , so that

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

Now the problem arises. From the previous analysis of @xmath we know
that, if @xmath , then @xmath so that

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

Furthermore, we find that

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

so that also @xmath in @xmath . So there clearly is a region in @xmath
where @xmath and @xmath . This means that in the region @xmath there are
instanton solutions with negative action. The situation is shown in Fig.
4.2 for @xmath . A region where @xmath and @xmath is clearly visible in
@xmath .

### 4.3 Instantons for the @xmath-discrepancy

In order to investigate the instantons for the @xmath -discrepancy in
one dimension, we analyze @xmath , with @xmath as Eq. ( 4.4 ), because
this new action does not depend on @xmath :

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

where @xmath and @xmath . Extremal points of this action are solutions
of the field equation

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

that also satisfy the boundary conditions, which are @xmath at this
point. We proceed however by applying the gauge transformation @xmath ,
so that @xmath and, in this gauge, the equation becomes

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

Integration over @xmath of this equation leads to the identity @xmath .
The problem is now reduced to that of the motion of a classical particle
with a mass @xmath in a potential

  -- -- -- --------
           (4.28)
  -- -- -- --------

and the solution can be written implicitly as

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

where the integration constant @xmath , the energy , has to be larger
than zero for solutions to exist. It is easy to see that the solutions
are oscillatory and that, if @xmath is a solution with one bending
point, then also

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

is a solution for @xmath . These new solutions have the same energy, but
a larger number of bending points, namely @xmath , and the value of
@xmath increases by a factor @xmath . Hence, we can classify the
solutions according to the energy and the number bending points. This
classification in terms of the number of bending points is quite natural
and this can best be understood by looking at the limit of @xmath .
Then, the equation becomes

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

with @xmath , and the solutions are given by

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

so that the instantons are completely classified with the number of
bending points @xmath . If @xmath becomes finite, these solutions are
deformed but keep the same value of @xmath (Fig. 4.3 ). For given @xmath
there are infinitely many solutions classified by @xmath .

#### 4.3.1 Existence of instantons

We now concentrate on the instantons with one bending point, because the
numerical value of the action is independent of the number of bending
points. Those instantons are completely characterized by their energy.
The values of @xmath for which these instantons exist are defined as a
function of @xmath by Eq. ( 4.29 ), which states that

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

where @xmath and @xmath are the classical turning points. They are
solutions of @xmath with @xmath . In classical mechanics, @xmath is
proportional to the period of a particle in the potential @xmath (cf. [
14 ] ).

The function @xmath cannot be expressed in terms of elementary
functions, but a number of its properties can be derived, as we shall
now discuss. For small @xmath , a quadratic approximation of the
potential can be made with @xmath with the result that

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

The question is now whether @xmath is increasing as a function of @xmath
. To calculate @xmath for large @xmath , @xmath can be approximated by
@xmath for @xmath and by @xmath for @xmath , so that

  -- -------- -- --------
     @xmath      (4.35)
  -- -------- -- --------

so @xmath is clearly increasing for large @xmath . To analyze @xmath for
small @xmath , we make an expansion in powers of @xmath . Therefore, we
write

  -- -- -- --------
           (4.36)
  -- -- -- --------

where @xmath is a continuous solution of the implicit equation

  -- -------- -- --------
     @xmath      (4.37)
  -- -------- -- --------

with @xmath for small @xmath . In Section 4.4 it is shown that it is
given by the function values on the principal Riemann sheet of the
general continuous solution and that is has an expansion @xmath with the
coefficients @xmath given by

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

and with the radius of convergence equal to @xmath . If we substitute
the power series into Eq. ( 4.36 ) and integrate term by term, we obtain
the following power series for @xmath :

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

The first few terms in this expansion are

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

The asymptotic behavior of the coefficients @xmath will be determined in
Section 4.4 , with the result that, for large and integer @xmath ,

  -- -- -- --------
           (4.41)
  -- -- -- --------

The results are summarized in Fig. 4.4 . Depicted are the behavior for
large @xmath , the expansion for small @xmath and a numerical evaluation
of the integral of Eq. ( 4.33 ). Notice the strong deviation of the
expansion from the other curves for @xmath , the radius of convergence.
For this plot the first @xmath terms were used. It appears that @xmath
is indeed an increasing function of @xmath .

#### 4.3.2 The wall

We now turn to the analysis of the value of the action for an instanton.
In the foregoing, we have shown for which positive values of @xmath no
instantons exist. Now we will show that the action indeed becomes
negative for @xmath positive and large enough. For an instanton solution
with one bending point, the action is given by

  -- -------- -- --------
     @xmath      (4.42)
     @xmath      (4.43)
  -- -------- -- --------

With the use of the same approximations for @xmath as in the derivation
of Eq. ( 4.35 ), it is easy to see that, for large @xmath , @xmath is
bounded by

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

so that @xmath clearly becomes negative for large @xmath .

To investigate the behavior of @xmath for small @xmath , we use an
expansion again. It can be obtained using Eq. ( 4.42 ) and the relation

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

A derivation of this relation is given in Appendix 4.6 . For @xmath a
quadratic approximation of the potential @xmath can be used in Eq. (
4.43 ) and we find that @xmath , so that the expansion of @xmath can be
substituted in Eq. ( 4.45 ) and an expansion of @xmath can be obtained
by integrating term by term. The expansions of @xmath and @xmath can
then be used to find the expansion of @xmath using Eq. ( 4.42 ). The
first few terms are

  -- -------- -- --------
     @xmath      (4.46)
     @xmath      (4.47)
  -- -------- -- --------

In Fig. 4.5 , we plot @xmath as obtained from the series expansion, from
the asymptotic behavior, and from numerical integration. The conclusion
is that @xmath is always negative.

### 4.4 Computer-aided analysis of Riemann sheet structures

In the previous section, we encountered the problem of finding solutions
to the implicit function equation ( 4.37 ), or at least series
expansions of solutions. It can be classified as a particular case of
slightly more general problems one encounters in theoretical physics
that are formulated as follows: consider an entire function @xmath such
that

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

with nonnegative integer @xmath (in practice, we have met cases with
@xmath and @xmath ). The task at hand is then to find information about
@xmath such that

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

In general, both the form of the series expansion of @xmath around
@xmath and the nature of its singularities are of interest. Apart from
Section 4.3 , such questions arise, for instance, in the combinatorial
problem of determining the number of Feynman diagrams contributing to
given scattering amplitudes in various quantum field theories [ 28 ] ,
in the statistical bootstrap model for hot hadronic matter (refs. in [
29 ] ), and in renormalization theory connected with the ’t Hooft
transformation [ 30 ] . An important and interesting example, studied in
detail in [ 29 ] , is the so-called bootstrap equation :

  -- -------- -- --------
     @xmath      (4.50)
  -- -------- -- --------

which obviously has @xmath . We shall consider functions @xmath of the
more general form

  -- -------- -- --------
     @xmath      (4.51)
  -- -------- -- --------

where @xmath and @xmath are polynomials of finite degree @xmath and
@xmath , respectively, with real coefficients. As our working example,
taken from Section 4.3 , we shall consider the function @xmath defined
as

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

for which @xmath . It is, in fact, closely related to the bootstrap
equation ( 4.50 ): by substituting, in Eq. ( 4.50 ), @xmath and @xmath ,
we obtain Eq. ( 4.52 ). Its Riemann sheet structure, however, is quite
different, as we shall see. We shall concentrate on the analysis of the
Riemann sheet structure of those solutions of these equations that have
a series expansion around @xmath . To determine the asymptotic behavior
of these expansions, the nature of the singularities will be analyzed
numerically. The results are justified by the fact that, in our
calculations, only finite computer accuracy is required, as we shall
demonstrate.

#### 4.4.1 Identification of the Riemann sheets

As a first step we identify the various Riemann sheets by their value of
@xmath : the sheet labeled @xmath will have @xmath for that sheet.
Obviously, @xmath is a solution with multiplicity @xmath . In general,
there will be @xmath solutions if @xmath , and infinitely many if @xmath
is non-vanishing. It will be helpful if we can identify the Riemann
sheet on which pairs @xmath lie when @xmath is small but nonzero. This
is indeed possible, and we shall illustrate it using @xmath . Let us
write @xmath with @xmath and @xmath real numbers. We are then looking
for solutions of @xmath , or

  -- -------- -------- -- --------
     @xmath   @xmath      (4.53)
     @xmath   @xmath      (4.54)
  -- -------- -------- -- --------

Inspecting the left-hand side of the last equation, we can immediately
see that its zeroes are quite nicely distributed. We can usefully
enumerate them as Im @xmath , where the sheet number @xmath takes only
the odd integer values @xmath . For positive @xmath , the zero @xmath is
certainly located in the interval where @xmath , i.e. @xmath , and
@xmath . We have @xmath , and for increasing @xmath the zero @xmath
moves upwards in its interval, until asymptotically we have @xmath with
@xmath . In Tab. 4.1 we give the values of @xmath for @xmath , for the
first few values of @xmath .

Because the values @xmath fall in disjoint intervals, for small @xmath
we need to know @xmath only to a limited accuracy in order to be able to
identify its Riemann sheet. The only nontrivial case is that of sheets
@xmath and @xmath , where it is sufficient to consider the complex
arguments: for @xmath we are on sheet @xmath , for @xmath we are on
sheet @xmath . Again, limited computer accuracy is acceptable here, and
for larger @xmath we simply have @xmath different values of the
argument, distinguished in an analogous manner. Note that of course the
labeling of the sheets is rather arbitrary: we have chosen the odd
integers in order to emphasize that both sheet @xmath and @xmath can be
considered the principal Riemann sheet. For the bootstrap equation (
4.50 ) it is more natural to label the single principal Riemann sheet
with @xmath as sheet number zero.

#### 4.4.2 Series expansion

We want to compute @xmath as a Taylor series around @xmath :

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

Obviously, @xmath can be chosen as one of the @xmath above. On principal
sheets, with @xmath , we also have immediately that @xmath must be
chosen out of the @xmath possibilities with @xmath . The other
coefficients must then be computed (algebraically or numerically) by
some recursive method, which we shall now discuss.

It would be straightforward to plug the expansion ( 4.55 ) into Eq. (
4.49 ) and equate the powers of @xmath on both sides, but notice that,
for @xmath non-vanishing, the number of possible products of
coefficients grows very rapidly, so that the computer time needed to
find the first @xmath coefficients grows exponentially with @xmath . As
already mentioned in [ 29 ] , the better way is to differentiate Eq. (
4.49 ) with respect to @xmath so that we obtain the nonlinear
differential equation

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

This equation yields a recursion relation involving products of at most
@xmath coefficients, so that a truncated power series can be computed in
polynomial time. As an example, for @xmath we find the following
differential equation:

  -- -------- -- --------
     @xmath      (4.57)
  -- -------- -- --------

and the following recursion relation:

  -- -------- -- --------
     @xmath      
     @xmath      (4.58)
  -- -------- -- --------

We see immediately that @xmath is necessarily even in @xmath if @xmath ,
i.e. on the non-principal Riemann sheets. In that case, we also see that
if @xmath , @xmath is a solution, then also @xmath , @xmath is a
solution, where the asterix stands for complex conjugation. This is a
result of the fact that if @xmath is a solution of Eq. ( 4.52 ), then
also @xmath is a solution. In practice, these solutions give the
function values on the different Riemann sheets of one solution. The
analysis of the previous section proves that @xmath so that the
solutions satisfy @xmath and the expansion coefficients satisfy

  -- -------- -- --------
     @xmath      (4.59)
  -- -------- -- --------

On the principal Riemann sheets we have @xmath and @xmath as mentioned,
and the two solutions on sheet @xmath and sheet @xmath are related by
@xmath . For @xmath we find, finally:

  -- -------- -- --------
     @xmath      (4.60)
  -- -------- -- --------

for @xmath . Using this relation we have been able to compute many
thousands of terms. The recursion appears to be stable in the forward
direction, but we have not tried to prove this or examine the stability
in the general case.

In series expansions it is of course always important to know the
convergence properties or, equivalently, the asymptotic behavior of
@xmath as @xmath becomes very large. In the next section, we therefore
turn to the singularity structure of @xmath .

#### 4.4.3 Singularities and branches

In order to find information about the singularity structure of @xmath ,
we employ the techniques developed in [ 28 ] , which we recapitulate
here. Singularities are situated at those values @xmath of @xmath where

  -- -------- -- --------
     @xmath      (4.61)
  -- -------- -- --------

Since @xmath is entire we also know that these singular points must form
an enumerable set, i.e. we can find, and label, them as distinct points.
We shall assume that these singularities are square-root branch points,
for which it is necessary that

  -- -------- -- --------
     @xmath      (4.62)
  -- -------- -- --------

If @xmath vanishes at @xmath but @xmath does not, we have a cube-root
branch point, and so on. If, for non-vanishing @xmath , all derivatives
vanish (as for instance when @xmath ) we have, of course, a logarithmic
branch point. We know that @xmath corresponds to a logarithmic branch
point, and it is to remove this to infinity in the @xmath plane that we
have required @xmath . In our examples all the singularities at finite
@xmath will be square-root branch points. The position of the
singularity in the @xmath plane, @xmath , is of course given by

  -- -------- -- --------
     @xmath      (4.63)
  -- -------- -- --------

so that there are @xmath different possible positions, lying equally
spaced on a circle around the origin. We shall denote them by @xmath
with @xmath . Note that, in first instance, it is not clear at all
whether @xmath for certain @xmath and @xmath is indeed a singular point
on a specific Riemann sheet. Later on, we shall describe how to
determine this numerically. For values of @xmath close to an observed
singular point @xmath we may expand the left-hand and right-hand side of
Eq. ( 4.49 ) to obtain

  -- -------- -- --------
     @xmath      (4.64)
  -- -------- -- --------

where we have dropped the higher derivative terms. Very close to the
branch point we may therefore approximate @xmath by

  -- -------- -- --------
     @xmath      (4.65)
  -- -------- -- --------

Note that there are only two possible values for @xmath , and each
singular point @xmath goes with one or the other of these. Again
numerical methods will help in determining which one of the two is the
correct choice.

We are now in a position to compute the asymptotic behavior of the
coefficients @xmath . To find it, we first determine, for a given
Riemann sheet , which are the @xmath that lie closest to the origin:
this gives us the radius of convergence of the expansion of @xmath in
that Riemann sheet. We then have to determine those @xmath for which
@xmath is actually a singular point. We shall do this numerically, in
the way described in the following section. Let us denote the set of
values of @xmath for which this is the case by @xmath . Now, we may use
the fact that

  -- -------- -- --------
     @xmath      (4.66)
  -- -------- -- --------

where we have chosen that square root that is real and positive for
@xmath real and positive. The asymptotic behavior of @xmath as @xmath
must therefore be given by

  -- -------- -- --------
     @xmath      (4.67)
  -- -------- -- --------

Amongst other things, this provides a powerful numerical check on the
accuracy of the @xmath as computed by the recursive technique. We shall
now discuss how the singularity structure of our problem can be
investigated numerically.

#### 4.4.4 Computer searches for sheet structures

The main tool we use for our computer studies is a method for taking
small steps over a Riemann sheet, that is, given the fact that for some
value @xmath the point @xmath is determined to belong to a certain
Riemann sheet, we perform a small step @xmath to a point @xmath and find
the point @xmath on the same Riemann sheet. Our method to do this is
nothing but Newton-Raphson iteration: we simply iterate the mapping

  -- -------- -- --------
     @xmath      (4.68)
  -- -------- -- --------

until satisfactory convergence is obtained. The starting value for this
iteration is just the point @xmath . A few remarks are in order here. In
the first place, it must be noted that for this method to work, @xmath
must be in the basin of attraction of @xmath . Since, except at the
branch points, which we shall expressly avoid, @xmath is a continuous
and differentiable function of @xmath , this can always be arranged by
taking @xmath small enough. In the second place, the accuracy with which
@xmath is actually a solution of Eq. ( 4.49 ) is not important as long
as it is in the basin of attraction of @xmath : therefore, there is no
buildup of numerical errors in this method if we restrict ourselves to
just keeping track of which Riemann sheet we are on. Finally, problems
could arise if two Riemann sheet values of @xmath for the same @xmath
are very close. But, since @xmath is an entire function, we know that
the solutions of Eq. ( 4.49 ) must either completely coincide or be
separated by a finite distance, any inadvertent jump from one sheet to
another can be detected and cured by, again, taking a small enough
@xmath .

We have applied the following method for detecting and characterizing
the various singular points. We start on a Riemann sheet @xmath at a
value @xmath close to zero, and determine @xmath on that Riemann sheet.
We then let the parameter @xmath follow a prescribed contour that
circles a selected would-be singularity @xmath once (and no other
singularities), and then returns to the starting point close to the
origin. We then determine to which Riemann sheet the resulting @xmath
belongs. In this way we can find whether @xmath is, in fact, a singular
point for the starting sheet, and, if so, which two sheets are connected
there. It is also possible, of course, to certify the square-root branch
point nature of a singular point by circling twice around it, and
checking that one returns to the original Riemann sheet.

One important remark is in order here. In our tracking over the Riemann
sheet, it is necessary that we do not cross branch cuts (except of
course the one connected to the putative singularity). Since these
branch cuts can be moved around in the complex @xmath plane, the contour
chosen defines the (relative) position of the branch cuts . The sheets
that are said to be connected at a particular branch cut are therefore
also determined by the choice of contour. Of course, choosing a
different contour will change the whole system of interconnected sheets
in a consistent manner, so that in fact, given one choice of contour and
its system of sheets, we can work out what system of sheets will
correspond to another choice of contour. We shall illustrate this in the
following.

Suppose, now, that @xmath is one of the singular points on a certain
sheet that is closest to the origin. We can then follow, on that sheet,
a straight line running from @xmath close to the origin to a point
@xmath for which @xmath is real and just a bit smaller than one. Since
@xmath is by assumption closest to the origin, there is then no
ambiguity involved in determining which one of the two possible complex
arguments of @xmath we have to take. Thus, we can find all the
information needed to compute the asymptotic behavior of @xmath on that
sheet.

#### 4.4.5 An example

Having established the necessary machinery, we shall now discuss a
concrete example of our method. For this, we have taken the function
@xmath of Eq. ( 4.52 ), which is closely related to the very
well-understood bootstrap equation ( 4.50 ), as we have shown. Note that
the origin @xmath , @xmath for @xmath corresponds to the first
singularity in @xmath .

##### 4.4.5.1 The singularities

The values of @xmath on the different Riemann sheets for @xmath , namely
@xmath for @xmath have already been discussed above. The singular values
@xmath are simply given by

  -- -------- -- --------
     @xmath      (4.69)
  -- -------- -- --------

so that the possible singular points @xmath satisfy

  -- -------- -- --------
     @xmath      (4.70)
  -- -------- -- --------

Note that @xmath does not correspond to a singular point. The positions
of the possible singularities in the complex @xmath plane are therefore
as follows. For positive integer @xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (4.71)
  -- -------- -------- -- --------

At all these various possible singularities, we have

  -- -------- -- --------
     @xmath      (4.72)
  -- -------- -- --------

and therefore we may write

  -- -------- -- --------
     @xmath      
     @xmath      (4.73)
  -- -------- -- --------

where the only number to be determined is @xmath . It must be kept in
mind that the value of @xmath depends of course on the sheet: we take
the convention that we work on the sheet with the lowest number (in
absolute value). When viewed from the other sheet, the value of @xmath
is simply opposite.

##### 4.4.5.2 The Riemann sheet structure

We now have to discuss how the branch cuts should run in the complex
@xmath plane. There are two simple options (and an infinity of more
complicated ones): in the first option (I), we choose to let the branch
cuts extend away from the origin parallel to the real axis. This
corresponds to tracking a contour that, say, first moves in the
imaginary direction, and then in the real direction, to arrive close to
the chosen singularity. The other option (II) is to take the cuts
parallel to the imaginary axis, so that a contour that does not cross
branch cuts en route first goes in the real direction, and then in the
imaginary direction. Note that these two alternatives do, indeed,
correspond to different implied relative positionings of the branch
cuts.

In Fig. 4.7 .a we show the contour used in examining singularity @xmath
under option I.

The contour starts on sheet number 1 close to the origin (so that @xmath
is close to @xmath ), moves upwards and then to the left, circles the
singularity once anti-clockwise, and returns to its starting point by
the same route in order to enable us to determine the resulting Riemann
sheet number. Fig. 4.7 .b shows the corresponding path in the @xmath
plane. It ends again close to @xmath so that, for this choice of contour
and its induced branch structure (indicated in the figure), sheet 1 does
not have a branch point at @xmath . Fig. 4.7 .c shows what happens if,
instead of sheet number 1, we start at sheet number 3: the @xmath track
starts then at close to @xmath , but ends up close to @xmath , so that
we conclude that sheets 3 and 5 are connected at @xmath . If we run
through the whole contour twice, we get the @xmath track presented in
Fig. 4.7 .d, where the @xmath track ends up again at @xmath as expected
for a square root branch cut.

Under option II, we rather use the contour indicated in Fig. 4.8 .a,
which first moves to the left and then upwards. Fig. 4.8 .b shows the
resulting @xmath path, which does not return to @xmath but rather to
@xmath , indicating that under this choice of contour the sheets labeled
1 and 5 are connected at @xmath . Fig. 4.8 .c shows that, now, sheet 3
is insensitive to this singularity.

In this way we have mapped the various singularities around the origin.

In Tab. 4.2 we present the pairs of sheets that are pairwise connected
at the first few singularities, under option I, and the observed value
for @xmath , which turns out to be @xmath in all cases. We point out
that at each singularity only two sheets out of all infinitely many are
connected. Note the somewhat atypical situation at the lowest-lying
singularities @xmath and @xmath .

The alternative option II results in Tab. 4.3 . Note that the
higher-lying singularities now show a sheet structure similar to the
lowest ones. In fact, this is the choice that corresponds most directly
to the analysis of the sheet structure of the bootstrap equation in [ 29
] , with of course the extra complication in the fact that the bootstrap
equation ( 4.50 ) has @xmath while for @xmath , @xmath . Note that, once
again, @xmath in all cases.

##### 4.4.5.3 Asymptotic behavior of the series expansion coefficients

We shall now illustrate how the information on the @xmath and @xmath
allows us to compute the asymptotic behavior of the series expansion
coefficients @xmath .

###### First Riemann sheet.

In this sheet, the singularities closest to the origin, and their
corresponding @xmath ’s are

  -- -------- -- --------
     @xmath      
     @xmath      (4.74)
  -- -------- -- --------

Using Eq. ( 4.67 ), we see that the asymptotic form of the coefficients
on sheet 1 is given by

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      (4.75)
  -- -------- -- --------

with integer @xmath .

In Fig. 4.9 we have plotted the observed behavior of

  -- -------- -- --------
     @xmath      (4.76)
  -- -------- -- --------

on the first Riemann sheet, against @xmath . The coefficients clearly
converge to the computed behavior, and we can even distinguish that the
leading corrections go as @xmath ; the four separate lines that emerge
are just the four different forms of @xmath . The series expansion for
Riemann sheet @xmath are simply obtained from

  -- -------- -- --------
     @xmath      (4.77)
  -- -------- -- --------

###### Higher Riemann sheets.

We first consider positive sheet label @xmath and put @xmath . We then
have

  -- -------- -- --------
     @xmath      (4.78)
  -- -------- -- --------

As we have already seen @xmath vanishes for odd @xmath , and for even
@xmath we have the following asymptotic form:

  -- -------- -- --------
     @xmath      (4.79)
  -- -------- -- --------

for integer @xmath . For negative @xmath , we use Eq. ( 4.59 ), which
also holds asymptotically.

### 4.5 Conclusions

For the @xmath -discrepancy and the Lego discrepancy, we have addressed
the problem that non-trivial extremal points of the the action in the
path integral representation of the generating function of quadratic
discrepancies, called instantons, might spoil the saddle point
approximation around the trivial extremal point. We have shown that
instantons appear in both cases, but only if the order parameter @xmath
of the generating function @xmath is larger then a certain positive
value. In the Lego-case this value is half of the size of the smallest
bin, and in the @xmath -case it is @xmath , the smallest positive value
of @xmath at which @xmath in the limit of an infinite number of random
points @xmath has a singularity. Although the instantons do not threaten
the perturbation expansion, they cause @xmath to be undefined for
asymptotically large @xmath when the real part of @xmath is larger then
the mentioned values.

For the analyses in the @xmath -case, a numerical method to investigate
the Riemann sheet structure of the solution of certain algebraic complex
equations is used, which is treated in Section 4.4 . The method is in
particular suitable for the determination of the series expansions
around the origin on the different sheets and the asymptotic behavior of
their coefficients. The results of the numerical analyses have been
justified by the fact that only finite computer accuracy was required in
the specific calculations.

### 4.6 Appendices

Appendix 4A:  Matrices of the form @xmath
The eigenvalues @xmath of a real-valued matrix @xmath are given by the
zeros of the characteristic polynomial @xmath . If @xmath is an @xmath
matrix with matrix elements

  -- -------- -- --------
     @xmath      (4.80)
  -- -------- -- --------

then the characteristic polynomial @xmath is given by

  -- -------- -- --------
     @xmath      (4.81)
  -- -------- -- --------

which is easily derived using @xmath , where the sum is over all
permutations of @xmath . Without loss of generality, we assume that the
coefficients @xmath are ordered such that @xmath . If a number of @xmath
coefficients @xmath take the same value, that is, if @xmath is @xmath
-fold degenerate, then a @xmath -fold degenerate eigenvalue of @xmath is
given by @xmath . The remaining eigenvalues are given by the zeros of
the function @xmath . Except of the poles at @xmath , @xmath , this
function is continuous and differentiable on the whole of @xmath .
Furthermore, the sign of the derivative is equal to @xmath . This means
that for each zero @xmath of @xmath except one, there is an @xmath ,
such that @xmath for the nearest and non-equal neighbor @xmath of @xmath
. The one other zero is smaller than @xmath if @xmath , and larger than
@xmath if @xmath . This is easy to see because @xmath .

Appendix 4B:  Derivation of Eq. ( 4.45 )
We use the definitions of @xmath as the r.h.s. of Eq. ( 4.33 ) and
@xmath as given in Eq. ( 4.43 ):

  -- -------- -- --------
     @xmath      (4.82)
  -- -------- -- --------

Because the end points @xmath and @xmath depend on @xmath such that
@xmath , we can use Leibnitz’s rule for differentiation under the
integral sign to write

  -- -------- -- --------
     @xmath      (4.83)
  -- -------- -- --------

Now we write @xmath and use that @xmath , so that

  -- -------- -- --------
     @xmath      (4.84)
  -- -------- -- --------

But the last integral is equal to zero, and as a result, we obtain Eq. (
4.45 ).

## Chapter 5 Gaussian limits for discrepancies

This chapter deals with the calculation of the generating function of
the probability densities of quadratic discrepancies in the limit of a
large number of truly random points. These densities depend on the
dimension @xmath of the integration region, or, in the case of the Lego
discrepancy, on the number of bins @xmath the integration region is
dissected in. We will derive a ‘Law of Large Number of Modes’, which
describes the conditions under which these densities approaches a normal
density if @xmath or @xmath become large. Throughout this discussion, we
shall only consider the asymptotic limit of a very large number of
random points. This implies that, in this chapter, we cannot make any
statements on how the number of points has to approach infinity with
respect to @xmath or @xmath , as was for instance done in [ 27 ] .

\minitoc

### 5.1 The generating function

We want to calculate the the probability density @xmath of quadratic
discrepancies in the limit of @xmath , where @xmath is the number of
points in the point set. We will use the generating function in this
limit, that we denote by

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

so that the probability density is given by the inverse Laplace
transform of @xmath (Section 2.1.3 ). It results in the weak limit of
@xmath . Starting from Eq. ( 3.28 ), it is easy to see that @xmath is
given by

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

In the Landau gauge, the boundary conditions on the functions @xmath
that give a contribution are such that @xmath (notice that @xmath
contains the same gauge freedom as @xmath ). We can apply the formalism
of Section 2.2.2 , and conclude that @xmath is equal to the sum of the
contributions of all possible connected diagrams consisting only of
vertices with two legs. Consequently, they are of the form

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

and carry a symmetry factor equal to @xmath , where @xmath is the number
of vertices. Every vertex contributes with a factor @xmath , and
represents a convolution of reduced two-point functions @xmath , so that

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

The coefficients @xmath can be written in terms of the eigenfunctions
@xmath and the eigenvalues @xmath of the two-point function @xmath
interpreted as an integration kernel (Section 3.2.5 ). We can use the
expression of Eq. ( 3.57 ) for @xmath , which tells us that we have to
repeatedly calculate the integral

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

@xmath is an infinite dimensional matrix, and the coefficients @xmath
can be written in terms of @xmath through

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath denotes the @xmath -fold matrix product, and the trace,
i.e., the sum over the diagonal elements. The generating function itself
can also be expressed directly in terms of @xmath , since

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

so that

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

where we used the well known rule that, for a general matrix @xmath ,
@xmath . In Appendix 5.4 , it is shown how @xmath can be written in
terms of the strengths @xmath and the weights @xmath , with the result
that

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

Notice that if the basis is in the Landau gauge, then @xmath for all
functions and @xmath , so that the generating function is just given by
@xmath . In the Landau gauge, the matrix @xmath is diagonal, so that
this result follows directly from Eq. ( 5.9 ). If the eigenvalues of
@xmath , in a general gauge, are denoted @xmath , then the generating
function is given by @xmath .

#### 5.1.1 Standardized variables and the Gaussian limit

We have now derived the expression for @xmath in the large- @xmath
limit. Given the form of @xmath , we can now compute @xmath for given
discrepancy @xmath , if only numerically; in fact this was done for the
@xmath -discrepancy in [ 23 ] for several dimensionalities. In some
special cases, @xmath can even be given as an analytic expression [ 24 ,
25 ] . Here, however, we are interested in possible Gaussian limits, and
therefore it is useful use the standardized variable instead of the
discrepancy itself (Section 2.1.2 ). Notice that the expectation value
and the variance of the discrepancy are just given by

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

The generating function @xmath of the standardized variable is given by

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

All information on the particulars of the discrepancy are now contained
in the constants @xmath , and we have that the standardized probability
density approaches the normal density whenever @xmath for all @xmath .
It remains to examine under what circumstances this can happen.

#### 5.1.2 A Law of Large Number of Modes

Since we know that @xmath has no singularities for negative values of
@xmath , the eigenvalues of @xmath are also nonnegative, and we may
write

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

where the various eigenvalues have been denoted by @xmath . Note that
the sum may run over a finite or an infinite number of eigenvalues, but
all these sums must converge since @xmath is finite. Note, moreover,
that @xmath is homogeneous of degree zero in the @xmath : therefore, any
scaling of the eigenvalues by a constant does not influence the possible
Gaussian limit (although it will, of course, affect the mean and
variance of @xmath ).

We now proceed by noting that @xmath , because

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where the first inequality is simply the Schwarz inequality, and the
second one holds because the @xmath are nonnegative. This means that
@xmath will approach zero for @xmath , whenever @xmath approaches zero.
To see when this happens we define

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

so that @xmath . It is then trivial to see that

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

from which we derive that the necessary and sufficient condition for the
discrepancy distribution, in the limit of an infinite number of points
in the point set, to approach a Gaussian is that

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

The Gaussian limit is thus seen to be equivalent to the statement that
even the largest eigenvalue becomes unimportant.

Clearly, a necessary condition for this is that the total number of
non-vanishing eigenvalues (number of modes ) approaches infinity.
Incidentally, the condition ( 5.18 ) also implies that

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

for all those discrepancies that have @xmath . This is eminently
reasonable, since a distribution centered around 1 and (by construction)
vanishing for negative argument can only approach a normal distribution
if its variance approaches zero. On the other hand, the condition @xmath
is by itself not sufficient, as proven by a counterexample given in
Appendix 5.4 .

Another piece of insight can be obtained if we allow the eigenvalues to
take on random values. We may introduce the rather dizzying concept of
an ensemble of different definitions of discrepancy, each characterized
by its set of eigenvalues (all nonnegative) @xmath , with the usual
constraint that they add up to 1; we keep @xmath finite for simplicity.
A natural probability measure on this ensemble is given by the
probability density @xmath of the random vector @xmath :

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

Here @xmath denotes Euler’s gamma-function and @xmath stands for the
Dirac delta-distribution. It is easily computed that the expectation and
variance of @xmath are given, for large @xmath , by

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

so that the @xmath become sharply peaked around their expectation for
large @xmath . In that case, we have

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

and we see that, in the above sense, almost all discrepancies have a
Gaussian distribution in the limit where @xmath , the number of modes,
approaches infinity.

### 5.2 Applications to different examples

#### 5.2.1 Fastest approach to a Gaussian limit

We now examine the various definitions of discrepancies, and assert
their approach to a Gaussian limit. Usually this is envisaged, for
instance in [ 27 ] , as the limit where the dimensionality of the
integration region becomes very large. But, as we have shown, this is
only a special case of the more general situation where the number of
relevant modes becomes very large: another possible case is that where,
in one dimension, the number of modes with essentially equal strength
@xmath becomes very large. As an illustration, consider the case where
the basis functions with the Gaussian measure are orthonormal and @xmath
of the nontrivial modes have equal strength @xmath , and the rest have
strength zero. The moment-generating function then takes on a
particularly simple form, and so does the discrepancy distribution [ 25
] :

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

It is easily seen that the gamma-distribution @xmath approaches a normal
one when @xmath becomes very large. At the same time, we see the
‘physical’ reason behind this: it is the fact that the singularity of
@xmath in the complex plane (in the more general case, the singularity
nearest to @xmath ) moves away to infinity. One observation is relevant
here: in the inverse Laplace transform, to go from @xmath to @xmath , we
have kept the integration along the imaginary axis @xmath . We might
consider performing a saddle-point integration, with a non-vanishing
value of @xmath . That may give us, for a finite number of modes, a good
approximation to the actual form of @xmath . It is quite possible, and,
indeed, it happens in the above equal-strength model, that this
approximation is already quite similar to a Gaussian. In the
equal-strength model, a saddle-point approximation for @xmath gives
precisely the form of Eq. ( 5.23 ), the only difference being that
@xmath is replaced by its Stirling approximation. On the other hand, for
not-so-large @xmath , this form is not too well approximated by a
Gaussian centered around @xmath , since the true maximum resides at
@xmath . Nevertheless, in this chapter we are only interested in the
limiting behavior of @xmath , and we shall stick to the use of condition
( 5.18 ) as an indicator of the Gaussian limit.

One interesting remaining observation is the following. For any finite
number @xmath of eigenvalues @xmath @xmath , the smallest value of the
indicator @xmath is obtained when @xmath for all @xmath . In this sense,
the equal-strengths model gives, for finite @xmath , that discrepancy
distribution that is closest to a Gaussian.

#### 5.2.2 The @xmath-discrepancy

Here we shall discuss the standard @xmath -discrepancy (Section 3.3.1 ).
The eigenfunctions @xmath are equal to @xmath so that @xmath where the
strengths, and the matrix @xmath , are given by

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

The components @xmath of the integer vector @xmath can take all
non-negative integer values, including zero. The eigenvalue equation for
the eigenvalues @xmath of @xmath can be written down easily:

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

The strengths @xmath are degenerate in the values they take. Labeling
the strengths with different values by @xmath with @xmath , the
degeneracy is given by

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

We introduced the logical step function here, which is simply defined by

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

So @xmath is a solution to the eigenvalue equation with a @xmath -fold
degeneracy. If we factorize these solutions we obtain the following
equation for the remaining eigenvalues:

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

Some assertions concerning the remaining eigenvalues can be made using
this equation. On inspection, it can be seen that there are no negative
solutions, nor solutions larger than @xmath , so that @xmath can be used
as an upper bound of the eigenvalues of @xmath . If we order the @xmath
such that @xmath , then @xmath . This implies that @xmath where @xmath .
Now we have

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

and therefore, for @xmath , that

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

The second factor decreases monotonically from @xmath for @xmath to one
as @xmath ; for the first factor, we note that @xmath for all @xmath .
Therefore @xmath can be made arbitrarily small by choosing @xmath large
enough, and the Gaussian limit of high dimensionality is proven. Note,
however, that the approach is not particularly fast: for large @xmath ,
we have @xmath , so that @xmath has to become of the order of one
hundred or so to make the Gaussian behavior manifest. In fact, this was
already noted by explicit numerical computation in [ 23 ] .

#### 5.2.3 The Fourier diaphony

In the case of the Fourier diaphony (Section 3.3.3 ), the eigenfunctions
are in the Landau gauge by definition, so that the matrix @xmath is just
given by

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

with the strengths @xmath as in Eq. ( 3.74 ). The normalization of the
strengths ensures that @xmath , independent of @xmath . In this case,
keeping in mind that sines and cosines occur in the eigenfunctions with
equal strength, we have to consider the multiplicity function

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

Actually, before assigning a strength @xmath , or rather @xmath , we
have to know the behavior of @xmath in order to ensure convergence of
@xmath . In order to do so, we introduce the Dirichlet generating
function for @xmath :

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

where we use the Riemann @xmath function. Since this function (and,
therefore, @xmath as well), converges for all @xmath , we are ensured
that @xmath exceeds the value @xmath at most for a finite number of
values of @xmath , for all positive @xmath and @xmath . This is proven
in Appendix 5.4 . It is therefore sufficient that @xmath decreases as a
power (larger than 1) of @xmath . In fact, taking

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

we immediately have that

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

which, for given @xmath , fixes @xmath such that @xmath , and, moreover,
gives

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

In Section 3.3.3 , the value @xmath is used, with @xmath . The supremum
of @xmath equals @xmath , as @xmath , and the (more interesting) infimum
is @xmath , about @xmath . We conclude that, for all diaphonies of the
above type, the Gaussian limit appears for high dimensionality. For
large @xmath , where the higher modes are greatly suppressed, the
convergence is slowest, in accordance with the observation that the
‘equal-strength’ model gives the fastest convergence; however, the
convergence is still much faster than for the @xmath -discrepancy, and
the Gaussian approximation is already quite good for @xmath . The
fastest approach to the Gaussian limit occurs when we force all modes to
have as equal a strength as is possible within the constraints on the
@xmath . The difference between the supremum and infimum of @xmath is,
however, not much more than a factor of @xmath .

Another possibility would be to let @xmath depend exponentially on
@xmath . In that way one can ensure convergence of the @xmath while at
the same time enhancing as many low-frequency modes as possible. It is
proven in Appendix 5.4 that the function

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

has radius of convergence equal to one, and therefore we may take @xmath
with @xmath between zero and one. If we choose @xmath to be very small,
we essentially keep only the modes with @xmath , and therefore in that
case we have @xmath . This is of course in reality the same type of
discrepancy as the above one, with @xmath . On the other hand, taking
@xmath we arrive at @xmath (see, again, Appendix 5.4 ). The difference
with the first model is, then, that we can approach the Gaussian limit
arbitrarily fast, at the price, of course, of having a function @xmath
that is indistinguishable from a Dirac @xmath -distribution in @xmath ,
and hence meaningless for practical purposes.

##### 5.2.3.1 Fourier diaphony with sum clustering

In the above, we have let the strength @xmath depend on the product of
the various @xmath . This can be seen as mainly a matter of expediency,
since the generalization to @xmath is quite simple in that case. From a
more ‘physical’ point of view, however, this grouping of the @xmath is
not so attractive, if we keep in mind that each @xmath corresponds to a
mode with wave vector @xmath . Under the product rule, wave vectors
differing only in their direction but with equal length may acquire
vastly different weights: for instance, @xmath and @xmath have equal
Euclidean length, @xmath , but their strengths under the product rule
are @xmath and @xmath , respectively. This lack of ‘rotational’ symmetry
could be viewed as a drawback in a discrepancy distinguished by its nice
‘translational’ symmetry. One may attempt to soften this problem by
grouping the strengths @xmath in another way, for instance by taking

  -- -------- -- --------
     @xmath      (5.38)
  -- -------- -- --------

so that @xmath depends on the sum of the components rather than on their
product. The multiplicity of a given strength now becomes, in fact,
somewhat simpler:

  -- -------- -- --------
     @xmath      (5.39)
  -- -------- -- --------

where the last identity follows from the generating function

  -- -------- -- --------
     @xmath      (5.40)
  -- -------- -- --------

This also immediately suggests the most natural form for the strength:
@xmath , where @xmath is @xmath as above. We see that @xmath converges
as long as @xmath , and moreover,

  -- -------- -- --------
     @xmath      (5.41)
  -- -------- -- --------

where @xmath has supremum @xmath , and decreases monotonically with
increasing @xmath . For @xmath close to one, we have @xmath , so that
the Gaussian limit can be reached as quickly as desired (again with the
reservations mentioned above). At the other extreme, note that for very
small @xmath we shall have

  -- -------- -- --------
     @xmath      (5.42)
  -- -------- -- --------

This just reflects the fact that, for extremely small @xmath , only the
@xmath lowest nontrivial modes contribute to the discrepancy; and even
in that case the Gaussian limit is attained, although much more slowly.
The criterion that determines whether the behavior of @xmath with @xmath
and @xmath is exponential or of type @xmath is seen to be whether @xmath
is considered to be large or small, respectively.

Another alternative might be a power-law-like behavior of the strengths,
such as @xmath . Also in this case we may compute the @xmath , as
follows:

  -- -------- -- --------
     @xmath      (5.43)
  -- -------- -- --------

from which it follows that @xmath to ensure convergence of @xmath . In
the large- @xmath limit, we therefore find that, also in this case,
@xmath .

##### 5.2.3.2 Fourier diaphony with spherical clustering

A clustering choice which is, at least in principle, even more
attractive from the symmetry point of view than sum clustering, is to
let @xmath depend on @xmath , hence assuring the maximum possible amount
of rotational invariance under the constraint of translational
invariance. We therefore consider the choice

  -- -------- -- --------
     @xmath      (5.44)
  -- -------- -- --------

For the function @xmath we now have the following two alternative forms,
related by Poisson summation:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.45)
  -- -------- -------- -- --------

of which the first converges well for large, and the second for small,
values of @xmath ; the sum over @xmath extends over the whole integer
lattice. The @xmath are, similarly, given by

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.46)
  -- -------- -------- -- --------

For large @xmath (where, again, only the first few modes really
contribute) we recover, again, the limit @xmath as @xmath : for small
@xmath we have, again, an exponential approach to the Gaussian limit:

  -- -------- -- --------
     @xmath      (5.47)
  -- -------- -- --------

The distinction between the two limiting behaviors is now the magnitude
of the quantity @xmath , which now takes over the rôle of the @xmath of
the previous paragraph.

#### 5.2.4 The Walsh diaphony

Another type of diaphony is based on Walsh functions, which are defined
as follows. Let, in one dimension, the real number @xmath be given by
the decomposition

  -- -------- -- --------
     @xmath      (5.48)
  -- -------- -- --------

and let the nonnegative integer @xmath be given by the decomposition

  -- -------- -- --------
     @xmath      (5.49)
  -- -------- -- --------

Then, the @xmath Walsh function @xmath is defined as

  -- -------- -- --------
     @xmath      (5.50)
  -- -------- -- --------

The extension to the multidimensional case is of course straightforward,
and it is easily seen that the Walsh functions form an orthonormal set.
The Walsh diaphony is then given by

  -- -------- -- --------
     @xmath      (5.51)
  -- -------- -- --------

In [ 21 ] , the following choice is made:

  -- -------- -- --------
     @xmath      
     @xmath      (5.52)
  -- -------- -- --------

Note that, in contrast to the Fourier case where each mode of frequency
@xmath contains two basis functions (one sine and one cosine), the
natural requirement of ‘translational invariance’ in this case requires
that the Walsh functions from @xmath up to @xmath get equal strength.
The clusterings are therefore quite different from the Fourier case. We
slightly generalize the notions of [ 27 ] , and write

  -- -------- -- --------
     @xmath      
     @xmath      (5.53)
  -- -------- -- --------

Here, we have disregarded the overall normalization of the @xmath ’s
since it does not influence the Gaussian limit. It is an easy matter to
compute the @xmath ; we find

  -- -------- -- --------
     @xmath      (5.54)
  -- -------- -- --------

so that the requirement @xmath implies that we must have @xmath .
Therefore, for not too small values of @xmath , we have

  -- -------- -- --------
     @xmath      (5.55)
  -- -------- -- --------

The choice made in [ 21 ] corresponds to @xmath and @xmath , for which
we find @xmath . The Gaussian limit should, therefore, be a good
approximation for @xmath larger than 6 or so. An interesting observation
is that for fixed @xmath , @xmath attains a minimum at @xmath , so that
the choice @xmath could in principle lead to @xmath with a marginally
faster approach to the Gaussian. The overall infimum is seen to be
@xmath . As in the Fourier case with product clustering and a power-law
strength, there is a limit on the speed with which the Gaussian is
approached: in both cases this is directly related to the type of
clustering.

At the other extreme, for very small @xmath we find the limiting
behavior

  -- -------- -- --------
     @xmath      (5.56)
  -- -------- -- --------

Again in this case, the slowest possible approach to the Gaussian limit
is like @xmath , directly related to the symmetry of the discrepancy
definition with respect to the various coordinate axes.

#### 5.2.5 The Lego discrepancy

In the case of the Lego discrepancy (Section 3.3.4 ), the matrix @xmath
has indices that label the bins @xmath @xmath the hypercube is dissected
into, where @xmath is the total number of bins. Because the
characteristic functions of the bins are not normalized, the matrix
looks a bit different:

  -- -------- -- --------
     @xmath      (5.57)
  -- -------- -- --------

where @xmath is the volume of bin @xmath . This matrix satisfies @xmath
for all @xmath . We shall now examine under what circumstances the
criterion ( 5.18 ) for the appearance of the Gaussian limit is
fulfilled. The eigenvalues @xmath of the matrix @xmath are given as the
roots of the eigenvalue equation

  -- -------- -- --------
     @xmath      (5.58)
  -- -------- -- --------

It is seen that there is always one zero eigenvalue (the corresponding
eigenvector has @xmath for its @xmath component). Furthermore the
eigenvalues are bounded by @xmath , and this bound is an eigenvalue if
there is more than one @xmath for which the maximum is attained. At any
rate, we have for our criterion, that

  -- -------- -- --------
     @xmath      (5.59)
  -- -------- -- --------

Since the generality of the Lego discrepancy allows us to choose from a
multitude of possibilities for the @xmath ’s and @xmath ’s, we now
concentrate on a few special cases.

1.   All @xmath equal. This models integrands whose local details are
    not resolved within areas smaller than @xmath , but whose magnitude
    may fluctuate. In that case, we have

      -- -------- -- --------
         @xmath      (5.60)
      -- -------- -- --------

    and a sufficient condition for the Gaussian limit is for this bound
    to approach zero. Note that here, as in the general case, only bins
    @xmath with @xmath contribute to the discrepancy as well as to the
    criterion @xmath , so that one has to be careful with models in
    which the integrand is fixed at zero in a large part of the
    integration region: this type of model was, for instance, examined
    in [ 22 ] .

2.   All @xmath equal. In this case, the underlying integrands have more
    or less bounded magnitude, but show finer detail in some places
    (with small @xmath ) than in other places (with larger @xmath ).
    Now, it is simple to prove that

      -- -------- -- --------
         @xmath      (5.61)
      -- -------- -- --------

    so that a sufficient condition is that @xmath should approach zero.

3.   All @xmath equal. In this case, the discrepancy is the @xmath
    -statistic for the data points distributed over the bins with
    expected fraction of points @xmath . We simply have

      -- -------- -- --------
         @xmath      (5.62)
      -- -------- -- --------

    and the Gaussian limit follows whenever @xmath .

### 5.3 Conclusions

We derived the probability distribution, in the limit of a large number
of points, over the ensemble of truly random point-sets of quadratic
discrepancies. We have shown under what conditions this distribution
tends to a Gaussian. In particular, the question of the limiting
behavior of a given distribution can be reduced to solving an eigenvalue
problem. Using the knowledge of the eigenvalues for a given function
class it is possible to determine under which conditions and how fast
the Gaussian limit is approached. Finally, we have investigated the
limiting behavior of the probability distribution for the discrepancy of
several function classes explicitly.

The discrepancy that fastest approaches the Gaussian limit is obtained
for the model in which the number of modes with non-zero equal strength
goes to infinity, while the sum of the strengths is fixed. In fact, we
give an argument why we cannot improve much on this limit. However, a
drawback of this model is that the discrepancy itself becomes a sum of
Dirac @xmath -distributions in this limit: it only measures whether
points coalesce, and is therefore not very useful in practice.

Secondly we looked at the @xmath -discrepancy. Here a Gaussian
distribution appears in the limit of a large number of dimensions. It is
however a very slow limit: only when the number of dimensions becomes of
the order @xmath does the Gaussian behavior become manifest.

For the different diaphonies the choice of the mode-strengths is more
arbitrary. The strengths we discuss are chosen on the basis of some
preferred global properties of the diaphony, such as translation- and/or
rotation-invariance. Again for large dimensions the Gaussian limit is
attained, either as a power-law or inverse of the number of dimension.
It is possible to choose the strengths in such a way that the Gaussian
limit is approached arbitrarily fast. But the diaphony corresponding to
that case again consists of a sum of Dirac @xmath -distributions.

Finally, for the Lego-discrepancy, we can assign strengths to the
different modes in several ways. One example is to keep the product of
the squared strength and volume of the modes fixed, then the Gaussian
limit is reached for a large number of modes.

All these results have been derived in the limit of large number of
points. It remains to be seen however whether this is reasonable in
practice. To determine when the asymptotic regime sets in, i.e. for
which value of @xmath , it is necessary to take into account the
next-to-leading contributions, which will be calculated in the following
chapter.

### 5.4 Appendices

Appendix 5A:  The form of @xmath
In this Appendix, we derive the result ( 5.10 ) for the form of @xmath .
We introduce the notation @xmath for matrices @xmath and vectors @xmath
, and the general form of the matrix @xmath :

  -- -------- -- --------
     @xmath      (5.63)
  -- -------- -- --------

The @xmath power of this matrix has the general form

  -- -------- -- --------
     @xmath      (5.64)
  -- -------- -- --------

with the constraint @xmath . The combinatorial factor follows directly
from the possible positionings of the dyadic factors @xmath .
Multiplying by @xmath and summing over the @xmath then gives us
immediately

  -- -------- -- --------
     @xmath      (5.65)
  -- -------- -- --------

where the factor with @xmath comes from the double sum over @xmath and
@xmath with @xmath . Upon integration of this result over @xmath from 0
to @xmath we find

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (5.66)
  -- -------- -------- -- --------

If we now take @xmath and @xmath , we obtain ( 5.10 ) with ( 5.11 ).
This result has, in fact, already been obtained for the case of the
@xmath -discrepancy in [ 23 ] , but here we demonstrate its general
validity for more general discrepancy measures. In those cases where
@xmath , the second term vanishes of course.

Appendix 5B:  A counterexample
In this Appendix we prove that the condition ( 5.18 ) for the occurrence
of a Gaussian limit is, in a sense, the best possible. Namely, consider
a set of eigenvalues @xmath , again adding up to unity as usual, defined
as follows: let @xmath be a positive number, and take

  -- -------- -- --------
     @xmath      (5.67)
  -- -------- -- --------

Clearly, @xmath will indeed be the maximal eigenvalue as long as @xmath
. Now,

  -- -------- -- --------
     @xmath      (5.68)
  -- -------- -- --------

and this ratio can be driven as close to unity as desired by choosing
@xmath sufficiently large. This shows that the simple condition @xmath
is not always enough to ensure the Gaussian limit.

Appendix 5C:  The magnitude of @xmath
Here we present the proofs of our various statements about the
multiplicity function @xmath of Eq. ( 5.37 ). In the first place, we
know that its Dirichlet generating function, @xmath , converges for all
@xmath . Now suppose that @xmath exceeded @xmath an infinite number of
times, with @xmath and @xmath . The Dirichlet generating function would
then contain an infinite number of terms all larger than @xmath , for
@xmath , and therefore would diverge, in contradiction with its
convergence for all @xmath .

In the second place, consider the ‘standard’ generating function, @xmath
. By inspecting how many of the vector components @xmath of @xmath are
zero, we see that we may write, for @xmath ,

  -- -------- -- --------
     @xmath      (5.69)
  -- -------- -- --------

so that @xmath counts in how many ways the integer @xmath can be written
as a product of @xmath factors, including ones; this function is
discussed, for instance, in [ 15 ] . Now, for @xmath prime, we have
@xmath , and therefore

  -- -------- -- --------
     @xmath      (5.70)
  -- -------- -- --------

The radius of convergence of @xmath is therefore at most equal to unity.
On the other hand, we can obtain a very crude, but sufficient, upper
bound on @xmath as follows. Since @xmath is a nondecreasing function of
@xmath , we may bound @xmath by @xmath . Now let @xmath be the number of
prime factors in @xmath ; then @xmath cannot exceed @xmath , and only is
equal to this when @xmath is a pure power of 2. Also, the number of ways
to distribute @xmath object in @xmath groups (which may be empty) is at
most @xmath , and is smaller if some of the objects are equal.
Therefore, @xmath is at most @xmath , and we see that

  -- -------- -- --------
     @xmath      (5.71)
  -- -------- -- --------

or, in short, is bounded ¹ ¹ 1 Note that equality cannot occur in this
case since the two requirements are mutually exclusive. by a polynomial
in @xmath . Therefore, the radius of convergence of @xmath is also at
least unity, and we have proven the assertion in Section 5.2.3 .

Finally, we consider the limit

  -- -------- -- --------
     @xmath      (5.72)
  -- -------- -- --------

The same reasoning that led us to the radius of convergence shows that,
for @xmath approaching 1 from below, the function @xmath behaves as
@xmath , with @xmath . Therefore, @xmath will behave as @xmath , and
approach zero as @xmath . Note that the upper bound on @xmath is
extremely loose: but it is enough.

## Chapter 6 Finite-sample corrections to discrepancy distributions

This chapter deals with the calculation of the @xmath -corrections to
the asymptotic probability distributions of quadratic discrepancies in
the limit of an infinite number of random points @xmath . In Section 6.1
, the explicit diagrammatic expansion of the logarithm of the generating
function up to and including @xmath will be given. For the Lego
discrepancy, the @xmath -discrepancy in one dimension and the Fourier
diaphony in one dimension, the explicit @xmath -correction is
calculated.

In Chapter 5 , criteria were given for the asymptotic probability
distribution of several quadratic discrepancies to become Gaussian when
a certain free parameter becomes infinitely large. This parameter often
is the dimension @xmath of the integration region. In the case of the
Lego discrepancy, it is the number of bins @xmath . In [ 27 ] , it is
shown that for the Fourier diaphony a Gaussian limit is obtained when
both @xmath and @xmath go to infinity such that @xmath , where @xmath is
some constant larger than @xmath . This theorem clearly gives more
information about the behavior of the probability distribution, for it
relates @xmath and @xmath , whereas in Chapter 5 the limit of @xmath is
assumed before considering the behavior with respect to @xmath or @xmath
. However, the techniques of this chapter to calculate @xmath
-corrections to the asymptotic distributions give the opportunity to
relate @xmath or @xmath with @xmath . In Section 6.2 , this leads to
limits for the Lego discrepancy, which is equivalent with a @xmath
-statistic for @xmath data points distributed over @xmath bins, if
@xmath as well as @xmath become infinite. In Section 6.3 , a Gaussian
limit is derived for the Fourier diaphony, which is stronger than the
one in [ 27 ] in the sense that it provides convergence of the moments
of the distribution, whereas the limit in [ 27 ] is weak.

\minitoc

### 6.1 The first few orders

The Feynman diagrams that contribute to the first few orders in the
@xmath -expansion of the generating function @xmath of the probability
distribution of quadratic discrepancies are determined, and are used in
a few examples.

#### 6.1.1 The diagrammatic expansion

To calculate a term in the @xmath -expansion of @xmath , the
contribution of all diagrams that can be drawn using the Feynman rules,
as given in Section 3.2.4 , and carry the right power of @xmath has to
be included. We want to stress again that we only need to calculate the
connected diagrams without external lines. The sum of the contributions
of all these diagrams gives

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

Usually, a Feynman diagram is a mnemonic representing a certain
contribution to a term in a series expansion, i.e. a label. We will use
the same drawing for the contribution itself, apart of the symmetry
factor of the diagram. For example, the contribution of the diagram is
equal to @xmath and its symmetry factor is equal to @xmath , so that we
write

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

##### 6.1.1.1 The zeroth order

The contribution to the zeroth order in @xmath can only come from
diagrams in which the power of @xmath coming from the vertices cancels
the power of @xmath , coming from the fermion loops. This only happens
in diagrams with vertices with two bosonic legs only, and in which the
fermion lines begin and end on the same vertex. To write down their
contribution, we introduce the two-point functions @xmath , @xmath ,
defined by

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

The zeroth order term @xmath is given by

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

The factor @xmath is the symmetry factor of this type of diagram with
@xmath fermion “leaves”. If we substitute @xmath in this expression, we
find exactly the result of Eq. ( 5.4 ).

##### 6.1.1.2 The first order

As we have seen before, bosonic two-point vertices with a closed single
fermion line contribute with a factor @xmath , and without any
dependence on @xmath . Therefore, it is useful to introduce the
following effective vertex

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

and the following dressed boson propagator

  -- -------- -------- -- -------
     @xmath   @xmath      (6.6)
     @xmath   @xmath      (6.7)
  -- -------- -------- -- -------

In terms of the basis in the Landau gauge, it is given by

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

which is, apart of a factor @xmath , the same expression as in Eq. (67)
in [ 24 ] . Notice that @xmath and @xmath satisfy the relation

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

Furthermore, notice that @xmath and @xmath satisfy

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

and that this relation determines @xmath uniquely, because we know that
@xmath has to be equal to @xmath in order for the asymptotic probability
distribution to be normalized to @xmath .

The first order term in the expansion of @xmath is

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

or, more explicitly,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.12)
  -- -------- -------- -- --------

##### 6.1.1.3 The second order

The second order term in the expansion of @xmath is denoted @xmath and
is given by

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.13)
  -- -------- -- --------

##### 6.1.1.4 One-vertex decomposability

For some discrepancies, the contribution of a bosonic part of a diagram
that consists of two pieces connected by only one vertex, is equal to
the product of the contribution of those pieces. Such diagrams we call
one-vertex reducible , and discrepancies with this property we call
one-vertex decomposable . Examples of such discrepancies are those for
which @xmath is translation invariant, i.e., @xmath @xmath , such as the
Fourier diaphony. Also the Lego discrepancy with equal bins is
one-vertex decomposable. In contrast, the @xmath -discrepancy is not
one-vertex decomposable.

As a result of the one-vertex decomposability, many diagrams cancel or
give zero. For example, the first and the second diagram in ( 6.11 )
cancel, and the fourth gives zero, so that

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

To second order, only the following remains:

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.15)
  -- -------- -------- -- --------

We now derive a general rule of diagram cancellation. First, we extend
the notion of one-vertex reducibility to complete diagrams, including
the fermionic part, with the rule that the two pieces both must contain
a bosonic part. Consider the following diagram

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

The only restriction we put one the “leave” @xmath is that it must be
one-vertex irreducible with respect to the vertex that connects it to
the fermion loop. For the rest, it may be anything. We define the
contribution of the leave by the contribution of the whole diagram
divided by @xmath , and denote it with @xmath . This contribution
includes internal symmetry factors. Now consider a diagram consisting of
a fermion loop as in diagram ( 6.16 ) with attached to the one vertex
@xmath leaves of type @xmath , @xmath leaves of type @xmath , and so on,
up to @xmath leaves of type @xmath . The extra symmetry factor of such a
diagram is @xmath , and, for one-vertex decomposable discrepancies, the
contribution is equal to the product of the contributions of the leaves,
so that the total contribution is given by

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

Now we sum the contribution of all possible diagrams of this kind that
can made with the @xmath leaves, and denote the result by

  -- -------- -- --------
     @xmath      (6.18)
  -- -------- -- --------

Because the little square in l.h.s. of Eq. ( 6.18 ) represents all
possible ways to put the leaves together onto one vertex, the sum of all
possible ways to put the leaves onto one fermion loop is given by

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

The @xmath in the sum comes from the vertices and @xmath is the extra
symmetry factor of such diagram with @xmath squares. The sum can be
evaluated further and is equal to

  -- -------- -- --------
     @xmath      (6.20)
  -- -------- -- --------

i.e., the sum of all possible ways to put @xmath different leaves onto
one fermion loop is equal to the sum of all leaves, each of them put
onto its own fermion loop. This means that diagrams, consisting of two
or more leaves put onto one fermion loop, cancel.

Now consider the following equation, which holds for every one-vertex
decomposable discrepancy:

  -- -------- -- --------
     @xmath      (6.21)
  -- -------- -- --------

where we only assume that @xmath is not of the type on the l.h.s. of
Eq. ( 6.19 ). The minus sign comes from the fact that the first diagram
has one vertex less. Because the number of fermion lines a fermion loop
consists of is equal to the number of vertices it contains, we can
always pair the diagrams into one diagram of the l.h.s. type and one of
the r.h.s. type so that they cancel. We can summarize the result with
the rule that

  -- -- -- --------
           (6.22)
  -- -- -- --------

#### 6.1.2 Applications

We apply the general formulae given above to the Lego discrepancy, the
@xmath -discrepancy in one dimension and the Fourier diaphony in one
dimension.

##### 6.1.2.1 The Lego discrepancy

We take the strengths @xmath equal to @xmath , so that the discrepancy
is just the @xmath -statistic that determines how well the points are
distributed over the bins (Section 3.3.4 ). The propagator is given by

  -- -------- -- --------
     @xmath      (6.23)
  -- -------- -- --------

and it is easy to see that @xmath , @xmath , so that the dressed
propagator is given by

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

The zeroth order term can be found with the relation of Eq. ( 6.10 ),
which results in the following expression

  -- -------- -- --------
     @xmath      (6.25)
  -- -------- -- --------

which is exactly the logarithm of the generating function of the @xmath
-distribution (notice that this is by definition the distribution of the
@xmath -statistic in the limit of an infinite number of random data). To
write down the first order term, we introduce

  -- -------- -- --------
     @xmath      (6.26)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (6.27)
  -- -------- -- --------

If the bins are equal, so that @xmath @xmath , then only the
contribution of the diagrams of Eq. ( 6.16 ) remains, and the result is

  -- -------- -- --------
     @xmath      (6.28)
  -- -------- -- --------

where we denote

  -- -------- -- --------
     @xmath      (6.29)
  -- -------- -- --------

To second order in @xmath , the contribution comes from the diagrams in
Eq. ( 6.15 ), and is given by

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.30)
  -- -------- -------- -- --------

In Appendix 6.5 , we present the expansion of @xmath in the case of
equal bins, up to and including the @xmath term. It is calculated using
the path integral expression ( 3.85 ) of @xmath and computer algebra.
The reader may check that this expression for @xmath and the above terms
of @xmath satisfy @xmath up to the order of @xmath .

##### 6.1.2.2 The @xmath-discrepancy

In one dimension, the basis in the Landau gauge is given by the set of
functions @xmath (Section 3.3.1 ), so that the propagator is given by

  -- -------- -- --------
     @xmath      (6.31)
  -- -------- -- --------

The dressed propagator is given by

  -- -------- -------- -- --------
     @xmath   @xmath      (6.32)
              @xmath      (6.33)
  -- -------- -------- -- --------

with

  -- -------- -- --------
     @xmath      (6.34)
  -- -------- -- --------

The zeroth order term can be obtained using Eq. ( 6.10 ):

  -- -------- -- --------
     @xmath      (6.35)
  -- -------- -- --------

which is the well-known result. After some algebra, also the first order
term follows:

  -- -------- -- --------
     @xmath      (6.36)
  -- -------- -- --------

##### 6.1.2.3 The Fourier diaphony

We consider the one-dimensional case, with a slightly different
definition than the one given in Section 3.3.3 : we multiply the
discrepancy with a factor @xmath , so that the propagator is given by

  -- -------- -- --------
     @xmath      (6.37)
  -- -------- -- --------

where we use the notation @xmath . The dressed propagator is given by

  -- -- -- --------
           (6.38)
  -- -- -- --------

where

  -- -------- -- --------
     @xmath      (6.39)
  -- -------- -- --------

This two-point function is, apart of a factor @xmath , the same as the
one in Eq. (26) in [ 25 ] . The zeroth order term can easily be obtained
from the dressed propagator and is given by

  -- -------- -- --------
     @xmath      (6.40)
  -- -------- -- --------

which is in correspondence with Eq. (21) in [ 25 ] . Because the
propagator is translation invariant, i.e., @xmath @xmath , the
contributions of the first two diagrams in Eq. ( 6.11 ) cancel, and the
contribution of the fourth diagram is zero. The contribution of the
remaining diagrams gives

  -- -------- -- --------
     @xmath      (6.41)
  -- -------- -- --------

### 6.2 Scaling limits for the Lego discrepancy

In this section, we take a closer look at the Lego discrepancy in the
case that it is equivalent with a @xmath -statistic for @xmath data
points distributed over @xmath bins (Section 3.3.4 ). First, we will
show that the natural expansion parameter in the calculation of the
moment generating function is @xmath , and calculate a few terms. We
will see, however, that a strict limit of @xmath does not exist, and, in
fact, this is well known because the @xmath -distribution, which gives
the lowest order term in this expansion, does not exist if the number of
degrees of freedom becomes infinite. We overcome this problem by going
over to the standardized variable, which is obtained from the
discrepancy by shifting and rescaling it such that it has zero
expectation and unit variance. In fact, it is this variable for which
the results in [ 27 ] and Chapter 5 were obtained. In this section, we
derive similar results for the Lego discrepancy, depending on the
behavior of the sizes of the bins if @xmath goes to infinity. We will
see that various asymptotic probability distributions occur if @xmath
such that @xmath with @xmath . If, for example, the bins become
asymptotically equal and @xmath , then the probability distribution
becomes Gaussian. Notice that this includes limits with @xmath , which
is in stark contrast with the rule of thumb that, in order to trust the
@xmath -distribution, each bin has to contain at least a few, say five
(see e.g. [ 3 ] ), data points. Our result states that, for large @xmath
and @xmath , the majority of bins is allowed to remain empty!

#### 6.2.1 Sequences and notation

In the following, we will investigate limits in which the number of bins
@xmath goes to infinity. Note that for each value of @xmath , we have to
decide on the values of the volumes @xmath of the bins. They clearly
have to scale with @xmath , because their sum has to be equal to one.
There are, of course, many possible ways for the measures to scale,
i.e., many double-sequences @xmath of positive numbers with

  -- -------- -- --------
     @xmath      (6.42)
  -- -------- -- --------

We, however, want to restrict ourselves to discrepancies in which the
relative sizes of the bins stay of the same order, i.e., sequences for
which

  -- -------- -- --------
     @xmath      (6.43)
  -- -------- -- --------

It will appear to be appropriate to specify the sequences under
consideration by another criterion, which is for example satisfied by
the sequences mentioned above. It can be formulated in terms of the
objects

  -- -------- -- --------
     @xmath      (6.44)
  -- -------- -- --------

and is given by the demand that

  -- -------- -- --------
     @xmath      (6.45)
  -- -------- -- --------

Within the set of sequences we consider, there are those with for which
the bins become asymptotically equal, i.e., sequences with

  -- -------- -- --------
     @xmath      (6.46)
  -- -------- -- --------

and @xmath , @xmath of course. They belong to the set of sequences with
@xmath @xmath , which will allow for special asymptotic probability
distributions.

In the following analysis, we will consider functions of @xmath and
their behavior if @xmath . To specify relative behaviors, we will use
the symbols “ @xmath ”, “ @xmath ” and “ @xmath ”. The first one is used
as follows:

  -- -------- -- --------
     @xmath      (6.47)
  -- -------- -- --------

If a limit as above is not necessarily equal to one and not equal to
zero, then we use the second symbol:

  -- -------- -- --------
     @xmath      (6.48)
  -- -------- -- --------

We only use this symbol for those cases in which @xmath . For the cases
in which @xmath we use the third symbol:

  -- -------- -- --------
     @xmath      (6.49)
  -- -------- -- --------

We will also use the @xmath -symbol, and do this in the usual sense. We
can immediately use the symbols to specify the behavior of @xmath with
@xmath , for the criterion of Eq. ( 6.45 ) tells us that

  -- -------- -- --------
     @xmath      (6.50)
  -- -------- -- --------

and that

  -- -------- -- --------
     @xmath      (6.51)
  -- -------- -- --------

In our formulation, also the number of data points @xmath runs with
@xmath . We will, however, never denote the dependence of @xmath on
@xmath explicitly and assume that it is clear from now on. Also the
upper index at the measures @xmath we will omit from now on.

#### 6.2.2 Feynman rules

The Feynman rules to calculate the generating function @xmath in a
diagrammatic expansion are given in Section 3.2.4 . The boson propagator
is a matrix in this case, i.e.,

  -- -------- -- --------
     @xmath      (6.52)
  -- -------- -- --------

and boson propagators are convoluted as @xmath in the vertices. Only
connected diagrams have to be calculated, since

  -- -------- -- --------
     @xmath      rule 1
  -- -------- -- --------

Furthermore, the bosonic part of each diagram decouples completely from
the fermionic part, and the contribution of the fermionic part can
easily be determined, for

  -- -- -- --------
           rule 2
  -- -- -- --------

Because of the rather simple expression for the bosonic propagator, we
are able to deduce from the basic Feynman rules some effective rules for
the bosonic parts of the Feynman diagrams. Remember that the bosonic
parts decouples completely from the fermionic parts. The following rules
apply after having counted the number of fermion loops and the powers of
@xmath coming from the vertices, and after having calculated the
symmetry factor of the original diagram. When we mention the
contribution of a diagram in this section, we refer to the contribution
apart from the powers of @xmath and symmetry factors. This contribution
will be represented by the same drawing as the diagram itself.

The first rule is a consequence of the fact that

  -- -------- -- --------
     @xmath      (6.53)
  -- -------- -- --------

and states that

  -- -- -- --------
           rule 3
  -- -- -- --------

The second rule is a consequence of the fact that for any @xmath -matrix
@xmath

  -- -------- -- --------
     @xmath      (6.54)
  -- -------- -- --------

and states that the contribution of a diagram is the same as that of the
diagram in which a boson line is contracted and the two vertices,
connected to that line, are fused together to form one vertex, minus the
contribution of the diagram in which the line is simply removed and the
vertices replaced by vertices with one boson leg less. This rule can be
depicted as follows

  -- -------- -- --------
     @xmath      rule 4
  -- -------- -- --------

By repeated application of these rules, we see that the contribution of
a connected bosonic diagram is equal to the contribution of a sum of
products of so called daisy diagrams ¹ ¹ 1 For example @xmath , which
are of the type

  -- -------- -- --------
     @xmath      (6.55)
  -- -------- -- --------

They are characterized by the fact that all lines begin and end on the
same vertex and form single loops. The contribution of such a diagram is
given by

  -- -------- -- --------
     @xmath      (6.56)
  -- -------- -- --------

where the last equation follows from Eq. ( 6.50 ). The maximal number of
leaves in a product in the sum of daisy diagrams is equal to the number
of loops @xmath in the original diagram, so that

  -- -- -- --------
           rule 5
  -- -- -- --------

The leading order contribution of a diagram with @xmath boson loops is
thus of the order of @xmath .

##### 6.2.2.1 Extra rule if @xmath

If @xmath @xmath , then all kind of cancellations between diagrams
occur, because in those cases @xmath @xmath . As a result of this, the
contribution of a daisy diagram is @xmath , and we can deduce the
following rule: the contribution of a diagram that falls apart in
disjunct pieces if a vertex is cut, is equal to the product of the
contributions of those disjunct pieces times one plus vanishing
corrections. Diagrammatically, the rule looks like

  -- -------- -- --------
     @xmath      (6.57)
  -- -------- -- --------

In Section 6.1.1.4 we called discrepancies for which Eq. ( 6.57 ) is
exact one-vertex decomposable, and have shown that for those
discrepancies only the one-vertex irreducible diagrams contribute, i.e.,
diagrams that do not fall apart in pieces containing bosonic parts if a
vertex is cut. The previous rule tells us that, if @xmath @xmath , then

  -- -------- -- --------
     @xmath      rule 6
  -- -------- -- --------

The connected one-vertex irreducible diagrams we call relevant and the
others irrelevant .

#### 6.2.3 Loop analysis

We want to determine the contribution of the diagrams in this section,
and in order to do that, we need to introduce some notation:

  -- -------- -------- -- --------
     @xmath   @xmath      (6.58)
     @xmath   @xmath      (6.59)
     @xmath   @xmath      (6.60)
     @xmath   @xmath      (6.61)
     @xmath   @xmath      (6.62)
     @xmath   @xmath      (6.63)
     @xmath   @xmath      (6.64)
  -- -------- -------- -- --------

These quantities are in principle functions of the diagrams, but we will
never denote this dependence explicitly, for it will always be clear
which diagram we are referring to when we use the quantities.

With the foregoing, we deduce that the contribution @xmath of a
connected diagram @xmath with no external legs satisfies

  -- -------- -- --------
     @xmath      (6.65)
  -- -------- -- --------

The Feynman rules and basic graph theory tell us that, for connected
diagrams with no external legs, @xmath and @xmath , so that

  -- -------- -- --------
     @xmath      (6.66)
  -- -------- -- --------

If we furthermore use that @xmath , we find that the contribution is
given by

  -- -------- -- --------
     @xmath      (6.67)
  -- -------- -- --------

Notice that this expression does not depend on @xmath . Furthermore, it
is clear that, for large @xmath and @xmath , the largest contribution
comes from diagrams with @xmath . Moreover, we see that we must have
@xmath , for else the contribution of higher-order diagrams will grow
with the number of boson loops, and the perturbation series becomes
completely senseless. If, however, @xmath , then the contribution of
each diagram with @xmath is more important than the contribution of each
of the diagrams with @xmath . Finally, we also see that the contribution
of the @xmath -corrections of a diagram (Eq. ( 6.56 )) is always
negligible compared to the leading contribution of each diagram with
@xmath . These observations lead to the conclusion that, if @xmath and
@xmath become large with @xmath , then the leading contribution to
@xmath comes from the diagrams with @xmath , and that there are no
corrections to these contributions. If we assume that @xmath is small,
then the importance of these diagrams decreases with the number of boson
loops @xmath as @xmath .

##### 6.2.3.1 The loop expansion of @xmath

Now we calculate the first few terms in the loop expansion of @xmath .
We start with the diagrams with one loop (remember that it is an
expansion in boson loops and that we only have to calculate connected
diagrams for @xmath ). The sum of all @xmath -loop diagrams with @xmath
is given by the l.h.s. of Eq. ( 6.4 ), resulting in the r.h.s. of Eq. (
6.25 ). To calculate the higher loop diagrams, we introduce the
effective vertex of Eq. ( 6.5 ) and the partly re-summed propagator

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.68)
  -- -------- -------- -- --------

The contribution of the @xmath -loop diagrams with @xmath is given by

  -- -- -------- -- --------
        @xmath      
        @xmath      
        @xmath      (6.69)
  -- -- -------- -- --------

where we define

  -- -------- -- --------
     @xmath      (6.70)
  -- -------- -- --------

Notice that the first three diagrams vanish if @xmath @xmath . The
contribution of the @xmath -loop diagrams with @xmath is given by

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (6.71)
  -- -------- -- --------

If @xmath @xmath , then only the first four diagrams are relevant, and
their contribution @xmath satisfies

  -- -------- -- --------
     @xmath      (6.72)
  -- -------- -- --------

#### 6.2.4 Various limits

In the previous calculations, @xmath was the expansion parameter and the
expansion of the generating function only makes sense if it is
considered to be small. Furthermore, a limit in which @xmath does not
exist, because the zeroth order term is proportional to @xmath . In
order to analyze limits in which @xmath as well as @xmath go to
infinity, we can go over to the standardized variable @xmath of the
discrepancy (Section 2.1.2 ), where

  -- -------- -------- -- --------
     @xmath   @xmath      (6.73)
     @xmath   @xmath      (6.74)
  -- -------- -------- -- --------

The generating function of the probability distribution of the
standardized variable is given by

  -- -------- -- --------
     @xmath      (6.75)
  -- -------- -- --------

Instead of the parameter @xmath , the parameter @xmath is considered to
be of @xmath in this perspective, in the sense that it are these values
of @xmath that give the important contribution to the inverse Laplace
transform to get from the generating function to the probability
density, and the contribution of a diagram changes from ( 6.67 ) to

  -- -------- -- --------
     @xmath      (6.76)
  -- -------- -- --------

In the following we will investigate limits of @xmath with, at first
instance, the criterion of Eq. ( 6.45 ) as only restriction. The fact
that the variance @xmath shows up explicitly in the contribution of the
diagrams, forces us to specify the behavior of @xmath more precisely. We
will take

  -- -------- -- --------
     @xmath      (6.77)
  -- -------- -- --------

Notice that @xmath if @xmath and that @xmath does not exist if @xmath .
Furthermore, we cannot read off the natural expansion parameter from the
contribution of the diagrams anymore, and have to specify the behavior
of @xmath . We will only consider limits in which

  -- -------- -- --------
     @xmath      (6.78)
  -- -------- -- --------

Although they are a small subset of possible limits, those that can be
specified by a pair @xmath show an interesting picture. We will derive
the results in the next section, but present them now in the following
phase diagram:

[]

It shows the region @xmath of the real @xmath -plane. In this region,
there is a critical line @xmath , given by

  -- -------- -- --------
     @xmath      (6.79)
  -- -------- -- --------

It separates @xmath into two regions @xmath and @xmath , neither of
which contains @xmath . Our results are the following. Firstly,

  -- -- -- --------
           (6.80)
  -- -- -- --------

In this region, the standardized variable is not appropriate, and we see
that there are too many diagrams that grow indefinitely with @xmath .
Secondly,

  -- -- -- --------
           (6.81)
  -- -- -- --------

Because we used the standardized variable, this distribution has
necessarily zero expectation and unit variance. Finally,

  -- -- -- --------
           (6.82)
  -- -- -- --------

One of these limits we were able to calculate explicitly. It appears if
@xmath @xmath , which is, for example, satisfied in the case of equal
binning. In this limit, the generating function is given by

  -- -------- -- --------
     @xmath      (6.83)
  -- -------- -- --------

In Appendix 6.5 , we show that the probability distribution @xmath
belonging to this generating function, which is the inverse Laplace
transform, is given by

  -- -------- -- --------
     @xmath      (6.84)
  -- -------- -- --------

It consists of an infinite number of Dirac delta-distributions, weighed
with a Poisson distribution. The delta-distributions reveal the fact
that, for finite @xmath and @xmath , the Lego discrepancy, and also the
@xmath -statistic, can only take a finite number of values, so that the
probability density should consist of a sum of delta-distributions. In
the usual limit of @xmath , the discrete nature of the random variable
disappears, and the @xmath -distribution is obtained. In our limit,
however, the discrete nature does not yet disappear. A continuous
distribution is obtained if @xmath , which corresponds with going over
from @xmath to @xmath . Then @xmath .

#### 6.2.5 Derivation of the various limits

We will deal with the cases @xmath , @xmath and @xmath separately.

##### 6.2.5.1 @xmath

We distinguish the three cases @xmath , @xmath and @xmath .

If @xmath , then @xmath , and the contribution @xmath of a diagram
@xmath satisfies @xmath , with

  -- -------- -- --------
     @xmath      (6.85)
  -- -------- -- --------

A short analysis shows that only diagrams with @xmath or @xmath give a
non-vanishing contribution, and those diagrams are

  -- -------- -------- -- --------
     @xmath   @xmath      (6.86)
     @xmath   @xmath      (6.87)
  -- -------- -------- -- --------

The first diagram gives a contribution that is linear in @xmath and
cancels with the exponent in Eq. ( 6.75 ). This has to happen for every
value of @xmath , and as we will see, this diagram will occur always.
Notice that the diagrams above are the first two diagrams in the series
on the l.h.s of Eq. ( 6.4 ). The logarithm of the generating function
becomes quadratic, so that the probability distribution becomes
Gaussian.

If @xmath , then again @xmath , so that @xmath , and we have to add the
diagrams with @xmath :

  -- -------- -- --------
     @xmath      (6.88)
  -- -------- -- --------

If @xmath , then @xmath and @xmath , so that, besides the diagram of
Eq. ( 6.86 ), only the diagrams of Eq. ( 6.88 ) give a non-vanishing
contribution, and this contribution is equal to @xmath .

##### 6.2.5.2 @xmath

In this case, @xmath , and the contribution @xmath of a diagram @xmath
satisfies @xmath with

  -- -------- -- --------
     @xmath      (6.89)
  -- -------- -- --------

If @xmath , then @xmath increases with the number of boson loops @xmath
, and we are not able to calculate the limit of @xmath .

If @xmath , then the only diagrams that have a non-vanishing
contribution are those with @xmath , @xmath or @xmath . These are
exactly the diagrams of Eq. ( 6.86 ), Eq. ( 6.87 ) and Eq. ( 6.88 ).
Notice, however, that the diagrams of Eq. ( 6.88 ) cancel if @xmath :
then they are irrelevant . The resulting asymptotic distribution is
Gaussian again.

If @xmath , then @xmath disappears from the equation for @xmath , and we
obtain a non-Gaussian asymptotic distribution. The diagrams that
contribute are those with @xmath or @xmath . There is, however, only one
relevant diagram with @xmath , namely the diagram of Eq. ( 6.86 ) that
gives the linear term. We have to be careful here, because the other
diagrams with @xmath still might be non-vanishing. A short analysis
shows that they are given by the sum of all ways to put daisy diagrams
to one fermion loop, and that their contribution is given by

  -- -------- -- --------
     @xmath      (6.90)
  -- -------- -- --------

We know that, if @xmath , then @xmath with @xmath , so that

  -- -------- -- --------
     @xmath      (6.91)
  -- -------- -- --------

The first term gives the leading contribution; the contribution of the
relevant diagram, which consists of a boson loop and a fermion loop
attached to one vertex. The second term is irrelevant with respect to
the first, but can still be non-vanishing, depending on the behavior of
@xmath . Remember that @xmath and @xmath , so that @xmath , and we can
see that the contribution is only vanishing if

  -- -------- -- --------
     @xmath      (6.92)
  -- -------- -- --------

For @xmath this relation is satisfied because @xmath . For @xmath this
relation is also satisfied if @xmath .

If the relation is also satisfied for the other values of @xmath , then
the only diagrams that contribute to the generating function are the
relevant diagrams with @xmath :

  -- -------- -- --------
     @xmath      (6.93)
  -- -------- -- --------

where we used the effective vertex ( 6.5 ) again. The contribution of a
diagram of this type with @xmath boson lines is given by

  -- -------- -- --------
     @xmath      (6.94)
  -- -------- -- --------

The factor @xmath is the symmetry factor of this type of diagram. If we
sum the contribution of these diagrams and use that @xmath , we obtain

  -- -------- -- --------
     @xmath      (6.95)
  -- -------- -- --------

##### 6.2.5.3 @xmath

In this case, @xmath and the contribution @xmath of a diagram @xmath
satisfies @xmath with

  -- -------- -- --------
     @xmath      (6.96)
  -- -------- -- --------

If @xmath , then @xmath increases with the number of boson loops @xmath
, and we are not able to calculate the limit of @xmath .

If @xmath , then the only diagrams that have a non-vanishing
contribution are those with @xmath , @xmath or @xmath . These are
exactly the diagrams of Eq. ( 6.86 ), Eq. ( 6.87 ) and Eq. ( 6.88 ).
Notice, however, that the diagrams of Eq. ( 6.88 ) cancel if @xmath :
then they are irrelevant . The resulting asymptotic distribution is
Gaussian.

If @xmath , then @xmath . Because @xmath , we have @xmath , and
non-vanishing diagrams have @xmath . Their contribution is given by the
r.h.s. of Eq. ( 6.91 ), the first term of which gives the term linear in
@xmath . The second term is non-vanishing, because @xmath and @xmath .

### 6.3 Stronger-than-weak limits for diaphony

In [ 27 ] , it is proven that the standardized variable of the Fourier
diaphony (Section 3.3.3 ) converges in distribution to a Gaussian
variable if the number @xmath of points in the point set, and with it
the number @xmath of dimensions of the integration region, goes to
infinity such that @xmath goes to zero, where

  -- -------- -- --------
     @xmath      (6.97)
  -- -------- -- --------

To be more precise, if @xmath is a nondecreasing sequence such that

  -- -------- -- --------
     @xmath      (6.98)
  -- -------- -- --------

where we include in the notation “ @xmath ” the dependence on @xmath
through @xmath . The proof makes use of the Central Limit Theorem as
given in Section 2.1.5.1 , and is, roughly speaking, based on the fact
that the conditions in the theorem are satisfied if @xmath .

#### 6.3.1 The observation

The Central Limit Theorem provides a weak limit for the distribution,
which becomes dramatically clear in a short calculation. The expectation
value and the variance of the diaphony are given by @xmath and @xmath ,
leading to

  -- -------- -- --------
     @xmath      (6.99)
  -- -------- -- --------

The moments of the diaphony contain contributions of all kind of
convolutions of the reduced two-point function @xmath . One such
convolution for the fifth moment @xmath is given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

If @xmath becomes large, the leading contribution in the expression
above is given by the first term. The convolution contributes to @xmath
with a third power of @xmath , and this means that the fifth moment of
the standardized variable behaves at least as

  -- -------- --
     @xmath   
  -- -------- --

if @xmath becomes large, and calculation of the other contributions to
@xmath shows that this behavior is not canceled. So clearly, the fifth
moment of the standardized variable may explode in the weak limit of (
6.98 ).

#### 6.3.2 The statement

In this section, we derive a limit in which all moments of the
standardized variable converge to the moments of a normal variable,
which is therefore ‘stronger’ than the weak limit, in the sense that if
@xmath grows with @xmath such that the ‘strong’ limit appears, it grows
such that the criterion of ( 6.98 ) is certainly satisfied (Section
2.1.7 ). Actually, we will see that the ‘strong’ limit appears under the
same type of condition, but of course with a constant @xmath . Our exact
statement shall be that if @xmath as @xmath such that

  -- -------- -- ---------
     @xmath      (6.100)
  -- -------- -- ---------

then all moments of the standardized variable converge to the moments of
a normal variable . We shall show that it works for @xmath and probably
even for @xmath , where

  -- -------- -- ---------
     @xmath      (6.101)
  -- -------- -- ---------

Note that @xmath and @xmath .

#### 6.3.3 The scenario

The logarithm @xmath of the generating function of the standardized
variable is given by

  -- -------- -- ---------
     @xmath      (6.102)
  -- -------- -- ---------

where @xmath is the logarithm of the generating function of the
probability distribution of @xmath . So @xmath can be calculated using
the Feynman rules of Section 3.2.4 if @xmath is replaced by @xmath .
This is equivalent with using @xmath and replacing the propagator @xmath
by @xmath , and it is this what we shall do. Only connected diagrams
contribute to @xmath , and there is only one diagram that gives a
contribution linear in @xmath , namely

  -- -------- -- ---------
     @xmath      (6.103)
  -- -------- -- ---------

which cancels against the term @xmath in Eq. ( 6.102 ). The second-order
in @xmath is given by

  -- -------- -- ---------
     @xmath      (6.104)
  -- -------- -- ---------

Note that the third and the fourth diagram cancel each other. These
results for the first two orders in @xmath are in correspondence with
the fact that we use the standardized variable. If we find a criterion
dictating how @xmath as @xmath such that the contribution of all other
diagrams vanishes, regardless of the value of @xmath , then this
vanishing happens order by order in @xmath because each order consists
of a sum of diagrams. This then implies that all moments of the
standardized variable converge to the moments of a normal variable.

#### 6.3.4 The calculation

In order to calculate the diagrams, we expand @xmath in terms of the
complex Fourier basis

  -- -------- -- ---------
     @xmath      (6.105)
  -- -------- -- ---------

where the sum is over all @xmath except the constant mode @xmath , and
we denote @xmath . The strengths @xmath are given by

  -- -------- -- ---------
     @xmath      (6.106)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (6.107)
  -- -------- -- ---------

Notice that, because we absorbed the factor @xmath in @xmath , the
strengths depend on @xmath . Because we expanded @xmath in terms of the
complex exponentials, convolutions of this two-point function can be
calculated as sums over products of the strengths @xmath . As a result
of this, we can go over to another boson propagator

  -- -------- -- ---------
     @xmath      (6.108)
  -- -------- -- ---------

and the rule that in a vertex with @xmath boson legs, boson propagators
have to be convoluted as

  -- -------- -- ---------
     @xmath      (6.109)
  -- -------- -- ---------

where the logical @xmath -function expresses that the sum of the labels
has to be zero. These Feynman rules give the same result as the original
rules.

Because the Fourier diaphony is translation invariant, it is one-vertex
decomposable , so that we only have to consider one-vertex irreducible
(1VI) diagrams (Section 6.1.1.4 ). The other diagrams cancel exactly.
For each 1VI diagram that has vertices that are not of the type of Eq. (
6.5 ), there exists a diagram that has exactly the same bosonic part,
but only effective vertices of the type of Eq. ( 6.5 ), and therefore
carries a smaller power of @xmath . Combining this with the fact that we
only have to consider connected diagrams to calculate @xmath , we see
that

  -- -------- -- ---------
     @xmath      (6.110)
  -- -------- -- ---------

The power of @xmath that is carried by a relevant diagram is given by
@xmath , where @xmath is the sum of all bosonic legs of all vertices,
and @xmath the number of vertices. Basic graph theory tells us that the
number of bosonic lines @xmath is equal to @xmath , and the number of
bosonic loops @xmath is equal to @xmath , so that

  -- -------- -- ---------
     @xmath      (6.111)
  -- -------- -- ---------

So the natural way to order the diagrams is by number of bosonic loops.
From now on, the drawing of a diagram only represents the contribution
coming from the bosonic part of the diagram, stripped from its factors
of @xmath , its factors of @xmath coming from the fermionic piece, and
the symmetry factors, which we call the bare contribution.

##### 6.3.4.1 One loop

Diagrams with no loops do not exist (because of the Landau gauge), and
the relevant diagrams with only one loop contribute with

  -- -------- -- ---------
     @xmath      (6.112)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (6.113)
  -- -------- -- ---------

Using the line of argument with Eq. ( 5.14 ) and Eq. ( 5.15 ), we derive
that @xmath . Explicit calculation shows that @xmath , so that @xmath
for all @xmath , and

  -- -------- -- ---------
     @xmath      (6.114)
  -- -------- -- ---------

The diagrams with one or two vertices contribute to the first two powers
in @xmath (Eq. ( 6.103 ) and Eq. ( 6.104 )). If @xmath and @xmath stays
finite, only one-loop diagrams do not vanish, and the analysis above was
just a repetition of what was done in Chapter 5 .

##### 6.3.4.2 More than one loop

In order to estimate the contribution of the higher-loop diagrams, we
observe that, because @xmath for all @xmath and all values of @xmath ,

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (6.115)
  -- -------- -------- -- ---------

As a result of this, the bare contribution of a relevant diagram can be
estimated by repeated application of the operation

  -- -------- -- ---------
     @xmath      (6.116)
  -- -------- -- ---------

until only one vertex remains. This operation leaves the number of loops
@xmath invariant, so that the bare contribution of a relevant diagram is
smaller than

  -- -------- -- ---------
     @xmath      (6.117)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (6.118)
  -- -------- -- ---------

Using this result, ( 6.110 ) and ( 6.111 ), we conclude that if @xmath ,
then the behavior of the contribution of any 1VI diagram with @xmath
loops is dominated by

  -- -------- -- ---------
     @xmath      (6.119)
  -- -------- -- ---------

so that all diagrams with more than one loop vanish if @xmath as @xmath
, such that

  -- -------- -- ---------
     @xmath      (6.120)
  -- -------- -- ---------

We have already established that, in the expansion of @xmath in @xmath ,
there is no linear term, and the quadratic term is given by @xmath , as
demanded by the fact that we are dealing with the standardized variable.
The one-loop diagrams contributing to the higher powers vanish if @xmath
, and all other diagrams vanish if ( 6.120 ) holds.

##### 6.3.4.3 Leading contributions

In the previous section we have put a bound on the contribution of each
diagram, which resulted in ( 6.120 ). This result comes from the bound
on the two-loop diagrams. For the lower-loop diagrams, however, the
determination of the actual leading behavior is attainable. There is,
for example, only one relevant two-loop diagram, which has the following
bosonic structure:

  -- -------- -- ---------
     @xmath      (6.121)
  -- -------- -- ---------

Its bare contribution is

  -- -------- -- ---------
     @xmath      (6.122)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (6.123)
  -- -------- -- ---------

so that it suffices to take @xmath in ( 6.120 ). The relevant three-loop
diagrams have bosonic parts

  -- -------- -- ---------
     @xmath      (6.124)
  -- -------- -- ---------

and using ( 6.115 ), we immediately see that the last two diagrams are
bounded by the first, which has a bare contribution

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (6.125)
  -- -------- -------- -- ---------

Application of ( 6.115 ) shows also that the bare contribution of the
second diagram is bounded by

  -- -------- -- ---------
     @xmath      (6.126)
  -- -------- -- ---------

and we have @xmath and @xmath , so that @xmath also suffices in ( 6.120
). We suspect that this works to all loop-orders, and that we can
actually take @xmath .

### 6.4 Conclusions

We have presented finite-sample corrections to the probability
distributions of quadratic discrepancies under sets of @xmath random
points. The corrections are terms in an @xmath expansion of the
generating function of the probability distribution, consisting of the
contribution of a finite number of Feynman diagrams. We presented the
diagrams up to and including the order of @xmath for the general case,
and derived a rule of diagram cancellation in the case of special
discrepancies, which we call one-vertex decomposable. We have applied
the formalism to the Lego discrepancy, the @xmath -discrepancy in one
dimension and the Fourier diaphony in one dimension, and calculated the
first two terms in the expansion. For the Lego discrepancy, this
resulted in Eq. ( 6.25 ) and Eq. ( 6.27 ), for the @xmath -discrepancy
in Eq. ( 6.35 ) and Eq. ( 6.36 ), and for the Fourier diaphony in Eq. (
6.40 ) and Eq. ( 6.41 ). The Fourier diaphony and the Lego discrepancy
with equal binning are one-vertex decomposable. For the latter, we also
calculated the @xmath -term, which is in correspondence with the result
of an alternative calculation up to the order of @xmath , given in
Appendix 6.5 .

In the second part of the chapter, we focused on the variant of the Lego
discrepancy that is equivalent with a @xmath -statistic of @xmath data
points distributed over @xmath bins. We have presented a procedure to
calculate the generating function perturbatively if @xmath and @xmath
become large. The natural expansion parameter we have identified to be
@xmath , and we have calculated the first few terms in the series
explicitly.

In order to calculate limits for the Lego discrepancy in which @xmath ,
we have introduced the objects of Eq. ( 6.44 ) and restricted the
behavior of the size of the bins such that they satisfy Eq. ( 6.45 ).
Furthermore, we have gone over to the standardized variable of the
discrepancy. For this variable, we have derived a phase diagram,
representing the limits specified by Eq. ( 6.77 ) and Eq. ( 6.78 ). We
have formulated the results in ( 6.80 ), ( 6.81 ) and ( 6.82 ). On of
these results is that there are non-trivial limits if @xmath such that
@xmath with @xmath . This result is in stark contrast with the rule of
thumb that, in order to trust the @xmath -distribution, each bin has to
contain at least a few data points.

Finally, we have derived a limit in which all the moments of the
standardized variable of the Fourier diaphony converge to the moments of
a normal variable, which is given in ( 6.100 ).

### 6.5 Appendices

Appendix 6A
If we define, for the Lego discrepancy with equal bins, @xmath , @xmath
and

  -- -------- -- ---------
     @xmath      (6.127)
  -- -------- -- ---------

then the only non-zero @xmath up to @xmath are given by

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Appendix 6B
We want to calculate the integral

  -- -------- -- ---------
     @xmath      (6.128)
  -- -------- -- ---------

We will make use of the fact that

  -- -------- -- ---------
     @xmath      (6.129)
  -- -------- -- ---------

for all @xmath , so that

  -- -------- -- ---------
     @xmath      (6.130)
  -- -------- -- ---------

Notice that the integral is independent of @xmath , so that the sum can
be interpreted as a sum of Dirac delta-distributions:

  -- -------- -- ---------
     @xmath      (6.131)
  -- -------- -- ---------

These delta-distributions restrict the values that @xmath can take. If
we use these restrictions and do the appropriate variable substitutions,
the remaining integral in ( 6.130 ) can be reduced to

  -- -------- -- ---------
     @xmath      (6.132)
  -- -------- -- ---------

where @xmath and the contour is closed around @xmath . According to
Cauchy’s theorem, the final integral is only non-zero if @xmath , and in
that case its value is @xmath . The combination of these results gives
Eq. ( 6.84 ).

## Chapter 7 Phase space integration

In particle physics, there is the need to integrate transition
probabilities of particle processes over phase space, the space of all
possible configurations of the final-state momenta (Section 1.2.2 ).
This is usually done with the Monte Carlo method, and the first sections
of this chapter deal with its basics and some useful techniques. The
formalism converges towards the application for phase space in Section
7.4 .

\minitoc

### 7.1 Monte Carlo integration

For the Monte Carlo (MC) method of numerical integration, the integral
of a function @xmath over an integration region @xmath has to be reduced
to the integral of a function @xmath over an @xmath -dimensional
hypercube @xmath . In order to do so, a suitable mapping @xmath and the
normalization function @xmath have to be determined (Section 1.1.2 ). A
conceptual help in the search for such mappings is considering them
algorithms to generate random variables with a certain probability
distribution. This probability distribution enters the integration
problem as follows. Given a probability density @xmath on @xmath , the
integral of @xmath over @xmath can be written as

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

so that the integral can be interpreted as the expectation value of
@xmath under the probability density @xmath on @xmath . The only
restriction on @xmath is that its support should contain the support of
@xmath . The Monte Carlo method can then be directly applied to @xmath .
Let us denote the average of a function @xmath over the first @xmath
points of a sequence @xmath in @xmath distributed following @xmath by

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

If @xmath is taken equal to @xmath , then the expectation value @xmath
and the variance @xmath , where

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

If @xmath exists, so that @xmath is a finite number, then @xmath
converges in probability to @xmath , and @xmath can be interpreted as
the expected squared error (Section 2.1.5.2 ). This number is positive
by definition, and extremalization with respect to @xmath leads to
@xmath , which minimizes @xmath to zero if @xmath is positive.
Importance sampling can be interpreted as the effort to make @xmath look
like @xmath as much as possible. The squared error can be estimated with

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

which satisfies @xmath . The integration is done most efficiently if the
numbers @xmath fluctuate as little as possible, so that @xmath is as
small as possible. That is what importance sampling should take care of.
The expected squared error on the error can be estimated with the help
of

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

which satisfies @xmath . If @xmath exists, then @xmath is a good
estimator for the the expected squared error on the estimate of the
expected squared integration error.

The integration problem is now for large part reduced to that of
generating the sequence of points in @xmath with the density @xmath . In
practice, the mapping @xmath still has to be made explicit, because
algorithms usually start with the generation of numbers between @xmath
and @xmath , but the analysis of the whole algorithm can be done on a
‘higher level’ by considering the piece that generates the @xmath
-variables a given ‘black box’. The connection with Section 1.1.2 can be
established by taking @xmath .

### 7.2 The unitary algorithm formalism

In general, it is hard to find an efficient algorithm to generate
sequences with a given distribution. It is often even hard to determine
the density under which a sequence, generated with a given algorithm, is
distributed. For a certain class of algorithms the latter can be done
analytically with the unitary algorithm formalism (UAF). This is the
class of unitary algorithms, that is, algorithms which produce an output
with probability one. This may sound a bit mysterious, and in order to
explain what we mean, we introduce the class of stepwise unitary (SU)
algorithms, of which all steps produce an output with probability one.
We illustrate this with an example of a non-SU algorithm to generate a
fair dice with five sides:

1.  throw a fair dice, output @xmath number of points;

2.  if the number of points is less than six: output @xmath number of
    points, else throw again.

The first step produces an output with probability one: if you throw a
dice, it generates a number. The second step, however, produces an
output with probability @xmath , so that the algorithm is not SU. The
whole algorithm, is unitary: the probability to produce no output is
@xmath . Consequently, stepwise unitary is a relative concept. The steps
may be “black boxes” that can be trusted to produce an output with
probability one. Consider the following algorithm to generate numbers
between @xmath and @xmath :

1.  throw a fair dice with five sides, output @xmath number of points;

2.  throw the dice again, output @xmath number of points plus previous
    number of points.

This is a SU algorithm as long as we do not ask the question how the
fair dice with five sides is generated.

#### 7.2.1 Notation

We shall introduce the UAF with the help of two examples, but first we
have to introduce some notation. We will frequently use the logical step
function @xmath , which returns @xmath if a statement @xmath is true,
and @xmath otherwise:

  -- -------- -- -------
     @xmath      (7.6)
  -- -------- -- -------

A relation that is satisfied by this function and that we will often use
is

  -- -------- -- -------
     @xmath      (7.7)
  -- -------- -- -------

Also the Dirac delta-distribution will often appear, and we recall its
most important features (cf. [ 5 ] ): if @xmath is a sufficiently
regular function on @xmath , then

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

and if @xmath is an invertible and differentiable mapping, then

  -- -------- -- -------
     @xmath      (7.9)
  -- -------- -- -------

where @xmath is the determinant of the Jacobian matrix of @xmath . An
integral over many variables will from now on start with a single @xmath
-symbol, and for every variable @xmath a @xmath means ‘integrate @xmath
over the appropriate integration region’. The order in which the
variables appear is irrelevant. If it is not evident what the
‘appropriate integration region’ is, we shall make it explicit with the
help of @xmath -functions.

#### 7.2.2 The UAF for SU algorithms

The following is an example of the use of the UAF for a SU algorithm. It
is an algorithm to generate @xmath numbers @xmath , @xmath uniformly
distributed in @xmath such that their sum is equal to @xmath , and we
are going to prove that it actually does. The algorithm goes as follows:

1.  generate @xmath numbers @xmath , @xmath in @xmath , distributed
    independently
    and with density @xmath ;

2.  put @xmath ;

3.  put @xmath for @xmath .

The algorithm clearly produces numbers the sum of which is equal to
@xmath . The question is whether they are distributed uniformly, i.e.,
whether the density is, up to a normalization constant, equal to @xmath
. The UAF can answer the question as follows. Write every generation of
a variable in the algorithm as the integral over the density with which
it is generated, and interpret every assignment as a generation with a
density that is given by a Dirac delta-distribution. Only the assignment
of the final output should not be written as an integral, but only with
the delta-distributions. The integral obtained gives the generated
density @xmath . So in this case we have

  -- -- -- --------
           (7.10)
  -- -- -- --------

The unitarity of the algorithm is represented by the fact that
integration over the @xmath -variables of this equation gives the
identity @xmath . To find the density, we have to eliminate the @xmath
-and @xmath -integrals. Application of the rules for the
delta-distributions gives

  -- -------- -- --------
     @xmath      (7.11)
  -- -------- -- --------

and we see that the the @xmath -variables are generated with the correct
density. We even calculated the normalization factor, which is @xmath .
The step ‘generate @xmath with density @xmath ’ is a black box in this
example, but can be made explicit. Such variable can be obtained by
generating @xmath uniformly in @xmath and putting @xmath , since

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

#### 7.2.3 The UAF for non-SU algorithms

As an application of the UAF to a non-SU algorithm, we show the
correctness of the ratio-of-uniforms method for the generation of a
random variable with a given density @xmath . The algorithm goes as
follows [ 6 ] . Let @xmath , @xmath and @xmath . Then one has to

1.  generate @xmath uniformly in @xmath ;

2.  generate @xmath uniformly in @xmath ;

3.  if @xmath then put @xmath , else reject @xmath and start anew.

Just as our algorithm for the fair dice with five sides, it uses the
rejection method, and the third step is not unitary. For this algorithm,
we can write down a recursive equation for the probability density
@xmath that is generated. If we denote the volume of the space in which
@xmath and @xmath are generated @xmath , then this equation is then
given by

  -- -------- -- --------
     @xmath      (7.13)
  -- -------- -- --------

Integration of the equation over @xmath gives the identity @xmath again,
expressing the unitarity of the algorithm. If we now use Eq. ( 7.7 ) and
replace the variables @xmath by @xmath and @xmath , we get the equation

  -- -------- -- --------
     @xmath      (7.14)
  -- -------- -- --------

The region in which @xmath and @xmath are generated are such that @xmath
. Furthermore is @xmath , so that the equation becomes

  -- -------- -- --------
     @xmath      (7.15)
  -- -------- -- --------

and we see that the algorithm is correct. We even see that the function
@xmath , used in the algorithm, does not have to be normalized: the
algorithm itself is unitary.

### 7.3 Some useful techniques

#### 7.3.1 Inversion

The most straightforward way of generating random variables @xmath
following a certain probability distribution @xmath is with an
invertible mapping @xmath , by generating @xmath and putting @xmath .
The generated density @xmath is given by

  -- -------- -- --------
     @xmath      (7.16)
  -- -------- -- --------

so that @xmath should be equal to @xmath . The search for @xmath is an
integration and inversion problem, and is usually very hard to solve in
practice, even for one-dimensional variables.

#### 7.3.2 Crude MC

Sometimes, part of the difficulty of the integration problem lies in the
shape of the integration region @xmath , which might be complicated.
Usually, however, it can be seen as a subspace of a simpler manifold
@xmath with the same dimension. One can look for a probability density
@xmath on @xmath then, and integrate the function @xmath , where @xmath
is the characteristic function of @xmath . This just means that a
density

  -- -------- -- --------
     @xmath      (7.17)
  -- -------- -- --------

is used on @xmath . The algorithm to generate a sequence of variables
@xmath following this density is very simple:

1.  generate @xmath in @xmath following @xmath ;

2.  if @xmath then put @xmath , else reject @xmath and start anew.

This is called crude or hit-and-miss MC. The proof of the correctness is
also simple. In the UAF, the generated density @xmath satisfies

  -- -------- -- --------
     @xmath      (7.18)
  -- -------- -- --------

If we use Eq. ( 7.7 ) and evaluate the integrals, ( 7.17 ) is found as
the solution to the equation.

In principle, this method always works, but can be inefficient if the
volume of @xmath is much larger than the volume of @xmath . If the
integrand is as simple as possible, i.e., @xmath so that the original
problem was that of determining the volume @xmath of @xmath , and if one
would take for @xmath the uniform distribution on @xmath , then

  -- -------- -- --------
     @xmath      (7.19)
  -- -------- -- --------

So if the difference between the volumes is large, one better chooses a
density @xmath that is substantially larger on @xmath than on @xmath .

#### 7.3.3 Rejection

Crude MC is a special case of the rejection method to generate variables
@xmath following a density @xmath on @xmath . One needs an algorithm to
generate variables on @xmath following some density @xmath and a number
@xmath such that @xmath for all @xmath . To obtain the density @xmath ,
one should

1.  generate @xmath following @xmath , and @xmath uniformly in @xmath ;

2.  if @xmath then put @xmath , else reject @xmath and start anew.

The generated density @xmath satisfies

  -- -------- -- --------
     @xmath      (7.20)
  -- -------- -- --------

and if we use Eq. ( 7.7 ) again and the fact that @xmath , the solution
@xmath is found. In principle, this method works for any bounded @xmath
, since there are always an easy to generate density @xmath and a @xmath
that will do the job: @xmath and @xmath . However, the algorithm can
become very inefficient. The efficiency can be expressed by @xmath , and
if this number is small, the variable @xmath will often be rejected in
step 2.

#### 7.3.4 Sum of densities

As an example in which integer random variables have to be generated, we
present a method to generate a density that is the normalized sum of a
number of positive functions @xmath with @xmath . To generate the
density

  -- -------- -- --------
     @xmath      (7.21)
  -- -------- -- --------

one has to

1.  generate an integer @xmath with probability @xmath and put @xmath ;

2.  generate @xmath with density @xmath and put @xmath .

To cast it into the UAF, a summation over the integer random variable
has to be included into the equation for the density generated:

  -- -------- -- --------
     @xmath      (7.22)
  -- -------- -- --------

The assignment ‘ @xmath ’ is represented by the Kronecker delta-symbol
@xmath . Evaluation of the integrals leads trivially to the correct
density. To generate @xmath , the unit interval @xmath can be dissected
into @xmath bins of size @xmath , and @xmath becomes the number of the
bin a random number @xmath , distributed uniformly in @xmath , falls in.

#### 7.3.5 Adaptive MC

If one considers the actual calculation of a MC-integral a real random
process, it is a small step to the question whether the density @xmath
may change during the process. The answer is yes (Section 2.1.5.2 ). If
@xmath is generated following a density @xmath , and then @xmath
following @xmath which depends on @xmath , and then @xmath following
@xmath which depends on @xmath and so on, then @xmath converges in
probability to @xmath , with an estimated squared error given by @xmath
, where

  -- -------- -- --------
     @xmath      (7.23)
  -- -------- -- --------

with

  -- -------- -- --------
     @xmath      (7.24)
  -- -------- -- --------

The explicit dependence on @xmath of this expression shows that, by
adapting the density for each integration point in the right way, the
error may be reduced and the integration process optimized.

An example of a method to adapt the variance is weight optimization in
multichannel-MC [ 31 ] , in which a density @xmath is generated, where
each function @xmath is a probability density itself, and the parameters
@xmath are positive and satisfy @xmath . Let us define

  -- -------- -- --------
     @xmath      (7.25)
  -- -------- -- --------

so that @xmath . It is not difficult to see that the variance, as
function of the parameters @xmath , has a (local) minimum if the values
of these parameters are such that @xmath has the same value for all
@xmath . Of course, the problem of finding these values is possibly even
more difficult than the original integration problem, but with adaptive
MC the values might be found approximately using an iterative procedure.
The variance will then improve with each step.

In [ 31 ] , the following is suggested: one starts with some (sensible)
values for the parameters @xmath and, after generating a number of
@xmath points @xmath following the density @xmath , one estimates @xmath
with

  -- -------- -- --------
     @xmath      (7.26)
  -- -------- -- --------

for all @xmath . These numbers are then used to improve the values of
the parameters, for example through the prescription

  -- -------- -- --------
     @xmath      (7.27)
  -- -------- -- --------

where @xmath is some constant. The plausibility of this prescription is
supported by the example of stratified sampling, in which the functions
@xmath are normalized characteristic functions @xmath of non-overlapping
subspaces of the integration region. In that case, @xmath , and we see
that putting @xmath will give the local minimum immediately, starting
from any configuration for the parameters @xmath .

### 7.4 Random momenta beautifully organized

As mentioned in Section 1.2.2 , particle physicists often need to
integrate differential cross sections over phase space (PS), which is
the space of all physically possible final-state momentum
configurations. Usually, it depends on the transition amplitude which
configurations are allowed, and here we mean by PS the space of all
final-state momentum configurations for which the separate momenta sum
up to a given momentum, and for which the particles have given masses.
Because these restrictions reduce the dimension of the integration
region, it has measure zero in the space of all momentum configurations
so that the crude MC method is no option.

One way to generate PS is by sequential two-body decays, i.e., by the
recursive splitting of each momentum generated so far into two momenta
(cf. [ 32 ] ). The drawback of this method is that the efficiency is
poor if the number of momenta and the total energy become large (cf. [
33 ] ). The high-energy limit is equivalent with the limit in which the
masses of the particles become negligible, and for this situation, RAMBO
[ 33 ] can be used. It generates any number of massless momenta with a
given total energy distributed uniformly in PS. We will not deal with
the algorithm adapted for the generation of massive momenta [ 34 ] .
Another approach to PS generation, which we will also not address, uses
the help of the metropolis algorithm [ 35 ] .

#### 7.4.1 Notation

The relativistic momentum of an elementary particle is a vector in
@xmath . Its first component, also called the @xmath -component, gives
the energy of the particle ¹ ¹ 1 We use units with which the speed of
light is equal to @xmath . , and the other three components give the
real momentum in three-dimensional space:

  -- -------- -- --------
     @xmath      (7.28)
  -- -------- -- --------

The momentum with the opposite @xmath -momentum is denoted by

  -- -------- -- --------
     @xmath      (7.29)
  -- -------- -- --------

The interpretation of @xmath as a real vector space can be carried
forward in the sense that a system of non-interacting particles has a
momentum that is equal to the sum of the momenta of the separate
particles. We shall need the first and the fourth canonical basis
vectors, which we denote

  -- -------- -- --------
     @xmath      (7.30)
  -- -------- -- --------

@xmath becomes Minkowski space if it is endowed with the Lorentz
invariant quadratic form

  -- -------- -- --------
     @xmath      (7.31)
  -- -------- -- --------

The same notation for the quadratic form and the @xmath -component will
not lead to confusion, because the @xmath -component will not appear
explicitly anymore after this section. The combination @xmath defines a
bi-linear product of two momenta, which is two times the scalar product

  -- -------- -- --------
     @xmath      (7.32)
  -- -------- -- --------

The notation with the parentheses shall be used in the next chapter. For
physical particles, @xmath has to be positive, and in that case, the
square root gives the invariant mass of the particle:

  -- -------- -- --------
     @xmath      (7.33)
  -- -------- -- --------

The group of linear transformations on @xmath that leave the quadratic
form invariant, and the members of which have determinant @xmath and
leave the sign of the @xmath -component of a momentum invariant, is
called the Lorentz group. It is generated by boosts, which are
represented by symmetric matrices, and rotations, which are represented
by orthogonal matrices. A boost that transforms a momentum @xmath , with
@xmath , to @xmath is denoted @xmath , so

  -- -------- -- --------
     @xmath      (7.34)
  -- -------- -- --------

More explicitly, such a boost is given by

  -- -------- -- --------
     @xmath      (7.35)
  -- -------- -- --------

A rotation that transforms @xmath to @xmath is denoted @xmath , so

  -- -------- -- --------
     @xmath      (7.36)
  -- -------- -- --------

Since rotations only change the @xmath -momentum, we shall use the same
symbol if a rotation is restricted to three-dimensional space.

The physical PS of @xmath particles is the @xmath -dimensional subspace
of @xmath , given by the restrictions that the energies of the particles
are positive, the invariant masses squared @xmath are fixed to given
positive values @xmath , and that the sum

  -- -------- -- --------
     @xmath      (7.37)
  -- -------- -- --------

of the momenta is fixed to a given momentum @xmath . The restrictions
for the separate momenta can be expressed with a ‘PS characteristic
distribution’

  -- -------- -- --------
     @xmath      (7.38)
  -- -------- -- --------

The generic PS integral, of a function @xmath of a set @xmath of
momenta, that has to be calculated is then given by

  -- -------- -- --------
     @xmath      (7.39)
  -- -------- -- --------

We explicitly write down the number of degrees of freedom in the
differentials and the delta-distributions in order to keep track of the
dimensions. Each momentum component carries the dimension of a mass.

#### 7.4.2 The algorithm

RAMBO was developed with the aim to generate the flat PS distribution of
@xmath massless momenta as uniformly as possible, and such that the sum
of the momenta is equal to @xmath with @xmath a given squared energy.
This means that the system of momenta is in its center-of-mass frame
(CMF), and that the density is proportional to the ‘PS characteristic
distribution’

  -- -------- -- --------
     @xmath      (7.40)
  -- -------- -- --------

The algorithm consists of the following steps:

###### Algorithm 7.4.1 (Rambo)

1.  generate @xmath massless vectors @xmath with positive energy without
    constraints but under some normalized density @xmath ;

2.  compute the sum @xmath of the momenta @xmath ;

3.  determine the Lorentz boost and scaling transform that bring @xmath
    to @xmath ;

4.  perform these transformations on the @xmath , and call the result
    @xmath .

Trivially, the algorithm generates momenta that satisfy the various
@xmath -constraints, but it is not clear a priori that the momenta have
the correct distribution. To prove that they actually do, we apply the
UAF. It tells us that the generated density @xmath is given by

  -- -------- -------- -- --------
     @xmath               
              @xmath      (7.41)
  -- -------- -------- -- --------

To calculate the distribution yielded by this algorithm, the integral
has to be evaluated. First of all, some simple algebra using @xmath ,
@xmath and the Lorentz and scaling properties of the Dirac @xmath
-distributions leads to

  -- -------- -- --------
     @xmath      (7.42)
  -- -------- -- --------

Furthermore, since we may write

  -- -------- -- --------
     @xmath      (7.43)
  -- -------- -- --------

under the integral, the l.h.s. of Eq. ( 7.41 ) becomes

  -- -------- -- --------
     @xmath      (7.44)
  -- -------- -- --------

In the standard RAMBO algorithm, the following choice is made for @xmath
:

  -- -------- -- --------
     @xmath      (7.45)
  -- -------- -- --------

where @xmath is a positive number with the dimension of an inverse mass.
Therefore, if we use that @xmath and that @xmath for any @xmath , then

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (7.46)
  -- -------- -------- -- --------

As a result of this, the variables @xmath , @xmath only appear in @xmath
, as required. The remaining integral is calculated in the Appendix at
the end of this section, with the result that RAMBO generates the
density

  -- -------- -- --------
     @xmath      (7.47)
  -- -------- -- --------

Incidentally, we have computed here the volume of the PS for @xmath
massless particles:

  -- -------- -- --------
     @xmath      (7.48)
  -- -------- -- --------

Note, moreover, that @xmath does not appear in the final answer; this is
only natural since any change in @xmath will automatically be
compensated by a change in the computed value for @xmath . Finally, it
is important to realize that the ‘original’ PS has dimension @xmath ,
while the resulting one has dimension @xmath : there are configurations
of the momenta @xmath that are different, but after boosting and scaling
end up as the same configuration of the @xmath . It is this reduction of
the dimensionality that necessitates the integrals over @xmath and
@xmath .

The first step of the algorithm consists of generating massless momenta
with positive energy. To generate such momenta, we use that

  -- -------- -- --------
     @xmath      (7.49)
  -- -------- -- --------

with @xmath , where

  -- -------- -- --------
     @xmath      (7.50)
  -- -------- -- --------

From this we can directly see that, to generate @xmath following a
density proportional to @xmath , one should

###### Algorithm 7.4.2 (Massless Momentum)

1.  generate @xmath in @xmath following a density @xmath ;

2.  generate @xmath uniformly distributed in @xmath and @xmath uniformly
    in @xmath ;

3.  construct @xmath and put @xmath .

To generate @xmath following the density @xmath , one can

###### Algorithm 7.4.3 (0-Component)

1.  generate @xmath and @xmath distributed uniformly in @xmath ;

2.  put @xmath ,

since

  -- -------- -- --------
     @xmath      (7.51)
  -- -------- -- --------

#### 7.4.3 Appendix

We have to calculate the integral

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

The ‘Euler substitution’ @xmath casts the integral in the form

  -- -------- --
     @xmath   
  -- -------- --

By the transformation @xmath it can easily be checked that the integral
from @xmath to @xmath is precisely equal to that from @xmath to @xmath ,
so that we may write

  -- -------- --
     @xmath   
  -- -------- --

where we have used, by writing @xmath , that

  -- -------- --
     @xmath   
  -- -------- --

## Chapter 8 Generating QCD-antennas

An algorithm to generate random momenta, distributed with a density that
contains the singular structure typically found in QCD-processes, is
introduced. For the notation used we refer to Section 7.4 .

\minitoc

### 8.1 Introduction

In future experiments with hadron colliders, such as the LHC, many
multi-jet final states will occur, which have very high particle
multiplicities. The initial states will consist of two hadrons in the
center-of-mass frame (CMF). The processes involved in one transition
(one event ) are very complicated, and are usually considered to consist
of three steps. The generic situation is depicted schematically in Fig.
8.1 . Time can be considered to flow from the left to the right in the
picture. The hadrons start the interaction with the emission of partons.
The transition of a hadron into the emitted parton and the leftover is
represented by the white blobs. This is the first step. In the second
step, represented by the grey blob, the partons interact, resulting in
@xmath new partons. In step three, these partons turn into jets with
high particle multiplicities.

The idea is that the contribution of the three steps more-or-less
factorize in the transition amplitude of the whole event, and that the
processes can be dealt with separately. In this chapter, we deal with
step two, the grey blob.

The multi-jet events that will occur in hadron colliders can be divided
into interesting events (IE) and very interesting events (VIE). The main
difference between the two classes is that the existing model of
elementary particles, the standard model, shall not have proven yet its
capability of dealing with the description of the VIE at the moment when
they are analyzed. The IE shall not manifest themselves as such a heavy
test for the standard model. However, we still need to know the cross
sections of the IE in order to compare the ratio of these and those of
the VIE with the predictions of the standard model.

#### 8.1.1 The problem

Large part of the IE can be described by quantum chromo dynamics (QCD),
the formalism of quarks and gluons, with which multi-parton
QCD-amplitudes are calculated. It is well known [ 36 ] that they
contribute to the cross section with a singular behavior in phase space
(PS), given by the so-called antenna pole structure (APS). In
particular, for processes involving only @xmath gluons the most
important contribution comes from the sum of all permutations in the
momenta of

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

and the singular nature stems from the fact that the scalar products
@xmath can become very small. If functions, containing this kind of
kinematical structures, are integrated using the RAMBO (Section 7.4 ),
which generates the momenta distributed uniformly in PS, then a large
number of events is needed to reach a result to acceptable precision. As
an illustration, we present in the left table of Tab. 8.1 the number of
PS points needed to integrate the single antenna of Eq. ( 8.1 ), so not
even the sum of its permutations, to an expected error of @xmath .

The antenna cannot be integrated over the whole of PS because of the
singularities, so these have to be cut out. This is done through the
restriction @xmath for all @xmath , ¹ ¹ 1 Remember that @xmath since
@xmath and @xmath are massless. and in the table the ratio between
@xmath and the total energy @xmath is given. These numbers are based on
the reasonable choice @xmath .

Performing MC integration with very many events is not a problem if the
evaluation of the integrand in each PS point is cheap in computing time.
This is, for example, the case for algorithms to calculate the squared
multi-parton amplitudes based on the so called SPHEL-approximation, for
which only the kinematical structure of ( 8.1 ) is implemented [ 36 ] .
Nowadays, algorithms to calculate the exact matrix elements exist, which
are far more time-consuming [ 38 , 39 ] . As an illustration of what is
meant by ‘more time-consuming’, we present the right table of Tab. 8.1
with the typical cpu-time needed for the evaluation in one PS point of
the integrand for processes of two gluons going to more gluons, both for
the SPHEL-approximation and the exact matrix elements [ 40 ] . It is
expected, and observed, that the exact matrix elements reveal the same
kind of singularity structures as the APS, so that, according to the
tables, the PS integration for a process with @xmath final gluons would
take in the order of @xmath days…

#### 8.1.2 The solution

The solution to this problem is importance sampling. Instead of RAMBO ,
a PS generator should be used which generates momenta with a density
including the APS. The following sections show the construction of such
a PS generator, called SARGE , which stands for S taggered A ntenna R
adiation GE nerator.

### 8.2 The basic antenna

As mentioned before, we want to generate momenta that represent radiated
partons with a density that has the antenna structure @xmath .
Naturally, the momenta can be viewed as coming from a splitting process:
one starts with two momenta, a third is radiated off creating a new pair
of momenta of which a fourth is radiated off and so on. In fact, models
similar to this are used in full-fledged Monte-Carlo generators like
HERWIG . Let us therefore first try to generate a single massless
momentum @xmath , radiated from a pair of given massless momenta @xmath
and @xmath . In order for the distribution to have the correct infrared
and collinear behavior, it should qualitatively be proportional to
@xmath . Furthermore, we want the density to be invariant under Lorentz
transformations and scaling of the momenta, keeping in mind that the
momenta are three out of possibly more in a CMF and that we have to
perform these transformations in the end, like in RAMBO . This motivates
us to define the basic antenna structure as

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

Here, @xmath is a function that serves to regularize the infrared and
collinear singularities, as well as to ensure normalization over the
whole space for @xmath : therefore, @xmath has to vanish sufficiently
fast for both @xmath and @xmath . To find out how @xmath could be
generated, we evaluate @xmath in the CMF of @xmath and @xmath . Writing

  -- -------- -- -------
     @xmath      (8.3)
  -- -------- -- -------

we have

  -- -------- -- -------
     @xmath      (8.4)
  -- -------- -- -------

where @xmath . The azimuthal angle of @xmath is denoted @xmath , so that
@xmath , with @xmath as in ( 7.50 ). We can write

  -- -------- -- -------
     @xmath      (8.5)
  -- -------- -- -------

where,

  -- -------- -- -------
     @xmath      (8.6)
  -- -------- -- -------

so that @xmath and @xmath . The integral over @xmath takes on the
particularly simple form

  -- -------- -- -------
     @xmath      (8.7)
  -- -------- -- -------

The antenna @xmath will therefore correspond to a unitary algorithm when
we let the density @xmath be normalized by

  -- -------- -- -------
     @xmath      (8.8)
  -- -------- -- -------

Note that the normalization of @xmath fixes the overall factor uniquely:
in particular the appearance of the numerator @xmath is forced upon us
by the unitarity requirement.

For @xmath we want to take, at this point, the simplest possible
function we can think of, that has a sufficiently regularizing behavior.
We introduce a positive non-zero number @xmath and take

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

The number @xmath gives a cut-off for the quotients @xmath and @xmath of
the scalar products of the momenta, and not for the scalar products
themselves. It is, however, possible to relate @xmath to the total
energy @xmath in the CMF and a cut-off @xmath on the invariant masses,
i.e., the requirement that

  -- -------- -- --------
     @xmath      (8.10)
  -- -------- -- --------

This can be done by choosing

  -- -------- -- --------
     @xmath      (8.11)
  -- -------- -- --------

With this choice, the invariant masses @xmath and @xmath are
regularized, but can still be smaller than @xmath so that the whole of
PS, cut by ( 8.10 ), is covered. The @xmath can be derived from physical
cuts @xmath on the transverse momenta and @xmath on the angles between
the outgoing momenta:

  -- -------- -- --------
     @xmath      (8.12)
  -- -------- -- --------

With this choice, PS with the physical cuts is covered by PS with the
cut of ( 8.10 ). To generate the physical PS, the method of crude Monte
Carlo (Section 7.3.2 ) can be used, i.e., if momenta of an event do not
satisfy the cuts, the whole event is rejected. We end this section with
the piece of the PS algorithm that corresponds to the basic @xmath :

###### Algorithm 8.2.1 (Basic Antenna)

1.  given @xmath , put @xmath and put @xmath ;

2.  generate two numbers @xmath , @xmath independently, each from the
    density @xmath , and @xmath uniformly in @xmath ;

3.  put @xmath , @xmath and @xmath ;

4.  put @xmath .

### 8.3 A complete QCD antenna

The straightforward way to generate @xmath momenta with the antenna
structured density is by repeated use of the basic antenna. Let us
denote

  -- -------- -- --------
     @xmath      (8.13)
  -- -------- -- --------

then

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- -- --------
     @xmath      (8.14)
  -- -------- -- --------

So if we have two momenta @xmath and @xmath , then we can easily
generate @xmath momenta @xmath with the antenna structure. Remember that
this differential PS volume is completely invariant under Lorentz
transformations and scaling transformations, so that it seems
self-evident to force the set of generated momenta in the CMF with a
given energy, using the same kind of transformation as in the case of
RAMBO . If the first two momenta are generated with density @xmath ,
then the UAF tells us that generated density @xmath satisfies

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (8.15)
  -- -------- -------- -- --------

If we apply the same manipulations as in the proof of the correctness of
RAMBO , we obtain the equation

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (8.16)
  -- -------- -------- -- --------

Now we choose @xmath such that @xmath and @xmath are generated
back-to-back in their CMF with total energy @xmath , i.e.,

  -- -------- -- --------
     @xmath      (8.17)
  -- -------- -- --------

If we evaluate the second line of Eq. ( 8.16 ) with this @xmath , we
arrive at

  -- -------- -- --------
     @xmath      (8.18)
  -- -------- -- --------

so that the generated density is given by

  -- -------- -- --------
     @xmath      (8.19)
  -- -------- -- --------

Note that, somewhat surprisingly, also the factor @xmath comes out,
thereby making the antenna even more symmetric. In fact, if the density
@xmath is taken instead of the one we just used, the calculation can
again be done exactly, with exactly the same result. The algorithm to
generate @xmath momenta with the above antenna structure is given by

###### Algorithm 8.3.1 (Qcd Antenna)

1.  generate massless momenta @xmath and @xmath ;

2.  generate @xmath momenta @xmath by the basic antennas @xmath ;

3.  compute @xmath , and the boost and scaling transforms that bring
    @xmath to @xmath ;

4.  for @xmath , boost and scale the @xmath accordingly, into the @xmath
    .

Usually, the event generator is used to generate cut PS. If a generated
event does not satisfy the physical cuts, it is rejected. In the
calculation of the weight coming with an event, the only contribution
coming from the functions @xmath is, therefore, their normalization. In
total, this gives a factor @xmath in the density.

### 8.4 Incoming momenta and symmetrization

The density given by the algorithm above, is not quite what we want.
First of all, we want to include the incoming momenta @xmath and @xmath
in the APS, so that the density becomes proportional to @xmath instead
of @xmath . Then we want the sum of all permutations of the momenta,
including the incoming ones.

#### 8.4.1 Generating incoming momenta

The incoming momenta can be generated after the antenna has been
generated. To show how, let us introduce the following “regularized”
scalar product:

  -- -------- -- --------
     @xmath      (8.20)
  -- -------- -- --------

where @xmath is a small positive number. This regularization is not
completely Lorentz invariant, but that does not matter here. Important
is that it is still invariant under rotations, as we shall see. Using
this regularization, we are able to generate a momentum @xmath with a
probability density

  -- -------- -- --------
     @xmath      (8.21)
  -- -------- -- --------

To show how, we calculate the normalization @xmath . Using the
Feynman-representation of @xmath , it is easy to see that

  -- -------- -- --------
     @xmath      (8.22)
  -- -------- -- --------

where @xmath . The integral over @xmath and @xmath can now be performed,
with the result that

  -- -------- -- --------
     @xmath      (8.23)
  -- -------- -- --------

where @xmath are the solutions for @xmath of the equation @xmath .
Further evaluation finally leads to

  -- -------- -- --------
     @xmath      (8.24)
  -- -------- -- --------

Notice that there is a smooth limit to the case in which @xmath and
@xmath are back-to-back:

  -- -------- -- --------
     @xmath      (8.25)
  -- -------- -- --------

The algorithm to generate @xmath can be derived by reading the
evaluations of the integrals backwards.

Because @xmath and @xmath are back-to-back, they can serve as the
incoming momenta. To fix them to @xmath and @xmath , the whole system of
momenta can be rotated. If we generate momenta with the density @xmath ,
use the first two momenta to generate the incoming momenta and rotate,
we get a density

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (8.26)
  -- -------- -------- -- --------

where we used the fact that the whole expression is invariant under
rotations, and that these are orthogonal transformations. The last line
of the previous expression can be evaluated further with the result that

  -- -------- -- --------
     @xmath      (8.27)
  -- -------- -- --------

The algorithm to generate the incoming momenta is given by

###### Algorithm 8.4.1 (Incoming Momenta)

1.  given a pair @xmath , calculate @xmath and @xmath ;

2.  generate @xmath in @xmath with density @xmath , and put @xmath ;

3.  generate @xmath uniformly in @xmath , @xmath in @xmath with density
    @xmath ;

4.  put @xmath and @xmath ;

5.  rotate all momenta with @xmath ;

6.  put @xmath and @xmath .

Notice that @xmath is invariant under the scaling @xmath with a constant
@xmath , so that scaling of @xmath and @xmath has no influence on the
density.

The pair @xmath with which @xmath is generated is free to choose because
we want to symmetrize in the end anyway. We should only choose it such,
that we get rid of the factor @xmath in the denominator of @xmath .

#### 8.4.2 Choosing the type of antenna with incoming momenta

A density which is the sum over permutations can be obtained by
generating random permutations, and returning the generated momenta with
permutated labels. This, however, only makes sense for the outgoing
momenta. The incoming momenta are fixed, and should be returned
separately from the outgoing momenta by the event generator. Therefore,
a part of the permutations has to be generated explicitly. There are two
kinds of terms in the sum: those in which @xmath appears, and those in
which it does not.

###### Case 1: antenna with @xmath.

To generate the first kind, we can choose a label @xmath at random with
weight @xmath where @xmath is the sum of all scalar products in the
antenna ² ² 2 Read @xmath when @xmath occurs in this section :

  -- -------- -- --------
     @xmath      (8.28)
  -- -------- -- --------

This is a proper weight, since all scalar products are positive. The
total density gets this extra factor then, so that @xmath cancels. The
denominator of the weight factor does not give a problem, because its
singular structure is much softer than the one of the antenna. The pair
@xmath can then be used to generate the incoming momenta, as shown
above. So in this case, a density @xmath is generated, where

  -- -------- -- --------
     @xmath      (8.29)
  -- -------- -- --------

###### Case 2: antenna without @xmath.

To generate the second kind, we can choose two non-equal labels @xmath
and @xmath with weight @xmath , where

  -- -------- -- --------
     @xmath      (8.30)
  -- -------- -- --------

Next, a pair @xmath of labels has to be chosen from the set of pairs

  -- -------- -- --------
     @xmath      (8.31)
  -- -------- -- --------

If this is done with weight @xmath , where

  -- -------- -- --------
     @xmath      (8.32)
  -- -------- -- --------

then the density @xmath is generated, where

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (8.33)
  -- -------- -------- -- --------

Before all this, we first have to choose between the two cases, and the
natural way to do this is with relative weights @xmath and @xmath , so
that the complete density is equal to

  -- -------- -- --------
     @xmath      (8.34)
  -- -------- -- --------

where the first sum is over all permutations of @xmath . One can, of
course, try to optimize the weights for the two cases using the adaptive
multichannel method (Section 7.3.5 ). The result of using the sum of the
two densities is that the factors @xmath in the numerator of @xmath and
@xmath in the numerator of @xmath cancel with the same factors in the
denominator of @xmath , so that we get exactly the pole structure we
want. The ‘unwanted’ singularities in @xmath and @xmath are much softer
than the ones remaining in @xmath , and cause no trouble. The algorithm
to generate the incoming momenta and the permutation is given by

###### Algorithm 8.4.2 (Choose Incoming Pole Structure)

1.  choose case 1 or 2 with relative weights @xmath and @xmath ;

2.  in case 1, choose @xmath with relative weight @xmath and put @xmath
    ;

3.  in case 2, choose @xmath with @xmath and relative weight @xmath ,
    and then
    choose @xmath from @xmath with relative weight @xmath ;

4.  use @xmath to generate the incoming momenta with Algorithm 8.4.1 ;

5.  generate a random permutation @xmath and put @xmath for all @xmath .

An algorithm to generate the random permutations can be found in [ 3 ] .
An efficient algorithm to calculate a sum over permutations can be found
in [ 37 ] .

### 8.5 Improvements

When doing calculations with this algorithm on a PS, cut such that
@xmath for all @xmath and some reasonable @xmath , we notice that a very
high percentage of the generated events does not pass the cuts. An
important reason why this happens is that the cuts, generated by the
choices of @xmath (Eq. ( 8.9 )) and @xmath (Eq. ( 8.11 )), are
implemented only on quotients of scalar products that appear explicitly
in the generation of the QCD-antenna:

  -- -------- -- --------
     @xmath      (8.35)
  -- -------- -- --------

The total number of these @xmath -variables is

  -- -------- -- --------
     @xmath      (8.36)
  -- -------- -- --------

and the cuts are implemented such that @xmath for @xmath . We show now
how these cuts can be implemented on all quotients

  -- -------- -- --------
     @xmath      (8.37)
  -- -------- -- --------

We define the @xmath -dimensional convex polytope

  -- -------- -- --------
     @xmath      (8.38)
  -- -------- -- --------

and replace the generation of the the @xmath -variables by the
following:

###### Algorithm 8.5.1 (Improvement)

1.  generate @xmath distributed uniformly in @xmath ;

2.  define @xmath and put,

      -- -------- -- --------
         @xmath      (8.39)
      -- -------- -- --------

    for all @xmath .

Because all the variables @xmath are distributed uniformly such that
@xmath , all quotients of ( 8.37 ) are distributed such that they are
between @xmath and @xmath . In terms of the variables @xmath , this
means that we generate the volume of @xmath , which is @xmath , instead
of the volume of @xmath , which is @xmath . In Section 8.8 , we give the
algorithm to generate variables distributed uniformly in @xmath . We
have to note here that this improvement only makes sense because the
algorithm to generate these variables is very efficient. The total
density changes such, that the function @xmath in Eq. ( 8.19 ) has to be
replaced by

  -- -------- -- --------
     @xmath      (8.40)
  -- -------- -- --------

where the variables @xmath are functions of the variables @xmath as
defined by @xmath . Because crude MC is used to restrict generated
events to cut PS, again only the normalization has to be calculated for
the weight of an event.

With this improvement, still a large number of events does not pass the
cuts. The situation with PS is depicted in Fig. 8.2 .

Phase space contains generated phase space which contains cut phase
space. The problem is that most events fall in the shaded area, which is
the piece of generated PS that is not contained in cut PS. To get a
higher percentage of accepted events, we use a random variable @xmath ,
instead of the fixed number @xmath , to generate the variables @xmath .
This means that the size of the generated PS becomes variable. If this
is done with a probability distribution such that @xmath can, in
principle, become equal to @xmath , then whole of cut phase space is
still covered. We suggest the following, tunable, density:

  -- -------- -- --------
     @xmath      (8.41)
  -- -------- -- --------

If @xmath , then @xmath is distributed uniformly in @xmath , and for
larger @xmath , the distribution peaks more and more towards @xmath .
Furthermore, the variable is easy to generate and the total generated
density can be calculated exactly: @xmath should be replaced by

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (8.42)
  -- -------- -------- -- --------

where @xmath is the maximum of the ratios of scalar products in ( 8.37
).

### 8.6 Results and conclusions

We compare SARGE with RAMBO in the integration of the SPHEL -integrand
for processes of the kind @xmath , which is given by

  -- -------- -- --------
     @xmath      (8.43)
  -- -------- -- --------

where @xmath and @xmath are the incoming momenta, and the first sum is
over all permutations of @xmath except the cyclic permutations. The
results are presented in Tab. 8.3 .

The calculations were done at a CM-energy @xmath with cuts @xmath on
each transverse momentum and @xmath on the angles between the momenta.
We present the results for @xmath , calculated with RAMBO and SARGE with
different values for @xmath (Eq. ( 8.42 )). The value of @xmath is the
estimate of the integral at an estimated error of @xmath for @xmath and
@xmath for @xmath . These numbers are only printed to show that
different results are compatible. Remember that they are not the whole
cross sections: flux factors, color factors, sums and averages over
helicities, and coupling constants are not included. The other data are
the number of generated events ( @xmath ), the number of accepted events
( @xmath ) that passed the cuts, the cpu-time consumed ( @xmath ), and
the cpu-time the calculation would have consumed if the exact matrix
element had been used ( @xmath ), both in hours. This final value is
estimated with the help of Tab. 8.2 and the formula

  -- -------- -- --------
     @xmath      (8.44)
  -- -------- -- --------

where @xmath and @xmath are the cpu-times it takes to evaluate the
squared matrix element once. Remember that the integrand only has to be
evaluated for accepted events. The calculations have been performed with
a single @xmath -MHz UltraSPARC-IIi processor.

The first conclusion we can draw is that SARGE outperforms RAMBO in
computing time for all processes. This is especially striking for lower
number of outgoing momenta, and this behavior has a simple explanation:
we kept the CM-energy and the cuts fixed, so that there is less energy
to distribute over the momenta if @xmath is larger, and the cuts become
relatively tighter. As a result, RAMBO gains on SARGE if @xmath becomes
larger. This effect would not appear if the energy, or the cuts, would
scale with @xmath like in Tab. 8.1 . Another indication for this effect
is the fact that the ratio @xmath for RAMBO , which estimates the ratio
of the volumes of cut PS and whole PS, decreases with @xmath .

Another conclusion that can be drawn is that SARGE performs better if
@xmath is larger. Notice that the limit of @xmath is equivalent with
dropping the improvement of the algorithm using the variable @xmath
(Eq. ( 8.42 )). Only if the integrand becomes too flat, as in the case
of @xmath with the energy and the cuts as given in the table, smaller
values are preferable. Then, too many events do not pass the cuts if
@xmath is large.

As an extra illustration of the performance of SARGE , we present in
Fig. 8.3 the evaluation of MC-integrals as function of the number of
accepted events. Depicted are the integral @xmath with the bounds on the
expected deviation coming from the estimated expected error, and the
relative error. Especially the graphs with the relative error are
illustrative, since they show that it converges to zero more smoothly
for SARGE then for RAMBO . Notice the spike for RAMBO around @xmath ,
where an event obviously hits a singularity.

### 8.7 Other pole structures

The APS of ( 8.1 ) is not the only pole structure occurring in the
squared amplitudes of QCD-processes; not even in purely gluonic
processes. For example, in the case of @xmath , also permutations of

  -- -------- -- --------
     @xmath      (8.45)
  -- -------- -- --------

occur [ 36 ] . If one is able to generate momenta with this density, it
can be included in the whole density with the use of the adaptive
multichannel technique. In the interpretation of the transition
amplitude as a sum of Feynman diagrams, this kind of pole structures
typically come from @xmath -channel diagrams, which are of the type

  -- -- --
        
  -- -- --

and where, for this case, @xmath and @xmath , so that @xmath . The
natural way to generate a density with this pole structure is by
generating @xmath with a density proportional to @xmath , a variable
@xmath that plays the role of @xmath , construct with this and some
generated angles the momenta @xmath , and then split new momenta from
each of these. For @xmath , only two momenta have to split off each
@xmath , and there is a reasonable simple algorithm to generate these.

We shall now just present the algorithm that generates the density (
8.56 ), and then show its correctness using the UAF. If we mention the
generation of some random variable @xmath ‘with a density @xmath ’ in
the following, we mean a density that is proportional to @xmath , and we
shall not always write down the normalization explicitly. Furthermore,
@xmath denotes the square of the CM-energy and @xmath the usual
Mandelstam variable

  -- -------- -- --------
     @xmath      (8.46)
  -- -------- -- --------

Of course, a cut has to be implemented in order to generate momenta
following ( 8.45 ), and we shall be able to put @xmath for the scalar
products occurring in the denominator, where @xmath only has to be
larger than zero. To generate the momenta with density ( 8.45 ), one
should

###### Algorithm 8.7.1 (T-Channel)

1.  generate @xmath and @xmath between @xmath and @xmath with density
    @xmath and @xmath ;

2.  generate @xmath between @xmath with density @xmath ;

3.  put @xmath and generate @xmath uniformly in @xmath ;

4.  put @xmath and @xmath ;

5.  for @xmath , generate @xmath with density @xmath and @xmath
    uniformly in @xmath , and put @xmath ;

6.  for @xmath , rotate @xmath to the CMF of @xmath , then boost it to
    the CMF of @xmath to obtain @xmath , and put @xmath ;

As a final step, the incoming momenta can be put to @xmath and @xmath .
The variables @xmath and @xmath can easily be obtained by inversion
(Section 7.3.1 ). The variable @xmath can best be obtained by generating
@xmath with the help of the rejection method (Section 7.3.3 ). In the
UAF, the steps of the algorithm read as follows. Denoting

  -- -------- -- --------
     @xmath      (8.47)
  -- -------- -- --------

and

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (8.48)
  -- -------- -------- -- --------

we have

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The various assignments imply the following identities. First of all, we
have

  -- -------- -- --------
     @xmath      (8.49)
  -- -------- -- --------

Using that @xmath we find

  -- -------- -- --------
     @xmath      (8.50)
  -- -------- -- --------

and the same for @xmath , so that

  -- -------- -- --------
     @xmath      (8.51)
  -- -------- -- --------

Denote @xmath , so that @xmath . Because @xmath , we find that

  -- -------- -- --------
     @xmath      (8.52)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (8.53)
  -- -------- -- --------

We can conclude so far that the algorithm generates the correct pole
structure. For the further evaluation of the integrals one can forget
about the factors @xmath , @xmath , @xmath and @xmath in the
denominators. Using that

  -- -------- -- --------
     @xmath      (8.54)
  -- -------- -- --------

and replacing step 4 by

  -- -------- -- --------
     @xmath      (8.55)
  -- -------- -- --------

the integrals can easily be performed backwards, i.e., in the order
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath ,
@xmath , @xmath . The density finally is

  -- -------- -------- -- --------
     @xmath   @xmath      
                          (8.56)
  -- -------- -------- -- --------

where @xmath and @xmath .

### 8.8 Generating a uniform distribution inside a polytope

We consider the @xmath -dimensional convex polytope @xmath defined in (
8.38 ). The task is to generate @xmath -dimensional points uniformly
inside @xmath . A straightforward way is to generate points inside the
hypercube @xmath and implement the other conditions by rejection, with
an efficiency given by @xmath , where @xmath is the volume of the
polytope. This may, however, become slow if @xmath does not increase
fast with @xmath . Let us, therefore, compute @xmath . We distinguish
positive and negative @xmath values. Define

  -- -------- -- --------
     @xmath      (8.57)
  -- -------- -- --------

We then have

  -- -------- -- --------
     @xmath      (8.58)
  -- -------- -- --------

In the calculation of @xmath we notice that the only nontrivial
constraints are of the type @xmath , with @xmath and @xmath . Writing
@xmath for @xmath , we therefore have

  -- -------- -- --------
     @xmath      (8.59)
  -- -------- -- --------

Relabeling such that @xmath and @xmath then leads us to

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (8.60)
  -- -------- -------- -- --------

so that we find

  -- -------- -- --------
     @xmath      (8.61)
  -- -------- -- --------

Accordingly, the rejection algorithm will quickly become inefficient,
below 1% for @xmath . The above calculation actually allows us to
construct an optimal algorithm by working backwards. In the following
each @xmath stands for a new call to the random number source.

###### Algorithm 8.8.1 (Polytope)

1.  choose an integer @xmath . Since @xmath , it should be chosen
    uniformly in @xmath , so

      -- -------- --
         @xmath   
      -- -------- --

2.  if @xmath we simply have

      -- -------- --
         @xmath   
      -- -------- --

    If @xmath we use

      -- -------- --
         @xmath   
      -- -------- --

3.  for @xmath , generate @xmath in @xmath according to the distribution
    @xmath . An efficient algorithm to do this is Cheng’s rejection
    algorithm BA for beta random variates (cf. [ 6 ] ) ³ ³ 3 There is an
    error on page 438 of [ 6 ] , where “ @xmath ” should be replaced by
    “ @xmath ”. , but also the following works:

      -- -------- --
         @xmath   
      -- -------- --

4.  generate @xmath in @xmath according to the distribution @xmath . The
    algorithm to do this is

      -- -------- --
         @xmath   
      -- -------- --

5.  generate the @xmath uniformly in @xmath and flip sign:

      -- -------- --
         @xmath   
      -- -------- --

6.  generate the @xmath uniformly in @xmath :

      -- -------- --
         @xmath   
      -- -------- --

7.  Finally, perform a random permutation of the whole set of @xmath
    values.

#### 8.8.1 Computational complexity

The number usage @xmath , that is, the expected number of calls to the
random number source @xmath per event can be derived easily. In the
first place, 1 number is used to get @xmath for every event. In a
fraction @xmath of the cases, only @xmath calls are made. In the
remaining cases, there are @xmath calls to get @xmath , and 1 call for
all the other @xmath values. Finally, the simplest permutation algorithm
calls @xmath times [ 3 ] . The expected number of calls is therefore

  -- -------- -- --------
     @xmath      (8.62)
  -- -------- -- --------

For large @xmath this comes to about @xmath calls per event. Using a
more sophisticated permutation algorithm would use at least 1 call,
giving

  -- -------- -- --------
     @xmath      (8.63)
  -- -------- -- --------

We observed that Cheng’s rejection algorithm to obtain @xmath uses about
2 calls per event. Denoting this number by @xmath the expected number of
calls becomes

  -- -------- -- --------
     @xmath      (8.64)
  -- -------- -- --------

for the simple permutation algorithm, while the more sophisticated one
would yield

  -- -------- -- --------
     @xmath      (8.65)
  -- -------- -- --------

We see that in all these cases the algorithm is uniformly efficient in
the sense that the needed number of calls is simply proportional to the
problem’s complexity @xmath , as @xmath becomes large. An ideal
algorithm would of course still need @xmath calls, while the
straightforward rejection algorithm rather has @xmath expected calls per
event.

In the testing of algorithms such as this one, it is useful to study
expectation values of, and correlations between, the various @xmath .
Inserting either @xmath or @xmath in the integral expression for @xmath
, we found after some algebra the following expectation values:

  -- -------- -- --------
     @xmath      (8.66)
  -- -------- -- --------

so that the correlation coefficient between two different @xmath ’s is
precisely 1/2 in all dimensions! This somewhat surprising fact allows
for a simple but powerful check on the correctness of the algorithm’s
implementation.

As an extra illustration of the efficiency, we present in Tab. 8.4 the
cpu-time ( @xmath ) needed to generate @xmath points in an @xmath
-dimensional polytope, both with the algorithm just presented ( OURALG )
and the rejection method ( REJECT ). In the latter, we just

1.  put @xmath for @xmath ;

2.  reject @xmath if @xmath for any @xmath and @xmath .

The computations were done using a single @xmath -MHz UltraSPARC-IIi
processor, and the random number generator used was RANLUX on level 3.

For @xmath and @xmath , the rejection method is quicker, but from @xmath
on, the cpu-time clearly grows linearly for OURALG and exponentially for
the rejection method.

#### 8.8.2 Extension

Let us, finally, comment on one possible extension of this algorithm.
Suppose that the points @xmath are distributed on the polytope @xmath ,
but with an additional (unnormalized) density given by

  -- -------- -- --------
     @xmath      (8.67)
  -- -------- -- --------

so that the density is suppressed near the edges. It is then still
possible to compute @xmath for this new density. Writing @xmath and
@xmath , we have

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (8.68)
  -- -------- -------- -- --------

where we used @xmath . Therefore, a uniformly efficient algorithm can be
constructed in this case as well, along the following lines. Using the
@xmath , the relative weights for each @xmath can be determined. Then
@xmath is generated as a @xmath -distribution. The generation of the
other @xmath ’s involves only manipulations with sine and arcsine
functions. Note that, for large @xmath , the weighted volume @xmath is

  -- -------- -- --------
     @xmath      (8.69)
  -- -------- -- --------

so that a straightforward rejection algorithm would have number usage

  -- -------- -- --------
     @xmath      (8.70)
  -- -------- -- --------

and a correspondingly decreasing efficiency.

] ] ]