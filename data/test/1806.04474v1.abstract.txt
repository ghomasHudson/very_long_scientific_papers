###### The Node Repair Problem

In the distributed-storage setting, data pertaining to a file is stored
across spatially-distributed nodes (or storage units) that are assumed
to fail independently. The explosion in amount of data generated and
stored has caused renewed interest in erasure codes as these offer the
same level of reliability (recovery from data loss) as replication, with
significantly smaller storage overhead. For example, a provision for the
use of erasure codes now exists in the latest version of the popular
Hadoop Distributed File System, Hadoop 3.0. It is typically the case
that code symbols of a codeword in an erasure code is distributed across
nodes. This ‘Big-Data’ setting also places a new and additional
requirement on the erasure code, namely that the code must enable the
efficient recovery of a single erased code symbol. An erased code symbol
here corresponds to a failed node and recovery from single-symbol
erasure is termed as node repair. Node failure is a common occurrence in
a large data center and the ability of an erasure code to efficiently
handle node repair is a third important consideration in the selection
of an erasure code. Node failure is a generic term used to describe not
just the physical failure of a node, but also its non-availability for
reasons such as being down for maintenance or simply being busy serving
other, simultaneous demands on its contents. Parameters relevant to node
repair are the amount of data that needs to be downloaded from other
surviving (helper) nodes to the replacement of the failed node, termed
the repair bandwidth and the number of helper nodes contacted, termed
the repair degree . Node repair is said to be efficient if either repair
bandwidth or repair degree is less.
