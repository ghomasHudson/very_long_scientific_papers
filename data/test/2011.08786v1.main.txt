##### Table of Contents

-    Abstract
-    Declaration
-    Acknowledgements
-    Dissemination
-    1 Introduction
    -    1.1 General Relativity
    -    1.2 The Standard Model of Cosmology
    -    1.3 The Standard Model: Problems and Challenges
        -    1.3.1 The Validity of the Cosmological Principle
        -    1.3.2 The Validity of GR on Different Scales
        -    1.3.3 Issues Related to Dark Matter
        -    1.3.4 Issues Related to Dark Energy
        -    1.3.5 Tensions in the Cosmological Parameters
    -    1.4 Modified Gravity: Tests and Current Developments
        -    1.4.1 Laboratory Tests
        -    1.4.2 Solar System Tests
        -    1.4.3 Gravitational Wave Tests
        -    1.4.4 Galaxy Scale Tests
        -    1.4.5 Galaxy Cluster Tests
        -    1.4.6 Large Scale Structure Tests
-    2 Galaxy Clusters
    -    2.1 The Structure and Basic Properties of Galaxy Clusters
    -    2.2 Intracluster Medium and X-ray Observations
    -    2.3 Methods To Estimate Cluster Masses
        -    2.3.1 Hydrostatic Equilibrium
        -    2.3.2 Galaxy kinematics
    -    2.4 Weak Lensing in Galaxy Clusters
        -    2.4.1 The Basics of Gravitational Lensing
        -    2.4.2 Weak Lensing by NFW Halos
-    3 Modified Gravity on Galaxy Cluster Scales
    -    3.1 Scalar-Tensor Gravity with Chameleon Screening
        -    3.1.1 The Action
        -    3.1.2 Properties of the Effective Potential
    -    3.2 @xmath Gravity
    -    3.3 Testing Modified Gravity on Galaxy Cluster Scales
        -    3.3.1 Non-Thermal Pressure and the Modified Hydrostatic
            Equilibrium Equation
        -    3.3.2 Weak Lensing in Chameleon Gravity
        -    3.3.3 X-ray Surface Brightness
        -    3.3.4 X-ray and Weak Lensing Datasets
        -    3.3.5 MCMC Fitting
        -    3.3.6 Results
    -    3.4 Implications for the Gravitational Slip Parameter
        -    3.4.1 Gravitational Slip in Galaxy Clusters
        -    3.4.2 Constraining the Deviations from the Hydrostatic
            Equilibrium and the Gravitational Slip Parameter
-    4 Testing Emergent Gravity on Galaxy Cluster Scales
    -    4.1 Motivations for Emergent Gravity
    -    4.2 Verlinde’s Emergent Gravity
        -    4.2.1 The Predictions of the Model
        -    4.2.2 The Main Assumptions
    -    4.3 Testing Emergent Gravity
        -    4.3.1 Testing Emergent Gravity with the Coma Cluster
        -    4.3.2 Testing Emergent Gravity with Stacked Galaxy Clusters
    -    4.4 The Current State of the Model
    -    4.5 Covariant Emergent Gravity
-    5 A Brief Introduction to Machine Learning
    -    5.1 Machine Learning and Artificial Intelligence
    -    5.2 Machine Learning in Cosmology and Astrophysics
    -    5.3 Decision Trees and Gradient Boosting
    -    5.4 Artificial Neural Networks
    -    5.5 Convolutional Neural Networks
    -    5.6 Generative Adversarial Networks
-    6 Using GANs for Emulating Cosmological Simulation Data
    -    6.1 The Need for Cosmological Emulators
    -    6.2 DCGAN Architecture for Emulating Cosmological Simulation
        Data
    -    6.3 Latent Space Interpolation
    -    6.4 Riemannian Geometry of GANs
    -    6.5 Datasets and the Training Procedure
        -    6.5.1 Weak Lensing Convergence Map Data
        -    6.5.2 Cosmic Web Slice Data
        -    6.5.3 Dark Matter, Gas and Internal Energy Data
        -    6.5.4 The Training Procedure
    -    6.6 Diagnostics
    -    6.7 Results
        -    6.7.1 Weak Lensing Map Results
        -    6.7.2 Weak Lensing Maps of Multiple Cosmologies
        -    6.7.3 Cosmic Web for Multiple Redshifts
        -    6.7.4 Cosmic Web for Multiple Cosmologies and Modified
            Gravity Models
        -    6.7.5 Dark Matter, Gas and Internal Energy Results
        -    6.7.6 Latent Space Interpolation Results
    -    6.8 Analysis and Conclusions
-    7 Conclusions and Future Work
-    A
    -    A.1 Availability of Data and Codes
    -    A.2 Samples of the GAN-produced Data
    -    A.3 MCMC Contours

\pdfbookmark

List of Tablestables

###### List of Tables

-    1.1 Base- @xmath CDM cosmological parameters from Planck 2018
-    3.1 Comparison of the best-fit parameters in Terukina et al. [ 2014
    ] and Wilcox [ 2016 ] .
-    3.2 Comparison of the best-fit parameters in Wilcox et al. [ 2015 ]
    and this work
-    3.3 Modified gravity constraints from previous works in the
    literature
-    4.1 The typical values of the @xmath parameter
-    4.2 The best-fit parameters for the Coma Cluster test of emergent
    gravity
-    4.3 Goodness of fit statistics for the Coma Cluster test of
    emergent gravity
-    4.4 Best-fit parameters for the EG and the standard model fits
    using the cluster stack data
-    4.5 Goodness of fit statistics for the cluster stack results
-    6.1 Architecture of the generator neural network
-    6.2 Architecture of the discriminator neural network
-    6.3 The used XGBoost settings

\pdfbookmark

List of Figuresfigures

###### List of Figures

-    1.1 Observational tests of modified gravity
-    1.2 Recent cosmological constraints from multiple probes in the
    Dark Energy Survey
-    1.3 The @xmath tension
-    1.4 Modified gravity models
-    1.5 Summary of laboratory tests of gravity
-    1.6 Summary of modified gravity models after GW170817
-    1.7 Summary of the observational constraints on @xmath gravity
-    2.1 The mass distribution in the Bullet Cluster
-    2.2 Sunyaev-Zel’dovich effect
-    2.3 The geometry of gravitational lensing
-    2.4 The weak lensing effects on an image of a background galaxy
-    3.1 The effects of the local density on the shape of the chameleon
    potential
-    3.2 Analysis of the cluster separation
-    3.3 X-ray and tangential shear profile dataset from Wilcox et al. [
    2015 ]
-    3.4 A sample of sources misclassified as galaxy clusters in the
    original XCS-CFHTLenS dataset in Wilcox et al. [ 2015 ]
-    3.5 A comparison of the original and the updated XCS-CFHTLenS
    datasets
-    3.6 The stacked X-ray surface brightness and tangential shear
    profiles for the updated 77 cluster dataset
-    3.7 X-ray surface brightness from two types of simulations from
    Wilcox [ 2016 ]
-    3.8 A comparison of the modified gravity constraints from the Coma
    Cluster [Terukina et al. , 2014 ] , the 58 galaxy cluster stack from
    [Wilcox et al. , 2015 ] and the 77 cluster stack described in this
    work
-    3.9 A comparison of the modified gravity constraints from the
    @xmath CDM and @xmath simulations with 103 and 99 clusters stacked
    correspondingly and the 77 cluster stack described in this work
-    3.10 Checking the validity of the assumption of hydrostatic
    equilibrium
-    3.11 The gravitational slip parameter calculated using the X-ray
    and the weak lensing data from Wilcox et al. [ 2015 ] .
-    3.12 Estimate of the gravitational slip constraints using @xmath
    @xmath and @xmath @xmath stacked galaxy clusters.
-    4.1 Emergent gravity effects
-    4.2 The Coma Cluster
-    4.3 The Coma Cluster galaxy mass distribution
-    4.4 The results for the test of emergent gravity using the Coma
    Cluster
-    (a) Gas temperature fit in EG and GR
-    (b) Weak-lensing fit in EG and GR
-    (c) Total mass profiles in the two models
-    (d) Ratio of the mass profiles
-    4.5 An illustration of the effects of stacking galaxy clusters
-    4.6 The galaxy mass distribution for the cluster stack
-    4.7 The results of testing emergent gravity with stacked galaxy
    clusters ( @xmath keV bin)
-    (a) X-ray surface brightness fits
-    (b) Weak lensing fits
-    (c) Mass distributions in the two models
-    (d) Ratio of the mass distributions
-    4.8 The results of testing emergent gravity with stacked galaxy
    clusters ( @xmath keV bin)
-    (a) X-ray surface brightness fits
-    (b) Weak lensing fits
-    (c) Mass distributions in the two models
-    (d) Ratio of the mass distributions
-    4.9 Analysis of the various systematics
-    (a) Results with different @xmath
-    (b) Results with modified @xmath
-    5.1 Gradient boosting training procedure
-    5.2 An example of a decision tree
-    5.3 Biological and artificial neurons
-    5.4 A multilayer perceptron
-    5.5 RGB images and the procedure of convolution
-    5.6 Edge extraction from an image
-    5.7 GAN training procedure
-    5.8 GAN training procedure for MNIST images
-    6.1 The pipeline of training a GAN on cosmic web slice data
-    6.2 Illustration of the linear latent space interpolation procedure
-    6.3 Riemannian geometry of generative adversarial networks
-    6.4 Samples of the Illustris dataset
-    (a) DM overdensity field
-    (b) Gas overdensity field
-    (c) Internal energy field
-    (d) All components combined
-    6.5 The effects of Gaussian smoothing on Minkowski functionals
-    6.6 The power spectrum and the pixel intensity histogram for the
    weak lensing convergence maps ( @xmath CDM)
-    (a) Power spectrum
-    (b) Pixel intensity histogram
-    6.7 Minkowski functional analysis for the weak lensing convergence
    maps ( @xmath CDM)
-    6.8 The power spectrum and the pixel intensity histogram for the
    weak lensing convergence maps ( @xmath )
-    6.9 Minkowski functional analysis for the weak lensing convergence
    maps ( @xmath )
-    6.10 The power spectra and the overdensity histogram for the cosmic
    web slices with different redshifts: @xmath
-    6.11 Minkowski functional analysis for the cosmic web slices with
    different redshifts: @xmath
-    6.12 The power spectra and the overdensity histogram for the cosmic
    web slices with @xmath
-    6.13 Minkowski functional analysis for the cosmic web slices with
    @xmath
-    6.14 The power spectra and the overdensity histogram for the cosmic
    web slices with @xmath
-    6.15 Minkowski functional analysis for the cosmic web slices with
    @xmath
-    6.16 Diagnostics for the dark matter, gas and internal energy
    results
-    6.17 Minkowski functional analysis for the dark matter, gas and
    internal energy results
-    6.18 Latent space interpolation results
-    (a) CW slice redshift interpolation
-    (b) WL @xmath interpolation
-    6.19 Samples of data produced by the latent space interpolation
    procedure
-    A.1 A comparison of 4 randomly selected weak lensing convergence
    maps.
-    A.2 A comparison of 4 randomly selected cosmic web slices.
-    A.3 The MCMC contours from Terukina et al. [ 2014 ]
-    A.4 The MCMC contours from Wilcox et al. [ 2015 ]
-    A.5 The MCMC contours of the best-fit parameters determined in this
    work

## Declaration

Whilst registered as a candidate for the above degree, I have not been
registered for any other research award. The results and conclusions
embodied in this thesis are the work of the named candidate and have not
been submitted for any other academic award.

Word count: 41,724 words.

## Acknowledgements

I would like to use this opportunity to express my sincere gratitude to
the people that made this thesis possible. Firstly, I would like to
thank my lovely family for their never-ending love and support. I am
grateful to my Dad, who has sparked in me the love for science and
education. I am grateful to my Mom for always being there for me. And I
am grateful to my brother for being someone I can look up to.

I would also like to thank my dear supervisors Prof. Bob Nichol, David
Bacon and Kazuya Koyama for their infinite patience, kindness and
keeping my passion for learning new things aflame. Thanks for teaching
me so much about myself and the Universe. In turn, I would like to
extend my sincere gratitude to all my teachers and mentors (from primary
school to university), who have taught me so much. Likewise I am
grateful to the people who have influenced me in many different ways
throughout the years including: Prof. Andrew Liddle, Bruce Bassett and
Kathy Romer.

Finally, my gratitude goes to my friends and colleagues in the ICG.
Thanks to my academic brothers and sisters – Michael, Mike, Maria, Sam
Y., Sam L. and Natalie, for sharing the academic path and for the fun
times and the endless nights in the pubs of Portsmouth. I am also
sincerely grateful to everyone in the ICG for making the institute a
warm and welcoming place to work and study in.

## Dissemination

Chapter 3 contains work in preparation for publication. Chapters 4 and 5
contain original work described in the following publications:
A. Tamosiunas, H. A. Winther, K. Koyama, D. J. Bacon, R. C. Nichol, B.
Mawdsley. Towards Universal Cosmological Emulators with Generative
Adversarial Networks . April 2020. Submitted for publication in the
Journal of Computational Astrophysics and Cosmology. arXiv:
arXiv:2004.10223 [astro-ph.CO] .
A. Tamosiunas. D. J. Bacon, K. Koyama, R. C. Nichol. Testing Emergent
Gravity on Galaxy Cluster Scales . January 2019. Published in JCAP. DOI:
10.1088/1475-7516/2019/05/053 . arXiv: arXiv:1901.05505 [astro-ph.CO]

## Notation and Abbreviations

## Chapter 1 Introduction

The science of cosmology dates back to ancient times. When defined in a
broad sense cosmology is inseparable from the earliest inquiries into
how nature works by ancient cultures as described in the historical
records [Kragh, 2017 ] . Initially such inquiries were tightly
intertwined with religion and superstition. The principles of what is
now known as modern cosmology were arguably first combined into a
consistent framework in ancient Greece. As an example, the great
philosopher Plato in his works Timaeus and Republic introduced the
two-sphere model, which placed Earth at the centre of the Universe,
surrounded by a celestial sphere holding the stars and other heavenly
bodies [Evans, 1998 ] . Another school of philosophy, the Pythagoreans,
sought to build models of the celestial motion based on known
mathematical principles. A key stepping stone to mention here is that
the Pythagoreans treated astronomy as one of the key mathematical arts
(along with arithmetic, geometry and music). Such a high regard for
natural philosophy eventually led to the formulation of the first known
heliocentric model of the solar system by the mathematician and
astronomer Aristarchus of Samos [Heath, 1991 ] . The model of
Aristarchus placed the Sun at the centre of the known Universe with the
Earth and the other known planets orbiting around it. Another visionary
insight by Aristarchus was that the stars were in fact objects analogous
to the Sun at such great distances from Earth that no parallax was
observed [Wright, 1995 ] . The original texts describing Aristarchus’
models were later lost for over a millennium with only references in
contemporary texts surviving.

Another visionary work that came from this historical period is The Sand
Reckoner (Gr: Ψαμμίτης) by Archimedes [Hirshfeld, 2009 ] . In this work
Archimedes sets out to calculate the number of grains of sand that fit
into the Universe. In order to do this, Archimedes had to estimate the
size of he Universe based on Aristarchus’ model. In addition, Archimedes
had to invent new mathematical notation for dealing with large numbers.
The obtained results estimated the diameter of the Universe to be no
more than @xmath stadia or roughly 2 light-years in contemporary units.
Equivalently, a Universe of such size could fit @xmath grains of sand.
The importance of this work lies in the fact that it is likely the first
known systematic estimate of the size of the Universe based on
mathematics and the known principles of astronomy.

Major leaps in understanding of cosmology and astronomy were made during
the Renaissance. The works of Nicolaus Copernicus reintroduced the
heliocentric model from relative obscurity due to the original works of
Aristarchus being mostly unknown. The works of Copernicus, when combined
with astronomical observations, allowed predictions of planetary motions
and orbital periods as well as experimental comparison of the two
competing theories of geocentrism and heliocentrism. The observational
tradition of Copernicus was carried on by later astronomers including
Tycho Brahe and Johannes Kepler, the work of whom led to the three laws
of Kepler. Another key discovery from this historical period came from
Galileo, who is traditionally credited as one of the discoverers of the
telescope. The mentioned theoretical and observational efforts
culminated in the work of Newton and in particular Newton’s law of
universal gravitation, which formed the core of our understanding of
gravity for over two centuries after its discovery [Taton et al. , 2003
, Curley, 2012 ] .

Arguably the most important theoretical development in the modern era of
cosmology came with Einstein’s theory of general relativity (GR)
[Einstein, 1916 ] . GR revolutionized our understanding of gravity by
promoting space and time from a mere stage in which events take place to
a 4-dimensional dynamical canvas that interacts with matter and energy
in intricate ways ( spacetime tells matter how to move; matter tells
spacetime how to curve according to John A. Wheeler).

After more than a century GR has been extensively confirmed
observationally and now forms the basis of our understanding of how
gravity behaves in a wide range of systems starting with the solar
system and galaxies and ending with the Universe as a whole. For this
reason GR is also the theoretical basis behind the currently most
complete model of standard cosmology – the @xmath CDM model. The @xmath
CDM model has been extremely successful in explaining structure
formation in the Universe along with the anisotropies of the cosmic
microwave background (CMB). However, as the name implies, in order to
make accurate predictions, the theory requires two extra components –
the cosmological constant ( @xmath ) or some other form of dark energy
along with non-luminous non-baryonic matter (CDM). Dark matter, in
particular, was first inferred to exist by Fritz Zwicky by studying the
mass distribution in the Coma Cluster in 1933 [Andernach and Zwicky,
2017 ] . Now we know that some form of dark matter is crucial for
explaining galaxy rotation curves and large scale structure formation in
general. Another key discovery came in 1998, when the Supernova
Cosmology Project and the High-Z Supernova Search Team found evidence
for the accelerating expansion of the Universe using data from type Ia
supernovae [Riess et al. , 1998 ] . To explain the accelerating
expansion, some form of dark energy is required.

Today we know a lot about dark energy and dark matter, but the physical
origin of them still eludes astronomers and particle physicists. This is
one of the key motivations for developing models that modify GR. Since
the publication of the original theory in 1915 a plethora of modified
gravity models have been proposed. These models can be generally
classified based on the type of modification they introduce to the
original GR framework. Namely, modified models can introduce extra
scalar, vector and tensor fields, extra spatial dimensions and higher
order derivatives. In addition, certain assumptions that exist in the
original model can be relaxed (e.g. non-local theories). These
approaches form a complex family of modified gravity models, each of
which comes with unique observational signatures. A need to discriminate
between the different families of modifications of gravity has led to a
variety of observational tests on scales ranging from the laboratory, to
the solar system and all the way to cosmological scales [Koyama, 2016 ]
. In this thesis special emphasis will be put on galaxy cluster-related
methods for testing for such modification of gravity. In addition,
various machine learning techniques will be explored as tools for
emulating modified gravity simulations. The rest of chapter 1 will
introduce the relevant basic concepts in GR and cosmology. In addition,
a brief overview of the current theoretical and observational
developments in the field of modified gravity will be given.

### 1.1 General Relativity

Einstein’s theory of general relativity published in 1915 forms the
basis of the modern understanding of gravity. One of the key postulates
of the theory is the equivalence principle, which equates the
gravitational and inertial masses of a given body. This at first glance
inconsequential idea, with its roots dating back to the observations of
Galileo, has led Einstein on a path towards finding deep connections
between gravity and the geometry of spacetime. Namely, by demonstrating
the equivalence of the forces felt by a body in an accelerated frame and
those felt in a gravitational field, Einstein was able to generalize the
tools and techniques first developed for his theory of special
relativity [Wald, 2010 ] .

GR relates the energy-momentum contents of a given gravitational system
to the geometric effects on spacetime. Hence, if the mass/energy
distribution in a given system is known, accurate predictions can be
made about the resulting dynamics of the system. A key equation in this
regard is the Einstein-Hilbert action. Describing GR in terms of an
action has a number of advantages. In particular, it allows to describe
the theory following a similar formalism as in the other classical field
theories (e.g. Maxwell theory). Varying the action allows a
straightforward way for deriving the field equations. In addition, the
effects of other fields (e.g. matter fields) can be easily added to the
total action. The Einstein-Hilbert action is given by:

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath is the spacetime metric, @xmath is the determinant of the
metric, @xmath , @xmath is the gravitational constant, @xmath is the
Ricci scalar, @xmath is the cosmological constant and @xmath is the
matter action governed by the matter field @xmath . The Ricci scalar can
be obtained by contracting the indices of the Ricci tensor @xmath ,
which, in turn, can be derived from the Riemann curvature tensor:

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

The Riemann curvature tensor quantifies the amount of curvature in the
4-D spacetime manifold. Here @xmath refers to the Christoffel symbols,
given by:

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

By varying the Einstein-Hilbert action, w.r.t. the spacetime metric the
Einstein field equations are obtained:

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

where @xmath is the Einstein tensor. @xmath refers to the
energy-momentum tensor, defined by:

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

In the case of a perfect fluid with density @xmath , pressure @xmath and
the four-velocity @xmath :

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

In this framework, the dynamics of bodies can be deduced from the
geodesic equation, which generalizes the notion of a straight line to
curved spaces:

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

Hence, if the metric @xmath describing a given gravitational system is
known, one can solve eq. 1.7 to obtain the trajectory of a body in terms
of the four spacetime coordinates and some affine parameter @xmath .

The key significance of GR in the context of cosmology comes from its
ability to relate mass/energy distributions to the corresponding effects
on spacetime and ultimately the resulting motion of bodies. This makes
GR one of the key foundations of the standard model of cosmology.

### 1.2 The Standard Model of Cosmology

The @xmath CDM model is currently the most well-tested framework capable
of describing a wide range of phenomena, such as the anisotropies of the
CMB and the underlying large-scale structure formation. The standard
model is based on three key assumptions [Peebles, 1993 , Li and Koyama,
2019a ] :

1.  The cosmological principle. This principle refers to the matter
    distribution, on large scales, being homogeneous and isotropic.

2.  The known laws of gravity are universal. Or, more specifically, in
    the context of cosmology, gravity is described by GR everywhere in
    the Universe.

3.  The matter-energy budget of the Universe contains a significant
    contribution from some form of non-luminous, non-baryonic matter
    (dark matter) along with the usual baryonic matter and radiation.

The first assumption can be expressed mathematically by choosing the
most general metric fulfilling the needed conditions of isotropy and
homogeneity – the Friedman-Lemaître-Robertson-Walker (FLRW) metric
[Friedmann, 1922 , Lemaître, 1931 ] . The FLRW metric is obtained by
starting with the most general metric in 4-D and constraining the form
of the metric to account for isotropy, homogeneity and the different
types of the spatial curvature of the Universe. This leads to the
following form:

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

where @xmath is proper time, @xmath are the usual spherical coordinates,
@xmath is the scale factor and @xmath is a constant related to spatial
curvature. The numerical values of @xmath (in the units of @xmath )
refer to an open, flat and closed spatial curvature of the Universe
correspondingly.

Applying the FLRW metric to the Einstein field equations results in the
two equations that govern the evolution of the scale factor @xmath known
as the Friedmann equations:

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

where @xmath and @xmath correspond to the time derivatives of the scale
factor, @xmath is the density, @xmath is the pressure and @xmath is the
cosmological constant. The Friedmann equations are profound as they
describe the expansion of space and relate it to the matter content.
Hence, assuming that the underlying density distribution can be
determined, one can deduce the future evolution of the Universe.

The @xmath CDM model is based on 6 main parameters that are needed to
fit the key observational datasets such as the CMB anisotropies, large
scale galaxy clustering and the redshift/brightness relation for
supernovae. These parameters are the baryon density parameter @xmath
(with @xmath as the dimensionless Hubble parameter), the dark matter
density parameter @xmath , the angular scale of the sound horizon at the
last scattering @xmath , the scalar spectral index @xmath , the initial
super-horizon curvature fluctuation amplitude (at @xmath @xmath ) @xmath
and the reionization optical depth @xmath .

Physically, the @xmath and the @xmath parameters quantify the amount of
baryonic and dark matter relative to the critical density. The @xmath
parameter quantifies the ratio between the sound horizon (i.e. the
distance sound waves could have traveled in the time before
recombination) and the distance to the surface of last scattering. The
spectral index @xmath quantifies the scale dependence of the primordial
fluctuations (with @xmath referring to scale invariant case). Finally,
in the context of the CMB observations, the optical depth to
reionization, @xmath , is a unitless quantity which provides a measure
of the line-of-sight free-electron opacity to CMB radiation. This is the
case as Thomson scattering of the CMB photons by the free electrons
produced by reionization serves as an opacity source that suppresses the
amplitude of the observed primordial anisotropies.

Table 1.1 lists the values of the 6 key parameters according to the
recent Planck results. Knowing these values with sufficient accuracy
allows us to determine other parameters of interest, such as the Hubble
parameter and the dark energy density. More generally, being able to
measure these parameters with accuracy leads to the most detailed
picture of the Universe we have as of yet: a spatially flat Universe
expanding at an accelerated rate.

The standard model is successful not only in being able to fit the
observational data, but also in terms of making testable predictions.
Namely, the polarization of the CMB, predicted by the model has been
discovered in 2002 [Kovács et al. , 2002 ] . Similarly, the prediction
and detection of the baryon acoustic oscillations is another recent
success of the model [Cole et al. , 2005 ] .

Despite the great successes of the @xmath CDM model, a number of
challenges remain. Starting with the validity of the outlined key
assumptions and ending with the reliance on the existence of dark energy
and dark matter, the issues facing the standard model must be discussed
in greater detail.

### 1.3 The Standard Model: Problems and Challenges

#### 1.3.1 The Validity of the Cosmological Principle

The key assumptions of the @xmath CDM framework have been criticized
thoroughly ever since the inception of the standard model. Namely, it is
clear that the cosmological principle, i.e. the homogeneity and isotropy
of the structure in the Universe, does not hold on some scales (e.g. the
Local Group with its complex structure is far from being homogeneous and
isotropic). Multiple observational tests have been performed to test the
cosmological principle, generally confirming it on large scales [Lahav,
2001 , Bengaly et al. , 2019 ] . However, on smaller scales multiple
questions remain, such as what effects do local deviations from isotropy
and homogeneity have on our measurements of the accelerating expansion
of the Universe. More specifically, different models of inhomogeneous
cosmology argue that inhomogeneities on different scales affect the
local gravitational forces leading to skewed measurements of the
expansion of the Universe. However, these models also suffer from
various issues (see Bolejko and Korzyński [ 2017 ] for an overview).

#### 1.3.2 The Validity of GR on Different Scales

The second key assumption of the standard model, i.e. GR being valid on
all scales, can be challenged as well. Firstly, it is known that the
theory is incomplete in terms of not being able to describe systems
where quantum effects have to be fully taken into account. This implies
that the very early Universe along with some astrophysical systems, such
as black holes, cannot be fully described by the theory. This touches a
more fundamental problem in theoretical physics of not being able to
reconcile GR with quantum field theory. GR is thought to be an effective
theory only valid up to around the Planck scale. This has led to a
search for a complete quantum gravity theory resulting in multiple
prominent approaches, such as string theory, the theory of loop quantum
gravity and a plethora of modified gravity models [Mukhi, 2011 , Agullo
and Singh, 2016 ] .

In a more observational context, the assumption of the validity of GR
has been tested exquisitely, but only on certain scales. Figure 1.1
summarizes the current state of tests of gravity on various scales, with
curvature and potential referring to @xmath and @xmath (for a spherical
object of mass @xmath and radius @xmath ) correspondingly.

A key takeaway from figure 1.1 is that even though modern observational
missions have explored a wide variety of scales, there is still a large
section of the parameter space that remains unexplored. Specifically,
gravity is well-tested in the solar system and binary pulsars, however
the low curvature regime remains to be explored and is of special
interest for understanding various relevant phenomena such as that
associated to dark matter. Similarly, tests of gravity in the strong
curvature regime could improve our understanding of systems where both
gravitational and quantum effects are important (e.g. black holes).
Overall, testing gravity in low and high curvature regimes will likely
provide a fuller understanding of how gravity works, which in turn will
improve our understanding of cosmology and astrophysics on all scales.

#### 1.3.3 Issues Related to Dark Matter

The third base assumption that the @xmath CDM model is based on is
related to the existence of dark matter. Historically some form of dark
matter was hypothesized to exist in order to explain the rotation curves
of galaxies. Most recent observational evidence indicates that dark
matter is crucial for explaining the formation and evolution of galaxy
clusters and large scale structure as well [Freese, 2017 ] . Other key
evidence comes from weak lensing surveys, CMB anisotropies and baryon
acoustic oscillations [Roos, 2010 ] . Figure 1.2 shows the combined
constraints on @xmath and @xmath coming from the weak lensing, large
scale structure, supernovae and baryon acoustic oscillation data. These
results clearly illustrate a need for some form of dark energy and
non-baryonic matter to explain the currently available observational
data.

Despite the great success of the cold dark matter paradigm, certain
questions remain unanswered. This is especially clear in the context of
galaxy formation where a number of challenges to the @xmath CDM model
have emerged in recent years. These include the missing satellites
problem, which indicates a mismatch between the observed dwarf galaxy
numbers and the corresponding prediction from numerical simulations.
Similarly, the cusp/core problem indicates a mismatch between the
predicted and observed cuspiness and density of the dark matter
dominated galaxies. Another inconsistency comes in the form of the too
big to fail problem, which states that the observed satellites in the
Milky Way are not massive enough to be consistent with the @xmath CDM
predictions [Bullock and Boylan-Kolchin, 2017 ] . These issues can be
viewed in a wider context of reconciling theoretical predictions with
the cosmological simulations and observational data. Inconsistencies
could originate due to the lack of understanding of the galaxy formation
processes, difficulty of building realistic simulations of such
processes or a lack of understanding of the fundamental nature of dark
matter.

#### 1.3.4 Issues Related to Dark Energy

Another key challenge that the standard model of cosmology is facing at
the moment is explaining the nature of dark energy. As illustrated by
figure 1.2 , @xmath dominates the total energy budget of the Universe.
Some form of dark energy is required to account for the accelerated
expansion of the Universe. The energy scale for the cosmological
constant deduced from the available observational data is of the order
of: @xmath @xmath [Koyama, 2016 ] . However, arguments in quantum field
theory and semi-classical gravity suggest existence of vacuum energy
@xmath , which should contribute to the total energy budget of the
Universe. Calculations in quantum field theory suggest @xmath @xmath ,
which is a huge value comparable to @xmath , where @xmath is the density
of atomic nuclei [Weinberg, 1989 ] . Such a major contribution to the
total energy budget is clearly not observed in the available data, which
leads to the old cosmological constant problem ( why doesn’t the vacuum
energy gravitate as expected? ). In addition, a related problem arises
when trying to explain the observed accelerating expansion of the
Universe. Namely, extreme fine tuning is required between the value of
the cosmological constant and the predicted vacuum energy in order to
explain the observed cosmological expansion. This is referred to as the
new cosmological constant problem.

Other conundrums include the why now? problem, as in why is the current
vacuum energy density of similar magnitude to the matter energy density
at this particular cosmic epoch [Lombriser, 2019 ] ? These issues have
been studied extensively and various possible solutions have been
proposed in the context of different models of dark energy and modified
gravity (e.g. see Li et al. [ 2011 ] ).

#### 1.3.5 Tensions in the Cosmological Parameters

Another key contemporary challenge to the standard model is the
existence of the various tensions between the different observables. A
prime example of this is the tension between the early and late Universe
measurements for the expansion rate parameter @xmath . In more detail,
the local measurements of @xmath using the distance ladder indicate a
significantly higher value when compared to the Planck CMB measurements
(at around 3.5- @xmath level) [Riess et al. , 2018 ] . Such a tension
could indicate various systematic problems both with the early and the
late Universe measurements or, alternatively, it could indicate new
physics. Figure 1.3 summarizes some of the recent measurements of @xmath
. Various ways of relieving the tension have been proposed, such as
independent ways of measuring @xmath through gravitational wave
measurements or through the calibration of the tip of the red giant
branch [Abbott et al., 2017 , Freedman et al. , 2019 ] . These
measurements relieve the tension to some extent, however more accurate
observational data might be needed to fully account for the discrepancy.

Another example of a tension between the different types of cosmological
measurements is the @xmath tension, where @xmath . The @xmath parameter
here refers to the amplitude of the linear power spectrum on the scale
of 8 @xmath Mpc. It is one of the key cosmological parameters due to
being related to the growth of the fluctuations in the early Universe.
As described in Joudaki et al. [ 2020 ] , there is a @xmath - @xmath
tension between the combined Kilo Degree Survey (KV450), DES-Y1 and the
Planck weak lensing measurements of the @xmath parameter. This tension
is also likely one of the key reasons behind the significant difference
in the cosmological parameter constraints observed in figure 1.2 . As is
the case with the @xmath tension, it is not exactly clear what is the
root cause for such a divergence of measurements. As illustrated by the
results in Joudaki et al. [ 2020 ] , the DES measurements reduce but do
not solve the tension observed between the KV450 and the Planck
datasets. Data from the surveys in the upcoming decade will likely give
additional clues about the nature of the @xmath and other related
tensions.

The outlined problems indicate that despite the great success of the
@xmath CDM model many issues remain. It is possible that these issues
could be resolved rather naturally with high quality observational data
from the upcoming surveys along with more realistic simulations and a
better understanding of the properties of dark matter and dark energy.
However, it could also indicate a need for new physics. In either case,
all the discussed phenomena are intimately related to our understanding
of how gravity works on different scales. Starting with the intricacies
of galaxy formation and ending with the issues related to the
accelerated expansion, a better understanding of gravity could help
resolve some of the key issues outlined above. Because of this,
modifying GR has been proposed as a possible solution to the many
conundrums facing the standard model of cosmology.

### 1.4 Modified Gravity: Tests and Current Developments

The motivations for modifying GR are generally trifold: accounting for
the accelerated expansion of the Universe, explaining the nature of the
missing mass on cosmological scales and giving a deeper understanding of
how gravity relates to quantum field theory. These are all goals of key
importance and making significant progress in any of these directions
could account for the various shortcomings of the @xmath CDM model. For
these reasons, a vast family of modified gravity models has been
developed.

One rather natural way of classifying modifications to GR can be defined
in the context of Lovelock’s theorem. Lovelock’s theorem states the
following: in 4-D the only divergence-free symmetric rank-2 tensor
constructed from only the metric @xmath and its derivatives up to second
order, and preserving diffeomorphism invariance, is the Einstein tensor
with a cosmological constant term . In slightly simpler words, Einstein
field equations are unique equations of motion for a single metric
derivable from a covariant action in 4-D [Berti et al., 2015 , Li and
Koyama, 2019a ] . This theorem is profound as it shows that GR in this
context is the simplest theory of gravity with the outlined properties.
Hence, if one was to modify GR, some of the outlined conditions would
necessarily have to be broken. In fact, Lovelock’s theorem gives a
recipe on how to generate modified gravity theories: a modification of
gravity will have one (or multiple) of the following features:

1.  Extra degrees of freedom. This refers to extra scalar, vector and
    tensor fields introduced to the action. This class of models
    includes the Horndeski theory, which is the most general
    scalar-tensor theory in 4 dimensions leading to second order
    equations of motion [Horndeski, 1974 ] . Horndeski theory includes
    many familiar theories such as Brans-Dicke gravity, chameleon
    gravity and quintessence. This class also contains models such as
    massive gravity and bi-gravity [Kenna-Allison et al. , 2019 , 2019 ]
    .

2.  Lorentz Violations. These models break the Lorentz invariance.
    Examples models include Hořava gravity, Einstein-Aether theory and
    n-DBI gravity [Blas and Lim, 2014 ] .

3.  Higher spacetime dimensionality. Early models including extra
    spacetime dimensions, such as the Kaluza-Klein theory, have inspired
    a number of contemporary models such as string theory. Other
    prominent models in this class include braneworld models [Maartens
    and Koyama, 2010 ] .

4.  Non-locality. Non-local models contain terms of the form of @xmath
    or @xmath in the Einstein-Hilbert action. More generally, various
    string-inspired non-local models have gained popularity in recent
    years. Such models have been used in the context of dark energy,
    inflation and bouncing cosmology scenarios [Koshelev, 2011 ] .

5.  Higher derivatives. These models introduce higher degree derivatives
    to the action. Such theories are difficult to construct as higher
    derivatives can lead to Ostrogradsky instability. However, there are
    ways to avoid such instabilities, as shown in beyond Horndeski
    models [Langlois and Noui, 2016 ] .

Figure 1.4 shows some of the more popular models classified according to
Lovelock’s theorem. It is important to note that there are many models
that do not easily fit into such classification. A prime example of this
in the context of this thesis refers to various emergent/entropic
gravity models. Emergent gravity can refer to a wide class of not
necessarily related theories that describe gravity as an emergent
phenomenon. Such theories combine ideas from black hole thermodynamics
and condensed matter physics in order to explore the possible emergence
of gravity with prime examples being approaches described in Padmanabhan
[ 2015 ] and Verlinde [ 2017 ] .

The mentioned models can give insight into the various conundrums of the
standard model. In particular, the mentioned classes of models can
explain the accelerating expansion with various degrees of success. Or,
additionally, some of the models can give insights into the problem of
dark matter and shine light on the various incompatibilities between GR
and quantum physics. However, as of yet, there is no single framework
that fully accounts for the effects associated with dark energy and dark
matter while also fitting all the key observational datasets.
Observational constraints, in particular, play a crucial role in
exploring the space of the allowed theories. There is a plethora of
astrophysical and cosmological tests on scales ranging from laboratory
and interferometry tests all the way to large scale structure tests of
modified gravity. Here we will review the main types of observational
and experimental tests in a rough order of scale. A deeper discussion of
the cluster scale tests will be given in chapters 2 and 3 .

#### 1.4.1 Laboratory Tests

Laboratory tests aim to detect fifth force effects on the smallest
scales accessible by the currently available instruments ( @xmath m and
larger). A key challenge for these types of experiments is reducing the
Newtonian force effects from the environment. This can be done by using
vacuum chambers and optimizing the geometry of the experiment (i.e. the
geometry of the mass/density distribution).

At sub-mm scales one could in principle detect the Casimir force
effects, which are predicted by quantum electrodynamics, manifesting as
an interaction between two parallel uncharged plates. At these scales
one can also detect chameleon forces, which would dominate over the
Casimir force. Hence, the deviation from the predicted Casimir force can
be used as a probe for chameleon force effects. Chameleon models refer
to a class of scalar-tensor theories that avoid the solar system
constraints by employing a special form of a non-linear potential. A
common general choice for chameleon models is of the following inverse
power law form [Burrage and Sakstein, 2018 ] :

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

where @xmath is the scalar field, and the different choices of the
@xmath parameters corresponds to different models. @xmath can be set to
@xmath eV to account for the accelerating expansion (discussed further
in chapter 3 ).

When it comes to Casimir force experiments, the most precise
measurements are achieved by measuring the force between a plate and a
sphere rather than two plates, which leads to the chameleon force
scaling with the distance between the sphere and the plate, @xmath as
follows:

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

with @xmath as the chameleon force and @xmath as a constant that
dictates the scaling. Stringent constraints can be put for @xmath and
@xmath models [Burrage and Sakstein, 2016 ] .

Other experiments that probe the Casimir force effects include optically
levitated dielectric spheres with radii ranging around @xmath . In these
types of experiments laser beams are used to counter the Earth’s
Newtonian gravity effects. Such an approach can put constraints on the
@xmath models [Burrage and Sakstein, 2018 ] .

Atom interferometry is another powerful technique that can be used for
constraining chameleon models. These experiments employ interferometers,
which allow probing the acceleration experienced by atoms due to
chameleon forces. In particular, atoms are put into a superposition of
states related to the two different paths that can be taken (the two
arms of the interferometer). The two paths are later recombined and a
measurement is made that allows to put constraints on the acceleration
of the atoms with precisions of around @xmath , with @xmath @xmath
[Elder et al. , 2016 ] .

Another class of laboratory tests that is worth mentioning is precision
neutron tests. Neutrons, being electrically neutral particles, are
perfect for isolating the fifth force effects from the gravitational and
electromagnetic forces due to the environment. Different experiments
using neutrons place constraints on the chameleon coupling strength
@xmath . For instance, using ultra cold neutrons interacting with a
mirror one can put a constraint in the range of @xmath @xmath [Jenke et
al. , 2014 ] . Figure 1.5 summarizes some the currently available
laboratory constraints on chameleon models.

#### 1.4.2 Solar System Tests

Solar system tests of GR date back to the very beginnings of Einstein’s
revolutionary theory. In fact, long before the development of GR,
deviations of the perihelion precession of Mercury from the Newtonian
gravity prediction were known. This observation later led to one of the
key tests confirming the validity of GR.

Another early test confirming the validity of GR was performed by
measuring the deflection of light by the Sun. The observations of Arthur
Eddington and collaborators during the solar eclipse of 1919 measured
the displacement of the position of stars behind the sun proving one of
the key tenets of the theory.

Modern tests put some of the tightest constraints on the deviations from
GR. Experiments, such as the Shapiro time delay measurements, which give
the relativistic time day experienced by radar signals in a round trip
to Mercury and Venus, agree with the theoretical GR prediction at 5%
level [Shapiro et al. , 1971 ] . More recently measurements based on the
same basic principle were performed using the data from the Cassini
spacecraft, which measured the frequency shift of radio photons to and
from the spacecraft. This experiment constrains the parametrized
post-Newtonian formalism Eddington parameter @xmath (which quantifies
the deflection of light by a gravitational source) with high precision:
@xmath [Bertotti et al. , 2003 ] .

Tests of the strong equivalence principle (laws of gravity are
independent of velocity and location) are of special importance in the
context of modified gravity models. A wide class of theories predict
violations to the strong equivalence principle on some level. In
general, tests of the strong equivalence principle test the universality
of free fall, which is measured by comparing accelerations @xmath and
@xmath of two different bodies:

  -- -------- -- --------
     @xmath      (1.13)
  -- -------- -- --------

with @xmath and @xmath as gravitational and inertial masses
correspondingly. In the case of the solar system tests of the
equivalence princple, the two bodies are the Earth and the Moon as
measured in the lunar laser ranging experiments. These experiments put
strong constraints on the anomalous perihelion angular advance of the
Moon: @xmath [Williams et al. , 2004 , Li and Koyama, 2019b ] . Such
experiments also constrain the time variation of Newton’s constant:
@xmath per year [Williams et al. , 2009 ] . Finally, the constraints
from the lunar laser ranging experiments can be combined with the
Eöt-Wash torsion balance measurements to provide a confirmation for the
strong equivalence principle at 0.04% [Merkowitz, 2010 ] .

The solar system constraints have had a profound influence on the
theoretical development of modified gravity models. The outlined
constraints clearly indicate that GR is valid in the solar system,
leaving nearly no space for even miniscule modifications of the model.
This has led to the development of various screening mechanisms, which
suppress the fifth force effects in the solar system, while still
allowing interesting effects on cosmological scales.

#### 1.4.3 Gravitational Wave Tests

In terms of observational constraints, one of the key developments at
the time of writing this thesis has been the detection of the
gravitational wave and gamma ray burst signals from a neutron star
merger event GW170817/GRB 170817A. The event resulted in a 100 second
gravitational wave signal and a corresponding 2 second duration
gamma-ray burst caused by the merger [Abbott et al. , 2017 ] . The
optical counterpart of the event has subsequently been observed by over
70 observatories marking the beginning of this type of multi-messenger
astronomy [Nicholl et al. , 2017 ] .

The key significance of the mentioned gravitational wave observation in
the context of this thesis comes in terms of the constraints on modified
gravity models. In general, introducing new fields coupled to gravity in
modified models of gravity affects the propagation speed of
gravitational waves. Hence, the speed of the propagation of
gravitational waves can be used as reliable probe of modified gravity.
Probing modified gravity models with gravitational waves has a number of
advantages, such as the fact that gravitational waves can be used to
test theories with screening mechanisms (given that the signals come
from extragalactic sources). In addition, even small deviations from the
speed of light in gravitational wave propagation can accumulate over
large distances, making such a probe extremely sensitive. In particular,
the observed event GW170817 allowed putting extremely tight constraints
on the speed of the gravitational waves: @xmath [Abbott and others, 2017
] . This result has single-handedly ruled out a wide subset of
modifications of gravity. More specifically, such a strong constraint
practically rules out any model that predicts variation of the
gravitational wave propagation speed with respect to the speed of light.
It is useful at this point to discuss some of the effects of the
gravitational wave results on the various classes of models discussed
previously without going into great detail. In this regard, it is useful
to introduce Horndeski theory, which contains several models important
to this thesis as subsets of the theory. Models of special importance to
this thesis will be discussed further in chapters 3 and 4 .

As previously mentioned, Horndeski theory refers to the most general
scalar-tensor theory with 2nd degree equations of motion. The theory can
be described by the following Lagrangian:

(1.14)

where @xmath , @xmath , @xmath and @xmath are free functions of the
scalar field @xmath and @xmath , @xmath is the Einstein tensor, @xmath
is the Ricci scalar, @xmath and the subscript commas denote derivatives
[Horndeski, 1974 ] . The different choices for the set of functions
@xmath represent different scalar-tensor models.

The gravitational wave speed in Horndeski theory can be deduced via the
tensor sound speed @xmath by noting that @xmath . The tensor sound speed
has been shown to have the following form [Kobayashi et al. , 2011 , Li
and Koyama, 2019a ] :

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

The observational requirement of @xmath (or equivalently @xmath ) can be
satisfied by setting @xmath and @xmath . This ultimately results in only
the following class of Lagrangians surviving:

  -- -------- -- --------
     @xmath      (1.16)
  -- -------- -- --------

The effect of this is that a wide class of Horndeski and beyond
Horndeski models are ruled out (see Sakstein and Jain [ 2017 ] and Baker
et al. [ 2017 ] for a wider discussion). This includes some important
models in the context of the accelerating expansion of the Universe. In
particular, a subclass of Galileon models, which can account for the
accelerated expansion without a need for a cosmological constant are
ruled out. Similarly, a wide subset of degenerate higher order
scalar-tensor theories (DHOST) has been ruled out. The same can be said
about the Fab Four models that contain interesting cosmological
solutions.

The surviving models include a class of theories where gravity is
minimally coupled like the kinetic gravity braiding models and
quintessence. The k-essence models are also still valid. So are the
models relevant to this thesis, such as the @xmath and Brans-Dicke
theories. Figure 1.6 summarizes the current state of the various
modified gravity models after the release of the gravitational wave
results.

#### 1.4.4 Galaxy Scale Tests

Observations of galaxies have historically played an important role in
the theoretical development of dark matter and modified gravity models.
Namely, galaxy rotation curve measurements acted as one of the initial
pieces of evidence for the existence of dark matter. More generally, the
complex morphology of galaxies allows testing modified gravity models
with different screening mechanisms along with alternative models of
dark matter.

Theories with screening mechanisms predict different effects on the gas
and the stars that make up galaxies. This is the case, as stars are
generally screened, while the diffuse gas is not. Hence, comparing the
rotation curves of stars and gas allows putting constraints on theories
with screening. As an example, this method has been used to constrain
the @xmath parameter to values of @xmath in @xmath models (see chapter 3
for a wider discussion of these models) [Vikram et al. , 2018 ] . More
generally, for theories with screening, the self-screening parameter has
been constrained to values of: @xmath .

In addition, screening can lead to morphological and kinematical
distortions of galaxies. In this case the stellar component of a dwarf
galaxy is self-screened while the surrounding dark matter halo and
gaseous component are unscreened. Different fifth force effects
experienced by the different parts of galaxies lead to an offset of
stellar disks from the HI (neutral atomic hydrogen) gaseous components.
In addition, galactic disks are warped in a way whereby the screened
stars are displaced from the principal axis. A recent example of such
measurements includes Desmond et al. [ 2018 ] , where offsets between
the optical and HI centroids were constrained. The mentioned
measurements also put a constraint on the @xmath theories: @xmath .

Galaxies also offer ways of testing gravity via gravitational lensing. A
recent example of such a measurement comes from the ESO 325-G004
elliptical galaxy. Comparing the mass estimates from the stellar motion
and weak lensing data coming from the Hubble Space Telescope and the
Very Large Telescope indicated no significant deviation from GR with
@xmath with 1- @xmath confidence [Collett et al. , 2018 ] .

The mentioned techniques are only a small subset of the tests performed
on galaxy scales in recent years. For a more systematic review see Jain
and VanderPlas [ 2011 ], Vikram et al. [ 2013 ], Koyama [ 2016 ] .

#### 1.4.5 Galaxy Cluster Tests

Galaxy clusters and superclusters, being the largest gravitationally
bound structures, offer a multitude of ways of testing the effects of
gravity on large scales. Galaxy clusters contain anywhere from hundreds
to thousands of galaxies, with total masses in the range of @xmath M
_(@xmath) . The mass distribution of galaxy clusters is dominated by
galaxies and the lower density intracluster medium (ICM) with
temperatures ranging between 2-15 keV [Kravtsov and Borgani, 2012 ] .
This combination of high density regions (where the fifth force would be
screened) and lower density intracluster gas, especially in the
outskirts of clusters (where there would be no screening), makes
clusters great for testing modified gravity theories.

Various modified gravity models with screening mechanisms can leave
imprints in the observational properties of galaxy clusters. More
specifically, modifications of GR can affect cluster density profiles
and correspondingly X-ray surface brightness and weak lensing profiles.
As an example, recent work in Schmidt et al. [ 2009 ] and Cataneo et al.
[ 2016 ] investigated the abundance of massive halos as a tool for
detecting @xmath gravity effects. Both studies found similar constraints
for @xmath models: @xmath .

As discussed, the effects of modified gravity with chameleon screening
would not be detectable in the high density galaxy cluster cores,
however, the fifth force would have an effect in the outskirts of
clusters. This introduces a deviation between the hydrostatic and
lensing masses, which, in principle, can be observed by combining X-ray
and weak lensing measurements. Using this technique, the constraints of
@xmath at 95% confidence were obtained in Terukina et al. [ 2014 ] and
Wilcox et al. [ 2015 ] .

These and other cluster scale constraints are discussed in greater
detail in chapter 3 .

#### 1.4.6 Large Scale Structure Tests

Large scale structure formation is sensitive to the underlying model of
gravity. Most types of deviations from GR should in principle be
detectable in the CMB anisotropy data. Furthermore, measurements of the
CMB power spectrum and the secondary bispectrum have some sensitivity to
modified gravity growth of structure effects through the large-scale
integrated Sachs-Wolfe effect and weak lensing. Another method of
constraining gravity is via redshift space distortions. This refers to
the spatial distribution of galaxies appearing distorted when their
positions are plotted as a function of their redshift rather than as a
function of their distance. Comparing these effects against the
theoretical GR predictions places stringent constraints on modifications
of the standard laws of gravity.

As a concrete example, competitive constraints on @xmath models were
obtained in Lombriser et al. [ 2012 ] , where a subclass of @xmath
models designed to reproduce the @xmath CDM expansion history was
tested. In the context of the expansion history, such models can be
parametrized by @xmath , which corresponds to the Compton wavelength
parameter. By combining the data from supernovae distances, baryon
acoustic oscillations and the CMB, constrains of @xmath at 95%
confidence were determined.

Needless to say, the outlined list of the observational probes is far
from complete. A number of techniques will be left undiscussed due to
being out of scope of this thesis. In addition, a much deeper discussion
of the @xmath model and the corresponding constraints is given in
chapter 3 . Finally, figure 1.7 shows a summary of the relevant
constraints on @xmath models.

## Chapter 2 Galaxy Clusters

Chapter 2 introduces the key ideas from galaxy cluster physics. In
particular, the basic structure of galaxy clusters is introduced and
discussed in the context of the underlying astrophysics. In addition,
the properties of the intracluster medium are discussed in the context
of measuring X-ray surface brightness as one of the key probes in our
tests of modified gravity. Similarly, the key features of the SZ effect
are introduced. Galaxy kinematics is discussed as an important technique
for measuring cluster masses. Finally, weak lensing by galaxy clusters
is summarized as a key tool for testing modified gravity as discussed in
chapters 3 and 4 .

### 2.1 The Structure and Basic Properties of Galaxy Clusters

Historically the observational studies of galaxy clusters date back to
the work of Herschel and Messier, who were the first to notice the
tendency of galaxies (then only known as galactic nebulae ) to cluster
[Kravtsov and Borgani, 2012 ] . Later work by Hubble in 1926 showed that
galactic nebulae are in fact galaxies, which in turn resulted in a
better understanding of the nature of galaxy clusters [Heilbron, 2005 ]
. In 1933, under the assumption of virial equilibrium, Zwicky made a
crucial discovery that the visible mass in the Coma Cluster is not
enough to account for the motion of galaxies in the cluster [Andernach
and Zwicky, 2017 ] . In particular, Zwicky calculated the dispersion of
radial velocities of 8 galaxies in the Coma Cluster and found the value
of @xmath km/s [Figueras et al. , 2007 ] . Comparing this result against
the prediction derived using hydrostatic equilibrium equations Zwicky
found that the Coma Cluster had to be over 400 times more massive than
the mass contained in the visible parts of galaxies in the cluster. This
marks the beginning of the observational studies of dark matter.

Modern multi-wavelength studies of galaxy clusters allow us to draw a
detailed picture of the physical properties of these objects. Galaxy
clusters, being among the largest gravitationally bound structures,
contain from hundreds to thousands of galaxies. Typical masses of galaxy
clusters fall in the range of @xmath M _(@xmath) [Sarazin, 1988 ] . A
key feature of galaxy clusters is the high energy intracluster medium
(ICM), consisting of heated, X-ray emitting gas with temperatures of
around @xmath @xmath [Fabian, 1992 ] . Measuring the composition of
galaxy clusters is difficult, as it varies significantly among
individual clusters. However, as a guideline, dark matter makes up
@xmath of the mass budget in clusters, with the leftover @xmath
corresponding to the high-energy ICM and stars [Rosati et al. , 2002 ] .

Modern studies of galaxy clusters have played a crucial role in
understanding the properties of dark matter. A prime example of this is
the case of merging galaxy clusters. During a merger the different
components that make up a cluster interact differently. In particular,
visible matter located in stars and galaxies is mostly not affected by
the collision. High energy ICM as detected by X-ray observations,
however, is slowed down significantly due to the electromagnetic
interactions. Finally, the major mass component in the form of dark
matter passes through the baryonic matter with no interaction. This
results in a mass distribution where the bulk of the mass resides in
regions different to those dominated by the X-ray emitting ICM. The most
well-known system of merging clusters is the Bullet Cluster, which
provides some of the best existing evidence for the existence of dark
matter on galaxy cluster scales [Clowe et al. , 2006 ] . Figure 2.1
illustrates the total and the ICM mass distributions in the Bullet
Cluster clearly showing that the two mass components appear in different
locations in the merging cluster system. In the case of modified gravity
models, such as modified Newtonian dynamics (MOND), which assume no
existence of dark matter, most of the mass in merging clusters should
coincide with the visible baryonic mass distribution. Hence merging
clusters, such as the Bullet Cluster, give important evidence against
modified gravity models of such kind.

Other properties that are worthy of discussion are related to the
formation and the spatial distribution of galaxy clusters. It is widely
accepted that the likely cause of the observed large scale structure in
the Universe is the seed density fluctuations that were formed during
the period of cosmic inflation. Due to gravitational instability these
initial perturbations have been amplified and eventually collapsed to
form an intricate web of filaments and voids. Observing galaxy clusters
allows us to deduce the various properties of the structure of the
Universe on the largest accessible scales. In addition, cluster
observations give information about the underlying cosmological model.
The simplest models of inflation assume the primordial density
fluctuations to be Gaussian. However, more complicated models predict
varying amounts of non-Gaussianity. Detecting such non-Gaussianity via
the CMB or other means would be of great interest in the early Universe
studies. A number of recent studies have been dedicated to investigating
the viability of using galaxy clusters as an alternative probe for
non-Gausianities [Mana et al. , 2013 , Trindade and da Silva, 2017 ] .

Clusters are known to be much more strongly clustered than galaxies
[Bahcall, 1988 ] . This can be expressed in terms of the two-point
correlation function that follows a power law of the form @xmath . More
specifically, @xmath refers to the two-point spatial correlation
function which is related to the joint probability of finding two
objects at separation @xmath . @xmath has been shown to obey the
following scaling relation: @xmath for @xmath and @xmath , with @xmath
as the mean space density of clusters [Bahcall and Cen, 1992 ] . This
agrees well with the more recent observational results described in
Basilakos and Plionis [ 2004 ], Balaguera-Antolínez [ 2014 ] . Recent
studies also reinforce the conclusion that the spatial distribution of
the tracers of the large scale structure, such as galaxies and clusters
of galaxies, are a powerful probe for the early Universe physics. In
particular, combining cluster and galaxy power spectra along with the
cross power spectrum offers a way of accessing a plethora of information
about the underlying cosmology. Hence future surveys such as Euclid will
place unprecedented constrains on primordial non-Gaussianity [Euclid
Theory Working Grp, 2018 ] .

Another key quantity to discuss is the cluster mass function, which is
of special importance when studying large scale structure formation. In
particular, the cluster mass function quantifies the number of clusters
of a given mass at a given redshift: @xmath . The mass function can be
estimated using the Press-Schechter formalism, which assumes that the
fraction of matter that ends up in objects with mass @xmath can be
deduced from the portion of the initial density field (smoothed on the
mass scale @xmath ) lying at an overdensity exceeding a given critical
threshold value @xmath . The gradient of the mass function can be shown
to take the following form [Press and Schechter, 1974 , Borgani, 2008 ]
:

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

with @xmath as the mean cluster density and @xmath is the variance at
mass scale @xmath linearly extrapolated to redshift @xmath . The key
takeaway from equation 2.1 is that there is an intimate link between the
early Universe primordial perturbations and the late Universe structure.
Through the linear perturbation growth factor the value of @xmath can be
directly related to the power spectrum and the cosmological density
parameters. This further illustrates the value of galaxy clusters as
probes for the formation and evolution of large scale structure and, in
turn, for the underlying model of cosmology.

There are other important galaxy cluster probes of the underlying
cosmology. These include the mass-to-light ratio and the baryon
fraction. The mass-to-light ratio quantifies the ratio between the total
mass in a given volume versus the corresponding luminosity. This ratio
can be used to deduce the matter density @xmath . The baryon fraction
also provides a constraint on the matter density parameter (assuming
that the cosmic baryon density parameter is known). With an additional
assumption that the baryon fraction does not evolve in galaxy clusters,
one can constrain the dark energy equation of state parameters [Borgani,
2008 ] .

### 2.2 Intracluster Medium and X-ray Observations

The ICM is composed of high-energy superheated X-ray emitting plasma.
The ICM mainly consists of ionized helium and hydrogen, which dominates
the total baryonic content of galaxy clusters. Heavier elements, such as
iron, can also be found as quantified by the ratio to hydrogen known as
metallicity. Average values of metallicity range from one third to a
half of the value observed in the Sun [Mantz et al. , 2017 ] . Studying
the chemical structure of the ICM and its evolution with redshift offers
a record of the overall element production and evolution throughout the
history of the Universe.

The high temperature of the ICM leads to X-ray emission via the process
known as bremsstrahlung radiation. Bremsstrahlung radiation refers to
the braking radiation produced by deceleration of charged particles when
deflected by other charges. A typical example of such a process is the
deflection of electrons by atomic nuclei leading to X-ray emission with
a frequency proportional to the energy change. The emissivity at
frequency @xmath for an ion of charge @xmath in a plasma with an
electron temperature @xmath is [Sarazin, 1988 ] :

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

with @xmath as the elementary charge, @xmath and @xmath as the number
densities of ions and electrons, @xmath as the electron mass, @xmath as
the Gaunt factor which corrects for quantum effects, @xmath as the
Boltzmann constant and @xmath as the Planck constant. The Gaunt factor
in this case is given by [Nozawa et al. , 1998 ] :

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

The total emission including all the other components (such as line
emission) is then given by:

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

The different emission processes described in equation 2.4 can be
written as follows:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where @xmath is the temperature and ion dependent cooling function that
is related to the emission mechanism. Observationally a more natural
quantity to work with is the surface brightness, which is equal to the
integral of @xmath [Terukina et al. , 2014 , Wilcox et al. , 2015 ] :

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

Here we assumed spherical symmetry and switched to projected
coordinates, such that a point at some radius @xmath from the centre of
the cluster is given by @xmath , with @xmath as the perpendicular radial
distance and @xmath as the distance from the centre in the direction
parallel to the line of sight. In addition, we assumed that the gas
within a given cluster at redshift @xmath is dominated by hydrogen, i.e.
@xmath . The @xmath factor comes from the assumption that the emissivity
is isotropic, while the @xmath term accounts for the cosmological
transformations of spectral surface brightness and energy. The electron
number density @xmath is clearly related to the gas distribution in a
cluster which allows us to use surface brightness as a probe for the
underlying mass distribution.

Another cluster scale observable worthy of discussion is the
Sunyaev-Zeldovich (SZ) effect. The SZ effect refers to the distortion of
the CMB through inverse Compton scattering by high-energy electrons from
the ICM. The SZ effect, more specifically, is a combination of multiple
primary and secondary effects. These include thermal interactions
between the CMB photons and the high-energy electrons as well as
secondary kinematic and polarization effects. This method does not
depend on redshift and provides a way of measuring cluster masses as
well as detecting clusters at great distances. In addition, it is
possible to use a combination of the SZ effect and X-ray measurements to
accurately deduce distances to clusters.

As CMB photons pass through massive clusters, there is around 1%
probability of interacting with a high-energy ICM electrons [Birkinshaw,
1999 ] . This results in a boost of energy of the photons by @xmath ,
where @xmath and @xmath , as before, are the temperature and the mass of
the electrons correspondingly. This results in distortion of @xmath mK
in the CMB spectrum (see figure 2.2 ). More accurately, the SZ effect
spectral distortions can be expressed as follows [Rephaeli, 1995 ] :

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where @xmath is a function of a dimensionless frequency @xmath , @xmath
is the electron number density, @xmath is the associated mass and @xmath
is the Thomson cross-section.

### 2.3 Methods To Estimate Cluster Masses

#### 2.3.1 Hydrostatic Equilibrium

A key notion when it comes to measuring galaxy cluster masses is that of
hydrostatic equilibrium. The hydrostatic equilibrium equation relates
the pressure gradient with the gravitational force in a galaxy cluster.
Under spherical symmetry, the hydrostatic equilibrium equation is given
by [Terukina et al. , 2014 , Wilcox et al. , 2015 ] :

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

where @xmath and @xmath are the pressure and the density of the gas and
@xmath is the radial coordinate. @xmath refers to the total gas
pressure, including the thermal and non-thermal pressure contributions:
@xmath (discussed in more detail in chapter 3 ). Similarly, @xmath is
the mass enclosed in radius @xmath and can also be split into the two
contributions [Laganá et al. , 2010 ] :

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

Intuitively, equation 2.8 represents a balance between the pressure and
the gravitational force in a galaxy cluster. This indicates that the
galaxy cluster is not undergoing formation processes or is not taking
part in a merger.

Using the equation of state for gas with a number density @xmath and
temperature @xmath : @xmath , one can rewrite the thermal mass component
in a more useful form:

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

Here the identity @xmath was used, with @xmath as the mean molecular
weight and @xmath as the proton mass. The mean molecular weight for the
fully ionised gas is given by @xmath , where @xmath and @xmath are the
number densities of hydrogen and helium correspondingly [Ettori et al. ,
2013 ] .

The fraction of the non-thermal contribution to the total pressure is
given by [Shaw et al. , 2012 ] :

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

where @xmath and @xmath are parameters determined by hydrodynamical
simulations. The @xmath parameter corresponds to the radius at which the
dark matter halo average density is equal to five hundred times the
critical density. Analogously, @xmath corresponds to the mass at @xmath
.

Equation 2.8 is of key significance as it allows us to relate the mass
distribution in galaxy clusters to the observed pressure/temperature
distribution, which can be inferred via X-ray surveys. Such a way of
measuring masses is based on two key assumptions: that the majority of
observed clusters are in fact in hydrostatic equilibrium and that most
clusters are on average spherical. In general, effects of non-spherical
geometries of clusters can be averaged out by stacking a large numbers
of clusters. The hydrostatic equilibrium equation can also be tested by
comparing independent measurements of cluster masses. More concretely,
one can compare the X-ray determined masses to those deduced by weak
lensing. As an example, recent measurements by Smith et al. [ 2016 ]
indicate that the mean ratio of X-ray to lensing masses for 50 LoCuSS
clusters at @xmath is @xmath , hence showing no significant deviation
from the hydrostatic equilibrium assumption. Note, however, that the
results of such measurements strongly depend on the method and the
dataset used. For instance, the results in Biffi et al. [ 2016 ] derived
using simulated galaxy clusters show variations up to 10-20% up to the
virial radius.

At this stage it is important to discuss the various astrophysical
effects that can lead to biases when estimating cluster masses. In
particular, an important concept in the context of galaxy cluster
formation is that of virialization. Theoretically, the cluster merging
and formation processes cease once virialization is reached (i.e. when
the forces acting on the cluster are in balance and the potential energy
is twice the negative kinetic energy as described by the virial
theorem). Many real-world clusters, however, are not fully virialized,
with the inner regions being more relaxed than the outer parts of the
cluster. In addition, there is an ongoing accretion of gas and dark
matter in the outer parts of the cluster. These effects complicate the
mass estimates using the SZ effect and the galaxy kinematics. In
addition, these effects could introduce extra bias when constraining
models of modified gravity using the methods described in chapters 3 and
4 . However, it is important to note that the mentioned effects have
been investigated in the previous studies and found to be subdominant
when compared to the non-thermal pressure effects and the predicted
deviation between the different mass estimates due to modified gravity
(for a wider discussion see Wilcox [ 2016 ], Rumbaugh et al. [ 2018 ],
Walker et al. [ 2019 ] ). Nonetheless, effects such as these are
important to understand and to quantify using both observational data
and simulations. Further analysis of some of these systematics is given
in chapters 3 and 4 .

#### 2.3.2 Galaxy kinematics

Another important method of estimating cluster masses uses the
kinematics of the member galaxies. Under the assumption of the virial
theorem, one can express the mass as follows [Borgani, 2008 ] :

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

where @xmath is the line-of-sight velocity dispersion and @xmath is the
viral radius. The virial radius can be estimated if a sufficient sample
of member galaxies is available:

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

with @xmath as the number of galaxies and @xmath as the separation
between the @xmath -th and the @xmath -th galaxies.

Such a method of determining cluster mass comes with a set of
challenges. Namely, as previously discussed, the assumption of the
virial theorem can be valid to varying degrees in different populations
of galaxies (for instance late and early types of galaxies). Another
challenge in the context of observational data, comes in terms of
non-member (foreground or background) galaxies that can bias the mass
measurements. Algorithms for filtering out such interloper galaxies are
of special importance for accurate mass estimates [Girardi et al. , 1993
, van Haarlem et al. , 1997 ] .

A key issue when it comes to such virial theorem-based approaches is not
knowing the full underlying dark matter distribution. Such approaches
are based on the assumption that, in general, dark matter follows the
visible mass distribution. However, if one were to relax this
assumption, the under/over-estimation of the cluster mass is given by
[Sadat, 1997 ] :

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where @xmath is the true ratio of the dark matter and galaxy masses,
while the @xmath are the relative concentration parameters.

A related method of determining the underlying mass distribution in
clusters is via the orbits of member galaxies. In particular, assuming
hydrostatic equilibrium, one can show that the mass distribution is
given by:

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

where @xmath is the number density of galaxies, and the dispersion
parameters @xmath and @xmath control the shape of the orbit ( @xmath for
isotropic orbits). The drawback of this particular method is that the
velocity dispersion profiles are not well known, leading to bias in the
mass estimates [Wojtak and Łokas, 2010 ] .

### 2.4 Weak Lensing in Galaxy Clusters

#### 2.4.1 The Basics of Gravitational Lensing

One of the most important astronomical probes used to track the
underlying cluster mass distribution is gravitational lensing.
Gravitational lensing effect refers to the bending of light by large
distributions of matter. The existence of such an effect was known long
before the development of GR. In fact, if one allows the possibility of
light having even a minuscule mass, Newtonian physics predicts bending
of light rays by massive bodies [Yajnik, 2019 ] . As later shown and
calculated by the German astronomer Johann Georg von Soldner, the
deflection angle of light due to a massive body is proportional to the
gradient of the gravitational potential [Giné, 2008 ] . This turned out
to be a surprisingly accurate prediction that agreed with the initial
calculations of Einstein as of 1911. Only in the final version of GR in
1915 Einstein managed to obtain the correct result, which was equal to
twice the predicted Newtonian value. More specifically, Einstein
predicted a deflection of 1.7 arc seconds for light passing the Sun
[Einstein, 1916 ] .

Here we will lay out some of the equations for a single lens system as
well as extended mass distributions e.g. galaxy clusters. The
derivations are based primarily on Wright and Brainerd [ 1999 ] and
Bartelmann and Maturi [ 2017 ] .

To estimate the deflection angle our starting point is assuming that the
Newtonian potential in cosmological systems is small, i.e. @xmath . In
addition, the peculiar velocities that mass distributions have on
cosmological scales are relatively small. These assumptions allow us to
describe gravitational lensing by using the perturbed Minkowski metric:

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

For propagating light @xmath , which gives:

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

where @xmath is the effective light speed (note that the gravitational
potential is negative) and @xmath . Note that in GR (in the absence of
anisotropic stress) @xmath , however, this is not generally the case in
modified gravity models. To be consistent with the analysis in the later
parts of this chapter, we are using a more general notation here. Also
note here that the form of equation 2.17 allows us to define the
refraction index in the usual manner: @xmath , giving @xmath . Here, by
analogy, we are treating a spacetime region with a gravitational
potential @xmath present as a material of refractive index @xmath . One
can then apply Fermat’s principle, which states that the path taken by
light rays minimizes the time of travel, i.e.:

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

where @xmath is the path of the photon, @xmath and @xmath are the
initial and final points and @xmath stands for a variation. Varying
equation 2.18 w.r.t. the light path leads to the identity for the
deflection angle:

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

where the gradient is taken perpendicular to the line of sight. Note
that this calculation gives the correct result as predicted by GR and is
twice larger than the corresponding Newtonian result.

The integral in equation 2.19 is not easy to evaluate, however a
simplified result can be obtained by using the Born approximation. In
particular, for small deflection angles (of the order arc seconds or
smaller), the integration path can be approximated by straight lines:

  -- -- -- --------
           (2.20)
  -- -- -- --------

where the deflection angle was calculated for a point mass @xmath at the
origin with the light ray propagating parallel to the @xmath axis with
an impact parameter @xmath . @xmath here refers to the Schwarzschild
radius. For the Sun, @xmath g, resulting in a deflection angle of @xmath
at the solar radius @xmath km. This is the famous result confirmed by
the observational data collected by Dyson, Eddington and Davidson in
1919 [Dyson et al. , 1920 ] .

More complicated mass distributions require a more complex treatment.
However, if the lensing mass distribution is thin compared to the
distances in the lens system, the light ray paths between the source,
the lens and the observer can be approximated as straight lines as shown
in figure 2.3 . This is known as the thin-lens approximation and is
sufficient to describe the basic lensing properties of isolated masses
such as galaxy clusters.

Following the geometry in figure 2.3 one can define the reduced
deflection angle @xmath :

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

with @xmath and @xmath as the (angular diameter) distance between the
lens and the source and the distance to the source correspondingly. This
allows us to relate the angles shown in figure 2.3 in the following way:
@xmath . Equation 2.21 can also be expressed as:

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

where equation 2.19 was used and the @xmath refers to the perpendicular
gradient. The perpendicular gradient can be replaced with the angular
gradient w.r.t. angle @xmath : @xmath . This finally allows writing
@xmath in terms of the quantity @xmath , which refers to the lensing
potential: @xmath , with:

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

The lensing potential captures the key imaging properties of a
gravitational lens.

Given equation 2.23 , one can define two quantities: @xmath
(convergence) and @xmath (shear), such that:

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

The magnitude of shear is then simply given by: @xmath . Figure 2.4
illustrates how a background source is deformed due to weak lensing and
how these effects are related to quantities @xmath and @xmath . More
specifically, ellipticity of a deformed source galaxy can be defined as:

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

where @xmath and @xmath are the semi-major and semi-minor axes as
illustrated in figure 2.4 . For most weak lensing systems @xmath ,
resulting in @xmath .

An archetypal example of a weak lensing system is that of galaxy
clusters distorting the shapes of the background galaxies. More
specifically, galaxy clusters imprint a coherent distortion pattern onto
the distant background galaxies as measured by their ellipticities.
Hence statistically studying the distortions of the ellipticity of the
background galaxies allows us to ultimately deduce the mass distribution
of the lens cluster. Such measurements, however, are highly complicated
by the fact that the background galaxies have an intrinsic ellipticity
@xmath , which is generally not known. A crucial assumption taken in
weak lensing studies is that the average intrinsic ellipticity, for a
sufficiently large sample of galaxies, is expected to be: @xmath [Hirata
et al. , 2007 ] . As discussed in Bartelmann and Maturi [ 2017 ] , the
standard deviation of the intrinsic ellipticity is measured to be @xmath
and averaging over @xmath faint galaxy images reduces the scatter of the
intrinsic ellipticity to:

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

As illustrated by the description above, the distortion of the
ellipticities of the background sources by a foreground lens mass
ultimately depends on the lensing mass distribution. Hence, studying the
weak lensing effects we can infer the underlying distribution of the
lensing system. In fact, weak lensing offers one of the most powerful
probes for studying the properties of galaxy clusters.

#### 2.4.2 Weak Lensing by NFW Halos

Recent observational evidence along with evidence from numerical
simulations strongly supports the idea that there is a universal density
profile for dark matter haloes [Navarro et al. , 1997 , Bartelmann et
al. , 1998 , Young, 2017 ] . In fact, the mentioned evidence shows that
systems ranging from globular clusters to large galaxy clusters can be
described by the same universal density profile. That, of course, refers
to the well-studied Navarro, Frenk, White (NFW) profile given by
[Navarro et al. , 1996 ] :

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

where @xmath is the critical density, and @xmath is the scale radius.
The virial radius term @xmath refers to the radius inside which the mass
density of the halo is equal to @xmath . Here @xmath refers to the
characteristic overdensity of the halo, given by:

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

Equation 2.29 describes the underlying dark matter distribution in
galaxy clusters. Hence, assuming the NFW profile, one can derive an
analytic expression for the radial dependence of the convergence and the
shear due to the dark matter halos in galaxy clusters [Wright and
Brainerd, 1999 ] .

The local value of convergence can be described by:

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

where @xmath refers to the surface mass density and @xmath is the
critical surface mass density given by:

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

Assuming spherical symmetry, the surface mass density is simply given
by:

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

where the integral is evaluated over the coordinate @xmath along the
line of sight and @xmath is the projected radius relative to the center
of the lens. Equation 2.29 can then be integrated along the line of
sight to give:

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

where a dimensionless radial distance was defined as @xmath . The
tangential shear for an NFW density distribution is then given by:

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

where @xmath refers to the mean surface density inside radius @xmath .
More specifically, the mean surface density is given by the following
integral:

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

Putting everything together and evaluting the integrals gives the
following result:

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

where the two functions @xmath and @xmath were defined for convenience.
The two functions are given explicitly by:

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

In summary, the concepts introduced in this chapter clearly illustrate
how various observational probes can be used to measure the underlying
mass distribution of galaxy clusters. In addition, astrophysics in
galaxy clusters is shown to be tightly related to the properties of dark
matter and the underlying model of gravity. For this reason galaxy
clusters have been extremely important in testing models of modified
gravity on cosmological scales. Chapter 3 delves deeper into the effects
of modified gravity on galaxy clusters. In addition, a technique for
testing chameleon gravity using combined X-ray and weak lensing data
from stacked galaxy clusters is introduced.

## Chapter 3 Modified Gravity on Galaxy Cluster Scales

This chapter introduces chameleon and @xmath gravity models along with
an effective technique for testing modified gravity on galaxy cluster
scales. More specifically, the chapter starts by introducing the
relationship between the scalar-tensor models and @xmath gravity. In
addition, a technique of testing models of modified gravity with
chameleon gravity using cluster X-ray and weak lensing data based on the
previous work in Terukina et al. [ 2014 ] and Wilcox et al. [ 2015 ] is
introduced. Original results reproducing the tests described in Wilcox
et al. [ 2015 ] with an updated dataset are presented. Finally, the
implications of the results for model-independent tests of gravity are
discussed in the last section of the chapter. The original results
presented in this chapter were produced in collaboration with Carlos
Vergara and Kathy Romer, as described in Vergara-Cervantes [ 2019 ] .

### 3.1 Scalar-Tensor Gravity with Chameleon Screening

#### 3.1.1 The Action

As discussed in chapter 1 , modified gravity models offer a novel
approach in tackling some of the key issues in modern cosmology.
However, a major shortcoming of such modified gravity approaches comes
in the context of the stringent observational constraints in the solar
system. In this respect, models with different types of screening
mechanisms are of special importance as they can avoid the rigid solar
system constraints while still possessing a cosmologically interesting
phenomenology. A natural question to ask, however, is how natural and
fine-tuned such models are? Undeniably, most models that allow screening
behaviour are fine-tuned to turn off the fifth force on the scales of
the solar system to avoid the strict constraints. However, it should be
noted that similar screening behaviour can be observed in various
scenarios in electromagnetism and hence it is not entirely unnatural to
expect a scalar field to posses screening. Here the key features of the
scalar-tensor models with chameleon screening are summarized based
primarily on Khoury and Weltman [ 2004 ], Waterhouse [ 2006 ], Burrage
and Sakstein [ 2018 ] .

The chameleon model can be described by introducing a scalar field
@xmath with a potential @xmath . The dynamics of the theory can then be
captured by the action, which, as usual, refers to a functional that,
when varied w.r.t. the metric and the scalar field, gives the set of
equations of motion. In this case the action for a scalar field @xmath
is given by:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

This can be combined with the standard Einstein-Hilbert action with a
term describing the matter fields @xmath (equation 1.1 ) giving the
following combined action:

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

Note that the last term is generalized to allow multiple matter species,
while @xmath refers to the Jordan frame metric, that is conformally
related to the Einstein frame metric @xmath by:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

Note that here we allow for different coupling constants @xmath for
different matter species. Jordan and Einstein frames refer to the two
different ways of expressing the scalar-tensor action. In particular, in
the Jordan frame the scalar field (or some function of it) is multiplied
by the Ricci scalar, while in the Einstein frame it is not. More
formally, the Jordan frame refers to the frame in which the matter is
minimally coupled to the metric. The equations appearing in this section
can be translated between the different frames by using the conformal
transformation defined in equation 3.3 . Also, it is important to note
that the Jordan frame metric @xmath is the metric that the matter
experiences.

As usual, varying the action w.r.t. @xmath allows us to obtain the
equations of motion for the scalar field:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

The terms in the brackets give the equation of motion for the field
@xmath . The last term can be expressed more explicitly by noting that
energy density for matter species @xmath in the Einstein frame is given
by:

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath relates the pressure and the density in the equation of
state and the index @xmath refers to the @xmath -th species of matter as
before. Using this expression the equation of motion can then be written
as:

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

The shape of equation 3.6 allows us to conveniently define an effective
potential @xmath , such that:

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

The equation of motion can then be written succinctly:

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

#### 3.1.2 Properties of the Effective Potential

The behaviour of the chameleon field can be controlled by choosing a
particular form of the bare potential @xmath . As discussed in
Waterhouse [ 2006 ] , as a starting point, one might choose a potential
such that it can give rise to cosmic acceleration via slow roll. In
addition, we also want it to have the screened behaviour, such that the
fifth force effects are suppressed in high density regions. It is
important to note, however, that there are certain no-go theorems that
prohibit scalar-tensor models, which possess both a screening mechanism
and self-acceleration [Wang et al. , 2012 ] . In other words, if our
theory possesses a screening mechanism it will still require some form
of dark energy to account for the accelerating expansion.

In order to possess screening, the potential @xmath has to be continuous
and bounded from below, while also strictly decreasing. In addition, its
first derivative @xmath should be negative and increasing. The second
derivative @xmath should be positive and decreasing. Finally, the
potential should have the following behaviour for vanishing @xmath
values: @xmath .

The two often-used potentials possessing the outlined properties are an
exponential potential of the form:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

and the inverse power-law potential of the form:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

@xmath here refers to a constant with a dimension of mass while @xmath
is a positive constant.

The effective potential has an important feature such that if the
coupling @xmath is positive, there exists a minimum at @xmath :

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

In addition, one can define a mass @xmath associated with the field
@xmath :

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

Setting @xmath in equation 3.12 gives @xmath . The minimum mass @xmath
is of special importance as it is equal to the inverse of the
characteristic range of the chameleon force. Figure 3.1 illustrates the
behaviour of the effective potential and the minimum mass for different
values of the local density. More specifically, as illustrated by
equation 3.12 , when the density @xmath increases, the minimum value
@xmath decreases while the @xmath value increases. In other words, for
larger density regions, such as the solar system, the characteristic
range of the chameleon force becomes very short and hence the modified
gravity effects are suppressed. This is true as @xmath and @xmath are
increasing functions of @xmath , while @xmath is a decreasing function
of @xmath .

The interaction between the chameleon field and matter can be determined
by the geodesic equation:

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

where @xmath is the Christoffel symbol corresponding to the Jordan frame
metric @xmath , while the dot is the derivative w.r.t. the proper time
@xmath . Remembering that the Jordan frame metric is related to the
Einstein frame metric via a conformal transformation (equation 3.3 ),
one can evaluate the needed metric derivatives:

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

This expression can be used to evaluate the Jordan frame Christoffel
symbols in terms of the Einstein frame metric. The geodesic equation can
then be expressed as:

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

where @xmath corresponds to the Einstein frame Christoffel symbols. The
second term in the equation above is the familiar gravitational term,
while the last term corresponds to the force due to the chameleon field.
More specifically, in the non-relativistic limit, a test mass of matter
species @xmath experiences a force @xmath , which can simply be
expressed as:

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

### 3.2 @xmath Gravity

Another type of modified gravity theory important in the context of this
chapter is @xmath gravity. This refers to a family of theories in which
the Ricci scalar is replaced by a general function @xmath . Depending on
the function used, such theories can modify GR in a way that accounts
for the accelerated expansion and offers possible solutions to some of
the other contemporary issues in cosmology. However, many functional
forms are ruled out by theoretical arguments and observational
constraints (e.g. see De Felice and Tsujikawa [ 2010 ], Jain et al. [
2013 ], de la Cruz-Dombriz et al. [ 2016 ] ). Such theories can also
contain a time and scale dependent gravitational constant and also
exhibit massive gravitational waves. Finally, an important feature of
@xmath theories is that performing a certain conformal transformation
allows us to write them in a form equivalent to scalar-tensor theories.
This means that the observational constraints on scalar-tensor theories
with chameleon screening can be converted to the equivalent constraints
on a subset of @xmath theories.

The action for @xmath gravity takes the following form:

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

where we simplified the notation for clarity by setting @xmath . Here
the tilde denotes quantities in the Jordan frame. Also note that now we
assume a single matter species. The equation of motion in @xmath
theories takes the following form:

  -- -------- -------- -- --------
     @xmath   @xmath      (3.18)
  -- -------- -------- -- --------

where @xmath is the energy-momentum tensor and @xmath , while the prime
symbol denotes a derivative w.r.t. @xmath .

The action described above can be recast as a scalar-tensor theory by
defining a scalar field @xmath as follows:

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

where @xmath . Using equation 3.19 and switching to Einstein frame
(equation 3.3 ) allows to rewrite the action 3.17 in the exact same form
as the scalar-tensor action in equation 3.2 , if we express the
potential as:

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

In other words, a subset of @xmath theories defined via equation 3.19
are equivalent to a scalar-tensor theory with the potential defined
above. The possibility of recasting the @xmath model as a scalar-tensor
model where a new scalar field sources the accelerated expansion (e.g.
quintessence) also leads to a nuanced question of what is the difference
between models of modified gravity and dark energy. As illustrated
above, in the case of the @xmath model, it can be interpreted as both a
modified gravity and a dark energy model depending on the chosen frame.
Similarly many other models, such as Brans-Dicke theory can be
transformed to the familiar scalar-tensor form (eq. 3.2 ) by choosing a
suitable conformal rescaling. It should be noted, however, that not
every model can be recast in such a way. Also, there has been a
long-running debate on whether the Einstein and the Jordan frames are
equivalent or alternatively, which of the frames is the physical one
(see Faraoni and Gunzig [ 1999 ] for a more in-depth discussion).

A concrete example of an @xmath model with an interesting phenomenology
is the Hu-Sawicki model, with:

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

with @xmath and @xmath as constants. The constants can be chosen such
that the accelerated expansion can be accounted for while also mimicking
the expansion history of the standard concordance model.

More specifically, in the high curvature regime when @xmath , equation
3.21 can be expanded [Hu and Sawicki, 2007 ] :

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

In the limiting case of @xmath , the @xmath term acts as the
cosmological constant. Furthermore, at finite @xmath , the curvature
freezes to a fixed value and stops declining with the matter density
resulting in a class of models which accelerate in a manner similar to
@xmath CDM. Finally, the constants can be chosen such that the potential
has a form that exhibits the chameleon mechanism as shown in figure 3.1
. Note, however, that the previous comments regarding the no-go theorems
that prohibit models with both screening and self-acceleration apply
here as well.

### 3.3 Testing Modified Gravity on Galaxy Cluster Scales

#### 3.3.1 Non-Thermal Pressure and the Modified Hydrostatic Equilibrium
Equation

Galaxy clusters, as discussed in the previous chapter, being among the
largest gravitationally bound structures in the Universe with regions of
high and low densities, offer a plethora of ways to test modifications
of gravity. In this section, a specific approach first introduced in
Terukina et al. [ 2014 ] and later extended in Wilcox et al. [ 2015 ] is
discussed and summarized. In particular, it is an approach based on
combining multiple galaxy cluster probes, such as X-ray surface
brightness and weak lensing data, in order to constrain modifications of
gravity predicted by chameleon scalar-tensor and the related @xmath
models. As discussed, a key feature of such models is the suppression of
the fifth force effects in the high density regions. In the context of
galaxy clusters, such suppression would manifest in the fifth force
being screened in the dense cluster cores, but not in the outskirts of
clusters. In the outskirts of clusters the intracluster gas would be
affected by the usual force of gravity plus an additional fifth force,
which would then result in the gas being slightly more compact than
predicted by GR. This, in turn, would lead to a slightly higher
temperature and the corresponding X-ray surface brightness. Analogously,
the hydrostatic mass inferred using X-ray measurements would be affected
as well. In contrast, the weak gravitational lensing profile is not
affected in chameleon gravity models (discussed later in the section).
Hence by comparing the X-ray and the weak lensing measurements, the
fifth force effects can be constrained observationally.

A key assumption when comparing the hydrostatic and the weak lensing
masses is that of the hydrostatic equilibrium (equation 2.8 ). In
particular, it describes the balance between the gas pressure gradient
and the gravitational force in the cluster. The total pressure described
in equation 2.8 can be split into thermal and non-thermal contributions:
@xmath . The non-thermal pressure component here is related to a variety
of effects such as the bulk motion and turbulence of the ICM gas along
with the effects of the cosmic rays and magnetic fields. Such effects
are important to account for when estimating the hydrostatic mass
[Laganá et al. , 2010 ] . More specifically, observational evidence and
hydrostatic simulations indicate a common trend of the non-thermal
fraction increasing towards large radii and becoming comparable to the
thermal pressure at around the virial radius [Shi and Komatsu, 2014 ] .
The hydrostatic equilibrium assumption then allows us to define the mass
components corresponding to thermal and non-thermal pressure: @xmath ,
where:

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

The thermal mass component can be re-expressed in terms of the density
and temperature distributions by using the equation of state: @xmath and
@xmath :

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

Here @xmath and @xmath refer to the mean molecular weight and the proton
mass correspondingly. The mean molecular weight for a fully ionised
cluster gas can be defined as: @xmath with @xmath , where @xmath ,
@xmath , @xmath refer to the number density of the electrons, hydrogen
and helium respectively [Terukina et al. , 2014 ] . Adopting the mass
fraction of hydrogen of @xmath leads to @xmath .

The non-thermal pressure effects can be redefined as a fraction @xmath
of the total pressure:

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

And hence @xmath , allowing us to write the non-thermal component as:

  -- -------- -- --------
     @xmath      (3.27)
  -- -------- -- --------

The functional shape of @xmath has been studied using hydrodynamical
simulations [Shaw et al. , 2010 , Battaglia et al. , 2012 ] . In
particular, the cited works show that the non-thermal pressure fraction
can be represented by:

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

with @xmath , @xmath , @xmath and @xmath as constants. The set of values
of @xmath was determined in Shaw et al. [ 2012 ] . These are also the
values used in the related works in the literature [Terukina et al. ,
2014 , Wilcox et al. , 2015 , Vergara-Cervantes, 2019 ] .

Having discussed the thermal and the non-thermal pressure terms, we can
now write down the modified hydrostatic equilibrium equation that takes
into account the fifth force effects:

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

where the last term is due to the chameleon force. The last term can
also be used to define a mass corresponding to the chameleon gravity
effects:

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

The @xmath then modifies the mass inferred by using the hydrostatic
equilibrium equation, such that the total mass is given by:

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

At this point it’s worthwhile to summarize the underlying assumptions
that allow us to calculate the cluster mass using the equations above.
In particular, the key assumptions that lead the equations to have the
form laid out above are those of hydrostatic equilibrium and spherical
symmetry. In addition, to get the correct mean molecular weight, a good
knowledge of the intracluster gas composition is assumed. Finally, the
non-thermal pressure effects are based on studies that come from
hydrodynamical simulations, which are assumed to be sufficiently
realistic to approximate the real cluster astrophysics. As previously
discussed, all these assumptions can be challenged to some degree and,
as always, further work is need both in the context of simulations and
observational data. These assumptions will be further examined in the
rest of this chapter and chapter 4 .

#### 3.3.2 Weak Lensing in Chameleon Gravity

A key aspect of the tests of the chameleon gravity described throughout
this chapter is that the weak lensing effects are not affected by the
fifth force in such models of modified gravity. This is the case as the
chameleon field is coupled to the trace of the energy-momentum tensor.
More specifically, as shown in Arnold et al. [ 2014 ] , if one adopts
the Newtonian gauge in a spatially flat background:

  -- -- -- --------
           (3.32)
  -- -- -- --------

then the gravitational lensing potential is given by: @xmath . Note that
@xmath here denotes conformal time, which is related to cosmic time
@xmath via the scale factor @xmath : @xmath . The equations for the two
potentials @xmath and @xmath can then be derived in @xmath gravity under
the assumptions of weak field limit ( @xmath and @xmath ), quasi-static
approximation and the energy-momentum being described by the
pressureless perfect fluid, such that @xmath . Then it can be shown that
the modified Poisson equation is given by:

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

where @xmath denotes the Laplace operator with respect to the physical
coordinates (rather than the comoving coordinates). Similarly, for the
@xmath potential:

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

Hence, for the lensing potential @xmath we have:

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

which has the usual form for the Newtonian gravitational potential
@xmath . This means that we can use the familiar equations described in
section 2.4 to described lensing in case of chameleon gravity as well.
Note that in the context of @xmath models this argument is true only for
@xmath .

Assuming the NFW profile (equation 2.29 ), the mass inferred from weak
lensing can then be expressed as follows:

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

where @xmath and @xmath as before are the characteristic density and the
characteristic scale – the two parameters used in the NFW profile. Note
that the @xmath term here can be expressed as follows:

  -- -------- -- --------
     @xmath      (3.37)
  -- -------- -- --------

where @xmath is given in equation 2.30 and @xmath refers to the mass
enclosed by @xmath , i.e. the radius at which the average density of the
halo is equal to @xmath , where @xmath . In summary, this means that the
NFW profile can be characterised by two free parameters, the
concentration parameter @xmath and the mass parameter @xmath .

Given the assumption of the hydrostatic equilibrium along with the fact
that the weak lensing mass is not affected by the fifth force effects we
can then relate all the defined masses as follows:

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

Hence, assuming that we can measure the @xmath and the @xmath terms
using X-ray data along with @xmath using the corresponding shear data,
the chameleon mass term can be constrained.

#### 3.3.3 X-ray Surface Brightness

The hydrostatic equilibrium equation for the thermal pressure component
can be integrated to obtain an expression for gas pressure:

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

where @xmath is the central pressure and we used: @xmath . Note that
this expression can also be written in terms of the electron pressure
@xmath and the electron number density @xmath by noting that:

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.41)
  -- -------- -- --------

where @xmath is the mean molecular weight as before. This then gives an
expression for the electron pressure:

  -- -- -- --------
           (3.42)
  -- -- -- --------

The electron distribution in a cluster dictates the form of the @xmath
function. A standard choice to parametrize it adapted in Terukina et al.
[ 2014 ] and Wilcox et al. [ 2015 ] is the isothermal beta model:

  -- -------- -- --------
     @xmath      (3.43)
  -- -------- -- --------

with @xmath , @xmath and @xmath as the free parameters.

By using the equation of state of gas in equation 3.42 , the temperature
of gas in the cluster can be directly related to the X-ray surface
brightness:

  -- -------- -- --------
     @xmath      (3.44)
  -- -------- -- --------

where @xmath is the cooling function and @xmath is the cluster redshift.
The form of the cooling function was obtained in Wilcox et al. [ 2015 ]
by using the XSPEC software and the APEC model over the range of @xmath
- @xmath keV [Arnaud, 1996 , Smith et al. , 2001 ] . The mentioned model
takes gas temperature, the cluster redshift and the cluster metallicity
and outputs X-ray cluster flux for a range of temperatures. The
metallicity value of @xmath was adopted following Sato et al. [ 2011 ] .
Fitting equation 3.44 to X-ray data, allows us to determine the free
parameters in the isothermal beta profile in equation 3.43 , which can
then be used to calculate the thermal and the non-thermal masses.

#### 3.3.4 X-ray and Weak Lensing Datasets

Here the key datasets used to compare our results against the previous
results in the literature are discussed. The general technique of
combining multiple observational probes on galaxy cluster scales in
order to constrain chameleon gravity was first described in Terukina et
al. [ 2014 ] . More specifically, in the mentioned work the modified
gravity constraints were obtained by performing a multi-dataset MCMC
analysis. In particular, this was done by using a combined dataset
consisting of the X-ray temperature data from Snowden et al. [ 2008 ]
and Wik et al. [ 2009 ] , X-ray surface brightness profile data from
Churazov et al. [ 2012 ] , SZ effect data from Ade and others [ 2016 ]
and the tangential shear data from Okabe et al. [ 2010 ] . The mentioned
datasets are described in more detail in the later parts of this chapter
and chapter 4 .

The techniques described in Terukina et al. [ 2014 ] were later expanded
Wilcox et al. [ 2015 ] where the surface brightness and tangential shear
profiles were produced by stacking data from 58 galaxy clusters. In
particular, the mentioned dataset contains data from 58 galaxy clusters,
at redshifts @xmath from the XMM Cluster Survey (XCS) and the Canada
France Hawaii Telescope Lensing Survey (CFHTLenS). The clusters were
stacked in order to improve the signal to noise ratio and to remove
various irregularities that individual clusters posses. Combining
observations of multiple clusters is a complicated procedure, as the
cluster images have to be rescaled in a consistent matter while also
taking into account the fact that each observation comes with different
background properties and flare corrected exposure times. In order to
produce a single stack, the 58 individual cluster images were rescaled
to a common projected size by estimating the @xmath and @xmath masses
using the approach described in Hu and Kravtsov [ 2003 ], Sahlén et al.
[ 2009 ] . Subsequently, the @xmath radius was calculated for each
cluster, which in turn allowed rescaling each image to a @xmath pixel
format, such that each cluster had an @xmath equivalent to 125 pixels.
Each of the images was then centered on the source centroid given in the
XCS data. The final stacked surface brightness map was produced by
taking the mean value for each pixel across all the images.

The tangential shear profiles were calculated using the ellipticity
components and the photometric redshifts for each source galaxy, as
given in the CFHTLenS catalogue. In particular, for each galaxy the
tangential and the cross-shear components @xmath were calculated as a
function of their position relative to the cluster position (as measured
by an angle @xmath relative to the baseline of zero declination). The
tangential shear around each XCS cluster centroid was then binned into
24 equally spaced logarithmic annuli reaching @xmath . The shear values
were then stacked by summing the profiles of each cluster and
calculating an average shear value in each radial bin.

The dataset was also split into two bins based on the X-ray temperature.
The temperature for each cluster was determined by using a
mass-temperature relation following the procedure laid out in Stott et
al. [ 2010 ] . In particular, the cluster stack was cut into a low
temperature bin ( @xmath keV) with a median redshift @xmath and a high
temperature bin ( @xmath keV) with @xmath . This roughly corresponds to
splitting the dataset into galaxy groups and galaxy clusters. The logic
for such a split was based on tests, where different splits were
considered with a goal of producing the tightest possible constraints of
the modified gravity parameters. More specifically, having multiple
temperature bins were shown not to have a significant effect on the
modified gravity constraints, hence two bins were used.

An important aspect of the dataset described in Wilcox et al. [ 2015 ]
is that the majority of the mentioned 58 clusters are sufficiently
isolated from the neighbouring clusters. This is of key importance, as
if clusters are not sufficiently isolated, they might be screened by the
neighbouring clusters essentially suppressing any fifth force effects.
In order to measure the separation of individual clusters in the
dataset, the separation parameter @xmath which quantifies the separation
between a given cluster and the nearest cluster scaled by the @xmath for
each given cluster was calculated [Zhao et al. , 2011 ] . In such a
parametrization, @xmath corresponds to a well isolated cluster. Figure
3.2 shows the @xmath values for each cluster in the dataset. Only 5
clusters are found to be not sufficiently isolated from the local
environment.

The stacked X-ray surface brightness and the tangential shear profiles
for both temperature bins are shown in figure 3.3 . The best-fit results
and the corresponding modified gravity constraints were obtained using
an MCMC analysis (described at the end of this section).

A recent analysis of the original 58 cluster dataset, as described in
great detail in section 3.2 in Vergara-Cervantes [ 2019 ] , indicated
that a number of sources were possibly misclasified as galaxy clusters.
In particular, the newest XCS master source list indicated that the XCS
automated pipeline algorithm (XAPA), that is used to detect X-ray
sources, flagged 8 of the objects as point sources and 11 as extended
sources with a point spread function warning flag (indicating a need for
further investigation). After further investigation, a total of 27
sources were removed as a precaution due to being possibly
misclassified. More specifically, the mentioned 27 sources were found to
resemble AGN sources rather than multiple galaxies with an extended
X-ray emission (see a sample of the misclassified sources in figure 3.4
). The removed sources can be split into bins in terms of the photon
counts of over and less than 200. For the case of @xmath the removed
sources had @xmath and @xmath keV. While, for the case of @xmath
photons, the mean redshift and X-ray temperature were correspondingly
@xmath and @xmath keV.

In addition to a significant part of the original dataset being removed,
new clusters were added to the original dataset as described in detail
in Vergara-Cervantes [ 2019 ] . More specifically, new XCS cluster
candidates in the CFHTLenS footprint were analyzed. A cross-match
between the latest XCS master source catalogue and the CFHTLenS 3-D
matched-filter catalogue was performed in order to find clusters with
both the X-ray and the corresponding weak lensing data. Extended sources
with a photon count of @xmath were chosen for the updated dataset.
Combining the new cluster samples with the existing correctly classified
samples from the original dataset resulted in a dataset of 77 X-ray
selected, optically confirmed clusters in the CFHTLenS footprint. Note
that the new clusters were also chosen to satisfy the @xmath condition
in the same fashion as the majority of the clusters in the original
dataset (see figure 3.2 ). Figure 3.5 shows a comparison between the
original and the updated datasets in terms of the redshift and the X-ray
temperature distribution.

The updated dataset of the 77 clusters was then used to produce stacked
X-ray surface brightness and weak lensing profiles using an identical
procedure to the one used to produce the original 58 cluster stack.

Combining the non-cluster sources along with the genuine galaxy clusters
has multiple effects on the X-ray surface brightness and tangential
shear profiles. Specifically, a lot of the misclassified sources were
AGNs, which have different surface brightness profiles when compared to
clusters. In addition, such sources would not produce a shear signal
comparable to that of galaxy clusters. These two factors affect the
shape and the errors of the resulting stacked profiles, however it is
important to emphasize that the magnitude of such effect is limited due
to the averaging procedure. In other words, irregularities of the
individual sources are mostly averaged out during the stacking procedure
as long as the number of the misclassified sources is not dominant in
the dataset. In our case the resulting updated dataset profiles are
generally similar to the original dataset profiles. However, as
expected, removing the misclassified sources resulted in lower
tangential shear errors. Nonetheless, the updated dataset had less
clusters being stacked for the lower X-ray temperature bin @xmath keV,
which results in slightly higher error bars for the corresponding
surface brightness profile. Figures 3.3 and 3.6 show the X-ray surface
brightness and the tangential shear profiles for the original and the
updated datasets.

Another dataset that is important to discuss for comparison purposes is
the dataset produced in Wilcox [ 2016 ] . In particular, this dataset
includes simulated galaxy clusters produced using the MGENZO simulation,
which is an extension of the ENZO code allowing hydrodynamical
simulations with @xmath and scalar-tensor gravity [Bryan et al. , 2014 ]
. Two types of simulations were produced, one with the standard @xmath
CDM parameters (103 clusters) and the second one with an @xmath gravity
(99 clusters) with @xmath . Both simulations were run with @xmath
particles with @xmath mass and @xmath @xmath box size. The Rockstar
Friends-of-Friends (FOF) algorithm was then used to locate the main dark
matter haloes [Behroozi et al. , 2013 ] . The X-ray images were created
using the PHOX software, which is designed to obtain synthetic
observations from hydro-numerical simulations [Biffi et al. , 2011 ] .
Since the used simulation does not simulate the effects of lensing, the
expected convergence @xmath was estimated using the following equation:

  -- -------- -- --------
     @xmath      (3.45)
  -- -------- -- --------

where the Born approximation was used and the sumation is over the
co-moving distance @xmath , using bins of width @xmath and @xmath is the
overdensity and @xmath as the scale factor. Once the X-ray and the weak
lensing images were produced, they were stacked using an analogous
procedure to the one in Wilcox et al. [ 2015 ] to allow a detailed
comparison of the results. Figure 3.7 shows the described datasets.

#### 3.3.5 MCMC Fitting

The same general procedure was used to constrain the modified gravity
parameters in Terukina et al. [ 2014 ] , Wilcox et al. [ 2015 ] and our
approach described in this thesis. More specifically, the only free
parameters appearing in the surface brightness and the weak lensing
equations are @xmath (from the electron number density profile), @xmath
(the central temperature value) and @xmath and @xmath characterizing the
NFW density profile. Given that the dataset is split into two bins in
terms of X-ray temperature, the total set of free parameters includes:
@xmath . The superscript notation here refers to the two temperature
bins of @xmath and @xmath keV respectively. The last two parameters are
used to parametrize the modifications of gravity and refer to the
rescaled coupling constant and the field value at @xmath , such that:
@xmath and @xmath . The following priors were used when exploring the
parameter space: @xmath keV, @xmath @xmath , @xmath , @xmath Mpc, @xmath
@xmath , @xmath , @xmath keV, @xmath @xmath , @xmath , @xmath Mpc,
@xmath @xmath , @xmath , @xmath , @xmath .

The goodness of fit can then be quantified as follows:

  -- -------- -- --------
     @xmath      (3.46)
  -- -------- -- --------

where the total @xmath is split into contributions due to the two
temperature bins for the weak lensing and surface brightness datasets.
The goodness of fit components can be quantified by evaluating the
squared residuals between the predicted and the observed values divided
by the corresponding error values:

  -- -------- -- --------
     @xmath      (3.47)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.48)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.49)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (3.50)
  -- -------- -- --------

Here @xmath is the X-ray surface brightness at a perpendicular radial
distance from the cluster centre, @xmath refers to tangential shear,
@xmath is the corresponding error and @xmath refers to the components of
the covariance matrix. The covariance, in particular, is a measure of
how changes in one surface brightness bin affect the values in the other
bins. Following the assumption in Terukina et al. [ 2014 ] and Wilcox et
al. [ 2015 ] , the covariance matrix for the weak lensing dataset was
approximated as diagonal. More specifically, this choice was based on
the correlation matrices having dominant diagonal terms in all the
described weak lensing datasets.

The @xmath in equation 3.46 was then optimized using an MCMC sampler. In
particular, the Zeus sampler was used to find the optimal values of the
outlined free parameters [Karamanis and Beutler, 2020 ] . The sampler
was run using 42 walkers for 10000 steps with 4000 steps removed as
burn-in.

#### 3.3.6 Results

Table 3.2 summarizes the best-fit parameter values corresponding to the
fits in figures 3.3 and 3.6 . Similarly, the best-fit results from
Terukina et al. [ 2014 ] and Wilcox [ 2016 ] are given for comparison in
table 3.1 . The best-fit parameter values were generally found to be
degenerate in the sense that multiple combinations of the parameters can
lead to equivalent best-fit results.

The results for the constraints on the modified gravity parameters are
shown in figures 3.8 and 3.9 . These figures compare the modified
gravity constraints derived in this work against the previous works in
the literature. More specifically, figure 3.8 shows the comparison
against the results described in Terukina et al. [ 2014 ] and Wilcox et
al. [ 2015 ] . The contours in light and dark grey correspond to the
parameter space regions that are ruled out at 95% and 99% confidence
correspondingly, while the dashed and dotted contours are the
corresponding results from the previous work in the literature. The
vertical lines in all the plots correspond to the value of @xmath and
hence allow converting the constraints on the @xmath parameter to the
constraints on the @xmath parameter. More specifically, the portion of
the line that is in the allowed region of the parameter space gives the
allowed values of the @xmath parameter, which can be converted back to
@xmath and then to the @xmath by noting that: @xmath . Figure 3.9 shows
the corresponding comparison against the @xmath CDM and @xmath
simulation results from Wilcox [ 2016 ] . The contours correspond to the
ruled-out parameter space regions coming from the 103 and 99 cluster
stacks produced in @xmath CDM and @xmath simulations correspondingly.
The red points in both plots refer to the fiducial @xmath simulation
value of @xmath .

In general, the constraints derived in this work using the updated 77
cluster stack are similar to the previous results in the literature.
More specifically, comparing the new results against the constraints
derived using the original dataset of 58 clusters in Wilcox et al. [
2015 ] shows that both results are capable of ruling out a region of the
parameter space of nearly identical size. This is somewhat expected as,
even though the tangential shear errors are smaller in the updated
dataset, the surface brightness errors for the @xmath keV bin are
significantly larger due to a different number of clusters being stacked
in that bin. The main difference between our results and the original 58
cluster results is the fact that the triangular area of the ruled out
parameter values is shifted towards lower values of @xmath . The
triangular shape of the contours originates from the relationship
between the critical radius @xmath and the values of the coupling
constant @xmath and the @xmath , which is connected to the effectiveness
of the screening mechanism. More specifically, the critical radius is
given by [Wilcox et al. , 2015 ] :

  -- -------- -- --------
     @xmath      (3.51)
  -- -------- -- --------

where @xmath is the density at this particular radius. Hence for very
small values of @xmath , the deviations from GR are too insignificant to
be observed given the observational data errors. Similarly, as @xmath
increases, a lower value of @xmath is required to obtain @xmath that is
inside the cluster. This sets an upper limit on @xmath and results in
the triangular shape seen in all the result plots.

Our results derived from the updated cluster stack profiles offer some
of the most competitive constraints on cosmological scales. Due to the
contours being shifted towards the lower @xmath values, when compared to
the results for the Coma Cluster and the 58 cluster stack, the
constraints on the @xmath parameter are slightly weaker. Table 3.3
summarizes the constraints on the @xmath parameter from all the
previously mentioned works. In summary, our results agree well with the
previous work with the @xmath constraints being slightly weaker that
those in Terukina et al. [ 2014 ] and Wilcox et al. [ 2015 ] , but
stronger than those in Wilcox [ 2016 ] in the case of @xmath CDM.

The outlined results show that combining cluster X-ray and weak lensing
data and, in particular, stacking cluster profiles offers a reliable
technique of putting some of the strongest constraints on galaxy cluster
scales. It is, however, important to discuss the validity of the key
assumptions taken in this work. One of such assumptions was that galaxy
clusters are spherically symmetric. This, of course, is not valid for
real clusters, however, in our work we stack multiple clusters, which
averages out the deviations from spherical symmetry. A natural question
to ask then is how our results would be affected by introducing small
deviations from spherical symmetry. This was investigated in Terukina et
al. [ 2014 ] , where a small perturbation @xmath in the electron number
density profile is introduced:

  -- -------- -- --------
     @xmath      (3.52)
  -- -------- -- --------

where @xmath is the mean electron number value. Assuming that @xmath and
@xmath , allows us to express the effect on the X-ray surface
brightness:

  -- -------- -- --------
     @xmath      (3.53)
  -- -------- -- --------

Here @xmath is usually referred to as the clumping factor and it can be
estimated observationally. As an example for the cluster Abell 1835, the
clumping factor is @xmath [Morandi et al. , 2013 ] . This can then be
used to calculate the effect on the estimates of the hydrostatic mass
resulting in a factor of @xmath . In summary, the systematics from the
clumpiness of typical galaxy clusters can then be estimated to be of
order of a few @xmath . For a more accurate estimate a detailed study is
required using observational and simulation data, which is out of the
scope of this work.

Another key point to discuss is how the quality of the data affects our
constraints. More specifically, it is clear that in all of the discussed
datasets the weak lensing data is the dominant source of uncertainty.
This is the case, as measuring weak lensing is complicated and, even
after stacking a significant number of clusters, the errors are
relatively large when compared to the corresponding surface brightness
errors. In addition, even after stacking, multiple outlier points
remain. In order to investigate how these points affect our results we
tested removing the first few lensing data points that are closest to
the cluster center. We found that the outlier points did not have a
significant effect on the best-fit parameters and the related
constraints. To understand why, it is important to emphasize that we are
fitting all the four datasets simultaneously rather than each dataset
individually. Hence a small change in the best-fit profile of the shear
data does not have a significant effect on the corresponding
constraints.

### 3.4 Implications for the Gravitational Slip Parameter

#### 3.4.1 Gravitational Slip in Galaxy Clusters

The modified gravity tests described in the previous sections are model
dependent – i.e. the results depend on the model-specific assumptions
and generally cannot be easily converted to the corresponding
constraints on other models (except the @xmath model, which is directly
related to the chameleon scalar-tensor model via a conformal
transformation). Recently there has been a lot of interest in exploring
model-independent tests of modified gravity. These are tests that do not
depend on a specific model and can be used to constrain deviations from
GR in a way that allows to apply the constraints to a wide class of
models. One such way of testing modifying gravity in a model-independent
way is by measuring the gravitational slip parameter. This section
discusses how our techniques can be adapted to calculated the
gravitational slip parameter. In addition, the estimation of the
constraints on the gravitational slip is calculated using different DES
datasets.

As mentioned, one way to parametrize deviations from GR is via the
so-called gravitational slip parameter. The gravitational slip parameter
is defined as the ratio of the two gravitational potentials appearing in
equation 3.32 , @xmath . More specifically, the gravitational slip
parameter can be interpreted as the ratio between the effective
gravitational coupling of light to the coupling of matter. In GR, @xmath
(in the absence of anisotropic stress), however, in a large class of
modified gravity models the gravitational slip parameter deviates from
unity.

A detailed study of constraining the gravitational slip parameter using
simulated galaxy cluster data was done in Pizzuti et al. [ 2019 ] . This
work, in particular, studied the viability of constraining @xmath using
a combination of simulated strong and weak gravitational lensing data
along with the data from galaxy dynamics. The key point presented in
Pizzuti et al. [ 2019 ] is that deviation from @xmath in the context of
galaxy clusters is equivalent to the deviation between the dynamical and
the lensing cluster masses. The key concepts discussed in Pizzuti et al.
[ 2019 ] are summarized here in the context of our results presented in
the previous sections.

The two gravitational potentials used in the definition of the
gravitational slip parameter can be related to the properties of galaxy
clusters. For instance, the cluster galaxy dynamics can be described by
the Jeans equation:

  -- -------- -- --------
     @xmath      (3.54)
  -- -------- -- --------

where @xmath is the number density of tracers, @xmath is the velocity
dispersion along the radial direction and @xmath , with @xmath and
@xmath as the velocity dispersion along the angular directions. The
potential @xmath is given by the Poisson equation:

  -- -------- -- --------
     @xmath      (3.55)
  -- -------- -- --------

with @xmath as the total mass density in a cluster (dominated by the
dark matter, gas and galaxy mass components). The total mass enclosed in
some radius @xmath is simply:

  -- -------- -- --------
     @xmath      (3.56)
  -- -------- -- --------

The gravitational potential @xmath can then be expressed as:

  -- -------- -- --------
     @xmath      (3.57)
  -- -------- -- --------

where we identified the total mass as the mass measured by galaxy
kinematics.

A similar expression can be found for the lensing mass. In this case,
the geodesics of light respond to the sum of the two mentioned
potentials: @xmath , such that:

  -- -------- -- --------
     @xmath      (3.58)
  -- -------- -- --------

The @xmath term here is the density corresponding to the lensing mass,
which is given by:

  -- -------- -- --------
     @xmath      (3.59)
  -- -------- -- --------

Equations 3.58 and 3.57 allow us to express the @xmath potential as:

  -- -------- -- --------
     @xmath      (3.60)
  -- -------- -- --------

Finally, this allows expressing the gravitational slip parameter in
terms of the lensing and dynamical mass of a galaxy cluster:

  -- -- -- --------
           (3.61)
  -- -- -- --------

This expression shows that the gravitational slip essentially quantifies
the deviation between the lensing and the dynamical masses at different
radii. Note that the gravitational slip parameter can be easily related
to the hydrostatic mass inferred from the intra-cluster gas studies, by
noticing that the hydrostatic equilibrium equation (eq. 2.8 ) is related
to the gravitational potential @xmath . In other words, the same
potential that dictates the cluster galaxy kinematics also affects the
intra-cluster gas, allowing us to treat the dynamical mass in equation
3.61 as equivalent to the hydrostatic mass, which is equal to the sum of
@xmath . Replacing @xmath with @xmath in equation 3.61 allows us to
calculate the gravitational slip parameter following an approach similar
to the one described in the previous sections in this chapter. More
specifically, the gravitational slip parameter is directly related to
the validity of the hydrostatic equilibrium, i.e. the equivalence of the
lensing and the hydrostatic masses in a galaxy cluster.

#### 3.4.2 Constraining the Deviations from the Hydrostatic Equilibrium
and the Gravitational Slip Parameter

The question of the validity of the hydrostatic equilibrium assumption
has been investigated in detail in Terukina et al. [ 2014 ] and Wilcox
et al. [ 2015 ] . In particular, cluster masses inferred by the weak
lensing data were compared against the masses inferred from the X-ray
data. The results are shown in figure 3.10 . The results show a general
agreement between the gas and the weak lensing masses for the full
radial range covered by the dataset. An important conclusion that can be
drawn from figure 3.10 is that the non-thermal pressure effects are
increasingly more important at large radii. However, if the non-thermal
term is added, given the weak lensing errors, there is a good agreement
between the lensing and the gas masses, justifying the hydrostatic
equilibrium assumption.

The mass profiles shown in figure 3.10 can be used to calculate the
gravitational slip parameter. In particular, @xmath is proportional to
the integrated difference between the two mass profiles, leading to the
results shown in figure 3.11 . The results indicate that the mean value
of @xmath approaches 1 only at high @xmath values. However, given the
dominant weak lensing errors, the value of @xmath is well within the
allowed region. This illustrates the two key issues when measuring
@xmath using galaxy cluster data. First, the available weak lensing data
has very high error bars, which result in poor constraints for the
gravitational slip. In addition, the complex astrophysics happening in
the core regions of clusters complicate the different mass estimates in
those regions, hence the validity of hydrostatic equilibrium in the
inner region of clusters has been debated extensively (eg. see Fabian [
1992 ], Peterson and Fabian [ 2006 ], Fujita and Ohira [ 2011 ] ).

An interesting question to ask is whether stacking more clusters for the
X-ray and the weak lensing profiles would bring the gravitational slip
constraint errors close to those predicted by simulated data. This,
however, is complicated by the lack of access to cluster data that
contains both high-quality X-ray and weak lensing information. Hence, in
order to estimate the errors on the gravitational slip resulting from a
larger weak lensing dataset (which is dominates the errors in figure
3.11 ), a significantly higher number of clusters were stacked when
calculating the tangential shear profile. For this estimation, instead
of the CFHTLenS data, the DES year 1 cluster catalogue was used hoping
to get smaller weak lensing errors. In particular, DES Y1 Gold catalogue
was used, which includes measurements of 137 million objects in 5
filters over 1800 square degrees of the sky as described in full detail
in Drlica-Wagner et al. [ 2018 ] . The clusters were chosen such that
the resulting dataset would be as similar as possible to the original 58
XCS-CFHTLenS clusters. In particular, as shown in figure 3.5 , the
original 58 cluster dataset span redshifts between @xmath and
temperatures between @xmath , hence the new clusters were chosen from
the DES data to have a similar redshift and temperature distribution. In
addition, cluster richness was taken into account, by removing clusters
that had richness significantly higher/lower than the mean richness of
the original 58 cluster stack. More specifically, values larger or
smaller than the mean value by 50% were removed from the list. Other
values, such as 25% and 75% were tried, but the effect on the resulting
weak lensing profiles was not significant. A list of @xmath and @xmath
clusters was then chosen in a way that the mean redshift and temperature
values were as close as possible to @xmath and @xmath keV (i.e. the
median values of the original dataset). Finally the results were split
into two temperature bins as before.

Once a list of clusters was obtained, the tangential shear profiles were
obtained by using xpipe , which is a software package that automates the
pipeline for producing weak lensing profiles using DES data [Tamas N.
Varga and Maria E. S. Pereira, 2020 ] . More specifically, xpipe
automates the stacking procedure by calculating the @xmath and @xmath ,
rescaling each cluster to the same size, calculating the tangential
shear and averaging the results in each radial bin. This resulted in two
tangential shear profiles for @xmath @xmath and @xmath @xmath clusters
with errors significantly lower than those shown in figure 3.3 .

Finally, the gravitational slip parameter was estimated in two different
ways. In the first instance, we used the X-ray data from the 58 cluster
stack ( @xmath keV bin) and the corresponding 58 cluster weak lensing
data, however, the lensing error bars were rescaled by a factor
determined from the @xmath and @xmath cluster stacks from the DES data.
In more detail, this was done by splitting the 100 and the 1000 cluster
stacks into radial bins and calculating the mean error bar size in each
bin, which was then compared against the original 58 cluster stack error
bars. The original dataset error bars were then rescaled by that factor
to estimate how the errors would be reduced by stacking a significantly
higher number of clusters. The second calculation was done by using the
original 58 cluster X-ray data, but the weak lensing data was replaced
entirely by the new weak lensing shear profiles. The obtained results
were compared against the previously mentioned simulation results from
Pizzuti et al. [ 2019 ] , where simulated data was used to derive the
gravitational slip estimate from the @xmath and @xmath masses. Figure
3.12 summarizes the results. In summary, the results indicate that
stacking 1000 galaxy clusters significantly improves the gravitational
slip constraints. Here it is important to emphasize that these
constraints should not be taken as a rigid estimation of @xmath , but
rather as a back of an envelope estimation of the associated errors, as
we are using different clusters for the X-ray and the shear data. The
results however are a good estimate of how competitive the constraints
can become once high quality X-ray and weak lensing data becomes
available from DES and future surveys. In comparison, the simulation
results from Pizzuti et al. [ 2019 ] are much stronger; however, it is
important to note that these results were produced using different
methodology and using simulated data that is significantly less noisy
than observational data. In order to improve our estimates in figure
3.12 , more high-quality X-ray data with the weak lensing counterpart is
required from the newest DES data and future X-ray surveys.

## Chapter 4 Testing Emergent Gravity on Galaxy Cluster Scales

Chapter 4 introduces a novel test of the theory of emergent gravity
using the data described in chapter 3 . The methods and techniques
described in chapter 3 are adapted to test a different type of a model
hence illustrating that our tests can be generalized quite easily. In
addition, a more detailed discussion of the various assumptions and
systematics is given.

The material in this chapter is primarily based on the results obtained
in Tamosiunas et al. [ 2019 ] . Note that at the time of writing the
paper, the newest 77 cluster stack data was not yet available. Hence,
the results presented in this chapter were produced using the Coma
Cluster data as described in Terukina et al. [ 2014 ] . The cluster
stack results were produced using the original 58 cluster stack produced
using CFHTLenS and XCS data as described in Wilcox et al. [ 2015 ] .
Reproducing the tests with the newest 77 cluster stack data is left for
future work.

### 4.1 Motivations for Emergent Gravity

Emergent gravity (EG) refers to a family of theories united by a common
principle that gravity can be described as an emergent phenomenon. Ideas
of such kind date back to the early work of Jacob Bekenstein, who, in a
great stroke of ingenuity in 1973, demonstrated that black holes are
thermodynamical objects with an entropy proportional to the area of the
event horizon:

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

with @xmath as the Boltzmann constant, @xmath as a numerical constant,
@xmath as the area of the event horizon, and @xmath as the @xmath
-dimensional Planck length [Bekenstein, 1973 ] . Soon after the results
by Bekenstein were published the numerical constant was determined by
Hawking to have the value of @xmath [Hawking, 1974 , 1975 ] . This
insight was shown to be far from just a mere analogy, as illustrated by
the later work of Bekenstein, Hawking and others, which established a
clear link between black hole physics and classical thermodynamics. In
fact, one can define consistent laws of black hole mechanics which are
analogous to the four laws of thermodynamics. The four laws can be
summarized as follows [Wald, 2010 , Carlip, 2014 ] :

1.  The event horizon has a constant surface gravity @xmath (for
    stationary black holes). In this respect a stationary black hole is
    comparable to a body in thermal equilibrium and @xmath is comparable
    to temperature @xmath in classical thermodynamics.

2.  The change in energy of a black hole is proportional to the changes
    to the area of the event horizon, the angular momentum and the
    electric charge, i.e.:

      -- -------- -- -------
         @xmath      (4.2)
      -- -------- -- -------

    where @xmath is the surface gravity, @xmath is the area of the event
    horizon, @xmath is the angular velocity, @xmath is the angular
    momentum, @xmath is the electrostatic potential and @xmath is the
    charge. As before, this law can be related to the first law of
    classical thermodynamics, which stems from the conservation of
    energy.

3.  The area of the event horizon does not decrease with time:

      -- -------- -- -------
         @xmath      (4.3)
      -- -------- -- -------

    Hence the event horizon area is analogous to entropy in the second
    law of thermodynamics. It is important to note that later work by
    Hawking showed that black holes radiate, which over long enough
    periods of time can eventually lead to the decrease of the event
    horizon area.

4.  Black holes cannot have @xmath . This indicates that one cannot
    produce black holes with naked singularities. The analogy with the
    classical thermodynamics here is a bit more subtle, but it turns out
    that this is indeed equivalent to the third law in classical
    thermodynamics, which states that as @xmath entropy is a well
    defined constant.

The exact nature of this correspondence between black hole and classical
thermodynamics has been a matter of an ongoing debate since the
formulation of the 4 laws. One fascinating possibility is that the
analogies laid out in the laws above hint towards certain fundamental
properties of gravity and spacetime. For instance, in classical
thermodynamics it has been long known that macroscopic quantities
ultimately have a microscopic nature. As an example, the temperature of
a body can be directly related to the energy stored in the discrete
microscopic degrees of freedom. Realizations of such kind historically
led to the discovery of the discrete atomic nature of matter. Given the
deep connections between gravitational systems and thermodynamics, as
outlined above, a natural question to ask is whether spacetime itself
has an underlying microscopic structure (sometimes referred to as atoms
of spacetime ). Suspicions of such kind have only been strengthened by
the discovery of the Fulling-Davies-Unruh effect, which allows
accelerating observers to observe the vacuum as having a well-defined
temperature [Fulling, 1973 , Davies, 1975 , Unruh, 1976 ] . These
observations have inspired a family of different approaches, which treat
gravity as a phenomenon that emerges from the underlying microscopic
dynamics that obeys the laws of thermodynamics.

The research program of EG has resulted in a number of important
breakthroughs in our understanding of the fundamental nature of gravity.
A prime example of this is the result by Jacobson, which demonstrates
that the Einstein field equations can be derived starting from general
considerations related to the entropy-area relation (equation 4.1 )
[Jacobson, 1995 ] . More generally, later results by Padmanabhan showed
that it is possible to derive the field equations for a large class of
gravitational theories from the thermodynamic extremum principle
[Padmanabhan and Paranjape, 2007 , Padmanabhan, 2008 ] . These and other
recent successes of the emergent paradigm are discussed in full detail
in Padmanabhan [ 2015 ] .

### 4.2 Verlinde’s Emergent Gravity

#### 4.2.1 The Predictions of the Model

One of the most recent additions to the family of emergent theories has
been proposed by Eric Verlinde. Here we will lay out some of the key
results of this approach primarily based on Verlinde [ 2017 ], Brouwer
et al. [ 2017 ], Tamosiunas et al. [ 2019 ] . From hereon, EG will refer
specifically to the approach introduced in Verlinde [ 2011 ] and
Verlinde [ 2017 ] . The figures and the results described in this
chapter are the author’s own (as described in Tamosiunas et al. [ 2019 ]
) unless specified otherwise.

In Verlinde [ 2011 ] Newton’s laws are derived starting from general
considerations in statistical mechanics and the holographic principle.
In particular, the mentioned work shows that gravitation can be
described as an entropic force arising from the changes in the
information associated with the positions of material bodies in a
gravitational system. In addition, Verlinde specifies a method for
deriving Einstein’s field equations from general considerations along
the same lines.

A key notion in such an emergent description of gravity is the
holographic principle, which states that the information in a volume of
space can be thought of as encoded on a lower-dimensional boundary to
the region. This principle has been originally inspired by the insight
in black hole thermodynamics, that the information about the objects
that have fallen into the hole might be stored entirely in the surface
fluctuations of the event horizon [Susskind, 1995 ] . A prime example of
an application of the holographic principle is the AdS/CFT
correspondence, which refers to a certain duality between string theory
models described in anti-de Sitter space and conformal field theories
[Maldacena, 1999 ] . In his work Verlinde uses the holographic principle
as a tool for relating changes in the configuration of masses to the
corresponding change in entropy. In particular, the holographic
principle allows us to generalize the ideas used in black hole
thermodynamics to other gravitational systems. As a concrete example,
one of the main results in Verlinde [ 2011 ] shows how changes in the
entropy of a gravitational system can be related to the changes in the
gravitational potential acting on a test mass near a spherical mass
distribution enclosed by a holographic screen:

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath is the change in entropy, @xmath is the number of bits of
information stored on the holographic screen bounding the system, @xmath
is the change in the gravitational potential and @xmath and @xmath are
the Boltzmann constant and the speed of light.

The more recent proposal by Verlinde extends these ideas in an attempt
to describe gravity as an emergent force in cosmological scenarios
[Verlinde, 2017 ] . As previously discussed, the entropy-area
relationship is of monumental importance in the EG model and can be used
to derive the familiar laws of gravity (e.g. the Einstein field
equations). However, in Verlinde [ 2017 ] the author argues that due to
the presence of positive dark energy in our Universe an extra
contribution to the total entropy ¹ ¹ 1 Technically, this is the
entanglement entropy of the underlying microscopic degrees of freedom
(see Verlinde [ 2017 ] for a more detailed explanation). From hereon the
terms entropy and entanglement entropy are used interchangeably. in the
form of a volume law must exist. In particular, Verlinde argues that
modifying the entropy-area relationship leads to extra gravitational
effects that become important on scales set by the Hubble acceleration
scale: @xmath . Another key achievement in Verlinde [ 2017 ] is
extending a number of important ideas in the EG paradigm, which are best
described in anti-de Sitter space, to a more cosmologically realistic de
Sitter space.

In terms of extra gravitational effects, Verlinde shows that introducing
a central baryonic mass distribution on galaxy and galaxy cluster scales
results in the reduction of the total entanglement entropy of the
system, which is equivalent to extra gravitational effects (i.e. a force
pointing towards the matter distribution (see figure 4.1 )). These extra
gravity effects are comparable in size to the effects usually associated
with those of cold dark matter.

The entropy change in a spherical system caused by introducing a
spherical central distribution of baryonic matter @xmath can be
expressed through the displacement field @xmath , such that:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

where @xmath is the amount of displaced entropy, @xmath is the surface
area of the system, @xmath is the gravitational constant, @xmath is the
speed of light, @xmath is the reduced Planck constant and @xmath is the
current value of the Hubble parameter.

To fully describe the entropy displacement effect by baryonic matter
Verlinde draws a useful analogy with the effects of inclusions in
elastic materials as described by the linear theory of elasticity. This
seemingly random connection turns out surprisingly useful in calculating
the changes in entropy caused by inclusions of baryonic matter in
gravitational systems. Namely, introducing inclusions into elastic
materials causes strain @xmath , which can be related to the change in
entropy of the system. In Verlinde [ 2017 ] the author notices that the
effects of inclusions in elastic materials share certain similarities
with the effects of baryonic matter distributions on the entanglement
entropy in de Sitter space. An elasticity/gravity correspondence ² ² 2
For a better understanding of this correspondence see table 1 in
Verlinde [ 2017 ] is then established to derive the exact result for the
extra gravitational effects due to entropy displacement. Note that
similarities between the theory of elasticity and gravity has been
studied previously in some detail in the literature. As an example, in
1967 Sakharov introduced the idea of induced gravity , which argues that
gravitation emerges from quantum field theory in roughly the same sense
that hydrodynamics or continuum elasticity theory emerges from molecular
physics [Visser, 2002 ] . More recently, Padmanabhan discusses a similar
approach treating gravity as elasticity of spacetime [Padmanabhan, 2004
] .

If spacetime in our system is mathematically treated as an
incompressible elastic medium, the strain caused by the baryonic matter
@xmath is then given by:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where @xmath is the area of a sphere we are integrating over and @xmath
is a quantity related to the amount of entropy displaced by the baryonic
matter distribution @xmath ³ ³ 3 Note that @xmath is equal to the volume
that would contain the amount of entropy that is removed by a mass
@xmath inside a sphere of radius @xmath , if that volume was filled with
the average entropy density of the universe (see Brouwer et al. [ 2017 ]
for a wider discussion). and is given below in equation 4.8 . In
Verlinde [ 2017 ] it is shown that in de Sitter space @xmath , which
leads to @xmath being given by:

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where @xmath refers to the apparent dark matter distribution ⁴ ⁴ 4 Here
we want to emphasize that in EG, there is only baryonic matter. However,
gravity acts differently on large scales, which can be modeled as a
consequence of an effective extra mass distribution, here called @xmath
. The effects of @xmath can then be compared against those of dark
matter in standard cosmology. . The @xmath term is given by:

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

Substituting equations ( 4.7 ) and ( 4.8 ) into ( 4.6 ) and integrating
leads to the main result which is tested in this chapter:

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

where @xmath is the apparent dark matter mass enclosed in @xmath and
@xmath is the baryonic mass. This can be interpreted as an effective
dark matter distribution caused by gravity acting differently on large
scales, rather than a new form of matter as in the @xmath CDM framework.
Hence Verlinde’s EG offers an alternative solution to the problem of
dark matter.

This result has a number of interesting consequences. For instance,
computing the total acceleration due to @xmath and @xmath , assuming
that the baryonic mass is concentrated in the centre, leads to the
result below that agrees well with the baryonic Tully-Fisher relation as
seen in MOND-like theories:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

with @xmath as the scale familiar from modified Newtonian dynamics
[Milgrom and Sanders, 2016 ] . Similarly, applying equation 4.9 , for
extended mass distributions in galaxy clusters, highly reduces the
missing mass problem, hence possibly offering an alternative to dark
matter on galaxy and cluster scales.

The points outlined above illustrate that the EG model is of special
interest in the context of dark matter on galaxy and galaxy cluster
scales. In the point mass approximation limit, EG reproduces the
original MOND predictions, while still leading to unique results for
more general mass distributions. This is an attractive feature of the
model, as it could potentially resolve some of the issues of the
non-relativistic MOND framework, such as generally poor fit to data on
galaxy cluster scales.

The result in equation 4.9 offers a testable prediction for a ratio
between the dark matter and baryonic matter mass distributions with no
free parameters. The rest of this chapter is dedicated to introducing a
novel test of this relation on galaxy cluster scales. In addition, we
will review the current theoretical criticisms and observational
constraints of Verlinde’s theory. However, before introducing the
methods for testing this relation, it is worthwhile to lay out all the
key assumptions under which equation 4.9 is valid.

#### 4.2.2 The Main Assumptions

The current predictions of EG are valid only under a certain set of
assumptions. Here I will list those key assumptions in the context of
the observational data that is used to test the model:

-   The EG predictions are only applicable for approximately spherically
    symmetric, sufficiently isolated and non-dynamic mass distributions.
    This means that, for instance, the Bullet Cluster would not be a
    valid test case for EG. This is rather unfortunate, as the Bullet
    Cluster offers a perfect test case for theories that predict a
    scaling relationship between the dark matter and baryonic matter
    distributions. Figure 2.1 clearly shows the bulk of the baryonic
    matter being centered in different parts of the merging cluster
    system when compared against the total dark matter distribution.
    This is clear evidence against scaling relations of the form of
    equation 4.9 . However, given that the EG prediction was derived
    assuming spherical symmetry and the mass distributions being
    approximately static, merging cluster systems, such as the Bullet
    Cluster, cannot be used to test this particular model.

-   Since there is no rigid description of cosmology in EG yet, all the
    equations are only valid for the current value of the Hubble
    parameter, @xmath (i.e. @xmath will be approximated as @xmath and
    only small redshift clusters will be considered). This also implies
    that Verlinde’s theory is not capable of addressing such phenomena
    as the CMB and structure formation. However, note that more recent
    EG approaches, such as Hossenfelder’s covariant approach (see
    Hossenfelder [ 2017 ] and section 4.5 ), could in principle address
    the CMB.

-   There is also no geodesic equation in EG as of yet, so a crucial
    assumption will be made that weak lensing works in EG the same way
    as in GR. In particular, following the work in Brouwer et al. [ 2017
    ] , it will be assumed that the extra gravity effects predicted by
    the model affect the paths of photons in the same way as dark matter
    does in GR. In turn, this implies that dark matter is distributed
    according to equation 4.9 , which allows us to derive the weak
    lensing predictions. Future theoretical and observational work will
    be required to test the validity of this assumption.

-   As discussed in [Brouwer et al. , 2017 ] , the effects of EG are
    only expected to become important in the regime where the volume law
    contribution to the total entropy ( @xmath ) is significantly larger
    than the entropy displaced by baryonic matter @xmath . This,
    following equation 18 in Brouwer et al. [ 2017 ] , is expressed by
    introducing a minimal radius, @xmath , above which we expect the EG
    effects to become noticeable, as described by the following
    inequality:

      -- -------- -- --------
         @xmath      (4.11)
      -- -------- -- --------

    Solving equation 4.11 gives the value for @xmath . Table 4.1 lists
    the typical values for @xmath for various systems of different sizes
    and masses.

### 4.3 Testing Emergent Gravity

#### 4.3.1 Testing Emergent Gravity with the Coma Cluster

Galaxy clusters, being the largest gravitationally bound systems, offer
a natural setting for testing models of gravity. Having regions of high
and low density as well as a mass distribution dominated by dark matter,
clusters have been used extensively for testing models with screening
mechanisms and comparing the predictions with general relativity. In
this section an approach similar to the one developed in Terukina et al.
[ 2014 ] and Wilcox et al. [ 2015 ] is used, where chameleon and @xmath
gravity models were tested in the Coma Cluster as well as a 58 cluster
stack coming for CFHTLenS and XCS surveys. More specifically, in these
works multiple probes are used to constrain the modified gravity effects
in the outskirts of galaxy clusters under the assumption of hydrostatic
equilibrium.

Here we use the intracluster gas temperature profile to determine the
baryonic mass distribution in the Coma Cluster and to calculate the
predicted weak lensing signal, which is then compared with the actual
weak lensing data. The same procedure is done for the standard model (GR
+ cold dark matter described by a Navarro-Frenk-White profile) and the
EG model. The results are then compared in terms of the @xmath and the
Bayesian information criterion (BIC) values.

The Coma Cluster (Abel 1656) is a large well-studied nearby ( @xmath )
galaxy cluster with over 1,000 identified galaxies [Gavazzi et al. ,
2009 ] . The cluster has an extensively-studied mass distribution and
has been the subject of numerous weak lensing and X-ray studies. Figure
4.2 shows the underlying distribution of the Coma Cluster as seen in SZ
and X-ray data. It is important to note that the cluster is not entirely
spherical and evidence of substructures of various sizes can be seen
distributed around the main mass distribution of the cluster.

The equations below illustrate how the temperature profile of the Coma
Cluster can be used to determine the total mass distribution and, in
turn, to calculate the predicted weak lensing signal. Assuming
hydrostatic equilibrium we can relate pressure to the mass using the
same equation as described in chapter 3 :

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

where @xmath is the gas density, @xmath is the total pressure and @xmath
is the mass enclosed in radius @xmath . This allows us to calculate the
gas temperature corresponding to the thermal pressure term by using the
ideal gas law: @xmath . Integrating equation 4.12 gives:

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

where we switched to electron number density and @xmath is the mean
molecular weight, @xmath is the proton mass, @xmath is the electron
number density and the last term, the central pressure, is an
integration constant. As before, for a fully ionised gas, the mean
molecular weight is given by @xmath . The non-thermal pressure terms can
be derived in the same manner (as already discussed in section 3.3.1 ).
Equation ( 4.13 ) then allows us to determine the underlying mass
distribution in the Coma Cluster, given that we have a way to measure
the temperature accurately.

In this work we adopted the standard beta-model electron density profile
[Cavaliere and Fusco-Femiano, 1976 ] . The baryonic mass distribution is
then given by:

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

where we summed the total stellar galaxy mass with the intracluster gas
mass and @xmath is the average mass of an atom in the cluster gas, given
by @xmath where @xmath is the Hydrogen mass and @xmath is the mass
fraction of the Hydrogen atoms.

In order to estimate the galaxy mass distribution in the Coma Cluster,
we queried the SDSS data catalogue (Data Release 14) for the median
estimate of the total stellar masses of galaxies located within the 180
arcminute diameter around the central point of the cluster for @xmath
[Pâris, Isabelle et al. , 2018 ] . This region was then split into
radial bins of 5 arcminutes, and for each cylindrical shell we summed
the stellar masses for all the detected galaxies. This results in a
galaxy mass distribution in a spherical region of @xmath Mpc around the
centre of the cluster. Figure 4.3 shows the results for the galaxy mass
distribution. Summing the stellar galaxy and the X-ray emitting gas mass
distributions gives a good measure of the total baryonic mass
distribution, which can then be used to calculate the total mass
distribution using eq. ( 4.9 ). Finally, having obtained the total mass
distribution for the cluster, we have all that is needed to compute the
weak lensing predictions.

In order to compare the predictions from EG with those from standard
cosmology (GR + dark matter), we chose to describe the dark matter
distribution in the cluster by the NFW profile:

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

where @xmath is the characteristic density and @xmath is the
characteristic scale [Wright and Brainerd, 1999 ] . This was then used
to calculate the total mass in the cluster and, in turn, to predict the
weak lensing profile. Note that in the case of EG, the model assumes
only the existence of baryonic matter and the apparent dark matter
effects are fully described by equation 4.9 .

Following the approach taken by Brouwer et al. [ 2017 ] and using the
equations described in Wright and Brainerd [ 1999 ] , we calculated the
weak lensing profiles as follows:

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

where @xmath is the tangential shear, while @xmath and @xmath are
correspondingly the surface density and critical surface density (see
section 2.4 for more information). The surface density of a given radial
density distribution is given by:

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

where we switched to cylindrical coordinates ( @xmath , @xmath , @xmath
) centered on the central point of our cluster. @xmath for both baryonic
and apparent dark matter can be calculated using the general expression:

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

In the case of EG, the shear equations are then given by:

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

where we have split @xmath into contributions from baryonic and apparent
dark matter for the surface density.

Having laid out the main equations at this point it is worth noticing
that in order to derive the total mass distribution of the cluster we
need to choose a way of parametrizing the electron number density @xmath
. As previously, this is done by using the simple isothermal beta
profile of the following form: @xmath . The only free parameters for the
EG model appearing in the equations above are then @xmath , @xmath ,
@xmath and @xmath (central temperature). On the other hand (given our
assumption that dark matter is distributed according to the NFW
profile), for the GR model we have the following free parameters: @xmath
, @xmath , @xmath and @xmath , @xmath and @xmath (where the last two
parameters refer to concentration and the mass enclosed by @xmath ). The
values for the free parameters were then obtained by looking for
solutions that fit the temperature profile data and, at the same time,
produce weak lensing predictions which agree well with the observational
data (in other words, both datasets were fit simultaneously by
minimizing the combined value of @xmath ). The data used included the
X-ray temperature profile (combined from Snowden et al. [ 2008 ] and Wik
et al. [ 2009 ] ) and the weak-lensing profile ( Gavazzi et al. [ 2009 ]
, Okabe et al. [ 2010 ] ) of the Coma Cluster.

The data fitting was performed by minimizing the combined residuals
using the limited memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)
algorithm available from the SciPy python library [Virtanen et al. ,
2020 ] . The 1- @xmath confidence intervals were determined using the
in-built features of the SciPy.optimize library, which use the estimated
inverse Hessian matrix to calculate the standard deviation of each
best-fit parameter. The @xmath values were calculated using the standard
formula: @xmath , where @xmath refers to the calculated values, @xmath
to the observed values, @xmath to the variance at a given data point.
The covariance matrix here was assumed to be diagonal, however, in the
case of the cluster stack data, we used the full covariance matrix. The
best-fit results for the standard model (GR + dark matter) and EG
results are summarized in table 4.2 and figure 4.4 . The goodness of fit
statistics are given in table 4.3 .

The Coma Cluster results above indicate that EG is capable of producing
fits that are generally comparable to the standard model fits and are in
agreement with the observational data (within the shown uncertainties).
The best-fit parameters from the gas temperature and the weak lensing
data then result in mass distributions for the two models that are in
agreement for @xmath kpc @xmath 700 kpc. The calculated mass
distributions can be compared with the other results in the literature,
such as Brownstein and Moffat [ 2006 ] , where the total mass profile
was determined using X-ray data or Lokas and Mamon [ 2003 ] , where
elliptical galaxy velocity moments were used instead. In general, our GR
+ dark matter profile, within the given uncertainties, is in good
agreement with the mentioned results from the literature, with the
exception of around @xmath Mpc, where the profiles in the mentioned
papers fall between our EG and GR results. Overall this indicates that
the EG result underestimates the total mass distribution for @xmath kpc
and overestimates it for @xmath kpc given 1- @xmath confidence. The
values in table 4.3 indicate that, despite requiring more free
parameters, GR is still the preferred model according to the @xmath
analysis. The BIC analysis is not conclusive, with both models having
very similar BIC values. This is the case, as, even though EG has a
poorer fit to the temperature data, it has significantly better fit to
the weak lensing data and less free parameters.

There are, however, a number of important issues that need to be
discussed in terms of using the Coma Cluster data for testing EG. In
particular, as is shown in figure 4.2 , the cluster is not exactly
spherical and is not completely isolated for external mass
distributions. Recent investigations in the structure of the Coma
Cluster show that Coma is a typical example of a @xmath cluster in terms
of its internal kinematics. However, the X-ray temperature in the
cluster has been shown to be significantly higher than in a sample of
clusters of similar masses [Pimbblet et al. , 2014 ] . In addition, as
shown in figure 4.4 , the data for the Coma Cluster is limited,
especially for the weak lensing profile. This is generally true when
using single galaxy cluster data, as acquiring high accuracy weak
lensing data is difficult. Hence, in order to avoid various biases and
problems due to non-spherical symmetry, a test of the EG model with the
data coming from 58 stacked galaxy clusters is introduced in the next
section.

#### 4.3.2 Testing Emergent Gravity with Stacked Galaxy Clusters

Stacking multiple galaxy clusters allows us to form a dataset that is
representative of typical galaxy cluster properties at a given redshift.
Figure 4.5 illustrates the effects on the projected mass distribution
due to stacking 50 galaxy clusters.

To mitigate some of the mentioned issues with using the data from the
Coma Cluster and to test the effects of EG with a larger sample of
galaxy clusters we followed an approach similar to that taken in chapter
3 , where 58 clusters with redshifts ranging between @xmath were stacked
using X-ray (from the XMM Cluster Survey) and weak lensing data (from
the Canada France Hawaii Telescope Lensing Survey) Mehrtens et al. [
2012 ], Erben et al. [ 2013 ] . Stacking clusters in such a way averages
away most irregularities in shape and density and provides an
approximation to an average galaxy cluster. In addition, the signal to
noise ratio is improved. More importantly, stacking multiple
well-isolated low redshift clusters produces a perfect dataset to test
the predictions of EG.

The dataset used to test EG consisted of the original 58 cluster stack,
as described in detail in chapter 3 and in Wilcox et al. [ 2015 ] . The
cluster stack has a number of important properties in the context of the
assumptions under which EG predictions are significant. In particular,
most galaxy clusters in the dataset are isolated from the other nearby
mass distributions (see figure 4 in Wilcox et al. [ 2015 ] ). In
addition, our dataset consists of clusters with a mean redshift of
@xmath , justifying the assumption that we can neglect the effects of
varying the Hubble parameter @xmath in our test. Finally, the cluster
stack has been binned in terms of temperature, to approximately separate
it into galaxy groups and galaxy clusters. This allows us to investigate
how well the theory in question works for objects of significantly
different masses.

In order to determine the galaxy mass distribution for our cluster
stack, we queried the CFHTLenS survey catalog (as described in Erben et
al. [ 2013 ] ) for each individual cluster following a similar procedure
as before for the Coma Cluster (the main difference being that we
adjusted the angular region that we queried based on the distance to
each cluster). In particular, for each cluster the galaxy stellar masses
were summed in concentric cylindrical shells (allowing the redshift to
vary in each direction by @xmath ). The results for each cluster were
then linearly fitted and extrapolated to cover the same range. Finally,
we then averaged over the masses for each cluster for each value of
radii to determine the mean galaxy mass and the corresponding
uncertainty. Figure 4.6 illustrates the procedure and the obtained
results.

As previously, we used equation 4.16 to calculate the tangential shear
profiles. In order to determine the underlying baryonic mass
distribution the projected surface brightness data was fit by using
equation 3.44 . This then results in the same free parameters of @xmath
, @xmath , @xmath and @xmath for EG plus and extra two free parameters
@xmath and @xmath for the standard model (GR + dark matter) due to the
assumption of the NFW profile. The free parameters were determined by
simultaneously fitting the surface brightness and the weak lensing
datasets. The total mass profiles were then calculated using the
obtained best-fit parameters. The results were compared with the
analogous results calculated in GR. The best-fit was performed using the
non-linear least-squares minimization using the python LmFit library
[Newville et al. , 2014 ] . In particular, the Levenberg–Marquardt
algorithm was used to determine the best-fit parameter values along with
the corresponding confidence limits.

The goodness of fit was evaluated by following the approach taken in
chapter 3 and the appendix A in Wilcox et al. [ 2015 ] . In particular,
for the weak lensing data we approximated the covariance matrix as
diagonal. For the surface brightness data the covariance matrix was
included in the @xmath calculations to account for the correlations
between the surface brightness radial bins. The results (split into two
temperature bins) are summarized in table 4.4 . The goodness of fit
statistics are summarized in table 4.5 .

Figure 4.7 shows the results for the clusters with temperatures higher
than 2.5 keV, which roughly corresponds to galaxy clusters (rather than
galaxy groups). In this case the surface brightness fits are comparable
for both models. However, the tangential shear profile fit in EG is
significantly worse than the corresponding GR result. In general we
found that EG could not simultaneously fit both datasets with accuracy.
In other words, if we want to fit the surface brightness profiles
accurately for @xmath , the resulting tangential shear profile will have
a gradient that is too large to agree with the observational data for
large values of @xmath . This results in the total mass distributions in
EG and GR that agree only at around @xmath kpc.

In figure 4.8 , for the @xmath keV bin (roughly corresponding to galaxy
groups) a similar trend emerges. In this case, the EG tangential shear
fits are even poorer resulting in total mass distributions that agree
well only for @xmath Mpc. Otherwise, for @xmath Mpc, the EG fits are in
strong tension with the data. The values in table 4.5 also indicate that
GR is strongly preferred. Also, it is important to note that the rather
high values of the @xmath are dominated by the contribution from the
outlier points at @xmath , @xmath for bin 1 and @xmath as well as @xmath
for bin 2. However, removing the outlier points does not lead to a
different conclusion regarding the preferred model.

Comparing the results for the galaxy clusters, groups and the Coma
Cluster indicates that, in general, EG seems to work better for massive
clusters. This in turn means that accurate measurements of the total
galaxy mass distribution (which dominates over the intracluster gas mass
at low radii and hence could push the predicted mass profiles closer to
those predicted in GR), are of special importance. In order to test the
importance of the stellar galaxy mass measurements on our final results,
we repeated the analysis outlined above, for the 58 cluster stack with
various @xmath distributions (which were compared against the galaxy
mass distribution of the Coma Cluster). In particular the total mass
distributions were deduced in the same way as in figure 4.7 , but now
with a galaxy mass distribution closer to that of the Coma Cluster (i.e.
being equal to @xmath , where @xmath is the Coma galaxy mass
distribution determined from the SDSS data). As figure 4.8(a)
illustrates, having significantly larger galaxy masses (while keeping
the intracluster gas component unchanged) results in a better agreement
between the standard model (GR + cold dark matter), EG and the
observational data.

Finally, we investigated how the results were affected by relaxing some
of the assumptions in the derivation of the scaling relation in equation
4.9 . In particular, as the authors point out in Halenka and Miller [
2018 ] , the mentioned scaling relation originally comes from the
inequality, which is ultimately set to be equal (this results in
equation 4.6 ; for more information see section 7.1 in [Verlinde, 2017 ]
). Hence, they propose a phenomenological model, in which the @xmath
term in the numerator of equation 4.9 is replaced by @xmath , where
@xmath is a constant, leading to:

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

The @xmath parameter, more specifically, describes how the elastic
medium in Verlinde’s theory is affected by the baryonic matter. Analysis
in Halenka and Miller [ 2018 ] shows that @xmath Mpc leads to a good
agreement between the EG prediction and the data.

If we use the modified scaling relation and carry out our analysis
again, the results in figure 4.8(b) are obtained. In agreement with the
results in Halenka and Miller [ 2018 ] and Tortora et al. [ 2018 ] , for
values of @xmath we find a good agreement between GR and EG.

The results in figures 4.7 and 4.8 are in agreement with most of the
other results in the literature. Specifically, in Ettori et al. [ 2017 ]
X-ray and SZ effect data is used to deduce the baryonic and, in turn,
the total mass distributions for EG and the standard model, resulting in
distributions very similar to ours (see figure 3 in Ettori et al. [ 2017
] in particular). More recently, in Ettori et al. [ 2019 ] the same
approach was extended for a larger sample of clusters, once again
resulting in mass distributions that agree only at around 1 Mpc radial
scales. In Hodson and Zhao [ 2017 ] the EG scaling relation is used to
calculate the acceleration radial distributions again resulting in
profiles for GR and EG, that only become comparable for @xmath Mpc.
Finally, the results reported in Halenka and Miller [ 2018 ] (without
modifying the original EG scaling relation) are closer to our results
for the Coma Cluster. Note, however, that such comparisons with other
results in the literature should be treated with caution, as the methods
and the datasets used to derive them are in general distinct and are
affected by different systematics.

To conclude, it should be noted that it is impressive that a prediction
derived from considerations in black hole thermodynamics and information
theory leads to a result (with no free parameters) for the dark matter
distribution that is of the right magnitude. However, as indicated by
our results and the results from the literature, it is now clear that
the scaling relation in equation 4.9 cannot fully account for the
effects associated with non-baryonic matter on galaxy cluster scales. To
some degree this was expected, as equation 4.9 was derived based on a
number of simplifying assumptions, such as the mass distributions being
spherical, non-dynamic and isolated. All of these assumptions are broken
in real clusters to varying degrees, hence a natural question to ask is
what effect does breaking these assumptions have on the main predictions
of the model. In this regard, the covariant EG model introduced in
Hossenfelder [ 2017 ] offers a way forward by introducing more general
covariant equations that could in principle be solved for non-spherical
mass distributions (see section 4.5 for a brief review of the key
features on the covariant EG model). In addition, the covariant approach
also allows for calculating the weak lensing predictions, which do not
rely on simplifying assumptions, such as that the lensing works in the
same way as it does in GR (assumption made in this work). Interestingly,
as pointed out in the literature, the covariant EG approach shares many
mathematical similarities with the model of superfluid dark matter
introduced in Berezhiani and Khoury [ 2015 ] . More specifically, both
models predict MOND-like effects in the non-relativistic limit similar
to those predicted in Verlinde’s EG. These similarities could ultimately
hint at some deeper connection between the phenomena described in
Verlinde’s work and the different phases of dark matter. Such
considerations are outside the scope of this thesis, however, they point
out some interesting directions for future work.

The next two sections contain a brief review of some of the theoretical
criticisms and other observational tests of the EG model along with a
summary of the key features of the covariant EG formulation.

### 4.4 The Current State of the Model

As previously mentioned, the theory has already been tested using
several methods and on a range of scales. Here for completeness the key
observational tests and their results are summarized with an emphasis on
their significance to the validity of Verlinde’s model on different
scales.

In Brouwer et al. [ 2017 ] the average surface mass density profiles of
isolated central galaxies were used to perform the first known test the
model. In particular, the average surface mass density profiles of
33,613 isolated central galaxies were compared against the theoretical
predictions. The study found that the predictions of the model are in
good agreement with the measured galaxy-galaxy lensing profiles in four
different stellar mass bins.

More recently the predictions of EG were compared against the
predictions from a list of selected modified gravity theories for a
single galaxy cluster, showing that the model only approaches the
measured acceleration profile data at the outskirts of the cluster
Hodson and Zhao [ 2017 ] . Similarly, the X-ray and weak lensing data
from the A1689 cluster was used in Nieuwenhuizen [ 2017 ] to compare the
predictions of EG with some selected modified gravity models, also
finding that model fails to account for the missing mass in the
mentioned cluster.

In Halenka and Miller [ 2018 ] the authors used mass densities from a
sample of 23 galaxy clusters to test the predictions of EG on galaxy
cluster scales. They found that EG could only correctly predict the
baryon and dark matter mass profiles at around the virial radius, while
being ruled out at a 5- @xmath level in the other parts of the clusters.
However, as the authors pointed out, fully accounting for the systematic
uncertainties and modifying certain assumptions in the model leads to a
much better agreement between GR and EG.

A similar study in ZuHone and Sims [ 2019 ] used matter densities of
relaxed, massive clusters of galaxies using a combination of X-ray and
weak lensing data. A key improvement in this work was to include the
baryon mass contribution of the brightest cluster galaxy in each system
along with the total mass profiles from gravitational lensing. The
results indicate that the EG predictions for the mass profiles and
baryon mass fractions disagree with the observational data by a factor
of up to @xmath - @xmath for radii in the range of @xmath - @xmath kpc.

In a more theoretical context, Verlinde’s theory has received numerous
criticisms. In Dai and Stojkovic [ 2017b ] the authors argue against the
idea of treating gravity as a force of entropic origin. Namely,
Newtonian gravitational force is conservative, which implies that the
dynamics of bodies in gravitational systems as well as the action should
be reversible. This implies that gravitational dynamics cannot be caused
purely by the increase of entropy in the gravitational system. More
specifically the authors point out that the equation @xmath in Verlinde
[ 2011 ] , where @xmath is the force, @xmath is the change in position,
@xmath is the temperature and @xmath is the change in entropy, is
missing a term corresponding to the change of kinetic energy of the
system @xmath . Including the missing term leads to a conclusion that
gravity cannot be an entropic force arising solely from the change in
entropy in the system.

A further criticism in Dai and Stojkovic [ 2017b ] points out a flaw
with the averaging procedure used in the derivation of the MOND scaling
relations (equations at the end of the section 4.4 in Verlinde [ 2017 ]
). This point, however, was later criticised in Yoon [ 2020 ] , where it
was shown that the criticism in Dai and Stojkovic [ 2017b ] stemmed from
a misunderstanding of the details of the gravity-elasticity
correspondence in the Verlinde’s original argument.

More generally, it has been shown that fully accounting for
energy-momentum conservation and cosmological homogeneity and isotropy
conditions severely restricts a wide class of possible entropic gravity
models [Wang, 2012 ] .

### 4.5 Covariant Emergent Gravity

Another recent development that is important to discuss in more detail
is the covariant EG model introduced by Hossenfelder [Hossenfelder, 2017
] . In particular, Hossenfelder introduces a model in which a vector
field fills the de Sitter space and via interactions with baryonic
matter results in effects similar to those of to dark matter. In the
non-relativistic limit this model reproduces the main predictions of
Verlinde’s EG along with correction terms. In addition, the covariant
formulation also demonstrates that the introduced vector field can mimic
the effects of dark energy.

In Hossenfelder’s covariant formulation, the displacement field @xmath
(introduced in equation 4.5 ) is treated as an extra vector field that
originates from the volume term in the total entropy equations in
Verlinde’s theory. The vector field couples to baryonic matter and drags
on it to create an effect similar to dark matter. The proposed
Langragian is given by:

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

with @xmath as the Ricci scalar, @xmath as the matter Lagrangian, @xmath
as the Hubble radius, @xmath as the stress-energy tensor, @xmath as the
kinetic term of the vector field and @xmath as the mass term.

Noting that the gravitational potential is given by @xmath , equation (
4.21 ) can used to derive an equation of motion for the field and solve
it for @xmath . For a covariant, spherically symmetric case the solution
is given by:

  -- -- -- --------
           (4.22)
  -- -- -- --------

where @xmath and @xmath is an integration constant that needs to be set
using boundary or initial conditions. As discussed in Hossenfelder [
2017 ] , @xmath should be set by taking the limit @xmath , however,
given the assumptions used to derive 4.22 this is not possible; the
other option is to treat it as a free parameter and to deduce it
numerically given some baryonic mass distribution. In general the result
given in equation 4.22 is expected to be different from the potential
due to apparent dark matter in Verlinde’s original formulation (i.e. it
will contain correction terms). Hence, solving equation 4.22 is the
natural next step in both the theoretical development and the
observational tests of the model.

More generally, the form of the Lagrangian in equation 4.21 indicates
some clear differences between the Verlinde’s and Hossenfelder’s
formulations. In particular, the extra terms in the Lagrangian indicate
that even when no baryonic mass is present in the system, the field
@xmath does not vanish. Or, in other words, stress-energy conservation
would require the field @xmath to be a source of gravity as well. This
means that the solutions for the total potential for general
gravitational systems will not be identical to those derived by Verlinde
and will contain correction terms. Another interesting feature of the
Lagrangian is the 2/3 power of the kinetic term. There have been
multiple modified gravity approaches that have a similar kinetic term,
most notably Berezhiani and Khoury [ 2015 ] , where a theory of dark
matter superfluidity is proposed.

As discussed in Hossenfelder [ 2017 ] , the Langrangian above can be
solved for @xmath , however the solution contains an integration
constant that cannot be determined analytically and would require
numerical solutions. Finding these solutions is out of the scope of this
thesis. However, further exploration of the covariant formulation of EG
for spherical and non spherical mass distributions, and comparison of
the results with the predictions from Verlinde’s original formulation
will be an interesting direction for future work.

It is also important to discuss some general criticisms to the covariant
formalism. Namely, as pointed out in Dai and Stojkovic [ 2017a ] , small
perturbations around the de Sitter space in Hossenfelder [ 2017 ] grow
rapidly indicating unstable cosmology. However the authors point out
that adding matter and radiation to the model could in principle provide
stability.

More generally, models with fractional powers of the kinetic term (such
as described in equation 4.21 and by the model of superfluid dark matter
in Berezhiani and Khoury [ 2015 ] ) have been criticised in Zatrimaylov
[ 2020 ] . In this work the author investigates the effects of enforcing
certain theoretical and observational constraints on the family of
models described above. More specifically, Zatrimaylov [ 2020 ] imposes
the constraints of the energy density being bounded from below,
superluminal propagation being absent in relativistic settings and the
models being able to account for gravitational lensing effects. The
conclusions indicate that scalar, vector and tensor theories with
fractional kinetic terms in generally struggle to satisfy the mentioned
energy density conditions while also abiding by the observational
constraints (for instance, the LIGO results for the speed of
gravitational waves). This also applies to @xmath models with MOND-like
potentials, which reproduce MOND effects on galaxy and cluster scales,
similar to those predicted by the superfluid dark matter and covariant
EG models.

## Chapter 5 A Brief Introduction to Machine Learning

Chapter 5 marks the beginning of the second part of the thesis. The main
focus of the second part is on machine learning techniques in the
context of @xmath -body simulation emulators. Chapter 5 contains an
overview of basic machine learning techniques and their relevance to
natural sciences. This includes a more in-depth look at decision tree
algorithms, artificial neural networks, generative adversarial networks
(GANs) and gradient boosting. Chapter 6 consists of a novel technique
for emulating @xmath -body simulation data using a GAN algorithm. In
particular, an algorithm capable of efficiently emulating cosmic web and
weak lensing convergence maps is introduced.

The key goal of the algorithms introduced in the upcoming chapters is to
produce realistic mock data quickly and efficiently. In particular,
emulating @xmath -body simulation data from @xmath CDM and modified
gravity simulations is of great importance for survey mock data
generation as well as modified gravity tests. More concretely, such
emulators could be used to generate mock weak lensing and galaxy cluster
data without resorting to computationally expensive hydrodynamic
simulations. In this respect, the topics discussed in chapters 5 and 6
are nicely linked with the topics discussed in the preceding chapters.

### 5.1 Machine Learning and Artificial Intelligence

The field of machine learning dates back to the beginning of the 20th
century and is intimately linked with the studies of the human brain and
the field of neuroscience. In fact, the theoretical basis for the
studies of the human brain in this context dates back even earlier to
the work by Alexander Bain and William James, who independently proposed
a model of the brain as a network of neurons [Bain, 1873 , James, 2012 ]
. Later, in 1943 Warren McCulloch and Walter Pitts created a
computational model for neural networks [McCulloch Warren, 1943 ] . In
1958 this culminated in the invention of the perceptron algorithm by
Frank Rosenblatt, which is a precursor to modern artificial neural
networks [Rosenblatt, 1958 ] .

The term machine learning itself dates back to 1959 and refers to the
study of techniques and algorithms that make decisions and predictions
without having been programmed to do so explicitly [Samuel, 1959 ] . In
this regard, machine learning is closely related to the fields of
computational statistics, automation, data science, mathematical
optimization and robotics. The studied algorithms can be broadly
classified into supervised learning, unsupervised learning and
reinforcement learning. Supervised learning algorithms are programmed to
deduce a rule that maps a certain set of inputs to a set of outputs
based on a training dataset, which contains data split into categories.
An archetypal example of such a machine learning task is image
classification, often done using artificial neural networks, decision
forests and other commonly used algorithms. Unsupervised learning
algorithms, on the other hand, deduce patterns and correlations in a
given dataset without it being explicitly classified into categories
before the training procedure. Algorithms of such type generally work
based on principle component and cluster analysis. Reinforcement
learning algorithms, on the other hand, are trained based on their
interactions with a dynamic environment with the aim of performing a
specified goal. Reinforcement learning algorithms are often applied to
solve problems in robotics and gaming (e.g. the AlphaZero algorithm)
[Silver et al. , 2017 ] .

Another class of models that does not easily fit into the classification
outlined above (and often contains a combination of supervised and
unsupervised techniques) contains generative models. Generative models
refer to a class of algorithms that aim to generate statistically
realistic mock data based on a training dataset. More specifically,
given a set of data instances @xmath and the corresponding set of labels
@xmath , a generative model is trained to capture the joint probability
@xmath . The two prime examples of generative algorithms are variational
autoencoders (VAEs) and generative adversarial networks (GANs) [Zamorski
et al. , 2019 ] . The latter will be discussed in greater detail at the
end of this chapter.

### 5.2 Machine Learning in Cosmology and Astrophysics

The key goal of machine learning (and data science more generally) is to
extract useful information from data. This makes machine learning
techniques an important tool in the natural sciences where the key goal
is to build physical models based on observational and experimental
data. Naturally, throughout the last few decades, machine learning
techniques have become an important tool in the toolset of
astrophysicists and cosmologists. Here we overview some of the key
machine learning techniques used in cosmology.

Recently a combination of techniques (naive Bayes, k-nearest neighbours,
support vector machines and neural networks) have been studied as a tool
for photometric supernova classification [Lochner et al. , 2016 ] .
Machine learning techniques are also key for the photometric LSST
astronomical time-series classification challenge (PLAsTiCC) [The
PLAsTiCC team et al. , 2018 ] .

In the field of CMB studies, extracting constraints on cosmological
parameters is of key importance. In this regard, machine learning has
been shown to provide competitive techniques for calculating these
constraints quickly and efficiently. These techniques are of special
importance when studying the non-Gaussian foreground contributions in
particular. An example of machine learning used for such data is the
DeepCMB algorithm, which uses deep convolutional neural networks (CNN)
for cosmological parameter estimation and lensing reconstruction
[Caldeira et al. , 2019 ] . Similarly, a 3-D CNN algorithm has been used
to extract cosmological paramters from large scale structure data in
Ravanbakhsh et al. [ 2017 ] .

Galaxy cluster mass estimation is another important task that has
greatly benefited from using different machine learning approaches.
Recently, it has been shown that machine learning techniques (support
distribution machines, support vector regression, decision trees, CNNs
and others) allow significant reduction in the scatter in cluster mass
estimates when compared to the more traditional statistical methods
[Ntampaka et al. , 2015 , Armitage et al. , 2019 , Ho et al. , 2019 ] .

In weak lensing CNNs have been used as quick and efficient tools for
discriminating between different models of modified gravity especially
in the context of non-Gaussian information encoded in the weak lensing
maps [Gupta et al. , 2018 , Ribli et al. , 2019 ] . Machine learning has
also been used with strong lensing data, where it was found to be
significantly faster and more efficient in identifying strong lensing
arcs [Lanusse et al. , 2017 ] .

It is also important to mention generative models, which have found
great use in emulating cosmological simulation data. GANs [Goodfellow et
al. , 2014 ] , in particular, have been employed to produce
statistically realistic mock data for both weak lensing convergence maps
and cosmic web slices [Rodríguez et al. , 2018 , Mustafa et al. , 2019 ]
.

The methods and techniques mentioned in this section clearly illustrate
the effectiveness of machine learning when applied to a variety of
problems in astrophysics and cosmology. However, it is also important to
discuss some of the common drawbacks that a lot of the mentioned models
have. In particular, many machine learning models suffer from being
difficult to interpret (i.e. the ”black box” problem). This is is
especially true in the case of neural networks. Similarly, when it comes
to most algorithms, it is generally difficult to introduce prior physics
knowledge that would be used when making predictions. With some models
one could introduce priors in a Bayesian fashion, however, with many
models that might not be possible. In addition, another important issue
with many models is that it is not easy to implement physical
constraints (i.e. conservation of energy etc.). These and similar issues
might not be problematic depending on the application at hand, but
nonetheless should be taken into consideration when choosing an
algorithm to tackle a specific problem.

The rest of this chapter is dedicated to introducing some of the key
machine learning algorithms in terms of relevance for this thesis. In
particular, decision trees and gradient boosting is introduced as a tool
for accurate classification. Similarly, different types of neural
networks are reviewed. Finally the GAN algorithm is discussed as a tool
for emulating cosmological data.

### 5.3 Decision Trees and Gradient Boosting

An algorithm that has recently gained significant popularity in the
literature is the XGBoost algorithm [Chen and Guestrin, 2016 ] . XGBoost
refers to extreme gradient boosting, which is a technique that produces
a prediction based on an ensemble of weak prediction models that are
optimized during the training procedure. The employed prediction models
are usually modeled using decision trees. Here we overview some of the
main features of the gradient boosting procedure more generally and in
the context of decision trees.

Generally speaking gradient boosting algorithms work by iteratively
improving the prediction of a model by fitting the residual points and
adding extra terms to the model to account for those residuals. This
leads to such models being very successive in approximating complex
functions, as during the training procedure gradient boosting allows the
algorithm to focus on the data points that the initial model struggled
to fit and to incrementally improve this. Here this procedure will be
described mathematically based primarily on Friedman [ 2001 ] .

The goal of most supervised machine learning models is to produce
accurate predictions based on a training dataset: @xmath . Here @xmath
is a vector corresponding to the training data, while @xmath is either a
class in a classification task or a value that the model tries to
predict in a regression task. More specifically, a machine learning
algorithm aims to find an accurate approximation @xmath of the function
@xmath that minimizes some cost function @xmath . More formally, the
function @xmath is determined by evaluating the following:

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath refers to the expectation function. Algorithms, such as
XGBoost , determine the function @xmath by expressing it as a sum of
weighted functions @xmath :

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath are the weight parameters and @xmath is the number of
iterations. In case of the XGBoost algorithm, the functions @xmath
represent decision trees that fit the residuals between the prediction
of the model and the training data at each iteration of training.

Here a brief overview of the gradient boosting training algorithm is
given in pseudo-code without going into full detail (for more
information see Friedman [ 2001 ], Hastie et al. [ 2013 ] ):

1.  The model is initialized with a constant value. This can be an
    average value based on the data or a simple fit based on the cost
    function (note that @xmath here should not be confused with the
    previously mentioned weight parameters, as it simply denotes the
    initial fit that minimizes the cost function):

      -- -------- -- -------
         @xmath      (5.3)
      -- -------- -- -------

2.  For iterations @xmath to @xmath do:

    1.  evaluate the pseudo-residuals (for @xmath ):

          -- -------- -- -------
             @xmath      (5.4)
          -- -------- -- -------

    2.  Fit a weak learner (e.g. a decision tree algorithm) @xmath to
        the residuals. This is done by training @xmath on the residual
        dataset: @xmath .

    3.  Evaluate the weight parameter @xmath via optimization:

          -- -------- -- -------
             @xmath      (5.5)
          -- -------- -- -------

    4.  Update the model:

          -- -------- -- -------
             @xmath      (5.6)
          -- -------- -- -------

    end for

3.  Output the final result @xmath .

Figure 5.1 illustrates the gradient boosting procedure on a sample
dataset.

The final point to discuss is how the @xmath functions are actually
determined. As mentioned, this is usually done by using decision trees,
however it could be any algorithm capable of fitting the residual data.
Decision trees refer to a technique of splitting a dataset in a way that
allows making accurate predictions (in a classification or a regression
task). The technique is easiest to understand by referring to a simple
example. A classic dataset used to illustrate machine learning
classification problems is the Fisher-Anderson Iris flower dataset
Anderson [ 1936 ], Dua and Graff [ 2017 ] . This dataset contains 50
data samples of the length and the width of the sepals and the petals
for 3 different species of Iris flowers ( Iris Setosa , Iris Virginica
and Iris Versicolor ). The dataset is often used as a pedagogical
example on building machine learning classification algorithms with the
goal of using the petal and sepal features in order to predict the
flower type. With decision trees this can be done by finding an optimal
way of splitting the dataset into categories based on the values of the
mentioned features. In particular, decision tree algorithms are
optimized to find the optimal way of splitting the training data into
categories in a way that allows predicting the flower type as accurately
as possible. Figure 5.2 illustrates such a decision tree.

A natural question to ask is how does one quantify which way of
splitting the data leads to the most accurate predictions. This is done
by measuring the pureness of a subset of the decision tree. A commonly
used measure for this is entropy @xmath . For a particular node of the
decision tree the entropy can be calculated as follows:

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath is the proportion of the samples that belong to class
@xmath for a particular node, @xmath is the training subset of the
parent node and @xmath refers to the number of unique class labels. The
entropy can then be used to estimate the information gain due to a given
split in the data, as quantified by the information gain parameter
@xmath :

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

where @xmath is the training subset of the node under consideration,
@xmath is the feature that the split is being performed for, @xmath is
the number of samples in the parent node, @xmath is the number of
samples in the left child node, @xmath is the number of samples in the
right child node, @xmath is the training subset of the left child node
and @xmath is the training subset of the right child node. Note that
this equation is only correct for decision trees where only two splits
are available at each node, however, it is easy to generalize the
equation above for further possible splits by adding analogous further
terms.

To put it simply, equation 5.8 is the measurement of the difference in
entropy before and after the split. In particular, equation 5.8 can be
better understood by looking at a concrete example, i.e. the decision
tree shown in figure 5.2 . For instance, the information gain parameter
corresponding to the right-hand side node (sepal length of @xmath ) can
be calculated by considering the number of the data samples that
correspond to Versicolor and Virginica Iris flowers. In this case @xmath
corresponds to the size of the subset of data in the left child note,
@xmath is analogous and @xmath corresponds to the subset of data that
has sepal length of @xmath and is used to calculate the entropy of the
parent node. Analogously, the entropy of child nodes can be calculated
by using the corresponding data subsets in those nodes.

Given these ways of quantifying the change in entropy due to any split
in the dataset, decision trees can then be optimized to maximise the
purity (information gain) of those splits. Decision trees are greedy
algorithms in the sense that they optimize the information gain for each
split sequentially.

In summary, decision tree algorithms can be used for both regression and
classification and are relatively easy to interpret (the decision splits
can be plotted graphically). However, the models tend to be prone to
overfitting, i.e. performing poorly when making predictions on unseen
data. There are a number of remedies for overfitting, for instance
carefully choosing the hyperparameters corresponding to the maximum
allowed depth and number of leaves of the decision tree during the
optimization procedure.

The described gradient boosting techniques and decision forests are
combined in the XGBoost algorithm that offers cutting edge accuracy when
it comes to classification and regression tasks. In the upcoming chapter
XGBoost will be used for classifying the produced weak lensing and
overdensity field data based on cosmological parameters. This will be
crucial for proper analysis of the emulated data sets.

### 5.4 Artificial Neural Networks

The main building block of artificial neural networks is the previously
mentioned Rosenblat’s perceptron which mimics the key features of
biological neurons. Figure 5.3 illustrates the key difference and
similarities of the biological and artificial neurons. In addition, the
figure also summarizes the key components of the biological neurons. The
signals received from the neighbouring neurons are delivered via
protoplasmic nerve extensions called dendrites and sent to the cell
body, where the signals are processed. If sufficient input signal is
received, the neuron generates an action potential. The action potential
is then transmitted via longer cytoplasmic protrusions (known as axons)
to the other neighbouring neurons. If sufficiently strong input is not
received, the signal quickly decays and no action potential is
generated.

Artificial neurons are built to mimic the key elements of their
biological counterparts. As illustrated by figure 5.3 , they contain a
node that is equivalent to the cell body, which receives multiple
inputs, processes them and produces an output. The received inputs are
usually weighted and processed by a non-linear activation function. This
process can be expressed as follows:

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

where @xmath refers to the set of the weight parameters, @xmath is the
activation function, @xmath is the set of inputs and @xmath refers to
the output(s). The activation function improves the training procedure
for multi-layered neural networks and allows the network to approximate
non-linear functions easier. A commonly used function is the sigmoid
logistic function:

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

The sigmoid function has a characteristic S shape and the following
asymptotic behaviour: @xmath for @xmath and @xmath for @xmath . Many
other functions can be used, which all share the common S -like shape:
@xmath , @xmath , @xmath (the error function), @xmath etc. The output
values (and the asymptotic behaviour) can be controlled by normalizing
the mentioned functions to the needed range of @xmath (which is usually
@xmath or @xmath depending on the value range of the training data).

Joining multiple artificial perceptrons into a layered structure results
in the familiar multilayer perceptron architecture shown in figure 5.4 .
In a multilayer perceptron each node is a single artificial neuron, with
multiple inputs coming in, being processed by an activation function and
sent out as output signals to be received by other neurons.

One minor difference in this architecture, is the addition of bias
nodes, which add a certain value to the output of each layer in the
network. The bias term introduces a slight shift to the activation
function, which has been shown to significantly improve the performance
of artificial neural networks [Hellström et al. , 2020 ] . A succinct
way of representing the multilayered structure of a neural network is by
using a function composition notation, where a neural network @xmath
maps an input vector @xmath to an output in each layer as follows:

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

Here each layer @xmath maps from an input @xmath to an output as shown
above with @xmath as the @xmath -th element of the @xmath -th layer,
@xmath as an activation function, @xmath as the bias term, @xmath as the
weight matrix. The function composition is an operation for two
functions @xmath and @xmath that can be defined as @xmath .

Hence an artificial neural network @xmath can be treated as a
complicated non-linear function that maps an input @xmath to an output
and the goal of the training procedure is to find the optimal set of the
weight parameters in the weight matrix @xmath . The training procedure
for multilayered neural networks as described above is usually done
using the backpropagation algorithm with gradient descent.

A key quantity when evaluating the performance of an artificial neural
network during the training procedure is the cost (error) function
@xmath . For an input-output data pair @xmath the cost function can be
something as simple as the square difference between the output of the
network and the true value corresponding to a given input (e.g. the
correct class of an image in an image classification task or the correct
value in a regression task): @xmath . More sophisticated cost functions
are usually used in modern neural networks, such as the cross-entropy
function:

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

Given a cost function, the goal of the optimization/training procedure
is then to minimize the cost function w.r.t. the set of the weight
parameters. The change in the weight parameters, @xmath is calculated in
an iterative gradient descent procedure:

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

where @xmath is the learning rate parameter, @xmath is the output of the
layer @xmath and @xmath is the gradient at the layer @xmath . Namely,
the aim of the procedure is then to calculate the partial derivative of
the cost function term w.r.t. the weight parameters, which is done by
backpropagation, i.e. evaluating the gradient terms for each layer
starting with the final layer. Assuming the cost and the activation
functions are well-behaved and their derivatives can be calculated, each
gradient can be evaluated and the weight parameters can be updated in a
way that reduces the value of the cost function. If the training
procedure converges, gradient descent finds the minimum (this is usually
one of the local minima). After the training procedure, assuming there
is enough data and the neural network architecture is well-chosen, the
network is capable of making accurate predictions in classification,
regression and other tasks. Modern software packages, such as TensorFlow
allow performing gradient descent quickly and efficiently for a
pre-defined architecture [Abadi et al. , 2015 ] .

### 5.5 Convolutional Neural Networks

Another type of artificial neural networks that are important to discuss
are CNNs. CNNs share a lot of the features with the previously discussed
multilayer perceptrons with one major difference being that they extract
useful features from the data using convolutions. To put it simply, the
convolution procedure refers to convolving a filter (kernel) with
different parts of an image, which allows extracting visual features
from that image. In this respect, CNNs draw inspiration from the human
visual cortex. The extracted visual features (edges, corners, main
shapes etc.) are then processed and combined in order to make a
prediction in a classification or another kind of machine learning task.

Mathematically, the procedure of convolving a kernel with an image can
be described as:

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

where @xmath refers to the kernel (filter) matrix, @xmath is the
original image and @xmath is the convolved image with the matrix values
spanning @xmath and @xmath . Figure 5.14 illustrates the convolution
procedure for a simple @xmath kernel with trivial values, which simply
sums all the values in the corresponding section of an image. Choosing
different values for the kernel matrix allows extracting different
visual features from a given image. Figure 5.6 illustrates the results
of this procedure for a simple greyscale image.

CNNs work most naturally with full-colour RGB images (figure 5.5 ) which
can be represented as 3-dimensional arrays. More generally, any dataset
can be represented as a @xmath -dimensional array (tensor ¹ ¹ 1 Note
regarding the terminology: tensors in machine learning and computer
science literature often simply refer to @xmath -dimensional arrays,
rather than algebraic objects with specific transformation properties.
). In particular, we can denote the input data to the @xmath -th CNN
layer as: @xmath , where @xmath , @xmath , @xmath refer to the height,
width and the number of channels in the input array (see figure 5.5 ).
During the training procedure CNNs are often trained on batches of input
images, which can be represented as 4-dimensional arrays: @xmath , where
@xmath is the number of the images in a batch. Hence a CNN receives an
input @xmath , transforms it (by convolving it or applying some other
operation) all the way till the final layer @xmath , which corresponds
to an array of values representing probabilities of the input image
belonging to some class (classification task). Alternatively, the output
of CNN could be another image @xmath .

Convolution kernels can be easily described as 2-dimensional or more
generally as multiple @xmath -dimensional arrays. In addition, in real
CNNs kernels do not necessarily have to convolve images by covering
every pixel and instead can skip every @xmath -th pixel. This behaviour
is summarized by the stride parameter. Putting everything together, the
output of the @xmath -th layer can be denoted as:

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where @xmath as before denotes the kernel matrix. Equation 5.15 looks
complex, but it simply generalizes equation 5.14 for any input image
shape and multiple kernels of custom size.

As mentioned the training procedure of CNNs is in principle the same as
for the multilayer perceptron networks. In particular, the arrays in the
network can be vectorized and each value weighted, making the goal of
the training procedure, as before, to find the optimal values for those
weight parameters (see equation 5.13 ). Using the chain rule, for the
@xmath -th layer, the main part of the optimization procedure is
calculating the dependence of the loss/error function on the input and
the weight parameters [Wu, 2017 ] :

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

where vec represents the vectorization operation. Calculating these
terms is more challenging than in the case of ordinary multilayer
networks, but can be done quite efficiently with modern software and
graphical processing unit (GPU) support.

Finally, an important topic to discuss in the context of the
architecture of CNNs is the variety of the types of layers that can be
used. Here some of the most important types of layers are listed and
discussed:

-    Batch normalisation layers . These layers basically normalize the
    input data to have a zero mean and unit variance (or other specified
    values). This has been demonstrated to increase the efficiency of
    the training procedure, while also adding stability.

-    Pooling layers . These layers reduce the dimensionality of the
    input data by using averaging, summing or maximization operations.
    For instance, this can simply refer to extracting the maximum values
    in each section of an input array by a simple kernel.

-    Softmax layer . A layer of this type is used as a final layer to
    provide the output values corresponding to probabilities of an
    object belonging to one of the @xmath classes in a classification
    problem.

-    Flatten layers . These layers vectorize the @xmath -dimensional
    input arrays to 1-dimensional vectors.

### 5.6 Generative Adversarial Networks

As mentioned, generative models form an important class of machine
learning algorithms that have been applied to solve a wide variety of
problems in science. With the discovery of GANs in Goodfellow et al. [
2014 ] , an entirely new way of using artificial neural networks has
been discovered. The GAN algorithm refers to a system of two neural
networks (these can be convolutional, but it is not necessary for the
algorithm to work), a generator and a discriminator that are trained
adversarially to produce novel statistically realistic data. In
particular, the two neural networks compete in an adversarial fashion
during the training process – the generator is optimized to produce
realistic datasets statistically identical to the training data and
hence to fool the discriminator. Mathematically, such an optimization
corresponds to minimizing the cost function @xmath :

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

where @xmath refers to the expectation function, @xmath to the
discriminator with weights @xmath , @xmath to the generator with weights
@xmath , @xmath to the distribution of the data we are aiming for,
@xmath to the generated distribution, @xmath to the data (real or
generated) analyzed by the discriminator and @xmath to the random noise
vector input to the generator.

Such an optimization procedure is a nice example of game theory where
the two agents (the generator and the discriminator) compete in a two
player zero sum game and adjust their strategies (neural network
weights) based on the common cost function. In case of perfect
convergence, the GAN would reach Nash equilibrium, i.e. the generator
and the discriminator would reach optimal configurations (optimal sets
of weights). In practice, however, reaching convergence is difficult and
the training procedure is often unstable and prone to mode collapse ² ²
2 This refers to the generator overpowering the discriminator, which
results in the generator getting stuck in producing a small subset of
identical or nearly identical realistic outputs. [Farnia and Ozdaglar,
2020 ] .

The two neural networks, the discriminator and the generator, have two
different training procedures. In particular, the discriminator
classifies the datasets into real (coming from the training dataset) or
fake (produced by the generator) and is penalized for misclassification
via the discriminator loss term. The discriminator weights are updated
through backpropagation as usual. The generator, on the other hand,
samples random noise, produces an image, gets the classification of that
image from the discriminator and updates its weights accordingly via
backpopagation using the generator loss function term. The full training
procedure is done by alternating between the discriminator and the
generator training cycles.

The described training procedure can be summarized more formally in
pseudo-code (this is the original algorithm for the simplest version of
a GAN described in great detail in Goodfellow et al. [ 2014 ] ):

1.  For training iterations @xmath to @xmath do :

    1.  for @xmath steps do :

        -   Sample a minibatch of @xmath random noise samples @xmath
            from the noise prior distribution @xmath .

        -   Sample a minibatch of @xmath samples @xmath from the
            training data distribution @xmath .

        -   Update the discriminator neural network by evaluating its
            stochastic gradient:

              -- -------- -- --------
                 @xmath      (5.19)
              -- -------- -- --------

        end for

    2.  Sample a minibatch of @xmath random noise samples @xmath from
        the noise prior distribution @xmath .

    3.  Update the generator neural network by evaluating its stochastic
        gradient:

          -- -------- -- --------
             @xmath      (5.20)
          -- -------- -- --------

        end for

2.  Output the updated discriminator and the generator neural networks:
    @xmath , @xmath .

The training procedure described in the algorithm above is illustrated
visually in figure 5.7 . Assuming the adversarial training is
successful, the generator @xmath can then be used separately to produce
realistic synthetic data from a randomized input vector @xmath .

A useful pedagogical example to examine is that of using a GAN to
produce realistic hand-written digits. In this case the training dataset
consists of 60,000 hand-written digits represented as @xmath px images
(the MNIST dataset described in LeCun and Cortes [ 2010 ] ). The
training procedure then consists of the generator producing a batch of
hand-written images from a random noise vector. The produced and the
training image batches are used by the discriminator for updating the
discriminator cost function, which in turn is passed to the generator
for updating the corresponding error function. During the initial stages
of training the generated images are not-realistic, however, after a few
epochs, the generator weights are updated sufficiently in order to
produce high quality images. Once the training is finished (provided
that the common problem of mode collapse is avoided), the generator
neural network can be used to produce high quality realistic
hand-written images from a batch of noise vectors. The training
procedure for the MNIST dataset is illustrated pictorially in figure 5.8
.

## Chapter 6 Using GANs for Emulating Cosmological Simulation Data

This chapter contains a novel GAN algorithm that is used for emulating
cosmological simulation data. More specifically, a brief summary of the
motivations and approaches to cosmological simulation emulators is
given. In addition, various properties of GAN algorithms are discussed.
And finally, an approach of emulating simulations of different
cosmological parameters, redshifts and modified gravity parameters is
introduced. The algorithm is a modified version of the cosmoGAN code
described in great detail in Mustafa et al. [ 2019 ] . Most figures are
from Tamosiunas et al. [ 2020 ] unless otherwise specified. The
calculations, coding and the result analysis was done by the author with
consultation and the supervision by the supervisors and the co-authors.
The L-PICOLA and MG-PICOLA simulation data was produced primarily by
Hans Winther. The other datasets are given in appendix A.1 .

### 6.1 The Need for Cosmological Emulators

In the era of precision cosmology an important tool for studying the
evolution of large scale structure is N -body simulations. Such
simulations evolve a large number of particles under the influence of
gravity (and possibly other forces) throughout cosmic time and allow
detailed studies of the non-linear structure formation. Modern
cosmological simulations are highly realistic and extremely complex and
may include galaxy evolution, feedback processes, massive neutrinos,
weak lensing and many other effects. Such complexity however comes at a
price in terms of computational resources and large simulations may take
several days or even weeks to run. In addition, to fully account for
galaxy formation and other effects various simplification schemes and
semi-analytical models are required. To address these issues a variety
of emulation techniques have been discussed in the literature [Kwan et
al. , 2015 , Winther et al. , 2019 , Knabenhans et al. , 2019 ] . In
light of upcoming surveys like Euclid, such emulators will be an
invaluable tool for producing mock data quickly and efficiently.

Lately, machine learning techniques have also been applied as an
alternative to the traditional emulation methods. For instance, deep
learning has been used to accurately predict non-linear structure
formation [He et al. , 2019 ] . Similarly GANs and variational
autoencoders have been used to produce novel realistic cosmic web 2-D
projections, weak lensing maps and to perform dark energy model
selection [Rodríguez et al. , 2018 , Mustafa et al. , 2019 , Li et al. ,
2019 ] . In addition the GAN approach has also been used to produce
realistic cosmic microwave background temperature anisotropy 2-D patches
as well as deep field astronomical images [Mishra et al. , 2019 , Smith
and Geach, 2019 ] . Finally, generating full 3-D cosmic web data has
been discussed in Perraudin et al. [ 2019 ], Kodi Ramanah et al. [ 2020
] . The cited works show that GANs are capable of reproducing a variety
of cosmological simulation outputs efficiently and with high accuracy.

However, certain challenges remain: the training process of the GAN
algorithm is complicated and prone to failure and producing full scale
3-D results is computationally expensive. A common problem when training
GANs is mode collapse , when the generator neural network overpowers the
discriminator and gets stuck in producing a small sample of identical
outputs. Mode collapse can be addressed in multiple ways – modern GAN
architectures introduce label flipping or use different loss functions,
such as the Wasserstein distance, which has been shown to reduce the
probability of mode collapse [Arjovsky et al. , 2017 ] . In this chapter
I address some of these issues and present the results on extending some
of the currently existing GAN algorithms. In particular, as mentioned, a
modified version of the cosmoGAN algorithm (introduced in Mustafa et al.
[ 2019 ] ) is used to produce weak lensing convergence maps and 2-D
cosmic web projections of different redshifts and multiple cosmologies,
including dark matter, gas and internal energy data. Furthermore, other
techniques from contemporary research in the field of deep learning are
explored, such as latent space interpolation, which offers a way to
control the outputs of the algorithm. This, to my best knowledge, is a
novel approach that in the context of cosmology has not been explored in
the literature so far. Finally, a discussion of GANs in the framework of
Riemannian geometry is given in order to put the problem on a more
theoretical footing and to explore the feature space learnt by the
algorithm. Ultimately, the goal of the research described in this
chapter is to adapt the existing algorithms towards becoming
fully-controllable, universal emulators capable of producing both novel
large scale structure data as well as other datasets, such as weak
lensing convergence maps.

### 6.2 DCGAN Architecture for Emulating Cosmological Simulation Data

As outlined in section 5.6 , the GAN algorithm can be used to generate
novel, statistically realistic data based on some training dataset. In
our case three types of training datasets are used: 2-D cosmic web
slices, 2-D weak lensing convergence maps and stacks of cosmic-web
slices for dark matter and baryonic simulation data along with internal
energy data. Here it is important to clarify what exactly is meant by
cosmic web slice and weak lensing convergence data. Cosmic web slices,
in particular, refer to the 2-D discrete dark matter overdensity field
@xmath data, where the value at each position @xmath refers to the
density relative to the average density (i.e. @xmath ). Similarly, weak
lensing maps refer to the discrete 2-D corvengence fields, where the
numerical value at each position @xmath simply refers to the value of
convergence @xmath . Both types of datasets can be represented as 2-D
arrays, where each entry of the array corresponds to the value of the
overdensity or convergence. Analogously, these arrays can be represented
visually as images, with each pixel value corresponding to the mentioned
quantities (note that in some cases the pixel values are rescaled for
visualization purposes).

Since we are dealing with data that can be naturally represented in a
visual form, a straightforward choice is to pick a GAN architecture that
works well with image data. In this regard, deep convolutional
generative adversarial networks (DCGANs) have shown good results in
producing statistically realistic novel visual data. Thus, in order to
generate cosmic web and weak lensing convergence data, we chose to use
the DCGAN architecture, as described in [Mustafa et al. , 2019 ] . More
specifically, as a starting point the DCGAN implementation publicly
available in Mustafa et al. [ 2017 ] was used.

Tables 6.1 and 6.2 describe the key features of the architecture. Both
the discriminator and the generator are standard convolutional neural
networks using primarily ReLU and leaky ReLU activation functions along
with transposed convolutional and standard convolutional layers.

To adapt the outlined architecture to the problem at hand, I
experimented with different activation functions, different strides and
different sizes of the convolutional layers. The results indicated that
the architecture used in Mustafa et al. [ 2019 ] with minor variations
generally worked well for producing realistic cosmic web and weak
lensing data as well as the combined dark matter and baryonic data
samples. More specifically, for the cosmic web data, the input shape
(i.e. the size of the random noise vector) was changed from @xmath to
@xmath to account for the higher complexity of the cosmic web images
when compared to the weak lensing maps. In the case of emulating dark
matter, gas and internal energy slices, the public code was adapted to
work with multi-channel data (i.e. RGB input arrays). In addition, the
default batch size (number of input arrays that the algorithm uses
during an iteration of training) was changed from @xmath to @xmath .
Extra functions were also added to the code to allow performing the
latent space interpolation easier (see section 6.3 and appendix A.1 ).
Finally, when training on weak lensing convergence maps, the
architecture was left unchanged as one of the key goals was to reproduce
the results described in Mustafa et al. [ 2019 ] .

In summary, if we choose convolutional neural networks (rather than
simple multi-layer perceptrons) for the generator and the discriminator,
the training procedure described in section 5.6 essentially remains the
same. In fact, any algorithm that is capable of producing and
classifying data could be used as the generator and the discriminator.
And then, if we represent the cosmic web and the weak lensing data as
2-D arrays, the pipeline of training a GAN on our data is summarized in
figure 6.1 . In particular, the simulation data is used to produce the
2-D cosmic web slices, which are then used as a training dataset. More
specifically, during the training procedure the generator network
produces a batch of @xmath images from a batch of @xmath random noise
vectors (drawn from a Gaussian distribution centered around @xmath ),
which is then sent to the discriminator network, where it is combined
with a batch of @xmath training images for classification. In the case
of a training dataset with multi-channel images (RGB images), the
generator and the discriminator process batches of images of the
following shape: @xmath , where the second value corresponds to the
number of channels.

To summarize, the problem at hand is to emulate cosmological simulation
data using GANs. More specifically, the aim is to emulate novel dark
matter overdensity fields, represented by 2-D arrays produced via mesh
painting from the raw simulation output data. In addition, we also aim
to produce realistic convergence field and gas density data from
hydrodynamic simulations. In all cases the training data consists of
batches of 2-D arrays, which, when plotted visually, represent the
mentioned fields (projected to 2-D). A more technical discussion of the
datasets is given in section 6.5 . Once the training is completed, the
generator neural network can be used to produce realistic novel 2-D
arrays in the same format as the training dataset. Note that the problem
at hand is fundamentally analogous to that of generating hand-written
images (i.e. figure 5.8 ) the only difference being the size of the
dataset arrays and the architecture of the neural networks used.

### 6.3 Latent Space Interpolation

Before discussing the different datasets and the training procedure, it
is important to review another important feature of the GAN algorithms.
Latent space interpolation refers to the procedure of interpolating
between a pair of outputs produced by a GAN. This procedure not only
allows us to study the feature space learnt by the algorithm, but also
allows us to control which outputs the algorithm produces. Here a review
of the procedure and its various uses is given.

If the training procedure is successful, the generator @xmath learns to
map the values of a random vector @xmath to the values of a
statistically realistic output vector @xmath , which can be reshaped to
the original 2-D array shape representing an image @xmath (a cosmic web
slice or a convergence map in our case). This can be viewed as mapping
from a low-dimensional latent space @xmath to a higher-dimensional data
(pixel) space @xmath (where @xmath is the size of the noise input vector
and @xmath is the total number of the of the output image pixels; for
more details see Shao et al. [ 2017 ] ). For a generator neural network
@xmath (in our case @xmath or @xmath , while @xmath ).

The training procedure can be viewed as the generator learning to map
clusters in the @xmath space to the clusters in the @xmath space. Hence,
if we treat the random input vectors ¹ ¹ 1 Note regarding notation: here
a superscript refers to input/output vectors, while a subscript refers
to the corresponding point in the latent/output data space. @xmath as
points in a @xmath -dimensional space, we can interpolate between
multiple input vectors and produce a transition between the
corresponding outputs. In particular, if we choose two input vectors
that correspond to points @xmath and @xmath and find a line connecting
them, sampling intermediate input points along that line leads to a set
of outputs that correspond to an almost smooth transition between the
output points @xmath and @xmath .

As an example, if we train the generator to produce cosmic web slices of
two different redshifts, we can produce a set of outputs corresponding
to a transition between those two redshifts by linearly interpolating
between the input points @xmath and @xmath (see figure 6.2 ). More
concretely, if we train the algorithm on cosmic web slices of redshifts
@xmath , somewhere between the two input points, one can find a point
@xmath , which produces an output that has a matter power spectrum
approximately corresponding to a redshift @xmath . This is fascinating
given that the training dataset did not include intermediate redshift
data. Here it is important to note that such an interpolation procedure
does not necessarily produce a perfectly smooth transition in the data
space, i.e. the produced outputs corresponding to the points @xmath
between @xmath and @xmath are not always realistic (in terms of the
matter power spectrum and other statistics; see figure 6.19 and section
6.7.6 for further details). Also, one might naively think that the point
@xmath lies in the middle of the line connecting @xmath and @xmath , but
in general we found it not to be the case (as the middle of the
mentioned line does not necessary correspond to the middle between
@xmath and @xmath in the data space, which is known to be non-Euclidean
(see section 6.4 )). In the upcoming chapters I investigate whether the
latent space interpolation procedure can be used to map between outputs
of different redshifts and cosmologies and whether the produced datasets
are physically realistic.

The latent space interpolation technique was performed by randomly
choosing two input points @xmath and @xmath , finding the line
connecting the two points in the 256 (64)-dimensional space (256 (64) is
the size of the corresponding input vectors) and then sampling 64
equally spaced points along that line. The outputs of the generator
neural network of those intermediate input points @xmath then correspond
to cosmic web slices and weak lensing maps that represent a transition
between the two outputs @xmath and @xmath .

In order to perform linear latent space interpolation it is crucial to
have the ability to distinguish between different data classes produced
by the GAN (e.g. cosmic web slices of different redshifts). This was
resolved by employing a combination of the usual summary statistics like
the power spectrum and the Minkowski functionals along with two
different machine learning algorithms. In particular, deep convolutional
neural network and gradient boosted decision trees were used for
distinguishing the different classes of datasets produced by the GAN
[Chen and Guestrin, 2016 ] .

### 6.4 Riemannian Geometry of GANs

The latent space interpolation procedure described in the previous
section is a good example of how Riemannian geometry can be employed to
describe certain features of the GAN algorithm. Recently various
connections between GANs and Riemannian geometry have been explored in
the machine learning literature in a more general context. Such
connections are important to explore not only for the sake of curiosity,
but also because they allow us to describe GANs and their optimization
procedure in a language more familiar to physicists. A Riemannian
geometry description of GANs is also powerful when exploring the latent
space of a trained generator neural network and the outputs that it
produces. Finally, a differential geometry description could shine some
light on the connections between generative models and information
geometry, which is a well-established field and could offer some new
insights into training and analyzing the outputs of such models.

Recent work in Shao et al. [ 2017 ] proposes treating the trained
generator neural network as a mapping from a lower dimensional latent
space @xmath to the higher dimensional data space @xmath : @xmath (see
fig. 6.3 ). More specifically, the generator @xmath maps the latent
space vectors of size @xmath (in our case @xmath or @xmath ) to a
manifold @xmath of dimensionality @xmath ( @xmath , i.e. the number of
pixels in the output images). Manifold @xmath here simply refers to a
subset of the data space (all possible combinations of pixel values),
which correspond to realistic images of weak lensing/cosmic web slices.
The existence of such a manifold is postulated by the manifold
hypothesis in deep learning, which states that high-dimensional data can
be encoded on a manifold of a much lower dimension [Fefferman et al. ,
2013 ] .

Hence if we treat the generator neural network @xmath as a mapping for
the latent space to the data space manifold, one can naturally define an
induced metric @xmath , which then allows to quantify the distance
between the points on the manifold and the length of curves. For a
mapping described by the generator neural network, the metric is simply
equal to a product of the Jacobian and the transposed Jacobian [Shao et
al. , 2017 ] :

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

The Jacobian in our case refers to the partial derivative of each output
value w.r.t. to each input value, i.e.:

  -- -------- --
     @xmath   
  -- -------- --

Once a metric is defined, one can use the usual tools to describe
geodesics on the manifold @xmath . For instance, one can define a curve
@xmath between two points @xmath and @xmath in the latent space @xmath
parametrized by some parameter @xmath . Using the mapping @xmath , the
corresponding curve on the manifold @xmath is then: @xmath . To find a
curve that corresponds to a geodesic on the manifold one has to solve
the Euler-Lagrange equation, which gives:

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where @xmath is the usual Christoffel symbol, given by:

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

As discussed in Shao et al. [ 2017 ] geodesics between points on the
manifold are of special importance, as they give the smoothest possible
transition between multiple outputs. One of the main findings in Shao et
al. [ 2017 ] was that the Riemannian curvature of the manifold
corresponding to the their data was surprisingly small and, hence,
linear interpolation produced realistic results comparable to the
results produced by calculating a geodesic curve between the outputs. In
our work we also found that linear interpolation generally produced
realistic results. However, to ensure that the outputs produced via the
latent space interpolation are indeed realistic, one would have to
interpolate on a curve in the latent space (corresponding to the
geodesic connecting the needed outputs on the data manifold @xmath )
rather than a line.

Another important connection to Riemannian geometry comes in the context
of the discriminator neural network. The discriminator can be viewed as
a mapping from the data manifold to a probability manifold @xmath ,
where each point on the manifold corresponds to the probability of a
given data sample being real (i.e. belonging to the training dataset).
Such a manifold looks remarkably similar to the statistical manifolds
studied in the field of information geometry. Insights from information
geometry have a long tradition of being used in neural network
optimization (e.g. Hauser and Ray [ 2017 ] ). Exploring such connections
could lead to deeper insights into the GAN training process, which is an
interesting direction for future work.

### 6.5 Datasets and the Training Procedure

This section contains a detailed introduction to the datasets that were
used to train the GAN algorithm described in the previous sections. In
each case the simulations that were used to produce the dataset are
described as well. Finally, the cosmological parameters and the used
smoothing techniques are described as well.

#### 6.5.1 Weak Lensing Convergence Map Data

Gravitational potentials influence the path of photons in such a way
that they introduce coherent distortions in the apparent shape (shear)
and position of light sources. Weak gravitional lensing introduces
ellipticity changes in objects of the order of @xmath 1% and can be
measured across the sky, meaning that maps of the lensing distortion of
objects can be made and related to maps of the mass distribution in the
Universe. The magnitude of the shear depends upon the combined effect of
the gravitational potentials between the source and the observer. An
observer will detect this integrated effect and maps of the integrated
mass, or convergence, can be made. Gravitational lensing has the
significant advantage that it is sensitive to both luminous and dark
matter, and can therefore directly detect the combined matter
distribution. In addition, weak lensing convergence maps allow for
detecting the growth of structure in the Universe and hence they can
also be used for probing statistics beyond two point correlation
functions, such as in the higher moments of the convergence field or by
observing the topology of the field with Minkowski functionals and peak
statistics [Dietrich and Hartlap, 2010 , Mawdsley et al. , 2020 ] . As
future surveys attempt to further probe the non-linear regime of
structure growth, the information held in these higher order statistics
will become increasingly important, and will also require accurate
simulations in order to provide cosmological constraints. This
requirement for large numbers of simulations that also model complex
physical phenomena means that more computationally efficient
alternatives to N -body simulations, such as the GAN approach proposed
in this work, are required.

In order to train the GAN algorithm to produce realistic convergence
maps, publicly available datasets were used. In particular, to test
whether we could reproduce the original results from Mustafa et al. [
2019 ] the publicly available data from Mustafa et al. [ 2017 ] was
used. The dataset consists of 8000 weak lensing maps that were
originally produced by running a Gadget2 [Springel, 2005 ] simulation
with @xmath particles in a @xmath @xmath box. To perform ray tracing the
Gadget weak lensing simulation pipeline was used. The simulation box was
rotated multiple times for each ray tracing procedure, resulting in 1000
12 sq. degree maps per simulation box.

In order to train the GAN algorithm on convergence maps of different
cosmologies and redshifts, the dataset publicly available at [Zorrilla
Matilla et al. , 2016 , Gupta et al. , 2018 , Columbia Lensing, 2020 ]
was used. The available dataset contains weak lensing convergence maps
covering a field of view of 3.5 deg @xmath 3.5 deg, with resolution of
1024 @xmath 1024 pixels. The maps were originally produced using Gadget2
DM-only simulation data with 240 Mpc @xmath side box and @xmath
particles. The dataset includes 96 different cosmologies (with varying
@xmath and @xmath parameters). The values of @xmath and @xmath were used
as the fiducial cosmology. In this work only a small subset of this
dataset was used, namely, the maps where only one of the two
cosmological parameter varies. In particular, the dataset consisting of
the maps with @xmath with a common value of @xmath was used. This was
done in order to simplify the latent space analysis.

For the weak lensing map data the same architecture as described in
tables 6.1 and 6.2 was used. In fact the same basic architecture with
minor variations was used for training all the datasets described later
on. The key parameter in terms of the training procedure is the learning
rate. For all the cosmic web slice datasets, I found the learning rate
value of @xmath to work well. In the case of all the considered weak
lensing datasets @xmath was used. The training procedure and all the key
parameters are described in great detail in the publicly available code
(see appendix A.1 for more information).

#### 6.5.2 Cosmic Web Slice Data

The cosmic web or the dark matter overdensity field refers to the
intricate network of filaments and voids as seen in the output data of N
-body simulations. The statistical features of the cosmic web contain
important information about the underlying cosmology and could hide
imprints of modifications to the standard laws of gravity. In addition,
emulating a large number of overdensity fields is important for reliable
estimation of the errors of cosmological parameters. Hence, emulators,
such as the one proposed in this work, are of special importance for the
statistical analysis in the context of the upcoming observational
surveys.

The cosmic web training dataset was produced by employing a similar
procedure to the one outlined in Rodríguez et al. [ 2018 ] . In
particular, we ran L-PICOLA [Howlett et al. , 2015 ] to produce a total
of 15 independent simulation boxes with different cosmologies.
Initially, the same cosmology as described in Rodríguez et al. [ 2018 ]
was used with @xmath , @xmath and @xmath . Subsequently, the effects of
varying one of the cosmological parameters, namely the @xmath parameter,
was studied. The values of @xmath along with @xmath , @xmath and @xmath
were explored. For each different set of simulations, snapshots at 3
different redshifts: @xmath were saved. For each simulation, a box size
of 512 Mpc/ @xmath was used with @xmath particles. For the latent space
interpolation procedure, the GAN was trained on slices with redshifts
@xmath , with a common value of @xmath .

To produce the slices for training the GAN, I used nbodykit [Hand et al.
, 2018 ] , which allows painting an overdensity field from a catalogue
of simulated particles. To obtain the needed slices, the simulation box
was cut into sections of 2 Mpc width in @xmath directions and for each
section a mesh painting procedure was done. This refers to splitting the
section into cells, where the numerical value of each cell corresponds
to the dark matter overdensity @xmath . Finally, after a 2-D projection
of each slice, a @xmath px image was obtained, with each pixel value
corresponding to the overdensity field. To emphasize the features of the
large scale structure, I applied the same non-linear transformation as
described in Rodríguez et al. [ 2018 ] : @xmath , with @xmath , which
rescales the overdensity values to @xmath and increases the contrast of
the images.

In order to emulate modified gravity effects the MG-PICOLA code was
used. MG-PICOLA extends the original L-PICOLA code in order to allow
simulating theories that exhibit scale-dependent growth [Scoccimarro et
al. , 2012 , Tassev et al. , 2013 , Winther et al. , 2017 , H. A.
Winther, 2020 ] . This includes models such as @xmath theories, which
replace the Ricci scalar with a more general function in the
Einstein-Hilbert action (see Li and Koyama [ 2019b ] and chapter 3 for
an overview of the phenomenology of such models). In particular,
multiple runs of MG-PICOLA were run with the following range of the
@xmath parameter: @xmath . Such a wide range was chosen to make the
latent space interpolation procedure easier. The @xmath simulations were
also run with the same seed as the corresponding @xmath CDM simulations,
making the two datasets described above directly comparable.

#### 6.5.3 Dark Matter, Gas and Internal Energy Data

Simultaneously generating dark matter and the corresponding baryonic
overdensity field data is a great challenge from both the theoretical
and the computational perspectives. Namely, generating the baryonic
distribution requires detailed hydrodynamical simulations that account
for the intricacies of galaxy formation and feedback processes, which
leads to a major increase in the required computational resources. For
this reason, emulating large amounts of hydrodynamical simulation data
is of special importance.

To produce the dark matter, baryonic matter and the internal energy
distribution slices I used the publicly available Illustris-3 simulation
data [Vogelsberger et al. , 2014 , Nelson et al. , 2015 ] . Illustris-3
refers to the low resolution Illustris run including the full physics
model with a box size of 75000 @xmath and over @xmath dark matter and
gas tracer particles. The cosmology of the simulation can be summarized
by the following parameters: @xmath , @xmath , @xmath . The simulation
included the following physical effects: radiative gas cooling, star
formation, galactic-scale winds from star formation feedback,
supermassive black hole formation, accretion, and feedback.

To form the training dataset I used an analogous procedure to the one
used for the cosmic web slices in section 6.5.2 . In particular, the
full simulation box was cut into slices of 100 @xmath and for each slice
mesh painting was done to obtain the overdensity field. This was done
for the dark matter and gas data. In addition, the available internal
energy (thermal energy in the units of @xmath ) distribution data was
used as well. Figure 6.4 shows a few samples from the dataset.

To investigate whether the GAN algorithm could be trained on
multidimensional array data, the DM, gas and energy distribution 2-D
slices were treated as RGB planes in a single image. In particular, a
common way of representing colors in an image is forming a full color
image out of three planes, each corresponding to the pixel values for
red, green and blue colours (see figure 5.5 ). In this framework, a
full-color image corresponds to a 3-D array. Convolutional neural
networks, including the one that the cosmoGAN algorithm is based on are
originally designed to be trained on such RGB images. Hence we combined
the mentioned DM, gas and internal energy slices into a set of RGB
arrays that were used as a training set.

#### 6.5.4 The Training Procedure

The initial stages of training (i.e. reproducing the results in
[Rodríguez et al. , 2018 , Mustafa et al. , 2019 ] were done using the
Google Cloud computing platform. The following setup was used: 4
standard vCPUs with 15 GB memory, 1 NVIDIA Tesla K80 GPU and 2TB of SSD
hard drive space.

Later stages of training (i.e. training the GAN on different cosmology,
modified gravity and redshift data) were done using the local Sciama HPC
cluster, which has 3702 cores of 2.66 GHz Intel Xeon processors with 2
GB of memory per core.

Given how unstable the GAN training procedure is, a simple way of
evaluating the best checkpoint was used: I calculated the mean square
difference between the mean values of the GAN-produced and the training
dataset power spectra, pixel histograms and the Minkowski functionals.
The set of GAN weights that minimizes this value was used for the plots
displayed in the result section.

### 6.6 Diagnostics

A key aspect of the analysis of the produced samples is being able to
quantify how realistic the GAN-generated data is. This was done at an
ensemble level – i.e. we generated multiple batches of data (see figures
in section 6.7 ) and calculated the average summary statistics, which
were then compared against analogous results produced using the training
dataset.

The results produced by the algorithm were investigated using the
following diagnostics: the 2-D matter power spectrum, overdensity
(pixel) value histogram and the three Minkowski functionals. In
addition, the cross and the auto power spectrum were computed in order
to investigate the correlations between the datasets on different
scales. The cross-power spectrum was calculated using:

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

where @xmath and @xmath are the Fourier transforms of the two
overdensity fields at some Fourier bin @xmath and @xmath is the Dirac
delta function.

The Minkowski functionals are a useful tool in studying the
morphological features of fields that provide not only the information
of spatial correlations but also the information on object shapes and
topology. For some field @xmath in 2-D we can define the three Minkowski
functionals as follows:

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

Where @xmath is the area and @xmath is the boundary of the field at the
threshold value @xmath . The integrals @xmath , @xmath , @xmath
correspond to the area, boundary length and the integrated geodesic
curvature @xmath along the boundary. To put it simply, the procedure of
measuring the Minkowski functionals refers to taking the values of the
field at and above a given threshold @xmath , evaluating the integrals
in eq. 6.5 and then changing the threshold for a range of values. In the
case of the 2-D fields one can imagine the field values at different
positions as height in the third dimension. Then a 2-D convergence map
or an overdensity field can be visualised as a 3-D surface. And the
Minkowski functionals then correspond to taking slices of the 3-D
surface at and above the different heights and measuring the area, curve
length and the geodesic curvature as described in equation 6.5 . In this
way Minkowski functionals allow to capture detailed morphological
features of the generated field data which can then be directly compared
against the training dataset.

Minkowski functionals are also a useful tool in weak lensing convergence
map studies as they allow us to capture non-Gaussian information on the
small scales, which is not fully accessed by the power spectrum alone.
In addition, Minkowski functionals have been used to detect different
cosmologies, modified gravity models and the effects of massive
neutrinos in weak lensing convergence maps [Petri et al. , 2013 , Ling
et al. , 2015 , Marques et al. , 2019 ] . Given the usefulness of
Minkowski functionals in accessing the non-Gaussian information on the
small scales, the functionals were chosen for studying the produced
cosmic web data as well. To calculate the Minkowski functionals properly
on a 2-D grid I used the minkfncts2d algorithm, which utilizes a
marching square algorithm as well as pixel weighting to capture the
boundary lengths correctly [Mantz et al. , 2008 , package, 2020 ] .

Minkowski functionals are sensitive to the Gaussian smoothing applied to
the GAN-produced images and the training data. Hence, it is important to
study the effects of Gaussian smoothing as it might give a deeper
insight into the detected differences between the datasets. The
procedure of smoothing refers to a convolution between a chosen kernel
and the pixels of an image. In more detail, a chosen kernel matrix is
centered on each pixel of an image and each surounding pixel is
multiplied by the values of the kernel and subsequently summed. In the
simplest case, such a procedure corresponds to averaging a chosen number
of pixels in a given image. In the case of Gaussian filtering, a
Gaussian kernel is used instead.

To filter the noise we used Gaussian smoothing with a @xmath kernel
window and a standard deviation of 1 px. The Minkowski functionals were
found to be especially sensitive to any kind of smoothing. For instance,
the position and the shape of the trough of the third Minkowski
functional is highly sensitive to the existence of any small-scale
noise. Figure 6.5 illustrates the effects of Gaussian smoothing with
different kernel sizes on the three Minkowski functionals.

### 6.7 Results

#### 6.7.1 Weak Lensing Map Results

After around 150 epochs (corresponding to around 96 hours on a local
HPC) the GAN started producing statistically realistic convergence maps
as measured by the power spectrum and the Minkowski functionals. The
diagnostics were computed at an ensemble level – 100 batches of 64
convergence maps were produced by the GAN and the mean values along with
the standard deviation were computed and compared with the training
data. An analogous procedure was done when calculating the pixel
intensity distribution histograms.

The power spectra agree well between the GAN-produced and the training
data, with minor differences on the small scales (see figure 6.6 ). In
particular, the difference between the training and the GAN-produced
dataset power spectra is around 5% or lower for most values of @xmath .
Only at the smallest scales a significant difference of 10% is reached.
Similarly, the pixel intensity histogram in general shows a good
agreement with significant differences appearing only for the highest
and the lowest pixel intensity values (which is also detected in the
original work in Mustafa et al. [ 2019 ] ). A selection of GAN-produced
maps are presented for visual inspection in figure A.1 .

Minkowski functionals were also calculated for the GAN-produced and the
training datasets. The results are shown in figure 6.7 . In general
there is a good agreement between the training data and the GAN-produced
maps, given the standard deviation, however, some minor differences can
be detected in the Euler characteristic and the boundary functional,
likely resulting from noise.

#### 6.7.2 Weak Lensing Maps of Multiple Cosmologies

The results also indicate that the GAN is capable of producing realistic
weak lensing maps for multiple cosmologies. This is an important result
as it shows that the algorithm is able to pick up on the various subtle
statistical differences between different cosmologies that usually
requires a detailed study of the power spectrum, Minkowski functionals
and other statistics.

However, the training procedure was found to be highly prone to mode
collapse. A wide hyperparameter search had to be performed to find an
optimal set of parameters that did not lead to full or partial mode
collapse. The most important parameter in this context was found to be
the learning rate. As a rule of thumb, decreasing the learning rate led
to mode collapse happening later in the training procedure. When the
learning rate was reduced below a certain value (discussed further in
the analysis section), mode collapse was avoided altogether. As in the
case with the cosmic web slice data, applying a transformation to each
pixel of the image in order to increase the contrast had a positive
effect in reducing the probability of mode collapse as well.

Figure 6.8 summarizes the results of training the GAN on shear maps with
different @xmath values. The results indicate an agreement of the power
spectra in the range of 5-10% for @xmath @xmath @xmath for @xmath . In
the case of @xmath the agreement is significantly better, ranging
between 1-3% on most scales. Interestingly, Gaussian smoothing increases
the difference to around 5-15% in this particular case. This shows that
for this dataset Gaussian noise is not the major source of the
statistical differences between the training and the GAN-generated
datasets.

Figure 6.9 compares the Minkowski functionals calculated using the
training and the GAN-produced datasets. Given the standard deviation in
both datasets, the results overlap for all threshold values. However,
for thresholds in the range of @xmath there is a significant difference
between the training and the GAN-generated datasets. We found that this
is partially due to small-scale noise in the GAN-produced data (see
figure 6.5 ). However, after experimenting with adding artificial noise
to the training dataset images, it is clear that the noise alone cannot
fully account for the observed differences in the Minkowski functionals.
Another reason for the observed differences could be a relatively small
size of the used dataset consisting of a few thousand weak lensing maps.
It is likely that having more training data samples could significantly
improve the results.

#### 6.7.3 Cosmic Web for Multiple Redshifts

The results also indicate that the GAN approach is capable of producing
realistic cosmic web slices for different redshifts. As before with the
weak lensing maps of different cosmologies, this illustrates that the
algorithm in general does not get confused between the two different
redshifts and is capable of detecting subtle statistical differences
between the different datasets (figure 6.10 ). In addition, I found that
using Gaussian smoothing, as before, led to a better agreement between
the training and the GAN-produced datasets. The effect is especially
noticeable in the Minkowski functional analysis (figure 6.11 ). Visual
samples of the produced cosmic web slices are shown in figure A.2 .

The power spectra results for both redshift values were found to be very
similar. Namely, for the non-smoothed case the difference between the
training and the GAN-produced power specta ranges between 5-10%. The
results are similar for the smoothed case, with exception of @xmath
values around 1 @xmath @xmath where the difference reaches 20%.

The effects of the Gaussian smoothing on both the power spectra and the
Minkowski functionals illustrate that one of the reasons for the
differences between the GAN-generated and the training datasets is noise
appearing on different scales in the GAN-produced images. Applying
Gaussian smoothing, in general, filters the majority of such noise,
however, it cannot fully account for all the differences appearing in
the different statistical diagnostics. In addition, smoothing can
improve the results on some scales, while worsening them on others. As
an example, in figure 6.10 , Gaussian smoothing increases the difference
between the GAN-produced and the training dataset power spectra on the
smallest scales.

#### 6.7.4 Cosmic Web for Multiple Cosmologies and Modified Gravity
Models

Training the GAN on the cosmic web slices of different cosmologies and
modified gravity models offered another way of testing whether the
algorithm would pick up on the subtle statistical differences between
the different datasets. In addition, the classification task for the
discriminator neural network is more difficult when training on datasets
with multiple cosmologies leading to longer training times.

The results indicate that the GAN is indeed capable of producing
statistically realistic cosmic web data of different cosmologies and
modified gravity models. With no Gaussian smoothing applied, the
relative agreement between the power spectra is 1-10% (see figure 6.12
). Applying smoothing in this case resulted in increasing the relative
power spectrum difference to over 10% on average. In the case of cosmic
web slices for different @xmath values, the agreement between the two
datasets was good, ranging between 1-10% on all scales. Smoothing
improved the situation only in the mid-range of the covered @xmath
values, reducing the agreement on the smallest scales (see figure 6.14
).

Figure 6.13 shows the Minkowski functional analysis. In this case, very
little deviation is observed. In general, there is a good agreement
between the GAN-produced and the training datasets, especially for the
first and the second Minkowski functionals. For the third Minkowski
functional, the results diverge around the lower trough area, which is
also observed for other datasets. This is at least in part related to
small-scale noise as indicated by the previous analysis.

The results are similar for the GAN trained on cosmic web slices
corresponding to different @xmath models (figure 6.15 ). In general, a
good agreement between the datasets was found (given the standard
deviation of the data and the GAN-produced results). Gaussian smoothing,
in this case, was more effective in reducing some of the offset observed
in the power spectrum analysis. However, it increased the offset on the
smallest scales.

#### 6.7.5 Dark Matter, Gas and Internal Energy Results

In the case of training the GAN algorithm on multiple components at the
same time, the training procedure was relatively quick and efficient
(around 1.3 times quicker compared to the datasets discussed previously)
despite the training dataset being 3 times bigger. This is most likely
due to the fact that the cosmic web slices in this particular dataset
corresponded to a much larger simulation box and hence were not as
detailed on the smallest scales.

As before, the relative difference between the GAN-produced and the
training datasets was calculated. The internal energy slices were
analysed using Minkowski functionals as well as the cross-power spectrum
(figure 6.16 ). The analysis was done for both dark matter and the gas
components. The relative difference between the power spectra for both
DM and gas cosmic web slices was found to be at around 5% level for all
the covered range. Gaussian smoothing reduced this value to 1-5%. In
addition, the cross-power spectrum was calculated for all the
components. For both the dark matter-gas and the gas-energy pairs there
is a good agreement between the training and the GAN-produced datasets
given the large standard deviation. Both plots show values well above
zero for most @xmath values, indicating a significant correlation
between the dark matter and the corresponding gas as well as the
internal energy distributions on all scales as expected.

The Minkowski functional analysis (figure 6.17 ) revealed a generally
good agreement between the two datasets, with significant differences
appearing only in the boundary and the Euler characteristic Minkowski
functionals for the energy cosmic web slices. This is somewhat
surprising as the internal energy slices, in general, are significantly
less complex on the smallest of scales when compared to the
corresponding dark matter and gas data (see figure 6.4 ), hence we
expected the GAN to easily learn to reproduce the named dataset.
However, we also found that the internal energy data and the
corresponding Minkowski functionals are especially sensitive to adding
any small scale artificial noise. A more detailed Minkowski functional
analysis is required to determine the reason for this divergence.

#### 6.7.6 Latent Space Interpolation Results

To perform the latent space interpolation procedure I trained the GAN to
produce cosmic web slices of two different redshifts along with weak
lensing maps of different @xmath values. Once trained, a batch of
outputs was produced and in each case a pair of slices/maps
corresponding to different redshifts or @xmath values was chosen.
Subsequently, I interpolated between the input points @xmath and @xmath
corresponding to the outputs with different redshifts and @xmath values
(see figure 6.2 ).

Figure 6.18 illustrates the results of the latent space interpolation
procedure. In particular, it shows that the technique does indeed
produce intermediate power spectra. However, the transition is not
linear – the power spectra lines corresponding to equally spaced inputs
(in the latent space) are not equally spaced in the power spectrum
space. This is the case as the produced data samples can be described as
points on a Riemannian manifold, which in general has curvature (see
appendix 6.4 for more details).

Figure 6.18 and 6.19 show the results of interpolating between cosmic
web slices with redshifts @xmath and @xmath and weak lensing maps with
@xmath and @xmath . The interpolated samples are statistically realistic
and the transition is nearly smooth. The power spectrum analysis was
done by comparing 100 latent space points drawn from the central region
(equal in length to 1/4 of the total length of the line) of the line
connecting the two latent space clusters corresponding to the different
redshifts and @xmath values against 100 training data samples (see
figure 6.2 and 6.18 for more information). The intermediate power
spectra was found to be in good agreement.

An important part of the latent space interpolation procedure is being
able to distinguish between the GAN-generated cosmic web slices and weak
lensing maps of different redshifts, cosmologies and modified gravity
parameters. In this regard, I have tested two machine learning
algorithms: a convolutional neural network and gradient boosted decision
trees. Initially a convoluational neural network architecture described
in table 6.2 was used, as we already knew that such neural networks are
effective in classifying cosmic web slices and convergence maps. This
resulted in accuracy of around @xmath % when classifying unseen data
samples. However, the training procedure was prone to overfitting,
requiring a thorough hyperparameter optimization. In addition, I found
that the small scale noise appearing would highly reduce the prediction
accuracy of the CNN. This is a known problem in the deep learning
literature and can be mitigated to a certain degree by adding artificial
noise to the training dataset [Liu et al. , 2017 ] . However, finding
the right amount of noise needed to mimic the noise appearing in the
GAN-generated outputs is difficult.

The gradient boosted decision tree algorithm ( XGBoost [Chen and
Guestrin, 2016 ] ) was found to be faster and more accurate in
predicting the dataset class. In particular, 95-98% accuracy was reached
(depending on the dataset and hyperparameters used), when predicting the
dataset class of unseen test samples. Table 6.3 summarizes the
parameters used when training the XGBoost algorithm.

Combining such a machine learning approach with a power spectrum
analysis allowed us to distinguish between the different classes of the
GAN-produced outputs reliably.

The latent space interpolation results illustrate a number of
interesting features of GANs. Firstly, the results illustrate that the
GAN training procedure tightly encodes the various features discovered
in our training dataset in the high-dimensional latent space. By finding
clusters in this latent space, corresponding to outputs of different
redshifts or cosmology parameters, and linearly interpolating between
them, we can produce outputs with intermediate values of the mentioned
parameters. This allows us to control the outputs produced by the
generator.

### 6.8 Analysis and Conclusions

The main goal of this work was to investigate whether GANs can be used
as a universal, fast and efficient emulator capable of producing
realistic and novel mock data. The results of this work are encouraging,
illustrating that GANs are indeed capable of producing realistic mock
datasets. In addition, I have shown that GANs can be used to emulate
dark matter, gas and internal energy distribution data simultaneously.
This is a key result, as generating realistic gas distributions requires
complex and computationally expensive hydrodynamical simulations. Hence,
producing vast amounts of realistic multi-component mock data quickly
and efficiently will be of special importance in the context of upcoming
observational surveys.

The GAN-produced data in general cannot be distinguished from the
training dataset visually. In terms of the power spectrum analysis, the
relative difference between the GAN-produced and the training data
ranges between 1-20% depending on the dataset and whether Gaussian
smoothing was applied. The Minkowski functional analysis revealed a
generally good agreement between the two datasets with an exception of
the third Minkowski functional corresponding to curvature, which showed
subtle differences for all studied datasets. In addition, greater
differences were observed when training the GAN on datasets with
multiple data classes. This is somewhat expected, as the training task
becomes more difficult. In general, these differences can be partially
accounted for as a result of small-scale noise in the GAN-generated
images. Gaussian smoothing with a @xmath pixel kernel size was found to
be effective in filtering away most of such noise. In addition, the
training datasets used in this work are smaller than those used in
[Rodríguez et al. , 2018 , Mustafa et al. , 2019 ] , which, at least
partially, accounts for the differences between our and their
corresponding results.

A commonly used technique of latent space interpolation was also
investigated as a tool for controlling the outputs of the generator
neural network. Interestingly, the results indicated that such a
procedure allows us to generate samples with intermediate
redshift/cosmology/ @xmath parameter values, even if our model had not
been explicitly trained on those particular values. In general, the
latent space interpolation procedure offers a powerful way of
controlling the outputs of the GAN as well as a tool for investigating
the feature space of the generator neural network. However, it is
important to point out some of the drawbacks of this procedure. Namely,
as pointed out in machine learning literature, the latent space of a
convolutional GAN is known to be entangled . In other words, moving in a
different direction in the latent space necessarily causes multiple
changes to the outputs of the GAN. As a concrete example, finding a
latent space line that induces a change in redshift of a given output
necessarily also introduces other subtle changes to the output (e.g. the
depth of the voids or the distribution of the filaments). So if we take
a random output of redshift @xmath and perform the linear interpolation
procedure to obtain a cosmic web slice of @xmath , the obtained slice
will correspond to a realistic but different distribution of the
required redshift. This is a drawback as in an ideal case we would love
to have full control of individual parameters, while not affecting other
independent features of a dataset. There are however other generative
models discussed in the literature that allow such manipulation of the
latent space. Namely, the @xmath -VAE variational autoencoder and the
InfoGAN algorithms, allow encoding features into the latent space in a
special way that allows full control of individual key parameters
without affecting the other features of the dataset (latent space
disentanglement) [Chen and Guestrin, 2016 , Higgins et al. , 2017 ,
Burgess et al. , 2018 ] .

Another important pitfall to discuss is the problem of mode collapse. As
is widely discussed in the literature, the generator neural network is
prone to getting stuck in producing a very small subsample of realistic
mock datapoints that fool the discriminator neural network. Resolving
mode collapse is an important open problem in the field of deep
learning, with a variety of known strategies ranging from choosing a
particular GAN architecture, to altering the training procedure or the
cost function [Srivastava et al. , 2017 , Yicheng and Hong, 2019 ] .
Mode collapse was encountered multiple times in our training procedure
as well. As a rule of thumb, I found that reducing the learning rate
parameter had the biggest effect towards resolving mode collapse for all
studied datasets. Learning rates around the values of @xmath for the
cosmic web data and @xmath for the weak lensing maps were found to be
the most effective in avoiding any mode collapse.

As indicated by the results, GANs can be used to generate novel 2-D data
efficiently. A natural question to ask is whether this also applies to
3-D data. As an example, an analogous emulator capable of generating 3-D
cosmic web data, such as that produced by state of the art hydrodynamic
and DM-only simulations would be very useful. In principle there is no
limit on the dimensionality of the data used for training a GAN,
however, in practice, going from 2-D to 3-D data leads to a significant
increase of the generator and the discriminator networks. In addition,
in the case of 3-D cosmic web data, forming a big enough training
dataset would become an issue, as running thousands of simulations would
be required. However, as previously mentioned, there are sophisticated
ways of emulating 3-D cosmic web data as shown in [Perraudin et al. ,
2019 ] , where a system of GANs is used to upscale small resolution
comic web cubes to full size simulation boxes. Note that the techniques
introduced in this work (e.g. latent space interpolation) can be readily
combined with the mentioned 3-D approach.

A number of interesting directions can be explored in future work.
Namely, it would be interesting to further investigate the latent space
interpolation techniques in the context of more advanced generative
models, such as the InfoGAN algorithm. In addition, a more detailed
investigation into the Riemannian geometry of GANs could lead to a
better understanding of the feature space of the algorithm. Finally,
many other datasets could be explored. With upcoming surveys such as
Euclid generating mock galaxy and galaxy cluster data quickly and
efficiently is of special interest. A GAN could be used to generate
galaxies with realistic intrinsic alignments, density distributions and
other properties. Similarly, GANs could be used to quickly emulate
realistic galaxy cluster density distributions at a fraction of the
computational cost required to run full hydrodynamic simulations.

To conclude, GANs offer an entirely new approach for cosmological data
emulation. Such a game theory based approach has been demonstrated to
offer a quick and efficient way of producing novel data for a low
computational cost. As we have shown in this work, the trade-off for
this is a 1-20% difference in the power spectrum, which can be
satisfactory or not depending on what application such an emulator is
used for. Even though a number of questions remain to be answered
regarding the stability of the training procedure and training on higher
dimensional data, GANs will undoubtedly be a useful tool for emulating
cosmological data in the era of modern N -body simulations and precision
cosmology.

## Chapter 7 Conclusions and Future Work

The main goal of this thesis was to introduce tools and techniques for
studying modified gravity. In summary, a method of testing modified
gravity, first introduced in Terukina et al. [ 2014 ] and Wilcox et al.
[ 2015 ] , was extended by generating a new more accurate dataset and by
testing a new theory. In addition, machine learning techniques were
explored in the context of emulating @xmath CDM and modified gravity
@xmath -body simulations.

In terms of all the mentioned projects, a lot of work remains to be
done. In particular, the outlined technique of testing modified gravity
relies on stacking multiple galaxy clusters. Stacking clusters, in some
sense, produces an idealized galaxy cluster by averaging out the various
irregularities that individual clusters possess. This is a powerful
technique capable of producing competitive modified gravity constraints,
however, the produced stack dataset is only an approximation of real
galaxy clusters. In nature, no cluster is exactly spherical, hence it is
important to understand what effects deviations from spherical symmetry
would have in the context of chameleon gravity. Deviations from
spherical symmetry must be better understood on two fronts. Firstly, the
hydrostatic equilibrium equations used in our work must be generalized
for arbitrary 3-D mass, pressure/temperature and surface brightness
distributions. Alternatively, the bias due to non-spherical mass
distributions can be quantified and accounted for in the mass
calculations. Such bias has been studied both observationally and in the
context of hydrodynamic simulations, e.g. see Morandi et al. [ 2010 ],
Martizzi and Agrusa [ 2016 ] . Secondly, it is important to understand
how the theoretical predictions are affected by breaking the assumption
of spherical symmetry. This is of special importance to the model of EG,
the main predictions of which were derived under the key assumption of
spherical symmetry. Hence, the predictions of the model for the
relationship between the baryonic and the apparent dark matter
distributions must be generalized for arbitrary mass distributions. This
might be easier to accomplish in different models of EG, such as
Hossenfelder’s covariant EG.

The original motivation for stacking galaxy clusters is mainly due to
the dominant weak lensing profile errors. This particular issue will be
possible to address when the newest data from DES and future surveys
such as Euclid becomes available. Another approach is to employ
different data. As shown in Terukina et al. [ 2014 ] , stringent
constraints can be calculated using the data from a single cluster.
However, the main difference in that work when compared to our approach
is that the SZ effect and temperature profile data is used in addition
to the surface brightness and weak lensing profiles. As the mentioned
results show, including these extra datasets leads to constraints
comparable to the results presented in this work, even if a single
cluster with high quality is used rather than a stack of clusters.
Hence, a straightforward extension of our work is to introduce extra
datasets, which could significantly improve the constraints.

In addition to the previously mentioned shortcomings of EG, it is
important to mention the lack of a rigid description of cosmology and
weak lensing. More concretely, the original formulation of EG as
described in Verlinde [ 2017 ] is only valid for redshifts @xmath and
@xmath . In addition, a thorough description of lensing would require
deriving a geodesic equation, which, in turn, requires a full covariant
description of the theory. A covariant description of the theory does
exist, as described in Hossenfelder [ 2017 ] , however, the lensing
equations have not been derived yet. Hence, on the theoretical front,
the key issues to tackle are related to extending the original
Verlinde’s EG framework to account for cosmological effects and weak
lensing. Alternatively, Hossenfelder’s framework can be extended by
deriving the geodesic equation. Here, however, it is important to point
out that, strictly speaking, these two theories are not identical and
agree only in the non-relativistic, stationary spherical mass
distribution limit. The exact relationship between the two approaches
deserves a more detailed investigation as well.

In summary, EG undoubtedly suffers from certain theoretical and
observational shortcomings. However, the theory has been successful in
encouraging further studies of the various connections between
thermodynamics and gravity. Multiple recent studies have extended
Verlinde’s ideas in different contexts, e.g. Vacaru and Bubuianu [ 2019
], Peach [ 2019 ] . Finally, Verlinde’s work has also contributed to the
current resurgence of the rather unique approach of treating gravity as
spacetime elasticity previously studied in Visser [ 2002 ], Padmanabhan
[ 2004 ] .

In terms of the machine learning algorithms discussed in chapters 5 and
6 a lot of work remains to be done as well. As mentioned, the results in
Tamosiunas et al. [ 2020 ] are encouraging, however, multiple issues
remain to be addressed. Firstly, using GANs for emulating 3-D simulation
data remains an issue in terms of the memory issues and the availability
of large 3-D training datasets. These and other issues have already been
partially addressed in the literature. As an example, the training
procedure of the GAN can be modified such that small 3-D overdensity
cubes are patched together to form a full-size 3-D overdensity field as
shown in Perraudin et al. [ 2019 ] . An alternative approach is to start
with a low resolution mock dataset, which is gradually upscaled during a
multi-stage training procedure (e.g. see Ledig et al. [ 2016 ] ). These
sort of approaches combined with modern GPU training will likely make
emulating 3-D datasets easy and efficient in the near future.

Another key issue encountered in our work was controlling the outputs.
In particular, when training the algorithm on a dataset consisting of
different data classes, the simple DCGAN architecture does not allow to
control which outputs will be produced. In other words, one cannot
choose the output class due to the inherent randomness of the generation
procedure. The proposed workaround discussed in this work was to use the
latent space interpolation procedure, which resolves the problem
partially. However, the key issue with the procedure is that it is not
efficient as it requires one to manually detect interesting regions of
the latent space that can be used in the interpolation. A much more
elegant approach is to use a different GAN architecture that is
specifically designed for producing multi-class data. In particular, the
conditional GAN architecture (CGAN) allows to take full control of the
generation procedure. In this architecture the neural networks are
trained using labeled data and hence the outputs of the generator can be
controlled by directly specifying the label of the class to be
generated. As a concrete example, this type of architecture has been
recently used to generate convergence maps with different cosmological
parameters as described in Perraudin et al. [ 2020 ] . Hence a natural
extension of our work would be to apply the techniques described in this
thesis on different GAN architectures. This would lead not only to more
control of the data generation, but also to a better understanding of
the latent space. Finally, alternative architectures, such as the
mentioned CGAN algorithm, would make it easier to explore the Riemannian
geometry of the latent space produced during the training procedure.

As shown in this work, generative models offer a completely new approach
of generating mock data. Like any algorithm such models come with a set
of shortcomings. However, the ability to generate large multi-class mock
datasets quickly and efficiently undoubtedly makes such algorithms
useful. This is especially true in the context of the upcoming large
scale surveys such as Euclid and SKA, which will require accurate and
fast emulators.

In conclusion, observational tests of gravity have traditionally played
an important role in the theoretical development of theories of modified
gravity and dark energy. Starting with the initial tests of GR and
ending with cutting edge gravitational wave tests, viable theories of
gravity have always been firmly constrained by the most recent
observational data. The set of techniques described in this work draws
an optimistic picture for the near future of cosmological tests of
modified gravity. With the next generation of observational surveys and
new high quality data becoming available, constraints of unprecedented
accuracy will become possible. Similarly, with the new machine learning
techniques becoming available, new ways of emulating modified gravity
will become possible as well. And hence the techniques described in this
work will hopefully play an important role in forming a better
understanding of gravity.

## Appendix A

### a.1 Availability of Data and Codes

The key datasets generated and analysed in this work are available at
the following GitHub repository: https://github.com/AndriusT/cw_wl_GAN .
The link also contains detailed instructions on how to produce the data
samples from the publicly available Illustris data. The full Illustris
datasets can be found at: https://www.illustris-project.org/data/ .

The used weak lensing data can be accessed at:
http://columbialensing.org/ .

### a.2 Samples of the GAN-produced Data

This section contains a selection of GAN-produced samples for visual
inspection. Fig. A.1 contains randomly selected weak lensing convergence
maps produced by the GAN algorithm (these are the samples described in
sections 6.7.2 and 6.7.3 ).

Fig. A.2 shows a selection of randomly selected cosmic web 2-D slices
for two different redshifts. Both the training data and the produced
slices have been Gaussian-smoothed.

### a.3 MCMC Contours

This section contains the MCMC contours along with the corresponding
likelihood distributions for the key datasets used in chapter 3 . In
particular, the MCMC results from Terukina et al. [ 2014 ] , Wilcox et
al. [ 2015 ] and our results produced using the newest dataset
consisting of 77 galaxy clusters. In all cases, the light gray contours
correspond to the 99% CL, while the dark gray contours are the 95% CL
for each best-fit parameter. Notice also that the colors in the
individual modified gravity parameter plots appearing in chapter 3 are
inverted for the sake of clarity when comparing the results from
different papers.