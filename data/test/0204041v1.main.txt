##### Contents

-    I Trust Broking
    -    1 Application: Universally Accessible Personal Information
        -    1.1 People
            -    1.1.1 Paper Lives
            -    1.1.2 Electronic Lives in a Honest World
            -    1.1.3 Electronic Lives in the Real World
        -    1.2 Organisations
-    II Database Security on the World–Wide Web
    -    2 Technology Model: The World–Wide Web
    -    3 Enterprise Model: Contracts
        -    3.1 Parties
        -    3.2 Grades of Anonymity
        -    3.3 Views of Records
        -    3.4 Duties of a Custodian to a Subject
        -    3.5 Duties of the Owner to the Custodian
        -    3.6 Duties of the Accessors to the Custodian
        -    3.7 Duties of the Custodian to the Accessors
    -    4 Enterprise Model: Adaptability Issues
    -    5 Rôle–based Access Control Systems: Information Model
    -    6 Computational Model
        -    6.1 Generating Views of Records
        -    6.2 Classifying Accessors
        -    6.3 Classifying Records and Fields
        -    6.4 Generating Access Rules
    -    7 Information Model — Views and Constraints
        -    7.1 Granting Access to Views
        -    7.2 Proofs of Contract Compliance
        -    7.3 Adaptability Support
    -    8 Engineering Model
        -    8.1 Functional Specifications
        -    8.2 Agents
    -    9 Enterprise Models: System Agents
    -    10 Engineering Model
        -    10.1 Accessor Memberships Agent
        -    10.2 Database Trader
        -    10.3 Access Negotiator
        -    10.4 Query Processing
            -    10.4.1 Query Formulator
            -    10.4.2 Query Delivery Agent
            -    10.4.3 Results Delivery Agent
        -    10.5 Results Presentation Agent
    -    11 Summary
    -    12 Issues
        -    12.1 Capabilities of Databases
        -    12.2 Ownership and Autonomy
        -    12.3 Portability and Adaptability
        -    12.4 Security and Safety
            -    12.4.1 Security
            -    12.4.2 Safety
    -    13 Technological Components
    -    14 System Agents
    -    15 Adaptive Components
    -    16 Software
        -    16.1 Implementation Engineering
        -    16.2 Design Engineering
    -    17 Testing and Discussion
        -    17.1 Tests
        -    17.2 Discussion
    -    18 Realizing the Proposed System
-    III Political Control Mechanisms
    -    19 Authorisation Policies for Databases
        -    19.1 Mandatory and Discretionary Access Control Policies
        -    19.2 Authoritarian and Self–governing Access Control
        -    19.3 Adaptive Discretionary Access Control Policies
        -    19.4 Preferential Logics and Operators
    -    20 Suitable Languages
    -    21 Enterprises, Goals and Norms
    -    22 Summary
    -    23 Generating Security Hierarchies
    -    24 Preferences, Values and Norms
        -    24.1 Norms from Preferences
        -    24.2 Values from Preferences
        -    24.3 Values are Relative
        -    24.4 Trading Goals
    -    25 Organisational Structure
        -    25.1 Managing Database Access
        -    25.2 Lattices as Organisational Structures
    -    26 About Preference Aggregation
        -    26.1 History and Importance
        -    26.2 Structure and Notes
            -    26.2.1 Preferences and Indifferences
            -    26.2.2 Lattices
        -    26.3 Methods and Representations
    -    27 Cycles and Topological Entropy
        -    27.1 Using an Adjacency Matrix
        -    27.2 Using a Transition Matrix
    -    28 Unanimities
        -    28.1 P–graph aggregates
        -    28.2 Unanimities, Sources and Sinks
        -    28.3 Unanimous Properties
        -    28.4 Degree Of Unanimity
    -    29 Chains
        -    29.1 Size of Largest Anti–Chain
    -    30 Distance Functions
        -    30.1 Bigraphs
        -    30.2 Changing the Orientation of Edges
        -    30.3 Maximum Likelihood Preference Relations
    -    31 Deciding Sets
        -    31.1 Spectral Analysis of Ranked Data
    -    32 In Conclusion
    -    33 Formation of Cultures
        -    33.1 Small–Scale Behaviour
        -    33.2 Local convergence leads to global polarisation
        -    33.3 Large–Scale Behaviour
        -    33.4 The Collective Choice Interpretation
    -    34 Formation of Cultures under Peer Pressure
        -    34.1 Egoistic Behaviour
        -    34.2 Peer Agreement
        -    34.3 Some Expectations
        -    34.4 Some Results
        -    34.5 Some Conclusions
    -    35 Predictability of Large–Scale Behaviour
        -    35.1 Analytic Research
            -    35.1.1 Voter Models and Initial Distributions
            -    35.1.2 Topologies and Initial Distributions
        -    35.2 Experimental Investigation
        -    35.3 Sequences of Interaction
        -    35.4 Collective Choice
    -    36 Summary
    -    37 Self–Organising Newsgroups
        -    37.1 Newsgroup Operation
            -    37.1.1 How it might work
            -    37.1.2 Initial Grouping
            -    37.1.3 New postings
            -    37.1.4 Group Preference Hierarchy
            -    37.1.5 New postings with a group hierarchy
        -    37.2 Summary
        -    37.3 The Cultural Model and Stability
    -    38 Self–Organising Permissions Policy System
        -    38.1 Newsgroups: Summarised
        -    38.2 Rôle–based Access Control System
        -    38.3 Healthcare
            -    38.3.1 Simplification of the Rôle Space
            -    38.3.2 Rôles and Interactions
            -    38.3.3 Features, Traits and a Data Structure
            -    38.3.4 Ontological Features and Traits
            -    38.3.5 Inconsistencies and Distance
        -    38.4 Collective Choice Expert System
        -    38.5 Comparison with Newsgroups Operation
    -    39 Conclusions
    -    40 Discussion
    -    41 Future Work
    -    42 Conclusion
    -    A Aidan Project Software System
    -    B Application Programs and Their Configurations
        -    B.1 Web-server
        -    B.2 Database
        -    B.3 JDBC Driver
    -    C Distributed Processing
        -    C.1 Computational Design Guidelines
    -    D Definitions
    -    E Conditions and Deductions
    -    F Representations
        -    F.1 Illustration
        -    F.2 Adjacency Matrices
        -    F.3 Transition Matrices
        -    F.4 Reachability and Reaching Matrices
        -    F.5 Chains and Anti–Chains
    -    G Overview
    -    H Values and Beliefs
    -    I Deontics
    -    J Knowledge Query and Manipulation Language
    -    K Knowledge Interchange Format

###### List of Tables

-    1 Name to Identity Relationships for Medical Information
-    2 Ownership, trust and use relationships for system agents
-    3 Prototype system: implementation vs. requirements (I)
-    4 Prototype system: implementation vs. requirements (II)
-    5 Test: Machines
-    6 Database Query Access Times
-    7 The Borda “Preferendum”
-    8 Comparative metrics for Security Clearance Class Membership
-    9 The voting paradox
-    10 Topological Entropy for the Borda “Preferendum” with and without
    an irrelevant alternative
-    11 Number of Different Preference Orderings for @xmath policies
-    12 Paired comparisons, @xmath , @xmath for all @xmath and @xmath
-    13 Sub–bigraphs uncertainty rankings
-    14 Identities in an evolved culture
-    15 Version Information
-    16 Aidan Directory Resources
-    17 Database Enquiry Resource
-    18 Database of Users
-    19 Authorisation Filter
-    20 A user account
-    21 Preferences
-    22 KQML Performatives (A to G) for sender S and recipient R
-    23 KQML Performatives (I to U) for sender S and recipient R
-    24 KQML Parameters for Performatives

###### List of Figures

-    1 Web–site used to provide personal information
-    2 Using the UK’s Driver Vehicle Licensing Agency
-    3 Access control mechanisms in place for Tax Details
-    4 Role–based access control: object information model
-    5 Role–based access control: subject information model
-    6 Role–based access control: constraint information model
-    7 Some example of record views
-    8 Medical practice and health authority
-    9 Consultancy and research body
-    10 Accessor Memberships Agent
-    11 Trader Agent
-    12 Database, Views, Ontology and Groups
-    13 Access Negotiator Agent
-    14 Custodians establishing new precedents
-    15 Query Formulation and delivery; results delivery and
    presentation
-    16 A take–grant path across islands and bridges
-    17 A lattice produced from a set of preferences
-    18 The voting paradox
-    19 Simplex for Voting
-    20 Aggregate bigraph from table 12
-    21 Maximal sub–bigraphs of bigraph in figure 20
-    22 Density plots of the evolving culture of figure 23
-    23 Surface plot of an evolving culture at @xmath and @xmath
-    24 Entropies and activity for the evolving culture in figure 22
-    25 Density plot for competing varieties in a culture at @xmath and
    @xmath
-    26 Entropy and activity for the evolving culture in figure 25
-    27 Culture that evolves to homogeneity
-    28 Culture unduly influenced by some agents
-    29 Peer Agreement Density Plot at @xmath and @xmath
-    30 Activity and entropy for figure 29
-    31 Peer Agreement Density Plot at @xmath and @xmath
-    32 Activity and entropy for figure 31
-    33 Newsgroups: implied topology
-    34 Newsgroups: implied topology with fewer neighbours
-    35 Tree structured patient record with features for classes of
    access
-    36 Preferences as Graphs
-    37 Relationship between a belief and a value
-    38 Corroborating beliefs about a value

{preface}

This thesis attempts to use the principles of the design of political
systems for information processing systems. Political systems are the
means by which governments order the affairs of states and a political
system in a liberal democracy uses elections as a feedback system which
allows the subjects of the political system to direct it in the
long–term.

The political systems that govern commercial businesses are not as well
evolved — the only feedback they have formalised is from their
shareholders. There should be inputs from customers, suppliers and
government agencies. The information processing systems used by
businesses reflect this lack of controlled feedback. Most information
systems are designed to solve the problems of a particular management
strategy for a business organisation.

The goal of this thesis is to liberate the management of information
from enterprises and return it to the people who are its subjects and
should be its owners. If ordinary people can control who has access to
their personal information then, the hope is, it will not be as easily
abused. Further than that, the behaviour of honest people should be the
norm, but it is more often the case in modern society that honest and
trustworthy people must prove themselves to be so, because most
organisations have no means of distinguishing between the honest and the
dishonest. Such information should be made available, so that genuinely
honest people would find it easier to function in society than those who
are not.

My hope is that this thesis will stimulate research into and development
of information systems that rank people and institutions according to
different metrics: how solvent they are, what areas of expertise they
have, and, generally, how trustworthy they are. And from this, be able
to give suitably–qualified people more influence over different aspects
of policy. It is, of course, unlikely that this would be the immediate
result and it would seem more sensible to prove this technology with
direct research towards managing resources where the ethical issues
would not cause so great an obstruction. Computer and telephone network
resources would be one such example of a good proving ground.

I must express my thanks to some of the people who have helped me over
the years: in particular, from the Electrical Engineering department: Dr
Clarke, my research supervisor, Mrs Margaret Saunders and Mrs Valerie
Hayes, our secretaries, who have helped me through the administrative
maze. Dr Robert Zimmer of Information Systems and Computer Science must
be thanked because he first introduced me to the more formal areas of
computer science. Mr Callum Downie of the Faculty of Technology and Mr
Theodorous Georgiou of Electrical Engineering for keeping the
outstanding computing services of Brunel University ticking over.

This research was supported by the Engineering and Physical Sciences
Research Council, but only thanks to the efforts of Bob Thurlby and Bill
O’Riordan of ICL and Russell–Wynn Jones of the Chorleywood medical
practice.

I would like to dedicate this thesis to the memory of Dr. Alan MacDonald
and Frau Marie Kohl née Schneider.

## Part I Trust Broking

### Chapter \thechapter Overview

The original brief for this project was to develop a software system
that would allow a community of individuals to access each other’s
information with the owner’s consent and yet to do so in such a way
that:

-   That consent could be delegated but never forced

-   and there would be no prejudice against an individual for choosing
    not to release information

The immediate goal was to realize a means whereby medical practitioners
based in surgeries and hospitals could access the medical records they
held on each other’s patients. Such facilities are already available in
Germany [ Blo01 ] and Iceland [ And98 ] , but neither system provides
assurances that consent will be obtained, nor that records will remain
confidential.

It became apparent that many of the problems of controlling the release
of this personal information would need to be resolved by a joint
decision of the owner of the database holding the information, the
person who wants to use the information and the person, or persons, who
are referred to by it. The decision would resolve whether the person
wanting to use the information was entitled to access it and, if so, how
much of it.

This is a fundamental business process and most people would recognise
it as one they take part in all the time. Even more fundamental than the
process is the commodity that is traded when the decision to grant
access is made. The sole criterion that both the database owner and the
subjects of the records held within it must feel is satisfied is simply:
“Can this new user be trusted not to misuse the information contained in
our records?” and trust is the commodity that is exchanged by all three
parties. Trust is a belief that someone else will keep their promises:

-   The subjects of the records trust the owner of the database and the
    user of their records.

-   Because:

    1.  The owner of the database promises the subjects to release
        information on them to mutually agreed users.

    2.  The owner of the database promises the subjects to release only
        the information they have agreed to release.

    3.  The user of the records promises not to misuse the information
        contained in the records.

The goal then is to develop systems which will provide individuals with
the degree of trust they require from each other: it is because of that,
that the systems and mechanisms proposed by this research are described
as, jointly, providing a trust–brokerage system.

Broadly, this research is in the area of computer–supported co–operative
working and it is an active field, but has concerned itself with
environments where there is enough implicit trust between all the
parties involved that confidentiality safeguards can be largely omitted
in system designs. Hubermann has developed a system known as Beehive [
HK96 ] which has been employed as the basis for computer–aided
engineering systems. There are also proposals for the joint management
of investment portfolios [ DSZ95 ] and there have been conferences
discussing a number of digital library projects [ Bat98 ] which provide
on–line texts from a number of sources and access is only slightly
restricted. All of these systems are to some extent predicated on the
existence of virtual organisation — an organisation that has been
created and designed to fulfil a function within one real organisation
or, more usefully, across a number of them. An interesting paper that
describes how a virtual organisation can be created to fulfil a need is
given in [ Mil95 ] .

Within the medical profession such systems are not as mechanised, but
are developing a trust model within national organisations [ And96a ] ,
[ DWS99 ] and standards for security mechanisms specific to healthcare
are being developed within Europe and elsewhere ¹ ¹ 1 [ And96b ] ,
“Standards” .

###### This Dissertation and Its Structure

This dissertation concentrates on providing co–operative working
environments built upon databases of information. The databases must
preserve confidentiality, so it divides naturally into two parts:

-   Secure Distributed Processing

-   Virtual Organisation for Resource Management

###### Secure Distributed Processing

This part re–examines the basic theory of data and database security and
applies it to a distributed processing environment. A practical
architecture for the secure management of access to any number of
databases is developed and a prototype implementation is discussed.

In the context of medical information systems, this part of the
documents describes mechanisms that must be implemented—such as those
proposed by the European working group [ oE96 ] .

###### Virtual Organisation for Resource Management

Originally this section concerned itself with generating a virtual
organisation for access control to databases, but it is now more
general: it addresses the issue of how to manage access to any resource
and the problem of how a virtual organisation for the management of
authorisation hierarchies can be evolved from the needs of owners and
users. This introduces three research issues:

1.  Forming, analysing and quantifying hierarchies

2.  Resolving conflicts within hierarchies

3.  Ensuring the stability of self–organising hierarchies

For medical information systems, this follows the argument of Anderson [
And96b ] that the medical profession operates under a collegiate
structure: policies for allocating access rights are applied within
autonomous organisations, but must be enforced within co–operating peer
organisations.

###### This Chapter

The remainder of this chapter provides a further introduction to the
research by providing some examples of how it might be used.

#### 1 Application: Universally Accessible Personal Information

In this section, the way in which the affairs of people are managed will
be described.

The difficulty most people face is that mechanised information
processing systems rely upon them to provide them with input — basically
people have to fill in forms. To add further annoyance, people have to
collect the information that the information processing systems generate
about them to be able to fill in more forms — apply for a bank account,
be given a bank account number and a sort code, then arrange a money
transfer by quoting your bank account and bank sort code.

It would be much simpler, and less error–prone, if people had a
repository of their personal information that was already in
machine–readable format, but which could be projected into a
human–readable form that people could reorganise. A simple drag and drop
environment would be very attractive. For example, log on to a system,
this generates an identity object which appears as an icon, go to a
folder which represents your bank, make a new cheque, find the identity
icon of the person you want to give the cheque to, drop his identity and
one’s own identity icon into the cheque icon to sign and address the
cheque and drop the cheque icon into an e-mail and send it. The e–mail
application will read the cheque’s identity icons, lookup the e–mail
addresses associated with them and address the e–mail correctly.

Such a system would greatly enhance the operation of information
systems. There are already prototype banking systems that operate in a
manner similar to that described above and it is hoped that more of them
will be developed. The following discussion looks at the difficulties of
mechanising the access and use of personal information. The discussion
begins with how people manage their confidential information in the
paper–based world we occupy now and then moves on to how they might
organise their information in an electronic environment.

##### 1.1 People

###### 1.1.1 Paper Lives

Most people will have a collection of papers in their possession that,
more or less, defines the person they have been and how they stand now:

  Certificates  

    a birth certificate, possibly a marriage certificate and,
    ultimately, a death certificate will complete the collection.

  Qualifications  

    there will be school–leaving certificates, examination results,
    degree certificates, driving licences.

  Earnings  

    Employers will have provided the Inland Revenue’s P60 .

  Status  

    Notifications of tax codes, a passport, valid visas, employment
    permits.

  Finances  

    Bank account and credit card statements. Direct debits, standing
    orders.

  Bills  

    Receipts showing bills have been paid, statements from suppliers,
    which, for most people, will be the utility bills they have settled.

  Memberships  

    Libraries, clubs, professional organisations.

  Properties and Contracts  

    One may own a property, rent one or hold a mortgage, similarly for
    cars. There may be contracts with managing agents, rental companies
    and service company warranties.

  Histories  

    There will be one’s medical history; perhaps details of legal cases
    in which one may have been involved.

  Addresses and Contacts  

    Just about everyone has a list of addresses and phone numbers
    listing all the companies and organisations who supply all of the
    information held on them. There will also be contact addresses for
    oneself and for friends and business associates.

If one were to bring all this information together it would amount to a
complex inter–related bundle and would suffer from all the typical
problems of data collections:

  Replication  

    If one were asked to prove one’s identity: one could use one’s
    passport, driving licence, cheque card. To prove one’s address: bank
    statements, driving licence or utility bills.

  Specialist Knowledge  

    Tax codes can only be deciphered by someone familiar with tax
    regulations; leasehold agreements need contract lawyers. X–rays,
    radiographers.

  Inconsistent  

    Driving licences can hold details that are out–of–date. Membership
    cards can have mis-spellings.

  Unsubstantiable  

    One piece of information can be useless without another to
    substantiate it. For example, if one possesses a national insurance
    card, it is quite possible to pose as the person whose card it is;
    there is no substantiation of the holder’s identity.

  Location  

    A piece of paper is easily lost, or one does not have it when one
    needs it.

###### 1.1.2 Electronic Lives in a Honest World

If we were to take all of the information used to lead one’s life and
make it available electronically, then software applications could be
developed, like the chequing account mentioned above, which would
require no paper–based input. The information would be easier to manage
and to access.

In an entirely honest world all of our personal information could be
placed on a web–site and we, and others, could freely access it.

###### A Person’s Web–site

The World–Wide Web could be used to host a web–site which would act as
an organised repository of all these documents in electronic form. They
could then be organised to fulfil their different purposes. A diagram of
how such a web–site might be organised is given in figure 1 . It is
owned and administered by an individual who has had a full professional
working life: working in a number of countries. The web–site could
contain the following pages.

1.  Addresses

    The current contact addresses of the web–site owner would be held.
    These would reference his residential property address and his
    employer’s address.

2.  Family and Friends

    The owner might provide references to his wife’s and children’s
    web–pages; friends and former business colleagues. He might include
    his employment and academic referees here.

3.  Advisors

    The owner might provide references to his doctor, solicitor,
    accountant and others.

4.  Permits and Licences

    The owner might store references to his driving licences, employment
    and residency permits and proof of certain tax exemptions.

5.  Projects Portfolios

    The owner may have decided to record his work to help in finding
    suitable employment, so he would keep web–pages detailing his work,
    his employers and so forth.

6.  Investment Portfolio

    The web–site owner may have made some investments and might want to
    have a financial adviser manage them for him, so he might construct
    a web–page that contains references to the current value of his
    investments, where they are held and the account details.

7.  Curriculum Vitae

    This composite document would reference to other pages, or part of
    their contents:

    -   Addresses

    -   Projects Portfolios

    -   Permit and Licences

    -   Family and Friends

    and would also reference educational qualifications, memberships of
    professional organisations.

8.  Liquidity Statement

    This shows the owner to be solvent and would be used to obtain
    credit or accounts. It would reference bank statements, accounts
    held elsewhere (credit cards and utility accounts) and property
    ownerships. It would of course refer to:

    -   Investment Portfolio

9.  Tax Details

    This would be used by the web–site owner’s accountant to pay his
    taxes. It would reference statements of his earnings, tax–deductible
    outgoings and would contain a reference to the address of his
    current tax office and possibly his past offices as well.

###### Providing Up–to–Date Information

A desirable enhancement would be for the web–site to provide current
information: bank account statements could be updated with each
transaction, as could the investment portfolio and any other records
that change frequently.

-   Either: provide a link to the provider of the information with the
    owner’s reference number to provide an index look–up in a directory
    service at the information providers web–site.

-   Or: have a dynamic web–page that would create itself on demand and
    carry out the look–ups and the formatting of records itself.

-   Or: have the information provider issue a new page by e–mail and
    have that page replace or be appended to the existing one.

The first of these two require the same operation: being able to make a
remote query on a database. As an example, consider looking up the
owner’s driving licence in the ‘‘Permits and Licences’’ web--page. The
page could simply contain a reference to the issuing authority of the
driving licence, with the index look--up ² ² 2 The URL used here is
fictitious, but most insurance brokers are able to acquire this
information and some make it available in web–based quotation systems. :

  \myURL

  http://www.open.gov.uk/dvla/drivers.htm/driver_number?EAVES60762WD9AK

A diagram for the interaction is given in figure 2 .

###### Using the Information

With this scheme an individual’s personal information could be kept
up–to–date and available over a universally accessible medium. If
someone wanted to join a library they need not fill in any forms, but
could just present the address of their web–page and have that recorded.
The library could then see if the web–page owner provides enough
information for its needs and could take whatever information it needed
from the individual whenever it needed it.

The library would then notify the web–page owner of his new account at
the library and send the address of the remote database that could be
queried for his account details with them.

###### Security Problems

Of course, no–one should have a web–site like this because there is no
protection of one’s privacy. For example, someone visiting this site
could learn the owner’s credit card details and commit fraud with them.

###### 1.1.3 Electronic Lives in the Real World

###### Access Control Mechanisms

What is is needed to make the web–site secure are a number of access
control mechanisms

-   Placed at the entrance to each web–page

-   Placed at every remote database that can be queried

The former restricts access to the page, the latter is a restriction put
in place by the owners of the remote database as to who may access the
data held on the owner. The owner would probably be entitled to see the
record held on himself, but that need not be the case. If the owner is
able to access his record, he may decide to allow other people to see it
as well.

For example, only the owner of the web–page for “Tax Details” and his
accountant would be allowed to access it, but he may also decide to
allow his accountant access to the tax details held at his different tax
offices, see figure 3 .

###### Granting Access Rights

There would also need to be a mechanism in place to grant access rights
to the web–page owner’s record held in remote database. These issues
have to be resolved:

-   Does the web–page owner have the right to grant access to his record
    to someone else?

-   Does the owner of the database where the record is held want to
    allow access to the person the web–page owner proposes?

The latter issue might appear contentious, but the data held at the
remote database may have an intellectual copyright attached to
it — there may be design documents in the “Project Portfolios” — or it
may give rise to a conflict of interest — it may contain a company’s
information that should not be released to a stockbroker who might be
involved in a rival bid for the company.

The web–page owner and the owner of the database must come to some kind
of agreement and in so doing they would want to be as well–informed as
possible about the individual to whom they are proposing to grant the
access rights to. If that individual were known to the web–page owner
then he should appear in his “Advisors” page or perhaps in his “Family
and Friends” pages and if he is there, then his web–pages could be
accessed and it should be possible to gain enough information about him
to make a decision. There would be a system of implicit and explicit
permissions. Company directors would implicitly grant rights to each
other to view information that is common to them in the course of their
business, but some rights may need to be granted explicitly: the right
to sign cheques would be explicitly granted to the finance director, for
example.

The mechanism for granting access rights could be anything: e–mail or a
secure web–form. All three parties should be notified and record the
rights granted and this information should also be made available
through a database.

##### 1.2 Organisations

###### Responsibilities

So far only the information needs of individuals have been discussed.
The organisations that own the databases that hold individuals’ records
would also want to use the data. Going back to the example of the
library the web–page owner had joined, it will have access rights to
some of the “Addresses” information so that it can send out statements
to the web–page owner, but, if it has access to the address information
for all of its borrowers, it can construct a mailing list and sell it to
a direct–mail company and more junk–mail is almost certainly something
that the library’s borrowers would not want.

On the other hand, the library may decide to do something useful with
the address data, it may use it for planning where to build a new
library. It is a matter of intent and a requirement that can be made is
that whenever data is required from an individual, a statement of intent
should be made with the request.

A statement of intent is no protection against abuse of the data held in
the individual’s web–site unless one can show that the data has been
properly used and the only way to do this is for the library to show
that it has fulfilled its intent. This would involve proving to owners
of the information that the data has been used correctly.

This is simple enough: if the library accesses an individual’s address
record, it should leave a token with him saying that it will be used to
send a statement, when the statement arrives, it will contain a
reference to the token. The token and its reference can then be
reconciled. In effect, every piece of information can be tracked to its
source: one can think of this process as like recording the progress of
a note of currency in the economy by tracking its serial number. There
are already mechanisms for this form of transaction processing in most
modern information processing systems: in particular, web servers have a
system of transferring “cookies”, unique session identities that can be
attached to the transactions an individual undertakes.

###### Possibilities

The possibilities for co–operative information sharing between
responsible organisations and individuals are enormous. The health-care
industry in particular could benefit by having near instantaneous access
to medical records. Insurers would be able to evaluate risks better; law
enforcers could isolate groups of people who may demonstrate a
propensity to become offenders and take preventive measures; they could
also determine which groups of people are most likely to have crimes
committed against them. Of course such systems would need to protect
civil liberties: if someone were determined as being a potential
offender, it must be possible for him to appeal against that
classification.

## Part II Database Security on the World–Wide Web

### Chapter \thechapter Requirements and Analysis

The original brief for this project was to develop a software system
that would allow medical practitioner’s databases to be accessed
securely and safely. In this first part of the dissertation, a
functional design is developed based on existing technology which could
be used for a system that could provide personal information in the
manner described in § 1 .

There are already systems in use that have similarities to that
proposed: OncoNet [ Blo01 ] provides oncological information to a high
degree of data security because it is operated and used by one
well–managed organisation.

When access is more open and management more collegiate, then such
systems are more akin to digital libraries [ Bat98 , MeD98 , SRI98 ] .
These are well–funded and have reached a high degree of sophistication,
but, it will be seen, they have a simple internal organisation, which
effectively allows only one level of access.

There are other research projects for the health-care industry; these,
too, are better developed — the LIOM project [ LIO99 ] , for example.
Although that project aims to reflect the more complicated internal
organisation of the health-care industry, it is not intended to be
self–administering, which this system aims to be. The LIOM project, like
others in this field, uses a meta–data model which has to be maintained
[ RHC @xmath 96 ] . This is feasible on a small–scale (100 users or so),
but the coordination effort needed would probably be excessive for
larger systems (1000 users).

In the security model proposed by Anderson [ And96a ] there are
recommendations for technology, ITSEC standards for operating systems
and databases. There are also recommendations for access control
schemes, variants of Role–based access control, [ San98 ] , appear to be
most adaptable to the collegiate federation of organisations proposed by
Anderson. There is an implicit argument for a Public Key Infrastructure,
PKI, which could support cross–certification, such as that analysed in [
Mau96 ] .

###### Secure Distributed Computing

The system design is also complicated by its attention to secure
distributed computing [ Sch94 , Bir96 ] , which is a relatively mature
field. Secure computing requires that processes have proven
implementations and execute on a safely constructed computer system with
the least privilege needed to complete successfully: the principal
requirement is that it should not be possible for other non–privileged
processes to access any of the information produced or consumed by the
secured processes.

In distributed secure computing, this problem is doubly difficult. One
safely constructed computer system, system @xmath , may hold
confidential data, @xmath , and another safely constructed computer
system, system @xmath , may hold a proven implementation of the process,
@xmath , to be used with the data. @xmath may be able to pass @xmath to
@xmath securely, but @xmath cannot be trusted not to compromise it.
@xmath can however send its process implementation, @xmath to @xmath
where it could execute and process the data, but @xmath must be sure
that @xmath has no means available to it which would allow it to
communicate @xmath by a covert channel as well as be sure that @xmath
does not compromise the integrity of @xmath . This interaction is fairly
simple: more problems ensue if the data from different sources has to be
merged.

###### Open Distributed Computing

The analysis and design process is within the framework of an Open
Distributed Processing, ODP, system, [ ISO97b ] , which is to address
the information processing problem from these five perspectives:

-   Enterprise: what has to be achieved

-   Information: what information is needed to do it

-   Computational: how can that information be obtained or deduced

-   Engineering: what quality of service can be achieved in providing it

-   Technological: what technology exists that could be used to achieve
    it

ODP merely recommends that the system design be addressed from all these
perspectives and that each one could be modelled, if need be  —  see for
example this discussion of working within the ODP framework [ Can96 ] .

The technological model is usually a given because it is the dominant
technology at the time of design. The remaining four models can be
traded off against one another. This chapter will be just a first pass
over design issues and does nothing more than describe how such a system
might work. Usually one begins with a sketch of the technology and
enterprise models, then one sketches the information and computational
models from the other two models to allow one to produce an engineering
model, which is a set of interacting agents and more or less defines the
operation of the system. The engineering model is then the basis for
another iteration of design, where the enterprise, information and
computational issues for each agent are addressed.

#### 2 Technology Model: The World–Wide Web

This section describes the technology available at the time of writing.
It will be seen, as the system is analysed, that the technology is fully
capable of achieving what is required of it. The real design problem is
to establish policies for authorising access and showing that they have
been followed. Distinct aspects of information security should be
clarified because they are addressed by different technologies.

-   Secure access: the information is protected against indiscriminate
    access.

-   Safe access: the information is protected against indiscriminate use
    by those who are allowed to access it.

Incidentally, the reason the World–Wide Web [ W3C97 ] has been chosen as
the communications medium ought to be stated.

-   Wide access: the information can be accessed from as widely
    available a medium as is available.

###### Wide Access — Web Technology

As far as end–users are concerned the World–Wide Web has only:

-   Web–browsers

-   Web–servers

It is worth mentioning that the Internet protocols underlying the
World–Wide Web can also offer secure electronic mail delivery [ RSA99 ]
.

Probably the most useful piece of web technology for system designers is
Java [ SUN98b ] . This is an object code interpreted language that runs
on a virtual machine; it can be constrained to only use specific
operating system resources (files, sockets and so forth). This
programming language allows system designers to load software from
anywhere on the World–Wide Web and run it on a designated host in a safe
environment. This is absolutely ideal for agent–based software.

###### Encryption and Authentication Products — Secure Access

The capabilities of this web technology are widely–known and some
specifications can be found in [ Cor98 ] . Most web–browsers can
establish secure connections with suitably enabled web–servers. (The
latest version of the secured socket protocol is called Transport Level
Security and is discussed in [ Eav99e ] .)

This hinges upon public–key cryptography and secure repositories for
public–key certificates. The standard governing this is X.509 , [ X5088
] , and the certificates are consequently known as X.509 certificates.
They are available widely, at a charge, from certification authorities
such as Thawte , [ THA99 ] . There is some discussion of their
limitations in [ Ros95 ] .

There are some well–evolved software security products producing
public–key infrastructures [ RSA98 ] .

###### Operating Systems and Database Products — Safe Access

Operating systems are now relatively safe. ITSEC [ ITS ] grades them
and, currently, there are a number of products that have reached the
acceptable security levels recommended by Anderson [ And96b ] : E3 and
above, [ UK 96 ] .

Database systems are also graded by ITSEC and there are a number of
suitable products for the system proposed [ ill96 ] and it is possible
to integrate these with web–servers [ Cor98 ] . Most databases support
SQL , the Structured Query Language [ Nor96 ] , which provides a set of
access control mechanisms which, it will be seen later, are adequate for
the system proposed.

#### 3 Enterprise Model: Contracts

The aim is to propose a suite of protocols that will allow access to
databases to be strictly controlled and thereby allow more and
qualitatively better information to be distributed and to simplify,
standardise and partly mechanise the procedure whereby individuals are
granted access. This section will describe the relationships between the
parties as a set of contracts in the style of an enterprise modelling
language as described in [ ISO97b ] and, at slightly greater length, in
[ Eav99a ] .

A key argument in Anderson’s model for the security of clinical
information systems is that individual information systems are assumed
to be well–managed and align to the structure of the organisation they
serve. The access control mechanisms of these information systems may
use either a Bell–Lapadula [ BL73 ] or Clarke–Wilson [ CW87 ] mechanism
for determining rights. They form part of a collegiate system which has
some federal infrastructure which manages access control lists for the
component systems. Anderson’s argues that the access control system for
the access control lists can only be Clarke–Wilson in form.

This section attempts to develop rôles that could be used within the
federal system, so that they might be used in a rôle–based access
control system with constraints such as RBAC2 described in [ San98 ] .

In this section, these rôles will be specified using the principles of
deontics [ MW93 ] , which aims to reduce difficult contractual
relationships to sets of rules concerning rights and duties.

###### Adjudication and Loss

Contracts describe expected behaviour — usually, the formalisation of an
existing behaviour. Each party to a contract believes that the expected
behaviour will be forthcoming because it would be too costly to do
otherwise. Either because it is practically too expensive not to behave
as required, (it might change existing procedures), or, that penalties
will be incurred by the party who breaches the contract. The latter
requires that an adjudication service be available to determine if one
party has not complied to the terms of the contract and that that party
be punished and the other recompensed for the loss suffered. The
operation of an adjudication service is quite sophisticated, but is
discussed, in outline, in chapter III ; recompense for loss suffered is
achieved by surety or insurance. The insurance industry already has some
policies for disclosure of information, but it would be desirable if
they were able to give cover at the time a contract is formed and it
should be a precondition that cover be arranged before granting access.
The insurance and surety process is capable of being mechanised [ LMN97
] ; this paper also discusses how licences could also be issued for
people offering services through web–servers.

##### 3.1 Parties

There are four types of party to the contracts. These are specified with
respect to the rights and duties they must possess and should fulfil.
These are the entities that must follow the principles given in
Anderson’s model [ And96b ] and would appear as rôles within the federal
superstructure of the collegiate organisation.

-   Subjects

    Subjects are the people (or organisations) on whom information is
    kept by the owners of databases.

-   Custodians

    These individuals are appointed by the subjects; the appointment is
    usually de facto , a person’s doctor is obliged to act as their
    medical representative and is therefore the custodian of their
    medical information. It should be possible to make the appointment
    of a custodian explicit and a subject should know who there
    custodian is. It may also be possible for a subject to be his own
    custodian. The function of a custodian is to make access control
    decisions for the subjects. It will almost certainly be necessary
    that custodians do this, because:

    1.  Subjects will not usually have the specialist knowledge needed
        to assess access requests.

    2.  Custodians can act on behalf of groups of subjects which have
        similar interests.

    Subjects will usually delegate decision–making to one (or more)
    custodians. If they choose to delegate to a group of custodians,
    then the subject can choose from a number of decision processes —
    e.g. veto, unanimity, majority vote — how the decision will be
    taken.

    Custodians are responsible for the safety of information. They are
    not responsible for the security of data storage and transfer of the
    information. That is the job of the owner of the database, or
    databases, upon which the information is stored and the facilities
    by which it is communicated. In short, custodians specify the
    policies for information use, storage and transfer; owners execute
    these policies. Very often the custodian and the owner will be the
    same person acting in two rôles. Within a medical practice, a doctor
    will make decisions regarding information safety when he decides
    what to include in a letter of referral and will make decisions
    regarding data security when he chooses to send the letter of
    referral by electronic mail.

    Custodians should have some legal responsibility to the subjects.
    Most custodians will be subject to legislation, such as, in the
    United Kingdom, the Data Protection Act [ DPA84 ] . It may be useful
    to think of legislation, and other policies a custodian should
    adhere to, as having a custodian, which could be made an active part
    of the system.

-   Owners

    These people (or organisations) own the databases and the
    communications infrastructures that hold and distribute the
    information held on the subjects. They follow policies from
    custodians

    The owners will add their own information to that the subjects have
    provided. The owners will want to protect this information in the
    same way that subjects will want to protect theirs. In this respect
    they can thought of as a subject who is its own custodian for all
    the records in the database.

-   Accessors

    These are the people (or organisations) who access the databases
    held by the owners. Accessors will be assigned a security clearance
    class and each of these will have a membership panel of trusted
    peers who should be known to appropriately qualified custodians.

    It may prove expedient for accessors to copy those parts of
    databases that interest them, add their own interpretations to the
    data and republish the data and, in so doing, they become
    custodians.

###### Closed Relationships

A simple set of relationship rules might help clarify the entities’
rôles with respect to one another. The aim here is to ensure that the
relationships are closed, so that the system can be self–governing.

There are four sets of rules.

1.  Database management and composition

2.  Subject–Record–Custodian

3.  Multiple rôles

4.  Accessor permissions

They are expressed as class relationships of the HAS–A and IS–A kind.
HAS–A relationships can be by aggregation or by reference. Aggregation
means that one entity is a composition of the others. The reference
relationship means that one entity knows about the existence of the
others and there is some association between them, usually ownership or
delegation or use.

IS–A can be of two kinds: by generalisation and by template. The latter
is best called the IS–KIND–OF relationship and means that the two
classes have the same meta–class, but have distinct identities and
behave autonomously. The IS–A is usually implemented by inheritance and
allows one class to be used in the same way as the other sharing a more
abstract identity and cannot always act autonomously.

###### Database management and composition

This first pair of rules state that an owner manages a database, which
is composed of a set of records. The relationships are one to many. An
owner may manage more than one database.

(Database should be a more general concept because the rules describe
control relationships. The more general concept is one of a Resource .
This would include networking resources. An example of which might be a
port number on a host computer. Only databases have been described here
because they refer back to subjects directly.)

  -- -------- --
     @xmath   
  -- -------- --

###### Subject–Record–Custodian

The second pair state that each record maps to a subject and that each
subject has a custodian. This is the most important rule: it associates
data with information.

All the relationships can be one to many. In particular, a subject may
have more than one custodian. There might be a custodian responsible for
policy regarding data encryption of medical records and another
responsible for the policy regarding the disclosure of confidential
information.

One to many also implies that if you have obtain a record, it will have
a subject and vice versa .

###### Multiple rôles

The following IS–A relationships closes the system, so that all the
entities introduced must have at least one custodian. There are three
cases to cover:

-   The database is a record which must describe a subject — this would
    be itself and the record would be the database’s meta–data. Each
    subject must have a custodian.

-   An owner is a subject and must therefore have a custodian.

-   A custodian is also a subject and so must have a custodian.

These rules encompass the relationships as they so often arise in
practice. That an owner also acts as a custodian, but in different
rôles — the example of a doctor sending a letter of referral by e–mail.
It also allows a custodian to be his own custodian. This is useful for
expressing supreme legal relationship: governments are only answerable
to themselves.

  -- -------- --
     @xmath   
  -- -------- --

###### accessor permissions

The last three HAS–A relationship state how accessors are involved. They
stand outside the system, because there are no practical means of
enforcing any behaviour upon them. They access records via permissions.
Permissions are created by custodians.

Each permission has a record so the custodians act as a linking entity
between records (and their corresponding subject) and the set of
permissions.

###### Summary

The important points are:

-   That a database of records has custodians for each of the records
    contained in it and for the database as a whole.

-   Subjects, owners and custodians themselves all have one or more
    custodians.

The latter point makes the system closed and self–governing.

This set of relationships is very similar to the architecture of
per-formative agents proposed by [ ISO97b ] and also described in [
Eav99a ] .

##### 3.2 Grades of Anonymity

A well–known problem with personal information is that anonymity is no
real protection if it is possible to obtain an identity and a profile
from one database and use the profile to isolate some other confidential
information from another database [ DSW90 ] . Anonymity can prove to be
an obstacle to legitimate use of data. A grading of degrees of anonymity
might prove useful in specifying access contracts. This grading, see
table 1 , is illustrated with reference to the medical profession, but
can be used elsewhere.

Pseudonymous identities are already widely–used in medical research; it
allows a particular patient to be referred to consistently and a thread
of discussion can be developed around that identity.

Eponymous identities are subtly different from anonymous ones, because
an individual is tagged as belonging to a particular group. Usually,
genuinely anonymous subject data is partitioned over and over until all
the subjects have eponymous identities. If it is possible to write back
to the original record that a particular subject has been designated as
belonging to a particular group, then the subject has an eponymous
identity. Under some methods of inference control, in particular random
sample queries [ Den80 ] , it is not possible to attach any deductions,
and therefore eponymous identities, to particular sets of records.

Some accessors may make local copies of datasets from databases and
would add their own classifications, if these are re–published then the
identities would be eponymous —  because it is possible to use the
original database to establish a profile and, by inference using that
profile, obtain the classification made in the copied and augmented
database.

##### 3.3 Views of Records

When a record of a subject is released, it should not be the entire
record, but rather a restricted view of the record that contains enough
information to allow the accessor to do their own processing. This is a
principle more or less enshrined in most information system security
texts: ‘‘The principle of least information’’ ³ ³ 3 See for example, [
Den76 ] . .

Having a restricted view of a record does allow identities to be
restricted to eponymous, but anonymity —  or at least the anonymity
granted by using random sample queries —  requires an additional
mechanism.

##### 3.4 Duties of a Custodian to a Subject

Custodians are usually practicing professionals in a particular field.
The relationship between a custodian and their subjects is usually
governed by an accepted code of practice from a professional body, which
is the collective identity of the custodians. These professional bodies
usually grant licences to their members to practice their profession and
there is usually an adjudication procedure to determine if a member has
acted improperly. This is the only contract between custodian and
subject that is needed. The duties of the custodian are to:

1.  Grant minimal views to accessors.

2.  Inform subjects of:

    1.  Classes of accessors granted access to their records.

    2.  View of record given to those accessors

    3.  Evaluation of risk and insurance of surety gained on their
        behalf

##### 3.5 Duties of the Owner to the Custodian

Owners are instructed by custodians as to whether or not the records for
the custodian’s subjects can be released. The duties of the owner are
to:

1.  Release no record without prior approval from a Custodian

    This is actually required by the Data Protection Act [ DPA84 ] .

2.  Provide Stated Degree of Anonymity

    If a custodian allows a subject’s record to be published then the
    degree of anonymity must be upheld.

3.  Minimal View of the Record

    The custodian will state the view of the record that can be granted
    to a class of accessor and will expect no more than that to be
    released.

4.  Minimal Set of Accessors

    A custodian will grant access to a particular security class of
    accessors.

    It is usually the case that a lattice model [ Den76 ] of secure
    information flow is in force ⁴ ⁴ 4 Lattice models are described in
    section § 25.2 . For now they can be thought of as hierarchical
    organisation structures. . This would allow accessors who have a
    “higher” security clearance to be given de facto access as well,
    without having to negotiate with the custodians. The lattice model
    is a generalisation of the Bell–LaPadula access control hierarchy [
    BL73 ] , which could be described as “read–down, write–up” or an
    accessor at a particular security clearance can read everything
    graded below his own grade, but what he writes can only be read by
    those above his grade.

    Whether de facto access is granted should be open to negotiation
    between custodians and accessors as well.

##### 3.6 Duties of the Accessors to the Custodian

The main problem is that accessors may need the right to re–publish the
data they have acquired from a database owner. An accessor could be an
organisation and might need to re–publish the data internally or the
accessor may decide to re–publish to a wider audience.

1.  External Re–publication

    The accessors must ensure that other accessors be vetted and
    approved by the custodians in the same way that they were vetted and
    approved for access.

2.  Internal Re–publication

    It may be possible for the accessor to show that his organisation’s
    own data security procedures are good enough to provide the
    custodian with enough assurance to forgo vetting procedures for
    every internal accessor.

##### 3.7 Duties of the Custodian to the Accessors

If an accessor is denied access then they have a right to know how they
can put themselves in a position whereby they may be granted access.

-   Provide justification for denial of access

    If an accessor fails to meet particular security clearance
    requirements they should be told which so that they can change their
    clearance.

#### 4 Enterprise Model: Adaptability Issues

There is enough tested and approved technology available to implement a
system that could implement these relationships and provide the means
for each entity to fulfil the duties imposed upon it, the challenge is
to design it in such that a way that authorisation policies can
formulated in a semi–automated way and that the system could be almost
wholly self–governing. The use–case scenario for accessors would be
something like this:

1.  Reference to a database appears at a secure web–server

2.  An accessor requests a particular set of records using secure e–mail
    from the relevant custodians

3.  Custodians make their decisions and return them by secure e–mail

4.  Based on the replies: a set of views for the records is generated
    and the owner of the database is instructed to publish it for the
    accessor’s eyes only

5.  Accessor is notified of the views to use

6.  Accessor uses secured connections to submit queries to the database
    on these generated views

There are a number of problems with this. Firstly, the custodians will
make their decisions based on the current state of the records: a
particular epoch of their existence. The accessor should either be
restricted to that epoch or negotiate access for all subsequent epochs.
Only a few databases directly support epochs — PostgreSQL [ PSQ ]
does — without that different tables would need to be created for each
epoch and separately maintained.

Secondly, the collation of the replies would need to employ some least
lower denominator for the records, because some custodians may not grant
access to particular fields within the record.

Thirdly, the process would generate a lot of request traffic, which
custodians would be hard–pressed to keep track of and, therefore, to be
consistent when applying their own criteria. It would be useful to
employ precedents, e.g. a custodian has granted similar access rights to
an accessor who has been classed at a lower level than the current
requesting accessor, is it permissible to allow this accessor the same
rights? This requires a lattice of information flow, which, it was
specified above, would be the subject of negotiation between custodians
and accessors. Using precedents requires that accessors and records be
classified. The process of generating precedents can be accelerated if
custodians are also classified, so that clearance gained from a
custodian who is ranked higher than another means that there is no need
to secure acceptance from the lower–ranked.

Fundamentally, custodians would either have to specify rules for access
which could be applied by a mechanical agent on their behalf, or a
mechanical agent would deduce rules from their actions and, after
checking them with custodians, add them to a rule base to be used later.
Similar systems to this have been deployed [ BW94 , Cas97 ] .

#### 5 Rôle–based Access Control Systems: Information Model

The information model proposed is that used for rôle–based access
control systems. These are described in Sandhu [ San98 ] , in which he
argues that rôle–based access control systems have such a sophisticated
information model that they can be constructed to support all the other
important forms of access control system.

Sandhu gives a simple information model, but this has been modernised,
using Booch’s notation [ Boo94 ] , and clarified. (A more suitable
notation would object role modelling , [ Hal95 ] , which is more easily
formalised for implementation, but Booch’s notation has been used for
consistency.)

###### Objects

The principal innovation of Sandhu’s rôle–based design can be seen in
figure 4 . Each object in the system has its own set of rôles associated
with it. Each rôle acts as an interface to the object. Each rôle has a
set of permissions associated with it. Both relationships — object–rôle
and rôle–permissions — have constraints objects associated with them.

###### Epochs and Constraints

These permissions can be qualified by applying a set of constraints, the
Epoch constraints.

Although Sandhu specifies that these constraints exist, he does not make
it especially clear what they constitute. This denotation follows the
practice in modern database design that access control rules and data
descriptions can be revised so that the previous generation is still
available as a different epoch [ ill96 ] . It will be seen that a
self–organising access control system will need to remember its previous
state.

###### Hierachies

The lattice structure of many access control systems is effected by
allowing rôles to have a hierachy. This is illustrated using the class
tree in figure 4 . There is an abstract role and this is sub–classed
twice for role A and role B and role A is sub–classed once for role A1 .
Modern database systems such as PostgreSQL [ ill96 ] have support for
sub–typed data classes.

###### Subjects

Figure 5 shows how a subject obtains the set of rôles by which he may
access the objects. A subject first obtains a session. A session is an
engineering entity that qualifies what rôles may be used by means of a
session constraint entity.

###### Constraints

The session constraint is dependent upon the manner in which the session
is established and is designed to reflect the different ways in which
the same subject may access the system. Access from a physically secure
local area network will be less constrained than from an insecure
dial–up line. Other constraints may be imposed because of accepted usage
practice: some records may only be available for specified dates and
times.

Each session may have a different rôle set. This allows the same subject
to act in the system in a different way.

###### Constraints

A simple information model for constraints entities is given. These
control the system who may access what within the system.

###### Summary

Sandhu’s model for rôle–based access control systems is very useful to
this design discussion. It will be seen that most of the design and
analysis for this system will focus on generating the rôle hierachy and
a structure for constraints objects and the information that must be
placed in the constraints objects.

#### 6 Computational Model

Most of the computation performed by the system would be to provide its
adaptability:

1.  Generating views of records

2.  Classifying accessors

3.  Generating access rules

##### 6.1 Generating Views of Records

Custodians would deny access to records or restrict access to certain
fields. This suggests two strategies:

-   Either provide a full record with NULL put into the field values
    where a custodian has denied access.

-   Or generate a least common denominator view.

###### Nulling fields

-   Either a new database table has to be created and the modified
    records inserted,

-   Or a set of triggers ⁵ ⁵ 5 SQL allows a function to be invoked when
    a record is operated upon, see [ ISO92 ] . to be generated to insert
    the NULL values where specified.

Neither of these is particularly desirable: the former requires a new
table which would need to be separately maintained; the latter requires
triggers to be written which would need to check a profile (specified by
the custodian) for each record for every access of the view, which would
greatly affect performance.

###### Least Common Denominator View

The collation procedure to produce the least common denominator view
could employ one of two strategies:

-   Maximum field coverage by minimising records included

-   Maximum record coverage by minimising fields included

Both of which could be qualified by the accessor stating percentages of
coverages and ranking fields to be included.

##### 6.2 Classifying Accessors

Accessors would need to be grouped and then those groups ranked relative
to one another to produce an authorisation lattice. Clearly, there are
many policies for this: most involve some arbitration outside of the
information system itself between the representatives of the different
entities.

One procedure would to make use of professional standing within a
respected professional institution. There are many groups extant that
could be used as the basis for accessor control groups. The British
Medical Association is the accrediting professional organisation for
practicing doctors in the United Kingdom. The Law Society for
solicitors. Belonging to a professional group implies that one performs
a certain rôle. It may be necessary to enforce members of groups not to
use their group identity if operating in a rôle not sanctioned by the
group.

###### Grouping Accessors

This is a proposal for system of grouping accessors together which makes
use of modern certification technology. The aim is that accessors and
their groups would be self–regulating.

Each accessor would have an X.509 certificate proving their identity.
They would then need to obtain a proposer and seconder from the group
they wish to join. The proposer and seconder would corroborate the
identity of the applicant and make some recommendation to the membership
committee.

This protocol can be secured using a certificate chain and blind–voting
protocols described in [ Sch96 ] . (Certificate chaining is just one
message encrypted using the private key of one certificate and then by
another. Blind voting allows a vote to be taken which allows each party
to prove to themselves that their vote has been counted, without knowing
who else has voted.)

Once an applicant has been granted membership of a group, they would
then need to be issued with a new certificate which would be their group
membership. This is an X.509 certificate with the group acting as the
certification authority. In the event that membership is revoked, the
certificate could be made void without inconveniencing any other members
of the group. It also reduces the amount of encryption needed to just
one pass.

(Incidentally, the method proposed above produces a “Web of Trust”.
There are a number of different mechanisms for achieving this, again see
[ Sch96 ] and also [ YKB94 ] ).

###### Ranking Groups

There are two other functions that need to be developed for
self–organising groups. They must be able divide themselves up and to
merge. This, combined with an authorisation hierarchy, will allow them
to better define who may access what information. Groups, in this
context, are abeyant to set theory and what is needed is a defining
membership function: much as one might say, @xmath is the set of all odd
dice throws. This requires a distance measures which would allow someone
to say that under, a particular distance measure, member @xmath is very
similar to @xmath . Statisticians and actuaries do this all the time, it
just remains to develop it for professional groupings.

###### Deference

For groups and their members to be ranked: the rôle of the group (or the
function of its members) has to be quantified. The principle of
deference is a useful basis since professional groups apply it.
Referring again to the British Medical Association, it has sub–groups:
student members, juniors, general practitioners, consultants and
specialists. At the same time, members of the BMA will have different
affiliations to other organisations, the Royal College of Surgeons,
British Pædiatric Association and so forth.

A family doctor with no special pædiatric expertise involved in a
pædiatric case would be expected to defer to another doctor who is a
member of a pæediatric association.

###### Rôles and Ontologies

This can be quantified by a distance measure. Doctors would collect
accreditations and whoever has the most of them over the range of issues
involved would be ranked above the others. The issues effectively
determine the rôles. This, again, is a collective choice procedure which
will be analysed in more detail later. Suffice to say, that the members
of the groups would rank themselves within their own groups and rank
their groups with respect to others with respect to their current rôle .
There is no objective ranking between groups, or, come to that amongst
group members, because it depends on the issue at hand, which demands
that individuals take certain rôles. This concept is explained in more
detail in [ GI97 ] .

“Issues” is too imprecise a term, so ontology ⁶ ⁶ 6 This is the term
used in KIF see chapter § III . will be used in place of it.

Some relationship diagrams might help clarify this. An individual
possesses certain rôles. The ontology within which the individuals are
operating will require that certain rôles be fulfilled. With regard to
accessors and custodians, these are both types of individual. This
relationship analysis is applicable to both accessors and custodians,
because mappings between the two sets using a common ontology will help
in allocating access views.

  -- -------- --
     @xmath   
  -- -------- --

Given the rôles and the ontology, an ordering of individuals for an
ontology can be formed.

  -- -------- --
     @xmath   
  -- -------- --

##### 6.3 Classifying Records and Fields

Initially, views of records, and the fields within them, will be
classified by the custodian for each accessor given the ontology. The
accessors will themselves be classified and it should be the case that
certain classes of accessors will require certain types of record view.
Consequently, classifications for record views will evolve for different
ontologies. This is another important requirement of the system so that
it can be self–organising: if views are ordered relative to group
memberships then it will be possible to recommend that groups be
sub–divided to match information protection requirements. Conversely, it
can be used to simplify access rules by merging similar groups.

These relationship diagrams might help to make clear how records can be
ordered. A record will have a number of views. Each ontology would
require certain views.

  -- -------- --
     @xmath   
  -- -------- --

Individuals have been ranked relative to one another for a particular
ontology, so if an individual is given access to a view, then granting
that permission effectively orders the views of the records in that
ontology.

  -- -------- --
     @xmath   
  -- -------- --

###### Discussion

There are three classification processes at work; the last is a
corollary of the first. This assumes that all the individuals are
working within the same ontology.

1.  Individuals classify one another

2.  Individuals classify views

3.  Custodians classify accessors

If the system is bootstrapped by a number of carefully deliberated
classification decisions, then more specific access rules can be
generated. When it is not possible to apply a rule, it will be resolved
by another classification decision by a custodian and a new rule can be
added.

##### 6.4 Generating Access Rules

This process relies upon the generation of an information flow model [
Den76 , Den82 ] . Without going into detail, an information flow model
requires:

-   A security classification scheme that classifies:

    1.  All views, and

    2.  All the accessors

Then, for a given set of views, an accessor must have a security
classification that is greater than or equal to the least upper bound of
all the views demanded. So, one can conclude, that computationally it is
relatively simple to determine access rights, if accessors and views are
graded.

In figure 7 , two sets of views are presented: Body Mass Indices, BMI,
and treatment costs. There is a choice of sub–views for each. § 6.1 ,
paragraph “Least Common Denominator View”, stated that there are two
parameters that a custodian varies in generating a view for an accessor:
the fields in the view and the range of records. The fields here are the
BMI entries in an historical medical record and the treatment costs. The
ranges varied are the age and ethnic groups.

A simple medical practice is shown in figure 8 . The practice has two
organisational functions: medical and administrative. Referring to the
views available, the medical staff, doctors and nurses, would be given
write access to Body Mass Index data, but not to treatment costs; the
administrative staff would be given write access to treatment costs, but
not the BMI data. The owner of the treatment cost data is the
administrative arm of the practice, the owner of the BMI data is the
medical arm.

The health authority which reimburses the medical practice for treating
people in its catchment area would need access to treatment costs
records for all medical practices in its area.

In administering the practice — aligning it to the needs of the health
authority — it would be necessary to value the cost of taking a BMI
reading and this would be discussed at a meeting of the Practice
Management Committee. In the classification of the information held by
the practice, the least–upper–bound of the medical and administrative
arms of the practice is the practice management committee.

A medical consultancy is shown in figure 9 . It has a similar structure
to a medical practice, see figure 8 , but would also undertake the
training of students, who would be answerable to the consultant. A
medical consultancy would undertake research and would be answerable to
a research organisation for any funding it receives.

The difficulty is to join the two organisations’ structures. This would
need to be performed in the appropriate ontology.

1.  Consultancy

    The most typical scenario is, of course, that patients would be
    referred to consultants. The consultant would obtain his own
    information on the patient. The patient’s practitioner would expect
    to be informed of the consultant’s findings. The consultant might
    want to use the information he has obtained from the patient for his
    research; the consultant should obtain the patient’s consent from
    his custodian the patient’s medical practitioner.

2.  Research

    If the medical practice decided to make available the BMI
    information it holds, the medical practice and the health authority
    would place stipulations on its release.

#### 7 Information Model — Views and Constraints

As stated, the technology to realize this system is already available,
so it is not necessary to detail all of the information the databases
and web–servers would need to operate. The information that is of
concern is that needed to provide the newer features:

1.  Granting access to views

2.  Provide assurances that each party is keeping its contract with the
    other.

3.  Facilitate classification of accessors and views.

##### 7.1 Granting Access to Views

There are two cases to consider:

-   Either an access rule exists and can be applied

-   Or there is no applicable access rule

(Bear in mind, that an access rule may actually deny access.)

It is only necessary to consider the latter case, the procedure is
simply a custodian receives a request from an individual who wants to
access some specified records.

Clearly, to do this the accessors will need to know who the custodians
are and how they can be contacted.

The custodians will need to know:

1.  Accessor’s identity and proof of group memberships

2.  Ontology under which the accessor is operating

3.  View of the record they require

4.  Precedents set by other custodians

5.  Precedents set by access rules

The accessor will then be informed of the custodian’s decision. The
decision could then be formalised as a precedent upon which an access
rule could be based.

##### 7.2 Proofs of Contract Compliance

Most of the information that needs to be retained by the system will
simply show that the duties of each party are being fulfilled. All of
this information would be available from the log files of the databases
and web–servers used.

1.  Custodian Actions

    Views of which records granted by a custodian to which accessors or
    groups of accessors. Whether the view is re–publishable by the
    accessor and whether any access rules are in force which would allow
    unvetted access.

    If a custodian rejects an accessor’s request for access, then it
    must retain a justification for that denial.

2.  Owner Actions

    Log all transactions by each accessor stating views and records
    accessed.

3.  Accessor Actions

    Retain notifications of access rights granted.

This is the information that would need to be reconciled to show a
subject that either his custodian or a database owner has acted
improperly.

##### 7.3 Adaptability Support

Enough information has to be retained to classify accessors, and to
group them, and to classify the record views they are granted.

###### Classifying Accessors

Each accessor has to retain the membership certificates they have been
granted by the groups they are members of. The group membership
committees should also retain their justifications of why each member
was given membership.

###### Classifying Record Views

This information can be derived: as custodians grant views to accessors,
their group memberships and the ontology under which they are working
will be known. Consequently, the views can be classified.

#### 8 Engineering Model

The engineering of an information system usually concerns itself with
how services can perform best. This is usually a choice between
resilience and speed. This information system has very different primary
requirements: security and safety, and, as noted above, this is a secure
distributed processing problem. There are two functions that must be
engineered safely and securely: the protocol and formatting of messages
and the processing of them.

1.  Security

    -   Integrity

    -   Confidentiality

    -   Authentication

2.  Safety

    -   Authorisation

    -   Information Flow

    -   Inference Control

There is no problem with message security, well–proven protocols, like
Transport Level Security [ Eav99e ] and its predecessors, the Secure
Socket Layers [ FKK95 ] are supported by web–servers and web–browsers.
It is the safety of the processing that needs to be considered. The most
important point is that a process is effectively an accessor and any
process that operates on secured data should have the appropriate
security clearance. A security clearance would be required both for the
implementation of the process and the host machine it runs on. This has
long been recognised in secure processing, see [ Ash99 ] for more
explanation on the difficulties this gives rise to.

Security clearance for implementations is needed because they may
possess covert channels of communication, see, for example, [ TGC90 ]
and security clearance for the host machine is required because the
process owner, or a corrupt systems administrator, could trace the
process as it runs and capture any information it holds.

Secure process implementations and secure execution environments are
collectively known as a Secure Computing Base; the need for which has
been well–known for some time [ Den82 ] ; the difficulties of developing
a secure computing base for mobile agent–based systems are discussed in
[ ST93 ] .

##### 8.1 Functional Specifications

The database access system proposed has the following functions to
fulfil:

-   Memberships accreditation

-   Database broking

-   Access negotiation

-   Query formulation

-   Query delivery

-   Results delivery

-   Results presentation

###### Memberships Accreditation

The accessors would organise group memberships amongst themselves at a
web–server which supports secure transactions. This would require that
the professional organisations that accessors would belong to have a
Certification Authority, CA, available. ( Thawte [ THA99 ] , for
example, offers a cross–certification service.)

###### Database broking

At a web–server, access to which may also be secured, accessors would
see which databases are available to them to negotiate access to. This
is essentially a trading service, [ ISO97c ] . This would seem the
sensible point to put them in contact with the database owners and begin
the process of access negotiation.

###### Access Negotiation

There are three processes that could be followed:

-   Access by rule

-   Access by custodian consent

-   Both of the above

If there is a rule that can be followed, then it is simply a matter of
checking the accessor’s credentials (group memberships) with the
requirements of the rule for the given database and the stated ontology
of the accessor.

The other two require that either all the custodians be contacted or
those custodians who have not delegated to a rule.

Some agent has to be put in place that can:

-   Apply access rules

-   Obtain access decisions from custodians

###### Query formulation

Most accessors would not want to formulate queries using standard SQL .
They would probably use some kind of forms interface, as is common with
most commercial web–based databases [ ill96 ] , but this might be
specific to their ontology and may be recommended by their professional
organisation.

###### Query delivery

The query, once formulated as SQL , would be encrypted and submitted to
the database back-end. The query should be archived as evidence in the
event of misuse.

###### Results delivery

The results of the query may need post--processing to minimise the
opportunities for inference ⁷ ⁷ 7 See [ Den82 ] for inference control
mechanisms. . The results would also need to be archived as evidence.

###### Results presentation

Again, it is unlikely that accessors would be able to use the results in
the format returned by the database and some post–processing may be
required to present the results in a usable format for them.

##### 8.2 Agents

The agents for the system can now be specified following the functional
specifications:

1.  Accessor Memberships Agent

2.  Database Trader

3.  Access Negotiator

4.  Query Formulator

5.  Query Delivery Agent

6.  Results Delivery Agent

7.  Results Presentation Agent

#### 9 Enterprise Models: System Agents

As stated there are three parties to each access contract:

-   Owner

-   Accessor

-   Custodians (Owner is a custodian of the database as a whole)

The functions performed by each agent have been listed and briefly
described. It is now necessary to specify who has responsibility for
providing each agent’s secure computing base and who uses it and to whom
the secure computing base must provide assurances. Table 2 clarifies
this. Supplier indicates which of the parties should provide the secure
computing base, Users is a list of the parties who would use the agent,
Assurances to is the list of those it must provide assurances to. (Bear
in mind, again, that the owner of the database is also the custodian of
it as a whole.) The concept of a supra–organisation has to be
introduced — Supra–accessors etc. — they vet their own members or act,
collectively, on their behalf.

What is unusual about these “ownership, use and trust” relationships is
that they are tri–partite. Most relationships between entities in
systems are bi–partite. All bi–partite system interactions can be
reduced to be (produce, consume), but tri–partite relationships have to
introduce a second interaction which is to observe the produce–consume
interaction: (produce, consume) and ((produce, consume), observe).
Observation is achieved by having the producer and consume both sign the
information they produce and consume and using that as a product the
observer consumes. The observer would reconcile the information produced
and consumed.

As Spreitzer [ ST93 ] has pointed out, one secure computing base will do
for all parties, if they are all satisfied that the computing base is
secure enough for all of them.

#### 10 Engineering Model

It is now possible to specify the system. Booch [ Boo94 ] object
interaction diagrams are used here. (The class relationship diagrams are
not given.) The interaction diagrams are easy to understand.

1.  Objects are described by @xmath —  the name may be omitted. The
    attributes of the object are listed under the name and class. Very
    often one of the attributes an object possesses is a back–reference
    to the object that contains it.

2.  The short arrows are method invocations by one object onto another.
    The arrows terminated by a small circle are the return values of the
    method.

3.  The lines connecting objects denote that the object sending a
    message has the object reference to the recipient a priori : it is
    part of the object’s state when the interaction starts. Object
    references can be qualified by an F if the object is a field of the
    one referring to it.

4.  New object references are obtained as the interaction proceeds by
    returning an object reference as the result of a method invocation.

5.  Objects can create other objects. The creation call is the name of
    the class with the construction parameters. An arrow to the object
    indicates which has been produced.

In the text, classes are denoted by this style Class , objects by this
@xmath .

##### 10.1 Accessor Memberships Agent

Every accessor is initially an object of class Member . Each one of
which has been issued an X.509 certificate, encapsulated in an object of
class Credential .

When an Member object applies to join a group, Group , it follows the
object interactions shown in figure 10 . Object @xmath has a credential
@xmath and applies to join group @xmath having credential @xmath . The
group has a membership committee which vets the application and, if
successful, asks a certification authority to create a new credential
specifying @xmath that states @xmath is a member of @xmath . This
credential is then passed back to @xmath , who accretes it for its own
use.

(Note in the interaction diagram, the Membership Committee object has a
reference to the prospective Member . This is just shorthand.
Ordinarily, the new credential would be sent back to @xmath by @xmath .)

##### 10.2 Database Trader

This system agent is best provided by some collective agency for the
owners of the databases — Supra–owners.

Custodians will want to prevent database owners from publishing their
databases indiscriminately, because it will mean they will have to vet
too many access requests and possibly incur greater risks of disclosure.
The database trader has to provide assurances to them.

Database owners will want to specify which type of accessors be allowed
to know what databases they possess.

(This is not specified in the interaction diagrams: the database trader
should also provide a justification for any denial of access to an
accessor.)

It should be apparent that publishing the database at a trader is a
similar access control problem to that of determining whether access to
the records to a particular accessor is allowed.

The interaction shown in figure 11 shows how a member would obtain a
list of databases. Each database has a set of views and each set of
views contains a statement of its relevant ontology.

Once the member has obtained a reference to a database, it can
interrogate it to see what ontologies it can be used for and the current
set of groups who have designated as existing in that ontology. This is
illustrated in 12 .

##### 10.3 Access Negotiator

The Access Negotiator agent is best provided by a collective agency
acting on behalf of the custodians  —  Supra–custodians. It services the
requests of the accessors and provides assurances to the custodians.

Figure 13 shows the arrangement of the objects, before access
negotiation begins in earnest. @xmath submits a request to the Access
Negotiator @xmath stating his credentials, the database @xmath and set
of views @xmath @xmath wishes to access. Because @xmath has been able in
figure 12 to interrogate the database directly, it can supply a subset
of its credentials @xmath which it knows will satisfy the criteria.

The Access Negotiator would be empowered to determine if @xmath has
supplied a set of credentials @xmath which do entitle @xmath to access
the requested views @xmath in @xmath by referring to a SetOfPrecedents
for the SetOfViews @xmath . This is not detailed in an interaction
diagram. In this activity, the access negotiator simply acts as an
access control list enforcer.

The real work of access negotiation is shown in figure 14 . The Access
Negotiator passes the access request to the SetOfCustodians . Each of
the custodians would obtain the ontology and the groups working within
that ontology for the views requested by the Member object making the
request and the ontologies and groups of the Member by referring to the
SetOfCredentials supplied by the Member object.

If the SetOfCustodians collectively agree that the Member object should
be allowed access they would create a new Precedent object allowing
members of group @xmath to access views @xmath .

Incidentally, it may be necessary to create a new group with
corresponding credentials to allow a particular accessor a set of views.

##### 10.4 Query Processing

A set of four agents form a call and reply chain. The incomplete object
interaction diagram appears in figure 15 . The Member object instructs a
Factory object to create the objects using the SetOfCredentials @xmath
to access the database @xmath , the views may need to be provided if the
accessor has a choice of views available to him. The remainder of the
object interaction is not diagrammed, but it would consist of
formulating a query, which would then send it on to @xmath , which would
encrypt it correctly for the database, and would probably share an
encryption key with @xmath . @xmath would send the results on to @xmath
which would then return them to the accessor.

Queries submitted and results delivered would need to signed and
returned to the respective originators as part of the observation
procedure. Again, this is not detailed in the interaction diagram.

###### 10.4.1 Query Formulator

The view granted to the accessor will provide meta–data describing its
contents. Although this information could be construed as being
sensitive, there seems little point in protecting this, so the query
formulator can be wholly owned by the accessor.

###### 10.4.2 Query Delivery Agent

This agent is responsible for delivering the query securely to the
database that can answer it. This agent is responsible for collecting
the query from the query formulator. Constructing a message containing
the query. Having that query signed as originating from the accessor and
sending it. On receipt, the server at the database would check the
signature and so be able to check that the accessor submitting the query
has the right privileges to do so.

The database owner knows best how to do this, but the query has to be
logged, should evidence of a breach of trust between owners and
custodians be needed.

###### 10.4.3 Results Delivery Agent

The results will contain classified information and they need to be
logged to prove breach of trust if needed. As pointed out above, there
may be a need to perform post–processing, so the results delivery agent
should be owned by the custodians.

##### 10.5 Results Presentation Agent

The results are classified, so the presentation agent has to be owned by
the custodians. The accessor would collect the results from the agent
using a secure channel.

#### 11 Summary

The design issues that have arisen from this analysis are that lattices
of information flow are needed.

-   Each accessor would require a set of security clearances assigned to
    them

-   Each view of the records available in the databases must be assigned
    a security clearance

-   Each of the system agents must have a security clearance

-   Each secure computing base must have a security clearance.

It should be possible to generate these lattices by interpreting the
following information:

-   Hierarchy of group memberships produced by accessors’
    supra–organisations

-   Adjudications by custodians of views granted to accessors

-   Hierarchy of secure computing bases.

Methods of generating lattices will be addressed later.

### Chapter \thechapter Access to a Database

Chapter II took the functionality — and the security features — of
databases very much for granted. This chapter sets that right, with a
positive result: SQL databases do have the functionality to support an
adaptable information service. The previous chapter proposed a system
architecture, this chapter is a technical analysis of the capabilities
of the most important component of that system.

#### 12 Issues

All that is required is that a database can be shared by two sets of
users: “native” local users and World–Wide Web-based remote users. This
produces its own set of issues:

-   Capabilities

-   Ownership and Autonomy

-   Portability and Adaptability

-   Security and Safety

The first of these addresses how different databases grant or deny
access — what information and computations can be performed by them. The
second addresses: how much can a database owner allow a foreign
administrator to operate upon the database. Thirdly, how flexible is
database access technology, can it adapt to a client’s environment or
must the client adapt to it. Finally, can databases be secured and how
safe are they.

##### 12.1 Capabilities of Databases

What follows is a brief summary of the differences in design between a
number of databases ⁸ ⁸ 8 Oracle, Informix, Postgres and Illustria [
ill96 ] . that support SQL . The differences considered are how may the
database be secured. Currently, SQL is at version 2.0 or that
standardised in 1992. Unfortunately, there are three types of SQL
conformance: full, intermediate and entry.

  Meta–data  

     SQL –conformant databases have different internal architectures,
    since it depends on the type of SQL conformance as to whether
    catalogues must be provided. Catalogues and schema are implemented
    as tables, so it is possible to emulate them. Only in full– SQL is
    it required that the full set of procedures to manipulate catalogues
    be available. Catalogues contain schema. If catalogues are supported
    then at least the Information Schema must be contained in each
    catalogue. The information schema contains meta-data about tables,
    views and procedures. It will contain names and descriptions of
    fields and the behaviour of procedures. All persistent objects named
    by a schema are associated with the authorisation identifier of the
    schema. When an SQL –session is started a cluster of catalogues is
    assigned to the session.

  Relations  

    Relations are tables or views and all SQL databases support both.

  Accessing  

    Two aspects to this: security and loading. Most SQL databases
    support the access control primitives. These are “Revoke” and
    “Grant” for a named user or group and are only applicable to
    relations. Rights are not associated with procedures. As for
    loading, most databases can limit the number of clients that can
    simultaneously access it, but not all of them allow clients to be
    differentiated between internal and external users.

  Rules  

    Some databases support an additional “rules” or “triggers” feature
    which is relatively recent and should be required in the next issue
    of the SQL standard. A rule allows one to specify actions to carry
    out in addition to or instead of the invoker’s action to select,
    update, delete or insert a record. One of those actions is to do
    nothing. The rule concept was originally introduced to allow indices
    to be updated or for exported keys to be updated. A “where” clause
    is permitted which allows one to perform any tests on or with a
    user’s identity. So this mechanism could be used to check whether a
    record can be released to a given user or not. Unfortunately, the
    behaviour of rules is difficult to specify and, consequently,
    implementations vary. The main point of debate is whether a rule is
    to be applied to a table or a record — with records inheriting rules
    from tables.

To secure SQL databases potentially every accessor would need to be
given an information schema of the set of views and procedures to use.
And there would need to be a corresponding set of grant and revoke
commands issued on those views.

The only rights that can be granted or revoked are select, insert,
update or delete. There is no means of preventing the execution of a
procedure, but there is no need to, since one can only operate upon
relations and access to them is constrained.

In addition, full SQL con-formant systems support the propagation of
grant rights by allowing a user to grant the “WITH GRANT OPTION” to
another. This particular feature is very useful for delegation and
re–publication and is discussed in more detail later § 12.4.2 .

##### 12.2 Ownership and Autonomy

Most organisations regret that their own database administrators have
complete access rights to their information and so are unlikely to
extend those rights to an external administrator. Further to that most
databases are so complex, it is unlikely that any administrator would
allow an external administrator to create views for each group of
external accessors, but both of these requirements are a necessity for
the system proposed.

##### 12.3 Portability and Adaptability

Portability addresses how easily a system can be used in a different
technological environment. Adaptability: how easily that system can be
modified for a different end-user.

###### Portability

A residual problem when remotely accessing a database has been the lack
of any standard for the database driver. This problem has been
eliminated by the adoption of the Open Database Connectivity, ODBC,
standard, see [ Nor96 ] for references. It provides an addressing scheme
that can locate databases on remote hosts and specify how access should
be gained (user-name and password). It does not propose a standard
protocol. ODBC drivers are still specific to the databases they
drive — a client side needs to be installed for each type of database.
There is directory service, one simply has to know the correct form of
the address.

It is now possible to use a platform independent database querying
engine, namely the Java Database Connectivity package. It relies upon
each database having a Uniform Resource Locator, URL, and the JDBC
manager attempts to load a driver class having a specified relationship
to the URL. The driver class can be loaded over the network, this means
that client machines can be configured for accessing a particular
database with no down-time. Also, because URLs are used to locate
databases, the directory service is a world-wide browser and one can use
catalogues of URLs to locate the resources needed.

###### Adaptability

Because a URL is used to specify the driver class and the driver class
can be loaded over the network, it is possible to load a different
driver class for the same database: one that might have different access
rights available or access to different catalogues.

##### 12.4 Security and Safety

###### 12.4.1 Security

This topic covers practical aspects of database access. Can the queries
and results sent to and received from a database be confidential to the
client? The simple answer is yes, but most databases do not support
secured channels directly, it is necessary to have an encrypting and
decrypting agent placed in the communications channel between client and
server database. Using the JDBC package, it is possible to write a
custom driver which encrypts queries and sends to them its decrypting
counterpart which would then forward the query to the database and then
encrypt the results. More sophisticated protocols can be implemented by
using remote procedure calls between the encrypting and decrypting
agents, such as the scheme described in [ Eav99d ] . These could
negotiate keys, add sequence numbers and perform time–stamping
transparently to the client and server.

###### 12.4.2 Safety

###### Views

One of the assumptions made in chapter II was that accessors would use
negotiated views of records and that these could be made safe by
granting rights to a set of accessors and revoking rights from all
others.

There are two authorities which justify this: Minsky [ Min76 ] and
Denning [ Den89 ] . Also one needs to consider how access rights might
be propagated.

###### Branding, Tickets and Capabilities

Minsky [ Min76 ] describes a concept called branding . Essentially,
every data type is branded and each user has a set of brands that can be
accessed. This is easily realized for a database in the following way:

  -- -- --
        
  -- -- --

Although this might seem facile, SQL is one of the few data access
languages that supports branding. It is not possible to brand objects in
most other programming languages because there is no access control
mechanism which could require a brand. It is possible to implement
branding in object–oriented programming languages that support a
call–back mechanism, but this is cumbersome. Only Java has
institutionalised it with the SecurityManager , GuardedObject and the
Permission classes of their security architecture [ SUN98a , GS98 ] . An
object–oriented system presents an additional problem for branding,
because it might be desirable to brand a base class and allow access to
it, but not to branded derived classes. Java avoids this problem by only
allowing implementation classes to be extensions of GuardedObject — not
interfaces or abstract base classes.

Finally, it should be said that object–oriented database access
languages, such as those proposed by the Object Database Management
Group [ Cat93 ] , have not really addressed security issues in their
language proposals. It is well--known that safe and secure programming
needs to be implemented in the programming language ⁹ ⁹ 9 Again, see
Denning’s discussion of flow control in [ Den82 ] . . Denning in [ Den89
] advocated a database system known as System–R , developed by IBM ,
which supported branding and had other attractive features. The query
language developed for System–R was the prototype for SQL .

It should be added that branding is now considered to be a variant of
ticket–(or capability–)based authorisation. An accessor must be in
possession of a valid “ticket” to access an object [ BW94 ] .

###### Subjects, Objects and Rights

On the occasions that authorisation systems are discussed it will be
necessary to use some special terminology. This is a précis of a
description found in [ JLS76 ] . Users of an information system are
usually designated as subjects ¹⁰ ¹⁰ 10 This kind of subject is
different from the subject that was described in the requirements
chapter II . Subject in an authorisation context would be an accessor in
the context of the system requirements. and the data entities they
access as objects . Which subjects may access which objects is specified
by a lattice model: this acts as an organisation hierarchy, lattice
models have useful properties which are discussed in § 25.2 .

Every subject and object has a specified access classification. It is
this that determines whether access is granted. There are just two
access rights — read and write — and one special right: invoke and two
meta–rights, take and grant. These latter two are discussed in detail in
the next section, but it should be pointed out that the grant right is
something of a misnomer, it does include the ability to deny a right as
well. There is one relationship which is ownership.

The create and destroy rights can be thought of as the right to invoke
the create operation on a factory object or the destroy operation on an
object itself.

The creator of an object is its initial owner. Ownership can be
transferred and shared. Subjects are not owned by any other subject or
object. All objects are owned.

Subjects can create and destroy objects and objects can create and
destroy objects if they are the owner of the object or the owner grants
permission to invoke the destroy operation. Subjects can only be created
and destroyed by some special means.

A subject can also take on the rôle of an object, but an object cannot
take on the rôle of a subject. Subjects may try to access other objects,
objects other objects and, because subjects are also objects, subjects
may try to access other subjects and objects subjects.

Because every subject or object may access every other subject or
object, then, as far as data access is concerned, they can all be
thought of as objects. This makes the rule for granting access easier to
express:

  Access is only granted if the accessing object has a high enough
  security clearance for the object it wishes to access for the
  specified right.

There are two other rights which are more subtle in their operation.
These are discussed next.

###### Taking and Granting Rights

Denning in [ Den82 ] gives a fairly complete discussion of the
difficulties of taking and granting rights to data objects. In
particular, the “Take–Grant” model of Jones, Lipton and Snyder [ JLS76 ]
. This model is constructed as follows: between a subject @xmath
possessing certain access rights and a data object @xmath which requires
particular access rights, there must exist a path of actions to take or
grant rights from and to other agents before @xmath can access @xmath .

  Take  

    A take action is performed by @xmath , or an entity acting on the
    behest of @xmath , and takes access rights from others.

  Grant  

    A grant action is performed by another agent @xmath who grants
    access rights to @xmath or an agent, @xmath , acting for it.

Clearly, if @xmath is to access @xmath , then @xmath must find a @xmath
–connected ( @xmath for “take”, @xmath for “grant”) path to @xmath .
Jones, Lipton and Snyder show that this path can be found in linear
time. They introduce the concepts of bridges and islands . Islands are
maximal @xmath –connected subgraphs of subjects only, where everyone may
take whatever rights the others (on the island) have. A bridge is a
@xmath –connected path which gains access to an island — a chain of take
actions is an example of a bridge. An initial span is a bridge to an
island from a user. Figure 16 shows a principal @xmath , using an
initial span to reach an island where @xmath has a @xmath –path across a
bridge to another island where a terminal span from @xmath to @xmath
gains access to an object @xmath .

This is exactly what people do every day when they make use of computer
systems. They log-on — the initial span — they are placed in the island
of their group and may access particular objects because of that.

As for databases, SQL only supports a “GRANT” action —  there is a
corresponding “REVOKE” action — and it is quite difficult to conceive of
a system that actually employs a “TAKE” action. Bishop [ Bis81 ] argues
that take actions are only performed by privileged users — the superuser
in a Unix system, Administrator under Windows and the DBA , database
administrator, under most SQL systems — because they are able to grant
rights to whomsoever they wish. It could be argued that if a user in one
role, such as system administrator, grants to himself in another role,
an ordinary user, a right that he would not normally have, then this
constitutes a take action. It would seem then, that a take action is a
grant action that does not require inducing the owner of the right to
grant it.

In which case, there are many examples of take actions. If one inspects
the CERT Coordination Centre’s archives, see [ CER98 ] for example, it
is all too clear that there are many ingenious ways to take rights. A
common method is to force an overrun on a statically allocated command
buffer, which, if correctly formatted, will overrun into another more
privileged command buffer which can then make an illegal grant command.
These are implementation errors which one can only hope would not appear
in well–designed software — CERT do issue guidelines for application
programmers.

SQL users can only grant access rights, this requires the owner’s
permission which must be negotiated. If that negotiation is subject to
peer review, such as the group membership committees, then one must hope
that such membership committees will not grant membership rights
lightly.

This, still, may not be a sufficient safeguard, because it may be the
case that members of a group have access rights they do not use, but,
were they to make use of them, they would have access to data they
should not have.

It is therefore necessary to require that as security clearance lattices
are constructed, it must be proven that there is no access path from
lower classes to information accessible to only higher classes and as
Jones et al. make clear, this is possible in linear time.

###### Republishing Databases

There are two aspects to republication that need to be considered.
Firstly, if a user has legitimate access to the database through a safe
system, what prevents him from republishing it through an unsafe system.
Secondly, it would be desirable if a group can decide to grant a limited
right of republication to a sub–group, under the ægis of one of its
members, and possibly to allow access to other accessors who may not
have been vetted by the group, but who are vouched for by the member to
whom the republication rights have been granted.

The former can be dismissed quickly. It is practically impossible to
prevent republication of material gathered electronically, but it can be
costly to do. Some web–browsers now have the ability to deny the user
the ability to print or save a web–page [ SUN99 ] (and not store the
page in a local cache), but it is possible to capture the page by other
means.

It is the cost of republishing without permission that makes
republication with permission so attractive that system designers should
it make easy to do. SQL fortunately makes provision for this facility,
but does not require that it be implemented: it is the “GRANT” “WITH
GRANT OPTION”: it grants to a user the right to grant the named right.
This is exactly what is needed to give to one trusted accessor the
ability to name his own group of accessors without having to duplicate
the database. Unfortunately, it will probably be the case that the
people he wants to grant access to will not be members of any groups
known to the database owner. However, if it is possible for the trusted
accessor to give certificates to the members of his group of accessors,
then it should be possible to give them a security clearance.

### Chapter \thechapter A Prototype System

This chapter describes a prototype database access system which is a
proof of concept development for many of the ideas and analysis
discussed so far.

In the chapter discussing requirements, chapter II , emphasis was placed
upon making the relationships between the different parties closed, see
§ 3.1 , so that the management of access rights could be self–governing.

The prototype developed implements some key processes described in § 10
.

-   Access negotiator agent in figure 13 .

-   Custodians establishing new precedents in figure 14 .

The latter of these is the self–governing process and the former
triggers its operation.

Some other processes were implemented for convenience: the database
trader agent figure 11 and there is a simple means of submitting queries
and receiving formatted replies figure 15 .

The implementation is far from complete. The procedures implemented
might allow one to claim the system is safe, but it is by no means
secure. There is no encryption and no certificate technology has been
deployed; so, data transfer is all in cleartext and authentication is
rudimentary.

#### 13 Technological Components

The prototype system contains just two components:

-   A Web–server Jigsaw [ Jig ]

    The web–server is unusual because it is wholly implemented in Java .
    This makes the use of software agents much simpler — they can loaded
    into the web–server and work within its secure environment. The
    web–server acts as the secure computing environment for all users
     — accessors, owners and custodians.

    Referring to § 8.2 the agents that were implemented in the prototype
    were implemented as Java classes which were loaded and run by the
    web–server.

-   A Database PostgreSQL [ PSQ ]

    This database system does provide some advanced features not usually
    found in similar products. It did not prove necessary to use them.
    The real attraction of using this product is that the source code
    for the database and for the Java Database Connectivity driver [
    Eav97 ] is freely available.

The prototype system’s security features are very limited. The
requirements for the system and whether they were implemented are listed
in table 3 and 4 .

#### 14 System Agents

Regarding the implementation status of the agents § 8.2 :

1.  Accessor Memberships Agent

    Not implemented. There are suitable systems available, for example,
    a moderated e–mail list would suffice for some applications. The
    prototype system used the configuration feature of the Jigsaw
    web–server to add new members.

2.  Database Trader

    Implemented. It is possible for database owners to post the URLs of
    their databases to the Jigsaw web–server.

3.  Access Negotiator

    This is the key feature and both of its parts, figures 13 and 14 ,
    have been implemented, and there no adaptive access control.

4.  Query Formulator

    Not implemented. There are suitable systems available. There was a
    JDBC–based database query agent freely available, but this has been
    superseded by a commercial product. In the prototype, one can only
    submit queries on the name fields and one receives the whole record,
    from the assigned view, in return.

5.  Query Delivery Agent

    Implemented. A query delivery agent is part of the database driver
    that is supplied by the database owner to the accessor.

6.  Results Delivery Agent

    Implemented in the same way as the results delivery agent.

7.  Results Presentation Agent

    Implemented. The accessor can choose which results format is used.
    The classes to present the data are posted to the web–server.

Most of the information needed to prove that the contracts between the
parties have been adhered to § 7.2 is available from the web–server’s
logs. These would need to be reconciled with the database logs of the
queries submitted and results returned.

#### 15 Adaptive Components

The systems needed to provide adaptability support, § 6 , are not part
of the prototype. They are more experimental and need a surer
mathematical basis before they can be implemented.

#### 16 Software

This section describes which parts of the system were implemented by
which component of the software. The software is described in more
detail in appendix III . The software used by the web–server to
implement the functions of the system has a different engineering
configuration.

##### 16.1 Implementation Engineering

1.  Database driver

    This provides the following system agents:

    -   Query Delivery Agent

    -   Results Delivery Agent

2.  Queriers

    This provides:

    -   Query Formulator

3.  Formatters

    This provides:

    -   Results Presentation Agent

The following agents have not been implemented in a way in which they
can be recognised as agents.

-   Access Negotiator — the access request component is implemented as a
    web–form; if access is granted, it is carried out using the
    configuration tools of the web–server.

-   Database Trader — appear as web–pages within the web–server
    describing the databases available.

##### 16.2 Design Engineering

1.  Accessor Memberships Agent

    This was implemented by using the configuration features of Jigsaw
    and therefore custodians interacted with a set of web–pages. As new
    users (or accessors) were added to the system, they were made part
    of the UserRepository . Each user is allocated to a “Realm” which
    approximates to their ontology. (Unfortunately, a user can have only
    one realm in this implementation of Jigsaw .) The UserRepository
    actually makes use of a database as well. The idea underlying this
    is that each custodian, or supra–group for custodians, would provide
    a database of their members. Access to this database would be
    subject to a contract between the web–server provider and the
    supra–group administrators.

2.  Database Trader

    This was implemented using an HTML form within the web–server known
    as the ResourceAdder . A database owner fills in the form on the
    web–server describing the database he plans to make available. He
    must provide URLs for a suitable JDBC database driver and for
    compatible query formulator agent — QueryByNames .

    The JDBC database driver provides the two delivery agents (query and
    results). The query formulator provided is a simple one that only
    allows a single query by a patient name to be submitted.

3.  Access Negotiator

    This has been implemented as a web-page form which generates an
    e–mail which is processed by an e–mail filtering program operated by
    the custodian. If the custodian grants access, an e–mail is sent to
    the owner who then creates the view granted, a user identity and
    posts a new database resource to the web–server. The UserRepository
    is then modified by the owner using the configuration editor of the
    web–server. Periodically, e–mail messages were sent to users to tell
    them which database resources were available to them.

    There is also some simple rule application. When the e–mail from the
    web–server is sent to the custodian, the custodian is informed in
    that message who else from which realms has been granted what views.
    The custodian can reply to the e–mail specifying which of these is
    applicable can be applied to the incoming access request.

4.  Query Formulator

    A simple version of a query formulator was implemented and appears
    as the resource QueryByNames in the web–server.

5.  Query Delivery Agent

    This is provided by the database owner when it posts the URL of the
    database into the database trader.

6.  Results Delivery Agent

    Same as the Query Delivery Agent.

7.  Results Presentation Agent

    This is posted with the database driver at the database trader. It
    need not be provided by the database owner, it is a set of
    text–processors which are compatible with the output of the
    database.

#### 17 Testing and Discussion

##### 17.1 Tests

A fairly large database (1400 records) was made available. Two sets of
tests: functional and performance.

###### Functional

One set of tests showed that it was possible for accessors to send
e–mail messages to custodians who could then instruct database owners to
add views and that the users were notified of the views now available to
them.

Another set of tests showed that the queries presented at the web–server
were satisfied in exactly the same way as they would have been had there
been no web-server interceding.

###### Performance

The test conditions are summarised below:

1.  Machines

2.  Query Parameters

    1.  Select on indexed key

    2.  Display 38 field record

3.  Database Parameters

    1.  1356 records in table

    2.  Query satisfied in 4.5 seconds with text formatting by the
        database acting alone.

The results were:

###### Some analysis

The web-server system is a lot slower than the database alone: up to ten
times slower on start-up and a constant 3 times slower when running.

1.  Implementation: The performance has the characteristic of Java
    systems, which is that first time operation is particularly slow,
    since the Java virtual machine must load all the classes needed.
    Thereafter, access can be faster, but Java memory management is
    non-deterministic because classes are unloaded if space becomes
    cramped.

2.  Communication Overheads: the browser is connected to the web-server
    which is connected to the database twice, once for the meta-data and
    once for the query. PostgreSQL operates by receiving a query on one
    process — the postmaster — and forking another process for the
    connection and processing the query. When using the database’s own
    front–end, the forking has been performed already and the same
    process is used for both meta–data and data enquiries.

3.  Extra Processing: the web-server implementation formats the query
    results into an HTML table which the PostgreSQL query frontend does
    not. The implementation of this formatting code is quite a general
    parser, so it is fairly inefficient.

###### Some discussion

There is unlikely to be much improvement in performance from running the
processes on the same machine, since most operating systems treat
internal pipes in the same way as remote sockets.

Despite there being three processes involved there is not much
parallelism to exploit: the only opportunity might be with sparse
queries in a very large database, one could could be retrieved and
formatted while the next is sought at the database.

There is one very real reason why performance suffers:

-   Looking up catalogues on each query submission

HTTP is a stateless protocol — the server holds no state — the client
must present all credentials on every access. The client only presents
his identifier when accessing the server, so a call has to be made to
the database to collect the catalogue and the view and then another call
to submit the query and the results. When one considers this, it becomes
clear why the best performance of the system is at least twice as slow
that of the PostgreSQL frontend.

The obvious improvement is to cache the catalogues against each
accessor. This is quite feasible in Java , it would be possible to hold
catalogues against a cookie ¹¹ ¹¹ 11 Cookies are just randomly generated
signatures which both a client and a server hold, effectively a session
identifier. associated with the client at the web–server. Unfortunately,
caching is troublesome to implement. It might be the case that a view is
revised, in which the catalogues held in the client’s cache at the
server would be out–of–date. This might lead to a security breach.

Lately, Java has added a better database access facilities under the
Enterprise Beans framework. This manages cookies and reuses a pool of
database connections. Future implementations of web–based database
access products should use this technology.

##### 17.2 Discussion

The proposed system’s use of agent technology has been proven in concept
and it only remains to consider how it might be improved and extended to
provide all of the functionality given in § 8 .

###### Improved Implementation

The performance can be improved with a cache for catalogues and improved
implementations of the formatting functions. However, as always with
software systems, it is best not to concentrate any coding effort on
improving performance until the design is complete, but the need for
improved performance must be recognised in the design.

#### 18 Realizing the Proposed System

The prototype system is quite similar to the proposed system in its use
of agent technology: the database driver contains the query and results
delivery agents, which do the bulk of the work. The query formulator and
results presentation agents are pieces of software that the accessor
runs. The former uses the catalogues generated for the accessor, the
latter uses the results when delivered. The prototype system does not
implement re–publication, but it would be relatively easy for it to do
so.

To secure the system: encryption and certificate technologies have to be
used and each agent implementation has to be signed with a manifest so
that accessors, owners and custodians can be sure that the agent
implementations are authorised. Encryption is available, certificates
can be bought and there are application development libraries that allow
the two to be used together. Application signatures under certificates
are already available. The only difficulty is to implement and integrate
it all.

Integrating the application signatures for the agent implementations of
the query formulator and results presentation agents would take the form
of presenting them on the the web–pages where they are selected. The
server at the database can check the signature of the query delivery
agent and the web–server can check the signature of the results delivery
agent.

All of the agents — query and results delivery, query formulator and
results presentation — would need to be assured that they have been
invoked on a mutually acceptable secure computing base, (SCB). This SCB
would need to sign the messages going between client, web–server and
database as well.

There is currently no well–evolved technology to do this. The only
method that is appropriate is secured remote procedure calls. There is a
proposal to extend the remote procedure call system of Java to support
this [ Eav99d ] . When there is a solid technology on which to base
these interactions then the system will be fully realizable.

The prototype does not perform any adaptive work — it does not attempt
to classify views granted, secure computing bases or accessors.

## Part III Political Control Mechanisms

### Chapter \thechapter Political Control of Access Privileges

The first part of this dissertation has described sophisticated
mechanisms for securing data and making information safe, all of which
would require that policies be specified stating what information may be
accessed by whom, § 6.4 — when and where data may be accessed would also
need to be made plain. It is the notion of policies and the formulation
of them that prompts one to consider using political procedures to
control them.

As will be seen, this thinking is not entirely new but it does not
appear to have been openly acknowledged that policy–making for system
management is a political process. Add to that, that politics is not as
amenable to quantitative analysis as economics. There are many excellent
mathematical analyses of the operation of auctions, see [ MRS90 ] , for
example, but, in comparison, there are relatively few that describe how
an organisation structures itself.

The remainder of this chapter describes how authorisation systems can be
thought of as political systems and introduces the quantifiable concept
of norms. It then briefly describes some more suitable languages for
communicating rules and concludes with a discussion of the enterprise:
its goals and norms.

#### 19 Authorisation Policies for Databases

Access control policies will be discussed in this section. The same
terminology used in § 12.4.2 in the paragraphs Subjects, Objects and
Rights and Taking and Granting Rights will be used. The rules described
there will be changed here to demonstrate how different access control
policies are implemented.

##### 19.1 Mandatory and Discretionary Access Control Policies

The formulation of authorisation policies can be carried out in,
broadly, two ways ¹² ¹² 12 This is a summary of a longer, illustrated
argument in [ Eav99a ] : mandatory or discretionary access control.

Before describing the differences between them, it is best to describe
what they have in common. Both systems are under control of an
administrator. The administrator is the only entity that can create
subjects. In access control system design, the systems are idealised. It
is not possible to copy an object and use it in place of the original.
This is form of object protection is cryptographically feasible see [
GS98 ] .

###### Mandatory access control

If the lattice of information flows between subjects and objects cannot
be changed by the subjects while the system is operational, then access
control is mandatory. This is the case with military systems which use
variants of the Bell–LaPadula model [ BL73 ] .

The rights management rules for mandatory access control systems are as
follows:

1.  Ownership may not be transferred or shared by any subjects or
    objects. Only the administrator can change ownership. When ownership
    is changed, the object can be thought of as being destroyed and
    created anew. One would then apply § 3 .

2.  When a subject creates an object, the object’s access rights are
    fixed in that state and can only be changed by the administrator.

3.  When a subject reads an object and modifies it, a new object is
    created and rule § 2 is applied.

4.  The take and grant rights are denied to all except the
    administrator.

Usually, the delete right is denied to all except the administrator.

###### Discretionary access control

The alternative is to allow enough information to flow within a system
so that subjects and objects can interact with one another. They will
then evolve a lattice of information flows by specifying security
clearances for the objects they create between
themselves — Discretionary Access Control. This is the case in most
operating systems and SQL databases. A system administrator creates an
initial set of subjects and objects and the initial lattice flows by
allocating the subjects and objects to groups. As each subject or object
creates another object, it can specify which groups may access it. Only
the system administrator can create subjects and give them group
memberships.

1.  Rules § 1 , § 3 from mandatory access control are applicable.

2.  When a subject creates an object, the object’s access rights may
    only be changed by the owner.

3.  All owners possess the grant right, but the grant right may only be
    possessed by owners. It may not itself be granted. The take right is
    denied to all except the administrator.

Usually the delete right is available to owners and may be granted to
others.

###### Unix System V release 4

This system of rights is the same as that found in Unix operating system
since System V release 4, (SVR4). System V release 3 and prior versions
allowed owners to transfer ownership which proved to be a major security
flaw. It has made inter-working between subjects more difficult.

###### Berkeley Standard Distribution @xmath Unix

The BSD of Unix overcame the security problems of transferring ownership
in a much more flexible way. It does so by applying set semantics to
access control. Each subject has a group of his own, of which he is the
sole member and there are additional groups of which he is also a
member. When a user changes group ownership of a file, it can only be to
a group of which he is a member of; in this way subjects can act in a
limited way as an administrator.

BSD also allows designated areas to belong to a particular group. The
setgid bit of a directory can be set and this ensures that every file in
that directory belongs to the group of the directory, which was set by
the subject when he created the directory.

Only the administrator can create subjects and groups, but the subjects
now have a means of administering group membership of objects
indirectly. What subjects designate as belonging to a group defines the
membership of the group. For example, if there are three users @xmath
and four groups @xmath , @xmath , @xmath and @xmath , such that @xmath ,
@xmath , @xmath and @xmath . If @xmath then he allows everyone to read
it, but if @xmath then only @xmath and @xmath may read @xmath .

(The problem with conventional operating systems is that @xmath may now
copy @xmath and make it available to @xmath , which may not have been
what @xmath intended. This is not allowed in an idealised system.)

###### Sql

Finally in this section, the discretionary access control system of SQL
must be analysed: it is essentially the same as Unix SVR4 , but allows a
grant right. The grant right can be constrained to grant only read or
write from objects. In effect, this gives to table owners in databases a
setgid bit on any of the views they create from the table.

##### 19.2 Authoritarian and Self–governing Access Control

###### Authoritarian

Both mandatory and discretionary access control policies are effectively
authoritarian because of the privileged position of the administrator.
Under the more liberal schemes used in BSD and SQL, it is possible for
the subjects to determine the information flow, but the administrator
controls group membership and subject creation.

###### Self–governing

For a system to be self–governing the privileged rôle of the
administrator must be removed by distributing it amongst the subjects.
They will vote collectively to specify group membership and whether new
subjects can be admitted.

Subjects could then choose to migrate to groups which are able to accept
them and would be able to provide them with better information. The act
of migrating to a different group with a different collective
administration is very similar to electing a political leader. It is, in
fact, a generalisation of the election process. This is a simplification
of political control that is used in the analysis of a well–known
economic policy issue: the Tiebout model [ Tie56 ] . It can be thought
of as an adaptive optimisation problem [ KMP95 ] where voters cluster
around norms of their expectations.

##### 19.3 Adaptive Discretionary Access Control Policies

There are some system proposals which attempt to discern norms of
behaviour and use them to constrain the information flows between the
subjects and objects of a system. Minsky proposes a system of laws under
which software systems would work [ Min89 , Min95 ] . A paper by Rabitti
et al. [ RBKW91 ] describes additional authorisation generation
mechanisms to support the lattice model for an object–oriented database.
The innovation of the system is that it generates its authorisation
policy as it operates by generalising current behaviour to form norms.
Authorisation is viewed as having three dimensions:

  Expression  

    Authorisations specified by users, which are known as explicit and
    those that are derived by the system, known as implicit .

  Direction  

    An authorisation can be positive , stating what may be done, or
    negative stating what may not be done.

  Strength  

    An authorisation may be strong , in which case it may not be
    overridden, or weak , in which case it can. Strong authorisations
    can be thought of as axiomatic.

This model has been extended [ BW94 ] and a recent contribution by
Castano [ Cas97 ] introduces metrics that can be used to generate norms,
including:

-   Operation compatibility

-   Subject similarity co–efficient

-   Authorisation compatibility

-   Semantic correspondence

-   Clustering of subjects

All of which seem to be methods of ascertaining if different subjects
(or objects) belong to the same type. If the subjects and objects are
typed already, then some concrete questions can be resolved by using
abstract rules. If a subject @xmath is able to grant rights to an object
@xmath , if @xmath and @xmath then @xmath is implied.

##### 19.4 Preferential Logics and Operators

If two organisations are to share information, then a new organisation
is formed which contains the authorisation hierarchy of both. This
requires that the two information flow lattices be combined and this, in
turn, requires a well–defined logic to do so. There has been some
research into preferential logics [ ARS98 ] and some useful operators
have been defined. This work is based on graph–theoretical analysis of
the flow lattices which is something taken up later in this
dissertation.

#### 20 Suitable Languages

If preferential logics are to operate on authorisation hierarchies of
organisations using databases, the combined hierarchy will need to be
communicated to all interested parties. There are already some suitable
languages for this purpose.

  KQML  

    The Knowledge Query Manipulation Language is an agent–communication
    language and is described in [ Fin93 ] . It is part language and
    part protocol.

  KIF  

    The Knowledge Interchange Format is an ontology definition language
    defined in [ GF92 ] .

Both of these languages are explained in more detail in appendix III .
For the time being, KIF would be used to define the boundaries within
which agents may operate, (see § 6.2 , paragraph Ranking Groups .) KQML
would be used to communicate KIF descriptions of group membership lists
and rules to the different databases.

The attraction of KQML is that it is a more enterprise–oriented language
and has been proposed for governing the interactions of agents. KIF is a
formal language which would define the information model for a set of
collaborating organisations.

#### 21 Enterprises, Goals and Norms

In chapter II the Open Distributed Processing modelling perspectives
were introduced. The least understood of these is the enterprise model.
It is considered to be a statement of the goals that an enterprise
wishes to achieve — it would include “The Mission Statement” — and the
management structure that coordinates the enterprise to achieve its
mission. An enterprise is a network of performative , constative and
normative agents.

  Performative  

    A performative agent can claim that it has carried out some action:
    Executive function.

  Constative  

    A constative agent can judge if the action has been carried out:
    Judicial function.

  Normative  

    A normative agent determines which performative agent should do what
    task and which constative agent should verify it: Legislative
    function.

Most information processing systems have performative and constative
agents which are machine–processes, but the normative agents are usually
human.

###### Example: A Library

System designers allow policies to be specified for the number of books
that may be borrowed simultaneously for particular classes of borrowers.
They also allow policies for fines to be specified. The policies
actually in place at a particular library will vary.

Implicit in the design of the system are norms of behaviour expected
from borrowers. The local policies can be tuned to the behaviour of the
borrowers at a particular library when the library system is installed.

The enterprise model for the library system has been specified in part
by both the designers of the library systems (the policies that may be
effected) and its administrators (how those policies are put into
effect). The library system will have many performative and constative
agents: when a borrower takes a book out a performative agent initiates
a process which will invoke a constative agent to determine if the
borrower would exceed his quota. The only normative agents are the
library administrators who determine the local policies. It may be
possible to have adapting normative agents which are programs — they
might, for example, set the level of fines relative to a cost index, for
example.

But if there are new norms of behaviour: can the enterprise model’s
system of normative agents cater for them? A simple case might be a new
class of borrower, the system designers may have made it possible to add
new classes of user. A normative agent, in the form of one of the
library administrators, will then add a new class of user and allocate
people to it.

But some norms of behaviour may be not be so easy to cater for. For
example, if borrowers feel that fines are too high and borrowing periods
are too short, they might organise some collective action: they choose
to take their full quota of books out and return them on the same day.
This appears to have no effect on the library system other than an
increase in turnover, but it is very annoying for the library staff.

Under these circumstances, would it be possible to cater for this new
behaviour by the borrowers? Would it be possible, for example, to charge
for re–shelving if books are returned too soon? Would it be possible to
isolate the borrowers who are taking part in the collective action and
enforce the re–shelving cost on them alone? Would it be possible to
invoke a new process altogether, for example, preventing users from
entering the library if they appear to be taking part in the collective
action. If that were the case, how could a borrower appeal against the
decision to bar him.

Most library systems, like most information systems, do not have the
degree of flexibility needed to integrate new policies based on new
norms of behaviour without significant re–engineering. There are even
fewer information systems that are able to generate and instigate new
polices without human intervention.

#### 22 Summary

Political processes cater for changes in norms of behaviour: the
judiciary reports increased numbers of adjudications, the executive
reports increased workloads, the legislative modifies the law to
accommodate the changes in behaviour observed by the judiciary and
legislature.

By contrast, information systems are usually incapable of changing to
accommodate new norms of behaviour. One of the reasons for this is that
it was, until recently, very difficult to re–engineer an information
system. As can be demonstrated with the prototype system described
earlier, chapter II , processes can be specified by the interactions of
agents and these agents can be replaced, upgraded and relocated without
any loss of service.

The remainder of this dissertation addresses norms of behaviour and how
to isolate them.

### Chapter \thechapter Preference Aggregation

The database access system proposed should provide some adaptive
discretionary access control partly supported by automated deduction
based on a rule-base of precedents § 19 . There will be some relevant
information available within the system on which to base these decisions
§ 7.1 , but it is unquantified. This chapter concentrates on finding
general quantification methods — preference aggregation and collective
choice procedures.

#### 23 Generating Security Hierarchies

Generating security hierarchies is an exercise in classification. This
section describes the information that needs to be classified. This is a
generalisation of the descriptions and examples given in § 6.4 .

###### Information

The information available is derived from the relationships in § 3.1 and
§ 6.2 :

-   Databases hold records

-   Views are made up of sets of fields

-   Views are made up of sets of record ranges

-   Custodians make views

-   Accessors hold views

-   Accessors have group memberships

Group membership is effected by issuing certificates as described in the
engineering model § 10 , in particular in figure 10 .

###### Rules

There are information–ordering rules which are derived from the ranking
of individuals and their membership of groups, see § 6.2 paragraph Rôles
and Ontologies .

1.  A set of fields may be accessed by an ontology.

2.  There is some set of group memberships which maps an accessor to an
    ontology.

3.  Within each group, members rank one another by seniority.

and because of this, information can be ordered.

Two consistency rules can be added.

-   The most senior member of an ontology is allowed access to a maximal
    set of records for the view.

-   The most senior member of an ontology is allowed access to a maximal
    set of fields in the view.

###### Hierarchies

Seniority hierarchies are needed for the following:

-   Within groups

-   Within an ontology, i.e. across groups.

###### Voting Processes

Votes will need to be taken within an ontology to determine:

-   Which fields, and

-   Which records

it would be desirable to access. Accessors vote by making requests of
custodians.

Votes will need to be taken from custodians to determine:

-   Which fields, and

-   Which records, by

-   Which accessors

can be accessed. Custodians vote by granting (or denying) requests.
Custodians may also create views to meet accessors’ requirements.

###### Note Bene

Groups are usually of a broad professional concern, e.g. British Medical
Association, Royal College of Surgeons; ontologies will usually be
project–related, e.g. “study of cellular immune responses against KSHV
in HIV infected patients during anti-retroviral treatment”.

Only groups rank, ontologies inherit the rankings from groups, but
groups have to be ranked relative to one another or seniority levels
within groups have to be equated with those in other groups. This may
give rise to anomalies, it will be seen later that these anomalies
should appear in the preference hierarchies and, hopefully, will be
detected before the combined preference hierarchy is put in place.

#### 24 Preferences, Values and Norms

In section § 19.2 , it was argued that a self–organising access control
system would allow subjects wishing to access objects a choice of groups
to join. These groups would emerge around norms of behaviour. In section
§ 6.2 , it was argued that an ordering for individuals could be used to
form new groupings which would better align with the ordering of views
of data in different ontologies.

What is needed is a means of converting the preference hierarchies
derived from classifying individuals and data views into norms of
behaviour which would then be used to reclassify groups. To do this, a
metric is needed.

That there is a means to do this is partly justified by Swanson et al. [
SBM97 ] . Swanson, a theoretical accountant, analyses the function of
money in economies and notes that it is used as an indicative measure of
abstract quantities such as: “worth”, “liquidity” and “earnings
potential” in real instances. Anyone who analyses systems within an
object–oriented programming paradigm knows the distinction between a
class and an object: an object is an instance of a class. In the real
world, classes are an artificial construct and there are only objects,
so given an object, how does one know to which class it belongs in a
particular context. So, for example, a particular dog would belong to
the class Canine and possibly the class Pet . A metric is needed to
measure how many of the qualities of Pet exist in this dog.

A class is an expected norm of behaviour and classification involves
ranking behaviours relative to one another. If one could say that a
class has a set of behaviours with expected rankings, then it would be
possible to state, with some statistical certainty, if an object could
belong to a class if its actual rankings for its behaviours are close to
the expected rankings of the class.

##### 24.1 Norms from Preferences

Norms are expressed by means of preferences. Table 7 is a simple voting
procedure that illustrates a difficulty in collective choice theory [
Eav99a ] , which is known as Susceptibility to Irrelevant Alternatives .

There are three voters: @xmath and they are asked to rank:

-   Four policies: @xmath — rankings in plain text

-   Three policies: @xmath — rankings given in brackets

Using the Borda Preferendum voting system to aggregate their
preferences, the results are:

-   Four policies: @xmath

-   Three policies: @xmath

When there were four policies, it was quite clear that @xmath was
preferred to @xmath , but with only three policies @xmath and @xmath are
equally favoured. An anomaly like this undermines confidence in
political choice.

##### 24.2 Values from Preferences

Voters @xmath and @xmath rank @xmath and voter @xmath @xmath . They all
agree that @xmath , but not on @xmath . If one were to group @xmath with
@xmath to form @xmath and @xmath with @xmath to form @xmath , then they
disagree on the merits on @xmath . This seems to indicate that the two
groups of voters @xmath and @xmath have different values at a higher
level of abstraction.

To give this example a little more intuitive credibility, the three
voters, @xmath , @xmath and @xmath , have been asked to rank four
individuals @xmath and @xmath on their ability to fulfil a task. This
poll of their opinions has actually revealed that one voter has a
different idea (or value) of what is the most important ability needed
to fulfil this task: for example, @xmath and @xmath may have concurred
that “trustworthiness” is the most important ability. @xmath feels that
“trustworthiness” is important, but that some other quality such as
“ability to take the initiative” is more important.

##### 24.3 Values are Relative

Put simply, @xmath have assessed @xmath with a different set of values
from @xmath . Discerning the underlying values of voters provides two
courses of action, which could be used to eliminate the effect of an
irrelevant issue, the courses are:

-   Either: refine the issues

-   Or: refine the voters

###### Refining Issues

Refining the issues would require conditional questions to be posed. For
example: “Rank @xmath in order of their trustworthiness” and “Rank
@xmath in order of their ability to take the initiative”.

###### Refining Voters

Refining the voters is simply discarding or downgrading the rankings of
voters who do not meet one norm. Although this might seem undemocratic,
most proportional representation voting systems do this.

##### 24.4 Trading Goals

In the management of information systems, it is not appropriate that
more abstract goals be traded against one another. For example, some
users may require an information system to be “safe” and some require it
to be “fast”. Safe and fast are abstract. The users will request quite
specific features: the general goal of safe may appear as a wish-list of
a dozen or more safety features; the general goal of fast as another
dozen or so. If these were presented on a combined wish-list, and a vote
taken, then if the voters were equally split between those who want a
fast system and those who want a safe system, then the resulting system
would have some fast features, some safe, but fulfil the requirements of
neither.

The more fruitful approach is to find those who want the system to be
safe and those who wish it to be fast and separate them so that they use
different, relatively autonomous information systems.

#### 25 Organisational Structure

##### 25.1 Managing Database Access

To relate this work to the management of database access hierarchies,
Castano et al. proposed a number of metrics to determine similarities of
usage between subject and operations. They are described in section §
19.3 . These metrics could be used to inform a custodian of a record if
the accessor requesting access was behaving normally for someone with
the accessor’s interests.

All that is known of a potential accessor is his group memberships and
the view of the records he wishes to see. There are also precedents set
by others which could be used.

There would also need to be a set of measures that would rank the
trustworthiness of different secure computing environments.

##### 25.2 Lattices as Organisational Structures

The metrics referred to above and described in section § 19.3 are just
some of those that might prove to be useful in helping a custodian
decide if access should be granted to a set of records. There will no
doubt be other metrics that might prove to be useful. What has to be
addressed is how one should go about quantifying the choices that
custodians have made vis à vis those who have been allowed access to
records.

The metrics will be distance measures which effectively measure two
orthogonal qualities:

-   Trustworthiness

-   Relevance

These are used to form lattices: the vertical dimension indicates the
degree of trustworthiness, the horizontal dimension indicates the degree
of relevance to an ontology. Referring again to the medical profession:
seniority should be a reliable trustworthiness measure and the field of
medical work would be a suitable relevance measure: so a General
Practitioner conducting private research into cardiology would be ranked
lower than a consultant cardiologist with respect to cardiology, but the
cardiologist would be ranked lower than the GP with respect to general
practice issues.

These lattices are the basis for the structure of an organisation (or
community): they form an organisation chart of the relationships that
exist between groups. Organisation charts take the form of multi–way
trees and lattices can be forced to take the same structure. This is
done by introducing lowest and highest common points to all of the
branches and creating joining groups where cycles or multiple choices
exist. An example, is given in figure 17

The custodians will be called voters when discussing preference
aggregation, since their choices are effectively votes. Each
custodian/voter will have their own lattice of preferences for the
different types of accessors. The problem is to aggregate their personal
lattices with those of their peers to form an aggregate lattice.

#### 26 About Preference Aggregation

##### 26.1 History and Importance

A leading contributor to the field of decision theory (to which
preference aggregation belongs) feels that computer–aided
decision–making would take the following form.

  Computers will play an increasingly important role in applications
  during the next century. Along with routine tasks of data compression
  and high speed analysis, computers will have ever more sophisticated
  programs to ferret out interactively the most salient features of
  decision problems and structure problems accordingly. A few
  well--directed questions about values, acceptable risks and
  probabilities will yield a proposed solution or menu of solutions.
  Programs will discern the most likely directions for improvements and
  determine their promise by means of challenge questions. Sensitivity
  analyses that account for vagueness in preferences and probability
  judgements, and tend to discount marginal improvements, will be
  standard ¹³ ¹³ 13 [ Fis91 ] This paper is also an excellent summary of
  key results and the directions that research in decision theory is
  taking. .

There are a number of problems with preference aggregation: it is a
collective choice procedure and whenever there are more than two choices
from which voters can choose there is no method of choosing that cannot
be subverted by sophisticated voting strategies ¹⁴ ¹⁴ 14 Based on the
theorems of Arrow, [ Arr63 ] , and summarised in [ Eav99a ] . . This
does not invalidate collective choice procedures that decide between
more than two options, it means that one must be cautious when
interpreting the results.

##### 26.2 Structure and Notes

Before moving onto the analysis of preference aggregation, it is best to
clarify some terms. This notation is the same as that used in [ Eav99a ]
. The mechanics of the analysis will use graph theory, unfamiliar terms
can be found in the appendix III . There are a number of examples of the
operations, these are also given in the appendix.

###### 26.2.1 Preferences and Indifferences

###### Indifferences

A weak ordering allows voters to express their indifference between two
choices. A strong ordering does not allow indifferences. A weak ordering
is a tri–state logic. Indifference is represented by @xmath . Preference
is, incidentally, represented by @xmath .

###### Transitivity

It is usually assumed that preference relations are transitive, i.e.
@xmath and @xmath then @xmath .

###### 26.2.2 Lattices

###### Acyclic

A lattice of preferences must be an acyclic structure. It is usually
assumed that individuals will not have a cycle in their own lattice of
preferences, but when aggregated it is possible a cycle will arise. The
voting paradox, see table 9 and figure 18 , is a simple, and
irreducible, example of this.

Referring to the figure showing a lattice produced from a set of
preferences, figure 17 , it can be seen that cycles can be removed by
grouping classes together. In the case of the voting paradox, this is
not possible, since all classes are equally highly–rated.

###### Unanimities

These are very useful. A unanimity is a preference upon which everyone
concurs. A unanimity can be said to express a Pareto--optimal ¹⁵ ¹⁵ 15
See [ Eav99a ] for the original definition in the context of economic
welfare. choice and there are degrees of paretian choice.

##### 26.3 Methods and Representations

The remainder of this chapter looks at methods and representations that
could be used to form, manipulate and quantify the ordering of lattices
of preferences. Essentially, individuals preference ordering
superimposed upon one another give rise to a connected graph, which has
to be reduced to a spanning tree which will be the collective preference
hierarchy. This is a well–analysed task of graph theory [ Chr75 ] , but
there are a choice of spanning trees for a graph, the preference
hierarchy has to be the most preferred.

To simplify preference hierarchies to spanning trees, a number of graph
and set manipulation techniques have to be used and some distance
measures developed.

1.  Cycles

    These are the principal indicator of an anomaly in choice. They have
    to be detected and, in some way, eliminated.

2.  Unanimities

    It will be seen that unanimities can be used to partition lattices
    and can therefore be used to simplify them, which will allow
    anomalies to be avoided.

3.  Chains and Anti–chains

    Another useful structural indicator is the length of each chain in a
    lattice and the number of anti–chains in the lattice. Chains and
    anti–chains are described in appendix III , but they can be thought
    of the branches of a lattice. Each chain has at its head an
    anti–chain. There will always be at least as many anti-chains as
    chains. A small number of chains of similar length will give rise to
    fewer anomalies than a large number of chains of dissimilar lengths.

4.  Distance measures

    A useful distance measure will be introduced which will allow
    lattices to be compared to one another. This, combined with a
    knowledge of the chain/anti–chain composition of the lattice will
    allow less anomalous but sufficiently similar lattices to be chosen.

5.  Deciding Sets

    There are alliances between voters on issues. A method of
    determining the underlying values of voters will be introduced which
    will allow issues to be grouped together within lattices.

#### 27 Cycles and Topological Entropy

The most obvious sign of an anomaly of choice is a cycle in a preference
lattice. An aggregate preference lattice should be free of cycles or the
effect of cycles should be controlled.

1.  Complete Cycles

    A complete cycle can be interpreted as the voters assessing policies
    with values which are entirely different. The voting paradox, figure
    18 is an example of a complete cycle.

2.  Incomplete Cycles

    A cycle that is not complete is a statement that a group of people
    differ on the merits of some subset of policies. This may be due to
    the policies being too similar or too different. An example of this
    is the Borda Preferendum anomaly in 7 . In the former case, the
    “conflict” represented by the cycle may be manageable by eliminating
    one of the policies. In the latter case, it would be best to
    partition the voters and allow them to resolve their differences
    using some arbitration process.

The following two methods are recommended for detecting and quantifying
the effect of cycles.

##### 27.1 Using an Adjacency Matrix

Meyer and Brown [ MB98 ] have developed a measure which they call the
topological entropy of voting preferences ¹⁶ ¹⁶ 16 Topological entropy
is more formally defined in [ AKM65 ] . . It enumerates cycles and their
length.

Using the adjacency matrix representation for each individual’s
preferences — the graphs of which and the matrices themselves are given
in appendix III — these can be added together and normalised by dividing
by @xmath to give:

  -- -- --
        
  -- -- --

Following Meyer and Brown [ MB98 ] , the topological entropy of a choice
matrix @xmath can be expressed thus:

  -- -------- --
     @xmath   
  -- -------- --

The value of @xmath gives the length of the longest cycle were one to
generate @xmath . If logarithms are taken to the base @xmath then an
entropy of 1 indicates a policy cycle of length @xmath and an entropy of
0 a policy cycle of length @xmath , i.e. only each policy with itself.
For this example, @xmath . (This is different from Meyer and Brown’s
formulation as they had applied simple majority rule to the matrix
@xmath , effected by rounding up to 1 or down to 0.) By not applying the
social choice function, one can analyse how the choices of the
individuals would be interpreted by a social choice function.

Unfortunately, this is not as useful a measure as one might hope. The
graph representation, and thus the matrix form, does not handle
statements of indifference particularly well. For example, @xmath and
@xmath have different representations:

  -- -------- --
     @xmath   
  -- -------- --

and correspondingly different entropies: @xmath because the largest
eigenvalue for each is 1 and 2 respectively.

##### 27.2 Using a Transition Matrix

Probably better is to follow the approach used in games theory [ BO82 ]
and use transition matrices ¹⁷ ¹⁷ 17 This approach has been used by at
least one other author to show that ergodic Markov processes are in fact
voting processes [ Mat77 ] , so that would substantiate its use here. .
In the language of probability theory, either voter @xmath or @xmath
will get their way — assuming they are statistically independent, which
means that they would vote sincerely. One transition matrix can be
formed by addition, if one assumes they are equally probable to
influence the election. The formation of the transition matrices is not
difficult but is long–winded, so it is described in the appendix III .
Normalisation here simply ensures that the sum of the probabilities in
each row ¹⁸ ¹⁸ 18 Some authors prefer column sums to be 1 and so use the
transpose of the matrix. continues to be 1. The final result is very
easy to interpret: the probability of the system reaching a state where
@xmath , @xmath or @xmath is more dominant than the other is exactly
equal @xmath so @xmath . More formally, using the formulation of the
matrices described in the appendix III .

  -- -- --
        
  -- -- --

Although it is now self–evident in this example that no one policy is
preferred over the others, the method is to find the eigenvalue that has
the value 1 and the most probable final state of the system will be
described by the corresponding eigenvector, which specifies the
probability of each policy being in force in the infinitely long–term.
An entropy measure can then be generated from this steady state
probability vector in the usual way.

The transition matrix representation is more intuitive for
indifferences, for @xmath , the matrix would be @xmath , i.e.
indifference means equiprobable. Unfortunately, this representation is
still not satisfactory, since mutual indifference has the same
representation as a mutual contra–position, i.e. if @xmath and @xmath
for two voters and @xmath for another two voters both would yield the
same steady state eigenvector and entropy, but, with the former, the two
voters are in conflict over the relative merits of @xmath and @xmath
and, in the latter, they are in agreement.

Nonetheless, this formulation of topological entropy is still quite
useful for examining the effect of irrelevant alternatives, see table 7
, which has a transition matrix representation as follows

  -- -------- --
     @xmath   
  -- -------- --

in the first, four option case and when the second–ranked option is
removed, the matrices are

  -- -------- --
     @xmath   
  -- -------- --

Referring to table 10 , it is clear that the order produced by the
transition matrices preserves the supremacy of option @xmath over @xmath
. Neither method ranks @xmath as being the same as @xmath — which is
another anomaly of the Borda Preferendum  — but it preserves the
ordering of @xmath over @xmath .

(A Mathematica package is available that calculates the topological
entropy using transition matrices [ Eav99c ] .)

Incidentally, this entropy measure as described only yields the entropy
of the largest cycle: it may be the case that there are a number of
lesser cycles, and that would mean that the entropy of the system is
greater, since there is more confusion over its content.

#### 28 Unanimities

The preceding section looked at measures that detected and quantified
the length of a cycle in an aggregation of a set of lattices, where the
aggregation was achieved by simple addition to produce a likelihood of a
particular policy being chosen amongst all others. A number of
difficulties arose from that discussion. These are addressed now and, it
will be seen, that the exploitation of any unanimous choices can be used
to isolate cycles so that they can resolved.

1.  Policies or Policy Preference

    It will prove to be more useful to determine which policy preference
    is most often, or unilaterally, stated. For example, in the
    discussion of table 10 , it was clear that all parties preferred
    @xmath over @xmath . If this were represented in a graph, it would
    be clear that one edge is traversed more than any of the others.
    This can be used as a measure of how well–ordered a preference
    hierarchy.

    If an edge (policy preference) is always traversed in one direction,
    then, it is a pareto–optimal preference: one policy, @xmath , is
    always preferred over another, @xmath , but other policies may be
    preferred to @xmath .

2.  Similarities or Contrasts

    Under a strong ordering, two statements by voters @xmath and @xmath
    of @xmath and @xmath respectively could indicate:

    -   Either: a juxtaposition: @xmath and @xmath have entirely
        different values and that @xmath and @xmath are also different
        from one another and exemplify the differences between voters
        @xmath and @xmath .

    -   Or: a similarity: @xmath and @xmath have similar values and that
        @xmath and @xmath both embody that same value, so that @xmath
        and @xmath are unable to sincerely and consistently choose
        between @xmath and @xmath .

    This problem does not exist with a weak–ordering, but in an
    aggregation it can be obscured: an equal number preferring @xmath
    and @xmath would suggest @xmath in the aggregate.

Pareto--optimality is a desirable quality for a edge in an aggregate
preference lattice ¹⁹ ¹⁹ 19 Sen [ Sen77 ] proposed that
pareto–optimality should be ranked higher than simple majority
preference and [ FN79 ] has quantified this. and it is denoted here as
unanimity. In their paper, Batteau et al. [ BBM81 ] defined two forms of
Paretian choice: weak and strong. The former was defined as the case
when all voters agree on @xmath for one, or more, @xmath and the latter
was defined as the case where all agree on @xmath for all possible
@xmath . It will be seen that both forms can be discerned using methods
described here (a strong pareto choice is, in fact, a source ).

What is needed is an additional fitness measure that highlights if a set
of preference lattices contain Pareto–optimal statements of preference.

If one can find some unanimities, so much the better, but, if there are
no unanimities, one simply has to remove enough voters or enough
policies to produce one (or more). Both of which are quite meaningful
ways of partitioning the two sets, since a unanimity is a value.

###### Definition 28.1 (Preference and Indifference Graphs).

This simple innovation makes use of two graphs to represent the lattice.
One graph will represent the strong orderings between the vertices and
will be the transitive closure of the directed graph. The other will be
an undirected graph expressing the indifferences, which will, usually,
contain mostly isolated vertices.

The directed graph will be the preference graph P–graph and the
undirected graph will be the indifference graph I–graph .

##### 28.1 P–graph aggregates

The preference lattices will be aggregated in some optimal way for a
particular voting rule. Aggregate weights will be assigned to each
directed edge — the aggregates of the di-graphs is a 2–di-graph, i.e. a
multi–graph with at most two directed edges between each pair of
vertices, going in opposite directions.

###### Definition 28.2 (For and Against–Weights).

If the P–graphs are aggregated then for any pair of policies @xmath ,
@xmath two weights will be assigned to each directed edge between the
pair of policies. The greater will be known as the for–weight and the
lesser will be known as the against–weight . The vertex having the
greater for–weight with respect to another vertex will be said to
dominate in aggregate the other vertex.

##### 28.2 Unanimities, Sources and Sinks

###### Definition 28.3 (Unanimity).

When an edge within an aggregate of the P–graphs has a zero
against–weight and a non–zero for–weight then it will be called a
unanimity .

A complete cycle does not prohibit or invalidate a unanimity, since the
latter is a statement of preference between just two (or more) policies.
The other policies, which cause the cycle, can be dismissed as
irrelevant alternatives, if need be.

###### Definition 28.4 (Simple unanimity).

A unanimity @xmath is called simple if it is the only unanimity which
involves either @xmath or @xmath .

Note that a unanimity can be a compound simple unanimity, e.g. @xmath ,
if @xmath and @xmath are both simple unanimities.

###### Definition 28.5 (Complex unanimity).

A complex unanimity @xmath is one where either @xmath or @xmath is not
unanimously linked to another unanimity of the other. For example:
@xmath , @xmath are both unanimous, but not @xmath .

A unanimity may prove to be a sink or a source .

It may arise that each vertex of an incomplete cycle is part of a
unanimity. It might be helpful to interpret this as follows: all the
voters may agree that @xmath is the best policy, but each voter ranks
the other policies @xmath , @xmath and @xmath differently. These
definitions for cycles will be used:

###### Definition 28.6 (A Dominated Cycle).

If each vertex in an incomplete cycle is dominated by the same vertex,
then this configuration is a dominated cycle .

The voters are agreed on what is best, but cannot agree on what is
worst. For example, all prefer @xmath to a cycle of @xmath and @xmath .

###### Definition 28.7 (A Dominating Cycle).

If each vertex in an incomplete cycle dominates the same vertex, then
that cycle is known as a dominating cycle .

The voters are agreed on what is worse, but cannot agree on what is
best. For example, all prefer @xmath or @xmath to @xmath .

It should be apparent that a complex unanimity is a form of cycle, since
two (or more) policies have a policy which unanimously dominates the
cycle or is dominated by it. Complex unanimities can be treated as if
they were one of the cycles.

###### Definition 28.8 (Condensed Aggregate I–Graph).

If all individuals have the same sub–set of vertices connected in their
respective I–graphs then that sub–set can be reduced to one
“super–vertex” and this change can be carried over to the individual
P–graphs.

In effect, the individuals are unanimous in their indifference between
particular policy pairs.

###### Definition 28.9 (Condensed Aggregate P–Graph).

A condensed graph of the aggregate P–graph can be formed by reducing a
sub–set of vertices to a single super–vertex if the sub–set of vertices
forms one of the following configurations:

1.  A simple unanimity including compound simple unanimities

2.  A dominated cycle

3.  A dominating cycle

A cycle may also be a complex unanimity.

The point of doing this is that it simplifies the aggregate graph.
Simple unanimity can be replaced by the dominating vertex without loss
of information, but the cycles may not. In effect, the resolution of the
cycle has been deferred. This appears to be the process followed in [
Den82 ] when producing a lattice from a set of relationships, see figure
17 .

##### 28.3 Unanimous Properties

###### Common Indifferences

This simple algorithm can be used to determine if any of the vertices in
the I–graphs contain the same indifferences.

###### Rule 28.1 (Common Indifferences).

The algorithm is as follows:

1.  Form the adjacency matrix for the I–graph of each individual.

2.  Form the intersection of the adjacency matrices, @xmath . If any
    element of the resulting matrix is non–zero, then the edge
    represents a common indifference.

###### For and Against–Weights

###### Rule 28.2 (For and Against–Weights).

For a set of preference lattices: let @xmath range over the voters, let
@xmath and @xmath range over the policy vertices.

1.  Form the P and I–graphs, @xmath and @xmath for each individual.

2.  Find any common indifferences and carry them from the aggregate
    I–graph to the P–graphs.

3.  Form the reach matrices for the P–graphs for each individual: @xmath
    .

4.  Form the sum of the reach matrices, @xmath .

The resulting aggregate matrix will have a central diagonal of zeroes
and the entries can be evaluated for their properties.

All the other elements of the matrix @xmath will have a complementary
edge @xmath , the following conditions apply:

1.  @xmath and @xmath

    Then there is a unanimity of @xmath

2.  @xmath

    Then preference @xmath has a for–weight of @xmath and an
    against–weight of @xmath .

3.  @xmath i.e. a row is zero

    Then @xmath is a source.

4.  @xmath i.e. a column is zero

    Then @xmath is a sink.

One can also find just the unanimities by performing a logical
conjunction of all the matrices and locating the entries that remain
true.

##### 28.4 Degree Of Unanimity

Given that it is possible to find unanimities within aggregate graphs,
it would be useful to have an entropy metric based upon it. Entropy is a
probabilistic measure and it is required that the total number of events
needs to be calculated and also the number of events observed.

Unfortunately, there is no simple calculation for the total number of
different preference orders given a set of policies, because using weak
ordering complicates the formation of the permutations. The algorithm
for the calculation of the total number of preference orders is
relatively simple however, but there are no tables that can be
consulted, so:

###### Rule 28.3 (Preference Orders).

The total number of different weak preference orders for @xmath policies
can be calculated as follows:

1.  Generate all the partitions ²⁰ ²⁰ 20 [ Ski90 ] , p. 56 of @xmath .

2.  Calculate the number of permutations for each partition, call this
    @xmath .

3.  For each partition find the number of ways in which the policies
    could be allocated to the elements of the partition, @xmath .

4.  Multiply @xmath by @xmath for each partition and sum them together.

      -- -------- -- -----
         @xmath      (1)
      -- -------- -- -----

A Mathematica package is available [ Eav99c ] that performs the
calculation. Table 11 lists the total number of different preference
orders for up to 6 policies and clearly shows how large the search space
becomes.

As for the number of events observed (or distinct P–graphs produced by
the voters) the data needed for the entropy calculation is the count of
voters for each distinct P–graph. A suitable entropy metric will be
presented in the next chapter. For now, if an entropy metric is
available, then there are two entropy values for a preference hierarchy
that can be calculated. These will give an indication of the degree of
cohesion amongst the voters:

1.  P–graphs before condensing

2.  P–graphs after condensing

The entropy of the former will indicate how varied the opinion of the
voters is with respect to the totality of choices available to them. The
latter is best used in generating a conditional entropy amongst the
voters. Just to illustrate why one would need both figures: consider two
sets of votes, @xmath and @xmath , the same issues but different equal
numbers of voters. Both yield only two distinct P–graphs with the same
proportion of voters supporting each: in vote @xmath , the two P–graphs
share no unanimities, while in vote @xmath there are a number of
unanimities which can be exploited which allow both P–graphs to be
combined to one. In vote @xmath there is still complete disagreement,
but in vote @xmath , apart from perhaps some “agreements to differ” in
the form of incomplete cycles, there is enough general agreement to form
a single preference lattice.

#### 29 Chains

The form and number of chains and anti–chains is a useful indicator of
the structure of aggregate lattice. These concepts are described more
precisely and references are given in appendix III . Briefly, one can
say that a chain is an arm of the lattice and an anti–chain can be
formed from all those elements in each chain that are not directly
connected.

Under some circumstances, it may prove preferable that there be a few
long chains and one short anti–chain. The anti–chain will contain all
the maximal elements of a preference hierarchy and each chain will
contain one element from the anti–chain.

For example, a company with a manager and a clerk in each of four
departments has four chains and four anti–chains. The length of each
chain is two. If each manager is responsible to a director and the
directors meet together on a board, then there are four chains and no
anti–chains. The anti–chains are removed by the board of directors where
all conflicts are resolved. The length of each chain is now three.

To eliminate all the anti–chains it is necessary to extend all the
chains by one. If a chain is long, it is more likely to produce a cycle
under preference aggregation.

##### 29.1 Size of Largest Anti–Chain

An important theorem regarding the Decomposition of Partial Orders can
be used as a fitness measure. (A partially ordered set is a set of
orderings and is the most appropriate mathematical structure to use for
the analysis of preference hierarchies.) It is relatively easy to
compute the maximum anti–chain of a partially–ordered set (and therefore
a preference lattice). The largest anti–chain is the maximum independent
set of the order. The theorem is given in appendix III .

#### 30 Distance Functions

The problem of preference aggregation has been addressed by researchers
in other fields with different goals. In particular, statisticians have
researched pairwise comparisons , there are two papers which have a
direct relevance to preference aggregation as described here: Thompson
and Remage [ TR64 ] which deals with generating rankings from sets of
pairwise comparisons each of which form a strong ordering; and, Singh
and Thompson [ ST68 ] which generates rankings from weak orderings.
Singh and Thompson’s paper is the basis of what follows: all theorems,
corollaries and lemmas are due to them. Unfortunately, Singh and
Thompson analyse preferences with the goal of producing alternative
rankings, effectively “league tables”, of all the policies rather than
an aggregate lattice, but their analysis is also valid for the latter.

##### 30.1 Bigraphs

Singh and Thompson define a bigraph , @xmath , which has a set of
vertices @xmath , @xmath contains all the statements of indifference and
@xmath all the statements of preference — the I– and P–graphs
respectively as described above § 28.1 . They make a distinction between
circuits , which are directed cycles, and loops , which are cycles that
may contain undirected edges. They also define semi–completeness — a
generalisation of completeness — a lattice can be said to be
semi–complete if for every distinct @xmath in @xmath , @xmath , there
exists a path between @xmath and @xmath or vice–versa. With these they
are able to state the following theorem:

###### Partial Rank Order

###### Theorem 30.1 (Partial Rank Order).

A partial rank order @xmath is an ordering of the elements of @xmath ,
so for @xmath , then each @xmath . @xmath is a effectively a permutation
of @xmath . A relation @xmath is a partial rank order when @xmath
whenever @xmath . A partial rank order is reflexive, anti–symmetric and
transitive.

-    A relation @xmath on @xmath determines at least one partial rank
    order iff it is loop–free

-    A relation @xmath on @xmath determines a unique partial rank order
    iff it is loop–free and semi–complete.

This simply states that for any graph, if there are no loops, then a
ranking of the elements can be imposed. It does not specify how the
elements of @xmath are compared to one another.

###### Pairwise Comparisons

Three relations are introduced which permit pairwise comparisons; they
are: @xmath , equivalence; @xmath , strong order; @xmath , weak order.

###### Definition 30.1 (Indirect Relations).

Let @xmath be a strong order relation (asymmetric, anti–reflexive and
transitive), @xmath be an equivalence relation (symmetric, reflexive and
transitive) and @xmath be a weak order relation (reflexive and
transitive). For any pair @xmath of elements of @xmath :

1.  @xmath iff @xmath or they are in the same loop of @xmath .

2.  @xmath iff @xmath or there is a path from @xmath to @xmath in @xmath
    .

3.  @xmath iff there is a directed path from @xmath to @xmath in @xmath
    .

Using these definitions it is possible to develop the following theorems
for @xmath .

1.  There is at least one partial rank order

    ###### Theorem 30.2.
    In which case, the following conditions are equivalent:

    -   @xmath is circuit–free.

    -   @xmath is a preference relation.

    -   @xmath is loop–free.

    -   @xmath determines at least one partial rank order on @xmath .

    So a preference relation @xmath defines a partial rank order if it
    is loop–free.

    ###### Corollary 30.1.
    @xmath determines at least one partial rank order, @xmath , on
    @xmath for @xmath for @xmath iff @xmath is semi–complete and circuit
    free.

    ###### Lemma 30.1.
    @xmath is a partial rank order iff @xmath and @xmath is
    circuit–free.

2.  Exactly one partial rank order

    ###### Theorem 30.3.
    The following conditions are equivalent:

    -   @xmath , @xmath is circuit–free and semi–complete.

    -   @xmath is a simple order (transitive, anti–symmetric and
        reflexive).

    -   @xmath is loop–free and complete.

    -   @xmath determines a unique partial rank order on @xmath .

    ###### Corollary 30.2.
    @xmath determines a unique partial rank order, @xmath , on @xmath
    for @xmath for @xmath iff @xmath and @xmath for @xmath .

##### 30.2 Changing the Orientation of Edges

Bigraphs are not always circuit–free and it will be necessary to change
the orientation of edges to make them so. There are three ways an edge
can be re–oriented:

1.  Reverse the direction of a directed edge.

2.  Assign a direction to an undirected edge.

3.  Make a directed edge undirected.

If an aggregate graph is generated, it can be forced to be circuit–free
by applying a combination of re–orientations. How many of these, and
which of the three they are, can form the basis of a distance metric.

Singh and Thompson prove a theorem which states that it is immaterial if
one deletes edges or re–orients them. This might at first seem
contentious, but if one bears in mind that a partial rank order is a
transitive relation, then deleting an edge would remove a circuit and
the transitivity of the relation would retain some preferences. For any
graph there is a class of maximal circuit–free sub–bigraphs each one of
which forms a partial rank order.

###### Theorem 30.4.

If @xmath is a maximal circuit–free sub–bigraph of a complete bigraph
@xmath , then @xmath determines a unique partial rank order iff @xmath .

The maximal circuit–free sub–bigraphs can be enumerated by generating
all of the Hamiltonian paths.

###### Theorem 30.5.

If @xmath is a complete bi–graph, then there is a one–to–one
correspondence between the maximal circuit–free sub–bigraphs @xmath of
@xmath and Hamiltonian paths in @xmath .

##### 30.3 Maximum Likelihood Preference Relations

###### Probability Function

Fortunately, Singh and Thompson developed their graph–theoretical
analysis into a probabilistic model. Unfortunately, some more notation
has to be introduced.

###### Notation 30.1.

 @xmath 

    the set of policies, @xmath . Typical elements @xmath and @xmath .
    Each distinct pair of elements is compared in, statistically,
    independent trials to yield:

 @xmath 

    An ordered pair of @xmath which can be either @xmath or @xmath or
    @xmath . Each comparison may be carried out @xmath times.

 @xmath 

    is the set of all subscript pairs @xmath that have been compared and
    @xmath . The total number of comparisons is @xmath and @xmath .

 @xmath 

    is a population parameter and is the probability that the voters
    prefer @xmath to @xmath , viz. @xmath .

 @xmath 

    is a population parameter and is the probability that the chooser is
    indifferent between @xmath and @xmath , viz. @xmath .

 @xmath  and @xmath  

    are the number of times that @xmath and @xmath occur respectively.

These can then be used as parameters to a multinomial distribution. (The
multinomial distribution is the binomial but has more than two
outcomes.) This distribution is used to determine the probability that
the number of statements of preference of one policy over another @xmath
is exactly equal to the number of times of times a preference is stated.
It measures the strength of a preference: how often it is stated against
how often the chooser is indifferent between them. This equates to the
graph–theoretic notion that traversing an arc between nodes in the same
direction is a better measure of order than counting the number of times
a node is chosen, see § 28 .

  -- -------- -------- -------- -----
     @xmath   @xmath            (2)
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- -----

Because @xmath sum to one, it is possible to eliminate @xmath and use
just @xmath and @xmath as the parameters. In which case, because there
are @xmath parameters in @xmath , the set of all pairings, there are
@xmath parameters in all.

What is needed now is a measure of the most likely preference ordering:
this can be represented as a point @xmath being the ordered pair @xmath
. A sequence of these will form a preference relation @xmath in a more
constrained portion @xmath of the search space @xmath .

###### Notation 30.2.

 @xmath 

    is the parameter space, a subset of @xmath dimensional space.

 @xmath 

    is a typical point in @xmath .

 @xmath 

    is the maximum likelihood estimate of @xmath . It will have
    coordinates drawn from: @xmath and @xmath .

 @xmath  and @xmath  

    @xmath is a preference relation over some portion @xmath of @xmath .

 @xmath 

    is the maximum likelihood estimate of @xmath which is restricted to
    @xmath , i.e. to where @xmath is in force.

It is now possible to form a maximum likelihood measure.
(Maximum–likelihood is analogous to the traditional entropy measure
@xmath , but does not necessarily have the same properties.
Statisticians use it as a variance measure.) It is used as estimator, it
can be formulated thus:

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

which, when maximised in @xmath , will yield @xmath , which is the
maximum likelihood ordering of the elements of @xmath .

###### Graphs and Voting Rules

Referring again to the graph @xmath , @xmath is the I–graph of
indifferences and @xmath is the P–graph of preferences. @xmath can be
defined thus:

###### Definition 30.2.

@xmath iff @xmath in @xmath , where:

 @xmath 

    @xmath iff @xmath .

 @xmath 

    @xmath iff @xmath .

This definition also defines a particular sub–graph of @xmath which will
be called @xmath .

In effect, this defines a pair of voting rules for this distance
function, which is simple majority, but requires that the “for”–vote be
greater then the “don’t care”–vote as well. It has similarities to the
normalised simplexes used by Saari [ Saa94 ] , an illustration is given
in figure 19 .

Singh and Thompson’s result does hinge upon the definitions of @xmath
and @xmath , but it should be the case that they can be adapted to
different voting rules ²¹ ²¹ 21 Theorem 10 of [ ST68 ] is the key to
this argument since it relies on the properties of the voting rules. .

Under these voting rules, if one requires a unique strong ordering of a
complete set of comparisons, i.e. there exists a @xmath that is
contained in @xmath , which is what most theoretical political
scientists want, then it can be safely calculated that any such order:

-   Either: just does not exist, i.e. there are no strong orderings at
    all.

-   Or: a strong ordering exists, but is not more likely than any weak
    orderings.

-   Or: there is a strong ordering and it is maximally likely, in which
    case it will be unique.

###### Minimising Uncertainty

An interesting insight by Thompson and Remage [ TR64 ] is as follows:
the uncertainty of a single comparison of @xmath to @xmath is:

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

For all @xmath comparisons of @xmath to @xmath the uncertainty is:
@xmath and for all comparisons:

  -- -------- --
     @xmath   
  -- -------- --

From which it is clear that @xmath but in a specified subset of @xmath
only, so maximising the likelihood of @xmath is equivalent to minimising
the uncertainty of @xmath . Thompson and Remage [ TR64 ] make it quite
clear by stating that @xmath represents the total number of sample
preferences which are violated by the ranking @xmath .

###### An Example

Table 12 contains some illustrative data for four options @xmath . From
this an aggregate bigraph has been generated, see figure 20 , which is
not circuit–free.

All of the possible maximal circuit–free sub–bigraphs have been
enumerated in figure 21 and each of these has its own preference order,
@xmath .

For each of these the uncertainty has been calculated and is shown in
table 13 . From which it is clear that the most certain order is @xmath
, but this is a weak ordering; there are two strong orderings which are
equally certain: @xmath and @xmath , which differ markedly in their
ranking of @xmath .

Finally, it can be said that Thompson and Remage’s work provides a good
analytic method of determining which of a set of orderings is most
preferred and it gives us the ability to choose between popular weak
orderings which are less decisive, in that they cannot differentiate
between as wide a range of choices, and less popular strong orderings
which are more decisive.

A popular weak ordering is less prone to anomalies of choice than a
strong one, but is not as useful in decision–making. Computationally,
this calculation is not particularly difficult. The search space sizes
are given in table 11 . For this search space of four policies, table 11
requires a maximum of 75 running totals to be kept for the voter
population.

#### 31 Deciding Sets

Deciding sets are those groups of voters whose support is needed to win
a vote. This concept is clarified (and re–named) in a paper by Batteau
et al. , [ BBM81 ] , as the preventing set . Most people would know them
as coalitions, but they can be decisive without actually imposing their
preferences for policies. In this sense, a deciding set acts as a
dictator or, more typically, as a vetoer.

Voters usually express their values by grouping sets of issues together
and setting them against other sets of issues. Under some circumstances,
a “Kingmaker” group [ How89 ] can emerge, which has an unfair influence
according to Arrow’s theorems [ Eav99a ] . If one were to analyse an
issue space then it would be necessary to search it for all the voter
alignments that might give rise to “Kingmaker” groups emerging. The
space is large, given by ( 1 ). This problem would be a simpler variant
of the bin–packing problem, which is known to be NP–complete . There are
some efficient genetic algorithms for the bin–packing problem [ Fal94 ]
and these could be adapted to search for voter alignments.

##### 31.1 Spectral Analysis of Ranked Data

A paper by Diaconis [ Dia89 ] introduced some interesting analysis of
ranked data which is a Borda preferendum, see table 7 : @xmath , i.e.
@xmath is a ranked order @xmath to @xmath of @xmath policies such that
@xmath and @xmath voters have chosen this ordering. Diaconis then
introduces a suite of first–order functions which return counts, let
@xmath be the index of the policy @xmath , then:

  -- -------- --
     @xmath   
  -- -------- --

Or, simply, the count of all those who place policy @xmath in position
@xmath .

And a suite of unordered second–order functions which return counts
thus:

  -- -------- --
     @xmath   
  -- -------- --

Or the count of all those who:

-   Either: place candidate @xmath in position @xmath and candidate
    @xmath in position @xmath

-   Or: place candidate @xmath in position @xmath and candidate @xmath
    in position @xmath

It is also possible to have a set of ordered second–order functions
written thus: @xmath which returns the count of all those who place in
the order @xmath at @xmath and @xmath at @xmath .

And it is possible to have higher orders still, unordered and ordered.

In his paper, Diaconis used these functions to analyse the data of an
election that used a proportional representation voting system and
demonstrated that the election contained two main types of voters @xmath
and @xmath who voted on two issue blocks @xmath and @xmath , in the
following way: @xmath and @xmath .

This analysis would yield fitness metrics for selection. It would be
used to partition the voters and the issues, so that each partition
would demonstrate fewer cycles and more unanimities.

The computational complexity of spectral analysis is very high. It
requires repartitioning the search space for each possible combination.
If it is compared to calculation of distance between preference
orderings given in § 30 which required no more than keeping 75 running
totals for a four choice system, spectral analysis would require @xmath
totals to be kept.

#### 32 In Conclusion

A comprehensive set of analytic techniques have been presented which
should allow aggregate hierarchies of preferences to be generated from
individual statements of preferences with an option to choose either a
strong– or weak–ordering and to quantify how acceptable they would be to
the individuals. Other techniques have been described which would allow
voters and issues to be partitioned so that the level of acceptance
within those sub–groupings could be higher.

What this brings to the discussion of collective choice procedures is a
reinterpretation of the limitations of Arrow’s Impossibility Theorem [
Eav99a ] using information theory: it is impossible to design a
representative collective choice procedure that can select one from more
than two issues if the preference orderings have too high an entropy .
If the preference orderings are sufficiently well–ordered, the
collective choice cannot be subverted by a perverse and sophisticated
clique.

The principal entropy measure is due to Thompson et al. and is given by
( 4 ). This allows a choice between strong and weak–orderings to be
made. It should be possible to develop this to use Diaconis’ spectral
analysis of collective choices to generate preference orderings for
different sub–groups of the voters.

In short, the Thompson metric allows issues to be merged, the Diaconis
method allows voters to be merged. Unfortunately, the computational
burden for this latter measure would be very high, see § 31 , so only
the Thompson and Remage method will be used in the remainder of the
analysis in this dissertation.

### Chapter \thechapter Stability of Self–Organisation

From the discussion in chapters II and II it was made clear that
software technology is mature enough to support a self–organising
database access system, but it was also apparent that such a system
would need precisely specified policies in order to operate. Chapters
III and III showed that these policies would necessarily be aggregations
of preference hierarchies.

Chapter III introduced some distance measures which could be used to
compare different preference hierarchies. This chapter uses distance
measures to produce a self–stabilising system of interacting agents,
which act autonomously but are controlled by their own peers.
Essentially, this chapter addresses whether a self–organising system
based on group memberships can be active enough to allow policies to
emerge, but stable enough so that the agents do not follow inconsistent
policies in a short time period.

#### 33 Formation of Cultures

The system of creating access hierarchies based on group memberships is
very similar to a series of simulations carried out by Axelrod [ AAEC96
] , which were an attempt to elucidate the processes underlying the
adoption of standards in industry [ Axe95 ] .

Axelrod’s analytical method is unusual in that he constructs systems
which have agents that have very simple behaviour. He then allows the
agents to interact in a series of simulations and then analyses the
behaviour of the system as a whole. The hope is that by specifying the
behaviour of the agents, what Axelrod describes as small–scale behaviour
, it is possible to control the large–scale behaviour of the system.

Axelrod gives a number of common–place examples of the formation of
cultures. Consider nightclubs: people visiting nightclubs often want to
meet other people going there and they try to emulate one another’s
style of dress and manners. A set of features that might be considered
important at nightclubs would be: hair length and cut, style of dress,
dancing style and so on and so forth. So a person who dances in a
particular way might see someone who dances in the same way and would
choose to become more alike to them in the hope they might meet. To do
this, that person might change their haircut. Now another person with
the same haircut may choose to change his dancing style. If this
behaviour continues, then either all the people visiting this nightclub
will look the same and dance in the same way, or cliques will emerge.
One group of people will dress and dance in one particular way and
another group will do so in another particular way, which can be more or
less incompatible. If these two cliques are completely incompatible,
they can never become alike to one another, because they have nothing in
common to begin with. It may also be the case, that there are a number
of competing cliques who are slightly incompatible; they interact with
one another only rarely and there is no long–term effect. It may also be
the case that the cliques interact with one another so often that there
is no discernible similarity between the people visiting the nightclub
from night to night.

Surprisingly, many systems where standards have evolved are very similar
in operation. Consider the variation of standards for electricity
supply: for consumers in the United States, 120 volts at 60 hertz is the
standard; in Europe, 240 volts at 50 hertz. In the US, industry uses 380
volts three phase, in Europe 440 volts. This kind of dissimilarity goes
right across the electricity supply industry. Similar arguments can be
made for the evolution of the metric and imperial measurement systems,
or the morphology of human (and programming) languages. There do appear
unique similarities that cut across all systems: for example, even
though people in the United Kingdom drive on the left and people in the
Europe drive on the right, the system of traffic law is very similar:
pedestrian crossings, giving way at junctions and so on.

To add some formalism, using the evolution of culture in a nightclub as
an example, the way people visiting a nightclub choose to become similar
to one another is the small–scale behaviour , the appearance of the
nightclub and its constellation of cliques is the large–scale behaviour
. Each individual’s small–scale behaviour is based on a particular
probability distribution which is time–invariant and will be different
from everyone else’s. The large–scale behaviour probability distribution
is the superposition of all of the small–scale behaviours and may
aggregate to something quite predictable: in the same way that the sum
of a set of independent random variables can be assumed to behave as a
normal distribution.

Consider the quality control of paint colours. The manufacturing of
paint is a sophisticated process, the amount of dye added can vary
because of changes in the granularity of the dye powder interacting with
variation of tolerances in the injection heads, which can also interact
with the frequency and thoroughness of the head–cleaning procedures. One
should also consider variations in the temperature of the oil resin, the
efficacy of the mixing motors, variations in phase in power supply. All
these factors can be approximated as independent random variables,
because the causality of one set is counteracted by the causality of
another.

A statistician if asked to make an estimate of the intensity of a colour
of a tin of paint would not attempt to analyse the whole of the paint
manufacturing process but would simply assume the final probability
distribution is a normal one.

Axelrod’s cultural evolution model aims to perform the same
simplification: to determine a simple probability distribution for the
large–scale behaviour of the system, but he allows himself the facility
to control one causal probability distribution for the components of the
larger system.

##### 33.1 Small–Scale Behaviour

The simulation model employed by Axelrod was deceptively simple and is
an example of the use of cellular automata simulation [ vN66 ] . Each
agent was given a fixed set of features @xmath , each feature had an
enumerated set of values, known as the traits, for that feature. The
traits were initially randomly generated and assigned to the features.

###### Definition 33.1 (Distance Functions).

Axelrod developed a simple similarity function @xmath which was used to
evaluate how alike two agents were. If @xmath and @xmath are their
respective sets of features, with each feature being @xmath or @xmath
respectively, then if the features had the same trait, @xmath , a @xmath
is scored, otherwise @xmath .

This describes the behaviour of the trait comparator.

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

This describes the behaviour of the feature set comparator.

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

This is Axelrod’s similarity function. Clearly, two agents, @xmath ,
@xmath , are identical if @xmath . Therefore a distance function would
be

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

This can then be used as the basis for an interaction criterion. For
example, if @xmath is equal to at least @xmath the agents can interact,
if @xmath is greater than one they are more likely to interact. If it is
zero, they will be unable to interact.

###### Definition 33.2 (Interaction Criterion).

Axelrod used a simple dice–throw to simulate human decision–making, the
uniform distribution: @xmath . The distance between two agents must then
be scaled to fall within @xmath , so a factor of @xmath is introduced.
An offset of @xmath can be set in the range @xmath to make interactions
less likely.

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

The interaction would be that @xmath would choose from @xmath a feature
that was different from its own and accrete it, i.e. set a feature to
have the same trait as @xmath .

It should be clear from the specification of the change procedure that
each agent is biased towards agents that are similar to itself. For
agents to interact, it is required that @xmath . The choice of @xmath
and @xmath determine under what small–scale conditions interactions
cease. If @xmath were zero and @xmath , then no interaction can take
place if the agents are completely dissimilar. But setting @xmath to
@xmath and @xmath would mean there would be no interaction even if the
agents had one identical trait.

##### 33.2 Local convergence leads to global polarisation

The software simulation Axelrod used has been replicated [ Eav99b ] .
The agent to undergo the accretion was randomly chosen; the parameters
of the simulations were initially:

-   5 features

-   10 traits per feature

-   100 agents

-   4 neighbours

-   Topology was a square field

This is quite a testing culture. There are @xmath different individuals
to choose from. A significant emergent property appeared that is typical
of large systems:

A particular trait would become current within a group, this would make
members of that group more attractive to one another and they would
exchange more traits until their features were identical to one another.
Should such a group encounter another group that had undergone the same
process, it would be relatively improbable that they would be able to
interact. In this way, islands of homogeneity emerged.

###### Typical Culture

This can be seen in the following density and three–dimensional height
plot, see figures 22 and 23 respectively, both of which are reasonably
typical. The features have been mapped to a continuous valued metric
using the function given by ( 9 ) and then logged to the base @xmath so
that the metric is linear, see ( 10 ):

  -- -------- -------- -- ------
     @xmath   @xmath      (9)
     @xmath   @xmath      (10)
  -- -------- -------- -- ------

###### Density plot

This is shown as a pair of plots in figure 22 . It maps the identity
metric to a red–green–blue colour–code. The location of an agent can be
determined from the @xmath and @xmath axes. The colours form into blocks
where the agents are similar. The upper plot is the state of the system
before interaction commences, the lower when it has reached stasis and
no more interaction is possible.

###### Height plot

Colour density plots show where regions of identical agents have formed,
but do not clearly show how incompatible the regions are. It is
difficult to tell if a yellow region is incompatible with a blue. There
may be hint of blue colour in the yellow or vice versa . This is even
more difficult if the plots are only viewed in grey-scale.

Incompatible regions are easy to see using the height plot in figure 23
. The z axis is marked id and the physical location of the individual is
specified by the x and y axes as with density plots. The surface marked
by the solid lines is the initial state. The surface marked by the
dotted lines is the final state.

If the difference in height between two plateaus is greater than @xmath
unit then the regions are incompatible — if @xmath and @xmath in ( 8 ).

The two types of plot are complimentary. It is difficult to interpret
one without the other. The density plot shows compatibilities clearly,
the height plot incompatibilities. The distribution of the varieties is
given in table 14 .

##### 33.3 Large–Scale Behaviour

The model is a Markov process with absorbing states [ Pap84 , p. 396,
“Birth Processes”] , so it should settle after some initial transient
behaviour, but may, possibly, possess limit cycles.

In Axelrod’s analysis, these following points were considered important:

1.  How many areas of dissimilarity?

2.  How many different areas of dissimilarity?

3.  How large were they relative to one another?

4.  How quickly did the system stabilise?

###### Metrics

Axelrod’s method of to determine the fluctuation of the areas of
dissimilarity used time–series plotting and peak detection. A set of
metrics were developed that more conveniently measured the qualities of
regions. Some system activity metrics were also introduced.

 @xmath 

    is an efficiency measure and is the ratio of the number of
    interactions to the number of selections in each period. This is
    used to measure system activity.

 @xmath 

    The entropy of the different varieties of agents on the field. This
    is a single metric for measuring variance, @xmath is a normalised
    entropy, i.e. @xmath .

      -- -------- -- ------
         @xmath      (11)
      -- -------- -- ------

    @xmath is the probability of selecting an agent having variety
    @xmath , simply @xmath ; @xmath is the maximum number of varieties
    that can exist simultaneously, which, in this case, is the same as
    the total number of agents that can exist simultaneously, in these
    simulations @xmath . The variety entropy is the measure of the
    homogeneity of the agents in the population as a whole, when it is
    zero, the population has only one variety of agent and no further
    interaction is possible.

 @xmath 

    The compatibility entropy is a measure of how compatible the
    varieties are with one another. The probability on which the measure
    is based is that of an agent of variety @xmath interacting with an
    agent of variety @xmath , denoted by @xmath :

      -- -------- -------- --
         @xmath   @xmath   
                           
         @xmath   @xmath   
         @xmath   @xmath   
      -- -------- -------- --

    The event @xmath is not acted upon, so it is removed from the
    probability space as are the events @xmath when @xmath is not
    compatible with @xmath — the agents can only interact with one
    another if they are similar in one feature and have at least one
    dissimilar feature. The events that form part of the entropy measure
    do not cover the entire event space, so they need to be normalised.
    Once that is done, the entropy can be formed in the usual way. The
    entropy metric is itself normalised using the factor @xmath , i.e.
    the maximum number of pairs of different agents it would be possible
    to have.

      -- -------- -- ------
         @xmath      (12)
      -- -------- -- ------

    When the compatibility entropy is zero, every agent is of an
    incompatible variety with every other agent and no further
    interaction is possible. It is possible for a system to have a
    non–zero compatibility entropy and for no further interaction to be
    possible; this would arise if two (or more) “islands” of compatible
    agents are separated by a sea of agents with which they are
    incompatible.

###### Stasis Condition

Because the system may fall into a limit cycle, a condition needs to be
put into place that will cause the simulation to terminate. Neither of
the entropy metrics is useful for this, so a simple test is to see if
the number of varieties has changed over a certain number of periods.
(This actually needs to be improved upon, because under some short
duration limit cycles the number of varieties does change. This is
inavoidable, it will be argued later that this model shows chaotic
behaviour and the length of limit cycles is a fractal number.)

###### Typical Metrics and some Characterisation

The plot of these metrics against time for the culture whose initial and
final states are shown in figure 22 appears in figure 24 . The
horizontal axis, marked T , represents the number of cycles.

Neither the variety nor the compatibility entropy has reached zero, but
after 25 periods of no activity and no change in the number of
varieties, the system is static. Entropy drives activity, the higher the
entropy of the system the greater the activity can be. One can divide
the activity of the system into four broad epochs:

1.  Grouping and Simplifying - Anarchy

    Entropy is high and the activity @xmath rises quickly and is
    relatively constant for the first epoch @xmath ; the entropies fall
    rapidly. The compatibility entropy is more or less synchronised with
    the variety entropy, meaning that as a new variety is formed it is
    compatible with the majority of other varieties (which one would
    expect, since there are so many varieties around.) The end of the
    anarchic epoch is characterised by an entropy dip. This is due to
    some traits being annihilated at the edges of the square. There is a
    lull in activity during this dip.

2.  Migrating - Collectivism

    During the epoch @xmath , the activity increases, but the entropies
    remain relatively constant. Critically, the compatibility entropy
    increases and is slightly advanced in phase relative to the variety
    entropy: implying that an interaction between two agents generates
    another agent having a different variety. The traits are migrating
    across the population.

3.  Concentration - Oligarchy

    Epoch @xmath has a constant level of activity, but both entropies
    begin to fall: no more new varieties are being generated and
    varieties are forming into incompatible groups. The compatibility
    entropy now falls behind the variety entropy in phase and the
    difference between the two increases. This implies that when two
    agents interact the agent is either unchanged or becomes identical
    to the neighbour with which it interacted.

4.  Isolation and Stasis - Authoritarianism

    From @xmath the activity falls as do the entropies. The
    compatibility entropy falling behind in phase and having more
    pointed peaks than the variety entropy. By period @xmath , the
    system is inactive.

At the end of the simulation there are @xmath varieties: these are
ordered and inter–related as shown in table 14 . The first–ranked is
incompatible with all the others and has a numeric majority over all the
others combined. It is now a perennial dictator, it cannot be changed
and has a numeric majority.

###### Some Expected Deductions

The activity plot in figure 24 shows that the system stabilised in
@xmath periods — each period allows up to a hundred interactions.
Axelrod was able to substantiate some intuitive deductions:

1.  More features, More interaction

    The more features agents possessed the more likely they were to
    interact. The efficiency @xmath had a higher average. (There were no
    conclusions as to its expected distribution.)

2.  More traits, Less interaction

    The more traits per feature the less likely agents were to interact.
    The efficiency @xmath would be lower in this case.

3.  Bigger neighbourhoods, More interaction

    The larger the neighbourhood (that is, the number of adjacent
    neighbours), the more likely agents were to interact.

###### An Unexpected Deduction

The relative spread of regions of dissimilarity and their distinctness
proved to be less intuitive. Axelrod found that the larger the system,
the fewer the number of dissimilar regions . The explanation for this is
rather subtle: traits migrate across the system on a random walk [ Pap84
, p. 389] ; the more random the system is, the further they will
progress, the system is random for longer if it is larger; therefore,
the larger the system, the wider the spread of a particular trait,
therefore the less likely it is that the trait will be confined to one
isolationist group.

This result can be interpreted as a thermodynamic effect: the system is
a hot liquid that is cooling, substances dissolve in it and are
dispersed by Brownian motion; the greater the volume of liquid the more
mixing takes place.

This observation also helps in understanding the variation in the number
of distinct regions. On the whole, one variety will tend to dominate all
others: it is probabilistically more likely to reach stasis in this way.
If two blocks of varieties were to form which were, more or less, of
equal size and they were identical in every respect except one, then a
limit cycle would develop. At the border between the two blocks, some
would accrete a trait and join the other block while a similar number
would accrete the other trait and join the other block. Figure 25 shows
a density plot which has three blocks surrounded by a fourth. The block
of three are compatible with one another and differ very slightly from
the block surrounding. Figure 26 shows the metrics over time, the system
behaves very much as any other would up to period @xmath , thereafter it
enters a cycle.

##### 33.4 The Collective Choice Interpretation

Axelrod modified the cultural model in a number of ways and drew further
conclusions which will prove to be useful later. The relationship
between features and traits can be interpreted as a collective choice
procedure.

###### Axelrod’s Investigations

Axelrod’s simple model does help to explain large–scale system behaviour
given an intuitively appealing small–scale behaviour. Axelrod
investigated the effects of different topologies and different
stochastic inputs for selection — in particular, selecting central
agents more often, because traits can be destroyed at the edges — and
the initial allocation of traits — using a Gaussian distribution, to see
how much the final varieties could vary from a variety of Gaussian
averages.

Axelrod chose not to vary the rules agents used to accrete traits. This
does affect how the model can be used to make a collective choice.

###### Collective Choice Interpretation

The earlier analysis of preference hierarchies and preference
aggregation, see chapter III , gives an insight into the operation of
the cultural model as a collective choice procedure.

Assume there are three issues, @xmath , a system must determine the
relative strengths of each of them given that each agent (or voter) is
allowed to express a preference ordering using, for simplicity, a strong
ordering. The orderings, @xmath , is the set given in ( 13 ).

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

If the features are @xmath and the traits for each feature are @xmath
then an agent preferring @xmath would have a feature set of traits:
@xmath , and an agent having @xmath would have @xmath . If these two
agents interacted they would quickly settle their differences on the
relative merits of @xmath and @xmath and form a single variety of agent,
similarly for agents preferring @xmath and @xmath .

If an agent preferring @xmath were to interact with an agent preferring
@xmath then @xmath meeting @xmath would allow them to exchange their
primary preference and the internal state of the receiver would become
inconsistent, so the criterion for interaction is that the two agents
must agree on two preferences before they can interact.

This interpretation is valid for a wider issue set and for weak
orderings, the choice of the number of traits on which to agree does
become more complicated. Referring to table 11 , for three issues there
are 13 different weak orderings of those issues with respect to one
another. The features would be @xmath and the trait set would be @xmath
. An agent having a preference ordering: @xmath would have a feature
set: @xmath and @xmath would be @xmath .

It is possible to be more liberal in this interpretation. If one has
@xmath issues and the number of orderings is @xmath , as given by table
11 , if one then wants to simplify the orderings to some sub–set, then
one should set the number of features @xmath and the number of traits
@xmath so that @xmath . The cultural model then acts as a genetic
algorithm, but of a peculiar kind: it crosses varieties with one
another, but does not mutate, and requires no global fitness function.

One must then impose some kind of topology that allows each variety to
interact with every other. This need not be a mesh topology, because the
traits only have to find a migration path. A spanning tree for the
preference orderings will suffice and it is easy enough to set a size
for a useful spanning tree using ( 14 ), where @xmath is, once again,
the number of issues to be resolved.

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

The cultural model can then be thought of as collective choice procedure
that forms a spanning tree of a well–ordered graph. Unlike the
techniques given in § 27 , it is a stochastically–driven heuristic
method. It attempts to reduce a prefereence ordering search space of
order @xmath to a strongly–ordered subset of those orderings where some
of the issues have been merged by allowing a weak–ordering.

(The fact that some of the feature sets produced during the operation of
the cultural model may give rise to inconsistent states can be justified
in the same way as it is with genetic algorithms. It is simply a
transient state that allows new varieties to be developed. This is
argued more persuasively in [ Dav91 ] .)

#### 34 Formation of Cultures under Peer Pressure

Axelrod’s simple behavioural rule and the large–scale effects it appears
to introduce is discussed first. A new rule is introduced and a system
using it is simulated and the results analysed.

##### 34.1 Egoistic Behaviour

The small–scale behaviour that agents follow is described in the title
of figure 23 as Egoistic . Each agent can accrete a trait from one of
its neighbours regardless of the state of its other neighbours.

###### Increasing Heterogeneity

Some of the simulations conducted used a smaller playing field ( @xmath
by @xmath ) with only @xmath features and @xmath traits per feature.
These show that agents can quickly agree to not differentiate amongst
themselves, see figure 27 . (The axes are labelled in the same way as in
23 : identity, id , on the vertical axis, location within the playing
surface on the x and y axes.)

It is unlikely that a wholly homogeneous culture should arise with a
larger more complex playing field, but it is usually the case that one
variety of agent wholly dominates the others. It might be desirable to
control the degree of variety.

###### Restricting the migration of traits

Of particular interest for information security is how a small
sophisticated group might be able to enforce a consensus that certain
views should be globally accessible. Figure 28 illustrates this. Here,
two agents placed at the origin and at @xmath have seen their principal
traits migrate across almost the whole of the surface.

It would be desirable that there be distinct regions having access to
particular views/traits, but it is not desirable that views/traits
migrate indiscriminately across regions.

##### 34.2 Peer Agreement

Another simple behaviour that could be employed by an agent is to
require that one of its other neighbours also be compatible with the
neighbour from which it would accrete the trait. Unfortunately, to make
this rule more precise a modal logic is required. Modal logics are
briefly discussed in appendix III .

The model under which an agent @xmath operates is denoted @xmath .
Containing a set of agents @xmath , which are its neighbours @xmath and
a set of truths, the traits, distributed amongst the worlds, @xmath .

###### Egoistic Behaviour

Firstly, egoistic behaviour can be more formally specified. The
probabilistic fuzziness of @xmath in ( 8 ) is not given in this formal
specification. The form is of a schema, the conditions above the line
must appertain and the condition below the line can be enforced.

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

@xmath has been selected and holds the trait @xmath but not @xmath .
There is another agent in his neighbourhood where both @xmath and @xmath
are held. @xmath accretes @xmath ²² ²² 22 The class of modal logic
employed has to be irreflexive. In particular any axiom which prevents
@xmath , i.e. @xmath and @xmath must reside in the same agent.
(Incidentally, @xmath is not equal to @xmath because @xmath would be
false.)

###### Peer Agreement Behaviour

A form of peer agreement behaviour can be expressed thus:

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

The first condition is the same as in ( 15 ). The second has two parts,
there is someone else in the neighbourhood who:

-   Either: holds @xmath but not @xmath .

-   Or: could also accrete @xmath , i.e. is compatible with @xmath in
    some other way.

This form of behaviour — implemented as the class PeerPossible in [
Eav99b ] — is effectively a membership rule. If @xmath is the holder of
@xmath who also holds @xmath , then @xmath proposes @xmath and someone,
@xmath , seconds it.

In the former case, if the number of neighbours is limited to four (as
they are in the Axelrod square topology) then five may vote and a
majority, @xmath , @xmath and @xmath , have stated that they are
compatible with @xmath — @xmath votes “for” because it already holds
@xmath , the other two, @xmath and @xmath , because they hold something
that @xmath also holds.

##### 34.3 Some Expectations

1.  Slower Trait Migration and More Probable Limit Cycles

    Clearly, one can expect the rate of trait migration to be slower. A
    trait not already extant in a neighbourhood will have to wait for a
    trait that is extant to join it, before it can propose itself. If
    trait migration is very slow and in pairs, it might be the case that
    traits repeatedly cross and re–cross the field without actually
    appearing in the same agent together. This could lead to very long
    limit cycles.

2.  Edge Effects

    Under egoistic behaviour, if a trait becomes isolated at an edge, it
    had one less degree of freedom in the direction in which it could
    migrate. Trait migration on a square field tends to be from the
    centre to the edges. Under PeerPossible behaviour it should be the
    case that traits will be more difficult to dislodge from the edges,
    because the neighbourhood they belong to has one less voter, but, as
    proposer and a seconder are still needed, three out of four must
    concur; at the corners, the condition is even more stringent,
    requiring three out of three to concur.

3.  Dormancy and Second Waves

    The edge effects might lead to agents being able to preserve traits
    at the edges and corners so that as the system stabilises, and the
    traits held at the centre migrate outward, the traits held at the
    edges would overcome the traits that originated from the centre.
    This would lead to a second wave of activity.

##### 34.4 Some Results

A set of simulations was undertaken using PeerPossible behaviour instead
of Egoistic . The model is susceptible to the effects of different
starting conditions — the initial allocation of traits to each
individual, @xmath different configurations — and the number of
different sequences of interactions — @xmath for a typical @xmath period
run. Nonetheless, some useful results were observed. A typical pair of
density plots and an activity plot appear as figures 29 and 30
respectively.

###### Large–scale behaviours

PeerPossible is similar to Egoistic behaviour in that it gives rise to
diverse populations which can either be ultimately quiescent or fall
into a limit cycle. PeerPossible leads to limit–cycling populations more
often than Egoistic — as was expected. Unlike Egoistic behaviour these
can be predicted and can remain relatively stable. PeerPossible
behaviour very often results in limit cycles between comparably sized
groups, but these are more or less defined after @xmath periods (this is
discussed in more detail later.)

This can be summarised:

-   If the system does stabilise quickly, it invariably results in one
    dominant variety.

-   If the system takes longer to stabilise, then a limit cycle with a
    dominant variety varying in the number of members is the usual
    result.

An analogy to political systems might be useful here: systems that
stabilise rapidly to an authoritarian regime are similar in behaviour to
third world political systems — an immutable consensus emerges; systems
that exhibit limit cycles are comparable to first world political
systems — a variable consensus emerges.

Whether a population will stabilise quickly can be determined quite
reliably by the changes in its entropy characteristics as it evolves.

Figure 29 shows different identities are usually attached to an edge.
This is also true of cultures produced by egoistic behaviour, but it
appears to be more marked for PeerPossible behaviour. This is a result
that was also expected.

An interesting and useful side–effect of slower trait migration is that
large–scale behaviour becomes more predictable because traits are more
likely to cluster in their original locations and individuals at the
edges tend to become the dominating variety. Referring to figure 29 ,
there are two large distinct regions:

-   The turquoise lower left–hand side

    The colours in the lower left–hand side corner of the initial state
    are more often of the turquoise hue that will prove to be dominant.
    There are some agents on the edge, at @xmath , @xmath and @xmath ,
    that are already of colour that will prove to be dominant in that
    corner. Note that @xmath and @xmath , already closely related to the
    dominant turquoise, have joined the red variety.

-   The garnet upper right–hand side

    This area appears to have been constructed in response to the
    turquoise area. There are no explicitly garnet individuals in the
    initial populations, the final colour appears to be a blend of red
    and the light puce coloured individuals. Notice that the individuals
    in the upper corners are unchanged throughout the evolution.

###### System Activity

Referring to the four epochs that were characterised for egoistic
behaviour, there are some differences for PeerPossible :

1.  Anarchy

    The anarchic period appears to last about twice as long as it does
    under egoistic behaviour, as one might expect, because the level of
    activity is about half. When both entropies fall to @xmath the
    collectivistic epoch commences. It also exhibits the dip associated
    with traits being annihilated at the edges.

2.  Collectivism

    This is markedly different from egoistic behaviour. The entropy
    falls throughout the collectivistic epoch — meaning that the system
    is organising itself faster. Other than that, it behaves in a
    similar manner: the difference between the variety and compatibility
    entropy reduces and the latter leads the former.

3.  Oligarchy

    Under egoistic behaviour, this epoch is marked by a fall in entropy,
    and an increasing difference between variety and compatibility
    entropies which leads to a phase lead becoming a lag. Under
    PeerPossible behaviour only the phase change is noticeable, because
    the entropy has fallen to critical during the period of
    collectivism.

4.  Authoritarianism

    The authoritarian epoch is the same under both egoistic and
    PeerPossible behaviour.

Generally PeerPossible behaviour has a level of activity that is @xmath
lower than egoistic but takes about twice as long to stabilise. The
latter is commensurate with the requirement under PeerPossible behaviour
that an individual must gain a corroborating neighbour — suggesting that
two agents, probabilistically, take twice as long to agree as one — but
the level of activity is not half of what it was under egoistic
behaviour. This would suggest that PeerPossible behaviour is more
efficient — in that, the interactions between agents are not as often
undone.

###### Limit Cycles

PeerPossible behaviour, as predicted, does suffer more from limit
cycles. In a set of 32 simulations only 11 reached stasis. It would seem
that the limit cycle is the preferred global behaviour for local
PeerPossible behaviour. A good example of a limit cycle’s activity
appears in figure 32 . The state of the agents appears in figure 31 .

Although the system cannot reach stasis, which is arrived at when the
number of varieties is constant for 25 periods, the system has been more
or less stable since period 200, which is quite typical of PeerPossible
systems. Looking at the state of the agents, one variety has dominated
the others and, because of the lack of variation of the variety entropy,
has done so for some time.

One could be fairly confident in saying that when the variety entropy
has fallen below @xmath and the compatibility entropy is less than the
variety entropy, a system is probably stable, in that the dominant
variety will remain so.

It appears to be very rare for a system in a limit cycle with a dominant
variety to further evolve so that variety is no longer dominant.

##### 34.5 Some Conclusions

PeerPossible behaviour does seem to lead to more predictable systems
which, more often than not, avoids an authoritarian terminal state and
that trait accretions are less frequently reversed later.

#### 35 Predictability of Large–Scale Behaviour

The simulation model has been used to determine emergent properties of
large–scale behaviour given different small–scale behaviours. In § 33.3
it was seen that larger playing fields led to more homogeneous cultures.
The simulations used to demonstrate this property were highly
stochastic: individuals were given random traits, they were then
randomly located and then in each period randomly chosen to interact
with one another.

It is also hoped that this analysis of the evolution of cultures can
give us some assurance that, were a system allowed to organise itself,
it would consistently arrive at more or less the same set of cultures if
the individuals within it start with the same traits and behave in the
same way. This would mean that a safe access control management system
would arrive at very much the same allocations of access rights if the
individuals start with the same interests.

In the context of the simulation model developed above, some assurance
must be gained that the final state of the system is statistically
independent of the location of the individuals and when they are chosen
to interact with one another.

Before describing the experiments that were conducted to provide this
assurance, some insight will be gained from the analytic research that
has been conducted in this field.

##### 35.1 Analytic Research

Essentially, some guidance is needed on how to construct a simulation
model whose final state will be almost wholly dependent on the initial
states of the individuals: their location with respect to one another
and the sequence they interact with one another is not important.

###### 35.1.1 Voter Models and Initial Distributions

Axelrod pointed out in his paper [ AAEC96 ] , that the simulation model
is a variant of the voter model in which a particle aligns itself with
its neighbours based on whether they hold the same value or not.

###### Consonant voting

Bramson and Griffeath, [ BG80 ] , have made a comprehensive analysis of
voter models having only one trait. They quote results showing that
voters in one– or two–dimensional space tend to converge weakly to a
majority, either for or against, which is dependent only on the initial
ratio of voters for and against. The voter model they analysed was a
consonant voter model meaning that a voter aligned himself to be the
same as his neighbours.

Bramson and Griffeath’s main interest was to establish conditions under
which the process would be ergodic, i.e. under what conditions limit
cycles would not occur. They showed, analytically, that in
one–dimensional systems the consonant voting model for one trait
individuals was ergodic, but for more than two or more dimensions, i.e.
four neighbours or more, it was not ergodic.

They also showed that even though two– or more dimensional systems might
not be ergodic, the ratio law still applied. In that, the probability of
the system attaining a state where the ratio of for and against voters
was reversed from the initial state of the system was ergodic: under a
consonant voting model, the same simple majority will be maintained.
There were a number of provisos to this. If the ratio was close to
@xmath there was as a possibility of short excursions when the majority
would be reversed, but not indefinitely.

###### Dissonant voting

Bramson and Griffeath’s paper also analysed dissonant voting models and
discovered that they were unable to ascertain whether the ergodicity
theorems they had developed could be shown or not. Their analytical
technique was lacking because dissonance introduced cumulatively larger
probabilities of dissimilarity. It appears that, under dissonant
behaviour, chaotic behaviour can develop which can lead to very long
limit cycles which may not hold an initial majority in place. This can
be demonstrated with reference to another of Axelrod’s behavioural
investigations.

The basis of Axelrod’s simulation is a simple behavioural interaction,
which is best expressed in modal logic. The modal logic expression is a
useful formalisation, but it has proved difficult to extend it to
describe the dynamics of interacting systems. Axelrod’s cultural model
was preceded by, and is, in some ways, an extension of, the Iterated
Prisoners’ Dilemma — IPD, see [ Stu97 ] for example. Each prisoner has
one neighbour, so it a very simple model under Bramson and Griffeath’s
analysis, but the dynamics can be very complex. An analysis of the
simple interaction underlying the IPD was carried out by Mar [ MD94 ] .
He showed that this could lead to a system which possessed chaotic
self–similar behaviour if one of the prisoners acted consistently
dissonantly .

Unfortunately, consonance and dissonance become non–bivalent concepts
when more than one trait is involved and the properties of the distance
metric and the behavioural rule that uses it become important.

Because of this property, it is not possible to make any useful
predictions about large–scale behaviour in dissonant systems. Axelrod’s
cultural model can move from a disordered to an ordered state with
predictable large–scale behaviour, but it cannot move from an ordered
state to a more disordered one and remain predictable.

###### The relative consonance of PeerPossible

Referring to the behavioural models that have been investigated:
PeerPossible behaviour, it was noted in § 34.5 , gives rise to fewer
trait reversals than Egoistic . This would suggest that PeerPossible is
a more consonant rule.

###### 35.1.2 Topologies and Initial Distributions

Bramson and Griffeath’s one trait voter models were immune to changes in
topology. There was no difference in large–scale behaviour if the voters
were laid out on squares, circles, cylinders, toroids or spheres. When
the voters have more than one trait, superposition effects occur which
make topology important. An analysis of the effects of topology leads to
a concept called meta–behaviour and suggests topologies that will be
more predictable.

###### Meta–behaviour and Topology

As pointed out in the discussion of the PeerPossible behaviour, agents
located at the corners are more intransigent than those on the edges who
are more intransigent than those in the centre because corner agents
have only two neighbours, edge agents three and inner agents four.

It may be that the limiting distribution of identities is towards their
meta–behaviour determined by their intransigence which is, in turn,
determined by the number of neighbours they have.

###### Squares

This would mean that for a square topology, there would be three types
of meta–behaviour. The corner agents separate the groups of edge agents
from one another and vice versa ; this would suggest a mean of nine
varieties would evolve: four different corner varieties, four different
edge varieties and one variety for the inner agents. The inner agents
would outnumber the corner and edge agents when, for a square having
sides of length @xmath : @xmath , i.e. @xmath and the number of agents
is @xmath .

The inner agents would align themselves to have one variety and would
then separate the other types of agent from one another preventing them
from coalescing.

A useful analogy to a political system might help here: the United
States of America has more than 49 states and has a relatively stable
political spectrum. Changes to the constitution of the United States
must be ratified by 66% of the legislative assemblies of its constituent
states. This closely approximates to the allocation of behaviours in the
behavioural model.

###### Circles

The number of meta–behaviours can only be changed by using a different
topology.

A circular topology could be constructed as a coiled helix — like a
string of beads. The two end–agents would have two neighbours. The edge
agents would form one outer circle and the inner agents would be all the
agents within that circle: giving rise to three meta–behaviours. There
would then be four varieties: two types of end–agent, one type of edge
agent (they are now connected) and one type of inner agent.

The two end–agents could then be connected to one another to give one
agent with three neighbours, i.e. another edge agent. There would then
be only two meta–behaviours. The number of inner agents would exceed the
number of edge agents when @xmath , i.e. @xmath or the total number of
agents is greater than @xmath . This topology has been called the Möbian
circle by Axelrod in [ Axe97 ] .

The inner agents would align themselves to have one variety and would
then be able to dominate the other group of edge agents. This circle is
a more responsive topology than the square, because the edge agents
would be in the majority given one defection by an inner agent. That is,
it reduces to a simple majority voting model.

The Axelrod cultural model can thus reduce a random selection of
behaviours to a choice between two aggregated behaviours:

-   A conservative policy held by agents at the edge

-   A broad consensus policy held in the centre

Axelrod conducted a number of simulations to determine if this was the
emergent property for circular topologies and the result was more or
less in the affirmative, see [ Axe97 ] .

##### 35.2 Experimental Investigation

From the analysis above, it would appear that this model would lead to
more predictable large–scale behaviour:

-    PeerPossible small–scale behaviour

-   Möbian circle

-   More than 49 agents

The following experimental procedure was carried out in addition to
Axelrod’s experiments: generate one set of agents and place them
randomly on a Möbian circle and allow the system to interact. When
stasis was reached, the final set of varieties of agent was recorded and
the experiment repeated with another random allocation of the same
agents.

###### Regions

These results agree closely with those of Axelrod’s for the Möbian
circle where the simulations produced just two varieties for 70% of the
simulations and these simulations reached an authoritarian state. 20% of
simulations resulted in either, three varieties which were all mutually
incompatible and reached stasis, or, three varieties which remained
compatible but the system did not reach stasis. When three varieties
emerged it was invariably the case that two large varieties were
incompatible and separated by a small buffer region occupied by the
third small variety. This buffer region invariably contained the two
agents that linked the outer edge with the inner core.

The remaining 10% of the simulations seemed to be a variant of the
buffer region where there were two varieties in the buffer zone, which
were incompatible with one of its neighbouring zones.

No simulation resulted in more than four varieties. Clearly, this is
very consistent large–scale behaviour.

###### Varieties

The simulations proved to be less decisive with regard to final
varieties. The initial population of agents was seeded using a binomial
distribution of twelve traits for half of their features and a uniform
distribution of twelve for the remainder. The binomial distribution was
a throw of two six--sided dice ²³ ²³ 23 It was decided to operate in
base twelve, because twelve has more divisors than ten which helped to
simplify the calculations. . The conditions for the simulation were
these:

-   144 agents were laid out in a Möbian circle

-    PeerPossible behaviour for the agents

-   There were twelve features, each having twelve traits

-   @xmath of the agents had their lower six features assigned traits
    using the binomial and the upper six features using the uniform
    distribution.

-   The remaining @xmath had their upper six features generated using
    the binomial and the lower six generated using the uniform
    distribution.

It was seen that only when @xmath , were the final varieties noticeably
similar across simulations with different initial distributions of the
same agents. When @xmath , one or more arbitrary traits from the smaller
group could establish themselves in the larger group. A similarity
between the varieties remained which did indicate a consensus had
emerged.

It should be noted that @xmath is a statistically significant number. It
is one standard deviation of the normal distribution and the sum of a
large number of binomial distributions approximates to the normal.

This agrees with the Bramson and Griffeath’s analysis: that the
varieties that emerged from the larger group were consistently in the
majority.

##### 35.3 Sequences of Interaction

It appears then that with a suitable choice of topology and an intrinsic
bias in the population a consistent consensus can be achieved. It was
decided that an investigation into the effects of the sequence of
interaction between agents was unnecessary. This may not be so easily
dismissed in a real culture where the choice of agents who may interact
with others may be biased towards particular individuals. This is worth
further investigation, but, for the time being, it is assumed that the
agents chosen to interact with one another can be safely assumed to be
uniformly random.

##### 35.4 Collective Choice

Referring to the discussion of the theory of collective choice § 26.1 ,
the Möbian circle topology has the attractive property of being able to
reduce collections of issues to just two and thus appears to reduce
choice systems to one of simple majority and thereby circumvents the
limitations imposed by Arrow’s Impossibility Theorem , as summarised in
[ Eav99a ] .

Referring again to the political structure of the United States, it
would appear to display the characteristics of this circular topology.
There are just two dominant political ideologies and the states
neighbour each other in different circular topologies on the different
political issues presented to them. The net effect is a superposition of
pairs of different behaviours all of which can be encompassed by the two
political parties’ platforms. This construction bears a great deal of
similarity to the dynamic analysis of the Tiebout model by Kollman et
al. [ KMP95 ] .

#### 36 Summary

###### Protocol adoption

This chapter has shown that large systems where individual agents make
choices constrained by a simple, rational behaviour can lead to stable
behaviour for the system as a whole. This result, it is claimed [ Axe95
] , helps to explain the emergence of de facto standards. Axelrod’s
analysis was prompted by the evolution of different Unix standards, but
it might be applied to different Internet protocols. For example, 90% of
Internet traffic is carried over TCP connections rather than in UDP
datagrams. TCP has very useful technical advantages over UDP: it, unlike
UDP, is rate–adaptive, does not require an application programmer to
fragment his own data, transparently recovers from IP packet loss and
the arrival of IP packets out of sequence — in summary, it provides a
relatively simply session layer. UDP is however to easier to manage: it
requires no connection management — simply one listening endpoint from
each party for each connection — it is therefore easily adaptable to
multi–cast protocols and it is easier to define rules for screening
firewall routers. Had it been the case that a sufficiently capable
session layer were available to application programmers early in the
development of the Internet, UDP might have become the de facto standard
for IP communications rather than TCP. Similar arguments can be made for
the domination of other protocols: the Sun Micro-systems RPC protocols
based on the portmapper, could have been supplanted by the Domain Naming
Service based Hesiod protocol from MIT.

###### Protocol adoption and behaviour

The number of incompatible standards (meta–behaviours) that can emerge
is a function of topology; how long the system takes to arrive at a
stable set of standards is a function of the choices — features and
traits — available. The behaviour that each agent employs when making
choices controls the rate of migration of the traits and the number of
conflicts over their selection: Egoistic behaviour allows traits to
migrate quickly but introduces proportionally more conflicts to resolve,
PeerPossible behaviour the converse.

It has also been argued that PeerPossible should be a more consonant
voting behaviour than Egoistic and, when coupled with a Möbian topology
allows very homogeneous cultures to evolve. This cultural system also
has the attractive property that it does not remove any intrinsic bias
in population.

###### Engineering behaviours

A point that has not been addressed is how behaviours like Egoistic and
PeerPossible would be engineered so that they may be used to simplify
policy choices in working systems. A simpler example than access control
policy evolution might be IP address allocation. Looking at figure 31 ,
it could be the case that a hundred small separate networks at @xmath
have interacted with one another and joined each other’s networks to
yield a few larger networks by @xmath . The criteria for the features
they might employ would include some of the following:

-   Connections to different types of carrier network: some networks may
    consist almost entirely of single–homed 100BaseTX on the same
    subnetwork; some may have a number of dual–homed routers with access
    to ATM or SDH leased lines linking to other subnetworks.

-   Different protocols used for communication: some networks may make
    extensive use of multi–cast, point–to–point IP routes, or Generic
    Router Encapsulation (GRE) tunnels.

-   Traffic types: some networks may be simply web–browsers; some may
    use remote file-store.

-   Screening subnets: it may be the case that some networks must not be
    visible to one another.

There could be a very large set of traits for each of these and others.

For an operational protocol, one must consider resource–locking. Each
individual would operate autonomously from every other, but would need
to acquire locks on their neighbours when they are about to make the
decision to change their configuration. This is quite a difficult
lock–acquiring exercise since dead– and livelock are distinct
possibilities.

###### Population statistics

The key point about the behaviours Egoistic and PeerPossible is that
they operate locally and can, consequently, adapt very quickly to their
neighbours. The entropy measures that have been introduced are
population measures and, in a working system, would be expensive to
compute. They are, as has been seen above, a very useful guide to the
operational state of the system — whether anarchic, collectivistic,
oligarchic or authoritarian. A system administrator could use the
entropy measures to determine if a system has simplified itself enough
to be allowed to continue to fulfil its chosen function: more
efficiently and with less conflicts than before it organised itself. To
obtain an accurate statistic, it may be necessary for the administrator
to quell all interactions and request the status of all the individuals.

###### Migration of access rights

The results obtained in this chapter give us confidence that a
self–organising system that allows access rights to be migrated from one
individual to another should be predictable. Some of the metrics
developed could be used to monitor the migration.

The cultural model gives us a reference model of behaviour. When one
designs a system, one can attempt to reduce its operation to that of the
simulation described in this chapter. This then gives us some
expectations for its behaviour.

### Chapter \thechapter Self–Organising Permissions Policy System

The findings of chapters III together with the small–scale behaviours
investigated in chapter III can be used as the basis for a
self–organising permissions policy system. In chapter III , integrity
checks and distance measures for preference hierarchies were introduced.
In chapter III , a cultural model illustrated how a system of
interacting agents with a fixed set of choices using a simple
behavioural rule based on a distance measure would have reasonably
predictable large–scale behaviour so that agents within cultures should
segregate themselves into large groups.

To assure ourselves that a system will behave like the cultures
described in III , it must have the same construction:

-   A small–scale behavioural rule

-   A fixed set of discernible features with traits

-   A distance measure for two feature sets

-   A fixed topology, preferably in two dimensions

Before discussing how a self–organising permissions system might work, a
simpler example of a self–organising set of newsgroups will be
developed.

#### 37 Self–Organising Newsgroups

One application of this system as proposed would be a set of
self–organising newsgroups or mailing lists. The problem with USENET
newsgroups is they have a very low “signal to noise” ratio. There are
lots of postings of dubious worth, some nothing more than
advertisements. Very often the quality of debate degenerates to a
squabble. Often cliques of users develop threads of discussion which are
of no interest to anyone else. Cross–posting is another problem:
subscribers send the same message to a number of newsgroups
simultaneously.

Newsgroups could be self–organising: so that squabblers will be moved to
their own groups as would persistent advertisers. Cross–posting will be
limited by using managers who may choose to refer a posting to another
group rather than have it posted in their own.

There are three types of newsgroup management in place for the USENET.

  Moderated  

    every posting to the newsgroup is vetted by a moderator.

  Managed  

    subscribers are only allowed to post if they have applied for
    permission to do so from the newsgroup manager. The newsgroup
    software then checks if each posting comes from an allowed
    subscriber.

  Unmanaged  

    Access is completely open.

Moderation is usually too burdensome. Managed newsgroups are quite rare
(but common for mailing lists), so the usual form is an unmanaged
newsgroup. The system proposed will be a sophisticated managed
newsgroup.

To put newsgroup postings under some sort of control, a preference
hierarchy needs to be developed from some norms. This would then be used
to partition the subscribers to the newsgroup into sub–groups and to
rank the newsgroups with respect to one another. This section will
continue with a description of how such a self–managing newsgroup
protocol would work and an analysis of it as a cultural model like that
in chapter III .

##### 37.1 Newsgroup Operation

###### 37.1.1 How it might work

Newsgroups are organised by a set of news administrators who require a
group of people to issue a charter and have a number of newsgroup
subscribers sign it before a group is propagated within the USENET
hierarchy. Discussions around an operating system, such as Linux , in
the comp.os hierarchy has been split into these groups:

  comp.os.linux.advocacy
  comp.os.linux.announce
  comp.os.linux.hardware
  comp.os.linux.software
  comp.os.linux.setup
  comp.os.linux.networking
  etc.

Within the comp.os.linux.advocacy newsgroup there will be a number of
postings comparing Linux with the Microsoft Windows operating system.
The Linux vs. Windows debate is an interest which all of the
comp.os.linux newsgroups share, but only comp.os.linux.advocacy would
usually discuss it, but a debate comparing Windows network interface
card driver support vis à vis that of Linux would be of interest to the
comp.os.linux.networking newsgroup.

A self–organising set of newsgroups for comp.os.linux should finally
evolve a structure similar to that above, but it would use who takes
part in which debates to evolve the structure, rather than having one
imposed on it.

The operation of a self–organising newsgroup has only a small amount of
information to use: postings from subscribers to particular threads.
There are four procedures involved in self–management:

1.  Initial group allocation

2.  Posting using subscriber referral and access management

3.  Generating group allocations

4.  Posting using group referral and access management

5.  Repeat from 3

###### Protocol

A protocol of some kind has to be imposed on the newsgroup to yield more
information from the postings.

-   When a subscriber initiates a thread, it must be followed up for it
    to considered.

-   A subscriber cannot follow up himself.

-   A follow–up must be followed-up to be considered.

-   A follow–up is closed if it is acknowledged by the subscriber who
    initiated the thread.

This set of rules makes for civilised debate, a simple example of which
might be:

1.  @xmath initiates thread @xmath

2.  @xmath follows up @xmath

3.  @xmath acknowledges @xmath

@xmath ’s initiation of the thread @xmath counts as a statement of his
preferences because @xmath followed it up. @xmath ’s follow up is
counted as a preference, because @xmath acknowledged it. A more
complicated scenario might be:

1.  @xmath initiates thread @xmath

2.  @xmath follows up @xmath , creating @xmath

3.  @xmath follows up @xmath

4.  @xmath or @xmath acknowledges @xmath on @xmath

5.  @xmath acknowledges @xmath on @xmath

Probably, the simplest discipline is to allow the initiator to
acknowledge all contributors, except those he considers irrelevant. If
any of the contributors follows a thread that the initiator feels is
irrelevant then that subscriber who followed up the irrelevant thread
can acknowledge it and form a new thread. So, in the interaction above,
if @xmath does not acknowledge @xmath , then @xmath must acknowledge his
contribution and their contributions count to the sub–thread @xmath .

###### 37.1.2 Initial Grouping

This is the first phase of generating a preference hierarchy. It is used
to derive a partitioning of the subscribers to the newsgroup. As each
subscriber posts to a thread, it generates a preference. If, for
example, there are three threads: @xmath and an apathy thread is added
to this so that all the threads can be related, call it @xmath , then if
a subscriber posts one or more times to threads @xmath and @xmath and
not to thread @xmath then the preference ordering is @xmath .

###### Newsgroup partitioning by thread interest

A preference hierarchy for each of the subscribers would be generated.
It would then be aggregated using the techniques described in § 28.3 .
The preference hierarchies would be just two–ply — those subscribed to
and those not — but it might be the case that global indifferences
emerge from the @xmath –graphs. For example, everyone who posts to
@xmath will always post to @xmath . Thread @xmath would then be merged
with @xmath and would form a new sub–newsgroup.

In the aggregation there would almost certainly be cycles — a cycle such
as @xmath , @xmath and @xmath could arise. These would have to be
removed by merging threads or by removing subscribers who introduce
cycles. Using some set nomenclature, @xmath is a statement of set
membership for subscribers @xmath who have ordered the preferences in
the specified way. For the example of three initial threads, all of the
possible new sub–newsgroups that could emerge are given below:

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

There are only 7 rather than 13 groups that table 11 would suggest. This
is because the simplification discussed in § 33.4 has been used. These 7
groups represent all the valid shades of opinion there might be. This
has been achieved in the newsgroup system by only allowing two–ply
preferences for the subscribers.

In more colloquial terms the subscribers have been partitioned into “one
interest only”, “two–interests” and “interested in all” groups.

###### The Entry Group

This group is used by all groups to post new threads and by people who
wish to join the newsgroup to see what threads are being discussed. A
thread that is never followed up would stay in the entry group, when
there is a follow–up, it would move to one of the sub–newsgroups. The
group is called the entry group.

Every subscriber is a member of the entry group. Every subscriber may
initiate a thread in the entry group.

###### Ordering Subscribers and Appointing Managers

For each sub–newsgroup generate an ordering of the subscribers based on
the number of times they have posted to the threads of their
sub–newsgroup. Form the most active members into a collective that acts
as the sub–newsgroup’s access managers — it might be the top 5% of
subscribers for each group, for example. (This, incidentally, is one of
the problems of this system’s design: a sophisticated group of
subscribers can make themselves managers of groups by answering each
other’s threads. This is similar to tactical voting. It will be seen
that an entropy measure can be used to determine how much support
managers have.)

###### 37.1.3 New postings

When an existing subscriber initiates a new thread, it would be posted
to his sub–newsgroup and also to the entry group. Followups to the new
thread would appear only in the sub–newsgroup. If a subscriber in
another sub–newsgroup wants to follow a new thread in a sub–newsgroup,
he would be able to review the postings for that new thread but not make
any.

If he did want to make a posting to that thread, his mail would be sent
to the access managers. They would then decide whether to permit the
posting and would thus grant to the new subscriber membership of their
sub–newsgroup. He would then be allowed to initiate a thread in that
sub–newsgroup as well as submit postings.

###### 37.1.4 Group Preference Hierarchy

After a number of new threads have been started, the subscribers can be
re–partitioned and, additionally, a hierarchy of the groups can be
generated. This hierarchy expresses a norm of behaviour between the
groups.

###### Norm of behaviour

It is best to explain this by example: if a member of sub–newsgroup
@xmath , the one containing all those who chose @xmath , chooses, via
the entry group, to join group @xmath for two threads, @xmath and only
takes part in one thread in his own group @xmath then he has the
following preference orderings:

-   An ordering between threads as before: @xmath

-   An ordering between sub–newsgroups: @xmath .

The latter preference is formed because the subscriber has posted twice
to threads originating in group @xmath and only once in @xmath . The
ordering between threads will be only two–ply, as before, but that
between newsgroups can be as long a chain as there are newsgroups.

The ordering between threads represents a subscriber’s current interests
and the ordering between newsgroups relates his current interests to his
past interests. (The relative strengths of his interests are not used in
forming his preference hierarchy, these are used in the aggregation
across the newsgroups.)

This forms a norm of behaviour. One would expect a subscriber in group
@xmath to post subsequently solely to that group, but if he posts to
@xmath as well, this would suggest he should join @xmath .

For all the members of a particular newsgroup, the ordering between
sub–newsgroups is aggregated using the maximum likelihood preference
ordering procedure described in § 30.3 . This yields an ordering across
the newsgroups that is specific to each newsgroup. The newsgroup
effectively decides who its neighbours are. If one considers a three
thread system, the arrangement of the newsgroups will develop from that
given in figure 33 .

###### 37.1.5 New postings with a group hierarchy

When a subscriber posts a new thread in his own group, it will be sent
to the access managers of the groups adjacent to his own group in the
preference hierarchy of groups. They can then choose to accept it or
not. If they do, then the interested members in the neighbouring groups
will post to the new thread and make their group more similar to the
thread initiator’s group.

##### 37.2 Summary

The preference hierarchy generated decides the order in which the groups
refer to one another in figure 33 . There is an order imposed: when a
subscriber posts to both @xmath and @xmath he indicates that he would
prefer to join a group @xmath .

The access managers are the most frequent message posters to their own
groups and they are responsible for vetting the subscribers allowed to
join a debate and thus a newsgroup. In this way, very active subscribers
to a particular set of threads will find themselves acting as access
managers for groups that discuss mostly their sort of interests.

The groups derived from the first set of postings are not tied to
discussing debates concerning the subject or subjects they first
expressed an interest in. Each group will be constantly redefining
itself: both in the subscribers it has and the issues it discusses.

The use of automatic referral between newsgroups adjacent in the
hierarchy makes access management easier and allows for the migration of
traits. Using the entry group to initiate a thread will be relatively
rare.

This system is self–organising in a rather subtle way. The access
managers are representative of the subscribers to the group. In the
example of a permissions’ policy management system, it will be seen that
this authority by which subscribers are allowed to join a group can be
determined from another preference hierarchy.

##### 37.3 The Cultural Model and Stability

Now the self–organising newsgroup system has been defined, a structural
iso–morphism has to be made to ascertain which entity in the newsgroup
system fulfils which function in the cultural model.

When that is done it will be possible to make some predictions about the
behaviour of the system using the analysis of the dynamics of the
cultural model.

###### The Cultural Model

Referring to Axelrod’s model in § 33.2 and figure 33 , the simulation
model has only three entities — agents, features and traits. An agent
has a set of features, each feature can take any one of a number of
different trait values. (Each trait is therefore an object of a
particular feature class.) The set of traits for the different features
is the identity of the agent.

  -- -------- --
     @xmath   
  -- -------- --

These are equated with entities in the newsgroup system in the following
way.

  -- -------- --
     @xmath   
  -- -------- --

The agents would be the newsgroups referred to in ( 17 ) not the
subscribers. They are located with respect to one another given by
figure 33 .

###### Interests as meta–threads as features

One might ask what is an interest in the context of the newsgroups. An
interest within a newsgroup manifests itself as a thread of discussion
and a thread of discussion is nothing more than a set of related
postings — they are related by their common interest. All the newsgroups
do have the same set of interests, whether they foster any interest in
any particular subset is what the postings determine.

###### Distance Function

The access managers of the group and the existing subscribers determine
which postings to accept within a newsgroup. The access managers accept
new threads, the existing subscribers choose whether to follow them up.
Together they act as the distance function does in Axelrod’s model ( 7
). The mechanics of the distance function are very different: the access
managers and the existing members vote and the maximum–likelihood
preference ordering is used to determine which of its neighbours each
newsgroup is compatible with.

###### Topology

There are a number of possible topologies for the system. It can be
either as in figure 33 , which for three interests gives each one three
neighbours and the central point six. This could be simplified to that
given in figure 34 .

This latter topology implies that subscribers to the single interest
group only become interested in all three issues after they have become
interested in two of them. The number of neighbours is two, three and
three for the single–interest, two–interest and every–interest groups.
It is a more appealing topology because it gives each agent fewer
neighbours and can easily be formed by using a binary tree.

###### Dynamics

The system described in figures 33 and 34 is not particularly complex.
Following the argument given in § 33.4 , if one only allows seven
different opinions to be held regarding the preferences across three
meta-interests then one need only set @xmath for the meta–groups @xmath
and have a trait set of @xmath . The playing field can have only 7
different newsgroups, using ( 14 ). This would mean that every
subscriber would be allocated to one primary group which could be either
a single–, dual– or every interest one.

This is a fairly simply model and it does usually reduce to just two or
three varieties regardless of which of the two topologies is used. The
more connected graph, figure 33 , usually reaches stasis faster. It
hardly ever develops a limit cycle. Typical patterns are fairly
predictable, whichever variety starting at @xmath asserts itself in the
middle point @xmath usually asserts itself over at least one other
interest, so, for example, if the variety starting at @xmath reaches
@xmath first, it might arise that @xmath takes on the same variety so
that, effectively, @xmath .

###### More typical systems

Usually newsgroups will have more than three interests, looking at the
comp.os.linux hierarchy, the newsgroup administrators were expecting to
keep debate focused on about six broad subjects. This is a more complex
system, but easily derived using the spanning tree construction of §
33.4 (which was used for figure 34 ). The size of the playing field
would be 62: @xmath . These 62 newsgroups represent all the different
shades of opinion there are allowed to be.

With a feature set of @xmath , we might say that subscribers have @xmath
interests (they are hoping to pair the six meta–threads to form the
simpler spanning tree), this would give them a trait set of @xmath ,
from table 11 , if we ask them to state their preferences on the three
pairs they have chosen using a weak ordering. This would mean that when
re–partitioning the subscribers, the preference ordering would be a
chain that is three–ply in length: i.e. @xmath .

This then gives quite similar parameters to the Axelrod cultural model
analysed in chapter III and the dynamic behaviour can be expected to be
the same.

###### A PeerPossible System

Peer-Possible behaviour can be implemented in effect in the newsgroup
system. As each subscriber makes a posting, if he is not already a
member of the group with the thread, it must be shown that he is a
member of a neighbouring group to the one he wishes to make the posting
in.

This makes the access managers’ job easier and should lend to the system
some of the properties that PeerPossible behaviour was deduced to
possess in the cultural model.

#### 38 Self–Organising Permissions Policy System

It now remains to apply what has been learnt from the newsgroups example
to a system to simplify preference hierarchies for access control
systems to produce a self–organising permissions policy system for
healthcare.

##### 38.1 Newsgroups: Summarised

The newsgroups example had the following similarities to the cultural
model.

-   The subscribers were not the agents that interacted in the cultural
    model, but rather the groups that they belonged to.

-   The individual subscribers acted as traits for a feature. A group
    collected members which formed its identity.

##### 38.2 Rôle–based Access Control System

Little mention was made in the newgroups example of the target
technology for the access control system to the newgroups. In this
discussion of the self–organising permissions policy system a rôle–based
access control system will be the target technology. This is because it
can be adapted to suit all types of access–control system in current use
and its information model, § 5 , has the key entities needed to manage
the system: the rôles and constraints objects.

##### 38.3 Healthcare

Within the healthcare sector, the collegiate organisational model
described by Anderson [ And96b ] applies. Within each college — that is,
every clinic or surgery — the security rights are well–defined, but they
will almost certainly not be consistent across the colleges.

###### An example: receptionists in different practices

The rights given to receptionist in one general practice may only
entitle them to view the records of patients who are visiting their
general practitioner on that day — such a system might be wholly
electronic with access policies defined within a relational database
that holds all patient records; another surgery may allow their
receptionists to see all records of all patients at all times — a
paper–based system where the receptionist is given a key to the records
room.

The difficulty here is that within their own colleges, the receptionists
have been assigned the same nominal rôle, but the rôle has different
rights and privileges within each college.

It would probably be the case that the surgery that has the paper–based
system needs more trustworthy receptionists because they would have
access to so much more information. Consequently, the qualifications and
experience of the receptionists would need to be qualitatively better
than those of receptionists working in the surgery that has a better
protected electronic system. Because these better qualified
receptionists are considered more trustworthy, they might be given more
rights and privileges in other colleges.

###### 38.3.1 Simplification of the Rôle Space

A system administrator, if he were to define this rôle across the
practices ²⁴ ²⁴ 24 Anderson makes it clear that this scenario is not one
that would arise: the receptionist rôle would be constrained to allow
only access to records in the receptionist’s own practice. This scenario
is though, in miniature, the problem that would be faced in allocating
access rights across all parts of the healthcare sector. , would need to
know what the rights and privileges associated with the rôles are and
the quality of the people assigned the rôle before he can determine who
should be given the federal rôle. The system administrator would find it
expedient to divide the rôle of receptionist into a number of other
sub–rôles. Some rôles would require that an individual assigned to the
rôle must meet certain requirements that would give one more confidence
that the individuals will be trustworthy.

A rôle, as a data structure, is probably best represented as an ordered
pair @xmath — the rôle of the initiator and the rôle of the acceptor.
The total number of rôles in the federation is @xmath , where @xmath is
the number of rôles in one college of the @xmath there are. The total
number of federal rôle pairs will be less than @xmath . The hope is that
by simplifying them and eliminating the irrelevant alternatives, it may
be possible to reduce them to a more manageable number.

###### 38.3.2 Rôles and Interactions

When one analyses the operation of the newsgroup system, one sees that
each posting to a newsgroup by a subscriber is an interaction between
newgroups: the newsgroup that the subscriber belongs to and the
newsgroup the subscriber posts to.

Within healthcare, interactions are between individuals in one of their
defined rôles: when a general practitioner refers a patient to a
consultant, he is known as the referring physician and the consultant is
the consulting physician ; within a hospital a doctor present at the
treatment of a patient by another doctor is fulfilling the rôle of
attending physician . There are a number of other such rôles played by
physicians within hospitals; these are currently being codified for the
proposed HL7 standard [ Unk01b ] . HL7 also defines rôles that are not
medical: administrative, auditing and clerical rôles are also defined.

In addition to the rôles defined within HL7, there are other rôles that
do not fall into its remit, but one would expect them to be defined
within the federal system: in particular, medical researcher and
comptroller.

With regard to the referral process: an interaction takes place when a
general practitioner refers a patient to a consultant, whether his
referral is accepted or rejected, an interaction has taken place.
Similarly, when a medical researcher requests access to a set of records
in a database, this constitutes an interaction.

###### Distance Measure

The HL7 rôles are already well–defined, they would in fact form part of
the distance measure between rôles and would be axiomatic pre–conditions
for an interaction.

###### 38.3.3 Features, Traits and a Data Structure

Figure 35 illustrates a proposed data structure for a patient record.
Its design is intended to illustrate an instance of one of the four
broad classifications of access that were introduced in in § 1 .

They are constituted in the following way using three branches of the
patient record tree: naming , history and status . These correspond to
synonymous, anonymous and eponymous in the following way. Synonymous
information is any combination of facts from the naming branch of the
tree that would allow a patient to uniquely identified. The anonymous
branch, history , contains the history of the patient: these records are
all of the same type which is generically called event . The eponymous
branch, status , contains summary statements — age group, sex, diabetic,
allergies, body–mass index and so forth. It should not be possible to
identify a patient uniquely using sets of anonymous and eponymous
information. This means that the inference threats that Denning
describes in [ DSW90 ] are protected against, probably by a query system
enabled in the way Denning describes.

A fourth class of access is synonmous: this is viewed as being knowledge
of a set of anonymous facts and a set of eponymous facts, but it is also
possible to uniquely identify the patient in question. This would be
used by another physician to ask the custodian of the patient’s record
for additional information. This is typical of the referral process that
physicians employ; they initially discuss a patient under an assumed
name, passing on what they consider to be salient facts before the
consulting doctor assumes responsibility and is told the patient’s
identity.

Finally, the reason for choosing a numbered tree structure is that every
entity — be it a field or linked historical record — can be given a
unique vector relative to the id . The type of a linked record is given
a unique vector identifier; this is concatenated with the id of the
record of having the contents. The id fields in linked records would be
informative. They might specify a date, in which case it would be
possible to limit historical access to dates within a range.
Essentially, the tree structure is a linked structure of all the normal
forms ²⁵ ²⁵ 25 A record is decomposed into a composite record with links
to records in other files. Each of these should be in a normal form
which depends wholly upon each primary key for that file; all of fields
within the record are either atomic or a link to another record, see [
Wie83 ] for a discussion of normal forms and the construction of
database keys. The key would be the id field described in the text. of
the records in a database that pertain to a patient.

Between different rôles under different contexts different views of
patient records will be granted. As patient record views are defined
they can be compared with one another as trees: when one rôle grants
access to another they agree on a standard view; one of those rôles may
in a later interaction with another rôle decide that the current rôle is
entitled to the same view as the previous one. They may choose to
standardise or diverge, but a number of standard representations in
different contexts will emerge.

The definitions of anonymous, eponymous and synonymous access will
differ for each rôle. Their meanings in each context will be refined and
generic types will emerge between rôles.

The set of facts permitted for each class of access constitutes a trait
for that feature. This is the first set of features and traits defined
for the permissions policy system.

###### 38.3.4 Ontological Features and Traits

In addition to the views that are granted between rôles, the context in
which the interaction takes place must be defined: these are the
ontological features:

  Administrative  

    Some of these may be administrative: working in the same clinic, the
    same hospital, health district and so forth.

  Discipline  

    In healthcare, this would usually be derived from the medical
    clinic: pædiatric, oncological, genito–urinary and so forth. It may
    also be derived from affiliations to professional organisations.

  Procedural  

    The procedure that is being followed. Whether a referral,
    consultation, prescription, treatment and so forth. Many of these
    procedures are defined within HL7.

It may well prove expedient to introduce others.

###### 38.3.5 Inconsistencies and Distance

In more conventional database access control systems, the problem that a
security rights administrator faces when a new object is put under his
jurisdiction is whether to grant to existing groups rights to access
that object or whether to sub–divide those groups to form a new group
and grant access rights only to it. The choice is usually determined by
whether an inconsistency results: if the right granted to an existing
group gives them information that would allow them to compromise the
existing group structure then they should not be granted that right. For
example, if the right to grant read access is given away too freely,
then a number of groups might disappear because they effectively become
the same group. Or, it might be the case, that one group is denied
access to a object it cannot function without.

For the tree data structure described above, it may prove the case that
one individual in one rôle grants access to a view which would make the
system inconsistent in a similar way. This can be determined by using
the take–grant method of analysis introduced in § 12.4.2 .

If such an inconsistency would result, then this should contribute to
the distance measure and would introduce an axiomatic inferred rule to
its evaluation. (One might assign an unbroachable distance to it.)

Another contributing factor to the distance measure might be statistical
belief in the authority of an identity. This can be quantitatively
stated using the methods proposed by Maurer [ KM00 , Mau96 ] .

##### 38.4 Collective Choice Expert System

The operation of the permissions policy system is that of a collective
choice expert system ²⁶ ²⁶ 26 This is a relatively new software system,
Hubermann’s Beehive system, [ HK96 ] , is one such example. . Whenever
an interaction takes place, one rôle grants a data view to another, the
cultural model is constructed and is allowed to evolve as a statistical
experiment to determine how stable it is; a desirable outcome may be
specified and a sample of outcomes might lead one to conclude that the
proposed data view should or should not be be granted, depending on
whether it is more or less likely that a desirable consistent
configuration would be statistically likely to arise.

###### Modelling, Statistical Experiment and Comparison

The model is constructed using the current attributes of the individuals
in the healthcare business sector: their rôles and business
relationships are analysed and a set of rôle assignments is made.

The data view is imposed on the model and it is allowed to evolve under
stochastic inputs. A sample of evolutions would be used to analyse the
behaviour of the system.

The trajectory of each cultural evolution experiment would be recorded.
The parameters measured for the trajectory would be the variety and
compatibility entropies: @xmath and @xmath , § 33.3 , their rates of
change, their phase difference and the activity of the system. These
trajectories of the different samples could be compared to one another.
They would also be recorded for comparison with the real system as it
evolves.

With this knowledge, it should be possible to show that, for a large
enough group of rôles, a degree of stability could be reached and the
data views would not allow inconsistencies and that the system could be
shown to be near pareto–optimal in moving to its next operational state.

###### Final States

The cultural model statistical experiments will either degenerate and
develop an inconsistency or become static or continue to evolve in a
limit cycle. All of these outcomes can be determined from an analysis of
the cultural model’s global state and statistics of its evolution under
the stochastic input. The frequency distribution of stable to unstable
outcomes is indicative of the stability of the real–world system.

The state that would normally be considered to be most satisfactory is a
static condition, such as that shown in table 14 , where the majority
group is incompatible with all the others, their rôle is isolated, but
the other groups are able to continue to migrate views across one
another. The majority group here might be the comptroller rôle which is
unable to obtain anything more than accounting details in aggregate from
any of the other rôles.

A limit cycle condition may also be a desirable, if the dominant rôles
are policy--making ones: high--level custodians such as ethics
committees for hospitals and the professional organisations of
physicians, as well as representative bodies for groups of accessors ²⁷
²⁷ 27 It may well be the case that cultural systems having limit cycles
are exhibiting another behavioural phenomenon examined by Axelrod, the
Tribute system, [ Axe97 ] . .

###### Dynamics

Hopefully there will be a similarity in the trajectories of the cultural
model evolutions. The rate of change of the entropies and their rate of
change with respect to each other is an indicator of when the final
state will appear. These metrics may also serve as an indicator of how
complex that final state might be.

The rates of change of entropy can be measured in the permissions policy
system as it evolves; it might then be possible to show that it is
following a similar trajectory to a cultural model experiment.

That cultural experiment could then be re-run with revised starting
parameters to see how it evolves. The process of analysis and comparison
could be repeated with more statistical certainty.

###### Generating Rules

If it becomes apparent that a large rôle will be dominant and will be
isolated from the others then it could be imposed as an axiomatic data
view within the system and be enforced through the distance function.

###### Second and Subsequent Phases of Resolution

The process could then be repeated with the large isolated majority
group’s rôle eliminated from the cultural model, but its presence within
the system contained as a constraint on the distance measure. It would
then be possible to repeat the process with another series of cultural
model experiments until all of the rôles are clearly defined with
respect to one another.

##### 38.5 Comparison with Newsgroups Operation

The operation of the newsgroup system was as follows:

-   Membership of a group entitled its members to make postings to that
    group.

-   All subscribers belonged to an entry group.

-   Subscribers were assigned to the group that they contributed to most
    often.

-   Access Managers determined if a new subscriber could contribute to a
    group and therefore be a member of it.

-   An access manager was given his position within a group because he
    contributed most often to a group.

-   A finite number of interests were assumed to be expressed.

-   The length of the preference ordering across those interests
    determined how many different opinions would be allowed.

In the newsgroup example, only three interests were chosen to make
illustration easier and a preference ordering that was only two–ply
limited the number of varieties of newsgroup to 7 (of a total of 13). It
was seen that it could be easily extended to cover six interests, but
the preference ordering was limited in length to three ply, which meant,
in effect, that only 62 (of 4683) varieties of opinion were allowed.

The system was bootstrapped by allowing the subscribers to make some
initial statements of interest and they were then allocated to a group.
After that, they were moved from group to group and groups could refer
members to one another.

###### Access Rights for Rôles

Access rule generation is very similar using the collective choice
expert system to assign rôles and define the inter–rôle data views.

-   The accessors will be placed in access rôles . These rôles will be
    the interacting agents of the system. A rôle is entitled to claim
    certain views from other rôles.

-   All accessors will have an entry rôle — unprivileged enquirer.

-   Accessors will be assigned to the rôles which would give them the
    most utility, i.e. access to as much information as they can.

-   Interactions take place as custodians determine if an accessor can
    be assigned a rôle by determining what data view they may have.

-   Custodians will be appointed by the subjects of the views available
    in a given rôle — they are not chosen from the accessors.

-   A finite number of ontologies will be allowed to be expressed. The
    number of plies for the preference ordering of the rôles that
    accessors have been assigned by the custodians must also be set.

The treatment of the accessors is identical to that of subscribers in
the newsgroups example. It is the appointment of custodians which is
different. Custodians do not have to be chosen from the accessors, they
are already in place.

A very important difference in the operation of the two systems must be
made clear: in the permissions policy system being a member of an access
group does not entitle the member to use all of the views the other
members of the access group have acquired, it makes it more probable.

###### An Example of the Bootstrapping Procedure

If accessor @xmath makes an access request for a particular view of a
database then if the custodians @xmath of that database view grant the
access request, they collectively become the custodians of a rôle
created uniquely for that accessor. Call the access group @xmath meaning
custodians @xmath for @xmath .

If accessor @xmath makes another access request for a different view,
then the custodians of that data, @xmath , are also able to take into
consideration @xmath . With this, the custodians could make the
following access rule:

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

###### Re-organisation

There is however no explicit need for the custodians to make such a rule
explicit because it is already implicit in the system. When the rôles
are reorganized, rôles @xmath and @xmath would be merged, because they
have no other members, so it just an inconvenience to have them appear
distinct, but if another accessor were to be granted access to @xmath
but not to @xmath then the groups must be made distinct. This would also
imply that there is an ordering between the groups, @xmath , meaning
that membership of @xmath grants more access than @xmath .

To fully exploit the implicit rules, it must be possible for the system
to accurately compare the trustworthiness of the two accessors without
having to call upon the custodians repeatedly. In the newsgroups
example, this was achieved in a democratic way by having the access
managers of the newsgroup be chosen from the subscribers. In the
permissions policy system, the custodians are outside of the system, but
are able to compare accessors. As was explained in chapter II , each
accessor would have acquired certificates from other permission policy
systems and these could be compared. These other policy management
systems would also be able to order the rôles the accessors were members
of.

It would thus be quantitatively possible to compare the amount of trust
that has been placed in two different accessors based on the rôles that
have assigned to them.

###### Large–scale behaviour

The net effect of this procedure will be that accessors will be
classified accurately and will be expected to behave in a particular way
and they can be reasonably safely compared. This will make the
generation of explicit access rules much easier. Explicit access rules
will usually be more abstract but essentially of the same form as ( 18
). The explicit access rules would in fact be specified in the system’s
operation and would take on the character of axioms of the system.

###### Safety

It is the responsibility of the custodians to ensure that accessors do
not belong to any access groups which they should not ethically be
members of. The self–organising permissions system only simplifies the
management of access rights for accessors; it does not itself apply the
principle of least privilege. It should not contravene it though. It is
therefore important that some meta–data rules be specified which state a
preference ordering across views.

There is an implicit ordering across database views if they were
classified as in table 1 . In more concrete terms, one could say that a
view that contains an indication of a subject’s age is more confidential
than one that does not and, therefore, anyone who has access to such a
view has had more trust bestowed upon them that someone who does not.

###### Summary

It should be apparent now how different permissions policy hierarchies
inter–work to generate implicit access rules. The custodians are able to
compare accessors by using the permission policy orderings generated in
other systems. This is the procedure that underlies much of modern
business where it is often required that companies have to prove
creditworthiness to one another by using bank statements and asset
holdings. The procedures described above simply apply this principle on
less quantitative information than money.

#### 39 Conclusions

It has been seen in this chapter that self–organising systems can be
easily specified and designed and should display the useful
self–stabilising dynamics of the Axelrod cultural model of interaction.
In the following chapter, the permissions policy management system will
be discussed in relation to the other requirements given in this
dissertation.

### Chapter \thechapter Discussion, Future Work and Conclusion

In this final chapter, the systems described in the preceding chapter
will be more critically assessed against the requirements. Future work
to validate the concepts discussed in this dissertation will be outlined
and a final conclusion on its usefulness will be given.

#### 40 Discussion

It has already been made clear that the permissions policy system makes
use of other permissions policy systems preference hierarchies to be
able to compare accessors against one another. The broad goals of the
requirements chapter — the duties of custodians to their
subjects — cannot be directly met by this procedure but it does make it
possible to detect any infringements by the custodians since they too
should conform to a norm of behaviour which could be placed in a
preference hierarchy.

An outstanding question is how safe are the decisions that are made.
This hinges upon the granularity of the grouping. If there are too many
groups, then it will not be possible to establish a reliable norm of
behaviour from a preference hierarchy based upon it. If there are two
few groups, the preference hierarchy will not be able to generate enough
implicit rules and to produce a useful organisation. It is worthwhile
restating the key relationship upon which self–organisation is based.

  Ontologies  

    The number of ontologies is fixed at the first organisation of the
    accessors. In the newsgroup example, the number of interests (or
    meta–threads) was fixed at 3 because that made illustrations easier.
    It could easily be set to any arbitrary number.

  Variety  

    Once the number of ontologies has been fixed, one must then decide
    how much variety is allowed. For the newsgroup example, three
    ontologies could be combined in seven different ways. But for larger
    ontologies the number of combinations becomes very large. For the
    example of six newsgroups, the variety was limited by restricting
    the number of newsgroups expressed across a smaller playing field of
    choices. The number of which is given by ( 14 ). The assumption in
    this enumeration of choices is that they are grouped by varying
    degrees of indifference and this leads to a binary tree structure.

The underlying relationship between the topology of agents and the
complexity of the preference orderings they hold is the key to
understanding organisational phenomena.

In chapter III , an observation of Axelrod’s in [ AAEC96 ] led to a
series of investigations of behaviour, topology of proximity and system
size. It was concluded that some topologies gave rise to more stable and
predictable behaviours.

In the discussion above, it was made clear that the only configuration
parameters for a self–organising permission system seemed to be the size
of the issue space and its internal connectedness. A spanning tree leads
to systems which can simplify complex issues very quickly. This is the
basis of the work carried out by Miller et al. [ Mil95 ] , but it is
well–known from political theory that binary–tree systems can be easily
subverted [ Bla58 , Far69 ] .

To make a system safe, it should not be possible to subvert it, but it
is well-known that no collective choice procedure can be fair (and
therefore safe) from Arrow’s Impossibility Theorem . Arrow gives a
possibility theorem, but it may now be possible to have a confidence
level for a collective choice procedure based upon the entropy of the
preference hierarchies. What is a significant level of entropy is
difficult to decide, but the analysis of evolving cultures shows that it
may be the point where the phase reverses between the variety and
compatibility entropies of an evolving population.

The design of the self–organising permissions policy system only showed
how much interconnectedness is needed to make useful decisions on a
quantitative basis about abstract concepts, such as “trustworthiness”.

#### 41 Future Work

There are four outstanding problems with the design of self–organising
preference aggregation systems:

1.  Critical entropy for fair collective choice

2.  Topologies for self–organising systems

3.  Entropy measure for spectral analysis of votes

4.  Cross–certification metrics

The last of these has not been discussed at any length within this
dissertation, but it would be fundamental to a high–security system. It
requires that different preference hierarchy systems be able to
cross-validate one another. There is already a means whereby X.509
certification authorities can validate one another, but they all have
the same degree of mutual trust. Recently, research by Maurer [ KM00 ]
suggests that this may be quantified reliably.

A prototype database management system has been presented in this
dissertation which could make use of X.509 certification, chapter II ,
and a practicable system for self–organisation of access was proposed in
§ 38 . There should be further practical investigation into the
following types of system.

-   Secure self–organising mailing list

-   Licencing, surety and insurance systems

-   Permissions policy system based on insurance

The self–organising mailing list has already been presented and it would
be a good proving ground for analysing some of the methods described in
this thesis. The cross–certification metrics would ultimately have to be
based on risk and the evaluation of risk demands that liability can be
limited. This would require that licences and sureties be obtainable in
the same way as information, i.e. electronically. Finally, a
self–organising permissions policy system could be implemented using
risk as the basis for grading accessors.

Finally, an interpretation of the principles proposed by Anderson [
And96a ] as economic goals should be made. The system that Anderson
proposes for resolving the access decision issues of a collegiate
healthcare sector is a widely applicable model of an economic market.
They are essentially fair , which might imply they are pareto–optimal. A
pareto–optimal system cannot be said to exist by Arrow’s impossibility
theorem, but Arrow’s analytic choice system is based upon a memoryless
system. It may be the case that a pareto–optimal system exists if a
best–of– @xmath vote is allowed. There are some interesting games theory
scenarios that suggest this may be so [ BCP94 ] .

#### 42 Conclusion

This thesis has looked at the problem of providing a secure environment
for collaborative computing. This reduced itself to the problem of
providing safe and secure access to databases. Secure access is already
mature technology, but safe access relies upon access control. This can
only be decided with foreknowledge of the information flow. The
information flow can only be determined by classifying all subjects and
objects within the system. Such an information system would be very
large and it could not be reliably managed without using some
self–organising technology. This would require an adaptive discretionary
control mechanism.

The method proposed in this dissertation would be based on aggregating
the information flows specified by different subjects for the objects
they own. Such a system has been shown to be self–organising because it
is essentially a voter model: these have been analytically proven to be
ergodic for one–ply information flows and simulations indicate they are
also self–stabilising for an arbitrary number of plys, but take
exponential time to stabilise.

It is well worth continuing the investigation of organisational
structure using quantitative techniques based on the degree of
information flow. This is fundamentally an entropy measure where the
degree of uncertainty about an individual’s behaviour is the random
variable in a maximum likelihood estimate.