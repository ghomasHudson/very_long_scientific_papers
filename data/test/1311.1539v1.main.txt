##### Acknowledgements. I would like to begin by thanking my
supervisors, Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Pulman, for
their guidance and help over the last three years. In particular, I
thank Mehrnoosh for her invaluable support in the day-to-day aspects of
my work, and for overseeing the production and publication of the papers
we co-authored during the course of my DPhil. On the academic front, I
would also like to thank, in no particular order, Stephen Clark, Phil
Blunsom, Marco Baroni, Georgiana Dinu, Yao-Zhong Zhang, Tom Melham,
Samson Abramsky, Yorick Wilks, and the many other world-class
researchers whom I have had the pleasure of interacting with and being
challenged by as I learned the ropes of the research world. Thanks also
go to the Engineering and Physical Sciences Research Council for funding
my doctoral work through a Doctoral Training Account and an Enhanced
Stipend. From home, I thank my loving parents Irene and Greg for their
emotional and material support over the last twenty-seven years, and for
giving me the opportunity to reach this level of research (as well as
proof-reading my dissertation). I thank Grandma and Grandpa Grefenstette
for supporting my studies and personal development both at Oxford and
throughout life. I also thank my maternal grandparents, Bernard and
Olga, my brother Nicholas, my sister Natalie, and the rest of my family
for all the love I’ve received over the years. This work would not have
been possible without it. Finally, thanks to my friends in and out of
Oxford, and in particular to Chris, John, Renie, Kelsey, Tom and Emily,
Sean, Jenn, Karl, and Jan, for putting up with me throughout the
doctoral experience. Listing all those who have made the last few years
of my life an adventure is beyond the scope of this thesis, but to all
those I’ve omitted: thank you. It’s been a blast.

{romanpages}

###### Contents

-    1 Introduction
-    I Background
    -    2 Literature Review
        -    2.1 Formal Semantics
        -    2.2 Distributional Semantics
        -    2.3 Compositionality and Vector Space Models
            -    2.3.1 Additive Models
            -    2.3.2 Multiplicative Models
            -    2.3.3 Mixture Models
            -    2.3.4 Tensor-Based Models
                -    2.3.4.1 Dimensionality Problems
                -    2.3.4.2 Syntactic Expressivity
            -    2.3.5 SVS Models
        -    2.4 Matrix-Based Compositionality
            -    2.4.1 Generic Additive Model
            -    2.4.2 Adjective Matrices
            -    2.4.3 Recursive Matrix-Vector Model
    -    3 Foundations of DisCoCat
        -    3.1 Pregroup Grammars
            -    3.1.1 Pregroups
            -    3.1.2 Pregroups and Syntactic Analysis
            -    3.1.3 A graphical calculus for pregroups
        -    3.2 Categories
            -    3.2.1 The Basics of Category Theory
            -    3.2.2 Compact Closed Categories
            -    3.2.3 A Graphical Calculus for Compact Closed
                Categories
        -    3.3 A Categorical Passage from Grammar to Semantics
            -    3.3.1 @xmath
            -    3.3.2 Syntax Guides Semantics
            -    3.3.3 Example
-    II Theory
    -    4 Syntactic Extensions
        -    4.1 Functorial Passages
            -    4.1.1 Functors
            -    4.1.2 From Product Categories to Functors
        -    4.2 Supporting Context Free Grammars
            -    4.2.1 Context Free Grammar
                -    4.2.1.1 General Definition
                -    4.2.1.2 Restrictions
            -    4.2.2 CFGs as Categories
            -    4.2.3 Defining a Functor
                -    4.2.3.1 CFG to Pregroup Translation
                -    4.2.3.2 From Translation Dictionaries to Functors
        -    4.3 Supporting Lambek Grammar
            -    4.3.1 Lambek Grammar
            -    4.3.2 Lambek Grammars as Monoidal Bi-Closed Categories
            -    4.3.3 Defining a Functor
    -    5 Learning Procedures for a DisCoCat
        -    5.1 Defining Sentence Space
            -    5.1.1 Intuition
            -    5.1.2 A Concrete Proposal
        -    5.2 Noun-Oriented Types
            -    5.2.1 Dealing with Nouns
            -    5.2.2 Dealing with Relational Words
        -    5.3 Learning Procedures
            -    5.3.1 Groundwork
            -    5.3.2 A Learning Algorithm
            -    5.3.3 Problems with Reduced Representations
            -    5.3.4 A Generalised Algorithm for Reduced
                Repesentations
            -    5.3.5 Example
            -    5.3.6 An Efficient Alternative
-    III Practice
    -    6 Evaluating a DisCoCat
        -    6.1 First Experiment
            -    6.1.1 Dataset Description
            -    6.1.2 Evaluation Methodology
            -    6.1.3 Models Compared
            -    6.1.4 Results
        -    6.2 Second Experiment
            -    6.2.1 Dataset Description
            -    6.2.2 Evaluation Methodology
            -    6.2.3 Models Compared
            -    6.2.4 Results
        -    6.3 Third Experiment
            -    6.3.1 Dataset Description
            -    6.3.2 Evaluation Methodology
            -    6.3.3 Models Compared
            -    6.3.4 Results
        -    6.4 Discussion
    -    7 Further Work
        -    7.1 Distributional Logic
            -    7.1.1 Distributional Formal Semantics
            -    7.1.2 Tensors as Functions
            -    7.1.3 Formal Semantics with Tensors
            -    7.1.4 Simulating Simple Predicate Calculi
            -    7.1.5 Logical Operations, and Integrating Non-Linearity
        -    7.2 Learning Tensors by Multi-Step Linear Regression
            -    7.2.1 Multi-Step Linear Regression
            -    7.2.2 Experiments
            -    7.2.3 Discussion
        -    7.3 Further Syntactic Extensions: Combinatory Categorial
            Grammar
            -    7.3.1 Combinatory Categorial Grammar
            -    7.3.2 Categorical Semantics for CCGs
            -    7.3.3 Defining a Functor
        -    7.4 Next Steps
    -    8 Conclusions

###### List of Figures

-    2.1 A simple model of formal semantics.
-    2.2 A simple model of distributional semantics.
-    3.1 Diagrammatic examples of pregroup reductions.
-    3.2 Diagrammatic pregroup parse of “John loves Mary”.
-    3.3 Examples of yank-slide equalities in the graphical calculus for
    compact closed categories.
-    3.4 Sample diagrammatic representation of distributional
    composition.
-    4.1 Sample parse tree for the CFG in Table 4.1 .
-    4.2 Procedure for translating a CFG with @xmath production rules
    into @xmath pregroup grammars.
-    4.3 Basic diagrammatic language constructs.
-    4.4 Rewrite rules for @xmath and @xmath .
-    4.5 Diagrams for currying rules.
-    4.6 Diagrams for composition rules.
-    4.7 Diagrams for type raising rules.
-    4.8 Diagrams for sample Lambek Grammar parses.
-    4.9 Diagrams for compact closed currying.
-    5.1 Diagrammatic form of reduced representations.
-    5.2 Composition under reduced representations.
-    5.3 Procedure for learning weights for matrices of words ‘P’ with
    relational types @xmath of @xmath arguments.
-    5.4 Composition under the Kronecker model.
-    5.5 Composition under the generalised Kronecker model.
-    7.1 A simple formal semantic model.
-    7.2 Estimating a tensor for eat in two steps.
-    7.3 Simple diagrammatic form of @xmath morphisms.
-    7.4 Simple diagrammatic form of @xmath morphisms.
-    7.5 Simple diagrammatic form of @xmath morphisms.
-    7.6 Diagrams for @xmath and @xmath morphisms.
-    7.7 Expanded diagrammatic form of @xmath morphisms.
-    7.8 Diagrammatic representation of @xmath morphisms.
-    7.9 Expanded diagrammatic form of @xmath morphisms.
-    7.10 Expanded diagrammatic form of @xmath morphisms.

###### List of Tables

-    3.1 Basic elements of the graphical calculus for compact closed
    categories.
-    3.2 Structural morphisms in the graphical calculus for compact
    closed categories.
-    3.3 Rewrite rules in the graphical calculus for compact closed
    categories.
-    4.1 A simple CFG.
-    4.2 Axioms of Lambek Grammar.
-    5.1 Sample weights for selected noun vectors.
-    5.2 Sample semantic matrix for ‘show’.
-    6.1 Example entries from the intransitive dataset without annotator
    score, first experiment.
-    6.2 Model correlation coefficients with human judgements, first
    experiment. @xmath for each @xmath .
-    6.3 Example entries from the transitive dataset without annotator
    score, second experiment.
-    6.4 Model correlation coefficients with human judgements, second
    experiment. @xmath for each @xmath .
-    6.5 Example entries from the adjective-transitive dataset without
    annotator score, third experiment.
-    6.6 Model correlation coefficients with human judgements, third
    experiment. @xmath for each @xmath .
-    7.1 Spearman correlation of composition methods with human
    similarity intuitions on two sentence similarity data sets (all
    correlations significantly above chance).

### Chapter 1 Introduction

Language is fundamentally messy. Since the early days of philosophical
thought, thinkers have sought to tease structure out of it, from the
Aristotlean syllogisms to the development of mathematical logic in the
work of Peano and Frege, inter alia , at the dawn of the twentieth
century. Others, such as the later Wittgensteinian school of thought,
held that language was perfectly ‘in order’ as it was, and that it is by
measuring the correctness of our speech acts against the linguistic
community we live in that we learn how to properly use language, rather
than by appealing to some objective standard dictating the correct
structure of meaning.

As the world evolved into the present age of information, language
became not only the purview of linguists, philosophers and logicians,
but also of engineers and scientists. The rapid increase of machine
readable and publicly available text through the development of the
world wide web prompted a need for new technological tools to
systematically classify, search, translate, summarise, and analyse text.
As this need grew, new mathematical and computational methods were
developed to extract structure from unstructured data.

While some thinkers of the modern age sought to adapt the tools and
methods of logicians, leading to the development of mathematical
accounts of natural language meaning such as formal semantics, others
wished to accept the messiness of language. They instead turned to the
large amount of data available in order to model meaning through
statistical and distributional means. Thus while formal semanticists
treated natural language as a programming language, with grammar as its
syntax and logic as its semantics, distributional and statistical
semanticists interpreted the meaning of our words as being a function of
their contexts, as observed in text from various sources.

Both in the case of philosophers of language and computational
linguists, there is an apparent mutual inconsistency between the formal
and distributional/empirical approaches. They seem orthogonal, in that
the former portrays language meaning as well-organised, tidy, and well
structured; while the latter aims to learn the underlying meaning of
words without appeal to any underlying structure beyond the superficial
relations words hold to one another through grammatical relations, or
through simply occurring within the same sentence or document. However,
it could be argued that such a distinction between these two views of
semantics forms a false dichotomy, and that there is some middle ground
on which we could both think of language as being a matter of structured
relations and functions on the one hand, and empirically learnable
meaning on the other.

This thesis deals with this middle ground. In the past decade,
computational linguists have sought to combine the structured approach
of formal semantics and the empirical methods of distributional
semantics to produce a new class of models of natural language
semantics, dubbed compositional distributional semantic models , capable
of exploiting the structured aspects of language while learning meaning
empirically.

I begin, in Chapter 2 , by giving an overview of these two seemingly
different ways of modelling natural language semantics. I discuss formal
semantics, which creates an association between grammatical elements and
parts of logical expressions, in which syntactic analysis yields the
production of logical statements based on how grammatical elements
combine to form sentences.

+-----------------------+-----------------------+-----------------------+
|   ------------        | @xmath                |   ----------          |
| ---------------- ---- |                       | --------------------- |
| --------------------- |                       | --------------------- |
|   Syntactic           |                       |   \Tree [.            |
| Analysis           Se |                       |  @xmath @xmath [. @xm |
| mantic Interpretation |                       | ath @xmath @xmath ] ] |
|   S @xmath NP VP      |                       |   ----------          |
|                @xmath |                       | --------------------- |
|   NP @xmath cats      |                       | --------------------- |
| , milk, etc.   @xmath |                       |                       |
|   VP @xmath Vt N      |                       |                       |
| P              @xmath |                       |                       |
|   Vt @xmath like      |                       |                       |
| , hug, etc.    @xmath |                       |                       |
|   ------------        |                       |                       |
| ---------------- ---- |                       |                       |
| --------------------- |                       |                       |
+-----------------------+-----------------------+-----------------------+

[.5cm]

A simple formal semantic model.

I then introduce distributional semantic models, which model the meaning
of words as vectors in high dimensional spaces, constructed from the
other words they co-occur with.

[]

[.5cm]

Graphical representation of a simple distributional semantic model.

Furthermore, I discuss past attempts to reconcile both approaches with
mathematical operations that compose word vectors to form semantic
vectors for larger units of text such as phrases or sentences, surveying
the different approaches to this problem in the literature.

In Chapter 3 , I give an overview of an existing
framework—DisCoCat—developed by [ 19 ] , which provides a general
account of how grammatical formalisms and distributional semantics can
be combined to generate compositional distributional models of
semantics. The DisCoCat framework borrows a powerful mathematical tool
frequently used in quantum information theory, namely category theory,
which allows information to be communicated between different
mathematical formalisms, provided that they share some underlying
structure. I survey the background knowledge required to understand this
framework by introducing pregroup grammars, a syntactic formalism with a
convenient algebraic structure, and which is easily represented as a
category.

[]

[.5cm]

Pregroup parse of “John loves Mary”.

I also introduce the basics of category theory, and the diagrammatic
calculus which allows us to reason about compositional operations within
a category.

[]

[.5cm]

Graphical representation of subject-verb-object composition in DisCoCat.

Finally, I discuss how distributional semantics fits into the
categorical discourse within DisCoCat, and how giving categorical
representations to both pregroup grammars and vector spaces allows us to
produce syntactically motivated composition operations.

In Chapter 4 , I develop new syntactic extensions to the DisCoCat
framework. I begin by presenting the notion of a functorial passage, and
discuss how functors can be used to extend the framework to incorporate
syntactic formalisms other than pregroup grammars. I then illustrate
this by showing how Context Free Grammars can be incorporated into the
extended framework by translating them into pregroup grammars. I also
show how Lambek Grammars can be integrated into the framework by
interpreting them as a specific kind of category called a bi-closed
monoidal category. I show how an existing graphical calculus for such
categories can be applied to develop a functorial passage from these
categories to the categories representing our semantic models.

[] []

[.5cm]

Example of the diagrammatic calculus for categorical representation of
Lambek Grammar.

In Chapter 5 , I present a new learning procedure for generating
concrete compositional distributional semantic models from the DisCoCat
framework. This procedure learns the semantic representations for
relations by summing over the information that the relations’ arguments
hold in a corpus. I present a reduced representation for such relations
which allows us to efficiently compute the semantic representations for
relations which would otherwise require large amounts of data to model.
I define composition operations for these reduced representations.
Furthermore, I demonstrate that, under certain assumptions, the reduced
representations are equivalent to the full representations of the
relation they model and are embedded within these full representations.

[]

Diagrammatic example of embedded reduced representations.

I furthermore provide a simplified alternative learning procedure for
these reduced representations, which ignores some aspects of the
relations it provides a model for, but is significantly quicker to learn
and more efficient to store, while outperforming the model it supplants
in certain experiments.

[]

[.5cm]

Diagrammatic form of generalised Kronecker composition model.

In Chapter 6 , I evaluate the new models described in the previous
chapter in a series of sentence similarity detection experiments,
comparing these models with other compositional distributional models of
semantics. These experiments test how well compositional models can
produce sentence representations for sentences containing ambiguous
words, and disambiguate the constituent words through the compositional
process.

  -----------------------------------------------------------------------
  Sentence 1                          Sentence 2
  ----------------------------------- -----------------------------------
  butler bow                          butler submit

  head bow                            head stoop

  company bow                         company submit

  government bow                      government stoop
  -----------------------------------------------------------------------

[.5cm]

Sample sentence pairs from a phrase similarity detection experiment.

These experiments show that the models generated by the DisCoCat
framework and the learning procedures developed in this thesis
outperform other approaches to distributional compositionality.

Finally, in Chapter 7 , I discuss additional work that I have done on
this topic with various collaborators, and outline future work to
continue to develop this growing field. Three further developments are
described. The first addresses the issue of incorporating logical
operations into compositional distributional models. I discuss how a
simple predicate calculus might be simulated using tensors, and how this
simulation fits into the DisCoCat framework, before considering some of
the difficulties that arise when trying to model genuine logical calculi
using distributional methods. Second, I discuss how machine learning
methods such as linear regression may be adapted to the DisCoCat
framework, as an alternative to the learning procedures I initially
presented. Third, I provide the foundations for integrating a complex
grammatical formalism, Combinatory Categorial Grammars, into the
DisCoCat framework, and discuss issues faced when trying to accommodate
the many variants this formalism possesses. I conclude by suggesting
general directions future research might take based on these three
points.

The aim of this thesis is to not only provide an exploration of the ways
in which the DisCoCat framework can be extended, but to also give a
proof-of-concept, showing that this abstract framework can be
instantiated, that concrete models can be developed based on it, and
that these models offer an interesting new direction in the search for
sophisticated models of natural language semantics.

## Part I Background

### Chapter 2 Literature Review

  Chapter Abstract

Compositional formal semantic models represent words as parts of logical
expressions, composed according to grammatical structure. These models
embody classical ideas in logic and philosophy of language, mainly
Frege’s principle that the meaning of a sentence is a function of the
meaning of its parts [ 31 ] . Well studied and robust, logical
formalisms offer a scalable theory of meaning which can be used to
reason about language using logical tools of proof and inference. In
contrast, distributional models are a more recent approach to semantic
modelling, representing the meaning of words not as logical formula but
as vectors whose values have been empirically learned from corpora.
Distributional models have found their way into real world applications
such as thesaurus extraction [ 41 , 21 ] or automated essay marking [ 55
] , and have strong connections to semantically motivated information
retrieval [ 59 ] . This new dichotomy in defining properties of meaning:
‘logical form’ versus ‘contextual use’, has left the question of the
foundational structure of meaning, initially of sole concern to
linguists and to philosophers of language, even more of a challenge.

In this chapter, I present an overview of the background to the work
developed in this thesis by briefly describing formal and distributional
approaches to natural language semantics, and providing a non-exhaustive
list of some approaches to compositional distributional semantics. For a
more complete review of the topic, I encourage the reader to consult one
of the many excellent surveys of the field [ 80 , 21 , 12 ] .

#### 2.1 Formal Semantics

The approach commonly known as Formal Semantics , principally due to the
work of Richard Montague in the area of inductive logic, provides
methods for translating sentences of natural language into logical
formulae, which can then be fed to computer-based reasoning tools [ 2 ]
.

To compute the meaning of a sentence consisting of @xmath words, the
individual meanings of each words must interact . In formal semantics,
this interaction is represented as a function derived from the
grammatical structure of the sentence. Formal models consist of a
pairing of syntactic analysis rules (in the form of a grammar) with
semantic interpretation rules, as exemplified by the simple model
presented on the left hand side of Figure 2.1 .

The semantic representations of words here are expressions of a
higher-order logic formed by lambda expressions over parts of
first-order logical formulae, which can be combined with one another to
form well-formed expressions of first-order logic. The function @xmath
maps elements of the lexicon @xmath to their interpretation in the
logical model @xmath used. Proper nouns are typically just logical
atoms, while nouns, adjectives, verbs, and other relational words are
interpreted as predicates and relations. The parse of a sentence such as
“cats like milk”, represented here as a binarised parse tree, is used to
produce its semantic interpretation by substituting semantic
representations for their grammatical constituents and applying @xmath
-reduction where needed. Such a derivation is shown on the right hand
side of Figure 2.1 .

What makes this class of models attractive is that it reduces language
meaning to logical expressions, a subject well-known to philosophers of
language, logicians, and linguists. The properties of first-order and
higher-order logics are well studied, and it is possible to evaluate the
meaning of a sentence if given a logical model and domain (the
model-theoretic approach to logic), as well as to verify whether or not
one sentence entails another according to the rules of logical
consequence and deduction, based on syntactic rules (the syntactic
approach to logic).

However, such logical analysis says nothing about the closeness in
meaning or topic of expressions beyond their truth-conditions and which
models satisfy these truth conditions. Therefore, formal semantic
approaches to modelling language meaning do not perform well on language
tasks where the notion of similarity is not strictly based on truth
conditions, such as document retrieval, topic classification, etc.
Furthermore, an underlying domain of objects and a valuation function
must be provided, as with any logic, leaving open the question of how we
might learn the meaning of language using such a model, rather than just
use it.

#### 2.2 Distributional Semantics

The distributional semantics approach to lexical semantics represents
the meaning of words as distributions in a high-dimensional vector
space. This approach is based on the distributional hypothesis of Harris
[ 44 ] , who postulated that the meaning of a word was dictated by the
context of its use. The more famous dictum stating this hypothesis is
Firth’s statement [ 28 ] that “You shall know a word by the company it
keeps”. This view of semantics has furthermore been associated [ 33 , 80
] with earlier work in philosophy of language by Wittgenstein, presented
in [ 88 ] , who stated that language meaning was equivalent to its real
world use.

Practically speaking, in this approach, the meaning of a word can be
learned from a corpus by looking at what other words occur with it
within a certain context . The resulting distribution can be represented
as a vector in a semantic vector space. This vectorial representation is
convenient because vectors are a familiar structure with a rich set of
ways of computing vector distance, allowing us to experiment with
different word similarity metrics. The geometric nature of this
representation entails that we can not only compare individual words’
meaning with various levels of granularity (e.g. we might, for example,
be able to show that cats are closer to kittens than to dogs, but that
all three are mutually closer than cats and steam engines), but also
apply methods frequently called upon in information retrieval tasks such
as those described by [ 59 ] , to group concepts by topic, sentiment, or
other semantic classes.

The distribution underlying word meaning here is a vector in a vector
space, the basis vectors of which are dictated by the context. In simple
models, the basis vectors will be annotated with words from the lexicon.
Traditionally, the vector spaces used in such models are Hilbert spaces,
i.e. vector spaces with orthogonal bases, such that the inner product of
any one basis vector with another (other than itself) is zero. The
semantic vector for any word can be represented as the weighted
superposition (i.e. the vector sum) of the basis vectors:

  -- -------- --
     @xmath   
  -- -------- --

where some set of orthogonal unit vectors @xmath is the basis of the
vector space which the meaning of the word lives in, and @xmath is the
weight associated with basis vector @xmath .

A common source of confusion when encountering these models for the
first time is to think of these annotated basis vectors as
representations of the words which annotate them. In fact, they are
simply representations of those words as context tokens. For example,
the word ‘furry’ may be used to annotate one of the basis elements of a
vector space in its role as a context. However, when we wish to reason
about the meaning of ‘furry’, we will use the semantic vector associated
with it in the vector space, which is distinct from the basis vector
annotated with ‘furry’. This distinction may help explain why the
orthogonality of the basis vectors is acceptable despite the fact that
some of the words which annotate them may have similar meaning: we
assume their context-theoretic properties to be independent, even if the
semantic vectors we will eventually construct for these words may be
closer in the space.

The construction of the vector for a word is done by counting, for each
lexicon word @xmath associated with basis vector @xmath , how many times
@xmath occurs in the context of each occurrence of the word for which we
are constructing the vector. This count is then typically adjusted
according to a weighting scheme (e.g. term frequency inverse document
frequency). The “context” of a word can be something as simple as the
other words occurring in the same sentence as the word or within @xmath
words of it, or something more complex, such as using dependency
relations or other syntactic features.

To give an example, suppose that we wish to construct the semantic
vectors for the meaning of ‘dog’, ‘cat’ and ‘snake’. Let us fix the set
of possible context words to be @xmath ‘furry’, ‘pet’, ‘stroke’ @xmath .
Let us consider ‘context’ to mean “words occurring in the same sentence
as the target word”. We begin by looking at the instances of ‘dog’ in
some training corpus of real text usage and see that it occurs twice
with ‘furry’, twice with ‘pet’ and once with ‘stroke’. We therefore
learn from the training corpus the vector:

  -- -------- --
     @xmath   
  -- -------- --

We then observe that ‘cat’ occurs thrice in the same sentence as
‘furry’, once as ‘pet’ and does not occur in the same sentence as
stroke. Likewise, we note that ‘snake’ does not occur in the context of
‘furry’, but twice in the context of ‘pet’ and ‘stroke’. We therefore
build the vectors:

  -- -------- --
     @xmath   
  -- -------- --

Figure 2.2 shows a graphical representation of the semantic space in
which we built these vectors. We can visually observe that ‘cat’ and
‘dog’ seem closer in this space than ‘cat’ and ‘snake’ or ‘dog’ and
‘snake’. We can also see that ‘dog’ appears closer to ‘snake’ in this
space than the vector for ‘cat’ is close to ‘snake’. The geometric
notion of a semantic space therefore yields this non-binary aspect of
similarity: concepts are not classified as similar or dissimilar, but
are instead part of a continuum of similarity defined by the similarity
metric used to compare vectors.

Commonly, the similarity of two semantic vectors is computed by taking
their cosine measure, which is the sum of the product of the basis
weights of the vectors:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are the basis weights for @xmath and @xmath ,
respectively. However, other options may be a better fit for certain
implementations, typically dependent on the weighting scheme. Some of
these are surveyed by [ 21 ] . Throughout this thesis, I will use the
cosine measure as the principal similarity metric for vector comparison
because it is the most commonly used metric for the models I will
consider and compare my work to.

To continue with our example, we wish to compute the relative similarity
of ‘dog’ and ‘cat’ versus that of ‘dog’ and ‘snake’, to verify that dogs
and cats are more similar in our model than dogs and snakes. We first
compute:

  -- -- --
        
  -- -- --

To obtain the cosine measure, we must normalise the inner product by the
product of vector lengths

  -- -------- --
     @xmath   
  -- -------- --

to obtain the cosine measure

  -- -------- --
     @xmath   
  -- -------- --

We apply the same procedure for ‘dog’ and ‘snake’, first calculating the
inner product of the semantic vectors

  -- -- --
        
  -- -- --

and then the normalisation coefficient

  -- -------- --
     @xmath   
  -- -------- --

to obtain the cosine measure

  -- -------- --
     @xmath   
  -- -------- --

Interpreting the cosine measure as a percentage or degree of conceptual
similarity, we see that ‘dog’ is more than 10% closer in meaning to
‘cat’ than it is to ‘snake’.

Readers interested in learning more about these aspects of
distributional lexical semantics are invited to consult [ 21 ] , which
contains an extensive overview of implementation options for
distributional models of word meaning.

#### 2.3 Compositionality and Vector Space Models

In the above overview of distributional models of lexical semantics, we
have seen that distributional semantic models (DSMs) are a rich and
innovative way of learning word meaning from a corpus, and obtaining a
measure of semantic similarity for words. However, it should be fairly
obvious that the same method cannot be applied to sentences, whereby the
meaning of a sentence would be given by the distribution of other
sentences with which it occurs.

First and foremost, a sentence typically occurs only once in a corpus,
and hence substantial and informative distributions cannot be created in
this manner. More importantly, human ability to understand new sentences
is a compositional mechanism: we understand sentences we have never seen
before because we can generate sentence meaning from the words used, and
how they are put into relation . To go from word vectors to sentence
vectors, we must provide a composition operation allowing us to
construct a sentence vector from a collection of word vectors. In this
section, I will discuss several approaches to solving this problem,
their advantages, and their limitations.

##### 2.3.1 Additive Models

The simplest composition operation that comes to mind is straightforward
vector addition, such that:

  -- -------- --
     @xmath   
  -- -------- --

Conceptually speaking, if we view word vectors as semantic information
distributed across a set of properties associated with basis vectors,
using vector addition as a semantic composition operation states that
the semantic information of a set of lemmas in a sentence is simply the
sum of the semantic information of the individual lemmas. While crude,
this approach is computationally cheap, and appears sufficient for
certain NLP tasks: [ 55 ] shows it to be sufficient for automated essay
marking tasks, and [ 33 ] shows it to perform better than a collection
of other simple similarity metrics for summarisation, sentence
paraphrase, and document paraphrase detection tasks.

However there are two principal objections to additive models of
composition: first, vector addition is commutative, therefore @xmath ,
and thus vector addition ignores syntactic structure completely; and
second, vector addition sums the information contained in the vectors,
effectively jumbling the meaning of words together as sentence length
grows.

The first objection is problematic, as the syntactic insensitivity of
additive models leads them to equate the representations of sentences
with patently different meanings. [ 63 ] propose to add some degree of
syntactic sensitivity—namely accounting for word order—by weighting word
vectors according to their order of appearance in a sentence as follows:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . Consequently @xmath would not have the same
representation as @xmath .

The question of how to obtain weights and whether they are only used to
reflect word order or can be extended to cover more subtle syntactic
information is open, but it is not immediately clear how such weights
may be obtained empirically and whether this mode of composition scales
well with sentence length and increase in syntactic complexity. [ 42 ]
suggests using machine-learning methods such as partial least squares
regression to determine the weights empirically, but states that this
approach enjoys little success beyond minor composition such as
adjective-noun or noun-verb composition, and that there is a dearth of
metrics by which to evaluate such machine learning-based systems,
stunting their growth and development at the time of writing.

The second objection states that vector addition leads to increase in
ambiguity as we construct sentences, rather than decrease in ambiguity
as we would expect from giving words a context. For this reason, [ 63 ]
suggest replacing additive models with multiplicative models as
discussed in the next section, or combining them with multiplicative
models to form mixture models as discussed in @xmath 2.3.3 .

##### 2.3.2 Multiplicative Models

The multiplicative model of [ 63 ] is an attempt to solve the ambiguity
problem, discussed in the previous section, and provide implicit
disambiguation during composition. The composition operation proposed is
the component-wise multiplication ( @xmath ) of two vectors. Vectors are
expressed as the weighted superposition of their basis vectors, and the
weights of the basis vectors of the composed vector is the product of
the weights of the original vectors; for @xmath , and @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Such multiplicative models are shown by [ 63 ] to perform better at verb
disambiguation tasks than additive models for noun-verb composition,
against a baseline set by the original verb vectors. The experiment that
they use to demonstrate this improvement will also serve to evaluate our
own models, and form the basis for further experiments, as discussed
below in Chapter 6 .

This approach to compositionality still suffers from two conceptual
problems: first, component-wise multiplication remains commutative and
hence word order is not accounted for; second, rather than ‘diluting’
information during large compositions and creating ambiguity, it may
remove too much information through the ‘filtering’ effect of
component-wise multiplication.

The first problem is more difficult to deal with for multiplicative
models than for additive models, since both scalar multiplication and
component-wise multiplication are commutative linear operations and
hence @xmath and thus word order cannot be taken into account using
scalar weights.

To illustrate how the second problem entails that multiplicative models
do not scale well with sentence length, let us look at the structure of
component-wise multiplication again: @xmath . For any @xmath , if @xmath
or @xmath , then @xmath , and therefore for any composition, the number
of non-zero basis weights of the produced vector is less than or equal
to the number of non-zero basis weights of the original vectors: at each
composition step information is filtered out (or preserved, but never
increased). Hence as the number of vectors to be composed grows, the
number of non-zero basis weights of the product vector stays the same
or—more realistically—decreases. Therefore for any composition of the
form @xmath , if there exist two subsets of the set of vectors involved
such that the component-wise multiplication of the vectors in one subset
forms a vector orthogonal to that formed from the component-wise
multiplication of the vectors in the other, then @xmath . It follows
that purely multiplicative models alone are not apt as a single mode of
composition beyond binary composition operations.

One solution to this second problem not discussed by [ 63 ] would be to
introduce some smoothing factor @xmath for point-wise multiplication
such that @xmath , ensuring that information is never completely
filtered out. Seeing how the problem of syntactic insensitivity still
stands in the way of full-blown compositionality for multiplicative
models, I leave it to those interested in salvaging purely
multiplicative models to determine whether some suitable value of @xmath
can be determined.

##### 2.3.3 Mixture Models

The problems faced by multiplicative models presented in @xmath 2.3.2
are acknowledged in passing by [ 63 ] , who propose mixing additive and
multiplicative models in the hope of leveraging the advantage of each
while doing away with their pitfalls. This is simply expressed as the
weighted sum of additive and multiplicative models:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are pre-determined scalar weights.

The problems for these models are threefold. First, the question of how
scalar weights are to be obtained still needs to be addressed. Mitchell
and Lapata [ 63 ] concede that one advantage of purely multiplicative
models over weighted additive or mixture models is that the lack of
scalar weights removes the need to optimise the scalar weights for
particular tasks (at the cost of not accounting for syntactic
structure), and avoids the methodological concerns accompanying this
requirement.

Second, the question of how well this process scales from noun-verb
composition to more syntactically rich expressions must be addressed.
Using scalar weights to account for word order seems ad-hoc and
superficial, as there is more to syntactic structure than the mere
ordering of words. Therefore an account of how to build sentences
vectors for sentences such as “The dog bit the man” and “The man was
bitten by the dog” in order to give both sentences the same (or a
similar) representation would need to give a richer role to scalar
weights than just token order. Perhaps specific weights could be given
to particular syntactic classes (such as nouns) to introduce a more
complex syntactic element into vector composition; but it is clear that
this alone is not a solution, as the weight for nouns “dog” and “man”
would be the same, allowing for the same commutative degeneracy observed
in non-weighted additive models, in which @xmath . Introducing a mixture
of weighting systems accounting for both word order and syntactic roles
may be a solution; however, it is not only ad-hoc but also arguably only
partially reflects the syntactic structure of the sentence.

The third problem is that [ 63 ] show that in practice, while mixture
models perform better at verb disambiguation tasks than additive models
and weighted additive models, they perform equivalently to purely
multiplicative models with the added burden of requiring parametric
optimisation of the scalar weights.

Therefore while mixture models aim to take the best of additive and
multiplicative models while avoiding their problems, they are only
partly successful in achieving the latter goal, and demonstrably not
much better in achieving the former.

##### 2.3.4 Tensor-Based Models

From @xmath 2.3.1 – 2.3.3 we observe that the need for incorporating
syntactic information into DSMs to achieve true compositionality is
pressing, if only to develop a non-commutative composition operation
that can take into account word order without the need for ad-hoc
weighting schemes, and hopefully to integrate richer syntactic
information as well.

An early proposal by Smolensky [ 74 , 75 ] to use linear algebraic
tensors as a composition operation solves the problem of finding
non-commutative vector composition operators. The composition of two
vectors is their tensor product, sometimes called the Kronecker product
when applied to vectors rather than vector spaces: for @xmath , and
@xmath , we have:

  -- -------- --
     @xmath   
  -- -------- --

To illustrate with an example consider the following two vectors:

  -- -------- --
     @xmath   
  -- -------- --

Their Kronecker product is as follows:

  -- -- --
        
  -- -- --

The composition operation takes the original vectors and maps them to a
vector in a larger vector space @xmath which is the tensor space of the
original vectors’ spaces. Here the second instance of @xmath is not a
recursive application of the Kronecker product, but rather the pairing
of basis elements of @xmath and @xmath to form a basis element of @xmath
. The shared notation and occasional conflation of Kronecker and tensor
products may seem confusing, but is fairly standard in multi-linear
algebra.

The advantage of this approach is twofold. First, vectors for different
words need not live in the same spaces but can be composed nonetheless.
This allows us to represent vectors for different word classes
(e.g. topics, syntactic roles, etc.) in different spaces with different
bases, which was not possible under additive or multiplicative models.
Second, because the product vector lives in a larger space, we obtain
the intuitive notion that the information of the whole is richer and
more complex than the mere sum or product of the information of the
parts.

###### 2.3.4.1 Dimensionality Problems

However this increase in dimensionality brings two rather large problems
for tensor based models. The first is computational: the size of the
product vector space is the product of the size of the original vector
spaces. If we assume that all words live in the same space @xmath of
dimensionality @xmath then the dimensionality of an @xmath -word
sentence vector is @xmath . If we have as many basis vectors for our
word semantic space as there are lexemes in our
vocabulary---e.g. approximately 170k in English ¹ ¹ 1 Source:
http://www.oxforddictionaries.com/page/howmanywords ---then the size of
our sentence vectors quickly reaches magnitudes for which vector
comparison (or even storage) are computationally intractable ² ² 2 At
four bytes per integer, and one integer per basis vector weight, the
vector for “John loves Mary” would require roughly @xmath of storage,
which is over ten times the data Google processes on a daily basis
according to [ 23 ] . . Even if, as most DSM implementations do, we
restrict the basis vectors of word semantic spaces to the @xmath (e.g.
@xmath ) most frequent words in a corpus, the sentence vector size still
grows exponentially with sentence length, and the implementation
problems remain.

The second problem is mathematical: sentences of different length live
in different vector spaces, and if we assign different vector spaces to
different word types (e.g. syntactic classes), then sentences of
different syntactic structure live in different vector spaces. As a
result, they cannot be compared directly using inner product or cosine
measure, leaving us with no obvious mode of semantic comparison for
sentence vectors. If any model wishes to use tensor products in
composition operations, it must find some way of reducing the
dimensionality of product vectors to some common vector space so that
they may be directly compared.

One notable method by which these dimensionality problems can be solved
in general are the holographic reduced representations proposed by [ 67
] . The product vector of two vectors is projected into a space of
smaller dimensionality by circular convolution to produce a trace
vector. The circular correlation of the trace and one of the original
vectors produces a noisy version of the other original vector. The noisy
vector can be used to recover the clean original vector by comparing it
with a pre-defined set of candidates (for example the set of word
vectors if our original vectors are word meanings). Traces can be summed
to form new traces effectively containing several vector pairs from
which original vectors can be recovered. Using this encoding/decoding
mechanism, the tensor product of sets of vectors can be encoded in a
space of smaller dimensionality, and then recovered for computation
without ever having to fully represent or store the full tensor product,
as discussed by [ 87 ] .

There are problems with this approach that make it unsuitable for our
purposes. First, there is a limit to the information that can be stored
in traces, which is independent of the size of the vectors stored, but
is a logarithmic function of their number. As we wish to be able to
store information for sentences of variable word length without having
to directly represent the tensored sentence vector, setting an upper
bound to the number of vectors that can be composed in this manner
limits the length of the sentences we can represent compositionally
using this method.

Second, and perhaps more importantly, there are restrictions on the
nature of the vectors that can be encoded in such a way: the vectors
must be independently distributed such that the mean euclidean length of
each vector is equal to one. Such conditions are unlikely to be met in
word semantic vectors obtained from a corpus, and as the failure to do
so affects the system’s ability to recover clean vectors, holographic
reduced representations are not prima facie useable for compositional
DSMs. However, it is important to note that [ 87 ] considers possible
linguistic application areas where they may be of use, although once
again these mostly involve noun-verb and adjective-noun compositionality
rather than full-blown sentence vector construction. We retain from [ 67
] the importance of finding methods by which to project the tensored
sentence vectors into a common space for direct comparison, as will be
discussed further in Chapter 3 .

###### 2.3.4.2 Syntactic Expressivity

An additional problem of a more conceptual nature is that using the
tensor product as a composition operation simply preserves word order.
As I discussed in @xmath 2.3.3 , this is not enough on its own to model
sentence meaning. We need to have some means by which to incorporate
syntactic analysis into composition operations.

Early work on including syntactic sensitivity into DSMs by [ 40 ]
suggests using syntactic dependency relations to determine the frame in
which the distributions for word vectors are collected from the corpus,
thereby embedding syntactic information into the word vectors. This idea
is built upon by [ 14 ] who suggest incorporating dependency relations
into tensor-based composition operations as vectors themselves. For
example in the sentence “Simon loves red wine”, “Simon” is the subject
of “loves”, “wine” is its object, and “red” is an adjective describing
“wine”. Hence from the dependency tree with “loves” as root node, its
subject and object as children, and their adjectival descriptors (if
any) as their children, we read the following structure: @xmath . Using
the equality relation for inner products of tensor products:

  -- -------- --
     @xmath   
  -- -------- --

we can therefore express inner-products of sentence vectors efficiently
without ever having to actually represent the tensored sentence vector:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

This example shows that this formalism allows for the comparison of
sentences with identical dependency trees to be broken down to
term-to-term comparison without the need for the tensor products to ever
be computed or stored, reducing computation to inner product
calculations.

However while matching up terms with identical syntactic roles in the
sentence works well in the above example, this model suffers from the
same problems as the original tensor-based compositionality of [ 74 ] in
that, by the authors’ own admission, sentences of different syntactic
structure live in spaces of different dimensionality and thus cannot be
directly compared. Hence we cannot use this to measure the similarity
between even small variations in sentence structure, such as the pair
“Simon likes red wine” and “Simon likes wine”, which are sentences of
different length and grammatical structure, and therefore live in
different vector spaces under this approach.

##### 2.3.5 SVS Models

The idea of including syntactic relations to other lemmas in word
representations discussed in @xmath 2.3.4 , above, is applied
differently in the structured vector space model presented by [ 26 ] .
They propose to represent word meanings not as simple vectors, but as
triplets:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the word vector, constructed as in any other DSM, @xmath
and @xmath are selectional preferences, and take the form of @xmath maps
where @xmath is the set of dependency relations and @xmath is the set of
word vectors. Selectional preferences are used to encode the lemmas that
@xmath is typically the parent of in the dependency trees of the corpus
in the case of @xmath , and typically the child of in the case of @xmath
.

Composition takes the form of vector updates according to the following
protocol. Let @xmath and @xmath be two words being composed, and let
@xmath be the dependency relation linking @xmath to @xmath . The vector
update procedure is as follows:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where @xmath are the updated word meanings, and @xmath is whichever
vector composition (addition, component-wise multiplication) we wish to
use. The word vectors in the triplets are effectively filtered by
combination with the lemma which the word they are being composed with
expects to bear relation @xmath to, and this relation between the
composed words @xmath and @xmath is considered to be used and hence
removed from the domain of the selectional preference functions used in
composition.

This mechanism is therefore a more sophisticated version of the
compositional disambiguation mechanism discussed by [ 63 ] in that the
combination of words filters the meaning of the original vectors which
may be ambiguous (e.g. if we have one vector for all senses of the word
“bank”); however, contrary to [ 63 ] the information of the original
vectors is modified but essentially preserved, allowing for further
combination with other terms, rather than directly producing a joint
vector for the composed words. The added fact that @xmath and @xmath are
partial functions associated with specific lemmas forces grammaticality
during composition, since if @xmath holds a dependency relation @xmath
to @xmath which it never expects to hold (for example a verb having as
subject another verb, rather than the reverse) then @xmath and @xmath
are undefined for @xmath and the update fails. However, there are some
problems with this approach if our goal is true compositionality.

First, this model does not allow some of the ‘repeated compositionality’
we need because of the update of @xmath and @xmath . For example, we
expect that an adjective composed with a noun produces something like a
noun in order to be further composed with a verb or even another
adjective. However here, because the relation adj would be removed from
@xmath for some noun @xmath composed with an adjective @xmath , this new
representation @xmath would not have the properties of a noun in that it
would no longer expect composition with an adjective, rendering
representations of simple expressions like “the new red car” impossible.
Of course, we could remove the update of the selectional preference
functions from the compositional mechanism, but then we would lose this
attractive feature of grammaticality enforcement through the partial
functionality of @xmath and @xmath .

Second, this model does little more than represent the implicit
disambiguation which is expected during composition, rather than
actually provide a full blown compositional model. The inability of this
system to provide a novel mechanism by which to obtain a joint vector
for two composed lemmas—thereby building towards sentence
vectors—entails that this system provides no means by which to obtain
semantic representations of larger syntactic structures which can be
compared by inner product or cosine measure as is done with any other
DSM. Of course, this model could be combined with the compositional
models presented in @xmath 2.3.1 – 2.3.3 to produce sentence vectors,
but while some syntactic sensitivity would have been obtained, the word
ordering and other problems of the aforementioned models would still
remain, and little progress would have been made towards true
compositionality.

Finally, [ 26 ] state that the selectional preferences for each word
must be pre-defined, but provide no procedure by which we would learn
these from a corpus. This is an open problem for this model rather than
a conceptual objection, but it remains to be seen if such preferences
can be empirically determined without affecting the performance or
tractability of such a model.

We retain from this attempt to introduce compositionality in DSMs that
including information obtained from syntactic dependency relations is
important for proper disambiguation. We also note that having some
mechanism by which the grammaticality of the expression being composed
is a pre-condition for its composition is a desirable feature for any
compositional mechanism.

#### 2.4 Matrix-Based Compositionality

The final class of approaches to vector composition I wish to discuss
are three matrix based models.

##### 2.4.1 Generic Additive Model

The first is the Generic Additive Model of [ 90 ] . This is a
generalisation of the weighted additive model presented in @xmath 2.3.1
. In this model, lexical vectors are not just multiplied by fixed
parameters @xmath and @xmath before adding them to form the
representation of their combination. Instead, they are the arguments of
matrix multiplication by square matrices @xmath and @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath and @xmath represent the added information provided by
putting two words into relation.

The numerical content of @xmath and @xmath is learned by linear
regression over triplets @xmath where @xmath and @xmath are lexical
semantic vectors, and @xmath is the expected output of the combination
of @xmath and @xmath . This learning system thereby requires the
provision of labelled data for linear regression to be performed.
Zanzotto et al. suggest several sources for this labelled data, such as
dictionary definitions and word etymologies.

This approach is richer than the weighted additive models, since the
matrices act as linear maps on the vectors they take as ‘arguments’, and
thus can encode more subtle syntactic or semantic relations. However,
this model treats all word combinations as the same operation,
e.g. treating the combination of an adjective with its argument and a
verb with its subject as the same sort of composition. This may not be
fundamentally wrong, but it would be worthwhile evaluating how well this
system works in the context of experiments such as those presented in
Chapter 6 . But because of the diverse ways there are of training such
supervised models, we leave it to those who wish to further develop this
specific line of research to perform such evaluations.

##### 2.4.2 Adjective Matrices

The second approach is the matrix-composition model of [ 5 ] , which
they develop only for the case of adjective-noun composition, although
their approach can seamlessly be used for any other predicate-argument
composition. Contrary to most of the approaches above, which aim to
combine two lexical vectors to form a lexical vector for their
combination, Baroni and Zamparelli suggest giving different semantic
representations to different types, or more specifically to adjectives
and nouns.

In this model, nouns are lexical vectors, as with other models. However,
embracing a view of adjectives that is more in line with formal
semantics than with distributional semantics, they model adjectives as
linear maps taking lexical vectors as input and producing lexical
vectors as output. Such linear maps can be encoded as square matrices,
and applied to their arguments by matrix multiplication. Concretely, let
@xmath be the matrix encoding the adjective’s linear map, and @xmath be
the lexical semantic vector for a noun; their combination is simply

  -- -------- --
     @xmath   
  -- -------- --

Similarly to the Generic Additive Model described above, the matrix for
each adjective is learned by linear regression over a set of pairs
@xmath where the vectors @xmath are the lexical semantic vectors for the
arguments of the adjective in a corpus, and @xmath is the semantic
vector corresponding to the expected output of the composition of the
adjective with that noun.

This may, at first blush, also appear to be a supervised training method
for learning adjective matrices from ‘labelled data’, seeing how the
expected output vectors are needed. However, Baroni and Zamparelli work
around this constraint by automatically producing the labelled data from
the corpus by treating the adjective-noun compound as a single token,
and learning its vector using the same distributional learning methods
they used to learn the vectors for nouns.

This same approach can be extended to other unary relations without
change. It bears some similarity to some of the work presented in this
document, and a direct comparison of our frameworks would be
interesting. However, this requires a method of extending this approach
to binary predicates for full comparison to be made. Furthermore, this
approach has a larger set of free parameters, such as the method of
dimensionality reduction used, the linear regression learning algorithm
and its parameters, and so on. While this entails that this class of
models can certainly be tweaked to improve results, the difficult task
of finding optimal parameters for such a framework is outside of the
scope of this thesis, and we will not attempt in our evaluations of
Chapter 6 an unfair direct comparison by using substandard parameters.

##### 2.4.3 Recursive Matrix-Vector Model

The third approach is the recently-developed Recursive Matrix-Vector
Model (MV-RNN) of [ 76 ] ³ ³ 3 Other similar related deep-learning
approaches are also worth investigating, such as that of [ 46 ] . ,
which claims the two matrix-based models described above as special
cases. In MV-RNN, words are represented as a pairing of a lexical
semantic vector @xmath with an operation matrix @xmath . Within this
model, given the parse of a sentence in the form of a binarised tree,
the semantic representation @xmath of each branch node in the tree is
produced by performing the following two operations on its children
@xmath and @xmath .

First, the vector component @xmath is produced by applying the operation
matrix of one child to the vector of the other, and vice versa, and then
projecting the concatenation of both of the products back into the same
vector space as the child vectors using a projection matrix @xmath ,
which must also be learned:

  -- -- --
        
  -- -- --

Here, @xmath is a component-wise activation function to be determined by
the model implementation (e.g. a sigmoid function), used to introduce an
element of non-linearity to the compositional process.

Second, the matrix @xmath is calculated by projecting the pairing of
matrices @xmath and @xmath back into the same space, using a projection
matrix @xmath , which must also be learned:

  -- -- --
        
  -- -- --

The pairing @xmath obtained through these operations forms the semantic
representation of the phrase falling under the scope of the segment of
the parse tree below that node.

This approach to compositionality yields good results in the experiments
described in [ 76 ] . It furthermore has appealing characteristics such
as treating relational words differently through their operation
matrices, and allowing for recursive composition, as the output of each
composition operation is of the same type of object as its inputs.
However, the learning procedure for the projection matrices and the
operation matrices for each word, described in [ 76 ] , is non-trivial.
It relies on a supervised learning algorithm, and thus is open to the
same problems of data availability and sparsity as any other supervised
learning system. This also makes it difficult to produce a version of
the model to run the experiments described in Chapter 6 , although
working towards joint evaluation of the models described in this section
with the MV-RNN approach will certainly need to be the object of further
work.

The other significant difference with the compositional framework
presented in this thesis is that composition in MV-RNN is always a
binary operation, e.g. to compose a transitive verb with its subject and
object one would first need to compose it with its object, and then
compose the output of that operation with the subject. The framework
discussed in this thesis allows for the construction of representations
for relations of larger arities, permitting the simultaneous composition
of a verb with its subject and object. Whether or not this theoretical
difference leads to significant differences in composition quality
requires joint evaluation which, once again, I will leave to further
work.

### Chapter 3 Foundations of DisCoCat

  Chapter Abstract

In Chapter 2 , I discussed distributional semantic models (DSMs) and
attempts to provide a vector composition operation over word meanings to
form distributional sentence representations. In this chapter, I will
present an existing formalism aiming to solve this compositionality
problem, as well as the mathematical background required to understand
it and further extensions, building on the features and failures of
previously discussed attempts at syntactically-sensitive
compositionality.

[ 13 , 19 ] propose to adapt a category theoretic model initially used
to describe information flow in quantum information theory to the task
of composing of semantic vectors. Syntactic analysis in the form of
pregroup grammars—a type of categorial grammar—is given categorical
semantics in order to be represented as a compact closed category @xmath
(a concept explained below), the objects of which are syntactic types
and the morphisms of which are the reductions forming the basis of
syntactic analysis. Vectors for words reside in vector spaces containing
semantic vectors for lemmas of a particular syntactic type, and the set
of vector spaces is represented as a compact closed category @xmath with
vector spaces as objects and linear maps as morphisms.

The key feature of category theory exploited here is the ability to
express different mathematical formalisms as structures which can be
related, even if the original formalisms belong in different branches of
mathematics. Hence the product category @xmath allows us to relate
syntactic types to vector spaces and syntactic reductions to linear maps
so that we obtain a mechanism by which syntactic analysis guides
semantic composition operations .

This pairing of syntactic analysis and semantic composition ensures both
that grammaticality restrictions are in place as in the model of [ 26 ]
, and that syntactically-driven semantic composition in the form of
inner-products provides the implicit disambiguation features as in the
compositional models of [ 26 ] and [ 63 ] . The composition mechanism
also involves projection of tensored vectors into a common semantic
space without the need for computing the full representation of the
tensored vectors (in a manner similar to [ 67 ] ), but without the added
restriction as to the nature of the vector spaces it can be applied to.
This avoids the complexity and comparison problems faced by other
tensor-based composition mechanisms such as those of [ 74 ] and [ 14 ] .

The word vectors can be specified model-theoretically and the sentence
space can be defined over boolean values to obtain grammatically-driven
truth-theoretic semantics in the style of [ 65 ] , as proposed by [ 13 ]
. Some logical operators can be emulated in this setting, such as using
swap matrices for negation as shown by [ 19 ] . Alternatively,
corpus-based variations on this formalism have been proposed by [ 39 ]
to obtain a non-truth theoretic semantic model of sentence meaning for
which logical operations have yet to be defined.

Before explaining how this formalism works, in @xmath 3.3 , I will
introduce pregroup grammars in @xmath 3.1 , and the required basics of
category theory in @xmath 3.2 .

#### 3.1 Pregroup Grammars

Presented by Lambek in [ 52 , 53 ] as a successor to his non-commutative
type-logical calculus presented in [ 51 ] , pregroup grammars are a
class of categorial grammars with pregroup semantics. They comprise
atomic grammatical types which can combine to form compound types. A
series of application rules allow for type-reductions, forming the basis
of syntactic analysis. The pregroup semantics of this syntactic
formalism are what interest us, as will be discussed in @xmath 3.3 .
However, our first step will be to show how this syntactic analysis
formalism works, which will in turn require an introduction to
pregroups.

##### 3.1.1 Pregroups

A pregroup is an algebraic structure of the form @xmath . Let us explain
these elements individually:

-   @xmath is simply a set of objects @xmath .

-   @xmath is a partial ordering relation on @xmath .

-   @xmath is an associative, non-commutative monoid multiplication
    operator, and can be conceived of as a function @xmath such that if
    @xmath then @xmath . Therefore @xmath is closed under this
    operation.

-   @xmath is the unit, satisfying @xmath for all @xmath .

-   @xmath and @xmath are the left and right adjoints, and can be
    conceived of as functions @xmath and @xmath such that for any @xmath
    , @xmath . Adjoints are further described by the following axioms:

    -   Reversal: if @xmath then @xmath (as for @xmath , @xmath ).

    -   Ordering: @xmath and @xmath .

    -   Cancellation: @xmath .

    -   Equality of identity: @xmath .

    -   Self-adjoint multiplication: @xmath .

I say that a pregroup is freely generated by some basic set of types
@xmath to mean that all elements of the pregroup, such as the adjoints
@xmath and complex types @xmath are formed by applying the adjoint
operations @xmath and @xmath and the multiplication operation @xmath to
elements of the basic set or those thus-generated from it. Notationally,
this means that the only alphabet used in complex types is that used to
enumerate objects of the basic set.

As a notational simplification I write @xmath for @xmath , and if @xmath
I write @xmath and call this a reduction, omitting the identity wherever
it might appear. Monoid multiplication is associative, so parentheses
may be added or removed for notational clarity without changing the
meaning of the expression as long as they are not directly under the
scope of an adjoint operator.

An example reduction in pregroup might be:

  -- -------- --
     @xmath   
  -- -------- --

I note here that the reduction order is not always unique, as I could
have reduced the expression as follows: @xmath . As a further notational
simplification, if there exists a chain of reductions @xmath we may
simply write @xmath (in virtue of the transitivity of partial ordering
relations). Hence in our above example, we can express both reduction
paths as @xmath .

##### 3.1.2 Pregroups and Syntactic Analysis

Pregroups can be used for grammatical analysis by freely generating the
set @xmath of a pregroup from the combination of basic syntactic types
@xmath and defining one type ( @xmath ) to be the sentence type. As in
any categorial grammar, words of the lexicon are assigned one or more
possible types (corresponding to different syntactic roles) in a
pre-defined type dictionary , and the grammaticality of an expression is
verified by demonstrating the existence of a reduction from the type of
the expression to the sentence type @xmath .

For example, let us assign to nouns the type @xmath , and to transitive
verbs the compound type @xmath . We can read from the type of a
transitive verb that it is something which ‘expects’ a noun on its left,
and one on its right, in order to reduce to a sentence. A sample
reduction of “John loves cake” with ‘John’ and ‘cake’ being nouns of
type @xmath and ‘loves’ being a verb of type @xmath is as follows:

  -- -------- --
     @xmath   
  -- -------- --

And thus we see that the transitive verb has combined with the subject
to become something that requires an object, which it obtains and then
becomes a sentence. The expression reduces to @xmath , and hence the
expression is grammatical.

Intransitive verbs can be given the type @xmath such that “John sleeps”
would be analysed in terms of the reduction @xmath . Adjectives can be
given the type @xmath such that “red round rubber ball” would be
analysed by @xmath . And so on and so forth for other syntactic classes…

Lambek, in [ 53 ] , presents the details of a slightly more complex
pregroup grammar with a richer hierarchy of types than presented here.
It is hand-constructed and iteratively extended by expanding the type
hierarchy as previous versions of the grammar encounter unparseable
expressions.

##### 3.1.3 A graphical calculus for pregroups

Pregroups can be represented using a simple graphical calculus [ 69 ]
allowing us to visually exhibit the simultaneous nature of type
reductions in an elegant and intuitive manner. Cancellations of the type
@xmath or @xmath are represented as ‘cups’ as shown in Figure 3.1 . I
designate the non-reduction of a type by a single downward line, which
can be seen as the ‘output’ of the reduction.

Figure 3.2 shows the diagrammatic reduction for the pregroup parse of
“John loves Mary”, whereby a noun (“John”) of type @xmath combines with
the leftmost adjoint of the compound term for a transitive verb
(“loves”) of type @xmath , and another noun (“Mary”) combines with the
rightmost adjoint of the verb to form a sentence type @xmath .

This diagrammatic calculus bears some striking similarities to the
diagrammatic calculus for compact closed categories, described in @xmath
3.2.3 . This similarity is no coincidence because of the relation
between pregroups and compact closed categories, discussed in @xmath
3.2.2 . This diagrammatic similarity will make it easier to visually
describe the process of passing from syntactic analysis to semantic
interpretation, as discussed in @xmath 3.3 .

#### 3.2 Categories

Category theory is a branch of pure mathematics which allows for the
formulation of other mathematical structures and formalisms in terms of
objects, arrows, and a few axioms. This simplicity and restricted
conceptual language makes category theory both specific and general. It
is specific in that the new properties of existing theories can be
deduced from categorical axioms. It is general in that properties of
these theories can be related to properties of other theories if they
bear the same categorical representations.

It is this ability category theory provides to communicate information
both within and across mathematical structures which makes it such a
powerful tool. In this function, it has been at the centre of recent
work in the foundations of physics and the modelling of quantum
information flow, as presented in [ 1 ] . The connection ¹ ¹ 1 I
interpret this connection as one of loose analogy, at best, rather than
holding the view that there is some fundamental link between quantum
mechanics and language. It just happens that in both quantum mechanics
and language, there is a notion of information being communicated,
exchanged, or affected by objects with an “uncertain” state. In the case
of quantum mechanics, this uncertainty takes the form of state
superpositions, while in language it takes the form of ambiguity and
polysemy. It should therefore come as no surprise that the mathematics
developed to deal with one of these domains might be adapted to deal
with the other, but I believe that the connection stops there. If
anything, the ability to straightforwardly adapt the mathematics of
quantum information flow to linguistic information flow exemplifies the
advantages provided by the abstractness and generality of category
theory. between the mathematics used for this branch of physics and
those potentially useful for linguistic modelling has been noted by
several sources, such as [ 86 , 54 , 83 ] .

In this section, in order to demonstrate how these mathematical methods
carry over to semantic analysis, I will briefly examine the basics of
category theory, monoidal categories, and compact closed categories. The
focus will be on defining enough basic concepts to proceed rather than
provide a full-blown tutorial on category theory and the modelling of
information flow, as several excellent sources already cover both
aspects, e.g. [ 58 , 85 , 18 ] . A categories-in-a-nutshell crash course
is also provided in [ 13 , 19 ] .

##### 3.2.1 The Basics of Category Theory

Let us first consider the simplest definition of a category. A basic
category @xmath is defined in terms of the following elements:

-   A collection of objects @xmath .

-   A collection of morphisms @xmath .

-   A morphism composition operation @xmath .

Each morphism @xmath has a domain @xmath and a codomain @xmath . For
@xmath and @xmath , I abbreviate these definitions as @xmath . Despite
the notational similarity to function definitions, it is important to
state that nothing else is pre-supposed about morphisms, and we should
not treat them as functions.

The following axioms hold:

-   For any @xmath and @xmath there exists @xmath and @xmath .

-   For any @xmath , @xmath and @xmath , @xmath satisfies @xmath .

-   For every @xmath there is an identity morphism @xmath such that for
    any @xmath , @xmath .

We can express various mathematical formalisms using such basic
categories, and verify that these axioms hold. For example there is a
category of sets with sets as objects and functions as morphisms, a
category of posets with posets as objects and order-preserving maps as
morphisms, and a category of groups with groups as objects and group
homomorphisms as morphisms, to name a few.

A product category @xmath of two categories @xmath and @xmath is a
category with pairs @xmath as objects, where @xmath and @xmath . There
exists a morphism @xmath in @xmath if and only if there exists @xmath
and @xmath . Product categories are useful in attaining this desired
generality of category theory, in that they allow us to relate objects
and operations (morphisms) in one mathematical formalism or structure to
those in another. However, this method of relating structures is not
ideal, as will be discussed in Chapter 4 , where a more elegant
alternative will be provided (namely functors). For the time being,
though, I will use product categories for the sake homogeneity with the
work of [ 19 ] , which this present work extends.

##### 3.2.2 Compact Closed Categories

A slightly more complex class of categories is that of monoidal
categories, which allow us to reason not just about objects and the
relations between them, but also about combinations of objects in terms
of the objects which they comprise. Formally, a (strict) monoidal
category @xmath is a basic category to which we add a bifunctor @xmath
(sometimes referred to as a monoidal tensor ) satisfying the following
conditions:

-   For all @xmath there is an object @xmath .

-   For all @xmath , we have @xmath .

-   There exists some @xmath such that for any @xmath , we have @xmath .

-   For @xmath and @xmath in @xmath there is @xmath in @xmath .

-   For @xmath , @xmath , @xmath and @xmath the following equality
    holds:

      -- -------- --
         @xmath   
      -- -------- --

The strictness of the category entails that the isomorphisms described
above are equalities.

A compact bi-closed category @xmath is a monoidal category with the
following additional axioms:

-   Each object @xmath has left and right ‘adjoint’ objects @xmath and
    @xmath in @xmath . The following isomorphism shows the distribution
    of adjoints over tensored objects:

      -- -------- --
         @xmath   
      -- -------- --

-   There exist four structural morphisms for each object @xmath :

    -   @xmath .

    -   @xmath .

    -   @xmath .

    -   @xmath .

-   All such structural morphisms satisfy the following equalities:

    -   @xmath .

    -   @xmath .

    -   @xmath .

    -   @xmath .

Furthermore, for product categories involving compact closed categories,
if there are pairings @xmath and @xmath , then there is a pairing @xmath
. One might describe compact closed categories as monoidal categories
where we not only deal with the combination of objects, but also qualify
how such combinations relate to simpler objects through ‘cancellations’
( @xmath morphisms) and ‘productions’ ( @xmath morphisms).

It is worth noting the obvious similarity between compact closed
categories and the pregroup structures discussed in @xmath 3.1.1 . I
note that each object in a compact closed category has a left and a
right adjoint, as do objects in pregroups. The monoidal tensor behaves
identically to monoidal multiplication, and is also associative. There
is a unit object @xmath with the same equality properties as @xmath in a
pregroup. Furthermore, we note that if morphisms in a compact closed
category are considered as ordering relations, the structural morphisms
hold the same inequality relations as the object-adjoint pairings do in
a pregroup.

We can therefore consider a pregroup as a compact closed category @xmath
modelling a poset. The elements of the pregroup’s set are the category’s
objects; the ordering relations are its morphisms, @xmath as @xmath ,
and monoidal multiplication is the bifunctor @xmath . Notationally,
instead of the single ordering relation symbol @xmath we instead can
write @xmath to denote the morphism expressing @xmath . Likewise, the
unary operators @xmath and @xmath can be turned into a set of morphisms
linking types to their adjoints, where each morphism can be individually
denoted @xmath for the case @xmath , and similarly @xmath for the case
@xmath , for any such @xmath in @xmath .

The procedure described above is called giving categorical semantics to
the pregroup. In @xmath 3.3 , I will discuss how the little category
theory we have seen here and this notion of giving categorical semantics
to other formalisms can aid us in achieving our goal of syntax-sensitive
compositional DSMs.

##### 3.2.3 A Graphical Calculus for Compact Closed Categories

Compact closed categories may appear to be very abstract mathematical
entities to reason with and about. Fortunately, a graphical calculus,
surveyed in [ 73 ] , has been developed to provide both visual and
practical support for these tasks. Proofs in this graphical calculus
take the form of applications of diagrammatic rewrite rules which are
sound and complete, and correspond to mathematical proofs about compact
closed categories. This graphical calculus has basic elements and
rewrite rules (and associated categorical meanings), some important ones
of which are shown in Tables 3.1 – 3.3 .

In Table 3.1 , the basic elements are shown. I depict the flow of
information as going from the top of the diagram towards the bottom,
along the paths of wires typed with objects of the category. In some
applications of this calculus, an opposite convention is used, whereby
information flows from bottom to top.

Identity is seen as a naked wire. Morphisms are a box which transforms
an input wire into an output wire of (possibly) different type. Morphism
composition is two boxes on the same wire, while tensored morphisms are
two wires side by side (with morphism boxes on them). A function over
tensors is a box that take two wires in and outputs one or more wires.
States (morphisms from the unit @xmath to other elements of the
category) are triangles with one or more output wires. Their co-state
(morphisms from objects of the category to @xmath ) are represented as
upside-down triangles with one or more input wires, where the star (
@xmath ) usually stands for an adjoint, as shown in the ‘swing’ and
‘float’ rewrite rules, discussed below.

In Table 3.2 , the diagrammatic forms for the structural morphisms of a
compact closed category are shown. The “special” structural morphisms of
a compact closed category have a specific representation in this
diagrammatic calculus, instead of boxes. I represent the @xmath
morphisms as ‘cups’ similar to those found in the diagrammatic pregroup
calculus presented in @xmath 3.1.3 , and the @xmath morphisms as ‘caps’.
Naturally, in similar diagrammatic calculi where the flow of information
is from bottom to top, cups and caps stand for @xmath and @xmath
morphisms, respectively.

Finally, in Table 3.3 , some of the key graphical re-write rules are
shown, with names which are (for the most part) not “official”, but
principally there to make it easier to talk about them. First the two
“yank” rewrites show how the combination of a cup and a cap ‘cancel’
each other to produce an identity morphism, following the definitions of
the structural morphisms. The second shows how morphisms can “slide” up
and down straight wires without changing the categorical meaning of the
diagram. Both of these rewrite rules can be combined to show that
morphisms can slide along non-straight wires (i.e. those including cups
and caps) without changing the meaning of the diagram. This is
exemplified in Figure 3.3 , which shows how a morphism can slide across
such a non-straight wire. From left to right: I first use the yank
equality, then the slide equality, then yank again to obtain the
rightmost diagram from the leftmost. For those interested, this property
is one of the central elements behind the diagrammatic proof of quantum
teleportation [ 1 , 16 ] , the diagrammatic representation of which
closely resembles that shown in Figure 3.3 , with the inclusion of
additional morphisms.

Finally, the swing and float rules show that the yank operations can be
separated out into separate steps, allowing us to ‘move’ states along
cups and caps.

#### 3.3 A Categorical Passage from Grammar to Semantics

In @xmath 3.2.2 I discussed how any pregroup grammar could be
represented as a compact closed category @xmath . In @xmath 3.2.1 I
described how product categories allowed us to relate the objects and
morphisms of one category to those of another. In this section, I will
present how [ 13 , 19 ] suggest building on this by using categories to
relate semantic composition to syntactic analysis in order to achieve
syntax-sensitive composition in DSMs.

##### 3.3.1 @xmath

Let @xmath be the symmetric monoidal compact closed category of
finite-dimensional Hilbert spaces over @xmath , i.e. vector spaces over
@xmath with orthogonal bases of finite dimension, and an inner product
operation @xmath for every vector space @xmath . The objects of @xmath
are the vector spaces, and the morphisms are linear maps between vector
spaces. The unit object is @xmath and the monoidal tensor is the linear
algebraic tensor product of vector spaces. The symmetric aspect of this
category means that for any two objects @xmath and @xmath in the
category, there exists an isomorphism @xmath , corresponding here to the
fact that any tensor is isomorphic to its permutations.

As a result of its symmetric nature, the category is degenerate in its
adjoints, in that for any vector space @xmath , we have the isomorphisms
@xmath . This is because the adjoint of a vector space @xmath is its
co-vector space @xmath , the elements of which are the conjugate
transposes of the vectors from that vector space. Since the conjugate
transpose of a real-valued vector is just the transpose of that vector,
each vector in some space @xmath can be isomorphically mapped to a
covector (its transpose) in @xmath , hence @xmath . As such, we can
effectively do away with adjoints in this category, and ‘collapse’
@xmath , @xmath , @xmath , and @xmath maps into ‘adjoint-free’ @xmath
and @xmath maps. The structural morphisms of the category are the inner
product operations @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and the @xmath maps from real numbers to tensored vector spaces

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the superposition of all the basis vectors @xmath of
@xmath

  -- -------- --
     @xmath   
  -- -------- --

On the diagrammatic front, I treat a vector @xmath as a state @xmath and
a co-vector as its co-state. This means that the application of @xmath
maps to model the composition of vectors with a tensor corresponds to
the application of the swing operation described above, showing how the
vectors are brought into relation with the tensor through inner
products, as shown in Figure 3.4 .

##### 3.3.2 Syntax Guides Semantics

If we consider the product category @xmath , we observe that is has as
objects pairs @xmath where @xmath is a pregroup type and @xmath is a
vector space, and as morphisms pairs @xmath where @xmath is a pregroup
ordering relation and @xmath is a linear map. By the definition of
product categories, for any two vector space-type pairs @xmath and
@xmath , there exists a morphism @xmath only if there exists both an
ordering @xmath and a linear map @xmath . If we view these pairings as
the association of syntactic types with vector spaces containing
semantic vectors for words of that type, this restriction effectively
states that a linear map from @xmath to @xmath is only “permitted” in
the product category if @xmath .

Both @xmath and @xmath being compact closed, it is simple to show that
@xmath is as well, by considering the pairs of unit objects and
structural morphisms from the separate categories: @xmath is now @xmath
, and the structural morphisms are @xmath , @xmath , @xmath , @xmath .
We are particularly interested in the @xmath maps, which are defined as
follows (from the definition of product categories):

  -- -------- --
     @xmath   
  -- -------- --

This states that whenever there is a reduction step in the grammatical
analysis of a sentence, there is a composition operation in the form of
an inner product on the semantic front. Hence if nouns of type @xmath
live in some noun space @xmath and transitive verbs of type @xmath live
in some space @xmath , then there must be some structural morphism of
the form:

  -- -------- --
     @xmath   
  -- -------- --

We can read from this morphism the functions required to compose a
sentence with a subject noun, a transitive verb, and an object noun, in
order to obtain a vector living in some sentence space @xmath , namely
@xmath . Diagrammatically, this composition is represented as in Figure
3.4 , where @xmath is the vector for the subject, @xmath is the vector
for the object, and @xmath is the tensor representing the noun.

The form of a syntactic type is therefore what dictates the structure of
the semantic space associated with it. The structural morphisms of the
product category guarantee that for every syntactic reduction there is a
semantic composition morphism provided by the product category:
syntactic analysis guides semantic composition .

##### 3.3.3 Example

To give an example, we can give syntactic type @xmath to nouns, and
@xmath to intransitive verbs. The parse for “kittens sleep”, namely
@xmath , corresponds to the morphism @xmath in @xmath . The syntactic
types dictate that the noun @xmath lives in some vector space @xmath ,
and the intransitive verb @xmath in @xmath . The reduction morphism
@xmath gives us the composition morphism @xmath , which we can apply to
@xmath .

Since we can express any vector as the weighted superposition of its
basis vectors, let us expand @xmath and @xmath . We can then express the
composition as follows:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The above equations are hopefully fairly clear at this stage: I express
the vectors in their explicit form. I consolidate the sums by virtue of
distributivity of the linear algebraic tensor product over addition; I
then apply the tensored linear maps to the vector components (as the
weights are scalars); and finally, I simplify the indices since @xmath
if @xmath and @xmath otherwise. I obtain a vector that lives in sentence
space @xmath .

Transitive sentences can be dealt with in a similar fashion:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

In both cases, it is important to note that the tensor product passed as
argument to the composition morphism, namely @xmath in the intransitive
case and @xmath in the transitive case, never needs to be computed.

## Part II Theory

### Chapter 4 Syntactic Extensions

  Chapter Abstract

The DisCoCat framework presented in Chapter 3 makes use of pregroup
grammars for syntactic analysis, principally because of their convenient
algebraic structure which allows us to interpret any pregroup as a
compact closed category. However, on a general level, the core strength
of the framework is its ability to define the passage from syntax to
semantics without commitment to the particular syntactic formalism used,
or indeed the particular semantic mode of representation.

In this chapter, I will discuss how some other syntactic analysis
formalisms can be substituted for the pregroup grammars of Chapter 3 .
In @xmath 4.1 , I discuss the notion of a functor and of a functorial
passage between categories, and demonstrate that the pair category
@xmath used in @xmath 3.3.2 gives rise to a functor mapping syntactic
analyses to semantic representations. In @xmath 4.2 , I present a way in
which generative grammars such as context free grammars may be supported
in the framework. In @xmath 4.3 , I will present work describing how
such a functor can be defined for Lambek Grammar, how this requires the
use of non-compact categories, and how an existing diagrammatic calculus
can be used to reason about such categories.

#### 4.1 Functorial Passages

In Chapter 3 , a product category was used to model the passage from
syntactic aspects of natural language composition to semantic
representations of compositional operations and their arguments. There
is, however, no requirement that we use a product category to model this
passage. In fact—considering the compositional properties of functors
described above—a functorial passage from syntax to semantics might fit
our needs better. The idea of using a functorial passage from pregroup
categories to @xmath had been previously suggested by [ 68 ] , and such
a functorial passage has been described in a paper I co-authored with
Coecke and Sardzadeh [ 17 ] . In this section, I will show that we can
also define such a functorial passage from a product category to the
category of vector spaces given certain restrictions, which are provided
by the association made between pregroup types and vector spaces
introduced in @xmath 3.3.2 .

In @xmath 3.3.2 I discussed the construct developed by [ 13 ] of the
pair category @xmath formed from a pregroup category @xmath and the
category of vector spaces @xmath . We saw that this category was compact
closed because @xmath and @xmath were, and that for any linear map
@xmath in @xmath there are morphisms @xmath for every @xmath and @xmath
such that @xmath . This definition actually covers a larger number of
objects than we actually need, since it gives no explicit association
between pregroup types and vector spaces. Yet this association is
implicitly relied upon so that for every pregroup parse in @xmath there
is one and only one linear map in @xmath corresponding to the
composition operation associated with the parse. In other words, we rely
on a pre-defined mapping @xmath of pregroup types to vector spaces in
order to be able to talk about the association between syntactic
analysis and semantic composition, rather than an association between
syntactic analysis and semantic composition. To make the implicit
explicit: we say that a morphism @xmath describes how @xmath composes
semantic vectors in vector space @xmath standing for words of type
@xmath to produce a phrase vector in @xmath for words of type @xmath if
and only if @xmath and @xmath . This restriction effectively allows us
to ignore all the other products in @xmath , and focus only on those
pairs of products for which the morphisms between elements of the pair
correspond to unique semantic interpretations of a pregroup parse.

To give a concrete example, consider the sentence “John loves Mary”, and
suppose we give nouns the pregroup type @xmath and transitive verbs the
type @xmath . We wish to model the semantic vectors for nouns in some
vector space @xmath and sentences of type @xmath in some vector space
@xmath . Given the isomorphic relation between adjoints in @xmath , we’d
expect the vectors for transitive verbs to live in @xmath , for the
composition of nouns with the ‘argument places’ of the verb to even be
possible. The parse of “John loves Mary” corresponds to the ordering
@xmath . This ordering corresponds to an arrow standing for the function
@xmath in @xmath mapping the object @xmath to @xmath . To find the
morphism in @xmath allowing for the composition of the word vectors for
‘John’ in @xmath with ‘loves’ in @xmath and ‘Mary’ in @xmath to form a
vector in the sentence space @xmath , we look at the product category to
find what maps @xmath to @xmath . This morphism is, of course, one of
the structural morphisms in @xmath , namely @xmath , as described in
@xmath 3.3.2 . However it should be noted that there are an infinite
number of other pairings of vector spaces and linear maps with this
parse which could have been considered, such as @xmath or @xmath ,
amongst others, simply by virtue of the definition of product
categories. These other objects and morphisms of @xmath are completely
irrelevant to our task of associating syntax with semantics, yet they
are there for us to consider. What allows us to discount them is the
assumption, made earlier in this paragraph, that @xmath should be
associated with @xmath , @xmath with @xmath , and @xmath with @xmath ,
which is essentially an instance of the sort of mapping @xmath discussed
above. This shows us how the product-category approach works if such a
mapping is defined, but that product categories come with a lot of extra
material that is irrelevant, whereas the idea of defining a functor
between @xmath and @xmath has no such superfluous objects or morphisms
and focuses on just the association of syntactic parses with semantic
morphisms.

In this section, I begin by briefly describing functors in @xmath 4.1.1
, before demonstrating in @xmath 4.1.2 that the mapping @xmath between
pregroup types and vector spaces and the product category @xmath can be
used to canonically define a functor between @xmath and @xmath .

##### 4.1.1 Functors

The simplest way to think of a functor is as a map between two
categories @xmath and @xmath which associates each object @xmath in
@xmath with some object @xmath in @xmath , and each morphism @xmath in
@xmath with some morphism @xmath in @xmath . Functors must furthermore
preserve the structure of @xmath in @xmath by ensuring that the
following two equations hold for all morphisms @xmath and @xmath of
@xmath and all objects @xmath of @xmath :

1.  @xmath , i.e. if @xmath maps to some @xmath then its identity
    morphism maps to that same object’s identity morphism.

2.  @xmath , i.e. if two morphisms @xmath and @xmath compose to some
    @xmath in @xmath , then there must be some @xmath in @xmath which is
    the composition of @xmath and @xmath .

Furthermore, a strict monoidal functor between monoidal categories is
such that the following equality holds for two such categories @xmath
and @xmath , equipped with monoidal tensors @xmath and @xmath . For any
objects @xmath and @xmath of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

To give a trivial example of a functor, consider the following two
pregroups:

-   @xmath , freely generated from the set @xmath with:

    -   the ordering relation @xmath ,

    -   the multiplicative operation @xmath ,

    -   the adjoint maps @xmath and @xmath ,

    -   and the unit @xmath .

-   @xmath , freely generated from @xmath , with:

    -   the ordering relation @xmath ,

    -   the multiplicative operation @xmath ,

    -   the adjoint maps @xmath and @xmath ,

    -   the unit @xmath ,

    -   and the stipulation that @xmath .

We can define a functor @xmath as follows:

-   @xmath , @xmath , @xmath (by the definition of functors between
    monoidal categories, we therefore also have @xmath , @xmath , and so
    on).

-   @xmath and @xmath for any @xmath in @xmath . E.g. @xmath , so we map
    the adjoints of @xmath in @xmath , such as @xmath , to those of
    @xmath ’s “image” @xmath in @xmath , such as @xmath .

-   @xmath for any @xmath in @xmath . E.g. @xmath , so effectively we
    map the reduction @xmath in @xmath to @xmath in @xmath .

The first condition for functors is satisfied by mapping identity
morphisms of @xmath to those of @xmath . The second is satisfied
primarily through the natural transitivity of ordering relations in both
categories.

The important point to note here is that no type in @xmath is mapped by
@xmath to the type @xmath of @xmath , or indeed to any compound types
containing @xmath (e.g. @xmath , @xmath , @xmath , etc.). Likewise, even
though @xmath maps @xmath to @xmath and @xmath to @xmath , the absence
of a morphism @xmath in @xmath means that there exist ordering morphisms
in @xmath , such as @xmath , which are not images of morphisms in @xmath
even though the objects they connect are images of objects in @xmath .
Functors can, in this sense, act like surjective mappings. They may, in
this context, be seen as a way of explicitly embedding one mathematical
structure into another (possibly larger) structure.

A functor is said to be ‘forgetful’ if it maps complex objects in one
category to simpler objects in another category, where the simpler
objects are “parts” of the more complex objects of the original
category. A trivial example is a functor @xmath between a category of
pairs of atoms (of the form @xmath ) and a category of atoms (e.g.
@xmath ) which maps each pair in the original category to the atom
corresponding to the left element of the pair in the simpler category,
and morphisms between pairs to morphisms between the left elements of
the pair. For example @xmath would map @xmath and @xmath to @xmath , and
some morphism @xmath to the morphism @xmath . We can see here that
@xmath ‘forgets’ some of the information from the objects and morphisms
of the original category by ignoring aspects of morphisms and objects
relating to the right element of each pair.

Finally, functors can be interpreted as morphisms in the category of
categories @xmath , and hence the usual morphism composition rules apply
for functors. Explicitly, for categories @xmath , @xmath and @xmath ,
the functors @xmath and @xmath define a functor @xmath .

##### 4.1.2 From Product Categories to Functors

We have just seen that functors can be composed like morphisms to define
a functor from a category @xmath to a category @xmath through
composition, if we are provided with ‘intermediate’ functors from @xmath
to @xmath and @xmath to @xmath . This is an important property of
functors within the context of our desire to link syntax to semantics.
If a functor is defined between a pregroup category @xmath and @xmath
(which, as will be discussed below, can be done for any pregroup
category), then to use another grammatical formalism in lieu of pregroup
grammars, it suffices to show that this other grammatical structure can
be given categorical semantics as well, and that a functor can be
defined from the relevant categorification of the grammar to some
pregroup category. A functor from the new grammatical category to @xmath
then arises directly from composition. Putting this more formally: if it
can be shown that there is a functor @xmath mapping pregroup types in
some pregroup category @xmath to vector spaces in @xmath , and reduction
morphisms in @xmath to linear maps in @xmath , then if some grammatical
formalism (e.g. a context free grammar, or combinatorial grammar, etc.)
can be represented as a category @xmath for which there exists a functor
@xmath , it follows that the types and analysis morphisms of @xmath can
be mapped directly to @xmath by some functor @xmath .

Let us then consider how we might define a functor using a mapping
@xmath between pregroup types and vector spaces, and the structure of
the product category @xmath . We begin by formally defining @xmath as
follows. Let @xmath be the set of basic pregroup types generating a
pregroup @xmath . This can be formalised by stating that any type @xmath
in @xmath is written as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath for all @xmath , and @xmath is an optional adjoint
marker for @xmath (e.g. @xmath , @xmath , @xmath , etc. , or no
adjoint).

We associate, via @xmath , a vector space @xmath to each element @xmath
of the generating set @xmath , such that @xmath , without any
commitments to the uniqueness of this association. This allows us to
associate one vector space to several elements of the generating set,
which might be desirable if, for example, we wish for nouns and noun
phrases to have separate pregroup types (say @xmath and @xmath ), but
for words/phrases of both types to reside in the same vector space so
that we can compare the noun “dog” with the phrase “Mary’s favourite
pet”.

We then recursively define the association of compound pregroup types
with vector spaces as follows:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

for any types @xmath and @xmath in @xmath , and where @xmath . Because
the set of pregroup types is freely generated, we can view the objects
of a category @xmath as being freely generated by a set @xmath , such
that any object of the category @xmath can be written as follows:

  -- -------- --
     @xmath   
  -- -------- --

To give an example, if @xmath and @xmath , we can systematically
generate the vector space for transitive verbs of type @xmath as being
@xmath .

Next, we consider the full subcategory @xmath of @xmath . A subcategory
of a category @xmath is a category @xmath where the objects of @xmath
are objects of @xmath (hence @xmath ), and likewise the morphisms of
@xmath are also morphisms of @xmath (hence @xmath ). A subcategory is
full if for all @xmath in @xmath , the sets @xmath and @xmath are equal,
i.e. if all morphisms between @xmath and @xmath in @xmath are also
morphisms in @xmath . The subcategory @xmath of @xmath is defined as
follows:

-   For all @xmath , @xmath if and only if @xmath .

-   For all morphisms @xmath in @xmath , @xmath if and only if @xmath
    and @xmath are in @xmath , and for @xmath and @xmath the following
    conditions hold: @xmath , and @xmath is a structural morphism of
    @xmath if @xmath is a structural morphism of @xmath .

The first condition ensures that we only consider the object pairings
relevant to our task of associating syntax with semantics, and the
second condition states that the subcategory is full, and places
restrictions on which morphisms exist in the subcategory. We can verify
that this category is still compact closed by the same reasoning used in
@xmath 3.3.2 , namely that if @xmath is a structural morphism of @xmath
and @xmath is a structural morphism of @xmath , then if @xmath and
@xmath are in @xmath , @xmath is also a structural morphism of @xmath ,
by virtue of the subcategory being full.

The map @xmath ensures that for every @xmath there is a unique @xmath ,
and that for every morphism @xmath there is a unique morphism @xmath by
virtue of the fact that all morphisms in @xmath are composed of
structural morphisms, and that there is an injective map from the set of
structural morphisms in @xmath to those of @xmath . It follows that
there is a functor @xmath defined as above, using @xmath .

Furthermore, there is a trivial forgetful functor @xmath which
associates any object @xmath to @xmath , and any morphism @xmath to a
morphism @xmath . It follows that the functor @xmath maps the objects
and morphisms of @xmath to those of @xmath . I have therefore shown that
from the product category @xmath and a mapping @xmath between pregroup
types and vector spaces, a functor between the pregroup category @xmath
and the category of vector spaces @xmath can naturally be defined.

#### 4.2 Supporting Context Free Grammars

While the term “context free grammar” technically refers to any grammar
which is context free in the language theoretic sense, computational
linguists usually refer to a specific sort of phrase-structure-grammar
when talking about “Context Free Grammars” (CFGs). In this sense, the
expression refers to a well-known and well-studied syntactic formalism
for analysing the phrase structure of sentences, formalised by Chomsky
in [ 11 ] . As a topic, it is covered in virtually any introduction to
language theory, computational linguistics, or mathematical linguistics.
Efficient parsing algorithms have been developed to recover the CFG
structure from digital text (e.g. CYK [ 15 , 50 , 89 ] , Earley [ 25 ]
), and a large number of corpora and other resources have been created
to assist the learning of such generative grammars (e.g. the Penn
Treebank [ 60 ] ). It is a popular tool for computational linguistics,
and it would make little sense to develop a formalism for syntactically
motivated compositional distributional semantics that does not offer the
opportunity to leverage the vast amounts of work done on and with CFGs.

In this section, I introduce a way of integrating context free grammars
into the DisCoCat formalism described in Chapter 3 . I begin, in @xmath
4.2.1 by briefly introducing CFGs and specifically the sort of CFGs I
will be discussing later in this section. In @xmath 4.2.3 , I then show
how a CFG can be given a simple categorical structure, and how a functor
from this category to @xmath can be defined by showing that there exists
at least one functor between every CFG-as-a-category and some pregroup
category @xmath , guided by the fact that pregroup grammars and CFGs are
weakly equivalent [ 9 ] .

##### 4.2.1 Context Free Grammar

In this section, I begin by giving a general definition of context free
grammar, followed by the specification of some restrictions on CFGs
which will be useful when giving CFGs categorical structure.

###### 4.2.1.1 General Definition

Formally, a context free grammar is defined by four elements:

-   a finite set @xmath of non-terminal symbols, which can be seen as
    being the syntactic types of words and phrases. In simple grammars,
    these are usually given intuitive names such as @xmath for
    noun-phrases, @xmath for verb phrases, @xmath for sentences, and so
    on.

-   a set @xmath of terminals, which we can interpret, within the
    context of linguistics, as being the words of our language.

-   a finite set @xmath of production rules, usually written in the form
    @xmath , @xmath , @xmath and so on, where @xmath , @xmath and @xmath
    may be elements of @xmath or @xmath , but @xmath must be an element
    of @xmath since it ‘generates’ the other symbols (and therefore is a
    non-terminal).

-   the starting symbol of the grammar (from the set @xmath ). We will
    use @xmath as starting symbol in this section, but there is no
    obligation to do so.

Rules of the form @xmath can be read as “A generates the types B C”, or,
going the other way, as saying that “a B and a C combine to form an A”.
On the one hand, the generative direction from “top” to “bottom” is
usually followed when parsing a string to assign to each word a
syntactic type. The “bottom-to-top” approach, on the other hand, is more
akin to what we do with pregroups when the types of words are known, and
the CFG can be used to describe the phrasal structure of a sentence.
Going this way, the generative rules can be seen as substitution rules
where the element on the left of the arrow is substituted for the
elements on the right to determine the type of the phrase comprising
them.

To give an example, a simple CFG is shown in Table 4.1 . The set of
non-terminal symbols is @xmath , the set of terminal symbols is @xmath ,
and the production rules are as shown. The @xmath is used as shorthand
for the definition of several rules (namely @xmath and @xmath ) on one
line. A parse tree for the sentence “The dog chases the cat.” is shown
in Figure 4.1 . Such parse trees are generated from the grammar’s
production rules by interpreting each rule as the formation of a tree
with, as root, the symbol on the left of the arrow; and with, as
children of the root, the symbols on the right of the arrow, themselves
being the roots of subtrees of one or more nodes. In this case, we see
that ‘the’ and ‘dog’, as well as ‘the’ and ‘cat’, both being
determiner-noun pairs @xmath , each combine to form a small tree with a
noun phrase @xmath as a root. The second of these noun phrases (“the
cat”) combines with the transitive verb @xmath , namely “chases”, to
form a tree with a verb phrase @xmath as a root. This @xmath combines
with the first @xmath to produce a tree with a sentence @xmath as root,
which is the whole parse tree for this sentence.

As a notational convenience, we state that if for some symbols @xmath
there exists a set of rules such that @xmath generates @xmath , we write
@xmath and call this a substitution sequence. We treat such substitution
sequences as being uniquely characterised by a set of production rules
used to generate @xmath from @xmath . In this sense, each distinct
@xmath stands for a different subtree with @xmath as root and @xmath as
leaves. Naturally, any two substitution sequences @xmath and @xmath can
be combined to form a unique substitution sequence @xmath .

To give a short example, in the sample CFG in Table 4.1 , we can see
that there exists a substitution sequence @xmath because @xmath and
@xmath .

###### 4.2.1.2 Restrictions

While the above description of CFGs suffices to fully define the basic
mechanics and components of phrase-structure CFGs, I add a further four
restrictions on the form of the CFGs we will consider in this section.
The reason for these restrictions will be explained at the end of @xmath
4.2.3.2 .

First, we will only consider pseudo-proper CFGs. A CFG is pseudo-proper
if:

-   Every non-terminal except @xmath is on the left-hand side of some
    production rule;

-   For every terminal or non-terminal, there is some sequence of
    production rules such that the root symbol @xmath generates a set of
    symbols including that terminal or non-terminal;

-   There are no cycles, i.e. for any symbol @xmath there is no sequence
    of production rules @xmath beginning with that symbol on the left
    and ending with that symbol as the sole symbol on the right.

-   Any sequence of symbols containing the empty string @xmath is
    equivalent to the same sequence of symbols with all instances of
    @xmath removed. Any sequence of symbols is trivially equivalent to
    itself.

These rules entail that for any sequence of symbols, there is at most a
finite set of finite trees with @xmath as root that form valid parses of
that sequence. I call these CFGs pseudo-proper because of the last rule
about @xmath . The definition of proper CFGs replaces this rule with one
stating that the empty string will not be on the right-hand side of any
production rule, as a way of ensuring that there are a finite number of
trees with a finite sequence of symbols as leaves and @xmath as a root.
The modification of this rule which I introduced seeks to preserve this
property without removing @xmath from our list of symbols, as it will be
useful in the next section. The last rule instead simply equates all
sequences of symbols which are identical once @xmath is removed, thereby
treating it as an ‘invisible’ symbol potentially present in any
sequence, and effectively eliminating it from the parsing process.

Second, we only consider CFGs that produce binarised parsed trees,
meaning that every production rule has at least one symbol on the right
of the arrow, and at most two. We do not count the ‘invisible’ symbol
@xmath in this restriction. This restriction can be applied without loss
of generality, as any CFG can be modified to fit this constraint by
introducing a finite number of additional symbols while still
recognising the same language. For any rules of the form

  -- -------- --
     @xmath   
  -- -------- --

I introduce a new symbol @xmath and a rule

  -- -------- --
     @xmath   
  -- -------- --

and produce an amended version of the first rule

  -- -------- --
     @xmath   
  -- -------- --

We apply this rewrite rule recursively to the newly produced rule in
order to produce symbols @xmath , @xmath , etc. and associated
production rules, until we obtain some symbol @xmath which has as
production rule

  -- -------- --
     @xmath   
  -- -------- --

at which point the original rule has been fully binarised. To give a
short example, assume we have the rule

  -- -------- --
     @xmath   
  -- -------- --

in our CFG. We begin by rewriting it as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a newly introduced symbol. We then introduce the rule

  -- -------- --
     @xmath   
  -- -------- --

which is not binarised, so we must rewrite it as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is newly introduced symbol. We then introduce the rule

  -- -------- --
     @xmath   
  -- -------- --

which is binarised, so the rewrite process is complete. We have
therefore binarised the rule

  -- -------- --
     @xmath   
  -- -------- --

by replacing it with the set of rules

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

This rewrite system is part of the process of translating any grammar to
Chomsky Normal Form (CNF), although the thus-produced binarised CFG is
not technically in CNF, as we allow rules for the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a non-terminal (whereas a CFG in CNF has only binary
production rules with non-terminals on the right, and unary production
rules with terminals on the right). The point of binarising a grammar
will be made clear in @xmath 4.2.3 when we discuss the categorical
structure of CFGs, but full-blown translation into CNF is not required.

Third, we will disallow production rules which have a mix of terminals
and non-terminals, excluding @xmath , on the right-hand side of the
arrow. This assumption can be made without loss of generality as any
rule with such a mixture of non-terminals and terminals can be rewritten
by replacing each terminal symbol with a new non-terminal, and
introducing a new production rule in which the newly created
non-terminal produces the terminal symbol.

Finally, the fourth restriction is that for any non-terminal which
appears on the left-hand side of a production rule with non-terminals on
the right-hand side, there is no rule with that symbol on the left-hand
side and non-terminals, excluding @xmath , on the right-hand side.
Again, this restriction can be applied without loss of generality: we
modify any rule violating the restriction, and of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is one or more non-terminals (in the form @xmath ), by
rewriting it as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a new non-terminal symbol, and adding the rule

  -- -------- --
     @xmath   
  -- -------- --

We call such non-terminals, which are on the left-hand side of
production rules with terminals on the right-hand side, basic types .
All other non-terminals will be referred to as complex types . Rules
with basic types on the left-hand side will be referred to as terminal
rules , while rules with complex types on the left-hand side will be
referred to as non-terminal rules . We call @xmath the subset of @xmath
consisting of all the non-terminal rules.

##### 4.2.2 CFGs as Categories

To use CFGs in the DisCoCat formalism instead of pregroup grammars, two
things must be done. First, we need to show that CFGs can be given a
categorical interpretation. Second, we need to show that there is a
functor between any CFG-as-a-category and the category @xmath .

Here, I show that any CFG satisfying the restrictions described above
can be interpreted as a category @xmath :

1.  Let @xmath be the set of non-terminals of our CFG.

2.  Let @xmath be a set such that @xmath and @xmath .

3.  Let the sequence of symbols of form @xmath found on the right-hand
    side of production rules in a CFG be the concatenation of the
    objects @xmath and @xmath .

4.  Let the concatenation operation for types be the associative
    monoidal tensor @xmath in the category, such that for any @xmath and
    @xmath in @xmath , the object @xmath is in @xmath .

5.  Let @xmath be a set of arrows, where for any substitution sequence
    @xmath there is one unique arrow @xmath in @xmath .

6.  Let @xmath for any @xmath , by the rule for equality of sequences
    containing @xmath , defined above.

7.  Let self equivalence of sequences of symbols define an arrow from
    each object to itself.

I hypothesise that these stipulations define a monoidal category @xmath
. Stipulations 1–4 define the set of objects in the category.
Stipulation 5 defines the set of morphisms in the category. Stipulations
4 and 6 state that if @xmath is a category, then it is a monoidal
category with @xmath as unit. To confirm that this is a valid definition
of a category, we verify that it satisfies the axioms outlined in @xmath
3.2.1 :

-   By the definition of substitution sequences, any two sequences
    @xmath and @xmath can be combined to form a substitution sequence
    @xmath . This combination corresponds to arrow composition: for
    @xmath there is some arrow @xmath ; for @xmath there is some arrow
    @xmath ; for @xmath there is some arrow @xmath , and the combination
    of substitution sequences states @xmath .

-   Substitution sequences are uniquely characterised by the production
    rules involved, hence if @xmath and @xmath combine to form a
    sequence @xmath , and @xmath and @xmath combine to form a sequence
    @xmath , then the combinations of @xmath with @xmath and @xmath with
    @xmath form the same sequence @xmath . With morphisms @xmath as
    @xmath , @xmath as @xmath and @xmath as @xmath , then @xmath , and
    therefore arrow composition is associative.

-   The self equivalence arrows @xmath from any object @xmath to itself
    correspond to replacing a sequence of symbols with itself. Doing
    this before applying a substitution sequence @xmath associated with
    some morphism @xmath does not change the outcome of the
    substitution, hence @xmath . Furthermore, because the replacing of
    the sequence of symbols @xmath with itself after the application of
    a substitution sequence @xmath does not change the outcome of the
    substitution sequence, it follows that @xmath . It follows that self
    equivalence arrows satisfy the identity arrow axiom.

So we have a collection of objects, a collection of morphisms, and a
composition operation, all of which satisfy the three axioms defining a
category. Furthermore, we have an identity object and a monoidal tensor,
so it therefore follows that any @xmath defined from a context free
grammar satisfying the restrictions outlined in @xmath 4.2.1 is a
monoidal category.

##### 4.2.3 Defining a Functor

We now turn to the task of defining a functor between some
CFG-as-a-category @xmath and @xmath . Contrary to what we did with
pregroup categories @xmath where we relied on the compact closed nature
of both @xmath and @xmath to define a functor, things are not as simple
with @xmath . While with pregroup categories, the structure of pregroup
types dictates that a transitive verb of form @xmath be mapped to a
vector space @xmath , nothing about the transitive verb ‘type’ in CFGs
tells us that we should map @xmath to something of the structure @xmath
rather than any other object in @xmath . Likewise for arrows: the
structure of pregroup reduction morphisms in @xmath dictates the
structure of the morphisms they are mapped to in @xmath , such that for
example @xmath in @xmath should map to @xmath in @xmath rather than some
other linear map between @xmath and @xmath . In short, contrary to
pregroups which directly reveal their ‘functional structure’ for both
objects and reductions, CFGs conceal it, and an additional step must be
taken before defining a functor between some category @xmath and @xmath
. Here, I will show how for every @xmath there exists one or more
pregroup categories @xmath such that a functor @xmath can be
systematically defined between @xmath and each @xmath , thereby allowing
functorial passage from @xmath to @xmath via some @xmath . I do this by
first presenting an algorithm translating any CFG with @xmath
non-terminal production rules into at most @xmath pregroup grammars.
Second, I will show that each such CFG-to-pregroup grammar translation
constitutes a functorial passage between a CFG-as-a-category @xmath and
some pregroup category @xmath .

###### 4.2.3.1 CFG to Pregroup Translation

###### Inner Structure

To translate a CFG to a pregroup, we must “reveal”, or rather make
assumptions about, the inner structure of types and generative rules. In
other words, we wish to treat a production rule as a type reduction, and
spell out the mechanisms of such a reduction, in order to add compact
closure to the categorical representation of the grammar. Recalling that
for each production rule in a CFG of the form

  -- -------- --
     @xmath   
  -- -------- --

there is some arrow @xmath in the categorification of that CFG such that
@xmath , the simplest way to reinterpret this in a compact closed
setting is to assume that one of the types @xmath or @xmath contains the
adjoint of the other, and the type of @xmath . Stating this explicitly,
let’s assume that CFG type @xmath corresponds to some pregroup type
@xmath . Then either @xmath corresponds to some pregroup type @xmath and
@xmath to some compound type @xmath , in which case @xmath in @xmath
corresponds to @xmath in @xmath ; or @xmath corresponds to some pregroup
type @xmath and @xmath to some compound type @xmath , in which case
@xmath in @xmath corresponds to @xmath in @xmath .

It is important to note that these two ways of translating a production
rule make no assumptions about whether the types which are not inferred
to be compound types—namely @xmath and @xmath in the first case, and
@xmath and @xmath in the second—are compound or simple types themselves.
For example, if we have already inferred that @xmath must be of some
pregroup type @xmath , then the type of @xmath in the first case will be
@xmath , and the type of @xmath in the second case will be @xmath .
Similarly, if we had previously inferred the pregroup type of @xmath to
be @xmath , then the type of @xmath in the first case becomes @xmath ,
and similarly for the type of @xmath in the second case if that of
@xmath had previously been inferred.

###### Simple Type Inference

Since each production rule in a binarised parse tree has at most two
elements on the right-hand side, and each such rule generates two
options in terms of pregroup interpretation of the CFG types, we
immediately get the result that each CFG with @xmath production rules
generates at most @xmath different pregroup translations using this
process. At first blush, it seems that all we need to do is set some
convention such as always interpreting the left element of the pair of
types on the right-hand side of each production rule as being a compound
type in order to obtain a canonical pregroup translation of a CFG. For
example, we could always treat a generative rule of the form

  -- -------- --
     @xmath   
  -- -------- --

as a pregroup reduction of the form

  -- -------- --
     @xmath   
  -- -------- --

hence interpreting @xmath as some type @xmath , @xmath as some type
@xmath and @xmath as some type @xmath . However, translating a CFG into
a pregroup is not quite so straightforward, as there are two cases where
a rule of the form

  -- -------- --
     @xmath   
  -- -------- --

may only be translated in one way.

The first case, where @xmath or @xmath , is fairly trivial. If @xmath ,
then we cannot interpret @xmath as @xmath , since this would entail
@xmath , which generates an infinite type if @xmath , as @xmath and so
on. Hence if @xmath , then the rule must be interpreted as @xmath , and
so @xmath must be translated as @xmath , and @xmath must therefore be
translated as @xmath . By a similar argument, if @xmath , then the rule
must be interpreted as @xmath , @xmath must be translated as @xmath ,
and @xmath must therefore be translated as @xmath .

The second case is a little more subtle, and regards the fairly frequent
situation where a non-terminal symbol appears on the right-hand side of
more than one rule. Let us assume that the following two rules are in
our CFG:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

If we infer from the first rule that the pregroup types of @xmath and
@xmath are @xmath and @xmath respectively, and that @xmath has the
compound type @xmath , then we cannot make a new type inference for
@xmath in the second case whereby @xmath is a compound type. If we
could, then we would, assuming @xmath and @xmath have pregroup types
@xmath and @xmath respectively, obtain the following equality:

  -- -------- --
     @xmath   
  -- -------- --

which holds only for the case where @xmath and @xmath . A similar
argument holds for @xmath in the pairings:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

thereby covering all cases without loss of generality.

###### Type Inference Algorithm

Considering both of these cases and formalising them, we can produce an
algorithm, shown in Figure 4.2 , which takes a CFG and outputs a set of
dictionaries each mapping the symbols of the CFG to pregroup types in a
pregroup grammar @xmath . These dictionaries then allow for the
production rules of the CFG to be interpreted as reduction rules in
@xmath .

Let’s unpack the algorithm shown in Figure 4.2 a little. Step 1 defines
an initial injective mapping from CFG symbols to hypothetical types in a
pregroup, assuming that each CFG non-terminal stands for a unique
pregroup type. This mapping, represented as a type dictionary, is used
to intialise the algorithm, but also during the running of the algorithm
to verify whether or not a type has been already been set by a rule
(i.e. whether or not the pregroup type assigned to a CFG non-terminal
has been changed since the beginning of the algorithm). We also assign
the empty string @xmath to the pregroup unit @xmath , an assignment
which will not be changed during the algorithm.

Step 2 defines a boundary list. As the algorithm is effectively
constructing a tree, this list can be seen as containing all the leaves
in the tree at any particular point in the algorithm’s running. Step 3
defines a list of production rules @xmath which we will use as a stack,
and which initially contains all the non-terminal rules in the CFG. Step
4 forms the beginning of a loop which runs while @xmath is not empty,
and which at each iteration pops a new production rule off the @xmath
stack and runs the steps which follow. If the stack is empty, it returns
the boundary list @xmath which contains the leaves of the fully
constructed tree.

Steps 5 is the body of a loop run for each new rule @xmath popped off
the @xmath stack, iterating over the CFG-to-pregroup type mapping
dictionaries in the boundary @xmath . It begins by creating an empty
list @xmath which will become the new boundary (as we are turning leaves
in the old boundary into branch nodes). Step 5a creates two new empty
dictionaries @xmath and @xmath for each dictionary @xmath in the old
boundary. If the @xmath is a unary rule (one non-terminal), step 5b is
run. Step 5b interprets rules of the form @xmath as mappings @xmath
where @xmath and @xmath are obtained from @xmath and @xmath . It copies
all the mappings in @xmath to the new dictionary @xmath while replacing
every instance of @xmath in the value of @xmath with @xmath . This new
dictionary is added to the new boundary @xmath , discarding @xmath . If
the rule is binary, steps 5c and 5d are run. Step 5c interprets rules of
the form @xmath as mappings @xmath where @xmath and @xmath are obtained
from @xmath and @xmath . If non-terminals @xmath and @xmath do not use
the same symbol (i.e. @xmath ), and if the pregroup type of @xmath has
not already been defined by some other rules (i.e. @xmath , namely if
the pregroup interpretation of @xmath is still the same as the initial
assignment in @xmath ), then the mappings of @xmath are copied to the
new dictionary @xmath , replacing all instances of @xmath with @xmath ,
and then adding @xmath to the new boundary list @xmath . Similarly, step
5c interprets rules of the form @xmath as mappings @xmath where @xmath
and @xmath are obtained from @xmath and @xmath . If non-terminals @xmath
and @xmath do not use the same symbol (i.e. @xmath ), and if the
pregroup type of @xmath has not already been defined by some other rules
(i.e. @xmath , namely if the pregroup interpretation of @xmath is still
the same as the initial assignment in @xmath ), then the mappings of
@xmath are copied to the new dictionary @xmath , replacing all instances
of @xmath with @xmath , and then adding @xmath to the new boundary list
@xmath .

Finally, step 6 sets the boundary @xmath to the newly produced boundary
list @xmath by redefining @xmath as being equal to @xmath . The
algorithm then returns to step 4.

Revisiting this algorithm in even more general terms, steps 1–3
initialise a tree construction algorithm. Step 4 creates a loop over
non-terminal rules in the CFG, creating a new set of nodes to be added
to leaves of the tree. And step 5 does the actual work of creating new
nodes for each leaf of the tree, the number of which depends on the
structure of the CFG rule being considered, and whether or not the types
being inferred have already been inferred by a previous rule, or whether
there are restrictions on what inferences can take place. Step 6
completes the loop started in step 4.

###### Algorithm Output

The thus constructed tree starts with a single type dictionary assuming
every CFG terminal corresponds to a unique atomic pregroup type. As we
progress down the tree, nodes correspond modified copies of their
parents where the pregroup structure of CFG types is progressively
inferred by considering the pregroup interpretations of CFG rules. Each
branch node has one or more children generated by different
interpretations of the CFG rule being considered at that depth. Left
children correspond to the inference made from the interpretation of
unary rules and of binary rules where the left element of the right-hand
side is interpreted as a compound type; while right children correspond
to the inference made from the interpretation of binary rules where the
right element of the right-hand side is interpreted as a compound type.
The leaves of the tree are the type dictionaries inferred once all the
CFG non-terminal rules have been considered.

###### Algorithm Complexity

Let @xmath be the number of production rules in the grammar being
processed by this algorithm, and @xmath be the number of non-terminals.
The algorithm creates a binary tree of depth @xmath where each node
costs @xmath operations to construct. The time complexity of the
algorithm is therefore @xmath .

The algorithm can be modified to run in @xmath time by skipping to step
6 once one of steps 5b, 5c, or 5d has been executed (i.e. produced a new
leaf). This corresponds to a dynamic algorithm descending the tree
produced by the full-blown algorithm by following the left-most path.

###### Algorithm Termination

As a byproduct of the complexity analysis, it is easy to see that the
algorithm is guaranteed to terminate: each loop in Step 5 changes the
definition of exactly one of the remaining unchanged types, and hence
the loop will run at most @xmath times before terminating the algorithm.
The algorithm is furthermore guaranteed to succeed if the CFG satisfies
the restrictions presented above, as is discussed at the end of this
section.

###### 4.2.3.2 From Translation Dictionaries to Functors

We now complete this section by showing that each of these dictionaries
@xmath is sufficient to define a functorial passage @xmath from the
categorification @xmath of a CFG to some pregroup @xmath associated with
@xmath . Each dictionary @xmath assigns to each CFG non-terminal symbol
a pregroup type in some pregroup @xmath , freely generated by the set of
atomic symbols in the dictionary @xmath . Therefore we obtain our
functorial map for objects:

  -- -------- --
     @xmath   
  -- -------- --

which covers symbol concatenation as follows:

  -- -------- --
     @xmath   
  -- -------- --

Therefore if some @xmath defines

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

then the CFG parse in of a sentence such as “The dog chased the cat”,
namely @xmath , would translate via @xmath to @xmath in @xmath , which
reduces to @xmath .

This brings us to the second aspect of the functor, which is the map
between morphisms which would allow us to formalise how a parse in a CFG
maps to a reduction operation in @xmath . We essentially get this “for
free” by the uniqueness up to isomorphism of morphisms between any two
objects in any pregroup category @xmath , since the morphisms stand for
ordering relations. The idea is then that for any morphism @xmath in
@xmath we map @xmath to the unique morphism @xmath in @xmath . This
presupposes that there is a morphism @xmath in @xmath when there is some
@xmath in @xmath , but this is precisely what the algorithm I presented
here ensures: we assign pregroup types to CFG non-terminals by
exploiting the fact that whatever the concatenation of symbols on the
right of the production rule is translated as reduces to whatever the
translation of the symbol on the left is. This is to say that for every
concatenation of CFG symbols in the grammar there is some morphism in
@xmath which stands for the reduction of their corresponding pregroup
types.

Throughout this section we have been talking about non-terminals.
However, an essential part of a pregroup grammar is the type dictionary
for terminal symbols which assigns to each word in our language one or
more pregroup types. We get this type dictionary for free from the
pregroup translation dictionaries defined above and the terminal rules
in our CFG. We simply state that for every map @xmath translating the
non-terminals and productions of some context free grammar into the
types and reductions of some pregroup grammar @xmath there exists a
term-to-type dictionary @xmath . This term-to-type dictionary is defined
as follows: for any terminal rule in the CFG of the form

  -- -------- --
     @xmath   
  -- -------- --

producing one or more terminal symbols from a non-terminal @xmath , the
pregroup type of those non-terminals is simply the type of the
non-terminal producing them; in other words:

  -- -------- --
     @xmath   
  -- -------- --

To conclude this section, let’s examine why the restrictions on CFGs
presented in @xmath 4.2.1.2 were helpful in defining this algorithm and
functorial passage:

1.  We only considered pseudo-proper CFGs so that:

    -   All non-terminals (except @xmath ) are on the left-hand side of
        some rule, allowing us to infer the type of all non-terminals
        with complex types, and produce the type dictionaries for the
        basic types.

    -   All symbols are connected to the root @xmath by some sequence of
        rules, ensuring once again that each type can be inferred.

    -   The prohibition of cycles ensures that the algorithm does not
        reach a deadlock.

    -   Equivalence under concatenation with @xmath allows us to use
        @xmath as special ‘empty’ type, purely so it can be mapped to
        the unit object in a pregroup category.

2.  Only considering binarised rules allowed for a fairly simple
    algorithm. This isn’t necessary per se , but was set as a
    restriction principally out of convenience.

3.  Disallowing production rules with a mix of terminals and
    non-terminals on the right-hand side served to separate type
    inference from the creation of a type dictionary mapping words to
    sets of basic types.

4.  The restriction prohibiting a symbol from being on the left-hand
    side of both rules with non-terminals on the right and rules with
    terminals on the right served to create a distinction between basic
    and complex types, which was, once again, done to separate type
    inference from the production of the type dictionary.

#### 4.3 Supporting Lambek Grammar

Another grammatical formalism which might be useful to support in the
DisCoCat framework is the categorial grammar developed by Lambek in the
50s, generally referred to as Lambek Grammar. Aside from showcasing the
ability of DisCoCat to adapt to a wide-range of grammatical systems,
supporting Lambek Grammar has the additional advantage of laying
groundwork for supporting a more expressive categorial grammar, namely
Combinatorial Categorial Grammar (CCG), which I will discuss in @xmath
7.3 . In this section, I will briefly present Lambek Grammar, and the
sort of category it will be represented as, before discussing how a
functorial passage can be defined from this category to the category of
vector spaces @xmath .

##### 4.3.1 Lambek Grammar

Lambek Grammar (LG), first described in [ 51 ] , is a context free
categorial grammar consisting of an infinite set of types, and a set of
type operations, which allow for the analysis of syntax to be completed
in a manner akin to logical proofs.

Types in LG are recursively defined as follows:

-   Atomic types are represented as single letters @xmath , @xmath , …

-   @xmath is a type if @xmath and @xmath are types ¹ ¹ 1 Here, I write
    backslashes with the subscript @xmath to indicate that I am
    following Lambek’s slash notation, which differs from the slash
    notation commonly used in CCG, discussed in @xmath 7.3 . .

-   @xmath is a type if @xmath and @xmath are types.

-   @xmath is a type if @xmath and @xmath are types, with @xmath being
    associative.

The axioms of LG are shown in Table 4.2 , in the form presented in [ 8 ]
. These axioms allow ‘empty’ values for @xmath , @xmath and @xmath . We
therefore assume the existence of an empty type @xmath acting as a
multiplicative unit for the operation @xmath such that @xmath for any
@xmath .

From these axioms, the following type operations are inferred [ 66 , 29
] :

-    Slash introduction: If @xmath then @xmath and @xmath .

-    Slash elimination : @xmath and @xmath .

-    Composition: @xmath and @xmath .

-    Type-raising: @xmath and @xmath .

To complete the definition of a Lambek Grammar, a type dictionary is
defined mapping each word in our lexicon to the set of types it can
hold. A sentence @xmath is grammatically correct if there is some @xmath
for each word @xmath in the type dictionary such that @xmath , where
@xmath the type associated with sentences, can be logically derived from
the axioms of @xmath .

##### 4.3.2 Lambek Grammars as Monoidal Bi-Closed Categories

Here, we show that a Lambek Grammar LG can be modelled as a monoidal
bi-closed category @xmath , as was discussed and formalised by Bob
Coecke, Mehrnoosh Sadrzadeh and myself in [ 17 ] , and previously
observed in the original work on Lambek Grammars by Lambek himself.

A monoidal bi-closed category is a monoidal category with unit @xmath
and associative monoidal tensor @xmath . What differentiates it from a
normal monoidal category is that for every pair of objects @xmath and
@xmath in the category, there are objects @xmath and @xmath , and the
morphisms

-   @xmath

-   @xmath

-   @xmath for any @xmath

-   @xmath for any @xmath

such that the following diagrams commute: {diagram} A ⊗C & \rTo ^1_A
⊗Λ^l(f) &A ⊗(A ⊸B)                   & C⊗B & \rTo ^Λ^r(g)⊗1_B&(A⟜B) ⊗B
& \rdTo _f& \dTo _ev^l_A,B                   && \rdTo _g & \dTo
_ev^r_A,B
&&     B                 & &&A The morphisms @xmath and @xmath are
referred to as evaluation morphisms, while the @xmath morphisms @xmath
and @xmath are called currying morphisms.

It is fairly straightforward to show that any LG can be represented as a
closed monoidal category @xmath which is free in its objects. The empty
type @xmath is the unit object @xmath . For any atomic type @xmath there
is an object @xmath in the category. For each type of the form @xmath
there is some object @xmath . For each type @xmath there is an object
@xmath and for each type @xmath there is an object @xmath . In short:
the objects of the category are freely generated from the set of of
atomic types. This feature will be essential to the definition of a
functor between the categorical representation of Lambek Grammars and
the category of vector spaces @xmath .

As for our type operations:

-    Slash introduction: let @xmath be the morphism @xmath . Slash
    introduction @xmath corresponds to the morphism @xmath , and @xmath
    corresponds to the morphism @xmath .

-    Slash elimination simply corresponds to the evaluation morphisms.

-    Composition: for any pair of objects @xmath and @xmath in the
    category, there is a morphism

      -- -------- --
         @xmath   
      -- -------- --

    Likewise, for any pair of objects @xmath and @xmath in the category,
    there is a morphism

      -- -------- --
         @xmath   
      -- -------- --

-    Type raising morphisms @xmath and @xmath are just the right and
    left currying of the left and right evaluation morphisms:

    -   @xmath

    -   @xmath

We saw, in Chapter 3 , that compact closed categories have a sound and
complete graphical calculus allowing us to reason diagrammatically about
structures within such categories. Do we need to give up on this
powerful tool when dealing with closed monoidal categories as we do
here? Fortunately not.

As I discussed in [ 17 ] , with Mehrnoosh Sadrzadeh and Bob Coecke, a
diagrammatic calculus for such categories has been developed by Baez and
Stay [ 3 ] . Apart from depicting the flow of meaning, this calculus can
also be applied to depict grammatical reduction of Lambek monoids,
resembling the constituency parse trees obtained from context free
grammars.

The basic constructs of the diagrammatic language for monoidal bi-closed
categories is shown in Figure 4.3 . These are read from top to bottom,
such that sequential composition of morphisms in the category
corresponds to the downwards extension of the diagram. Arrows are
annotated with objects of the category, and morphisms are represented as
‘blobs’ with one or more ‘input’ arrows and one or more ‘output’ arrows
standing for the domain and codomain. The tensoring of two objects
corresponds to the side by side placement of their arrows. Topological
equivalence between diagrams indicates an isomorphism between the
corresponding categorical objects and vice versa, hence the diagram for
@xmath is identical to that of @xmath . Finally, there exists a rewrite
rule for the objects of the form @xmath and @xmath as shown in the last
two diagrams of Figure 4.3 . The clasp is a restriction in the
diagrammatic calculus which prevents us from treating both arrows as
separate entities, e.g. such that a function cannot be applied to one
and not to the other.

Another more complex rewrite rule is that provided for the @xmath and
@xmath morphisms, shown in Figure 4.4 . These each take two arrows in
and output one: the in arrow of the form @xmath or @xmath is rewritten
in clasp form using the rewrite rules from Figure 4.3 ; and the
‘internal structure’ of the morphism is exposed by drawing a bubble
around cups and twists that redirect the non-clasped input arrow into
the upward arrow from the clasped pair with a cup. Figure 4.5 shows the
diagrams for currying, taken from [ 3 ] , which represent the slash
introduction rules. Figure 4.6 shows the diagrams for the composition
rules.

Finally, Figure 4.7 shows the diagrams for the type raising rules,
obtained by currying the evaluation rules. This last figure may require
a bit of explanation: note that the clasps on the bottom don’t seem to
fully match the direction of the @xmath and @xmath operators in the
mathematical definition shown above them. This is no error: since
expressions of the form @xmath and @xmath are represented
diagrammatically as an upwards pointing arrow labelled @xmath clasped
with a downward pointing arrow labelled @xmath ; when @xmath is of the
form @xmath or @xmath we must in turn expand the upward pointing arrow
into two clasped arrows. However, as the arrow we are ‘expanding’ is
pointing upwards, we must turn the clasped arrow representations
upside-down, thereby reversing the up/down direction of clasped arrows
and swapping their order (effectively rotating the clasped arrows by
@xmath ). This means that if @xmath is pointing upwards, then its
clasped expansion will have a clasp pointing from left to right (from
@xmath to @xmath ) if @xmath and from right to left (again, from @xmath
to @xmath ) if @xmath , thereby showing the clasp in the opposite
direction as it is written in the formula.

To give a short example of how parses are represented diagrammatically,
the diagrams for “men kill” and “men kill dogs” are shown in Figure 4.8
. We assume nouns have the type @xmath , intransitive verbs have type
@xmath and transitive verbs have the type @xmath . The parse for “men
kill” is just an @xmath . For the parse of “men kill dogs”, we start
with one line with the type @xmath , do an @xmath with the object, then
rewrite (marked with the dotted lines) this line to a clasp form with
two lines of type @xmath and @xmath respectively, then do an @xmath with
the subject.

##### 4.3.3 Defining a Functor

Equipped with this categorical representation @xmath of a Lambek
Grammar, we now turn to the task of defining a functorial passage from a
category @xmath to the category of vector spaces @xmath . Such a
categorical passage was described in [ 17 ] which I co-authored with
Mehrnoosh Sadrzadeh and Bob Coecke. I am indebted in particular to
Mehrnoosh, who worked on the definition of this functor, and has
permitted me to reproduce her work here in my own words.

The bi-closed monoidal categories we used to represent @xmath are
structurally similar to the compact closed monoidal categories used to
model pregroup categories @xmath and @xmath , in that the evaluation
morphisms ‘act’ like epsilon maps; for example:

  -- -------- --
     @xmath   
  -- -------- --

To generalise this similarity to a strict functor @xmath , let each
atomic object @xmath in @xmath be assigned to a vector space @xmath in
@xmath . To simplify things notationally, we will drop the subscripts
and just use the same letter. As a special case, the unit type @xmath in
@xmath maps to the unit type in @xmath , namely @xmath , such that
@xmath .

The functorial passage for the monoidal tensor is simply the map of the
monoidal tensor from @xmath to @xmath , such that for any @xmath and
@xmath in @xmath we have:

  -- -------- --
     @xmath   
  -- -------- --

Here too, we will dispense with indicating the subscript on the tensor
operator.

Next comes the functorial definition for the @xmath and @xmath
operations. Generally speaking, for the passage from a bi-closed
monoidal category to a compact closed monoidal category, we would have
the following two functorial passages:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

However let us remember that in @xmath the adjoints are degenerate such
that @xmath and @xmath for all vector spaces @xmath , so we can do away
with the adjoint notation. We therefore obtain a simpler functorial
definition:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

We now turn to the functorial passage for morphisms. The most complex
case to consider here is slash introduction. Slash introduction states
that for any morphism @xmath in @xmath there are morphisms @xmath and
@xmath in @xmath . Translating this to @xmath , this would entail that
for any morphism @xmath in @xmath there must be @xmath and @xmath in
@xmath . For this passage to hold, we must show that there are such
curried morphisms for every such @xmath in @xmath .

This is where a powerful feature of the graphical calculus for bi-closed
categories of [ 3 ] comes into play: we can go from the diagrams of
morphisms in bi-closed monoidal categories to those in compact closed
categories simply by removing the bubbles and the clasps. This allows us
to produce the diagrams for currying in @xmath simply by ‘cleaning up’
those for currying in @xmath (or any other bi-closed monoidal category).
I therefore show, in Figure 4.9 , the diagrams for currying in a compact
closed category, based on those shown earlier in Figure 4.5 . From these
diagrams, we can simply read the morphisms which are needed for currying
rules to hold in @xmath , and these turn out to require nothing more
than the @xmath maps, which we represent as caps in the diagrammatic
calculus, and which we know must exist in @xmath for any object @xmath
by virtue of the category being compact closed. This ‘magical’ use of
the diagrams implicitly corresponds to the functorial passage from the
bi-closed monoidal category representing the Lambek Grammar to a strict
compact closed monoidal category which is free in its objects, and then
passing from this category to @xmath using a similar functor as was
defined for the passage from @xmath to @xmath in @xmath 4.1.2 . Using
the diagrammatic calculus to avoid dealing with the underlying
mathematics greatly simplifies the operation of defining a function in
such a way, but it is important to be aware of the reliance on the
freely generated nature of the source category @xmath .

We can therefore read the following from these diagrams: for any
morphism @xmath in @xmath there must be @xmath and @xmath in @xmath ,
where:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

So slash introduction is well defined under the functorial passage
@xmath .

The rest of the operations are far simpler to define, and can be read
from Figures 4.4 , 4.6 and 4.7 :

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

This completes the definition of a functorial passage between @xmath and
@xmath .

### Chapter 5 Learning Procedures for a DisCoCat

  Chapter Abstract

Previously, in @xmath 3.3 I presented a categorical formalism which
relates syntactic analysis steps to semantic composition operations. The
structure of the syntactic representation used dictates the structure of
the semantic spaces, but in exchange for this constraint, we are
provided with natural composition functions by the syntactic analysis,
rather than having to stipulate them ad hoc . Whereas the approaches to
compositional DSMs presented in Chapter 2 either failed to take syntax
into account during composition, or did so at the cost of not being able
to compare sentences of different structure in a common space, this
categorical approach projects all sentences into a common sentence space
where they can be directly compared. However, this alone does not give
us a compositional DSM.

As we have seen in the above examples, the structure of semantic spaces
varies with syntactic types. We therefore cannot construct vectors for
different syntactic types in the same way, as they live in spaces of
different structure and dimensionality. Furthermore, nothing has yet
been said about the structure of the sentence space @xmath into which
expressions reducing to type @xmath are projected. If we wish to have a
compositional DSM which leverages all the benefits of lexical DSMs and
extends them to sentence-level distributional representations, we must
specify a new sort of vector construction procedure.

In the original formulation of this formalism by [ 13 , 19 ] , examples
of how such a compositional DSM could be used for logical evaluation are
presented, where @xmath is defined as a boolean space with True and
False as basis vectors. However, the word vectors used are hand-written
and specified model-theoretically, as the authors leave it for future
research to determine how such vectors might be obtained from a corpus.
In this chapter, I will discuss a new way of constructing vectors for
Compositional DSMs, and of defining the sentence space @xmath , in order
to reconcile this powerful categorical formalism with the applicability
and flexibility of standard distributional models.

#### 5.1 Defining Sentence Space

##### 5.1.1 Intuition

Assume the following sentences are all true:

1.  The dogs chased the cats.

2.  The dogs annoyed the cats.

3.  The puppies followed the kittens.

4.  The train left the station.

5.  The president followed his agenda.

If asked which sentences have similar meaning, we would most likely
point to the pair (1) and (3), and perhaps to a lesser degree (1) and
(2), and (2) and (3). Sentences (4) and (5) obviously speak of a state
of the world unrelated to that which is described by the other
sentences.

If we compare these by truth value, we obviously have no means of making
such distinctions. If we compare these by lexical similarity, (1) and
(2) seem to be a closer match than (1) and (3). If we are classifying
these sentences by some higher order relation such as ‘topic’, (5) might
end up closer to (3) than (1). What, then, might cause us to pair (1)
and (3)?

Intuitively, this similarity seems to be because the subjects and
objects brought into relation by similar verbs are themselves similar.
Abstracting away from tokens to some notion of property, we might say
that both (1) and (3) express the fact that something furry and feline
and furtive is being pursued by something aggressive (or playful) and
canine. Playing along with the idea that lexical distributional
semantics presents concepts (word meanings) as ‘messy bundles of
properties’, it seems only natural to have the way these properties are
acted upon, qualified, and related as the basis for sentence-level
distributional representations. In this respect, I here suggest that the
sentence space @xmath , instead of qualifying the truth value of a
sentence, should express how the properties of the semantic objects
within are qualified or brought into relation by verbs, adjectives, and
other predicates and relations.

The above intuitions serve primarily to guide the development of a
learning procedure as described below. The question of what are the
“true semantics” of phrase and sentence vectors, of what they represent,
is fundamental and difficult to answer. Such vectors could be
interpreted as being the distributions of contexts in which sentences
occur in an infinite idealised corpus; alternatively, they can be viewed
simply as vectors of feature weights to be used in supervised machine
learning algorithms such as classifiers, or as pre-training for
unsupervised machine learning algorithms such as clustering algorithms.
Either way, the construction procedure described below is general, and
makes no commitments in terms of the interpretation of phrase and
sentence vectors.

##### 5.1.2 A Concrete Proposal

More specifically, I examine two suggestions for defining the sentence
space, namely @xmath for sentences with intransitive verbs and @xmath
for sentences with transitive verbs. These definitions mean that the
sentence space’s dimensions are commensurate with either those of @xmath
, or those of @xmath . These are by no means the only options, but as I
will discuss here, they offer practical benefits.

In the case of @xmath , the basis elements are labelled with unique
basis elements of @xmath , hence @xmath , @xmath , and so on. In the
case of @xmath , the basis elements are labelled with unique ordered
pairs of elements from @xmath , for example @xmath , @xmath , @xmath ,
and so on. Because of the isomorphism between @xmath and @xmath , I will
use the notations @xmath and @xmath interchangeably, as both constitute
appropriate ways of representing the basis elements of such a space.

To propagate this distinction to the syntactic level, I define types
@xmath and @xmath for intransitive and transitive sentences,
respectively.

#### 5.2 Noun-Oriented Types

##### 5.2.1 Dealing with Nouns

Lambek’s pregroup types presented in [ 53 ] include a rich array of
basic types and hand-designed compound types in order to capture
specific grammatical properties. Here, for the sake of simplicity I will
use a simpler set of grammatical types for experimental purposes,
similar to some common types found in Combinatory Categorial Grammar
(CCG) [ 77 ] .

I assign a basic pregroup type @xmath for all nouns, with an associated
vector space @xmath for their semantic representations. Furthermore, I
will treat noun-phrases as nouns, assigning to them the same pregroup
type and semantic space.

##### 5.2.2 Dealing with Relational Words

CCG treats intransitive verbs as functions @xmath that consume a noun
phrase and return a sentence, and transitive verbs as functions @xmath
that consume a noun phrase and return an intransitive verb function,
which in turn consumes a noun phrase and returns a sentence. Using my
distinction between intransitive and transitive sentences, I give
intransitive verbs the type @xmath associated with the semantic space
@xmath , and transitive verbs the type @xmath associated with the
semantic space @xmath .

Adjectives, in CCG, are treated as functions @xmath consuming a noun
phrase and returning a noun phrase, and hence I give them the type
@xmath and associated semantic space @xmath .

With the provision of a learning procedure for vectors in these semantic
spaces, we can use these types to construct sentence vector
representations for simple intransitive verb-based and transitive verb
based sentences, with and without adjectives applied to subjects and
objects.

#### 5.3 Learning Procedures

##### 5.3.1 Groundwork

To begin, I construct the semantic space @xmath for all nouns in my
lexicon (typically limited by the words available in the corpus used).
Any distributional semantic model can be used for this stage, such as
those presented in [ 21 ] , or the lexical semantic models used by [ 63
] . It seems reasonable to assume that higher quality lexical semantic
vectors—as measured by metrics such as the WordSim353 test of [ 27 ]
—will produce better relational vectors from the procedure designed
below. I will not test this hypothesis here, but note that it is an
underlying assumption in most of the current literature on the subject [
26 , 63 , 5 ] .

Building upon the foundation of the thus-constructed noun vectors, I
construct semantic representations for relational words. In pregroup
grammars (or other combinatorial grammars such as CCG), we can view such
words as functions taking as arguments those types present as adjoints
in the compound pregroup type, and returning a syntactic object whose
type is that of the corresponding reduction. For example, an adjective
@xmath takes a noun or noun phrase @xmath and returns a noun phrase
@xmath from the reduction @xmath . It can also compose with another
adjective to return an adjective @xmath . I wish for my semantic
representations to be viewed in the same way, such that the composition
of an adjective with a noun @xmath can be viewed as the application of a
function @xmath to its argument of type @xmath .

To learn the representations of such functions, I assume that their
meaning can be characterised by the properties that their arguments hold
in the corpus, rather than just by their context as is the case in
lexical distributional semantic models. To give an example, rather than
learning what the adjective “angry” means by observing that it co-occurs
with words such as “fighting”, “aggressive” or “mean”, we can learn its
meaning by observing that it typically takes, as argument, words that
co-occur with words such as “fighting”, “aggressive” and “mean”. While
in the lexical semantic case, such associations might only rarely occur
in the corpus, in this indirect method we learn what properties the
adjective relates to even if they do not co-occur with it directly.

In turn, through composition with its argument, I expect the function
for such an adjective to strengthen the properties that characterise it
in the representation of the object it takes as argument. Let us assume
“angry” is characterised by arguments that have high basis weights for
basis elements corresponding to the concepts (or context words)
“fighting”, “aggressive” and “mean”, and relatively low counts for
semantically different concepts such as “passive”, “peaceful” and
“loves”. When I apply “angry” to “dog” the vector for the compound
“angry dog” should contain some of the information found in the vector
for “dog”. But this vector should also have higher values for the basis
weights of “fighting”, “aggressive” and “mean”, and correspondingly
lower values for the basis weights of “passive”, “peaceful”, “loves”.

##### 5.3.2 A Learning Algorithm

To turn this idea into a concrete algorithm for constructing the
semantic representation for relations of any arity, as first presented
in [ 38 ] , let’s examine how we would deal with this for binary
relations such as transitive verbs. If a transitive verb of semantic
type @xmath is viewed as a function @xmath which expresses the extent to
which the properties of subject and object are brought into relation by
the verb, we learn the meaning of the verb by looking at what properties
are brought into relation by the verb in terms of what arguments it
takes in a corpus. Recall that the vector for a verb @xmath , @xmath ,
can be expressed as the weighted superposition of its basis elements:

  -- -------- --
     @xmath   
  -- -------- --

I take the set of vectors for the subject and object of @xmath in the
corpus to be the set of pairs @xmath . I wish to calculate the basis
weightings @xmath for @xmath . Exploiting my earlier definition of the
basis @xmath of @xmath which states that for any value of @xmath and
@xmath there is some value of @xmath such that @xmath , I define @xmath
if @xmath and @xmath otherwise. Using all of the above, I define the
calculation of each basis weight @xmath as:

  -- -------- --
     @xmath   
  -- -------- --

This allows for a full formulation of @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

The nested sums here may seem computationally inefficient, seeing how
this would involve computing @xmath products. However, using the
decomposition of basis elements of @xmath into pairs of basis elements
of @xmath (effectively basis elements of @xmath ), we can remove the
@xmath term and ignore all values of @xmath where @xmath , since the
basis weight for this combination of indices would be @xmath . Therefore
I simplify the formulation of @xmath :

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

This representation is still bloated: we perform less calculations, but
still obtain a vector in which all the basis weights where @xmath are
@xmath , hence where only @xmath of the @xmath values are non-zero. In
short, the vector weights for @xmath are, under this learning algorithm,
entirely characterised by the values of a @xmath by @xmath matrix, the
entries of which are products @xmath where @xmath and @xmath have become
row and column indices.

Using this and my definition of @xmath as a space isomorphic to @xmath ,
I can formulate a compact, ‘reduced’ expression of @xmath as follows.
Let the Kronecker product of two vectors @xmath , written @xmath , be as
follows:

  -- -------- --
     @xmath   
  -- -------- --

Equipped with this definition, I can formulate the compact form of
@xmath :

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

In short, we are only required to iterate through the corpus once,
taking for each instance of a transitive verb @xmath the Kronecker
product of its subject and object, and summing these across all
instances of @xmath . It is simple to see that no information was
discarded relative to the previous definition of @xmath : the
dimensionality reduction by a factor of @xmath simply discards all basis
elements for which the basis weight was @xmath by default.

##### 5.3.3 Problems with Reduced Representations

This raises a small problem though: this compact representation can no
longer be used in the compositional mechanism presented in @xmath 3.3 ,
as the dimensions of @xmath no longer match those which it is required
to have according to its syntactic type. However, a solution can be
devised if we return to the sample calculation, shown in @xmath 3.3.3 ,
of the composition of a transitive verb with its arguments. The
composition is as follows:

  -- -------- --
     @xmath   
  -- -------- --

where the verb @xmath is represented in its non-compact form. By
introducing the compact representation permitted by the isomorphism
@xmath I can express this as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is represented in its compact form. Furthermore, by
introducing the component wise multiplication operation @xmath :

  -- -------- --
     @xmath   
  -- -------- --

I can show the general form of transitive verb composition using the
reduced verb representation to be as follows:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Considering the diagrammatic form of reduced representations may help
understand their construction and how composition works. Let @xmath be
the reduced representation of a verb, and the function @xmath be defined
as follows:

  -- -------- --
     @xmath   
  -- -------- --

The diagrammatic representation of a verb is shown in Figure 5.1 as
being embedded within the full representation of the verb. Composition
of a subject and object with the full representation results in a yank
which feeds the vectors to the function @xmath which applies the reduced
representation to the Kronecker product of the subject and object
vectors, as shown in Figure 5.2 . In sum, I am doing exactly the same
sort of computation when I use reduced representations as I do when I
used full representations. The ‘simplification’ of reduced
representations is only a reduction in the cost of the computation; in
all other respects, it is identical to computing using @xmath maps.

To summarise what I have done with transitive verbs:

1.  I have treated them as functions taking two nouns and returning a
    sentence in a space @xmath .

2.  I have built the semantic representations for these functions by
    counting which properties of subject and object noun vectors are
    related by the verb in transitive sentences in a corpus.

3.  I have assumed that output properties are a function of input
    properties by the use of the @xmath function, i.e. the weights
    associated to @xmath from the subject argument and with @xmath from
    the object argument only affect the ‘output’ weight for the sentence
    basis element @xmath .

4.  I have shown that this leads to a compact representation of the
    verb’s semantic form, and an optimised learning procedure.

5.  I have shown that the composition operations of this formalism can
    be adapted to this compact representation.

The compact representation and amended composition operation hinge on
the choice of @xmath as output type for @xmath as input type (the pair
of arguments the verb takes), justifying my choice of a transitive
sentence space. In the intransitive case, the same phenomenon can be
observed, since such a verb takes as argument a vector in @xmath and
produces a vector in @xmath . Furthermore, my choice to make all other
types dependent on one base type—namely @xmath (with associated semantic
space @xmath )—yields the same property for every relational word we
wish to learn: the output type is the same as the concatenated (on the
syntactic level) or tensored (on the semantic level) input types. It is
this symmetry between input and output types that guarantees that any
@xmath -ary relation, expressed in the original formulation as an
element of tensor space @xmath has a compact representation in @xmath
where the @xmath th basis weight of the reduced representation stands
for the degree to which the @xmath th element of the input vector
affects the @xmath th element of the output vector.

##### 5.3.4 A Generalised Algorithm for Reduced Repesentations

The learning procedure discussed above allows the specification of a
generalised learning algorithm for reduced representations, first
presented in [ 36 ] , which is as follows. Each relational word @xmath
(i.e. those words with compound pregroup types) with grammatical type
@xmath and @xmath adjoint types @xmath is encoded as an @xmath
multi-dimensional array with @xmath degrees of freedom (i.e. an order
@xmath tensor). Since the vector space @xmath has a fixed basis, each
such array is represented in vector form as follows:

  -- -------- --
     @xmath   
  -- -------- --

This vector lives in the tensor space @xmath . Each @xmath is computed
according to the procedure described in Figure 5.3 .

Linear algebraically, this procedure corresponds to computing the
following sum:

  -- -------- --
     @xmath   
  -- -------- --

The general formulation of composing a relational word @xmath with its
arguments @xmath is now expressed as

  -- -------- --
     @xmath   
  -- -------- --

For example the computation of “furry cats nag angry dogs” would
correspond to the following operation:

  -- -------- --
     @xmath   
  -- -------- --

##### 5.3.5 Example

To give an example, taken from [ 36 ] , I demonstrate how the meaning of
the world ‘show’ might be learned from a corpus and then composed.
Suppose there are two instances of the verb ‘show’ in the corpus:

  -------- --- -------------------
  @xmath   =   table show result
  @xmath   =   map show location
  -------- --- -------------------

The vector of ‘show’ is

  -- -------- --
     @xmath   
  -- -------- --

Consider a vector space @xmath with four basis vectors ‘far’, ‘room’,
‘scientific’, and ‘elect’. The TF/IDF-weighted values for vectors of
selected nouns (built from the British National Corpus) are as shown in
Table 5.1 .

Part of the matrix compact representation of ‘show’ is presented in
Table 5.2 .

As a sample computation, the weight @xmath for basis element @xmath ,
i.e. @xmath , is computed by multiplying weights of ‘table’ and ‘result’
on @xmath , i.e. @xmath , multiplying weights of ‘map’ and ‘location’ on
@xmath , i.e. @xmath then adding these @xmath and obtaining the total
weight @xmath .

I now wish to compute the vector for the sentence “[the] master shows
[his] dog” (omitting the determiner and possessive for simplicity). The
calculation will be:

  -- -------- --
     @xmath   
     @xmath   
              
              
  -- -------- --

The row-wise flattening of the final matrix representation gives us the
result we seek, namely the sentence vector in @xmath for “[the] master
shows [his] dog”:

  -- -------- --
     @xmath   
  -- -------- --

##### 5.3.6 An Efficient Alternative

To complete this chapter, let us consider an alternative and efficient
way of learning the representations for transitive verbs, which I will
call the Kronecker method. I first presented this efficient learning
method with Mehrnoosh Sadrzadeh in [ 37 ] , where we observed that the
compact representation of a verb in the DisCoCat framework, under the
assumptions presented earlier in this chapter, can be viewed as @xmath
matrices in @xmath . We considered alternatives to the algorithm
presented earlier for the construction of such matrices, and were
surprised by the results of the Kronecker method, wherein we replaced
the matrix learned by our algorithm with the Kronecker product of the
lexical semantic vectors for the verb. Concretely this means that given
the lexical vector @xmath for a transitive verb, built as we would build
noun vectors, we construct the matrix representation @xmath of the verb
as follows:

  -- -------- --
     @xmath   
  -- -------- --

Further analysis performed since the publication of that paper can help
to understand why this method might work. Using the following property
that for any vectors @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

we can see that the Kronecker model’s composition operation can be
expressed as

  -- -------- --
     @xmath   
  -- -------- --

Bearing in mind that the cosine measure we are using as a similarity
metric is equivalent to the inner product of two vectors normalised by
the product of their length

  -- -------- --
     @xmath   
  -- -------- --

and the following property of the inner product of Kronecker products

  -- -------- --
     @xmath   
  -- -------- --

we finally observe that comparing two sentences under Kronecker
corresponds to the following computation:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

where @xmath is the normalisation factor

  -- -------- --
     @xmath   
  -- -------- --

I note here that the Kronecker model is effectively a parallel
application of the multiplicative model presented in @xmath 2.3.2 ,
combining the subject and verb, and object and verb separately. In
short, it more or less constitutes the introduction of some mild
syntactic sensitivity into the multiplicative model of [ 63 ] .

I can generalise this Kronecker model to any relation of any arity,
provided that it takes @xmath inputs and produces @xmath outputs of
matching types (since all I am doing is component-wise multiplication of
reduced representations). The general learning procedure for learning an
@xmath -ary relation using the Kronecker method is as follows:

1.  We learn the lexical vector @xmath for the relation.

2.  We produce its tensor representation @xmath by computing

      -- -------- --
         @xmath   
      -- -------- --

3.  To compose @xmath with its @xmath arguments @xmath we compute:

      -- -------- --
         @xmath   
      -- -------- --

This is, in some sense, a Kronecker product-based generalisation of the
multiplicative model of [ 63 ] , as the case for @xmath is simply
component-wise multiplication.

To conclude this section, let us examine the diagrammatic representation
of composition under the Kronecker model. We saw, in Figure 5.2 , that
reduced representations could be seen as being diagrammatically embedded
inside full representations. Furthermore, these diagrams can be
simplified through yank operations to show the application of some
function @xmath to the Kronecker product of subject and object, where
@xmath corresponds to the component-wise multiplication of Kronecker
product two arguments with the reduced representation of the verb. In
the case of the Kronecker model, this reduced representation is itself
the Kronecker product of the lexical vectors for the verb, which I will
call @xmath . Let a function @xmath be defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

I can therefore express @xmath in terms of @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

which allows us to formulate the diagrammatic representation of
composition under the Kronecker model as shown in Figure 5.4 . The
general form of the Kronecker model can be shown in a similar way. If we
construct the representation of some @xmath -ary relation @xmath
according to the Kronecker model, by taking the Kronecker product of its
lexical vector @xmath @xmath times, then we have:

  -- -------- --
     @xmath   
  -- -------- --

and the composition of @xmath with its @xmath arguments @xmath can be
represented diagrammatically as shown in Figure 5.5 .

## Part III Practice

### Chapter 6 Evaluating a DisCoCat

  Chapter Abstract

Evaluating compositional models of semantics is no easy task: first, we
are trying to evaluate how well the compositional process works; second,
we also are trying to determine how useful the final representation—the
output of the composition—is, relative to our needs.

The scope of this second problem covers most phrase and sentence-level
semantic models, from bag-of-words approaches in information retrieval
to logic-based formal semantic models, via language models used for
machine translation. It is heavily task dependent, in that a
representation that is suitable for machine translation may not be
appropriate for textual inference tasks, and one that is appropriate for
information retrieval may not be ideal for paraphrase detection.
Therefore this aspect of semantic model evaluation ideally should take
the form of application-oriented testing, e.g. to test semantic
representations designed for machine translation purposes, we should use
a machine translation evaluation task.

The DisCoCat framework of [ 13 , 19 ] , described above in Chapter 3 ,
allows for the composition of any words of any syntactic type. The
general learning algorithm presented in @xmath 5.3 technically can be
applied to learn and model relations of any semantic type. However, many
open questions remain, such as how to deal with logical words,
determiners and quantification, and how to reconcile the different
semantic types used for sentences with transitive and intransitive
sentences. I will leave these questions for future work, briefly
discussing some of them in Chapter 7 . In the meantime, we concretely
are left with a way of satisfactorily modelling only simple sentences
without having to answer these bigger questions.

With this in mind, in this chapter I present a series of experiments
centred around evaluating how well various models of semantic vector
composition (along with the one described in Chapter 5 ) perform in a
phrase similarity comparison task. This task aims to test the quality of
a compositional process by determining how well it forms a clear joint
meaning for potentially ambiguous words. The intuition here is that
tokens, on their own, can have several meanings, and that it is through
the compositional process—through giving them context —that we
understand their specific meaning. For example, “bank” itself could
(amongst other meanings) mean a river bank or a financial bank; yet in
the context of a sentence such as “The bank refunded the deposit” it is
likely we are talking about the financial institution.

I will present three datasets designed to evaluate how well word-sense
disambiguation occurs as a by-product of composition. I begin by
describing the first dataset, based around noun-intransitive verb
phrases, in @xmath 6.1 . In @xmath 6.2 , I present a dataset based
around short transitive-verb phrases (a transitive verb with subject and
object). In @xmath 6.3 , I discuss a new dataset, based around short
transitive-verb phrases where the subject and object are qualified by
adjectives. I leave discussion of these results for @xmath 6.4 .

#### 6.1 First Experiment

This first experiment, which is based on that of Mitchell and Lapata [
63 ] , and which I presented in [ 36 ] with Mehrnoosh Sadrzadeh,
evaluates the degree to which an ambiguous intransitive verb
(e.g. “draws”) is disambiguated by combination with its subject.

##### 6.1.1 Dataset Description

The dataset ¹ ¹ 1 Available at
http://homepages.inf.ed.ac.uk/s0453356/results . comprises 120 pairs of
intransitive sentences, each of the form “NOUN VERB”. These sentence
pairs are generated according the following procedure, which will be the
basis for the construction of the other datasets discussed below:

1.  A number of ambiguous intransitive verbs (15, in this case) are
    selected from frequently occurring verbs in the corpus.

2.  For each verb @xmath , two mutually exclusive synonyms @xmath and
    @xmath of the verb are produced, and each is paired with the
    original verb separately (for a total of 30 verb pairs). These are
    generated by taking maximally distant pairs of synonyms of the verb
    on Wordnet (e.g. ), but any method could be used here.

3.  For each pair of verb pairs @xmath and @xmath , two frequently
    occurring nouns @xmath and @xmath are picked from the corpus, one
    for each synonym of @xmath . For example if @xmath is “glow” and the
    synonyms @xmath and @xmath are “beam” and “burn”, we might choose
    “face” as @xmath because a face glowing and a face beaming mean
    roughly the same thing, and “fire” as @xmath because a fire glowing
    and a fire burning mean roughly the same thing.

4.  By combining the nouns with the verb pairs, we form two high
    similarity triplets @xmath and @xmath , and two low similarity
    triplets @xmath and @xmath .

The last two steps can be repeated to form more than four triplets per
pair of verb pairs. In [ 63 ] , eight triplets are generated for each
pair of verb pairs, obtaining a total of 120 triplets from the 15
original verbs. Each triplet, along with its HIGH or LOW classification
(based on the choice of noun for the verb pair) is an entry in the
dataset, and can be read as a pair of sentences: @xmath translates into
the intransitive sentences “ @xmath @xmath ” and “ @xmath @xmath ”.

Finally, the dataset is presented, without the HIGH/LOW ratings, to
human annotators. These annotators are asked to rate the similarity of
meaning of the pairs of sentences in each entry on a scale of 1 (low
similarity) to 7 (high similarity). The final form of the dataset is a
set of lines each containing:

-   A @xmath triplet

-   A HIGH or LOW label for that triplet

-   An annotator identifier and the annotator’s score for that triplet

Sample sentences from this dataset are shown in Table 6.1 .

##### 6.1.2 Evaluation Methodology

This dataset is used to compare various compositional distributional
semantic models according to the following procedure:

1.  For each entry in the dataset, the representation of the two
    sentences “ @xmath @xmath ” and “ @xmath @xmath ”, which we will
    name @xmath and @xmath , formed from the entry triple @xmath is
    constructed by the model.

2.  The similarity of these sentences according to the model’s semantic
    distance measure constitutes the model score for the entry.

3.  The rank correlation of entry model scores against entry annotator
    scores is calculated using Spearman’s rank correlation coefficient
    @xmath .

The Spearman @xmath scores are values between @xmath (inverse rank
correlation) and @xmath (perfect correlation). The higher the @xmath
score, the higher the compositional model can be said to produce
sentence representations that match human understanding of sentence
meaning, when it comes to comparing the meaning of sentences. As such, I
will rank the models evaluated using the task by decreasing order of
@xmath score.

One of the principal appealing features of Spearman’s @xmath is that the
coefficient is rank based: it does not require models’ semantic
similarity metrics to be normalised for a comparison to be made. One
consequence is that a model providing excellent rank correlation with
human scores, but producing model scores on a small scale (e.g.values
between @xmath and @xmath ) will obtain a higher @xmath score than a
model producing models scores on a larger scale (e.g. between @xmath and
@xmath ) but with less perfect rank correlation. If we wished to then
use the former model in a task requiring some greater degree of
numerical separation (let’s say @xmath for non-similar sentences and
@xmath for completely similar sentences), we could simply renormalise
the model scores to fit the scale. By eschewing score normalisation as
an evaluation factor, we minimise the risk of erroneously ranking one
model over another.

Finally, in addition to computing the rank alignment coefficient between
model scores and annotator scores, [ 63 ] calculate the mean models
scores for entries labelled HIGH, and for entries labelled LOW. This
information is reported in their paper as additional means for model
comparison. However, for the same reason I considered Spearman’s @xmath
to be a fair means of model comparison, namely in that it required no
model score normalisation procedure and thus was less likely to
introduce error by adding such a degree of freedom, we consider the
HIGH/LOW means to be inadequate grounds for comparison precisely because
it requires normalised model scores for comparison to be meaningful. As
such, I will not include these mean scores in the presentation of this
experiment’s results, or any further experiments in this Chapter.

##### 6.1.3 Models Compared

In this experiment, I compare the best and worst performing models of [
63 ] to my own. I begin by building a vector space @xmath for all words
in the corpus, using standard distributional semantic model construction
procedures ( @xmath -word window) with the parameters of [ 63 ] .
Specifically, the basis elements of this vector space are the 2000 most
frequently occurring words in the British National Corpus (BNC),
excluding common stop words. For my evaluation, the corpus was
lemmatised using Carroll’s Morpha [ 62 ] , which was applied as a
by-product of my parsing of the BNC with the C&Ctools parser of [ 20 ] .

The context of each occurrence of a word in the corpus was defined to be
five words on either side. After the vector for each word @xmath was
produced, the basis weights @xmath associated with context word @xmath
were normalised by the following ratio of probabilities weighting
scheme:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the total number of context word in the corpus.

Let @xmath and @xmath be the lexical semantic vectors for the verb and
the noun, from @xmath . It should be evident that there is no
significant difference in using @xmath for nouns in lieu of building a
separate vector space @xmath strictly for noun vectors.

As a first baseline, Verb Baseline , I ignored the information provided
by the noun in constructing the sentence representation, effectively
comparing the semantic content of the verbs:

  -- -------- --
     @xmath   
  -- -------- --

The models from [ 63 ] which I evaluate here are those which were
strictly unsupervised (i.e. no free parameters for composition). These
are the additive model Add , wherein

  -- -------- --
     @xmath   
  -- -------- --

and the multiplicative model Multiply , wherein

  -- -------- --
     @xmath   
  -- -------- --

Other models with parameters which must be optimised against a held-out
section the dataset are presented in [ 63 ] . I omitted them here
principally because they do not perform as well as Multiply , but also
because the need to optimise the free parameters for this experiment and
other datasets makes comparison with completely unsupervised models more
difficult, and less fair.

I also evaluate the Categorical model from Chapter 5 here, wherein

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the compact representation of the relation the verb
stands for, computed according to the procedure described in Chapter 5 .
It should be noted that for the case of intransitive verbs, this
composition operation is mathematically equivalent to that of the
Multiply model (as component-wise multiplication @xmath is a commutative
operation), the difference being the learning procedure for the verb
vector.

For all such vector-based models, the similarity of two sentences @xmath
and @xmath is taken to be the cosine similarity of their vectors,
defined in @xmath 2.2 :

  -- -------- --
     @xmath   
  -- -------- --

In addition to these vector-based methods, I define an additional
baseline and an upper-bound. The additional baseline, Bigram Baseline ,
is a bigram-based language model trained on the BNC with SRILM [ 79 ] ,
using the standard language model settings for computing
log-probabilities of bigrams. To determine the semantic similarity of
sentences @xmath and @xmath I assumed sentences have mutually
conditionally independent properties, and computed the joint
probability:

  -- -------- --
     @xmath   
  -- -------- --

The upper bound of the dataset, UpperBound , was taken to be the
inter-annotator agreement: the average of how each annotator’s score
aligns with other annotator scores, using Spearman’s @xmath . The
procedure for this was to take all possible pairs of annotators, measure
their agreement using @xmath , and take the average @xmath across all
such pairs to be the inter-annotator agreement.

##### 6.1.4 Results

The results of the first experiment are shown in Table 6.2 . As expected
from the fact that Multiply and Categorical differ only in how the verb
vector is learned, the results of these two models are virtually
identical, outperforming both baselines and the Additive model by a
significant margin. However, the distance from these models to the upper
bound is even greater, demonstrating that there is still a lot of
progress to be made.

#### 6.2 Second Experiment

This second experiment, which I initially presented in [ 36 ] with
Mehrnoosh Sadrzadeh, is an extension of the first experiment to the case
of sentences centred around transitive verbs, composed with a subject
and an object. The results of the first experiment did not demonstrate
any difference between the multiplicative model, which takes into
account no syntactic information or word ordering, and the syntactically
motivated categorical compositional model. By running the same
experiment over a new dataset, where the relations expressed by the verb
have a higher arity than in the first, I hope to demonstrate that added
structure leads to better results for our syntax-sensitive model.

##### 6.2.1 Dataset Description

The construction procedure for this dataset ² ² 2 Available at
http://www.cs.ox.ac.uk/activities/compdistmeaning/GS2011data.txt . is
almost exactly as for the first dataset, with the following differences:

-   Verbs are transitive instead of intransitive.

-   For each verb pair, I select a set of subject and object nouns to
    use as context, as opposed to just a subject noun.

Lemmatised sentences from sample entries of this dataset are shown in
Table 6.3 .

The dataset was passed to a group of annotators, as was done for the
previous dataset, who scored each pair of sentences on the same scale of
@xmath (not similar in meaning) to @xmath (similar in meaning).

##### 6.2.2 Evaluation Methodology

The methodology for this experiment is exactly that of the previous
experiment. Models compositionally construct sentence representations,
and compare them using a distance metric (all vector-based models once
again used cosine similarity). The rank correlation of model scores with
annotator scores is calculated using Spearman’s @xmath , which is used
in turn to rank models.

##### 6.2.3 Models Compared

The models compared in this experiment are those of the first
experiment, with the addition of an extra trigram-based baseline
(trained with SRILM, using addition of log-probability of a sentence as
a similarity metric), and the Kronecker variation on our categorical
model, previously presented in @xmath 5.3.6 . With @xmath as the
distributional semantic space for all words in the corpus, trained using
the same parameters as in the first experiment, @xmath as the vectors
for subject, verb and object of a sentence, respectively, and with
@xmath as the compact representation of a transitive verb learned using
the algorithm presented in the previous chapter, we have the following
compositional methods:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
              
  -- -------- --

The upper bound UpperBound here is, again, the inter-annotator
agreement.

##### 6.2.4 Results

The results for the second experiment are shown in Table 6.4 . The
baseline scores fall in the @xmath – @xmath range, with best results
being obtained for the Trigram Baseline . The additive model Add
performs comparably to the additive model in the first experiment. This
is most likely due to the fact that, through addition, the semantic
noise caused by the overlapping word senses present in semantic vectors
is increased rather than cancelled out. In other words, information from
the verb vectors (namely relatively low or high values) which might be
used to implicitly discriminate senses through composition is instead
“drowned out” by the information from the subject and object vectors
through summation, rather than being used to provide a more
finely-grained representation. The multiplicative model Multiply
performs on par with the version used for the intransitive experiment,
but obtains a score comparable to the baselines.

The best performing models here are those developed and described in
this thesis. The Categorical model now outperforms both the baselines
and Multiply , obtaining a score of @xmath . Finally, the newly
introduced Kronecker model leads the pack by a steady margin, with a
score of @xmath .

The inter-annotator agreement UpperBound is much higher in this
experiment than in the previous experiment, indicating even more room
for improvement.

#### 6.3 Third Experiment

The third and final experiment I present is a modified version of the
second dataset presented above, where the nouns in each entry are under
the scope of adjectives applied to them. The intuition behind the
datasets presented in @xmath 6.1 and @xmath 6.2 was that ambiguous verbs
are disambiguated through composition with nouns. These nouns themselves
may also be ambiguous, and a good compositional model will be capable of
separating the noise produced by other meanings through its
compositional mechanism to produce less ambiguous phrase
representations. The intuition behind this dataset is similar, in that
adjectives provide both additional information for disambiguation of the
nouns they apply to, but also additional semantic noise. Therefore a
good model will also be able to separate the useful information of the
adjective from its semantic noise when composing it with its argument,
in addition to doing this when composing the noun phrases with the verb.

##### 6.3.1 Dataset Description

The construction procedure for this dataset was to take the dataset from
@xmath 6.2 , and, for each entry, add a pair of adjectives from those
most frequently occurring in the corpus. The first adjective from the
pair is applied to the first noun (subject) of the entry when forming
the sentences, and the second adjective is applied to the second noun
(object).

This new dataset ³ ³ 3 Available at
http://www.cs.ox.ac.uk/activities/compdistmeaning/GS2012data.txt . was
then annotated again by a group of 50 annotators using Amazon’s
Mechanical Turk service, asked to give each sentence pair a meaning
similarity score between @xmath and @xmath , as for the previous
datasets. As a form of quality control, I inserted “gold standard”
sentences in the form of identical sentence pairs and rejected
annotators that did not score these gold standard sentences with a high
score of @xmath or @xmath . Some 94 users returned annotations, of which
I kept 50 according to gold standard tests. I did not apply this “gold
standard” quality test to the second dataset as it was annotated by
friends, and am unaware of whether or not it was applied in the
production of the first dataset, but believe that this can only lead to
the production of higher quality annotations.

Sample sentences from this dataset are shown in Table 6.5 .

##### 6.3.2 Evaluation Methodology

The evaluation methodology in this experiment is identical to that of
the previous experiments.

##### 6.3.3 Models Compared

In this experiment, in lieu of simply comparing compositional models
“across the board”, e.g. using the multiplicative model for both
adjective-noun composition and verb-argument composition, I experimented
with different combinations of models . This evaluation procedure was
chosen because I believe that adjective-noun composition need not
necessarily be the same kind of compositional process as
subject-verb-object composition, but also because different models may
latch onto different semantic features during the compositional process,
and it would be interesting to see what model mixtures work well
together.

Each mixed model has two components: a verb-argument composition model
and an adjective-noun composition model. For verb-argument composition,
I used the three best models from the previous experiment, namely
Multiply , Categorical and Kronecker . For adjective-noun composition I
used three different methods. With @xmath and @xmath being the vectors
for an adjective and a noun in the distributional lexical semantic space
@xmath (built using the same procedure as the previous experiments) and
@xmath being the compact representation in the Categorical model, built
according to the algorithm from @xmath 5.3 , we have the following
models:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

The third model, AdjNoun , is a holistic (non-compositional) model,
wherein the adjective-noun compound was treated as a single token, as
its semantic vector @xmath was learned from the corpus using the same
learning procedure applied to construct other vectors in @xmath . Hence
the model defines adjective-noun “composition” as:

  -- -------- --
     @xmath   
  -- -------- --

In addition to these models, I also evaluated three baselines: Verb
Baseline , Bigram Baseline , and Trigram Baseline . As in previous
experiments, the verb baseline uses the verb vector as a sentence
vector, ignoring the information provided by other words. The bigram and
trigram baselines are calculated from the same language model as used in
the second experiment. In both cases, the log-probability of each
sentence is calculated using SRLIM, and the sum of log-probabilities of
two sentences is used as a similarity measure.

Finally, three addition-based models of sentence formation were
evaluated as well, using basic linear algebraic operations. With

  -- -------- --
     @xmath   
  -- -------- --

these are:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

##### 6.3.4 Results

The results for the third experiment are shown in Table 6.6 . Going
through the combined models, we can notice that in most cases the
results stay the same whether the adjective-noun combination method is
AdjMult or CategoricalAdj . This is because, as was shown in the first
experiment, composition of a unary-relation such as an adjective or
intransitive verb with its sole argument under the categorical model
with reduced representations is mathematically equivalent to the
multiplicative model. The sole difference is the way the adjective or
intransitive verb vector is constructed. I note, however, that with
Categorical as a verb-argument composition method, the CategoricalAdj
outperforms AdjMult by a non-negligible margin ( @xmath vs. @xmath ),
indicating that the difference in learning procedure can lead to
different results depending on what other models it is combined with.

Overall, the best results are obtained for AdjMult+Kronecker ( @xmath )
and CategoricalAdj+Kronecker ( @xmath ). Combinations of the adjective
composition methods with other composition methods than the two listed
above at best matches the best-performing baseline, Verb Baseline . In
all cases, the holistic model AdjNoun provides the worst results.

Finally, I note that the fairly simple model AddMult trails the best
performing models by only a few points.

#### 6.4 Discussion

In this Chapter, I evaluated the framework described in Chapter 3 ,
implemented using the learning procedures described in Chapter 5 , and
variants described earlier in this Chapter against other unsupervised
compositional distributional models. I used non-compositional models and
@xmath -gram language models as baselines. These experiments show that
the concrete categorical model developed here, and the Kronecker
product-based variant presented alongside it, outperform all other
models in each experiment save the first, where they perform on par with
the leading model. As the experiments involved progressively more
syntactically complicated sentences, the increased reliability of the
categorical approaches relative to competing models as sentence
complexity rises seems to indicate that both the Categorical and
Kronecker models successfully leverage the added information provided by
additional terms and syntactic structures. While these experiments
demonstrate that the added structure provided by the categorical passage
from syntax to semantics yields improvements in the quality of semantic
representations obtained through composition, they also reveal that
there is still significant ground to cover in order to approach levels
of performance comparable to that of human annotators.

The third experiment also served to show that using different
combinations of composition operations depending on the syntactic type
of the terms being combined can yield better results, and that some
models combine better than others. Notably, the adjective-noun
combination models AdjMult and CategoricalAdj , despite their
mathematical similarity, produce noticeably different results when
combined with the categorical verb-argument composition operation, while
they perform equally with most other verb-argument composition
operations. Likewise, the combination of additive and multiplicative
models in AddMult performs surprisingly well, despite its simplicity
(although the choice of operations was chosen by hand, and there is no
canonical way of extending this to every sentence structure). We can
conclude that different models combine different semantic aspects more
prominently than others, and that through combination we can obtain
better representations by assuming that different kinds of composition
play on different semantic properties. For example, predicates such as
intersective adjectives often add information to their argument (a red
ball is a ball that is also red), hence it may be no surprise that using
addition in AddMult performs better than across-the-board component-wise
multiplication. This opens the question of how to design models of
composition that systematically select which operations will match the
semantic aspects of the words being combined based on their syntactic
type. This is an open question, which I believe warrants further
investigation.

### Chapter 7 Further Work

  Chapter Abstract

Before concluding this thesis, let us examine some topics which have
constituted further research on this topic, both from my own work and
from work done in collaboration with colleagues. In this chapter, I will
examine three areas in which further developments of the DisCoCat
framework and models stemming from it have occurred, before discussing
how these three areas exemplify and provide a foundation for future
work. In @xmath 7.1 , I will discuss the issues surrounding the
modelling of logical operators within the DisCoCat framework, and what
difficulties our reliance on multilinear maps modelled as tensors gives
rise to. In @xmath 7.2 , I will report new work done on the topic of
integrating sophisticated machine learning methods into the learning
procedures for our semantic representations. In @xmath 7.3 , I will
present the foundations for further syntactic extensions allowing us to
integrate a more expressive grammatical formalism, Combinatory
Categorial Grammar, into the DisCoCat framework. Finally, in @xmath 7.4
, I conclude by showing how these three areas of research set the tone
for future work to be done on the topic of categorical compositional
distributional semantics.

#### 7.1 Distributional Logic

The DisCoCat formalism of [ 19 ] presented in Chapter 3 and the models
derived from it presented in Chapter 5 allow us to represent and learn
syntactically-motivated semantic objects which can be composed to form
distributional sentence representations. The learning algorithms
provided allow us to model a large class of word-types, technically
including logical words present in every day language use, such as
“and”, “or”, and “not”. However, this closed lexical class has a very
specific use in language, which historically has been modelled as
operations in propositional or predicate logical calculus. As such, it
may seem counter-intuitive to learn these from data; in fact, it would
be more reasonable to take the finite class of words used to express
quantification, conjunction, implication, and other logical concepts, as
semantic objects which are designed .

A good question following this observation might therefore be: how can
logic be modelled in a compositional distributional semantic model?
Furthermore, how would logical operators work when applied to non-truth
theoretic representations such as those developed in Chapter 5 ? This
second question is an open problem I leave for future work, but to even
begin to answer it, it would be helpful to see how classical predicate
logic can be simulated within the DisCoCat framework (or something
similar). The hope is that we could use such a simulation as a basis or
inspiration for the development of non-truth theoretic distributional
logic operations in future research. In this section, I discuss how such
a logic might be simulated, based on work presented in [ 34 ] , as well
as the issues faced by our models in doing so.

##### 7.1.1 Distributional Formal Semantics

A popular approach to compositionality in formal semantics is to derive
a formal representation of a phrase from its grammatical structure and a
set of associated rules. I represent the semantics of words as functions
and arguments, and use the grammatical structure to dictate the order
and scope of function application. For example, formal semantic models
in the style of [ 65 ] will associate a semantic rule with each
syntactic rule in a context-free grammar. A simple formal semantic model
is shown in Figure 7.1 .

Following these rules shown, the parse of a simple sentence like ‘angry
dogs chase furry cats’ yields the following interpretation: @xmath .
This model is very simplistic, and typically, a higher order logic such
as a lambda calculus will be used to provide additional structure, but
the key aspect retained by this simple model is that the grammar
dictates the translation from natural language to the functional form.
Generally, in formal semantic models, such functions will be logical
relations and predicates applied to arguments, namely objects from the
logical domain. In this section, I consider a way of defining such
functions as multilinear maps over geometric objects instead. This
framework is generally applicable to other formal semantic models than
that which is presented here.

##### 7.1.2 Tensors as Functions

The bijective correspondence between linear maps and matrices is a well
known property in linear algebra: every linear map @xmath can be encoded
as a @xmath by @xmath matrix @xmath , and conversely every such matrix
encodes a class of linear maps determined by the dimensionality of the
domain and co-domain. The application of a linear map @xmath to a vector
@xmath producing a vector @xmath is equivalent to the matrix
multiplication:

  -- -------- --
     @xmath   
  -- -------- --

This correspondence generalises for multilinear maps to a correlation
between @xmath -ary maps and order @xmath tensors [ 6 , 56 ] .

Tensors are best described as a generalisation of the notion of vectors
and matrices to larger degrees of freedom referred to as tensor orders
(one for vectors, two for matrices). To illustrate this generalisation,
consider how vectors, which are order 1 tensors, may be written as the
weighted superposition (summation) of their basis elements: for some
vector space @xmath with basis @xmath , any vector @xmath can be written

  -- -------- --
     @xmath   
  -- -------- --

where the weights @xmath are elements of the underlying field (e.g.
@xmath ), and thus vectors can be fully described by such a one-index
summation. Likewise, matrices, which are order 2 tensors, can be seen as
a collection of row vectors from some space @xmath with basis @xmath ,
or of column vectors from some space @xmath with basis @xmath . Such a
matrix @xmath is an element of the space @xmath , and can be fully
described by the two index summation:

  -- -------- --
     @xmath   
  -- -------- --

where, once again, @xmath is an element of the underlying field which in
this case is simply the element from the @xmath th row and @xmath th
column of the matrix @xmath , and the basis element @xmath of @xmath is
formed by a pair of basis elements from @xmath and @xmath . The number
of indices (or degrees of freedom) used to fully describe a tensor in
this superposition notation is its order, e.g., an order 3 tensor @xmath
would be described by the superposition of weights @xmath associated
with basis elements @xmath .

Matrix multiplications, inner products, and traces all generalise to
tensors as the non-commutative tensor contraction operation ( @xmath ).
For tensors @xmath and @xmath , with bases @xmath and @xmath , the
tensor contraction of @xmath is calculated:

  -- -------- --
     @xmath   
  -- -------- --

where the resulting tensor is of order equal to two less than the sum of
the orders of the input tensors; the subtraction reflects the
elimination of matching basis elements through summation during
contraction.

For every curried multilinear map @xmath , there is a tensor @xmath
encoding it [ 6 , 56 ] . The application of a curried @xmath -ary map
@xmath to input vectors @xmath to produce output vector @xmath
corresponds to the tensor contraction of the tensor @xmath with the
argument vectors:

  -- -------- --
     @xmath   
  -- -------- --

This isomorphism between tensors (objects) and multilinear maps (maps)
is also known in the quantum information theory literature (e.g. [ 91 ,
10 ] ) as map-state duality , whereby for each map there is a bi-partite
entangled state, with the map and state each forming part of the dual
representation of the same piece of information.

##### 7.1.3 Formal Semantics with Tensors

Using the correspondence between @xmath -ary maps and tensors of order
@xmath discussed in @xmath 7.1.2 , and the correspondence between
function arguments and semantic vectors in standard distributional
semantic models, we can turn any formal semantic model into a
compositional distributional semantic model. This is done by first
running a type inference algorithm on the semantic interpretations of
generative rules and obtaining both basic (i.e. uninferred) and function
(i.e. inferred) types, then assigning to each basic type a vector space
and to each function type a tensor space. Following this, we can
represent arguments by vectors and functions by tensors, and finally, we
can model function application by tensor contraction.

To give an example, in the simple formal semantic model presented in
Figure 7.1 , a type inference algorithm in the vein of [ 48 , 61 ] would
give the following type assignments:

-   It would leave the types of @xmath and @xmath uninferred, as they
    are not treated as functions in any rule; therefore, we treat nouns
    and sentences as vectors in some spaces @xmath and @xmath .

-   Noun phrases are simply assigned the same type as @xmath , hence
    elements of @xmath would also be vectors in @xmath .

-   Verb phrases map noun phrase interpretations to sentence
    interpretations, hence @xmath or, as we have inferred the relevant
    spaces, @xmath .

-   Intransitive verb interpretations are assigned the same types as
    verb phrase interpretations, and hence an intransitive verb “vi” can
    be represented by some tensor @xmath .

-   Transitive verbs are interpreted as maps @xmath , which can be
    expanded to @xmath , giving us, for some transitive verb “vt”, the
    tensor form @xmath .

-   Finally, adjectives are maps of form @xmath , and hence some
    adjective “adj” has the tensor form @xmath .

Putting all this together with tensor contraction ( @xmath ) as function
application, let us examine the computation of a sample sentence “angry
dogs chase furry cats” with interpretation @xmath . This would simply be
the calculation of @xmath , where @xmath and @xmath would be lexical
semantic vectors, @xmath and @xmath would be square matrices, and @xmath
would be an order 3 tensor.

It should be noted that the semantic tensors generated in this framework
are in fact identical to those found in the DisCoCat framework, as
tensor contraction is just iterated application of @xmath maps. To give
an example, let @xmath be tensor in @xmath modelling a multilinear map
@xmath . Let @xmath and @xmath be its arguments. Then the tensor
contraction corresponding to the application of @xmath to its arguments
can be expressed as @xmath maps as follows:

  -- -------- --
     @xmath   
  -- -------- --

The principal (superficial) difference between this framework and
DisCoCat is that tensors always ‘absorb’ arguments on the right through
contraction, forming a linear @xmath -calculus, whereas in DisCoCat,
contraction can happen on the left of a tensor. This difference is
indeed superficial, in that any tensor can be transposed to ‘take
arguments’ on the left or the right, thereby allowing us to produce an
isomorphism between the tensors in a DisCoCat model and those in this
distributional formal semantics framework, provided that the vector
spaces match.

##### 7.1.4 Simulating Simple Predicate Calculi

The approach described above allows us to adapt very simple formal
semantic models to work with tensors in a way similar to the DisCoCat
framework discussed throughout this thesis. Before turning to the
question of how logical connectives and quantifiers might be modelled
using tensors in either the distributional formal semantics described
above, or in the DisCoCat framework, I begin by showing how tensors can
represent model-theoretic predicates from a finite-domain predicate
calculus.

In [ 19 ] , a short example shows how simple predicate logic can be
simulated within the DisCoCat framework by setting @xmath to a boolean
space. This can be done in one of two ways: namely either by setting
@xmath to be a one dimensional space @xmath with basis vector @xmath ,
where the vector @xmath and @xmath ; or by setting @xmath to be a two
dimensional space @xmath with basis @xmath , where:

  -- -- --
        
  -- -- --

To represent a logical model in vector spaces using either of the above
options for boolean space, I consider the following translation
mechanism:

-   I assign to the domain @xmath —the set of objects in our logic—a
    vector space @xmath over @xmath of @xmath dimensions where each
    basis vector of @xmath is in one-to-one correspondence with elements
    of @xmath .

-   An element of @xmath is therefore represented as a one-hot vector in
    @xmath (a sparse unit-length vector with a single non-zero value),
    the single-non null value of which is the weight for the basis
    vector mapped to that element of @xmath .

-   Similarly, a subset of @xmath is a vector of @xmath where those
    elements of @xmath in the subset have @xmath as their corresponding
    basis weights in the vector, and those not in the subset have @xmath
    . Therefore there is a one-to-one correspondence between the vectors
    in @xmath with basis weights of either 0 or 1 and the elements of
    the power set @xmath .

-   Each unary predicate @xmath in the logic is represented in the
    logical model as a set @xmath containing the elements of the domain
    for which the predicate is true. Traditionally, we could view the
    predicate as a function @xmath where:

      -- -- --
            
      -- -- --

    Given that our sentence space is set to a boolean space, using one
    of the forms suggested above (i.e. @xmath or @xmath ), we could
    directly model this function as a tensor in @xmath as discussed in
    @xmath 7.1.3 . However, I suggest a different way of
    distributionally modelling predicates which is more in line with how
    predicates work in a DisCoCat. Let us instead view predicates @xmath
    as functions @xmath , defined as:

      -- -------- --
         @xmath   
      -- -------- --

    Therefore the distributional form of these functions will be tensors
    in @xmath . Through tensor contractions, they map subsets of @xmath
    (elements of @xmath ) to subsets of @xmath containing only those
    objects of the original subset for which @xmath holds (i.e. yielding
    another vector in @xmath ).

-   Finally, @xmath -ary relations @xmath such as verbs are represented
    in a logical model as the set @xmath of @xmath -tuples of elements
    from @xmath for which @xmath holds. Therefore such relations can be
    represented as functions @xmath where:

      -- -- --
            
      -- -- --

    Considering our choice of @xmath as a boolean space, and using the
    process described in @xmath 7.1.3 , we can represent these relations
    distributionally as tensors in @xmath .

To give a concrete example about how predicates and relations (namely
verbs) would work in our setting, let our domain be the individuals John
( @xmath ), Mary ( @xmath ) and Peter ( @xmath ). John and Mary love
each other and love themselves, but are indifferent about Peter. Peter
hates himself and John, but loves Mary. The logical model for this
budding romantic tragedy is as follows:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Distributionally speaking, the elements of the domain will be mapped to
the following one-hot vectors in some three-dimensional space @xmath as
follows:

  -- -- --
        
  -- -- --

The representation of the verbs will depend on our choice of sentence
space @xmath . If we set @xmath , we define the verbs as follows, using
the Kronecker product:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Using the distributivity of @xmath over @xmath we can express this more
compactly as:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

These definitions implicitly set the basis weight for sets of elements
not in @xmath and @xmath to zero.

If we set @xmath we simply have to explicitly state all the element
pairs for which the relation is false, in addition to those pairs for
which it is true. Using the more compact notation used above, this
gives:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Computing the value of a sentence such as “John loves Mary” would then
involve producing the distributional representation of @xmath . In the
distributional setting, this would be equivalent to the following
computation with @xmath , and with @xmath as the tensor contraction
operation:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

In contrast, “Peter hates Mary” would yield the computation:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

which is the origin in @xmath , which I interpret as false. In the case
of false statements, the nature of the computation is a lot less clear
mathematically. In contrast, it is explicit in the case of @xmath , for
which we reproduce both computations initially shown above here. For
“John loves Mary” we can notice very little change, since everybody
loves Mary:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
              
     @xmath   
  -- -------- --

But for “Peter hates Mary” the falsehood of the statement is more
clearly computed:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
              
     @xmath   
  -- -------- --

Let us quickly illustrate how predicates work distributionally: let us
consider a domain with two dogs ( @xmath and @xmath ) and a cat ( @xmath
). One of the dogs is brown, as is the cat. Let @xmath be the set of
dogs, and @xmath the predicate “brown”. I represent these statements in
the model as follows:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Distributionally, I model the domain as a three dimensional vector
space, the set of dogs as a vector

  -- -- --
        
  -- -- --

and the predicate ‘brown’ as a tensor in @xmath

  -- -- --
        
  -- -- --

The set of brown dogs is obtained by computing @xmath , which
distributionally corresponds to applying the tensor @xmath to the vector
representation of @xmath via tensor contraction, as follows:

  -- -- --
        
  -- -- --

Therefore using tensors and vectors, we can effectively simulate a
simple predicate calculus within either the DisCoCat framework or the
formal distributional semantics presented earlier in this chapter.

##### 7.1.5 Logical Operations, and Integrating Non-Linearity

In @xmath 7.1.3 , I presented a way of combining tensor-based
compositional distributional models with simple formal semantic models.
In @xmath 7.1.4 , I showed how a simple predicate logic could be
simulated within such a distributional formal semantics or the DisCoCat
framework discussed throughout this thesis. However, both in the case of
formal semantics and in the natural language statements we wish to model
the semantics of with DisCoCat models, there are not only predicates and
relations, but also logical words such ‘and’, ‘or’ and expressions such
as ‘every’, ‘some’, ‘for all’, which we model logically as logical
connectives ( @xmath , @xmath , @xmath , and so on) and as quantifiers (
@xmath , @xmath ). How are these to be represented within tensor-based
models? In this subsection, I show that the linear maps encoded by
tensors do not prima-facie suffice to simulate all these logical
elements, but that the distributional formal semantics described above
can be enhanced with non-linear operations that fit the bill.

###### Logical Operations

Consider the following additional rules for the formal semantics
presented in Figure 7.1 :

  Syntax                           Semantics
  -------------------------------- -----------
  S @xmath S @xmath and S @xmath   @xmath
  S @xmath S @xmath or S @xmath    @xmath
  S @xmath not S                   @xmath

Can these additional semantic functions, corresponding to conjunction,
disjunction and negation be modelled by linear maps (and thereby
represented as tensors)?

I considered two options for a boolean sentence space, @xmath and @xmath
, as suggested by [ 19 ] . For @xmath it is trivial to show that
negation cannot be modelled by a linear map. Assume there is some linear
map @xmath such that:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

All linear maps @xmath must satisfy

  -- -------- --
     @xmath   
  -- -------- --

so @xmath is not a linear map, and hence no tensor models negation for
@xmath . Similarly, the obvious representations for conjunction and
disjunction, namely

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

where @xmath and @xmath are component-wise minimum and maximum
operators, are not multilinear maps.

For @xmath , [ 19 ] show that the swap matrix

  -- -- --
        
  -- -- --

models negation, which can easily be verified:

  -- -- --
        
        
  -- -- --

I go further here and show that various other logical operators can be
modelled with @xmath . To make talking about order-3 tensors used to
model binary operations easier, I will use the following block matrix
notation for @xmath order-3 tensors @xmath :

  -- -- --
        
  -- -- --

which allows us to express tensor contractions as follows, for some
@xmath :

  -- -- --
        
  -- -- --

or more concretely:

  -- -- --
        
        
  -- -- --

Using this notation, we can define tensors for the following operations:

  -- -- --
        
        
        
        
  -- -- --

The design behind these tensors is not particularly complicated. They
are applied to two arguments: if the first argument is true ( @xmath )
then the left matrix of the block matrix is applied to the second
argument; if the first argument is false ( @xmath ) then the right
matrix of the block matrix is applied to the second argument. It is
fairly trivial, using the truth tables for any logical connective, to
design such partitioned matrices for the tensor representation of any
logical operator in a propositional calculus. For example, for the case
of @xmath , the truth table states that @xmath is true if and only if
@xmath is true and @xmath is true. Therefore if @xmath is true, @xmath
holds the truth value of @xmath , and if @xmath is false, @xmath is
false regardless of the value of @xmath . From this, we design the
tensor for @xmath as follows

  -- -- --
        
  -- -- --

When @xmath is true, the partial application of the tensor to the first
argument should yield an identity matrix, as its application to the
second argument should yield the truth value of @xmath :

  -- -- --
        
  -- -- --

When @xmath is false, the partial application of the tensor to the first
argument should yield a matrix which ignores the truth value of the
second argument, mapping both true and false to false:

  -- -- --
        
  -- -- --

Having established these partial applications, we can verify that the
truth table for @xmath is replicated:

  -- -- --
        
        
        
        
  -- -- --

A similar set of steps can be used to verify the other tensor encodings
of logical connectives specified above.

###### Non-linearity and Quantification

It may therefore seem that for the case where @xmath , logical operators
may be dealt with using only multi-linear maps represented as tensors.
However, when it comes to dealing with quantification, it is not obvious
that a solution using multilinear maps exists.

An intuitive way of modelling universal quantification is as follows:
expressions of the form “All @xmath s are @xmath s” are true if and only
if @xmath , where @xmath and @xmath are the sets of @xmath s and the set
of @xmath s, respectively. We saw earlier that sets of objects of the
domain could be represented as vectors @xmath and @xmath in a vector
space @xmath . The intersection of two such sets can be represented
distributionally using the component-wise minimum function @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Using this, we can define the map @xmath for distributional universal
quantification modelling expressions of the form “All @xmath s are
@xmath s” as follows:

  -- -- --
        
  -- -- --

Existential statements of the form “There exists X” can be modelled
using the function @xmath , which tests whether or not @xmath is empty,
and is defined as follows:

  -- -- --
        
  -- -- --

In both cases, the definitions are given for @xmath , but are adaptable
to @xmath by setting @xmath .

To give a simple example, let us take the domain @xmath , where @xmath
is a black dog, @xmath is a brown dog, and @xmath is a brown cat. Let
the set @xmath of brown things, the set @xmath of cats and @xmath of
dogs be represented in vector form as follows:

  -- -- --
        
  -- -- --

To evaluate a sentence such as “all brown things are dogs” we would
compute @xmath by first computing

  -- -- --
        
  -- -- --

then checking if the result is equal to @xmath , which it is not, and
hence @xmath . We can check if there exist any brown cats by first
computing the intersection of the set of brown things and the set of
cats:

  -- -- --
        
  -- -- --

We then check the size of this vector, which is @xmath . The definition
of @xmath tells us that @xmath .

Neither of the quantification functions defined above are multi-linear,
since a multilinear function must be linear in all arguments. A counter
example for @xmath is to consider the case where @xmath and @xmath are
empty, and multiply their vector representations by non-zero scalar
weights @xmath and @xmath .

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

I observe that the equations above demonstrate that @xmath is not a
multilinear map. This proof holds for @xmath and @xmath .

The proof that @xmath is not a multilinear map is equally trivial.
Assume @xmath is an empty set and @xmath is a non-zero scalar weight:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

This proof holds for @xmath .

To conclude this section, we have seen how the two choices for boolean
sentence space differ when we consider how to ‘implement’ logical
operations as tensors. For @xmath , most operations need be non-linear
maps, which can be defined in our distributional formal semantic models
but not in those constructed within the DisCoCat formalism. For @xmath ,
logical connectives can be modelled as multilinear maps (and therefore
as tensors), and can thus be used in the DisCoCat setting as well as in
the distributional formal semantics model presented in this section,
however the (perhaps naïve) functions used to model basic quantification
were shown to be non-linear maps for both settings. The question of how
to approach quantification and logic in the DisCoCat framework, either
using only multilinear maps or by using some semantic representation
other than @xmath , as well as the question of how to go from simulating
predicate logic with tensors to defining (or learning) logical
connectives for the non-truth theoretic compositional distributional
models described in this thesis (i.e. going from vector encodings of
classical logic to logic for continuous vector space models), are both
difficult and fundamental questions which would merit attention in
future research on this topic.

#### 7.2 Learning Tensors by Multi-Step Linear Regression

In Chapter 5 , I presented learning procedures that could be used to
produce distributional representations of words and relations within the
DisCoCat framework based on information provided by a corpus. A parallel
effort to describe distributional compositionality has been presented in
the work of Marco Baroni, Roberto Zamparelli and colleagues, which I
briefly described in @xmath 2.4.2 .

To quickly repeat how their approach, first presented in [ 5 ] , works:
nouns are vectors in some vector space @xmath , and adjectives are
square matrices in @xmath . Adjective-noun composition is simply
matrix-vector multiplication. In sum, this is exactly how things work in
the DisCoCat models I discussed in Chapter 5 . The novel aspect of their
approach is the learning procedure for adjective matrices. To learn the
matrix for an adjective @xmath , the following steps are taken:

1.  For each instance of the adjective applied to some noun @xmath , the
    vector @xmath is learned using the same procedure used to learn noun
    vectors.

2.  Let @xmath be the set of noun vectors @xmath takes as ‘input’ and
    @xmath be the set of adjective-noun vectors learned in the previous
    steps. Therefore for each input vector in @xmath there is an output
    vector in @xmath .

3.  The matrix for @xmath is learned through linear regression, such
    that the output set @xmath produced by multiplying the matrix for
    @xmath with the elements of @xmath is minimally different from
    @xmath .

This is an interesting machine-learning approach, with a high degree of
parametric freedom: we are free to select the procedure for building the
noun and adjective-noun vectors, the linear-regression algorithm used,
the distance metric used to compare @xmath and @xmath , what sort of
dimensionality reduction techniques are applied, and so on.

It should be noted that this approach can directly be applied to how we
dealt with intransitive verbs in Chapter 5 , since they are also
matrices in @xmath . The question is now: can this learning procedure be
scaled to deal with the higher order tensors used for other constructs,
such as transitive verbs? In [ 35 ] ¹ ¹ 1 I am indebted to my
collaborators Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh and
Marco Baroni for their contribution to this work, and for allowing me to
reproduce the contents. The figure in this section was produced by Marco
Baroni. The parameters for the learning procedure were described by
Georgiana Dinu. , in collaboration with Baroni and his colleagues,
Mehrnoosh Sadrzadeh and I have produced a generalised multi-step linear
regression learning procedure which permits the construction of higher
order semantic tensors for use in DisCoCat models. In this section, I
will first present this procedure before discussing how well tensors
produced using this new learning mechanism performed in the experiments
presented in Chapter 6 .

##### 7.2.1 Multi-Step Linear Regression

Multi-step regression learning is a generalisation of linear regression
learning for tensors of order 3 or higher, as procedures already exist
for tensors of order 1 (lexical semantic vectors) and order 2 (cf. [ 5 ]
). For order 1 tensors, we suggest learning vectors using any standard
lexical semantic vector learning model, and present sample parameters in
@xmath 7.2.2 below. Learning order 2 tensors (matrices) can be treated
as a multivariate multiple regression problem, where the matrix
components are chosen to optimise (in a least squares error sense) the
mapping from training instances of input (argument) and output (composed
expression) vectors. Consider for example the task of estimating the
components of the matrix representing an intransitive verb, that maps
subject vectors to (subject-verb) sentence vectors (Baroni and
Zamparelli discuss the analogous adjective-noun composition case):

  -- -------- --
     @xmath   
  -- -------- --

The weights of the matrix are estimated by least-squares regression from
example pairs of input subject and output sentence vectors directly
extracted from the corpus. For example, the matrix for sing is estimated
from corpus-extracted vectors representing pairs such as @xmath mom ,
mom sings @xmath , @xmath child , child sings @xmath , etc. Note that if
the input and output vectors are @xmath dimensional, we must estimate an
@xmath matrix, each row corresponding to a separate regression problem
(the @xmath -th row vector of the estimated matrix will provide the
weights to linearly combine the input vector components to predict the
@xmath -th output vector component). Regression is a supervised
technique requiring training data. However, as we are extracting the
training data automatically from the corpus, this approach does not
incur an extra knowledge cost with respect to unsupervised methods.

Learning tensors of higher order by linear regression involves iterative
application of the linear regression learning method described above.
The idea is to progressively learn the functions of arity two or higher
encoded by such tensors by recursively learning the partial application
of these functions, thereby reducing the problem to the same
matrix-learning problem as addressed by Baroni and Zamparelli. To start
with an example: the matrix-by-vector operation of [ 5 ] is a special
case of the general tensor-based function application model we are
proposing, where a ‘mono-argumental’ function (intransitive verbs)
corresponds to a order 2 tensor (a matrix). The approach is naturally
extended to bi-argumental functions, such as transitive verbs, where the
verb will be a order 3 tensor to be multiplied first by the object
vector and then by the subject, to return a sentence-representing
vector:

  -- -------- --
     @xmath   
  -- -------- --

The first multiplication of a @xmath tensor by a @xmath -dimensional
vector will return a @xmath -by- @xmath matrix (equivalent to an
intransitive verb, as it should be: both sings and eats meat are VPs
requiring a subject to be saturated). Note that given @xmath
-dimensional input vectors, the @xmath -th @xmath -dimensional vector in
the estimated tensor provides the weights to linearly combine the input
object vector components to predict the @xmath -th output component of
the unsaturated verb-object matrix. The matrix is then multiplied by the
subject vector to obtain a @xmath -dimensional vector representing the
sentence. Again, we estimate the tensor components by linear regression
on input-output examples. In the first stage, we apply linear regression
to obtain examples of semi-saturated matrices representing verb-object
constructions with a specific verb. These matrices are estimated, like
in the intransitive case, from corpus-extracted examples of @xmath
subject, subject-verb-object @xmath pairs. After estimating a suitable
number of such matrices for a variety of objects of the same verb, we
use pairs of corpus-derived object vectors and the corresponding
estimated verb-object matrices as input-output pairs for another
regression, where we estimate the verb tensor components. The estimation
procedure is schematically illustrated for eat in Fig. 7.2 . We first
estimate matrices for the VPs eat-meat , eat-pie etc. by linear
regression on input subject and output sentence vector pairs. We then
estimate the tensor for eat by linear regression with the matrices
estimated in the previous step as output examples, and the vectors for
the corresponding objects as input examples.

We can generalise this learning procedure to functions of arbitrary
arity. Consider an @xmath -ary function @xmath . Let @xmath be the set
of @xmath -tuples @xmath , where @xmath , corresponding to the words
which saturate the first @xmath arguments of @xmath in a corpus. For
each tuple in some set @xmath , let @xmath . Trivially, there is only
one such @xmath —namely @xmath itself—since @xmath (as there are no
arguments of @xmath to saturate for @xmath ). The idea behind multi-step
regression is to learn, at each step, the tensors for functions @xmath
by linear regression over the set of pairs @xmath , where the tensors
@xmath are the expected outcomes of applying @xmath to @xmath and are
learned during the previous step. We bootstrap this algorithm by
learning the vectors in @xmath of the set @xmath by treating the word
which each @xmath models combined with the words of its associated tuple
in @xmath as a single token. We then learn the vector for this token
from the corpus using our preferred distributional semantics method. By
recursively learning the sets of functions from @xmath down to @xmath ,
we obtain smaller and smaller sets of increasingly de-saturated versions
of @xmath , which converge towards @xmath .

To specify how the set of pairs used for recursion is determined, let
there exist a function @xmath which takes the index of a tuple from
@xmath and returns the set of indices from @xmath which denote tuples
identical to the first tuple, excluding the last element:

  -- -------- --
     @xmath   
  -- -------- --

Using this function, the regression set for some @xmath can be defined
as @xmath .

##### 7.2.2 Experiments

We evaluated tensors produced using this multi-step regression method
against the datasets presented in Chapter 6 , producing three new sets
of experimental results. In this section, I will first report the model
parameters initially described by Georgiana Dinu, before reporting the
experimental results that show this new learning procedure to produce
models that perform competitively against other leading models.

###### Source corpus

We extract co-occurrence data from the concatenation of the Web-derived
ukWaC corpus ² ² 2 http://wacky.sslmit.unibo.it/ , a mid-2009 dump of
the English Wikipedia ³ ³ 3 http://en.wikipedia.org and the British
National Corpus ⁴ ⁴ 4 http://www.natcorp.ox.ac.uk/ . The corpus has been
tokenised, POS-tagged and lemmatised with the TreeTagger [ 71 ] and
dependency-parsed with the MaltParser [ 43 ] . It contains about 2.8
billion tokens.

###### Co-occurrence extraction

We collect vector representations for the top 8K most frequent nouns and
4K verbs in the corpus, as well as for the subject-verb (320K) and
subject-verb-object (1.36M) phrases containing one of the verbs to be
used in one of the experiments below and subjects and objects from the
list of top 8K nouns. For all target items, we collect within-sentence
co-occurrences with the top 10K most frequent content words (nouns,
verbs, adjectives and adverbs), save for a stop list of the 300 most
frequent words. We extract co-occurrence statistics at the lemma level,
ignoring inflectional information.

###### Weighting

Following standard practice, raw co-occurrence counts are transformed
into statistically weighted scores. We tested various weighting schemes
of the semantic space on a word similarity task. We used a subset of the
MEN data-set [ 7 ] containing 2000 pairs of words (present in our
vocabulary) together with human-assigned similarity judgments obtained
through crowdsourcing. We observed that non-negative pointwise mutual
information (PMI) and local mutual information (raw frequency count
multiplied by PMI score) generally outperform other weighting schemes by
a large margin, and that PMI in particular works best when combined with
dimensionality reduction by non-negative matrix factorization (described
below). Consequently, we pick PMI weighting for our experiments.

###### Dimensionality reduction

Reducing co-occurrence vectors to lower dimensionality is a common step
in the construction of distributional semantic models. Extensive
evidence suggests that dimensionality reduction does not affect, and
might even improve the quality of lexical semantic vectors [ 55 , 70 ,
72 ] . In our setting, dimensionality reduction is virtually a necessary
step, since working with 10K-dimensional vectors is problematic for the
Regression approach, which requires learning matrices and tensors with
dimensionalities which are quadratic and cubic in the dimensionality of
the input vectors, respectively.

We consider two dimensionality reduction methods, the Singular Value
Decomposition (SVD) and Non-negative Matrix Factorization (NMF). SVD is
the most common technique in distributional semantics, and it was used
by [ 5 ] . NMF is a less commonly adopted method, but it has also been
shown to be an effective dimensionality reduction technique for
distributional semantics [ 24 ] . It has a fundamental advantage from
our point of view: the Multiply and Kronecker composition approaches,
because of their multiplicative nature, cannot be meaningfully applied
to vectors containing negative values. NMF, unlike SVD, produces
non-negative vectors, and thus allows a fair comparison of all
composition methods in the same reduced space. For this reason, we
follow [ 5 ] in performing dimensionality reduction to obtain
300-dimensional representations.

We perform the Singular Value Decomposition of the input matrix @xmath :
@xmath and, like Baroni and Zamparelli and many others, pick the first
@xmath columns of @xmath to obtain reduced representations. Non-negative
Matrix Factorization factorizes a @xmath non-negative matrix @xmath into
two @xmath and @xmath non-negative matrices: @xmath (we normalize the
input matrix to @xmath before applying NMF). We use the Matlab
implementation of the projected gradient algorithm ⁵ ⁵ 5 Available at
http://www.csie.ntu.edu.tw/~cjlin/nmf/ . proposed in [ 57 ] , which
minimizes the squared error of Frobenius norm @xmath . We set @xmath and
we use @xmath as reduced representation of input matrix @xmath . For
both SVD and NMF, the latent dimensions are computed using a “core”
matrix containing nouns and verbs only, subsequently projecting phrase
vectors onto the same space. In this way, the dimensions of the reduced
space do not depend on the ad-hoc choice of phrases required by our
experiments.

By experimenting with the MEN data-set of word similarity judgments (see
above), we found that the performance using distributional semantic
vectors from the original 10K-dimensional space or from the two reduced
spaces is very similar, confirming the hypothesis that dimensionality
reduction does not have a negative impact on the quality of
distributional word representations.

We evaluated this new learning procedure against other models of
composition, most of which were discussed in Chapter 6 . Here are the
ones we used in the new set of experiments.

Verb is a baseline measuring the cosine between the verbs in two
sentences as a proxy for sentence similarity (e.g., similarity of mom
sings and boy dances is approximated by the cosine of sing and dance ).

We adopt the widely used and generally successful multiplicative and
additive models of [ 64 ] and others. Composition with the Multiply and
Add methods is achieved by, respectively, component-wise multiplying and
adding the vectors of the constituents of the sentence we want to
represent. Vectors are normalised before addition, as this has
consistently shown to improve the performance of additive models in
earlier experiments run by Baroni and his colleagues.

In @xmath 5.3.6 , I proposed an alternative implementation of the
general DisCoCat approach to compositional distributional semantics [ 19
] , namely Kronecker . Under this approach, a transitive sentence is a
matrix @xmath derived from:

  -- -------- --
     @xmath   
  -- -------- --

That is, if nouns and verbs live in a @xmath -dimensional space, a
transitive sentence is a @xmath matrix given by the component-wise
multiplication of two Kronecker products: that of the verb vector with
itself and that of the subject and object vectors. I showed, in @xmath
6.2 , that this method outperforms other implementations of the same
formalism and is the current state of the art on the transitive sentence
task, which we also tackle below. For intransitive sentences, the same
approach reduces to the component-wise multiplication of verb and
subject vectors, that is, to the Multiply method.

Training of nouns and verbs under the proposed (multi-step) Regression
model is implemented using Ridge Regression (RR) [ 45 ] . RR, also known
as @xmath regularized regression, is a different approach from the
Partial Least Square Regression (PLSR) method that was used in previous
related work [ 5 , 42 ] to deal with the multicollinearity problem. When
multicollinearity exists, the matrix @xmath ( @xmath here is the input
matrix after dimensionality reduction) becomes nearly singular and the
diagonal elements of @xmath become quite large, which makes the variance
of weights too large. In RR, a positive constant @xmath is added to the
diagonal elements of @xmath to strengthen its non-singularity. Compared
with PLSR, RR has a simpler solution for the learned weight matrix
@xmath and produces competitive results at a faster speed. For each verb
matrix or tensor to be learned, we tuned the parameter @xmath by
generalized cross-validation [ 32 ] . The objective function used for
tuning minimizes least square error when predicting corpus-observed
sentence vectors or intermediate VP matrices; the data sets we evaluate
the models on are not touched during tuning.

The training examples for Regression are constructed by combining the 8K
nouns we have vectors for with any verb in the evaluation sets into
subject-verb-(object) constructions, and extracting the corresponding
vectors from the corpus, where attested (vectors are normalised before
feeding them to the regression routine). We use only example vectors
with at least 10 dimensions with non-zero basis weights before
dimensionality reduction, and we require at least 3 training examples
per regression. For the first experiment (intransitives), these
(untuned) constraints result in an average of 281 training examples per
verb. In the second experiment, in the verb-object matrix estimation
phase, we estimate on average 324 distinct matrices per verb, with an
average of 15 training examples per matrix. In the verb tensor
estimation phase we use all relevant verb-object matrices as training
examples.

###### Experimental Results

We present the results for two experiments testing our set of models in
Table 7.1 . ‘Humans’ is inter-annotator correlation. The
multiplication-based Multiply and Kronecker methods are not well-suited
for the SVD space and their performance is reported in NMF space only.
‘Kronecker’ is only defined for the transitive case, with ‘Multiply’
functioning as its intransitive-case equivalent.

We first tested our set of models against the intransitive verb dataset
presented in @xmath 6.2 , to verify that the regression approach
developed for adjectives worked suitably for intransitive verbs as well.
The results in Table 7.1 LABEL: show that the Regression-based model
achieves the best correlation when applied to SVD space, confirming that
the approach proposed by Baroni and Zamparelli for adjective-noun
constructions can be straightforwardly extended to composition of a verb
with its subject with good empirical results. The Regression model also
achieves good performance in NMF space, where it is comparable to
Multiply. Multiply was found to be the best model by Mitchell and
Lapata, and we confirm their results here (recall that Multiply can also
be seen as the natural extension of Kronecker to the intransitive
setting). The correlations attained by Add and Verb are considerably
lower than those of the other methods.

We then applied our regression method to the task of learning transitive
verbs from a corpus, and tested these representations against the
transitive verb dataset presented in @xmath 6.2 . As the results in
Table 7.1 LABEL: show, the Regression model performs very well again,
better than any other methods in NMF space, and with a further
improvement when SVD is used, similarly to the first experiment. The
Kronecker model is also competitive, confirming the results from @xmath
6.2 . Neither Add nor Verb achieve very good results, although even for
them the correlation with human ratings is significant.

##### 7.2.3 Discussion

The results presented in the previous section show that our iterative
linear regression algorithm outperforms the leading multiplicative
method on intransitive sentence similarity when using SVD (and it is on
par with it when using NMF), and outperforms both the multiplicative
method and the leading Kronecker model in predicting transitive sentence
similarity.

Here again, we saw that Kronecker also performs very well in our
modified experimental setup (although not as well as Regression). The
main advantage of Kronecker over Regression lies in its simplicity:
there is no training involved, all it takes is two vector outer products
and a component-wise multiplication.

While our new regression-based model’s estimation procedure is
considerably more involved than for Kronecker, the model has much to
recommend it, both from a statistical and from a linguistic point of
view. On the statistical side, there are many aspects of the estimation
routine that could be tuned on automatically collected training data,
thus bringing up the Regression model performance. We could, for
example, harvest a larger number of training phrases (not limiting them
to those that contain nouns from the 8K most frequent in the corpus, as
we did), or vice versa limit training to more frequent phrases, whose
vectors are presumably of better quality. Moreover, Ridge Regression is
only one of many estimation techniques that could be tried to come up
with better matrix and tensor weights.

On the linguistic side, the model is clearly motivated as an
instantiation of the vector-space “dual” of classic composition by
function application via the tensor contraction operation, as discussed
earlier in this chapter. Moreover, Regression produces vectors of the
same dimensionality for sentences formed with intransitive and
transitive verbs, whereas for Kronecker, if the former are @xmath
-dimensional vectors, the second are @xmath matrices. Thus, under
Kronecker composition, sentences with intransitive and transitive verbs
are not directly comparable, which is counter-intuitive (being able to
measure the similarity of, say, kids sing and kids sing songs is both
natural and practically useful).

#### 7.3 Further Syntactic Extensions: Combinatory Categorial Grammar

In Chapter 4 , I discussed procedures for extending the DisCoCat
framework to work with other grammatical formalisms, and showed how
Context Free Grammars and Lambek Grammars could be included into the
formalism in such a way. In this section, I sketch the foundations for
extending DisCoCat to work with a more powerful grammatical formalism
called Combinatory Categorial Grammar, a weakly context sensitive
grammar which is widely used and comes with efficient parsing tools such
as the C&C parser of [ 22 ] . I show that at least some aspects of the
grammar can be given categorical semantics, and briefly discuss aspects
which should be the subject of future work, should we wish to fully
integrate variants of CCG into the DisCoCat formalism.

##### 7.3.1 Combinatory Categorial Grammar

Combinatory Categorial Grammars (CCGs) are very similar in spirit—and
notation—to Lambek Grammar, discussed in @xmath 4.3 . They are
categorial, in that every word in a natural language belongs to one or
more syntactic categories; and they are combinatorial in that syntactic
categories act as functions which combine to produce syntactical
analyses of phrases. Because I also talk about category theory, I will
talk of CCG types instead of CCG categories to avoid confusion.

CCG types are generally defined recursively as follows:

-   Let there be a set of atomic types @xmath , @xmath , etc.

-   @xmath is a type if @xmath and @xmath are types.

-   @xmath is a type if @xmath and @xmath are types.

This completes the type definition. So far, it is very similar to the
type definition for Lambek Grammar. However, the backslash @xmath has a
slightly different meaning than the backslash @xmath used for Lambek
grammars, as will become clear below. Generally speaking the two
notations can be used interchangeably if the combination operations are
rewritten appropriately: @xmath is equivalent to writing @xmath , and
vice-versa. These are merely two different notational conventions used
in categorial grammars.

CCGs come equipped with a set of reduction rules. The principal ones
surveyed by [ 30 ] , are as follows:

-   Application:

      -- -------- --
         @xmath   
         @xmath   
      -- -------- --

-   Composition:

      -- -------- --
         @xmath   
         @xmath   
      -- -------- --

-   Type raising:

      -- -------- --
         @xmath   
         @xmath   
      -- -------- --

So far, these rules are almost exactly similar to the combination rules
for Lambek Grammar presented in @xmath 4.3 , with the difference that
@xmath operations become commas and the expressions of the form @xmath
are turned into @xmath in the rules.

Two sets of rules which are not found in Lambek Grammar are the
following:

-   Crossed composition:

      -- -------- --
         @xmath   
         @xmath   
      -- -------- --

-   Substitution:

      -- -------- --
         @xmath   
         @xmath   
         @xmath   
         @xmath   
      -- -------- --

Additionally, both composition and crossed composition have a
generalised form which allows the rule to be applied when @xmath is not
an atomic type.

These rules are used to perform grammatical analysis of sentences as
follows: if there is some type-assignment for each word in the sentence
such that the sequence of types assigned to the sentence reduces to some
sentence type @xmath through the combinatorial rules described above,
then the sentence is grammatical.

There are many additional rules, restrictions and modifications of the
above that are available to generate CCGs with certain properties. For
example, CCGs are generally mildly-context sensitive [ 84 ] , meaning
that they can identify or generate sentence structures which context
free grammars such as CFGs, LGs or pregroup grammars could not. However,
with certain rule restrictions such as removing the unrestricted type
raising rule described above, it can be shown [ 30 ] that some CCGs are
in fact context free.

##### 7.3.2 Categorical Semantics for CCGs

I now turn to the task of trying to represent CCGs as categories. This
section will provide foundations for this task and discuss some of the
issues faced. It does not aim to provide a complete solution, leaving
this for future work.

In @xmath 4.3.2 , we saw that Lambek Grammars could be viewed as
bi-closed monoidal categories. Since CCGs and LGs have some very similar
combination rules, let us first consider how far bi-closed monoidal
categories get us.

We begin by assuming that the categorical representation @xmath of a CCG
is a bi-closed monoidal category, with the following objects:

-   For each atomic type @xmath in the CCG there is some object @xmath
    in @xmath .

-   For each type of the form @xmath in the CCG there is some object
    @xmath in @xmath , where @xmath and @xmath are in @xmath .

-   For each type of the form @xmath in the CCG there is some object
    @xmath in @xmath , where @xmath and @xmath are in @xmath .

-   For each sequence of types @xmath permitted by the CCG, there is an
    object @xmath in @xmath , where @xmath and @xmath are in @xmath .

So far, this is effectively the same set of objects as one would expect
to find in the categorical representation @xmath of a Lambek Grammar.
The same goes for the first set of combination operations. As a
reminder, the biclosed category has the following general morphisms for
any @xmath , @xmath and @xmath in @xmath :

-   @xmath

-   @xmath

-   @xmath for any @xmath

-   @xmath for any @xmath

Using these we can define the first set of composition operations
categorically, as was done in @xmath 4.3.2 :

-    Application operations are simply the evaluation morphisms.

-    Composition: for any pair of objects @xmath and @xmath in the
    category, there is a morphism

      -- -------- --
         @xmath   
      -- -------- --

    Likewise, for any pair of objects @xmath and @xmath in the category,
    there is a morphism

      -- -------- --
         @xmath   
      -- -------- --

-    Type raising morphisms @xmath and @xmath are just the right and
    left currying of the left and right evaluation morphisms:

    -   @xmath

    -   @xmath

Note that the composition morphisms make no assumptions about the
structure of @xmath in the first case and @xmath in the second, thereby
representing both composition and generalised composition at the same
time. The diagrams for these operations are shown in Figures 4.3 – 4.7
of Chapter 4 .

So far, it seems like bi-closed monoidal categories are a suitable
categorical representation for CCGs. However, a problem comes up when we
consider crossed composition and substitution, which have the following
forms:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

It should be fairly clear from the domain and codomains of these
morphisms that bi-closed monoidal categories will most likely not
support these operations using just the evaluation and currying
morphisms provided above. This becomes especially clear when we consider
the simple diagrammatic form of these morphisms, shown in Figure 7.3 for
the @xmath morphisms, and in Figures 7.4 and 7.5 for the @xmath and
@xmath morphisms, respectively.

For the @xmath morphisms in Figure 7.3 we can see that within the
internal structure of the morphisms, there must be some point where the
wires for @xmath and the wire for @xmath cross over each other in order
for the two @xmath wires to form a cup, which is not possible in a
bi-closed monoidal category. For the substitutions morphisms in Figures
7.4 and 7.5 , the problem is a little more subtle: not only must wires
cross for the @xmath wires to form a cup, but there are two @xmath wires
as input and only one as output. This means that there must be some
mechanism within the morphism that takes two wires bearing the same
letter and direction, and produces one wire bearing the same letter and
direction as the other two.

Let us begin by addressing the issue of crossing wires: we modify our
hypothesis about the categorical structure of @xmath to state that it is
a closed monoidal category. Typically a closed monoidal category is
defined identically to a bi-closed monoidal category, except for the
fact that instead of the pair of objects @xmath and @xmath for each
@xmath and @xmath in the category, there is a single object @xmath which
satisfies @xmath . In short, @xmath in the closed monoidal category acts
like both @xmath and @xmath in a bi-closed monoidal category. Here, we
will modify our categorical definition explicitly to turn the bi-closed
category into a closed category by adding the following morphisms for
each @xmath and @xmath :

  -- -------- --
     @xmath   
  -- -------- --

such that @xmath and @xmath (i.e. @xmath and @xmath morphisms are
isomorphisms). We give these morphisms the diagrammatic representation
shown in Figure 7.6 . Using these morphisms, we can diagrammatically
show the inner structure for the @xmath morphisms initially shown in
Figure 7.3 , as demonstrated in Figure 7.7 .

To deal with the substitution morphisms, I suggest borrowing a
particular map from Frobenius Algebras represented in categories as done
in [ 49 ] , defined as follows for any object @xmath in our category:

  -- -------- --
     @xmath   
  -- -------- --

This map effectively works like a cup, cancelling out some of the
information provided as input, but instead of completely eliminating the
input information, it preserves some of it and outputs it.
Diagrammatically, it is represented as shown in Figure 7.8 . Using this
morphism, we can diagrammatically show the inner structure for the
substitution morphisms initially shown in Figures 7.4 and 7.5 , as
demonstrated in Figures 7.9 and 7.10 .

So to summarise what we have seen here, I hypothesise that closed
monoidal categories with an additional set of @xmath morphisms model
those composition operations in CCGs which we discussed here. I say
‘hypothesise’ for two reasons. Firstly, because the inner structure of
the cross composition and substitution morphisms presented here is
obtained through ‘diagrammatic reverse-engineering’, rather than through
any real appeal to the algebraic structure of CCGs (as there is not a
tidy definition of this structure, especially compared to Pregroup
Grammars or Lambek Grammars). As such, future work should seek to
determine whether or not the inner structure of these morphisms
presented here makes sense from an algebraic—or indeed a
linguistic—standpoint to confirm or reject this hypothesis. Secondly,
this is a hypothesis because it does not fully capture all the variants
of CCGs present in the literature. Earlier in this section, I talked
about optional restrictions placed on rules such as composition and
cross composition. Other work in the CCG literature (e.g. [ 4 ] ) goes
further in suggesting that the slashes should be augmented with
compositional modalities, and that restrictions should apply to most
rules based on the modalities of the slashes involved. For both cases,
there is no obvious way of modelling restrictions in closed monoidal
categories. I leave it to further work to determine whether or not this
matters, and if so, how these aspects of CCG can be incorporated into
our categorical discourse.

An additional topic for further work on this subject would be to check
that no issues arise from the introduction of swap morphisms into this
category. In [ 81 , 82 ] , van Benthem shows that associative and
commutative extensions of the Lambek calculus underlying Lambek grammar
recognises/generates all permutation closures of context-free languages,
which makes it unsuitable for syntactic analysis. In practical terms,
this means that the grammar will overgenerate, recognising sentences as
grammatical which are not meant to be recognised as such. This
‘overgenerative’ property also holds for the categorical representation
of CCG presented above, for the case of unrestricted cross-composition
morphisms. Insofar as this is an issue for CCG as a whole, rather than
the categorical representation, I will consider this outside of the
scope of this thesis. However, this strongly supports the case for
further investigation of how to include rule restrictions into the
categorical representation of CCGs.

##### 7.3.3 Defining a Functor

To complete this section, I will briefly present how a functorial
passage from the closed monoidal category @xmath defined here to the
category of vector spaces @xmath can be defined. Because of the
similarity between @xmath and @xmath , we can re-use substantial parts
of the work presented in @xmath 4.3.3 here.

Let @xmath be a functor between @xmath and @xmath . We begin by
assigning each atomic type @xmath in the CCG to a vector space @xmath .
We then define the compound objects as was done in @xmath :

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

The functors mapping those composition operations in CCGs that are also
in LGs are exactly as defined for the case of a functor between @xmath
and @xmath , and are as follows:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

This leaves us only with the task of defining a functorial passage for
the cross composition and substitution morphisms. We begin with the
internal structure of the cross composition morphisms, which we can read
from the diagrams:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

We know how the functorial passage for every element of the above
expressions above is defined, except for the swap morphisms. To complete
the functorial passage for the cross composition morphisms, we must
therefore describe what the swap morphisms correspond to in @xmath . As
@xmath is a symmetric monoidal category, there is an isomorphism @xmath
for all objects @xmath and @xmath in @xmath defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

This is the obvious target for the swap morphisms in @xmath , and hence:

  -- -------- --
     @xmath   
  -- -------- --

Linear algebraically, this morphism of @xmath corresponds to the
permutation of tensor indices. The simplest and most intuitive case to
illustrate this is that of applying @xmath to a matrix in @xmath , which
yields a matrix in @xmath which is the transpose of the original matrix.

So we can rewrite the functorial definition for the cross composition
morphisms as follows:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

With this in mind, we turn to the functorial passage for substitution
morphisms. First for the @xmath morphisms:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Next, for the @xmath morphisms:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

The remaining unknown here is now the functorial passage for @xmath
maps. An interpretation of such a Frobenius operation is provided in [
49 ] , and should suit our needs. In @xmath there are morphisms @xmath
for any object @xmath , defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

So we can define the functorial passage for @xmath maps in @xmath as
follows:

  -- -------- --
     @xmath   
  -- -------- --

Using this, we can rewrite the functorial passage for @xmath morphisms
as follows:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

And the functorial passage for @xmath morphisms as follows:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

This completes the functor definition, and this section.

#### 7.4 Next Steps

Throughout this chapter, we have seen three areas in which further
progress has been established based on the foundations provided by the
earlier parts of this thesis. First, I discussed the difficulties behind
integrating logical aspects of natural language into the framework, and
the problems that arise from our reliance on multilinear maps within the
DisCoCat compositional formalism. Second, I discussed ways in which
machine learning methods could be integrated into the framework to
replace or supplement the learning methods I had initially developed
with Mehrnoosh Sardzadeh. Third, I discussed how grammatical formalisms
with a more complex structure than those previously examined, and with
different linguistic and grammatical properties, could possibly be
integrated into the DisCoCat formalism. To conclude this chapter, I
offer some thoughts on how these three areas can form the foundations
for future work, and what issues other researchers in the field may wish
to examine.

In @xmath 7.1 , I showed how aspects of formal semantics and predicate
logic could be simulated within a tensor-based compositional framework.
In some cases, such as logical connectives, the possibility of modelling
operations using multilinear maps depended on the shape of the sentence
space. While I showed that some sentence spaces (e.g. @xmath ) did allow
multilinear maps to model connectives, it was not obvious how
quantification could be modelled without appealing to non-linear maps.
Researchers wishing to develop this aspect of the DisCoCat framework
would therefore do well to consider how this obstacle may be surpassed
using categorical logic, or by changing the semantic model from @xmath
to some other form of semantic representation which would naturally
allow the integration of non-linearity into the formalism.

A related problem is that of how to model logical operations in
non-truth-theoretic semantic representations, such as those developed in
Chapter 5 . While I have no specific suggestions concerning where to
begin addressing this problem, it should be fairly clear from my attempt
to simulate predicate logic that the nature and structure of the
sentence space @xmath will crucially affect the nature of logical
operations. It is possible to simulate predicate logic precisely because
we can interpret basis elements of the sentence space as truth values;
therefore when considering how logical connectives and quantification
would operate over other kinds of values, we must first consider how to
interpret the elements of the sentence space in linguistically or
logically sensible ways. Arguably, what is lacking in our current
non-truth-theoretic models is such an interpretation, and therefore
researchers interested in further developing this topic may wish to
begin by re-thinking what, precisely, it is that we are representing in
our sentence vectors.

In @xmath 7.2 , I discussed joint work performed with Georgiana Dinu,
Yao-Zhong Zhang, Mehrnoosh Sadrzadeh and Marco Baroni, in which we
showed that sophisticated machine learning methods could be adapted to
learn the semantic representations used in the DisCoCat framework from
data. As a by-product, this approach also addressed the problem present
in the approach presented in Chapter 5 whereby sentences with verbs of
different valency as their head would reside in different sentence
spaces. Continuing to develop a data-driven machine-learning approach to
learning semantic representations should constitute an important future
research direction for this formalism, as we move away from the task of
simply evaluating it against simple experiments, and towards the task of
applying it to concrete problems such as machine translation evaluation
and machine translation itself, paraphrase detection, information
retrieval, sentiment analysis, or any other text-based problems which
would benefit from including deeper semantic knowledge into solutions,
as is being done in a variety of work produced since the writing of this
thesis (e.g. [ 46 , 76 ] ).

Finally, in @xmath 7.3 , I laid down foundations for integrating CCGs
into the DisCoCat formalism, with the hope that fully integrating
variants of CCGs into the formalism will be taken up as a subject for
further work. This is not only useful because of the powerful parsing
tools available for this grammatical formalism, but also for the rich
linguistic aspects (cf. [ 78 ] ) underlying this class of grammars. One
of the main benefits of the categorical approach behind DisCoCat is that
it allows information to pass between syntactic and semantic structures.
Investigating how to exploit the expressive nature of CCGs to represent
more sophisticated linguistic phenomena in our semantic models should be
both interesting and help give us a better idea about exactly what
semantic properties we are modelling, and how our semantic
representations may need to evolve to cater to the linguistic properties
CCGs address.

### Chapter 8 Conclusions

To conclude this thesis, let me begin by summarising the work presented
throughout this document. In Chapter 2 , I provided a critical overview
of the some attempts, new and old, to address the problem of
compositionality in distributional models of semantics. We observed a
tension between the need to produce tractable representations which
could easily be learned and compared, and the need to integrate
syntactic, relational, or knowledge-based information into both our
semantic representations and our compositional process.

In Chapter 3 , I reported a recent effort to produce a general
framework, DisCoCat, within which syntax and semantics are put into
relation in order to produce compositional distributional models of
semantics that would take grammatical structure into account both in the
semantic representation of words, and into how they are composed to
produce representations of sentences. This established the foundation on
which the rest of the thesis was built, with the goal of showing that
this framework could indeed produce high quality, learnable models of
compositional distributional semantics.

In Chapter 4 , I presented various syntactic extensions to the DisCoCat
framework, allowing us to use a wider variety of grammatical formalisms
as part of the creation of compositional distributional models of
semantics. I also described a procedure by which grammatical formalisms
not described in this document could be integrated into the framework,
by giving them a categorical representation and defining a functorial
passage between such a representation and the categorical representation
of whichever semantic model we choose to use.

In Chapter 5 , I presented a learning procedure for the production of
semantic representations to be used within the DisCoCat formalism. This
procedure allows us to develop concrete models from the abstract
framework. I also discussed a reduced representation for semantic
objects used in such models, which allows us to efficiently compute
sentence representations. Finally, I introduced a variant on this
learning procedure for reduced representations, based on the Kronecker
product of lexical vectors, and showed that reduced representations can
generally be viewed as embedded within the full representation of
semantic relations, leading to a reduction in computational complexity
without modifying the mathematical nature of composition operations.

In Chapter 6 , I evaluated the concrete models from the previous chapter
in the context of three phrase similarity detection experiments, against
other leading models discussed earlier in the thesis. I showed that the
models produced by the DisCoCat framework match or outperform competing
models, and that the difference between DisCoCat models and rivals grows
with the complexity of the sentences used in the experiments.

In Chapter 7 , I presented three further extensions to the work done in
the rest of the thesis. I discussed options for integrating logical
operations into distributional compositional semantic models, and
outlined the difficulties and obstacles faced by the DisCoCat framework
in trying to accommodate such operations. I presented a new machine
learning algorithm developed with colleagues, which improved upon
certain aspects of the learning procedures presented earlier in this
thesis. I also discussed foundations for the inclusion of Combinatory
Categorial Grammars into the DisCoCat framework, and surveyed some of
the difficulties faced by trying to fully integrate all variants of such
grammars into our categorical formalism. I concluded the chapter by
providing suggestions as to future directions research on the topic of
categorical compositional distributional models of semantics might take.

Throughout this thesis, I have shown that the DisCoCat framework is not
a theoretical toy example of how category theory can be applied to
problems in linguistics. Indeed, not only can learning procedures be
developed to generate concrete models from it, but the category
theoretic aspects themselves provide powerful tools when it comes to the
expansion of the framework itself. I discussed how such expansions could
take place in the form of integrating new syntactic formalisms into the
framework, but also suggested that the categorical properties of the
framework may provide ways to surpass the limitations of our semantic
representations when it comes to modelling logical aspects of language.
In discussing both its potential for expansion and showing its ability
to admit machine-learning-inspired learning algorithms, I hope to have
demonstrated that the categorical approach to linguistics walks the path
between theory and practice. On these grounds, I am convinced that it
will constitute an important and fascinating area for future research,
as others have already noted [ 47 ] .