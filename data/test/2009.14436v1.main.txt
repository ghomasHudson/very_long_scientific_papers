# Chapter \thechapter Introduction

### 1 Why Online Convex Optimization?

We live in an era that is full of accessible data: the Internet,
different sensors, the consumer and financial markets, and so on. Many
people prefer to first store the entire data set and then process it
together like the classic machine learning algorithms do. But when the
volume of data is too large, such batch processing would fail or be
computationally inefficient due to the large-scale dataset that needs to
be loaded into the memory. In order to understand and analyze the data
quickly and efficiently, we can treat the large-scale dataset as a
stream, so that it can be processed one data point-by-one data point or
in a mini-batch fashion. Online learning is such a popular framework in
doing so, which is more computationally efficient because it does not
require loading the whole dataset. Moreover, it is theoretically
guaranteed to be competitive with the best fixed choice in hindsight
(the batch processing solution).

Besides the advantages for offline data analysis, online learning is
also a natural choice for treating sequential data. The reason is that
it meets the requirement of processing and understanding the data as
quickly as possible.

One important framework of online learning is Online Convex Optimization
(OCO), which considers the case when the objective function is convex.
Featured by the high computational efficiency and proven theoretical
guarantees against different notions of performance, OCO algorithms have
many applications in the areas of online routing [ 35 ] , online
auctions [ 9 ] , as well as online classification and regression [ 19 ]
. More specific applications are: 1. Online linear dynamical system
identification [ 32 ] that updates the identified system parameters
on-the-fly as the sequential observation comes in; 2. Online expert
selection [ 37 , 13 , 14 ] which is about online decision making on the
best expert; 3. Online Principal Component Analysis [ 54 , 55 , 56 , 49
] in picking up the subspace for the sequential data/observation to be
projected into; 4. Online resource allocation [ 58 , 60 , 43 ] to
sequentially allocate budgets or other resources.

Another property that makes the OCO framework unique from any other
framework is that there is no statistical assumptions on how the
data/observation is generated. It can be generated deterministically,
stochastically from a mixture of different fixed distributions, or even
adversarially. This property is preferable when there is no clear
conclusion on what kind of data distribution we should use. Also, it
makes more sense if the data is given by our opponent in an adversarial
manner. Such a game playing perspective [ 2 ] further enables the
application to the adversarial data processing like online portfolio
selection in the stock market [ 36 , 20 ] .

Influenced by the development of convex optimization tools, there are
many advances in the design of the OCO algorithms trying to fulfill
different needs under different considerations and scenarios. One of the
most important considerations is the guaranteed performance against
different types of comparators. For example, for the best fixed
comparator in hindsight (which is suitable in stationary environment),
the guaranteed performance is called static regret [ 65 ] . For a
changing environment, two other types of comparators are usually used.
One is the maximum static regret over any contiguous time interval,
which leads to the adaptive regret performance guarantee [ 33 ] . The
other one is against all comparator sequences in a constrained set,
having performance guarantee named dynamic regret [ 8 , 46 , 63 ] . In
this thesis, we show our contributions to the OCO’s development by
designing algorithms to adapt to the changing environment.

### 2 Motivation

The general motivation for this thesis is to design OCO algorithms to
enable the decision making on-the-fly with better adaptivity to the
changing environments and extend them to online resource allocation.

Tracking the changes of the environments is a key difference between OCO
algorithms and the batch processing based approaches, because sequential
data/observation tends to be shifting over time. However, previous works
on OCO problems are mainly focused on the static regret, a performance
metric well-suited for stationary environments. Since the algorithms
having sub-linear static regret will converge to the single best fixed
solution in hindsight, their claimed performance somehow contradicts the
original tracking goal. In order to be aligned with the tracking
objective, dynamic regret is proposed to let the cumulative loss of OCO
algorithms compete with any sequence of comparators within the
constrained set. Our first part of the thesis is motivated by designing
algorithms to upper bound the dynamic regret for different types of
problem setups.

Tracking the changing environments is usually achieved by running a pool
of algorithms with either different parameters (for upper bounding
dynamic regret) or different starting points (for upper bounding
adaptive regret). Such complex online implementaion is very
time-consuming and not appropriate in some problem setups. Proposing an
efficient and easy-to-implement algorithm is the goal of our second part
of the thesis. In particular, we show that such an algorithm exists and
can be applied to the online Principal Component Analysis (online PCA)
and the online variance minimization. Compared to the mentioned general
adaptive algorithms, our proposed algorithm uses only one update per
time step, while maintains the same adaptive regret theoretical
guarantee as the general adaptive algorithms.

For constrained OCO algorithms, a projection operator is almost
unavoidable. When the constraint set is complex, such operation is very
time-consuming and prevents the algorithms from having a true online
implementation. Our third part of the thesis starts from the question of
how to accelerate the computations. Previous works propose to replace
the true desired projection with an approximate closed-form one, since
closed-form update eliminates any minimization-based computation. The
’downside’ is the possible constraint violation from time to time. The
remedy for it is the guarantee for the constraint satisfaction on
average. However, on-average constraint satisfaction does not lead to
the desired small constraint violation for each time step. To achieve
that, we propose a new algorithm to enforce small constraint violation
not only on average but also for every time step. The idea of the
on-average constraint satisfaction is also applied to online resource
allocation by some previous works. However, such application is only
limited to the budget type resource because of the considered on-average
constraint form. Our second motivation in the third part of the thesis
is to extend our proposed algorithms to have time-dependent dynamic
regret guarantee in order to solve broader online resource allocation
problems.

### 3 Thesis Organization

The thesis is organized as follows:

1.  Chapter Online Convex Optimization in Changing Environments and its
    Application to Resource Allocation discusses related work for Online
    Convex Optimization, that are relevant to the algorithms or problem
    setups we consider in the later chapters.

2.  Chapter Online Convex Optimization in Changing Environments and its
    Application to Resource Allocation is mainly concerned with the
    question of how to enable the decision making on-the-fly with better
    adaptivity to the changing environments. Algorithms equipped with
    static regret performance guarantee are not appropriate due to the
    fixed comparator they converge to. One way to better track the
    changes of the environments is to use dynamic regret, which compares
    the algorithm’s cumulative loss against that incurred by a
    comparison sequence. Inspired by the forgetting factor used in the
    Recursive Least Squares algorithms, we propose a discounted Online
    Newton algorithm to have improved dynamic regret guarantee for both
    exp-concave and strongly convex objectives. Moreover, the trade-off
    between static and dynamic regret is analyzed for both Online
    Least-Squares and its generalization to strongly convex and smooth
    objectives. To obtain more computationally efficient algorithms, we
    also propose a novel gradient descent step size rule for strongly
    convex functions, which recovers the dynamic regret bound described
    above.

3.  Chapter Online Convex Optimization in Changing Environments and its
    Application to Resource Allocation develops an online adaptive
    algorithm for Principal Component Analysis (PCA) and its extension
    of variance minimization under changing environments. The main idea
    is mixing the exponentiated gradient descent with a fixed-share
    step. Compared with the previous algorithms having adaptive or
    dynamic regret guarantee, our algorithm saves the need of running a
    pool of algorithms in parallel, while achieves the same adaptive
    regret performance guarantee.

4.  Chapter Online Convex Optimization in Changing Environments and its
    Application to Resource Allocation contributes to the development of
    the OCO algorithms in achieving fast online computation as well as
    online resource allocation. The projection operator for the
    constrained OCO algorithms is the main bottleneck in preventing the
    algorithms from having a quick update. We propose algorithms to
    approximate the true desired projection with a simpler closed-form
    one at the cost of the constraint violation for some time steps.
    Nevertheless, our proposed algorithms lead to a sub-linear
    cumulative constraint violation to ensure the constraint
    satisfaction on average. It also has mild and bounded single step
    constraint violation. For convex objectives, our results generalize
    existing ones, and for strongly convex objectives we give improved
    regret bounds. Finally, we extend our proposed algorithms’ idea to
    solve the general time-dependent online resource allocation
    problems.

5.  Chapter Online Convex Optimization in Changing Environments and its
    Application to Resource Allocation draws some conclusions for the
    thesis.

### 4 Notation

For the @xmath dimensional vector @xmath , we use @xmath and @xmath to
denote the @xmath -norm and @xmath -norm, respectively. The gradient and
Hessian of the function @xmath at time step @xmath in terms of the
@xmath are denoted as @xmath and @xmath , respectively. In order to
differentiate between the vector at time step @xmath and the @xmath -th
element of it, we sometimes use bold lower-case symbols to denote the
vector. The @xmath -th element of a sequence of vectors at time step
@xmath , @xmath , is denoted by @xmath .

For two probability vectors @xmath , we use @xmath to represent the
relative entropy between them, which is defined as @xmath . @xmath is
the sequence of vectors @xmath , and @xmath is defined to be equal to
@xmath , where @xmath is defined as @xmath . The expected value operator
is denoted by @xmath .

For the matrix @xmath , its transpose is denoted by @xmath and @xmath
denotes the matrix multiplication. The inverse of @xmath is denoted as
@xmath . We use @xmath to represent the induced @xmath norm. For the two
square matrices @xmath and @xmath , @xmath means @xmath is negative
semi-definite, while @xmath means @xmath is positive semi-definite. For
a positive definite matrix, @xmath , let @xmath . The standard inner
product between matrices is given by @xmath . The determinant of a
square matrix, @xmath is denoted by @xmath . We use @xmath to represent
the identity matrix.

The quantum relative entropy between two density matrices ¹ ¹ 1 A
density matrix is a symmetric positive semi-definite matrix with trace
equal to 1. Thus, the eigenvalues of a density matrix form a probability
vector. @xmath and @xmath is defined as @xmath , where @xmath is the
matrix logarithm for symmetric positive definite matrix @xmath (and
@xmath is the matrix exponential).

## Chapter \thechapter Related Work

In this chapter, we do a literature review for the works that are
related to the contents of this thesis as well as some necessary
background and concepts.

As the previous chapter shows, online learning has attracted lots of
researchers to develop different algorithms for many interesting
settings and applications. Some of them are concerned with more
theoretical parts. One particular aspect is deriving lower and upper
bounds for the performance in various problem setups such as the expert
problem [ 37 , 16 , 13 , 14 ] , the general OCO setup [ 65 , 31 , 2 , 1
] , online Reinforcement Learning [ 25 ] , online non-convex
optimization [ 34 , 27 ] , the online bandit problem [ 6 , 3 , 11 ] ,
and so on. Other works apply or extend the existing algorithms to
different scenarios. Besides the ones mentioned in the previous chapter,
other scenarios include online time-series prediction with ARMA/ARIMA [
4 ] , online controller design [ 59 ] , as well as the well-known
classification algorithm AdaBoost [ 26 ] .

Amongst all the techniques and applications mentioned above, Online
Convex Optimization (OCO) is one of the most important unified
frameworks that provides efficient, and theoretically guaranteed
solutions to many problems and helps facilitate the development of
online learning’s theoretical analysis.

This chapter is divided into three sections with the literature review
ranging from classic OCO algorithms to the applications related to this
thesis. More specifically, Section 5 first discusses the basic concepts
and definitions in the OCO framework. It then covers popular OCO
algorithms like Online Gradient Descent and Online Newton’s method, with
different performance guarantees. Section 6 focuses on a specific
problem setup, online Principal Component Analysis (online PCA). It
describes one classic online algorithm as well as some extensions of it
from the literature. Section 7 does the literature review about how the
previous works try to accelerate the online update in the OCO
algorithms. Two different kinds of algorithms are described and
discussed for their pros and cons. Furthermore, the extensions of them
to handle online resource allocation are also included.

### 5 Online Convex Optimization (OCO)

The formula for Online Convex Optimization (OCO) is: at each time step
@xmath , before the true time-dependent convex objective function @xmath
is revealed, we need to make a prediction @xmath from the convex set
@xmath , based on the history of the observations @xmath , @xmath . Then
the value @xmath is the loss suffered due to the lack of the knowledge
of the true objective function @xmath . Our prediction of @xmath is then
updated to include the information of @xmath . This whole process is
repeated until termination. The convex function, @xmath , can be chosen
from the convex function class in an arbitrary, possibly adversarial
manner.

To better understand different OCO algorithms, we first describe the
basic definitions and concepts related to them.

#### 5.1 The Basics of OCO

Since the key to the design of OCO algorithms is the convex optimization
tools, we would like to first discuss some important concepts about the
convex optimization.

A set @xmath is a convex set if @xmath , @xmath such that @xmath , we
have that @xmath .

A function @xmath is convex if @xmath and @xmath , we always have:

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is first-order differentiable, then @xmath is convex if and
only if

  -- -------- --
     @xmath   
  -- -------- --

For second-order differentiable function @xmath , it is convex if and
only if @xmath .

For the non-differentiable convex function @xmath , the above inequality
still holds when we replace the gradient @xmath with any element of the
sub-gradient, @xmath , which is defined as the set of vectors satisfying
the above inequality for all @xmath .

When a convex function @xmath is @xmath -strongly convex, it means
@xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is second-order differentiable, @xmath -strong convexity is
equivalent to @xmath .

Sometimes the convex function @xmath is also @xmath -smooth, which means
its gradient @xmath satisfies the relation

  -- -------- --
     @xmath   
  -- -------- --

which is also equivalent to @xmath .

The projection operator @xmath is defined as @xmath . An important
property of this operator that we use a lot in this thesis is the
Pythagorean theorem, which is listed below for completeness:

###### Theorem \thechapter.1 (Pythagoras, circa 500 BC).

Let @xmath , @xmath be a convex set, and @xmath . Then we have the
following inequality

  -- -------- --
     @xmath   
  -- -------- --

Many OCO algorithms are designed by using the above convex optimization
tools. To measure the effectiveness of these OCO algorithms, one
commonly used metric is called regret . Static regret @xmath is one type
of the regret defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the prediction sequence given by the OCO algorithm,
@xmath is a fixed comparator, and @xmath is called time horizon.

According to [ 15 ] , the solution to the above static regret is called
Hannan consistent if @xmath is sub-linear in @xmath , which means the
prediction sequence will converge to @xmath , the best fixed solution in
hindsight. In order to achieve the useful regret bound, the following
assumptions are required: 1. the gradient @xmath is upper bounded; 2.
the convex constraint set @xmath is compact and bounded.

#### 5.2 Online Gradient Descent

The most classic OCO algorithm designed for convex objective is called
Online Gradient Descent (OGD) proposed by [ 65 ] in 2003. The update
rule after the observation @xmath is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the step size at time step @xmath and we abuse the
subgradient notation when @xmath is not differentiable by denoting it as
@xmath .

Although the above update rule is very simple, which is just doing
gradient descent and then projecting back to the feasible set, it has an
optimal static regret theoretical guarantee. In other words, by setting
@xmath to be equal to @xmath or @xmath , the @xmath can be upper bounded
by @xmath , which meets the lower bound shown in [ 2 ] . Note that when
@xmath is @xmath -strongly convex, @xmath can be upper bounded by @xmath
by having @xmath , which is also optimal [ 31 ] .

According to [ 7 ] , OGD is a special case of Online Mirror Descent
(OMD) when the distance function @xmath is the squared Euclidean one (
@xmath ). For the OMD algorithm, its prediction for the time step @xmath
is updated as

  -- -- --
        
  -- -- --

where @xmath is the Bregman divergence defined as @xmath with @xmath
being the strongly convex differentiable function.

When the constraint set @xmath is specified to the unit simplex
constraint @xmath , the above OMD update rule has closed-form solution
if the Bregman divergence @xmath is replaced by the relative entropy.
The closed-form update is

  -- -------- --
     @xmath   
  -- -------- --

which is called Exponentiated Gradient Descent [ 53 ] .

#### 5.3 Online Newton Step

The OMD and OGD are designed for the general convex objective function.
When the objective function is @xmath -exp-concave, we could use Online
Newton Step (ONS) to further reduce its upper bound from @xmath to
@xmath .

The definition of being @xmath exp-concave is that the function @xmath
is concave. If @xmath is twice differentiable, it can be shown that
@xmath is @xmath -exp-concave if and only if

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . What’s more, class of being exp-concave functions is
broader than the strongly convex class if the gradient is bounded, as
shown in [ 31 , 62 ] .

The update rule of ONS [ 31 ] is described below

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the projection onto @xmath with respect to the norm
induced by @xmath .

#### 5.4 Dynamic OCO

When the goal of the OCO algorithm is to track the changes of the
underlying environments, the classic static regret is not appropriate
anymore. This is because the algorithms achieving sub-linear static
regret only guarantee that the prediction will converge to the single
best fixed solution in hindsight [ 30 , 8 , 63 ] .

In order to better track the changes of the underlying environments,
dynamic regret is proposed to compare the cumulative loss against that
incurred by a comparison sequence, @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The classic OGD [ 65 ] achieves dynamic regret of order @xmath , where
@xmath is a bound on the path length of the comparison sequence:

  -- -------- --
     @xmath   
  -- -------- --

This has been improved to @xmath in [ 63 ] by applying a
meta-optimization over different step sizes.

There are also other ways to bound the dynamic regret including a
variant of path-length [ 30 ] , functional variation [ 8 ] , as well as
gradient variation [ 18 ] .

#### 5.5 Adaptive OCO

Adaptive OCO algorithms are also concerned with how to enable the
algorithms to better track changing environments. Different from the
Dynamic OCO setup, the Adaptive OCO uses a ’different’ performance
metric called adaptive regret defined as the maximum static regret over
any contiguous time interval

  -- -------- --
     @xmath   
  -- -------- --

To upper bound the adaptive regret @xmath , [ 33 ] proposed to run a
pool of OGD or OMD with different step sizes and different starting
points. Compared with the classic OGD or OMD, its running time is
increased by an order of @xmath due to the total number of the parallel
running algorithms is @xmath .

Most recently, [ 13 , 14 ] discovered that for the specific online
expert problem, there is no need to run a pool of algorithms in order to
bound the adaptive regret. Instead, they showed that the same adaptive
regret performance guarantee can be obtained by incorporating the
fixed-share step [ 37 ] into the Exponentiated Gradient update, which
not only reduces the running time by @xmath , but also makes the update
easy to implement.

### 6 Online Principal Component Analysis

The purpose of the online Principal Component Analysis (online PCA) is
to find the underlying subspace for the sequential data/observation to
be projected to [ 56 , 49 ] .

To achieve sub-linear static regret, [ 56 ] extended the idea of
selecting the subset of experts to the subset selection of the subspace.
Due to the eigendecomposition at every time step, the online PCA’s
computational complexity is @xmath , where @xmath is the dimension of
the data/observation. This online PCA idea was used in the online
variance minimization [ 55 ] .

In order to reduce the running time, we need to avoid the
eigendecomposition step in [ 56 ] . [ 41 ] proposed another algorithm
replacing the full eigendecomposition at each time step by the problem
finding @xmath principal components of the current covariance matrix
that is perturbed by Gaussian noise. In this way, the algorithm requires
@xmath per time step with a worse static regret bound, which is off by a
factor of @xmath .

### 7 OCO with Long-term Constraint

Online Convex Optimization (OCO) with long-term constraint is first
proposed by [ 44 ] in 2012, aiming to accelerate the OCO algorithms to
achieve real online computation. The problem it tried to solve is the
high computational complexity of the projection operator step for
constrained OCO algorithms.

To do that, it used a closed-form update to approximate the true desired
projection step at the cost of the constraint violation for some time
steps. Its main goal is still keeping the static regret in a sub-linear
order, but it also aims to make sure that there is no constraint
violation on average. More specifically, it can get @xmath , while the
sum of the constraint functions @xmath is upper bounded by @xmath .

The above result is later improved by [ 39 ] via designing a version
with time-dependent step size, which can have @xmath and @xmath with
@xmath .

Later on, [ 58 ] considered the stochastic version of the problem.
Instead of following update idea in [ 44 , 39 ] , it used the idea in
the stochastic network optimization to handle time-dependent
constraints. Although both the static regret and the long-term
constraint @xmath can be upper bounded by @xmath , it requires a very
strong additional Slater condition, which does not hold for many
problems like equality constraint.

The long-term idea is also extended to do the online resource
allocation. [ 58 ] applied it to the online job scheduling (although not
appropriate as explained in Chapter Online Convex Optimization in
Changing Environments and its Application to Resource Allocation ). [ 43
] used the long-term idea in an online budget allocation problem.
Compared with [ 58 ] , [ 43 ] has a tighter regret guarantee due to the
increasing difficulty in finding a feasible comparator in [ 58 ] .

## Chapter \thechapter Trading-Off Static and Dynamic Regret in Online
Least-Squares and Beyond

In this chapter, we are mainly concerned with online discounted
recursive least-squares and how the discounted factor idea can be used
to derive improved dynamic regret as well as dynamic/static regret
trade-off in different problem setups.

As discussed in the previous chapters, the general procedure for online
learning algorithms is as follows: at each time @xmath , before the true
time-dependent objective function @xmath is revealed, we need to make
the prediction, @xmath , based on the history of the observations @xmath
, @xmath . Then the value of @xmath is the loss suffered due to the lack
of the knowledge for the true objective function @xmath . Our prediction
is then updated to include the information of @xmath . This whole
process is repeated until termination. The functions, @xmath , can be
chosen from a function class in an arbitrary, possibly adversarial
manner.

The performance of an online learning algorithm is typically assessed
using various notions of regret . Static regret , @xmath , measures the
difference between the algorithm’s cumulative loss and the cumulative
loss of the best fixed decision in hindsight [ 15 ] :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a constraint set. For convex functions, variations of
gradient descent achieve static regret of @xmath , while for strongly
convex functions these can be improved to @xmath [ 35 ] . However, when
the underlying environment is changing, due to the fixed comparator [ 37
] the algorithm converges to, static regret is no longer appropriate.

In order to better track the changes of the underlying environments,
dynamic regret is proposed to compare the cumulative loss against that
incurred by a comparison sequence, @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The classic work on online gradient descent [ 65 ] achieves dynamic
regret of order @xmath , where @xmath is a bound on the path length of
the comparison sequence:

  -- -------- --
     @xmath   
  -- -------- --

This has been improved to @xmath in [ 63 ] by applying a
meta-optimization over step sizes.

In works such as [ 46 , 57 ] , it is assumed that @xmath . We denote
that particular version of dynamic regret by:

  -- -------- --
     @xmath   
  -- -------- --

In particular, if @xmath is the corresponding path length:

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

then [ 46 ] shows that for strongly convex functions, @xmath of order
@xmath is obtained by gradient descent. However, as pointed out by [ 63
] , @xmath metric is too pessimistic and unsuitable for stationary
problems, which will result in poor generalization due to the random
perturbation caused by sampling from the same distribution. Thus, a
trade-off between static regret @xmath and dynamic regret @xmath is
desired to maintain the abilities of both generalization to stationary
problem and tracking to the local changes.

Adaptive regret [ 33 ] is another metric when dealing with changing
environments, which is defined as the maximum static regret over any
contiguous time interval. Although it shares the similar goal as the
dynamic regret, their relationship is still an open question.

Closely related to the problem of online learning is adaptive filtering,
in which time series data is predicted using a filter that is designed
from past data [ 51 ] . The performance of adaptive filters is typically
measured in an average case setting under statistical assumptions. One
of the most famous adaptive filtering techniques is recursive least
squares, which bears strong resemblance to the online Newton method of [
31 ] . The work in [ 31 ] proves a static regret bound of @xmath for
online Newton methods, but dynamic regret bounds are not known.

In order to have an algorithm that adapts to non-stationary data, it is
common to use a forgetting factor in recursive least squares. [ 29 ]
analyzed the effect of the forgetting factor in terms of the tracking
error covariance matrix, and [ 64 ] made the tracking error analysis
with the assumptions that the noise is sub-Gaussian and the parameter
follows a drifting model. However, none of the analysis mentioned is
done in terms of the regret, which eliminates any noise assumption. For
the online learning, [ 28 ] analyzed the discounted UCB, which uses the
discounted empirical average as the estimate for the upper confidence
bound. [ 50 ] used the weighted least-squares to update the linear
bandit’s underlying parameter.

This chapter is adapted from the published work [ 62 ] , and we are
mainly concerned with exp-concave and strongly convex objectives. The
following is a summary of the main results:

1.  For exp-concave and strongly convex problems, we propose a
    discounted Online Newton algorithm which generalizes recursive least
    squares with forgetting factors and the original online Newton
    method of [ 31 ] . We show how tuning the forgetting factor can
    achieve a dynamic regret bound of @xmath . This gives a rigorous
    analysis of forgetting factors in recursive least squares and
    improves the bounds described in [ 63 ] . However, this choice
    requires a bound on the path length, @xmath . For an alternative
    choice of forgetting factors, which does not require path length
    knowledge, we can simultaneously bound static regret by @xmath and
    dynamic regret by @xmath . Note that tuning @xmath produces a
    trade-off between static and dynamic regret.

2.  Based on the analysis of discounted recursive least squares, we
    derive a novel step size rule for online gradient descent. Using
    this step size rule for smooth, strongly convex functions we obtain
    a static regret bound of @xmath and a dynamic regret bound against
    @xmath of @xmath . This improves the trade-off obtained in the
    exp-concave case, since static regret or dynamic regret can be made
    small by appropriate choice of @xmath .

3.  We show how the step size rule can be modified further so that
    gradient descent recovers the @xmath dynamic regret bounds obtained
    by discounted Online Newton methods. However, as above, these bounds
    require knowledge of the bound on the path length, @xmath .

4.  Finally, we describe a meta-algorithm, similar to that used in [ 63
    ] , which can recover the @xmath dynamic regret bounds without
    knowledge of @xmath . These bounds are tighter than those in [ 63 ]
    , since they exploit exp-concavity to reduce the loss incurred by
    running an experts algorithm. Furthermore, we give a lower bound for
    the corresponding problems, which matches the obtained upper bound
    for certain range of @xmath .

### 8 Discounted Online Newton Algorithm

As described above, the online Newton algorithm from [ 31 ] strongly
resembles the classic recursive least squares algorithm from adaptive
filtering [ 51 ] . Currently, only the static regret of the online
Newton method is studied. To obtain more adaptive performance,
forgetting factors are often used in recursive least squares. However,
the regret of forgetting factor algorithms has not been analyzed. This
section proposes a class of algorithms that encompasses recursive least
squares with forgetting factors and the online Newton algorithm. We show
how dynamic regret bounds for these methods can be obtained by tuning
the forgetting factor.

First we describe the problem assumptions. Throughout this chapter we
assume that @xmath are convex, differentiable functions, @xmath is a
compact convex set, @xmath for all @xmath , and @xmath for all @xmath .
Without loss of generality, we assume throughout the chapter that @xmath
.

In this section we assume that all of the objective functions, @xmath
are @xmath -exp-concave for some @xmath . This means that @xmath is
concave.

If @xmath is twice differentiable, it can be shown that @xmath is @xmath
-exp-concave if and only if

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

for all @xmath .

For an @xmath -exp-concave function @xmath , Lemma 4.2 of [ 35 ] implies
that the following bound holds for all @xmath and @xmath in @xmath with
@xmath @xmath @xmath :

  -- -------- -- ------
                 
     @xmath      (3a)
  -- -------- -- ------

In some variations on the algorithm, we will require extra conditions on
the function, @xmath . In particular, in one variation we will require
@xmath -strong convexity. As shown in the previous chapter, this means
that there is a number @xmath such that

  -- -------- -- ------
     @xmath      (3b)
  -- -------- -- ------

for all @xmath and @xmath in @xmath . For twice-differentiable
functions, strong convexity implies @xmath -exp-concavity for @xmath on
@xmath .

In another variant, we will require that the following bound holds for
all @xmath and @xmath in @xmath :

  -- -------- -- ------
     @xmath      (3c)
  -- -------- -- ------

This bound does not correspond to a commonly used convexity class, but
it does hold for the important special case of quadratic functions:
@xmath . This fact will be important for analyzing the classic
discounted recursive least-squares algorithm. Note that if @xmath and
@xmath are restricted to compact sets, @xmath can be chosen so that
@xmath is @xmath -exp-concave.

Additionally, the algorithms for strongly convex functions and those
satisfying ( 3c ) will require that the gradients @xmath are @xmath
-Lipschitz for all @xmath (equivalently, @xmath is @xmath -smooth as
discussed in the previous chapter), which means the gradient @xmath
satisfies the relation

  -- -------- --
     @xmath   
  -- -------- --

This smoothness condition is equivalent to @xmath and implies, in
particular, that @xmath .

Given constants @xmath , @xmath , and @xmath .

Let @xmath and @xmath .

for t=1,…,T do

Play @xmath and incur loss @xmath

Observe @xmath and @xmath (if needed)

Update @xmath :

  -- -------- -------- -- -- ------
                             
     @xmath   @xmath         (4a)
     @xmath   @xmath         (4b)
  -- -------- -------- -- -- ------

Update @xmath : @xmath

end for

Algorithm 1 Discounted Online Newton Step

To accommodate these three different cases, we propose Algorithm 1 , in
which @xmath is the projection onto @xmath with respect to the norm
induced by @xmath .

By using Algorithm 1 , the following theorem can be obtained:

###### Theorem \thechapter.1.

Consider the following three cases of Algorithm 1 :

1.  @xmath is @xmath -exp-concave. The algorithm uses @xmath , @xmath ²
    ² 2 The value used here is only for proof simplicity, please see
    Section 12 for more discussion. , and ( 4a ).

2.  @xmath is @xmath -exp-concave and @xmath -strongly convex while
    @xmath is @xmath -Lipschitz. The algorithm uses @xmath , @xmath ,
    and ( 4b ).

3.  @xmath is @xmath -exp-concave and satisfy ( 3c ) while @xmath is
    @xmath -Lipschitz. The algorithm uses @xmath , @xmath , and ( 4b ).

For each of these cases, there are positive constants @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath such that @xmath .

Before proving the theorem, let us first describe some consequences of
it.

###### Corollary \thechapter.1.

Setting @xmath with @xmath leads to the following form:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The first term is bounded as:

  -- -------- --
     @xmath   
  -- -------- --

where the inequality follows from @xmath for @xmath .

The other terms follow by direct calculation. ∎

This corollary guarantees that the static regret is bounded in the order
of @xmath since @xmath in that case. The dynamic regret is of order
@xmath . By choosing @xmath , we are guaranteed that both the static and
dynamic regrets are both sublinear in @xmath as long as @xmath . Also,
small static regret can be obtained by setting @xmath near @xmath .

In the setting of Corollary \thechapter .1 , the algorithm parameters do
not depend on the path length @xmath . Thus, the bounds hold for any
path length, whether or not it is known a priori. The next corollary
shows how tighter bounds could be obtained if knowledge of @xmath were
exploited in choosing the discount factor, @xmath .

###### Corollary \thechapter.2.

Setting @xmath leads to the form:

  -- -------- --
     @xmath   
  -- -------- --

The proof is similar to the proof of Corollary \thechapter .1 .

Note that Corollary \thechapter .2 implies that the discounted Newton
method achieves logarithmic static regret by setting @xmath . This
matches the bounds obtained in [ 31 ] . For positive path lengths
bounded by @xmath , we improve the @xmath dynamic bounds from [ 63 ] .
However, the algorithm above current requires knowing a bound on the
path length, whereas [ 63 ] achieves its bound without knowing the path
length, a priori.

If we view @xmath as the variation budget that @xmath can vary over
@xmath like in [ 8 ] , and use this as a pre-fixed value to allow the
comparator sequence to vary arbitrarily over the set of admissible
comparator sequence @xmath , we can tune @xmath in terms of @xmath .

In order to bound the dynamic regret without knowing a bound on the path
length, the method of [ 63 ] runs a collection of gradient descent
algorithms in parallel with different step sizes and then uses a
meta-optimization [ 15 ] to weight their solutions. In a later section,
we will show how a related meta-optimization over the discount factor
leads to @xmath dynamic regret bounds for unknown @xmath .

For the Algorithm 1 , we need to invert @xmath , which can be achieved
in time @xmath for the Quasi-Newton case in ( 4a ) by utilizing the
matrix inversion lemma. However, for the Full-Newton step ( 4b ), the
inversion requires @xmath time.

##### Proof of Theorem \thechapter.1:

Before proving the theorem, the following observation is helpful.

###### Lemma \thechapter.1.

If @xmath is updated via ( 4a ) then @xmath , while if @xmath is updated
via @xmath , then @xmath .

###### Proof.

First consider the quasi-Newton case. The bound holds at @xmath , so
assume that it holds at time @xmath for @xmath . Then, by induction we
have

  -- -------- --
     @xmath   
  -- -------- --

The full-Newton case is identical, except it uses the bound @xmath .

∎

The generalized Pythagorean theorem implies that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Re-arranging shows that

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

Let @xmath be the upper bound on @xmath from Lemma \thechapter .1 . Then
we can lower bound @xmath by

  -- -------- -------- -- -----
     @xmath   @xmath      
              @xmath      (6)
  -- -------- -------- -- -----

Combining ( 5 ) and ( 6 ) gives

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

Summing over @xmath , dropping the term @xmath , setting @xmath , and
re-arranging gives

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

Now we will see how the choices of @xmath enable the final sum from ( 8
) to cancel the terms from ( 3 ). In Case 1 , we have that @xmath and
the bound from ( 3a ) holds for @xmath . In Case 2 , @xmath . In Case 3
, @xmath . Thus in all cases, @xmath has been chosen so that combining
the appropriate term of ( 3 ) with ( 8 ) gives

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

Now we will bound the first sum of ( 9 ). Note that @xmath . In Case 1 ,
we have that @xmath , while in Cases 2 and 3 , we have that @xmath . So,
in Case 1 , let @xmath and in Cases 2 and 3 , let @xmath . Then in all
cases, we have that

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

Lemma 4.5 of [ 35 ] shows that

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

where @xmath is the dimension of @xmath .

Combining ( 10 ) with ( 11 ), summing, and then using the bound that
@xmath gives,

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (12)
  -- -------- -------- -- ------

Recall that @xmath , where @xmath or @xmath , depending on the case.
Then a more explicit upper bound on ( 12 ) is given by:

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

Combining ( 9 ) and ( 13 ) gives the bound:

  -- -------- --
     @xmath   
  -- -------- --

The desired regret bound can now be found by simplifying the expression
on the right, using the fact that @xmath . ∎

### 9 From Forgetting Factors to a Step Size Rule

In the next few sections, we aim to derive gradient descent rules that
achieve similar static and regret bounds to the discounted Newton
algorithm, without the cost of inverting matrices. We begin by analyzing
the special case of quadratic functions of the form:

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

where @xmath . In this case, we will see that discounted recursive least
squares can be interpreted as online gradient descent with a special
step size rule. We will show how this step size rule achieves a
trade-off between static regret and dynamic regret with the specific
comparison sequence @xmath . For a related analysis of more general
quadratic functions, @xmath , please see the appendix.

Note that the previous section focused on dynamic regret for arbitrary
comparison sequences, @xmath . The analysis techniques in this and the
next section are specialized to comparisons against @xmath , as studied
in works such as [ 46 , 57 ] .

Classic discounted recursive least squares corresponds to Algorithm 1
running with full Newton steps, @xmath , and initial matrix @xmath .
When @xmath is defined as in ( 14 ), we have that @xmath . Thus, the
update rule can be expressed in the following equivalent ways:

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (15a)
              @xmath      (15b)
              @xmath      (15c)
              @xmath      (15d)
  -- -------- -------- -- -------

where @xmath . Note that since @xmath , no projection steps are needed.

The above update is the ubiquitous gradient descent with a changing step
size. The only difference between standard methods is the choice of
@xmath , which will lead to the useful trade-off between dynamic and
static regret.

By using the above update, we can get the relationship between @xmath
and @xmath as the following result:

###### Lemma \thechapter.2.

Let @xmath in Eq.( 14 ). When using the discounted recursive
least-squares update in Eq.( 15 ), we have the following relation:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Since @xmath @xmath @xmath @xmath , for @xmath , we have:

  -- -------- --
     @xmath   
  -- -------- --

∎

Recall from ( 1 ) that the path length of optimizer sequence is denoted
by @xmath . With the help of Lemma \thechapter .2 , we can upper bound
the dynamic regret in the next theorem:

###### Theorem \thechapter.2.

Let @xmath be the solution to @xmath in Eq.( 14 ). When using the
discounted recursive least-squares update in Eq.( 15 ) with @xmath , we
can upper bound the dynamic regret as:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

According to the Mean Value Theorem, there exists a vector @xmath such
that @xmath . For our problem, @xmath . For @xmath , we have:

  -- -------- --
     @xmath   
  -- -------- --

where the second inequality is due to @xmath .

As a result, the norm of the gradient can be upper bounded as @xmath .
Then we have @xmath . Now we could instead upper bound @xmath , which
can be achieved as follows:

  -- -- --
        
  -- -- --

where in the second equality, we substitute the result from Lemma
\thechapter .2 .

From the above inequality, we get

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , we get

  -- -- --
        
  -- -- --

Thus, @xmath . ∎

Theorem \thechapter .2 shows that if we choose the discounted factor
@xmath we obtain a dynamic regret of @xmath . This is a refinement of
the Corollary \thechapter .1 since the bound no longer has the @xmath
term. Thus, the dynamic regret can be made small by choosing a small
@xmath .

In the next theorem, we will show that this carefully chosen @xmath can
also lead to useful static regret, which can give us a trade-off between
them.

###### Theorem \thechapter.3.

Let @xmath be the solution to @xmath . When using the discounted
recursive least-squares update in Eq.( 15 ) with @xmath , we can upper
bound the static regret as:

  -- -------- --
     @xmath   
  -- -------- --

Recall that the algorithm of this section can be interpreted both as a
discounted recursive least squares method, and as a gradient descent
method. As a result, this theorem is actually a direct consequence of
Corollary \thechapter .1 , by setting @xmath . However, we will give a
separate proof, since the techniques extend naturally to the analysis of
more general work on gradient descent methods of the next section.

Before presenting the proof, the following integral bound will be used
in a few places.

###### Lemma \thechapter.3.

If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

∎

##### Proof of Theorem \thechapter.3:

###### Proof.

To proceed, recall that the update in Eq.( 15 ) is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Then we get the relationship between @xmath and @xmath as:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Moreover, we write @xmath as @xmath , which combined with the previous
equation gives us the following equation:

  -- -------- --
     @xmath   
  -- -------- --

where the inequality is due to @xmath as shown in Theorem \thechapter .2
.

Sum the above inequality from @xmath to @xmath , we get:

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , @xmath , @xmath . Then for the static regret, we have:

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

Now we will use the integral bound from Lemma \thechapter .3 to bound
the regret. Since @xmath , @xmath . Since @xmath , @xmath . Thus, we
have @xmath . Then @xmath , which results in @xmath . ∎

Theorems \thechapter .2 and \thechapter .3 build a trade-off between
dynamic and static regret by the carefully chosen discounted factor
@xmath . Compared with the result from the last section, there are two
improvements: 1. The two regrets are decoupled so that we could reduce
the @xmath to make the dynamic regret result smaller than bound from
Corollary \thechapter .1 ; 2. The update is the first-order gradient
descent, which is computationally more efficient than second order
methods.

In the next section, we will consider the strongly convex and smooth
case, whose result is inspired by this section’s analysis.

### 10 Online Gradient Descent for Smooth, Strongly Convex Problems

In this section, we generalize the results of the previous section idea
to functions which are @xmath -strongly convex and @xmath -smooth. We
will see that similar bounds on @xmath and @xmath can be obtained.

Our proposed update rule for the prediction @xmath at time step @xmath
is:

  -- -- -- ------
           (17)
  -- -- -- ------

where @xmath and @xmath .

This update rule generalizes the step size rule from the last section.

Before getting to the dynamic regret, we will first derive the relation
between @xmath and @xmath to try to mimic the result in Lemma
\thechapter .2 of the quadratic case:

###### Lemma \thechapter.4.

Let @xmath be the solution to @xmath which is strongly convex and
smooth. When we use the update in Eq.( 17 ), the following relation is
obtained:

  -- -------- --
     @xmath   
  -- -------- --

Since the idea is similar to the proof of Lemma \thechapter .2 , please
refer to the appendix for the proof.

Following the idea of Theorem \thechapter .2 , now we are ready to
present the dynamic regret result:

###### Theorem \thechapter.4.

Let @xmath be the solution to @xmath . When using the update in Eq.( 17
) with @xmath , we can upper bound the dynamic regret:

  -- -------- --
     @xmath   
  -- -------- --

Since the proof follows the similar steps in the proof of Theorem
\thechapter .3 , please refer to the appendix.

Theorem \thechapter .4 ’s result seems promising in achieving the
trade-off, since it has a similar form of the result from quadratic
problems in Theorem \thechapter .2 . Next, we will present the static
regret result, which assures that the desired trade-off can be obtained.

###### Theorem \thechapter.5.

Let @xmath be the solution to @xmath . When using the update in Eq.( 17
) with @xmath , we can upper bound the static regret:

  -- -------- --
     @xmath   
  -- -------- --

The proof follows the similar steps in the proof of Theorem \thechapter
.3 . Please refer to the appendix.

The regret bounds of this section are similar to those obtained for
simple quadratics. Thus, this gradient descent rule maintains all of the
advantages over the discounted Newton method that were described in the
previous section.

### 11 Online Gradient Descent for Strongly Convex Problems

In this section, we extend step size idea from previous section to
problems which are @xmath -strongly convex, but not necessarily smooth.
We obtain the same order dynamic regret as the discounted online Newton
method: @xmath @xmath @xmath . However, our analysis does not lead to
the clean trade-off of @xmath and @xmath obtained when smoothness is
also used.

The update rule is online gradient descent:

  -- -- -- ------
           (18)
  -- -- -- ------

where @xmath , and @xmath .

We can see that the update rule is the same as the one in Eq.( 17 )
while the step size @xmath is replaced with @xmath .

By using the new step size with the update rule in Eq.( 18 ), we can
obtain the following dynamic regret bound:

###### Theorem \thechapter.6.

If using the update rule in Eq.( 18 ) with @xmath and @xmath , the
following dynamic regret can be obtained:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

According to the non-expansive property of the projection operator and
the update rule in Eq.( 18 ), we have

  -- -------- --
     @xmath   
  -- -------- --

The reformulation gives us

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

Moreover, from the strong convexity, we have @xmath , which is
equivalent to @xmath . Combined with Eq.( 19 ), we have

  -- -- -- ------
           (20)
  -- -- -- ------

Then we can lower bound @xmath by

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

Combining ( 20 ) and ( 21 ) gives

  -- -- --
        
  -- -- --

Summing over @xmath from @xmath to @xmath , dropping the term @xmath ,
setting @xmath , using the inequality @xmath , and re-arranging gives

  -- -------- --
     @xmath   
  -- -------- --

where for the second inequality, we use the following results: @xmath ,
@xmath , @xmath , and the definition of @xmath .

∎

Similar to the case of discounted online Newton methods, if a bound on
the path length, @xmath , is known, the discount factor can be tuned to
achieve low dynamic regret:

###### Corollary \thechapter.3.

By setting @xmath , the following bound can be obtained:

  -- -------- --
     @xmath   
  -- -------- --

This result is tighter than the @xmath bound obtained by [ 63 ] on
convex functions, but not directly comparable to the @xmath bounds
obtained in [ 46 ] for smooth, strongly convex functions.

Similar to the Corollary \thechapter .2 on discounted online Newton
methods, Corollary \thechapter .3 requires knowing @xmath . In the next
section, we will see how a meta-algorithm can be used to obtain the same
bounds without knowing @xmath .

Please refer to the appendix for the proof of Corollary \thechapter .3 .

### 12 Meta-algorithm

In previous sections, we discussed the results on dynamic regret for
both @xmath -exp-concave and @xmath -strongly convex objectives. The
tightest regret bounds were obtained by choosing a discount factor that
depends on @xmath , a bound on the path length. In this section, we
solve this issue by running multiple algorithms in parallel with
different discount factors.

For online convex optimization, a similar meta-algorithm has been used
by [ 63 ] to search over step sizes. However, the method of [ 63 ]
cannot be used directly in either the @xmath -exp-concave or @xmath
-strongly convex case due to the added @xmath regret from running
multiple algorithms. In order to remove this factor, we exploit the
exp-concavity in the experts algorithm, as in Chapter 3 in [ 15 ] .

In this section, we will show that by using appropriate parameters and
analysis designed specifically for our cases, the meta-algorithm can be
used to solve our issues.

Given step size @xmath , and a set @xmath containing discount factors
for each algorithm.

Activate a set of algorithms @xmath by calling Algorithm 1 (exp-concave
case) or the update in Eq.( 18 ) (strongly convex case) for each
parameter @xmath .

Sort @xmath in descending order @xmath , and set @xmath with @xmath .

for t=1,…,T do

Obtain @xmath from each algorithm @xmath .

Play @xmath , and incur loss @xmath for each @xmath .

Update @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Send back the gradient @xmath for each algorithm @xmath .

end for

Algorithm 2 Meta-Algorithm

#### 12.1 Exp-concave Case

Before showing the regret result, we first show that the cumulative loss
of the meta-algorithm is comparable to all @xmath :

###### Lemma \thechapter.5.

If @xmath is @xmath -exp-concave and @xmath , the cumulative loss
difference of Algorithm 2 for any @xmath is bounded as:

  -- -------- --
     @xmath   
  -- -------- --

This result shows how @xmath regret incurred by running an experts
algorithm is reduced in the @xmath -exp-concave case. The result is
similar to Proposition 3.1 of [ 15 ] . We also provide a proof in the
appendix.

Based on the above lemma, if we can show that there exists an algorithm
@xmath , which can bound the regret @xmath , then we can combine these
two results and show that the regret holds for @xmath as well:

###### Theorem \thechapter.7.

For any comparator sequence @xmath , setting @xmath with @xmath where
@xmath , @xmath , and @xmath leads to the result:

  -- -------- --
     @xmath   
  -- -------- --

As described previously, the proof’s main idea is to show that we could
both find an algorithm @xmath bounding the regret @xmath and cover the
@xmath with @xmath different @xmath choices. Please see the appendix for
the formal proof.

In practice, we include the additional case when @xmath to make the
overall algorithm explicitly balance the static regret. Also, the free
parameter @xmath used in Algorithm 1 is important for the actual
performance. If it is too small, the update will be easily effected by
the gradient to have high generalization error. In practice, it can be
set to be equal to @xmath or @xmath with @xmath like in [ 35 ] .

#### 12.2 Strongly Convex Case

For the strongly convex problem, since the parameter @xmath used in
Corollary \thechapter .3 is the same as the one in Corollary \thechapter
.2 , it seems likely that the meta-algorithm should work with the same
setup in as Theorem \thechapter .7 . The only parameter that needs to be
changed is @xmath , which was set above to @xmath , the parameter of
@xmath -exp-concavity.

To proceed, we first show that the @xmath -strongly convex function with
bounded gradient is also @xmath -exp-concave (e.g., @xmath @xmath @xmath
). Previous works also pointed out this, but their statement only works
when @xmath is second-order differentiable, while our result is true
when @xmath is first-order differentiable.

###### Lemma \thechapter.6.

For the @xmath -strongly convex function @xmath with @xmath , it is also
@xmath -exp-concave with @xmath .

Please refer to the appendix for the proof.

Lemma \thechapter .6 indicates that running Algorithm 2 with strongly
convex function leads to the same result as in Lemma \thechapter .5 .
Thus, using the similar idea as discussed in the case of @xmath
-exp-concavity and Algorithm 2 , the theorem below can be obtained:

###### Theorem \thechapter.8.

For any comparator sequence @xmath , setting @xmath with @xmath where
@xmath , @xmath , and @xmath leads to the result:

  -- -------- --
     @xmath   
  -- -------- --

Since the proof shares the same idea as Theorem \thechapter .7 , please
refer to the appendix for the proof.

As discussed in the previous subsection, in practice, we also include
the case when @xmath to make the overall algorithm explicitly balance
the static regret and set @xmath accordingly as in the exp-concave case.

#### 12.3 A Lower Bound

In the previous subsections, we show how to achieve the improved dynamic
regret for both the exp-concave and strongly convex problems without
knowing @xmath . In this subsection, we will give a lower bound, which
approaches the upper bound for large and small @xmath .

###### Proposition \thechapter.1.

For losses of the form @xmath , for all @xmath and all @xmath , there is
a comparison sequence @xmath such that @xmath and

  -- -------- --
     @xmath   
  -- -------- --

The above result has the following indications: 1. For @xmath but
approaching to @xmath , it is impossible to achieve better bound of
@xmath with @xmath . 2. For other ranges of @xmath like @xmath , its
lower bound is not established and still an open question.

##### Proof of Proposition \thechapter.1:

###### Proof.

Since strongly convex problem with bounded gradient is also exp-concave
due to Lemma \thechapter .6 shown in the next section, we will only
consider the strongly convex problem.

For the case when @xmath , @xmath reduces to the static regret @xmath ,
which has the lower bound @xmath as shown in [ 2 ] .

Let us now consider the case when @xmath . The analysis is inspired by [
57 ] . We will use @xmath as the special case to show the lower bound.
Here @xmath is a sequence of independently generated random variables
from @xmath with equal probabilities. For the dynamic regret @xmath ,
where @xmath , and @xmath . As a result, the expected value of @xmath is
@xmath @xmath @xmath @xmath @xmath @xmath @xmath . This implies that
@xmath . For the path length, @xmath . Let us set @xmath and @xmath .
Then @xmath and @xmath @xmath @xmath . Then @xmath @xmath @xmath @xmath
. In other words, @xmath , @xmath with @xmath .

In summary, we have that there always exist a exist a sequence of loss
functions @xmath and a comparison sequence @xmath such that @xmath and
@xmath .

∎

### 13 Conclusion

In this chapter, we propose a discounted online Newton algorithm that
generalizes recursive least squares with forgetting factors and existing
online Newton methods. We prove a dynamic regret bound @xmath which
provides a rigorous analysis of forgetting factor algorithms. In the
special case of simple quadratic functions, we demonstrate that the
discounted Newton method reduces to a gradient descent algorithm with a
particular step size rule. We show how this step size rule can be
generalized to apply to strongly convex functions, giving a
substantially more computationally efficient algorithm than the
discounted online Newton method, while recovering the dynamic regret
guarantees. The strongest regret guarantees depend on knowledge of the
path length, @xmath . We show how to use a meta-algorithm that optimizes
over discount factors to obtain the same regret guarantees without
knowledge of @xmath as well as a lower bound which matches the obtained
upper bound for certain range of @xmath . Finally, when the functions
are smooth we show how this new gradient descent method enables a static
regret of @xmath and @xmath , where @xmath is a user-specified trade-off
parameter.

## Chapter \thechapter Online Adaptive Principal Component Analysis and
Its extensions

In the previous chapter, we discussed several dynamic/static regret
results under changing environments, including the online least-squares
and its extension to the exp-concave and strongly convex problem setups.
In this chapter, we are mainly concerned with the problem of online
Principal Component Analysis (online PCA) under changing environments.

As discussed in Chapter Online Convex Optimization in Changing
Environments and its Application to Resource Allocation , previous
online PCA algorithms are based on either online gradient or matrix
exponentiated gradient descent [ 54 , 55 , 56 , 49 ] . These works bound
online PCA by the static regret, which, as argued in previous chapters,
is not appropriate for changing environments. To have better adaptivity
to the changing environments, previous works proposed to run a pool of
algorithms with either different parameters like in [ 63 , 62 ] (for
upper bounding dynamic regret) or different starting points like in [ 33
] (for upper bounding adaptive regret). This not only requires complex
implementation, but also increases the computational complexity per step
by a factor of @xmath due to the parallel running of different
algorithms.

It is thus desired to have an efficient and easy-to-implement algorithm,
which can eliminate the need of running multiple algorithms while having
the same theoretical guarantee. This chapter introduces such an
efficient algorithm for the specific online PCA problem under adaptive
regret measure, which is adapted from the published work [ 61 ] . The
proposed method mixes the randomized algorithm from [ 56 ] with a
fixed-share step [ 37 ] . This is inspired by the work of [ 13 , 14 ] ,
which shows that the Hedge algorithm [ 26 ] together with a fixed-share
step provides low regret under a variety of measures, including adaptive
regret.

Furthermore, we extend the idea of the additional fixed-share step to
the online adaptive variance minimization in two different parameter
spaces: the space of unit vectors and the simplex. In Section 18 , we
also do the experiments to test our algorithm’s effectiveness. In
particular, we show that our proposed algorithm can adapt to the
changing environments faster than the previous online PCA algorithm.

### 14 Problem Formulation

The goal of the PCA (uncentered) algorithm is to find a rank @xmath
projection matrix @xmath that minimizes the compression loss: @xmath .
In this case, @xmath must be a symmetric positive semi-definite matrix
with only @xmath non-zero eigenvalues which are all equal to 1.

In online PCA, the data points come in a stream. At each time @xmath ,
the algorithm first chooses a projection matrix @xmath with rank @xmath
, then the data point @xmath is revealed, and a compression loss of
@xmath is incurred.

The online PCA algorithm [ 56 ] aims to minimize the static regret
@xmath ,which is the difference between the total expected compression
loss and the loss of the best projection matrix @xmath chosen in
hindsight:

  -- -- -- ------
           (22)
  -- -- -- ------

The algorithm from [ 56 ] is randomized and the expectation is taken
over the distribution of @xmath matrices. The matrix @xmath is the
solution to the following optimization problem with @xmath being the set
of rank- @xmath projection matrices:

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

Algorithms that minimize static regret will converge to @xmath , which
is the best projection for the entire data set. However, in many
scenarios the data generating process changes over time. In this case, a
solution that adapts to changes in the data set may be desirable. To
model environmental variation, several notions of dynamically varying
regret have been proposed [ 37 , 33 , 13 ] . In this chapter, we study
adaptive regret @xmath from [ 33 ] , which results in the following
online adaptive PCA problem:

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

In the next few sections, we will present an algorithm that achieves low
adaptive regret.

### 15 Learning the Adaptive Best Subset of Experts

1: Input: @xmath and an initial probability vector @xmath .

2: for @xmath to @xmath do

3: Use Algorithm 4 with input @xmath to decompose @xmath into @xmath ,
which is a convex combination of at most @xmath corners of @xmath .

4: Randomly select a corner @xmath with associated probability @xmath .

5: Use the k components with zero entries in the drawn corner @xmath as
the selected subset of experts.

6: Receive loss vector @xmath .

7: Update @xmath as:

  -- -------- -- -------
                 
     @xmath      (25a)
     @xmath      (25b)
     @xmath      (25c)
  -- -------- -- -------

where @xmath calls Algorithm 5 .

8: end for

Algorithm 3 Adaptive Best Subset of Experts

1: Input: @xmath and @xmath .

2: repeat

3: Let @xmath be a corner for a subset of @xmath non-zero components of
@xmath that includes all components of @xmath equal to @xmath .

4: Let @xmath be the smallest of the @xmath chosen components of @xmath
and @xmath be the largest value of the remaining @xmath components.

5: update @xmath as @xmath and Output @xmath and @xmath .

6: until @xmath

Algorithm 4 Mixture Decomposition [ 56 ]

1: Input: probability vector @xmath and set size @xmath .

2: Let @xmath index the vector in decreasing order, that is, @xmath .

3: if @xmath then

4: return @xmath .

5: end if

6: @xmath .

7: repeat

8: (* Set first @xmath largest components to @xmath and normalize the
rest to @xmath *)

9: @xmath , @xmath , for @xmath .

10: @xmath , for @xmath .

11: @xmath .

12: until @xmath .

Algorithm 5 Capping Algorithm [ 56 ]

In [ 56 ] , it was shown that online PCA can be viewed as an extension
of a simpler problem known as the best subset of experts problem. In
particular, they first propose an online algorithm to solve the best
subset of experts problem, and then they show how to modify the
algorithm to solve PCA problems. In this section, we show how the
addition of a fixed-share step [ 37 , 13 ] can lead to an algorithm for
an adaptive variant of the best subset of experts problem. Then we will
show how to extend the resulting algorithm to PCA problems.

The adaptive best subset of experts problem can be described as follows:
we have @xmath experts making decisions at each time @xmath . Before
revealing the loss vector @xmath associated with the experts’ decisions
at time @xmath , we select a subset of experts of size @xmath
(represented by vector @xmath ) to try to minimize the adaptive regret
defined as:

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

Here, the expectation is taken over the probability distribution of
@xmath . Both @xmath and @xmath are in @xmath which denotes the vector
set with only @xmath non-zero elements equal to 1.

Similar to the static regret case from [ 56 ] , the problem in Eq.( 26 )
is equivalent to:

  -- -- -- ------
           (27)
  -- -- -- ------

where @xmath , and @xmath represents the capped probability simplex
defined as @xmath and @xmath , @xmath .

Such equivalence is due to the Theorem 2 in [ 56 ] ensuring that any
vector @xmath can be decomposed as convex combination of at most @xmath
corners of @xmath by using Algorithm 4 , where the corner @xmath is
defined as having @xmath non-zero elements equal to @xmath . As a
result, the corner can be sampled by the associated probability obtained
from the convex combination, which is a valid subset selection vector
@xmath with the multiplication of @xmath .

Connection to the online adaptive PCA. The problem from Eq.( 26 ) can be
viewed as restricted version of the online adaptive PCA problem from
Eq.( 24 ). In particular, say that @xmath . This corresponds to
restricting @xmath to be diagonal. If @xmath is the diagonal of @xmath ,
then the objectives of Eq.( 26 ) and Eq.( 24 ) are equal.

We now return to the adaptive best subset of experts problem. When
@xmath and @xmath , the problem reduces to the standard static regret
minimization problem, which is studied in [ 56 ] . Their solution
applies the basic Hedge Algorithm to obtain a probability distribution
for the experts, and modifies the distribution to select a subset of the
experts.

To deal with the adaptive regret considered in Eq.( 27 ), we propose the
Algorithm 3 , which is a simple modification to Algorithm 1 in [ 56 ] .
More specifically, we add Eq.( 25b ) when updating @xmath in Step @xmath
, which is called a fixed-share step. This is inspired by the analysis
in [ 13 ] , which shows that the online adaptive best expert problem can
be solved by simply adding this fixed-share step to the standard Hedge
algorithm.

With the Algorithm 3 , the following lemma can be obtained:

###### Lemma \thechapter.1.

For all @xmath , all @xmath , and for all @xmath , Algorithm 3 satisfies

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

With the update in Eq.( 25 ), for any @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Also, @xmath based on the proof of Theorem 1 in [ 56 ] . Thus, we get

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

Moreover, Eq.( 25c ) is the solution to the following projection problem
as shown in [ 56 ] :

  -- -- --
        
  -- -- --

Since the relative entropy is one kind of Bregman divergence [ 10 , 12 ]
, the Generalized Pythagorean Theorem holds [ 38 ] :

  -- -------- -- ------
     @xmath      (29)
  -- -------- -- ------

where the last inequality is due to the non-negativity of Bregman
divergence.

Combining Eq.( 28 ) with Eq.( 29 ) and expanding the left part of @xmath
, we arrive at Lemma \thechapter .1 . ∎

Now we are ready to state the following theorem to upper bound the
adaptive regret @xmath :

###### Theorem \thechapter.1.

If we run the Algorithm 3 to select a subset of @xmath experts, then for
any sequence of loss vectors @xmath , @xmath , @xmath @xmath @xmath with
@xmath , @xmath , @xmath , @xmath , and @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof sktech.

After showing the inequality from Lemma \thechapter .1 , the main work
that remains is to sum the right side from @xmath to @xmath and provide
an upper bound. This is achieved by following the proof of the
Proposition 2 in [ 13 ] . The main idea is to expand the term @xmath as
follows:

  -- -------- --
     @xmath   
  -- -------- --

Then we can upper bound the expression of @xmath with the fixed-share
step, since @xmath is lower bounded by @xmath . We can telescope the
expression of @xmath . Then our desired upper bound can be obtained with
the help of Lemma 4 from [ 26 ] . ∎

Please refer to the Appendix for all the omitted/sketched proofs in this
chapter.

### 16 Online Adaptive PCA

Recall that the online adaptive PCA problem is below:

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

where @xmath is the rank @xmath projection matrix set.

Again, inspired by [ 56 ] , we first reformulate the above problem into
the following ’capped probability simplex’ form:

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

where @xmath , and @xmath is the set of all density matrices with
eigenvalues bounded by @xmath . Note that @xmath can be expressed as the
convex set @xmath .

1: Input: @xmath and an initial density matrix @xmath .

2: for @xmath to @xmath do

3: Apply eigendecomposition to @xmath as @xmath .

4: Apply Algorithm 4 with @xmath to the vector @xmath to decompose it
into a convex combination @xmath of at most @xmath corners @xmath .

5: Randomly select a corner @xmath with the associated probability
@xmath .

6: Form a density matrix @xmath

7: Form a rank @xmath projection matrix @xmath

8: Obtain the data point @xmath , which incurs the compression loss
@xmath and expected compression loss @xmath .

9: Update @xmath as:

  -- -------- -- -------
                 
     @xmath      (32a)
     @xmath      (32b)
     @xmath      (32c)
  -- -------- -- -------

where we apply eigendecomposition to @xmath as @xmath , and @xmath
invokes Algorithm 5 with input being the eigenvalues of @xmath .

10: end for

Algorithm 6 Uncentered online adaptive PCA

The static regret online PCA is a special case of the above problem with
@xmath and @xmath , and is solved by Algorithm 5 in [ 56 ] .

Follow the idea in the last section, we propose the Algorithm 6 .
Compared with the Algorithm 5 in [ 56 ] , we have added the fixed-share
step in the update of @xmath at step @xmath , which will be shown to be
the key in upper bounding the adaptive regret of the online PCA.

In order to analyze Algorithm 6 , we need a few supporting results. The
first result comes from [ 55 ] :

###### Theorem \thechapter.2.

[ 55 ] For any sequence of data points @xmath , @xmath , @xmath with
@xmath and for any learning rate @xmath , the following bound holds for
any matrix @xmath with the update in Eq.( 32a ):

  -- -------- --
     @xmath   
  -- -------- --

Based on the above theorem’s result, we have the following lemma:

###### Lemma \thechapter.2.

For all @xmath , all @xmath with @xmath , and for all @xmath , Algorithm
6 satisfies:

  -- -- --
        
  -- -- --

###### Proof.

First, we need to reformulate the above inequality in Theorem
\thechapter .2 , we have:

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

which is very similar to the Eq.( 28 ).

As is shown in [ 56 ] , the Eq.( 32c ) is the solution to the following
optimization problem:

  -- -- --
        
  -- -- --

As a result, the Generalized Pythagorean Theorem holds [ 38 ] for any
@xmath :

  -- -------- --
     @xmath   
  -- -------- --

Combining the above inequality with Eq.( 33 ) and expanding the left
part, we have

  -- -- --
        
  -- -- --

which proves the result. ∎

In the next theorem, we show that with the addition of the fixed-share
step in Eq.( 32b ), we can solve the online adaptive PCA problem in Eq.(
30 ).

###### Theorem \thechapter.3.

For any sequence of data points @xmath , @xmath , @xmath with @xmath ,
and @xmath , if we run Algorithm 6 with @xmath , @xmath , and @xmath ,
for any @xmath we have:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof sktech.

The proof idea is the same as in the proof of Theorem \thechapter .1 .
After getting the inequality relationship in Lemma \thechapter .2 which
has a similar form as in Lemma \thechapter .1 , we need to upper bound
sum over @xmath of the right side. To achieve this, we first reformulate
it as two parts below:

  -- -------- -- ------
     @xmath      (34)
  -- -------- -- ------

where @xmath , and @xmath .

The first part can be upper bounded with the help of the fixed-share
step in lower bounding the singular value of @xmath . After telescoping
the second part, we can get the desired upper bound with the help of
Lemma 4 from [ 26 ] . ∎

### 17 Extension to Online Adaptive Variance Minimization

In this section, we study the closely related problem of online adaptive
variance minimization. The problem is defined as follows: At each time
@xmath , we first select a vector @xmath , and then a covariance matrix
@xmath such that @xmath is revealed. The goal is to minimize the
adaptive regret defined as:

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

where the expectation is taken over the probability distribution of
@xmath .

This problem has two different situations corresponding to different
parameter space @xmath of @xmath and @xmath .

Situation 1: When @xmath is the set of @xmath (e.g., the unit vector
space), the solution to @xmath is the minimum eigenvector of the matrix
@xmath .

Situation 2: When @xmath is the probability simplex (e.g., @xmath is
equal to @xmath ), it corresponds to the risk minimization in stock
portfolios [ 45 ] .

We will start with Situation 1 since it is highly related to the
previous section.

#### 17.1 Online Adaptive Variance Minimization over the Unit vector
space

We begin with the observation of the following equivalence [ 55 ] :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is any covariance matrix, and @xmath is the set of all
density matrices.

Thus, the problem in ( 35 ) can be reformulated as:

  -- -------- -- ------
     @xmath      (36)
  -- -------- -- ------

where @xmath .

To see the equivalence between @xmath in Eq.( 35 ) and @xmath , we do
the eigendecomposition of @xmath . Then @xmath is equal to @xmath @xmath
@xmath . Since @xmath , the vector @xmath is a simplex vector, and
@xmath is equal to @xmath with probability distribution defined by the
vector @xmath .

If we examine Eq.( 36 ) and ( 31 ) together, we will see that they share
some similarities: First, they are almost the same if we set @xmath in
Eq.( 31 ). Also, @xmath in Eq.( 31 ) is a special case of @xmath in Eq.(
36 ).

Thus, it is possible to apply Algorithm 6 to solving the problem ( 36 )
by setting @xmath . In this case, Algorithms 4 and 5 are not needed.
This is summarized in Algorithm 7 .

1: Input: an initial density matrix @xmath .

2: for @xmath to @xmath do

3: Perform eigendecomposition @xmath .

4: Use the vector @xmath with probability @xmath .

5: Receive covariance matrix @xmath , which incurs the loss @xmath and
expected loss @xmath .

6: Update @xmath as:

  -- -------- -- -------
                 
     @xmath      (37a)
     @xmath      (37b)
  -- -------- -- -------

where we apply eigendecomposition to @xmath as @xmath .

7: end for

Algorithm 7 Online adaptive variance minimization over unit sphere

The theorem below is analogous to Theorem \thechapter .3 in the case
that @xmath .

###### Theorem \thechapter.4.

For any sequence of covariance matrices @xmath , @xmath , @xmath with
@xmath , and for @xmath , if we run Algorithm 7 with @xmath , @xmath ,
and @xmath , for any @xmath we have:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof sktech.

Similar inequality can be obtained as in Lemma \thechapter .2 by using
the Theorem 2 in [ 55 ] . The rest follows the proof of Theorem
\thechapter .3 . ∎

In order to apply the above theorem, we need to either estimate the step
size @xmath heuristically or estimate the upper bound @xmath , which may
not be easily done.

In the next theorem, we show that we can still upper bound the @xmath
without knowing @xmath , but the upper bound is a function of time
horizon @xmath instead of the upper bound @xmath .

Before we get to the theorem, we need the following lemma which lifts
the vector case of Lemma 1 in [ 13 ] to the density matrix case:

###### Lemma \thechapter.3.

For any @xmath , @xmath , any covariance matrix @xmath with @xmath , and
for any @xmath , Algorithm 7 satisfies:

  -- -------- --
     @xmath   
  -- -------- --

Now we are ready to present the upper bound on the regret for Algorithm
7 .

###### Theorem \thechapter.5.

For any sequence of covariance matrices @xmath , @xmath , @xmath with
@xmath , if we run Algorithm 7 with @xmath and @xmath , for any @xmath
we have:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

In the proof, we will use two cases of @xmath : @xmath , and @xmath .

From Lemma \thechapter .3 , the following inequality is valid for both
cases of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Follow the same analysis as in the proof of Theorem \thechapter .3 , we
first do the eigendecomposition to @xmath as @xmath . Since @xmath is
either @xmath or @xmath , we will re-write the above inequality as:

  -- -------- -- ------
     @xmath      (38)
  -- -------- -- ------

Analyzing the term @xmath in the above inequality is the same as the
analysis of the Eq.( 34 ) in the appendix.

Thus, summing over @xmath to @xmath to the above inequality, and setting
@xmath for @xmath and @xmath elsewhere, we have

  -- -------- --
     @xmath   
  -- -------- --

since it holds for any @xmath .

After plugging in the expression of @xmath and @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Since the above inequality holds for any @xmath , we put a @xmath in the
left part, which proves the result. ∎

#### 17.2 Online Adaptive Variance Minimization over the Simplex space

We first re-write the problem in Eq.( 35 ) when @xmath is the simplex
below:

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

where @xmath , and @xmath is the simplex set.

When @xmath and @xmath , the problem reduces to the static regret
problem, which is solved in [ 55 ] by the exponentiated gradient
algorithm as below:

  -- -------- --
     @xmath   
  -- -------- --

As is done in the previous sections, we add the fixed-share step after
the above update, which is summarized in Algorithm 8 .

1: Input: an initial vector @xmath .

2: for @xmath to @xmath do

3: Receive covariance matrix @xmath .

4: Incur the loss @xmath .

5: Update @xmath as:

  -- -------- -- -------
                 
     @xmath      (40a)
     @xmath      (40b)
  -- -------- -- -------

6: end for

Algorithm 8 Online adaptive variance minimization over simplex

With the update of @xmath in the Algorithm 8 , we have the following
theorem:

###### Theorem \thechapter.6.

For any sequence of covariance matrices @xmath , @xmath , @xmath with
@xmath , and for @xmath , if we run Algorithm 8 with @xmath , @xmath ,
@xmath , @xmath , and @xmath , for any @xmath we have:

  -- -------- --
     @xmath   
  -- -------- --

### 18 Experiments

In this section, we use two examples to illustrate the effectiveness of
our proposed online adaptive PCA algorithm. The first example is
synthetic, which shows that our proposed algorithm (denoted as Online
Adaptive PCA) can adapt to the changing subspace faster than the method
of [ 56 ] . The second example uses the practical dataset Yale-B to
demonstrate that the proposed algorithm can have lower cumulative loss
in practice when the data/face samples are coming from different
persons.

The other algorithms that are used as comparators are: 1. Follow the
Leader algorithm (denoted as Follow the Leader) [ 40 ] , which only
minimizes the loss on the past history; 2. The best fixed solution in
hindsight (denoted as Best fixed Projection), which is the solution to
the Problem described in Eq.( 23 ); 3. The online static PCA (denoted as
Online PCA) [ 56 ] . Other PCA algorithms are not included, since they
are not designed for regret minimization.

#### 18.1 A Toy Example

In this toy example, we create the synthetic data samples coming from
changing subspace, which is a similar setup as in [ 56 ] . The data
samples are divided into three equal time intervals, and each interval
has 200 data samples. The 200 data samples within same interval is
randomly generated by a Gaussian distribution with zero mean and data
dimension equal to 20, and the covariance matrix is randomly generated
with rank equal to 2. In this way, the data samples are from some
unknown 2-dimensional subspace, and any data sample with @xmath -norm
greater than 1 is normalized to 1. Since the stepsize used in the two
online algorithms is determined by the upper bound of the batch
solution, we first find the upper bound and plug into the stepsize
function, which gives @xmath . We can tune the stepsize heuristically in
practice and in this example we just use @xmath and @xmath .

After all data samples are generated, we apply the previously mentioned
algorithms with @xmath and obtain the cumulative loss as a function of
time steps, which is shown in Fig. 1 . From this figure we can see that:
1. Follow the Leader algorithm is not appropriate in the setting where
the sequential data is shifting over time. 2. The static regret is not a
good metric under this setting, since the best fixed solution in
hindsight is suboptimal. 3. Compared with Static PCA, the proposed
Adaptive PCA can adapt to the changing environment faster, which results
in lower cumulative loss and is more appropriate when the data is
shifting over time.

#### 18.2 Face data Compression Example

In this example, we use the Yale-B dataset which is a collection of face
images. The data is split into 20 time intervals corresponding to 20
different people. Within each interval, there are 64 face image samples.
Like the previous example, we first normalize the data to ensure its
@xmath -norm not greater than 1. We use @xmath , which is the same as
the previous example. The stepsize @xmath is also tuned heuristically
like the previous example, which is equal to @xmath and @xmath .

We apply the previously mentioned algorithms and again obtain the
cumulative loss as the function of time steps, which is displayed in
Fig. 2 . From this figure we can see that although there is no clear
bumps indicating the shift from one subspace to another as the Fig. 1 of
the toy example, our proposed algorithm still has the lowest cumulative
loss, which indicates that upper bounding the adaptive regret is still
effective when the compressed faces are coming from different persons.

### 19 Conclusion

In this chapter, we propose an online adaptive PCA algorithm, which
augments the previous online static PCA algorithm with a fixed-share
step. However, different from the previous online PCA algorithm which is
designed to minimize the static regret, the proposed online adaptive PCA
algorithm aims to minimize the adaptive regret which is more appropriate
when the underlying environment is changing or the sequential data is
shifting over time. We demonstrate theoretically and experimentally that
our algorithm can adapt to the changing environments. Furthermore, we
extend the online adaptive PCA algorithm to online adaptive variance
minimization problems.

One may note that the proposed algorithms suffer from the per-iteration
computation complexity of @xmath due to the eigendecomposition step,
although some tricks mentioned in [ 5 ] could be used to make it
comparable with incremental PCA of @xmath . For the future work, one
possible direction is to investigate algorithms with slightly worse
adaptive regret bound but with better per-iteration computation
complexity.

## Chapter \thechapter Online Convex Optimization for Cumulative
Constraints

Previous chapter focuses on how to enable the online PCA algorithm to
have better adaptivity to the changing environments.

In this chapter, we come back to the general online convex optimization
(OCO) problem. For online convex optimization with constraints, a
projection operator is typically applied in order to make the updated
prediction feasible for each time step [ 65 , 24 , 23 ] . However, when
the constraints are complex, the computational burden of the projection
may be too high to have online computation.

To circumvent this dilemma, [ 44 , 39 , 58 ] proposed algorithms which
approximates the true desired projection with a simpler closed-form
projection. The algorithm can still upper bound the static regret @xmath
by @xmath as the optimal result in [ 65 ] , but the constraint @xmath
may not be satisfied in every time step. Instead, the long-term
constraint violation @xmath can be upper bounded in a sub-linear order
@xmath , which is useful when we only require the constraint violation
to be non-positive on average: @xmath . However, this bound does not
enforce that the violation of the constraint gets small, which is
originally desired. A situation can arise in which strictly satisfied
constraints at one time step can cancel out violations of the
constraints at other time steps.

Along the line of the long-term constraint work, there are some
variations, which make the long-term constraint idea apply to the online
resource allocation. This is achieved by regarding the total resource
consumption constraint for different time steps as the long-term
time-dependent constraint.

For the online job scheduling, [ 58 ] considered the stochastic
long-term constraint case. It achieves @xmath bound for both @xmath and
the expected long-term constraint @xmath with Slater condition
assumption. However, such Slater condition assumes that the compared
static action needs to be strictly feasible, which means there exists at
least one point lying in the intersection of constraints @xmath . This
limits the claimed regret performance due to the increasing difficulty
in satisfying all the constraints, resulting in loose regret .

To solve this loose regret problem, [ 43 ] came up with the idea that
the fixed comparator only needs to satisfy part of the time-dependent
(possibly adversarial) constraints. That is, it used a different fixed
comparator @xmath , where @xmath , @xmath is the fixed convex set, and
@xmath is a user-determined parameter.

Although @xmath can be used in the long-term budget constraint when
@xmath represents budget at each time step, it is sometimes not a
reasonable choice in that many other resource allocation problems’
constraints cannot simply be added together due to causality. For
example, in the online job scheduling, previous time step’s vacancy of
the server cannot be carried over to the next time step, while the
unfinished jobs can. Or we want to ensure that the rate of failures (the
constraint violation itself) is upper bounded.

In this chapter, we show how our proposed algorithms can be used to
tackle the two previously mentioned problems: not enforcing low
constraint violation and limited application in resource allocation.

In the first part of the chapter which is adapted from the published
work [ 60 ] , we will show how the proposed algorithms can enforce low
constraint violation for the following two different problem setups:

Convex Case: The first algorithm is for the convex case, which also has
the user-determined trade-off as in [ 39 ] , while the constraint
violation is more strict. Specifically, we have @xmath and @xmath where
@xmath and @xmath . Note the square term heavily penalizes large
constraint violations and constraint violations from one step cannot be
canceled out by strictly feasible steps. Additionally, we give a bound
on the cumulative constraint violation @xmath , which generalizes the
bounds from [ 44 , 39 ] .

In the case of @xmath , which we call ”balanced”, both @xmath and @xmath
have the same upper bound of @xmath . More importantly, our algorithm
guarantees that at each time step, the clipped constraint term @xmath is
upper bounded by @xmath , which does not follow from the results of [ 44
, 39 ] . However, our results currently cannot generalize those of [ 58
] , which has @xmath . It is unclear how to extend the work of [ 58 ] to
the clipped constraints, @xmath .

Strongly Convex Case: Our second algorithm for strongly convex function
@xmath gives us the improved upper bounds compared with the previous
work in [ 39 ] . Specifically, we have @xmath , and @xmath . The
improved bounds match the regret order of standard OCO from [ 31 ] ,
while maintaining a constraint violation of reasonable order.

We show numerical experiments on three problems. A toy example is used
to compare trajectories of our algorithm with those of [ 39 , 44 ] , and
we see that our algorithm tightly follows the constraints. The
algorithms are also compared on a doubly-stochastic matrix approximation
problem [ 39 ] and an economic dispatch problem from power systems. In
these, our algorithms lead to reasonable objective regret and low
cumulative constraint violation.

In the second part of the chapter, we will discuss how to apply the
proposed algorithms to the general resource allocation problems with
tight regret guarantee by using a variant of dynamic regret .

### 20 Problem Formulation

The basic projected gradient algorithm achieving @xmath for convex
problem was defined in [ 65 ] . Specifically, at each iteration @xmath ,
the update rule is:

  -- -------- -- ------
     @xmath      (41)
  -- -------- -- ------

where @xmath is the projection operation to the set @xmath .

Although the algorithm is simple, it needs to solve a constrained
optimization problem at every time step, which might be too
time-consuming for online implementation when the constraints are
complex.

In order to lower the computational complexity and accelerate the online
processing speed, the work of [ 44 ] avoids the convex optimization by
projecting the variable to a fixed ball @xmath , which always has a
closed-form solution. That paper gives an online solution for the
following problem:

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

where @xmath . It is assumed that there exist constants @xmath and
@xmath such that @xmath with @xmath being the unit @xmath ball centered
at the origin and @xmath .

Compared to the update in Eq. ( 41 ), which requires @xmath for all
@xmath , ( 42 ) implies that only the sum of constraints is required.
This sum of constraints is known as the long-term constraint .

To solve this new problem, [ 44 ] considers the following augmented
Lagrangian function at each iteration @xmath :

  -- -------- -- ------
     @xmath      (43)
  -- -------- -- ------

The update rule is as follows:

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

where @xmath and @xmath are the pre-determined step size and some
constant, respectively.

More recently, an adaptive version was developed in [ 39 ] , which has a
user-defined trade-off parameter. The algorithm proposed by [ 39 ]
utilizes two different step size sequences to update @xmath and @xmath ,
respectively, instead of using a single step size @xmath .

In both algorithms of [ 44 ] and [ 39 ] , the bound for the violation of
the long-term constraint is that @xmath , @xmath for some @xmath .
However, as argued in the last section, this bound does not enforce that
the violation of the constraint @xmath gets small due to the possible
cancellation from strictly feasible steps. This problem can be rectified
by considering clipped constraint, @xmath , in place of @xmath .

For convex problems, our goal is to bound the term @xmath , which, as
discussed in the previous section, is more useful for enforcing small
constraint violations, and also recovers the existing bounds for both
@xmath and @xmath . For strongly convex problems, we also show the
improvement on the upper bounds compared to the results in [ 39 ] .

In sum, in this chapter, our first goal is to solve the following
problem for the general convex condition:

  -- -- -- ------
           (45)
  -- -- -- ------

where @xmath . The new constraint from ( 45 ) is called the
square-clipped long-term constraint (since it is a square-clipped
version of the long-term constraint) or square-cumulative constraint
(since it encodes the square-cumulative violation of the constraints).

To solve Problem ( 45 ), we change the augmented Lagrangian function
@xmath as follows:

  -- -------- -- ------
     @xmath      (46)
  -- -------- -- ------

We will also see in a later section how this new function @xmath can be
used to get general time-dependent resource allocation problem under a
variant of dynamic regret .

Throughout this chapter, we will use the following assumptions as in [
44 ] : 1. The convex set @xmath is non-empty, closed, bounded, and can
be described by @xmath convex functions as @xmath . 2. Both the loss
functions @xmath , @xmath and constraint functions @xmath , @xmath are
Lipschitz continuous in the set @xmath . That is, @xmath , @xmath ,
@xmath and @xmath . @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

### 21 Algorithm

#### 21.1 Convex Case

1: Input: constraints @xmath , stepsize @xmath , time horizon T, and
constant @xmath .

2: Initialization: @xmath is in the center of the @xmath .

3: for @xmath to @xmath do

4: Input the prediction result @xmath .

5: Obtain the convex loss function @xmath and the loss value @xmath .

6: Calculate a subgradient @xmath , where:

  -- -------- --
     @xmath   
  -- -------- --

7: Update @xmath and @xmath as below:

  -- -------- --
     @xmath   
  -- -------- --

8: end for

Algorithm 9 Generalized Online Convex Optimization with Long-term
Constraint

The main algorithm for this chapter is shown in Algorithm 9 . For
simplicity, we abuse the subgradient notation, denoting a single element
of the subgradient by @xmath . We also replace the @xmath in Eq. ( 46 )
with @xmath . Comparing our algorithm with Eq.( 44 ), we can see that
the gradient projection step for @xmath is similar, while the update
rule for @xmath is different. Instead of a projected gradient step, we
explicitly maximize @xmath over @xmath . This explicit projection-free
update for @xmath is possible because the constraint clipping guarantees
that the maximizer is non-negative. Furthermore, this
constraint-violation-dependent update helps to enforce small cumulative
and individual constraint violations. Specific bounds on constraint
violation are given in Theorem \thechapter .1 and Lemma \thechapter .2
below.

Based on the update rule in Algorithm 9 , the following theorem gives
the upper bounds for both the regret on the loss and the
squared-cumulative constraint violation, @xmath in Problem 45 .

###### Theorem \thechapter.1.

Set @xmath , @xmath . If we follow the update rule in Algorithm 9 with
@xmath and @xmath being the optimal solution for @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Before proving Theorem \thechapter .1 , we need the following
preliminary result.

###### Lemma \thechapter.1.

For the sequence of @xmath , @xmath obtained from Algorithm 9 and @xmath
, we have the following inequality:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

First, @xmath is convex in @xmath . Then for any @xmath , we have the
following inequality:

  -- -------- --
     @xmath   
  -- -------- --

Using the non-expansive property of the projection operator and the
update rule for @xmath in Algorithm 9 , we have

  -- -- -- ------
           (47)
  -- -- -- ------

Then we have

  -- -------- -- ------
     @xmath      (48)
  -- -------- -- ------

Furthermore, for @xmath , we have

  -- -------- -- ------
     @xmath      (49)
  -- -------- -- ------

where the last inequality is from the inequality that @xmath , and both
@xmath and @xmath are less than or equal to @xmath by the definition.

Then we have

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is in the center of @xmath , we can assume @xmath without
loss of generality. If we sum the @xmath from 1 to @xmath , we have

  -- -- --
        
  -- -- --

where the last inequality follows from the fact that @xmath and @xmath .
∎

Now we are ready to prove the main theorem.

###### Proof of Theorem \thechapter.1.

From Lemma \thechapter .1 , we have

  -- -------- --
     @xmath   
  -- -------- --

If we expand the terms in the LHS and move the last term in RHS to the
left, we have

  -- -------- --
     @xmath   
  -- -------- --

We can set @xmath to have @xmath and plug in the expression @xmath to
have

  -- -------- -- ------
     @xmath      (50)
  -- -------- -- ------

Plugging in the expression for @xmath and @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Because @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, we have @xmath according to the assumption. Then we have

  -- -------- --
     @xmath   
  -- -------- --

Because @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

∎

From Theorem \thechapter .1 , we can see that by setting appropriate
step size, @xmath , and constant, @xmath , we can obtain the upper bound
for the regret of the loss function being less than or equal to @xmath ,
which is also shown in [ 44 ] [ 39 ] . The main difference of the
Theorem \thechapter .1 is that previous results of [ 44 ] [ 39 ] all
obtain the upper bound for the long-term constraint @xmath , while here
the upper bound for the constraint violation of the form @xmath is
achieved. Also note that the step size depends on @xmath , which may not
be available. In this case, we can use the ’doubling trick’ described in
the book [ 15 ] to transfer our @xmath -dependent algorithm into @xmath
-free one with a worsening factor of @xmath .

The proposed algorithm and the resulting bound are useful for two
reasons: 1. The square-cumulative constraint implies a bound on the
cumulative constraint violation, @xmath , while enforcing larger
penalties for large violations. 2. The proposed algorithm can also upper
bound the constraint violation for each single step @xmath , which is
not bounded in the previous literature.

The next results show how to bound constraint violations at each step.
Please refer to the Appendix for the proof.

###### Lemma \thechapter.2.

If there is only one differentiable constraint function @xmath with
Lipschitz continuous gradient parameter @xmath , and we run the
Algorithm 9 with the parameters in Theorem \thechapter .1 and large
enough @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Lemma \thechapter .2 only considers single constraint case. For case of
multiple differentiable constraints, we have the following:

###### Proposition \thechapter.1.

For multiple differentiable constraint functions @xmath , @xmath with
Lipschitz continuous gradient parameters @xmath , if we use @xmath as
the constraint function in Algorithm 9 , then for large enough @xmath ,
we have

  -- -------- --
     @xmath   
  -- -------- --

Clearly, both Lemma \thechapter .2 and Proposition \thechapter .1 only
deal with differentiable functions. For a non-differentiable function
@xmath , we can first use a differentiable function @xmath to
approximate the @xmath with @xmath , and then apply the previous Lemma
\thechapter .2 and Proposition \thechapter .1 to upper bound each
individual @xmath . Many non-smooth convex functions can be approximated
in this way as shown in [ 47 ] .

#### 21.2 Strongly Convex Case

For @xmath to be strongly convex, the Algorithm 9 is still valid. But in
order to reduce the upper bounds for both objective regret and the
clipped long-term constraint @xmath compared with Proposition
\thechapter .3 in next section, we need to use time-varying step size as
the one used in [ 31 ] . Thus, we modify the update rule of @xmath ,
@xmath to have time-varying step size as below:

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

If we replace the update rule in Algorithm 9 with Eq.( 51 ), we can
obtain the following theorem:

###### Theorem \thechapter.2.

Assume @xmath has strong convexity parameter @xmath . If we set @xmath ,
@xmath , follow the new update rule in Eq.( 51 ), and @xmath being the
optimal solution for @xmath , for @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

The paper [ 39 ] also has a discussion of strongly convex functions, but
only provides a bound similar to the convex one. Theorem \thechapter .2
shows the improved bounds for both objective regret and the constraint
violation. On one hand the objective regret is consistent with the
standard OCO result in [ 31 ] , and on the other the constraint
violation is further reduced compared with the result in [ 39 ] .

##### Proof of Theorem \thechapter.2:

###### Proof.

For the strongly convex case of @xmath with strong convexity parameter
equal to @xmath , we can also conclude that the modified augmented
Lagrangian function in Eq.( 51 ) is also strongly convex w.r.t. @xmath
with the strong convexity parameter @xmath . Then we have

  -- -------- -- ------
     @xmath      (52)
  -- -------- -- ------

From concavity of @xmath in terms of @xmath , we can have

  -- -------- -- ------
     @xmath      (53)
  -- -------- -- ------

Since @xmath maximizes the augmented Lagrangian, we can see that the RHS
is @xmath .

From Eq.( 47 ), we have

  -- -------- -- ------
     @xmath      (54)
  -- -------- -- ------

Multiply Eq.( 52 ) by @xmath and add Eq.( 53 ) together with Eq.( 54 )
plugging in:

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath , and plug in the expression for @xmath , we can get:

  -- -------- --
     @xmath   
  -- -------- --

Plug in the expressions @xmath , @xmath , and sum over @xmath to @xmath
:

  -- -- --
        
  -- -- --

For the expression of @xmath , we have:

  -- -------- --
     @xmath   
  -- -------- --

For the expression of @xmath , with the expression of @xmath and the
inequality relation between sum and integral, we have:

  -- -------- --
     @xmath   
  -- -------- --

Thus, we have:

  -- -- --
        
  -- -- --

If we set @xmath , and due to non-negativity of @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, we have @xmath according to the assumption. Then we have

  -- -------- --
     @xmath   
  -- -------- --

Because @xmath , we have:

  -- -------- --
     @xmath   
  -- -------- --

∎

### 22 Relation with Previous Results

In this section, we extend Theorem \thechapter .1 to enable direct
comparison with the results from [ 44 ] [ 39 ] . In particular, it is
shown how Algorithm 9 recovers the existing regret bounds, while the use
of the new augmented Lagrangian ( 46 ) in the previous algorithms also
provides regret bounds for the clipped constraint case.

The first result puts a bound on the clipped long-term constraint,
rather than the sum-of-squares that appears in Theorem \thechapter .1 .
This will allow more direct comparisons with the existing results.

###### Proposition \thechapter.2.

If @xmath , @xmath , @xmath , and @xmath , then the result of Algorithm
9 satisfies

  -- -------- --
     @xmath   
  -- -------- --

This result shows that our algorithm generalizes the regret and
long-term constraint bounds of [ 44 ] . Please refer to the Appendix for
this section’s proofs.

The next result shows that by changing our constant stepsize
accordingly, with the Algorithm 9 , we can achieve the user-defined
trade-off from [ 39 ] . Furthermore, we also include the squared version
and clipped constraint violations.

###### Proposition \thechapter.3.

If @xmath , @xmath , @xmath , @xmath , and @xmath , then the result of
Algorithm 9 satisfies

  -- -------- --
     @xmath   
  -- -------- --

Proposition \thechapter .3 provides a systematic way to balance the
regret of the objective and the constraint violation. Next, we will show
that previous algorithms can use our proposed augmented Lagrangian
function to have their own clipped long-term constraint bound.

###### Proposition \thechapter.4.

If we run Algorithm 1 in [ 44 ] with the augmented Lagrangian formula
defined in Eq.( 46 ), the result satisfies

  -- -------- --
     @xmath   
  -- -------- --

For the update rule proposed in [ 39 ] , we need to change the @xmath to
the following one:

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

where @xmath .

###### Proposition \thechapter.5.

If we use the update rule and the parameter choices in [ 39 ] with the
augmented Lagrangian in Eq.( 55 ), then @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Propositions \thechapter .4 and \thechapter .5 show that clipped
long-term constraints can be bounded by combining the algorithms of [ 44
, 39 ] with our augmented Lagrangian. Although these results are similar
in part to our Propositions \thechapter .2 and \thechapter .3 , they do
not imply the results in Theorems \thechapter .1 and \thechapter .2 as
well as the new single step constraint violation bound in Lemma
\thechapter .2 , which are our key contributions. Based on Propositions
\thechapter .4 and \thechapter .5 , it is natural to ask whether we
could apply our new augmented Lagrangian formula ( 46 ) to the recent
work in [ 58 ] . Unfortunately, we have not found a way to do so.

Furthermore, since @xmath is also convex, we could define @xmath and
apply the previous algorithms [ 44 ] [ 39 ] and [ 58 ] . This will
result in the upper bounds of @xmath [ 44 ] and @xmath [ 39 ] , which
are worse than our upper bounds of @xmath (Theorem \thechapter .1 ) and
@xmath ( Proposition \thechapter .3 ). Note that the algorithm in [ 58 ]
cannot be applied since the clipped constraints do not satisfy the
required Slater condition.

### 23 Experiment

In this section, we test the performance of the algorithms including OGD
[ 44 ] , A-OGD [ 39 ] , Clipped-OGD (this chapter), and our proposed
algorithm strongly convex case (Our-strong). Throughout the experiments,
our algorithm has the following fixed parameters: @xmath , @xmath ,
@xmath . In order to better show the result of the constraint violation
trajectories, we aggregate all the constraints as a single one by using
@xmath as done in [ 44 ] .

#### 23.1 A Toy Experiment

For illustration purposes, we solve the following 2-D toy experiment
with @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where the constraint is the @xmath -norm constraint. The vector @xmath
is generated from a uniform random vector over @xmath which is rescaled
to have norm @xmath . This leads to slightly average cost on the on the
first coordinate. The offline solutions for different @xmath are
obtained by CVXPY [ 22 ] .

All algorithms are run up to @xmath and are averaged over 10 random
sequences of @xmath . Since the main goal here is to compare the
variables’ trajectories generated by different algorithms, the results
for different @xmath are in the Appendix for space purposes. Fig. 3
shows these trajectories for one realization with @xmath . The blue star
is the optimal point’s position.

From Fig. 3 we can see that the trajectories generated by Clipped-OGD
follows the boundary very tightly until reaching the optimal point. This
can be explained by the Lemma \thechapter .2 which shows that the
constraint violation for single step is also upper bounded. For the OGD,
the trajectory oscillates widely around the boundary of the true
constraint. For the A-OGD, its trajectory in Fig. 3 violates the
constraint most of the time, and this violation actually contributes to
the lower objective regret shown in the Appendix.

#### 23.2 Doubly-Stochastic Matrices

We also test the algorithms for approximation by doubly-stochastic
matrices, as in [ 39 ] :

  -- -- -- ------
           (56)
  -- -- -- ------

where @xmath is the matrix variable, is the vector whose elements are
all 1, and matrix @xmath is the permutation matrix which is randomly
generated.

After changing the equality constraints into inequality ones (e.g.,
@xmath into @xmath and @xmath ), we run the algorithms with different T
up to @xmath for 10 different random sequences of @xmath . Since the
objective function is strongly convex with parameter @xmath , we also
include our designed strongly convex algorithm as another comparison.
The offline optimal solutions are obtained by CVXPY [ 22 ] .

The mean results for both constraint violation and objective regret are
shown in Fig. 4 . From the result we can see that, for our designed
strongly convex algorithm Our-Strong, its result is around the best ones
in not only the clipped constraint violation, but the objective regret.
For our most-balanced convex case algorithm Clipped-OGD with @xmath ,
although its clipped constraint violation is relatively bigger than
A-OGD, it also becomes quite flat quickly, which means the algorithm
quickly converges to a feasible solution.

#### 23.3 Economic Dispatch in Power Systems

This example is adapted from [ 42 ] and [ 52 ] , which considers the
problem of power dispatch. That is, at each time step @xmath , we try to
minimize the power generation cost @xmath for each generator @xmath
while maintaining the power balance @xmath , where @xmath is the power
demand at time @xmath . Also, each power generator produces an emission
level @xmath . To bound the emissions, we impose the constraint @xmath .
In addition to requiring this constraint to be satisfied on average, we
also require bounded constraint violations at each time step. The
problem is formally stated as:

  -- -------- --
     @xmath   
  -- -------- --

where the second constraint is from the fact that each generator has the
power generation limit.

In this example, we use three generators. We define the cost and
emission functions according to [ 52 ] and [ 42 ] as @xmath , and @xmath
, respectively. The parameters are: @xmath , @xmath , @xmath , @xmath ,
@xmath , and @xmath . The demand @xmath is adapted from real-world
5-minute interval demand data between 04/24/2018 and 05/03/2018 ³ ³ 3
https://www.iso-ne.com/isoexpress/web/reports/load-and-demand , which is
shown in Fig. 5 (a). The offline optimal solution or best fixed strategy
in hindsight is obtained by an implementation of SAGA [ 21 ] . The
constraint violation for each time step is shown in Fig. 5 (b), and the
running average objective cost is shown in Fig. 5 (c). From these
results we can see that our algorithm has very small constraint
violation for each time step, which is desired by the requirement.
Furthermore, our objective costs are very close to the best fixed
strategy.

### 24 Extension to Dynamic OCO with Long-term Constraint

In this section, we extend the Algorithm 9 to solve the general
time-dependent online resource allocation problems.

Let us use the long-term budget allocation problem solved in [ 43 ] as
an example, and assume that the per time step budget constraint is
@xmath , where @xmath is the budget at time step @xmath . Since we are
usually given the total budget @xmath over @xmath time steps and have no
idea on how to allocate it, we could set per time step budget constraint
being equal to @xmath . The OCO with long-term constraint algorithm can
dynamically allocate the per time step budget usage and make sure the
budget is satisfied on average as of the result @xmath . As mentioned in
the previous section, to solve the problem of the increasing difficulty
in satisfying all the constraints @xmath occurred in previous
algorithms, [ 43 ] used another comparator @xmath , where @xmath ,
@xmath is the fixed convex set, and @xmath is a user-determined
parameter.

However, as discussed at the beginning of this chapter, the constraint
set @xmath is not appropriate in other resource allocation problems such
as the job scheduling and rates of failure allocation (the constraint
violation itself), since many applications’ long-term constraint cannot
be simply added together. In general, there are three types of
time-dependent long-term constraint:

1.  @xmath , sum of the constraint functions, ideal for non-causal
    constraint such as budget one, which is used in [ 43 , 58 , 17 ] .

2.  @xmath , where @xmath , @xmath and @xmath , which considers the
    causality restriction when adding the constraints. For example, the
    queuing/job scheduling constraint as mentioned before. However,
    previous works dealing with queuing type long-term constraint such
    as [ 58 , 17 ] usually use @xmath , which is inappropriate.

3.  @xmath , cumulative constraint, which only considers the violation
    part and is ideal for the long-term failure rate constraint like
    mistake error.

This section’s goal is to enable our proposed algorithms to apply to
different types of time-dependent long-term constraint problems by
bounding the @xmath , since bounding the 3rd type implies the bound for
the other two types.

Since the constraint set @xmath cannot be used when bounding @xmath , we
need a new way to solve the loose regret due to the problem of the
increasing difficulty in satisfying all the constraints. As discussed in
the previous chapters, another tighter performance metric used in online
learning is called dynamic regret , which measures the difference of the
cumulative loss against a comparison sequence, @xmath :

  -- -------- --
     @xmath   
  -- -------- --

For the convex @xmath , @xmath is obtained by [ 63 ] , while for the
strongly convex or exp-concave @xmath , @xmath is shown in [ 62 ] ,
where @xmath is the comparator sequence’s path-length defined as:

  -- -------- --
     @xmath   
  -- -------- --

For the purposes of both solving the problem of the loose bound occurred
in static regret and mitigating the generalization issue in using the
set @xmath , we extend the Algorithm 9 to bound the dynamic regret
@xmath , where the comparator sequence @xmath is coming from the set
@xmath . This generalizes the comparator set @xmath by allowing the
changes of the comparator sequence as opposed to a fixed one, which has
a much tighter bound compared to the static regret and is more
appropriate under the changing environments.

The assumptions used in this section are the following:

-   The fixed convex set @xmath is compact with diameter equal to @xmath
    .

-   Both @xmath and @xmath are Lipschitz continuous with @xmath , and
    @xmath . Since @xmath is compact, without loss of generality, we
    assume @xmath , @xmath , @xmath .

-   The comparator sequence @xmath coming from @xmath is not empty.

where the first two assumptions are ubiquitous in the online convex
optimization. The 3rd one is used to define the dynamic regret used in
this chapter, which is less restrictive compared to both the @xmath in [
43 ] and the @xmath in [ 60 , 58 ] . Since @xmath can be generated
adversarially, it is possible to make @xmath infeasible by varying the
@xmath intentionally.

In order to solve the time-changing long-term constraint @xmath , we
modify the Eq. ( 46 ) as:

  -- -------- -- ------
     @xmath      (57)
  -- -------- -- ------

Although the analysis in the previous section can be used to deal with
time-changing @xmath , the results only hold true w.r.t. the very loose
static regret .

#### 24.1 Convex Case

Let us first discuss the update rule and the results associated with the
case when @xmath is convex.

We first change the @xmath in Eq. ( 57 ) by replacing the time-dependent
parameter @xmath with @xmath as:

  -- -------- -- ------
     @xmath      (58)
  -- -------- -- ------

The update rule for @xmath is

  -- -------- -- -------
                 
     @xmath      (59a)
                 
     @xmath      (59b)
  -- -------- -- -------

where @xmath is initialized in @xmath , and we abuse the sub-gradient
notation to denote a single element of the sub-gradient by @xmath .

With the update rule in Eq. ( 59 ), we can get the following result:

###### Theorem \thechapter.3.

For any comparator sequence @xmath , by setting @xmath and @xmath , we
can bound the @xmath and @xmath as

  -- -------- -- -------
                 
     @xmath      (60a)
                 
     @xmath      (60b)
  -- -------- -- -------

Please refer to the Appendix for all the omitted proofs in this section.

Theorem \thechapter .3 generalizes the results in the previous section
by both varying the comparator sequence and the constraint feasibility.
More specifically, Theorem \thechapter .3 recovers the result in
previous section by setting @xmath and @xmath .

One direct consequence of the above theorem is:

###### Corollary \thechapter.1.

If @xmath , @xmath , and @xmath , @xmath , then

  -- -------- -- -------
                 
     @xmath      (61a)
                 
     @xmath      (61b)
                 
     @xmath      (61c)
  -- -------- -- -------

###### Proof.

The first two inequalities are due the direct calculation by plugging
@xmath and @xmath into Eq. ( 60a ) and ( 60b ). The third inequality can
be obtained by viewing @xmath as a vector and using the vector norm
inequality @xmath . ∎

The above Corollary generalizes the result in [ 43 ] by considering the
dynamic regret w.r.t @xmath and more general long-term constraint bound.

#### 24.2 Strongly Convex Case

In this case, we use the Augmented Lagrangian function @xmath defined in
Eq. ( 57 ), which is rewritten here as:

  -- -------- --
     @xmath   
  -- -------- --

The update rule for @xmath is

  -- -------- -- -------
                 
     @xmath      (62a)
                 
     @xmath      (62b)
  -- -------- -- -------

where @xmath is initialized in @xmath , and we abuse the sub-gradient
notation to denote a single element of the sub-gradient by @xmath .

Compared to the update rule in Eq. ( 59 ), the one in strongly convex
case has time-dependent parameters like @xmath and @xmath . This is
aligned with the parameter setup in previous works like [ 31 , 60 , 62 ]
.

The update rule in Eq. ( 62 ) results in the following theorem:

###### Theorem \thechapter.4.

By using @xmath , @xmath , and @xmath , for @xmath with strong convexity
parameter @xmath and any comparator sequence @xmath , the following
results hold:

  -- -------- -- -------
                 
     @xmath      (63a)
                 
     @xmath      (63b)
  -- -------- -- -------

Compared to the result in convex case, both the @xmath and the @xmath
are improved. The improvement in terms of the order complexity only
happens when @xmath and @xmath (e.g., @xmath ). For the other cases, it
also reduces the additive value by about @xmath .

### 25 Conclusion

In this chapter, we propose algorithms for OCO with both convex and
strongly convex objective functions. By applying different update
strategies that utilize a modified augmented Lagrangian function, they
can solve OCO with a squared/clipped long-term constraints requirement.
The algorithm for general convex case provides the useful bounds for
both the long-term constraint violation and the constraint violation at
each time step. Furthermore, the bounds for the strongly convex case is
an improvement compared with the previous efforts in the literature.
Experiments show that our algorithms can follow the constraint boundary
tightly and have relatively smaller clipped long-term constraint
violation with reasonably low objective regret.

Furthermore, we extend the algorithms to solve the time-dependent
long-term constraint problem with a variant of dynamic regret guarantee,
which can be applied to more general resource allocation problems than
the previous algorithms.

## Chapter \thechapter Conclusion

Tracking the changes of the environments is a key difference between
Online Convex Optimization (OCO) algorithms and the batch processing
based approaches, since the sequential data/observation tends to be
shifting over time. In this thesis, we develop different OCO algorithms
for various problems to enable the decision making on-the-fly with
better adaptivity to the changing environments.

One way to have better adaptivity is to examine the proposed algorithms’
performance by the notion of the dynamic regret , which compares the
algorithm’s cumulative loss against that incurred by a comparison
sequence. For the general exp-concave or strongly convex problems, we
propose discounted Online Newton algorithm to have dynamic regret
guarantee @xmath , which is inspired by the forgetting factor used in
the Recursive Least Squares algorithms. Moreover, the trade-off between
static and dynamic regret is analyzed for both Online Least-Squares and
its generalization of strongly convex and smooth objective. To obtain
more computationally efficient algorithms, we also propose a novel
gradient descent step size rule for strongly convex functions, which
recovers the dynamic regret bounds described above.

Another way to deal with changing environments is to upper bound the
notion of adaptive regret . Previous literature has developed algorithms
for the online convex problems by running a pool of algorithms in
parallel, resulting in the unwanted increase in both the running time
and the implementation complexity. To avoid these problems, we propose a
new algorithm with same performance guarantee, which is the
exponentiated gradient descent algorithm with a mixture of fixed-share
step. We show that this algorithm can be applied to the online Principal
Component Analysis (PCA) and its extension of variance minimization
under changing environments.

For the constrained OCO algorithms, a projection operator is almost
unavoidable. When the constrain set is complex, such operation is very
time-consuming and prevents the algorithms from the true online
implementation. To accelerate the OCO algorithms’ update, our third part
of the thesis propose algorithms to replace the true desired projection
with an approximate closed-form one. Although the approximation may
cause constraint violation for some time steps, sub-linear cumulative
constraint violation is guaranteed to achieve the constraint
satisfaction on average. Furthermore, single step constraint violation
is bounded to avoid undesired large step violations. Finally, we extend
our proposed algorithms’ idea to solve the more general time-dependent
online resource allocation problems with performance guarantee by a
variant of dynamic regret .