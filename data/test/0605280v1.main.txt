##### Contents

-    1 Introduction
-    2 Foundations
    -    2.1 Geometric algebra @xmath
    -    2.2 Combinatorial Clifford algebra @xmath
    -    2.3 Standard operations
    -    2.4 Vector space geometry
    -    2.5 Linear functions
    -    2.6 Infinite-dimensional Clifford algebra
-    3 Isomorphisms
-    4 Groups
    -    4.1 Group actions on @xmath
    -    4.2 The Lipschitz group
    -    4.3 Properties of Pin and Spin groups
    -    4.4 Spinors
-    5 A study of lower-dimensional algebras
    -    5.1 @xmath
    -    5.2 @xmath - The complex numbers
    -    5.3 @xmath
    -    5.4 @xmath - The plane algebra
    -    5.5 @xmath - The quaternions
    -    5.6 @xmath
    -    5.7 @xmath - The space algebra / Pauli algebra
    -    5.8 @xmath - The spacetime algebra
    -    5.9 @xmath - The Dirac algebra
    -    5.10 Summary of norm functions
-    6 Representation theory
    -    6.1 Example I: Normed division algebras
    -    6.2 Example II: Vector fields on spheres
-    7 Spinors in physics
    -    7.1 Pauli spinors
    -    7.2 Dirac-Hestenes spinors
-    8 Summary and discussion
-    Acknowledgements
-    Appendix: Matrix theorems

## 1 Introduction

The foundations of geometric algebra, or what today is more commonly
known as Clifford algebra, were put forward already in 1844 by
Grassmann. He introduced vectors, scalar products and extensive
quantities such as exterior products. His ideas were far ahead of his
time and formulated in an abstract and rather philosophical form which
was hard to follow for contemporary mathematicians. Because of this, his
work was largely ignored until around 1876, when Clifford took up
Grassmann’s ideas and formulated a natural algebra on vectors with
combined interior and exterior products. He referred to this as an
application of Grassmann’s geometric algebra.

Due to unfortunate historic events, such as Clifford’s early death in
1879, his ideas did not reach the wider part of the mathematics
community. Hamilton had independently invented the quaternion algebra
which was a special case of Grassmann’s constructions, a fact Hamilton
quickly realized himself. Gibbs reformulated, largely due to a
misinterpretation, the quaternion algebra to a system for calculating
with vectors in three dimensions with scalar and cross products. This
system, which today is taught at an elementary academic level, found
immediate applications in physics, which at that time circled around
Newton’s mechanics and Maxwell’s electrodynamics. Clifford’s algebra
only continued to be employed within small mathematical circles, while
physicists struggled to transfer the three-dimensional concepts in
Gibbs’ formulation to special relativity and quantum mechanics.
Contributions and independent reinventions of Grassmann’s and Clifford’s
constructions were made along the way by Cartan, Lipschitz, Chevalley,
Riesz, Atiyah, Bott, Shapiro, and others.

Only in 1966 did Hestenes identify the Dirac algebra, which had been
constructed for relativistic quantum mechanics, as the geometric algebra
of spacetime. This spawned new interest in geometric algebra, and led,
though with a certain reluctance in the scientific community, to
applications and reformulations in a wide range of fields in mathematics
and physics. More recent applications include image analysis, computer
vision, robotic control and electromagnetic field simulations. Geometric
algebra is even finding its way into the computer game industry.

There are a number of aims of this Master of Science Thesis. Firstly, I
want to give a compact introduction to geometric algebra which sums up
classic results regarding its basic structure and operations, the
relations between different geometric algebras, and the important
connection to orthogonal groups via Spin groups. I also clarify a number
of statements which have been used in a rather sloppy, and sometimes
incorrect, manner in the literature. All stated theorems are accompanied
by proofs, or references to where a strict proof can be found.

Secondly, I want to show why I think that geometric and Clifford
algebras are important, by giving examples of applications in
mathematics and physics. The applications chosen cover a wide range of
topics, some with no direct connection to geometry. The applications in
physics serve to illustrate the computational and, most importantly,
conceptual simplifications that the language of geometric algebra can
provide.

Another aim of the thesis is to present some of the ideas of my
supervisor Lars Svensson in the subject of generalizing Clifford algebra
in the algebraic direction. I also present some of my own ideas
regarding norm functions on geometric algebras.

The reader will be assumed to be familiar with basic algebraic concepts
such as tensors, fields, rings and homomorphisms. Some basics in
topology are also helpful. To really appreciate the examples in physics,
the reader should be familiar with special relativity and preferably
also relativistic electrodynamics and quantum mechanics. For some
motivation and a picture of where we are heading, it could be helpful to
have seen some examples of geometric algebras before. For a quick
10-page introduction with some applications in physics, see [ 16 ] .

Throughout, we will use the name geometric algebra in the context of
vector spaces, partly in honor of Grassmann’s contributions, but mainly
for the direct and natural connection to geometry that this algebra
admits. In a more general algebraic setting, where a combinatorial
rather than geometric interpretation exists, we call the corresponding
construction Clifford algebra .

## 2 Foundations

In this section we define geometric algebra and work out a number of its
basic properties. We consider the definition that is most common in the
mathematical literature, namely as a quotient space on the tensor
algebra of a vector space with a quadratic form. We see that this leads,
in the finite-dimensional case, to the equivalent definition as an
algebra with generators @xmath satisfying @xmath for some metric @xmath
. This is perhaps the most well-known definition.

We go on to consider an alternative definition of geometric algebra
based on its algebraic and combinatorial features. The resulting
algebra, here called Clifford algebra due to its higher generality but
less direct connection to geometry, allows us to define common
operations and prove fundamental identities in a remarkably simple way
compared to traditional fomulations.

Returning to the vector space setting, we go on to study some of the
geometric features from which geometric algebra earns its name. We also
consider parts of the extensive linear function theory which exists for
geometric algebras.

Finally, we note that the generalized Clifford algebra offers
interesting views regarding the infinite-dimensional case.

### 2.1 Geometric algebra @xmath

The traditional definition of geometric algebra is carried out in the
context of vector spaces with an inner product, or more generally a
quadratic form. We consider here a vector space @xmath of arbitrary
dimension over some field @xmath .

###### Definition 2.1.

A quadratic form @xmath on a vector space @xmath is a map @xmath such
that

  -- -- --
        
  -- -- --

The bilinear form @xmath is called the polarization of @xmath .

###### Example 2.1.

If @xmath has a bilinear form @xmath then @xmath is a quadratic form and
@xmath is the symmetrization of @xmath . This could be positive definite
(an inner product), or indefinite (a metric of arbitrary signature).

###### Example 2.2.

If @xmath is a normed vector space over @xmath , with norm denoted by
@xmath , where the parallelogram identity @xmath holds then @xmath is a
quadratic form and @xmath is an inner product on @xmath . This is a
classic result, sometimes called the Jordan-von Neumann theorem.

Let @xmath denote the tensor algebra on @xmath , the elements of which
are finite sums of tensors of different grades on @xmath . Consider the
ideal generated by all elements of the form ² ² 2 Mathematicians often
choose a different sign convention here, resulting in reversed signature
in many of the following results. The convention used here seems more
natural in my opinion, since e.g. squares of vectors in euclidean spaces
become positive instead of negative. @xmath for vectors @xmath ,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

We define the geometric algebra over @xmath by quoting out this ideal
from @xmath .

###### Definition 2.2.

The geometric algebra @xmath over the vector space @xmath with quadratic
form @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

When it is clear from the context what vector space or quadratic form we
are working with, we will often denote @xmath by @xmath , or just @xmath
.

The product in @xmath , called the geometric or Clifford product , is
inherited from the tensor product in @xmath and we denote it by
juxtaposition (or @xmath if absolutely necessary),

  -- -------- --
     @xmath   
  -- -------- --

Note that this product is bilinear and associative. We immediately find
the following identities on @xmath for @xmath :

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

One of the most important consequences of this definition of the
geometric algebra is the following

###### Proposition 2.1 (Universality).

Let @xmath be an associative algebra over @xmath with a unit denoted by
@xmath . If @xmath is linear and

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

then @xmath extends uniquely to an @xmath -algebra homomorphism @xmath ,
i.e.

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, @xmath is the unique associative @xmath -algebra with this
property.

###### Proof.

Any linear map @xmath extends to a unique algebra homomorphism @xmath
defined by @xmath etc. Property ( 2.3 ) implies that @xmath on the ideal
@xmath and so @xmath descends to a well-defined map @xmath on @xmath
which has the required properties. Suppose now that @xmath is an
associative @xmath -algebra with unit and that @xmath is an embedding
with the property that any linear map @xmath with property ( 2.3 )
extends uniquely to an algebra homomorphism @xmath . Then the
isomorphism from @xmath to @xmath clearly induces an algebra isomorphism
@xmath . ∎

So far we have not made any assumptions on the dimension of @xmath . We
will come back to the infinite-dimensional case when discussing the more
general Clifford algebra. Here we will familiarize ourselves with the
properties of quadratic forms on finite-dimensional spaces. For the
remainder of this subsection we will therefore assume that @xmath .

###### Definition 2.3.

A basis @xmath of @xmath is said to be orthogonal or canonical if @xmath
for all @xmath . The basis is called orthonormal if we also have that
@xmath for all @xmath .

We have a number of classical theorems regarding orthogonal bases.
Proofs of these can be found e.g. in [ 23 ] .

###### Theorem 2.2.

If @xmath and char @xmath then there exists an orthogonal basis of
@xmath .

Because this rather fundamental theorem breaks down for fields of
characteristic two (such as @xmath , we will always assume that char
@xmath when talking about geometric algebra. General fields and rings
will be treated by the general Clifford algebra, however.

###### Theorem 2.3 (Sylvester’s Law of Inertia).

Assume that @xmath and @xmath . If @xmath and @xmath are two orthogonal
bases of @xmath and

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

This means that there is a unique signature @xmath associated to @xmath
. For the complex case we have the following simpler result:

###### Theorem 2.4.

If @xmath and @xmath are orthogonal bases of @xmath with @xmath and

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

If @xmath ( @xmath is nondegenerate) then there exists a basis @xmath
with @xmath .

From the above follows that we can talk about the signature of a
quadratic form or a metric without ambiguity. We use the short-hand
notation @xmath to denote the @xmath -dimensional real vector space with
a quadratic form of signature @xmath , while @xmath is understood to be
the complex @xmath -dimensional space with a nondegenerate quadratic
form. When @xmath or @xmath we may simply write @xmath or @xmath . A
space of type @xmath is called euclidean and @xmath anti-euclidean ,
while the spaces @xmath ( @xmath ) are called ( anti- ) lorentzian .
Within real and complex spaces we can always find bases that are
orthonormal.

###### Remark.

The general condition for orthonormal bases to exist is that the field
@xmath is a so called spin field . This means that every @xmath can be
written as @xmath or @xmath for some @xmath . The fields @xmath , @xmath
and @xmath for @xmath a prime with @xmath , are spin, but e.g. @xmath is
not.

Consider now the geometric algebra @xmath over a real or complex space
@xmath . If we pick an orthonormal basis @xmath of @xmath it follows
from Definition 2.2 and ( 2.2 ) that @xmath is the free associative
algebra generated by @xmath modulo the relations

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

We also observe that @xmath is spanned by @xmath , where @xmath . Thus,
one can view @xmath as vector space isomorphic to @xmath , the exterior
algebra of @xmath . This is a description of geometric algebra (Clifford
algebra) which may be more familiar to e.g. physicists.

###### Remark.

If we take @xmath we actually obtain an algebra isomorphism @xmath . In
this case @xmath is called a Grassmann algebra .

One element in @xmath deserves special attention, namely the so called
pseudoscalar

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

Note that this definition is basis independent up to orientation when
@xmath is nondegenerate. Indeed, let @xmath be another orthonormal basis
with the same orientation, where @xmath , the group of linear
transformations which leave @xmath invariant ³ ³ 3 The details
surrounding such transformations will be discussed in Section 4 . Then
@xmath due to the anticommutativity of the @xmath :s. Note that, by
selecting a certain pseudoscalar for @xmath we also impose a certain
orientation on @xmath . There is no such thing as an absolute
orientation; instead all statements concerning orientation will be made
relative to the chosen one.

The square of the pseudoscalar is given by (and gives information about)
the signature and dimension of @xmath . For @xmath we have that

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

We say that @xmath is degenerate if the quadratic form is degenerate, or
equivalently if @xmath . For odd @xmath , @xmath commutes with all
elements in @xmath and the center of @xmath is @xmath . For even @xmath
, the center consists of the scalars @xmath only.

### 2.2 Combinatorial Clifford algebra @xmath

We now take a temporary step away from the comfort of fields and vector
spaces and instead consider the purely algebraic features of geometric
algebra that were uncovered in the previous subsection. Note that we
could roughly write

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

for an @xmath -dimensional space @xmath over @xmath , and that the
geometric product of these basis elements behaves as

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

and @xmath is the symmetric difference between the sets @xmath and
@xmath . Motivated by this we consider the following generalization.

###### Definition 2.4.

Let @xmath be a finite set and @xmath a commutative ring with unit. Let
@xmath be some function which is to be thought of as a signature on
@xmath . The Clifford algebra over @xmath is defined as the set

  -- -------- --
     @xmath   
  -- -------- --

i.e. the free @xmath -module generated by @xmath , the set of all
subsets of @xmath . We may use the shorter notation @xmath , or just
@xmath , when the current choice of @xmath , @xmath and @xmath is clear
from the context. We call @xmath the scalars of @xmath .

###### Example 2.3.

A typical element of @xmath could for example look like

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

We have not yet defined a product on @xmath . In addition to being
@xmath -bilinear and associative, we would like the product to satisfy
@xmath for @xmath , @xmath for @xmath and @xmath for all @xmath . In
order to arrive at such a product we make use of the following

###### Lemma 2.5.

There exists a map @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We proceed by induction on the cardinality @xmath of @xmath . For @xmath
the lemma is trivial, so let @xmath and assume the lemma holds for
@xmath . Hence, there is a @xmath which has the properties ( i )-( v )
above. If @xmath we write @xmath and, for @xmath in @xmath we extend
@xmath to @xmath in the following way:

  -- -------- --
     @xmath   
  -- -------- --

Now it is straightforward to verify that ( i )-( v ) holds for @xmath ,
which completes the proof. ∎

###### Definition 2.5.

Define the Clifford product

  -- -------- --
     @xmath   
  -- -------- --

by taking @xmath for @xmath and extending linearly. We choose to use the
@xmath which is constructed as in the proof of Lemma 2.5 by
consecutively adding elements from the set @xmath . A unique such @xmath
may only be selected after imposing a certain order (orientation) on the
set @xmath .

Using Lemma 2.5 one easily verifies that this product has all the
properties that we asked for above. For example, in order to verify
associativity we note that

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

while

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

Associativity now follows from ( iv ) and the associativity of the
symmetric difference. As is expected from the analogy with @xmath , we
also have the property that different basis elements of @xmath commute
up to a sign.

###### Proposition 2.6.

If @xmath then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By the property ( v ) in Lemma 2.5 it is sufficient to prove this for
@xmath , @xmath , where @xmath are disjoint elements in @xmath and
likewise for @xmath . If @xmath and @xmath have @xmath elements in
common then @xmath by property ( ii ). But then we are done, since
@xmath . ∎

We are now ready to make the formal connection between @xmath and @xmath
. Let @xmath be a vector space over @xmath with a quadratic form. Pick
an orthogonal basis @xmath of @xmath and consider the Clifford algebra
@xmath . Define @xmath by @xmath for @xmath and extend linearly. We then
have

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

By Proposition 2.1 , @xmath extends uniquely to a homomorphism @xmath .
Since @xmath and @xmath is easily seen to be surjective from the
property ( v ), we arrive at an isomorphism

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

We make this equivalence between @xmath and @xmath even more transparent
by suppressing the unit @xmath in expressions and writing simply @xmath
instead of @xmath for singletons @xmath . For example, with an
orthonormal basis @xmath in @xmath , both @xmath and @xmath are then
spanned by

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

There is a natural grade structure on @xmath given by the cardinality of
the subsets of @xmath . Consider the following

###### Definition 2.6.

The subspace of @xmath -vectors in @xmath , or the grade- @xmath part of
@xmath , is defined by

  -- -------- --
     @xmath   
  -- -------- --

Of special importance are the even and odd subspaces ,

  -- -------- --
     @xmath   
  -- -------- --

This notation carries over to the corresponding subspaces of @xmath and
we write @xmath , @xmath etc. where for example @xmath and @xmath . The
elements of @xmath are also called bivectors , while arbitrary elements
of @xmath are traditionally called multivectors .

We then have a split of @xmath into graded subspaces as

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

Note that, under the Clifford product, @xmath and @xmath . Hence, the
even-grade elements @xmath form a subalgebra of @xmath .

In @xmath we have the possibility of defining a unique pseudoscalar
independently of the signature @xmath , namely the set @xmath itself.
Note, however, that it can only be normalized if @xmath is invertible,
which requires that @xmath is nondegenerate. We will almost always talk
about pseudoscalars in the setting of nondegenerate vector spaces, so
this will not be a problem.

### 2.3 Standard operations

A key feature of Clifford algebras is that they contain a surprisingly
large amount of structure. In order to really be able to harness the
power of this structure we need to introduce powerful notation. Most of
the following definitions will be made on @xmath for simplicity, but
because of the equivalence between @xmath and @xmath they carry over to
@xmath in a straightforward manner.

We will find it convenient to introduce the notation that for any
proposition @xmath , @xmath will denote the number @xmath if @xmath is
true and @xmath if @xmath is false.

###### Definition 2.7.

For @xmath define

  -- -------- --
     @xmath   
  -- -------- --

and extend linearly to @xmath .

The grade involution is also called the ( first ) main involution . It
has the property

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

for all @xmath and @xmath , as is easily verified by expanding in linear
combinations of elements in @xmath and using that @xmath . The reversion
earns its name from the property

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

and it is sometimes called the second main involution or the principal
antiautomorphism . This reversing behaviour follows directly from
Proposition 2.6 . We will find it convenient to have a name for the
composition of these two involutions. Hence, we define the Clifford
conjugate @xmath of @xmath by @xmath and observe the property

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

Note that all the above involutions act by changing sign on some of the
graded subspaces. We can define general involutions of that kind which
will come in handy later.

###### Definition 2.8.

For @xmath define

  -- -------- --
     @xmath   
  -- -------- --

and extend linearly to @xmath .

We summarize the action of these involutions in Table 2.1 . Note the
periodicity.

The scalar product has the symmetric property @xmath for all @xmath .
Therefore, it forms a symmetric bilinear map @xmath which is degenerate
if and only if @xmath (i.e. the signature @xmath ) is degenerate. This
map coincides with the bilinear form @xmath when restricted to @xmath .
Note also that subspaces of different grade are orthogonal with respect
to the scalar product.

Another product that is often seen in the context of geometric algebra
is the inner product , defined by @xmath . We will stick to the left and
right inner products, however, because they admit a simpler handling of
grades, something which is illustrated ⁴ ⁴ 4 The corresponding
identities with @xmath instead of @xmath need to be supplied with grade
restrictions. by the following

###### Proposition 2.7.

For all @xmath we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

This follows directly from Definition 2.7 and basic set logic. For
example, taking @xmath we have

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

The other identities are proven in an equally simple way. ∎

To work efficiently with geometric algebra it is crucial to understand
how vectors behave under these operations.

###### Proposition 2.8.

For all @xmath and @xmath we have

  -- -------- --
     @xmath   
  -- -------- --

The first three identities are shown simply by using linearity and set
relations, while the fourth follows immediately from the second. Note
that for 1-vectors @xmath we have the basic relations

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

It is often useful to expand the various products and involutions in
terms of the grades involved. The following identities are left as
exercises.

###### Proposition 2.9.

For all @xmath we have

  -- -- --
        
  -- -- --

In the general setting of a Clifford algebra with scalars in a ring
@xmath , we need to be careful about the notion of linear
(in-)dependence. A subset @xmath of @xmath is called linearly dependent
iff there exist @xmath , not all zero, such that

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

Note that a single nonzero 1-vector could be linearly dependent in this
context. We will prove an important theorem concerning linear dependence
where we need the following

###### Lemma 2.10.

If @xmath and @xmath are 1-vectors then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Since both sides of the expression are multilinear and alternating in
both the @xmath :s and the @xmath :s, we need only consider ordered
disjoint elements @xmath in the basis of singleton sets in @xmath . Both
sides are zero, except in the case

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

so we are done. ∎

###### Theorem 2.11.

The 1-vectors @xmath are linearly independent iff the m-vector @xmath is
linearly independent.

###### Proof.

Assume that @xmath , where, say, @xmath . Then

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

since @xmath .

Conversely, assume that @xmath for @xmath in @xmath and @xmath . We will
use the basis minor theorem for arbitrary rings which can be found in
the appendix. Assume that @xmath , where @xmath and @xmath are basis
elements such that @xmath . This assumption on the signature is no loss
in generality, since this theorem only concerns the exterior algebra
associated to the outer product. It will only serve to simplify our
reasoning below. Collect the coordinates in a matrix

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

and note that we can expand @xmath in a grade- @xmath basis as

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

where we used Lemma 2.10 . We find that the determinant of each @xmath
minor of @xmath is zero.

Now, let @xmath be the rank of @xmath . Then we must have @xmath , and
if @xmath then @xmath and @xmath for all @xmath , @xmath . But that
would mean that @xmath are linearly dependent. Therefore we assume that
@xmath and, without loss of generality, that

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

By the basis minor theorem there exist @xmath such that

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

Hence, @xmath are linearly dependent. ∎

For our final set of operations, we will consider a nondegenerate
geometric algebra @xmath with pseudoscalar @xmath . The nondegeneracy
implies that there exists a natural duality between the inner and outer
products.

###### Definition 2.9.

We define the dual of @xmath by @xmath . The dual outer product or meet
@xmath is defined such that the diagram

  -- -------- --
     @xmath   
  -- -------- --

commutes, i.e. @xmath .

###### Remark.

In @xmath , the corresponding dual of @xmath is @xmath , the complement
of the set @xmath . Hence, we really find that the dual is the
linearization of a sign (or orientation) -respecting complement. This
motivates our choice of notation.

###### Proposition 2.12.

For all @xmath we have

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

###### Proof.

Using Proposition 2.7 and the fact that @xmath , we obtain

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

and from this follows also the second identity

  -- -- -- --------
           (2.31)
  -- -- -- --------

It is instructive to compare these results with those in the language of
differential forms and Hodge duality, which are completely equivalent.
In that setting one often starts with an outer product and then uses a
metric to define a dual. The inner product is then defined from the
outer product and dual according to ( 2.29 ).

### 2.4 Vector space geometry

We will now leave the general setting of Clifford algebra for a moment
and instead focus on the geometric properties of @xmath and its newly
defined operations.

###### Definition 2.10.

A blade is an outer product of 1-vectors. We define the following:

  -- -------- --
     @xmath   
  -- -------- --

The basis blades associated to an orthogonal basis @xmath is the basis
of @xmath generated by @xmath , i.e.

  -- -------- --
     @xmath   
  -- -------- --

We include the unit 1 among the blades and call it the @xmath -blade.
Note that @xmath and that by applying Proposition 2.8 recursively we can
expand a blade as a sum of geometric products,

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

This expression is clearly similar to a determinant, except that this is
a product of vectors instead of scalars.

The key property of blades is that they represent linear subspaces of
@xmath . This is made precise by the following

###### Proposition 2.13.

If @xmath is a nonzero @xmath -blade and @xmath then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

This follows directly from Theorem 2.11 since @xmath are linearly
independent and @xmath are linearly dependent. ∎

Hence, to every nonzero @xmath -blade @xmath there corresponds a unique
@xmath -dimensional subspace

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

Conversely, if @xmath is a @xmath -dimensional subspace of @xmath , then
we can find a nonzero @xmath -blade @xmath representing @xmath by simply
taking a basis @xmath of @xmath and forming

  -- -------- -- --------
     @xmath      (2.34)
  -- -------- -- --------

We thus have the geometric interpretation of blades as subspaces with an
associated orientation (sign) and magnitude. Since every element in
@xmath is a linear combination of basis blades, we can think of every
element as representing a linear combination of subspaces. In the case
of a nondegenerate algebra these basis subspaces are nondegenerate as
well. On the other hand, any blade which represents a nondegenerate
subspace can also be treated as a basis blade associated to an
orthogonal basis. This will follow in the discussion below.

###### Proposition 2.14.

Every @xmath -blade can be written as a geometric product of @xmath
vectors.

###### Proof.

Take a nonzero @xmath . Pick an orthogonal basis @xmath of the subspace
@xmath . Then we can write @xmath for some @xmath , and @xmath by ( 2.32
). ∎

There are a number of useful consequences of this result.

###### Corollary.

If @xmath then @xmath is a scalar.

###### Proof.

Use the expansion of @xmath above to obtain

  -- -------- -- --------
     @xmath      (2.35)
  -- -------- -- --------

###### Corollary.

If @xmath then @xmath has an inverse @xmath .

###### Corollary.

If @xmath then @xmath is nondegenerate on @xmath and there exists an
orthogonal basis @xmath of @xmath such that @xmath .

###### Proof.

The first statement follows directly from ( 2.35 ). For the second
statement note that, since @xmath is nondegenerate on @xmath , we have
@xmath . Take an orthogonal basis @xmath of @xmath . For any @xmath we
have that @xmath . Thus, @xmath and we can extend @xmath to an
orthogonal basis of @xmath consisting of one part in @xmath and one part
in @xmath . By rescaling this basis we have @xmath . ∎

###### Remark.

Note that if we have an orthogonal basis of a subspace of @xmath where
@xmath is degenerate, then it may not be possible to extend this basis
to an orthogonal basis for all of @xmath . @xmath for example has two
null-spaces, but these are not orthogonal. If the space is euclidean or
anti-euclidean, though, orthogonal bases can always be extended (e.g.
using the Gram-Schmidt algorithm).

It is useful to be able to work efficiently with general bases of @xmath
and @xmath which need not be orthogonal. Let @xmath be any basis of
@xmath . Then @xmath is a basis of @xmath , where we use a multi-index
notation

  -- -------- -- --------
     @xmath      (2.36)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (2.37)
  -- -------- -- --------

Sums over @xmath are understood to be performed over all allowed such
indices. If @xmath is nondegenerate then the scalar product @xmath is
also nondegenerate and we can find a so called reciprocal basis @xmath
of @xmath such that

  -- -------- -- --------
     @xmath      (2.38)
  -- -------- -- --------

The reciprocal basis is easily verified to be given by

  -- -------- -- --------
     @xmath      (2.39)
  -- -------- -- --------

where @xmath denotes a deletion. Furthermore, we have that @xmath is a
reciprocal basis of @xmath , where @xmath . This follows since by Lemma
2.10 and ( 2.38 ),

  -- -------- -- --------
     @xmath      (2.40)
  -- -------- -- --------

We now have the coordinate expansions

  -- -------- -- --------
     @xmath      (2.41)
  -- -------- -- --------

In addition to being useful in coordinate expansions, the general and
reciprocal bases also provide a geometric understanding of the dual
operation because of the following

###### Theorem 2.15.

Assume that @xmath is nondegenerate. If @xmath and we extend @xmath to a
basis @xmath of @xmath then

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the reciprocal basis of @xmath .

###### Proof.

Using an expansion of the inner product into sub-blades (this will not
be explained in detail here, see [ 9 ] or [ 23 ] ) plus orthogonality (
2.38 ), we obtain

  -- -------- -- --------
     @xmath      (2.42)
  -- -------- -- --------

###### Corollary.

If @xmath and @xmath are blades then @xmath , @xmath , @xmath and @xmath
are blades as well.

The blade-subspace correspondence then gives us a geometric
interpretation of these operations.

###### Proposition 2.16.

If @xmath are nonzero blades then @xmath and

  -- -------- --
     @xmath   
  -- -------- --

The proofs of the statements in the above corollary and proposition are
left as exercises. Some of them can be found in [ 23 ] and [ 9 ] .

### 2.5 Linear functions

Since @xmath is itself a vector space which embeds @xmath , it is
natural to consider the properties of linear functions on @xmath . There
is a special class of such functions, called outermorphisms, which can
be said to respect the structure of @xmath in a natural way. We will see
that, just as the geometric algebra @xmath is completely determined by
the underlying vector space @xmath , an outermorphism is completely
determined by its behaviour on @xmath .

###### Definition 2.11.

A linear map @xmath is called an outermorphism or @xmath -morphism if

  -- -------- --
     @xmath   
  -- -------- --

A linear transformation @xmath is called a dual outermorphism or @xmath
-morphism if

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 2.17.

For every linear map @xmath there exists a unique outermorphism @xmath
such that @xmath .

###### Proof.

Take a general basis @xmath of @xmath and define, for @xmath ,

  -- -------- -- --------
     @xmath      (2.43)
  -- -------- -- --------

and extend @xmath to the whole of @xmath by linearity. We also define
@xmath for @xmath . Hence, ( i ) and ( ii ) are satisfied. ( iii ) is
easily verified by expanding in the induced basis @xmath of @xmath .
Unicity is obvious since our definition was necessary. ∎

Uniqueness immediately implies the following.

###### Corollary.

If @xmath and @xmath are linear then @xmath .

###### Corollary.

If @xmath is an outermorphism then @xmath .

In the setting of @xmath this means that an outermorphism @xmath is
completely determined by its values on @xmath .

We have noted that a nondegenerate @xmath results in a nondegenerate
bilinear form @xmath . This gives us a canonical isomorphism @xmath
between the elements of @xmath and the linear functionals on @xmath as
follows. For every @xmath we define a linear functional @xmath by @xmath
. Taking a general basis @xmath of @xmath and using ( 2.40 ) we obtain a
dual basis @xmath such that @xmath . Now that we have a canonical way of
moving between @xmath and its dual space @xmath , we can for every
linear map @xmath define an adjoint map @xmath by

  -- -------- -- --------
     @xmath      (2.44)
  -- -------- -- --------

Per definition, this has the expected and unique property

  -- -------- -- --------
     @xmath      (2.45)
  -- -------- -- --------

for all @xmath . Note that if we restrict our attention to @xmath this
construction results in the usual adjoint @xmath of a linear map @xmath
.

###### Theorem 2.18 (Hestenes’ Theorem).

Assume that @xmath is nondegenerate and let @xmath be an outermorphism.
Then the adjoint @xmath is also an outermorphism and

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

###### Proof.

We first prove that @xmath is an outermorphism. The fact that @xmath is
grade preserving follows from ( 2.45 ) and the grade preserving property
of @xmath . Now take basis blades @xmath and @xmath with @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

where we have used Lemma 2.10 . By linearity and nondegeneracy it
follows that @xmath is an outermorphism. The first identity stated in
the therorem now follows quite easily from Proposition 2.7 . For any
@xmath we have

  -- -------- --
     @xmath   
  -- -------- --

The nondegeneracy of the scalar product then gives the first identity.
The second identity is proven similarly, using that @xmath . ∎

From uniqueness of outermorphisms we also obtain the following

###### Corollary.

If @xmath is a linear transformation then @xmath .

This means that we can simply write @xmath for the adjoint outermorphism
of @xmath .

A powerful concept in geometric algebra (or exterior algebra) is the
generalization of eigenvectors to so called eigenblades . For a function
@xmath , a @xmath -eigenblade with eigenvalue @xmath is a blade @xmath
such that

  -- -------- -- --------
     @xmath      (2.46)
  -- -------- -- --------

Just as eigenvectors can be said to represent invariant 1-dimensional
subspaces of a function, a @xmath -blade with nonzero eigenvalue
represents an invariant @xmath -dimensional subspace. One important
example of an eigenblade is the pseudoscalar @xmath , which represents
the whole invariant vector space @xmath . Since @xmath is grade
preserving, we must have @xmath for some @xmath which we call the
determinant of @xmath , i.e.

  -- -------- -- --------
     @xmath      (2.47)
  -- -------- -- --------

Expanding @xmath in a basis using Lemma 2.10 , one finds that this
agrees with the usual definition of the determinant of a linear
function.

In the following we assume that @xmath is nondegenerate, so that @xmath
.

###### Definition 2.12.

For linear @xmath we define the dual map @xmath by @xmath , so that the
following diagram commutes:

  -- -------- --
     @xmath   
  -- -------- --

###### Proposition 2.19.

We have the following properties of the dual map:

  -- -------- --
     @xmath   
  -- -------- --

for all linear @xmath .

The proofs are simple and left as exercises to the reader. As a special
case of Theorem 2.18 we obtain, with @xmath and a linear map @xmath ,

  -- -------- -- --------
     @xmath      (2.48)
  -- -------- -- --------

so that

  -- -------- -- --------
     @xmath      (2.49)
  -- -------- -- --------

If @xmath we then have a simple expression for the inverse;

  -- -------- -- --------
     @xmath      (2.50)
  -- -------- -- --------

which is essentially the dual of the adjoint. @xmath is obtained by
restricting to @xmath . An orthogonal transformation @xmath has @xmath
and @xmath , so in that case @xmath .

### 2.6 Infinite-dimensional Clifford algebra

This far we have only defined the Clifford algebra @xmath of a finite
set @xmath , resulting in a finite-dimensional algebra @xmath whenever
@xmath is a field. In order for this combinatorial construction to
qualify as a complete generalization of @xmath , we would at least like
to be able to define the corresponding Clifford algebra of an
infinite-dimensional vector space, something which was possible for
@xmath in Definition 2.2 .

The treatment of @xmath in the previous subsections has been
deliberately put in a form which eases the generalization to an infinite
@xmath . Reconsidering Definition 2.4 , we now have two possibilites;
either we consider the set @xmath of all subsets of @xmath , or the set
@xmath of all finite subsets. We therefore define, for an arbitrary set
@xmath , ring @xmath , and signature @xmath ,

  -- -------- -- --------
     @xmath      (2.51)
  -- -------- -- --------

Elements in @xmath ( @xmath ) are finite linear combinations of (finite)
subsets of @xmath .

Our problem now is to define a Clifford product for @xmath and @xmath .
This can be achieved just as in the finite case if only we can find a
map @xmath satisfying the conditions in Lemma 2.5 . This is certainly
not a trivial task. Starting with the case @xmath it is sufficient to
construct such a map on @xmath .

We call a map @xmath grassmannian on @xmath if it satisfies ( i )-( v )
in Lemma 2.5 , with @xmath replaced by @xmath .

###### Theorem 2.20.

For any @xmath there exists a grassmannian map on @xmath .

###### Proof.

We know that there exists such a map for any finite @xmath . Let @xmath
and assume @xmath is grassmannian on @xmath . If there exists @xmath we
can, by proceeding as in the proof of Lemma 2.5 , extend @xmath to a
grassmannian map @xmath on @xmath such that @xmath .

We will now use transfinite induction , or the Hausdorff maximality
theorem ⁵ ⁵ 5 This theorem should actually be regarded as an axiom of
set theory since it is equivalent to the Axiom of Choice. , to prove
that @xmath can be extended to all of @xmath . Note that if @xmath is
grassmannian on @xmath then @xmath is also a relation @xmath . Let

  -- -- -- --------
           (2.52)
  -- -- -- --------

Then @xmath is partially ordered by

  -- -------- -- --------
     @xmath      (2.53)
  -- -------- -- --------

By the Hausdorff maximality theorem, there exists a maximal totally
ordered “chain” @xmath . Put @xmath . We want to define a grassmannian
map @xmath on @xmath , for if we succeed in that, we find @xmath and can
conclude that @xmath by maximality and the former result.

Take finite subsets @xmath and @xmath of @xmath . Each of the finite
elements in @xmath lies in some @xmath such that @xmath . Therefore, by
the total ordering of @xmath , there exists one such @xmath containing
@xmath . Put @xmath , where @xmath is this chosen element in @xmath .
@xmath is well-defined since if @xmath and @xmath where @xmath then
@xmath or @xmath and @xmath agree on @xmath . It is easy to verify that
this @xmath is grassmannian on @xmath , since for each @xmath there
exists @xmath such that @xmath . ∎

We have shown that there exists a map @xmath with the properties in
Lemma 2.5 . We can then define the Clifford product on @xmath as usual
by @xmath for @xmath and linear extension. Since only finite subsets are
included, most of the previous constructions for finite-dimensional
@xmath carry over to @xmath . For example, the decomposition into graded
subspaces remains but now goes up towards infinity,

  -- -------- -- --------
     @xmath      (2.54)
  -- -------- -- --------

Furthermore, Proposition 2.6 still holds, so the reverse and all other
involutions behave as expected.

The following theorem shows that it is possible to extend @xmath all the
way to @xmath even in the infinite case. We therefore have a Clifford
product also on @xmath .

###### Theorem 2.21.

For any set @xmath there exists a map @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, for any commutative ring @xmath and signature @xmath such
that @xmath is contained in a finite and multiplicatively closed subset
of @xmath , there exists a map @xmath such that properties ( i )-( v )
in Lemma 2.5 hold, plus

  -- -------- --
     @xmath   
  -- -------- --

Here, @xmath denotes the set of all subsets of @xmath with @xmath
elements. Note that for a finite set @xmath , @xmath so that for example
@xmath (in general, @xmath ) and @xmath . This enables us to extend the
basic involutions @xmath , @xmath and @xmath to infinite sets as

  -- -------- --
     @xmath   
  -- -------- --

and because @xmath still holds, we find that they satisfy the
fundamental properties ( 2.16 )-( 2.18 ) for all elements of @xmath .
The extra requirement ( vi ) on @xmath was necessary here since we
cannot use Proposition 2.6 for infinite sets. Moreover, we can no longer
write the decomposition ( 2.54 ) since it goes beyond finite grades. We
do have even and odd subspaces, though, defined by

  -- -------- -- --------
     @xmath      (2.55)
  -- -------- -- --------

@xmath and @xmath (with this @xmath ) are both subalgebras of @xmath .

It should be emphasized that @xmath needs not be zero on intersecting
infinite sets (a rather trivial solution), but if e.g. @xmath we can
also demand that @xmath . Theorem 2.21 can be proved using nonstandard
analysis / internal set theory and we will not consider this here.

Let us now see how @xmath and @xmath can be applied to the setting of an
infinite-dimensional vector space @xmath over a field @xmath and with a
quadratic form @xmath . By the Hausdorff maximality theorem one can
actually find a (necessarily infinite) orthogonal basis @xmath for this
space in the sense that any vector in @xmath can be written as a finite
linear combination of elements in @xmath and that @xmath for any pair of
disjoint elements @xmath . We then have

  -- -------- -- --------
     @xmath      (2.56)
  -- -------- -- --------

which is proved just like in the finite-dimensional case, using
Proposition 2.1 . The only difference is that one needs to check that
the homomorphism @xmath is also injective.

The @xmath -blades of @xmath represent @xmath -dimensional subspaces of
@xmath even in the infinite case. Due to the intuitive and powerful
handling of finite-dimensional geometry which was possible in a
finite-dimensional @xmath , it would be extremely satisfying to be able
to generalize the blade concept to e.g. closed subspaces of an
infinite-dimensional Hilbert space. One could hope that the infinite
basis subsets in @xmath provide this generalization. Unfortunately, this
is not so easy since @xmath depends heavily on the choice of basis
@xmath . Let us sketch an intuitive picture of why this is so.

With a countable basis @xmath , an infinite basis blade in @xmath could
be thought of as an infinite product @xmath . A change of basis to
@xmath would turn each @xmath into a finite linear combination of
elements in @xmath , e.g. @xmath . However, this would require @xmath to
be an infinite sum of basis blades in @xmath , which is not allowed.
Note that this is no problem in @xmath since a basis blade @xmath is a
finite product and the change of basis therefore results in a finite
sum. It may be possible to treat infinite sums in @xmath by taking the
topology of @xmath into account, but at present this issue is not clear.

Finally, we consider a nice application of the infinite-dimensional
Clifford algebra @xmath . For a vector space @xmath , define the
simplicial complex algebra

  -- -------- -- --------
     @xmath      (2.57)
  -- -------- -- --------

where we forget about the vector space structure of @xmath and treat
individual points @xmath as orthogonal basis 1-vectors in @xmath with
@xmath . The dot indicates that we think of @xmath as a point rather
than a vector. A basis @xmath -blade in @xmath consists of a product
@xmath of individual points and represents a (possibly degenerate)
oriented @xmath -simplex in @xmath . This simplex is given by the convex
hull

  -- -------- -- --------
     @xmath      (2.58)
  -- -------- -- --------

Hence, an arbitrary element in @xmath is a linear combination of
simplices and can therefore represent a simplicial complex in @xmath .
The restriction of @xmath to the @xmath -simplices of a simplicial
complex @xmath is usually called the @xmath -chain group @xmath . Here
the generality of the ring @xmath comes in handy because one often works
with @xmath in this context.

The Clifford algebra structure of @xmath handles the orientation of the
simplices, so that e.g. the line from the point @xmath to @xmath is
@xmath . Furthermore, it allows us to define the boundary operator

  -- -------- --
     @xmath   
  -- -------- --

Note that this is well-defined since only a finite number of points
@xmath can be present in any fixed @xmath . For a @xmath -simplex, we
have

  -- -------- -- --------
     @xmath      (2.59)
  -- -------- -- --------

This shows that @xmath really is the traditional boundary operator on
simplices. Proposition 2.7 now makes the proof of @xmath a triviality,

  -- -------- -- --------
     @xmath      (2.60)
  -- -------- -- --------

We can also assign a geometric measure @xmath to simplices, by mapping a
@xmath -simplex to a corresponding @xmath -blade in @xmath representing
the directed volume of the simplex. Define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and extending linearly. One can verify that this is well-defined and
that the geometric measure of a boundary is zero, i.e. @xmath . One can
take this construction even further and arrive at “discrete” equivalents
of differentials, integrals and Stokes’ theorem. See [ 23 ] or [ 17 ]
for more on this.

This completes our excursion to infinite-dimensional Clifford algebras.
In the following sections we will always assume that @xmath is finite
and @xmath finite-dimensional.

## 3 Isomorphisms

In this section we establish an extensive set of relations between real
and complex geometric algebras of varying signature. This eventually
leads to an identification of these algebras as matrix algebras over
@xmath , @xmath , or the quaternions @xmath . The complete listing of
such identifications is usually called the classification of geometric
algebras.

We have seen that the even subspace @xmath of @xmath constitutes a
subalgebra. The following proposition tells us that this subalgebra
actually is the geometric algebra of a space of one dimension lower.

###### Proposition 3.1.

We have the algebra isomorphisms

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath for which the expressions make sense.

###### Proof.

Take an orthonormal basis @xmath of @xmath such that @xmath , @xmath ,
and a corresponding basis @xmath of @xmath . Define @xmath by mapping

  -- -------- --
     @xmath   
  -- -------- --

and extending linearly. We then have

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

for all reasonable @xmath . By Proposition 2.1 (universality) we can
extend @xmath to a homomorphism @xmath . Since @xmath and @xmath is
easily seen to be surjective, we have that @xmath is an isomorphism.

For the second statement, we take a corresponding basis @xmath of @xmath
and define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Proceeding as above, we obtain the isomorphism. ∎

###### Corollary.

It follows immediately that

  -- -------- --
     @xmath   
  -- -------- --

In the above and further on we use the notation @xmath for completeness.

The property of geometric algebras that leads us to their eventual
classification as matrix algebras is that they can be split up into
tensor products of geometric algebras of lower dimension.

###### Proposition 3.2.

We have the algebra isomorphisms

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , @xmath and @xmath for which the expressions make sense.

###### Proof.

For the first expression, take orthonormal bases @xmath of @xmath ,
@xmath of @xmath and @xmath of @xmath . Define a mapping @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and extend to an algebra homomorphism @xmath using the universal
property. Since @xmath maps onto a set of generators for @xmath it is
clearly surjective. Furthermore, @xmath , so @xmath is an isomorphism.

The second expression is proved similarly. For the third expression,
take orthonormal bases @xmath of @xmath , @xmath of @xmath and @xmath of
@xmath , where @xmath etc. Define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Proceeding as above, we can extend @xmath to an algebra isomorphism. ∎

We can also relate certain real geometric algebras to complex
equivalents.

###### Proposition 3.3.

If @xmath is odd and @xmath then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Since @xmath is odd, the pseudoscalar @xmath commutes with all other
elements. This, together with the property @xmath , makes it a good
candidate for a scalar imaginary. Define @xmath by linear extension of

  -- -------- --
     @xmath   
  -- -------- --

for even basis blades @xmath . @xmath is easily seen to be an injective
algebra homomorphism. Using that the dimensions of these algebras are
equal, we have an isomorphism.

For the second isomorphism, note that Proposition 3.1 gives us @xmath .
Finally, the order of complexification is unimportant since all
nondegenerate complex quadratic forms are equivalent. ∎

###### Corollary.

It follows immediately that, for these conditions,

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , @xmath such that @xmath .

One important consequence of the tensor algebra isomorphisms in
Proposition 3.2 is that geometric algebras experience a kind of
periodicity over 8 real dimensions in the underlying vector space.

###### Proposition 3.4.

For all @xmath , there are periodicity isomorphisms

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Using Proposition 3.2 repeatedly, we obtain

  -- -------- --
     @xmath   
  -- -------- --

and analogously for the second statement.

For the last statement we take orthonormal bases @xmath of @xmath ,
@xmath of @xmath and @xmath of @xmath . Define a mapping @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and extend to an algebra isomorphism as usual. ∎

###### Theorem 3.5.

We obtain the classification of real geometric algebras as matrix
algebras, given by Table 3.1 together with the periodicity

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We have the following easily verified isomorphisms:

  -- -------- --
     @xmath   
  -- -------- --

Some of these will be explained in detail in Section 5 . We can now work
out the cases @xmath and @xmath for @xmath in a criss-cross fashion
using Proposition 3.2 and the tensor algebra isomorphisms

  -- -------- --
     @xmath   
  -- -------- --

For proofs of these, see e.g. [ 14 ] . With @xmath and Proposition 3.2
we can then work our way through the whole table diagonally. The
periodicity follows from Proposition 3.4 and @xmath . ∎

Because all nondegenerate complex quadratic forms on @xmath are
equivalent, the complex version of this theorem turns out to be much
simpler.

###### Theorem 3.6.

We obtain the classification of complex geometric algebras as matrix
algebras, given by

  -- -------- --
     @xmath   
  -- -------- --

together with the periodicity

  -- -------- --
     @xmath   
  -- -------- --

In other words,

  -- -------- --
     @xmath   
  -- -------- --

for @xmath

###### Proof.

The isomorphism @xmath gives us

  -- -------- --
     @xmath   
  -- -------- --

Then use Proposition 3.4 for periodicity. ∎

The periodicity of geometric algebras actually has a number of
far-reaching consequences. One example is Bott periodicity , which
simply put gives a periodicity in the homotopy groups @xmath of the
unitary, orthogonal and symplectic groups. See [ 14 ] for proofs using
K-theory or [ 18 ] for examples.

## 4 Groups

One of the foremost reasons that geometric algebras appear naturally in
so many areas of mathematics and physics is the fact that they contain a
number of important groups. These are groups under the geometric product
and thus lie embedded within the group of invertible elements in @xmath
. In this section we will discuss the properties of various embedded
groups and their relation to other familiar transformation groups such
as the orthogonal and Lorentz groups. We will also introduce a
generalized concept of spinor and see how such objects are related to
the embedded groups.

###### Definition 4.1.

We identify the following groups embedded in @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the set of invertible vectors.

The versor group @xmath is the smallest group which contains @xmath .
Its elements are finite products of invertible vectors called versors .
As is hinted in Definition 4.1 , many important groups are subgroups of
this group. One of the central, and highly non-trivial, results of this
section is that the versor and Lipschitz groups actually are equal.
Therefore, @xmath is also called the Lipschitz group in honor of its
creator. Sometimes it is also given the name Clifford group , but we
will, in accordance with other conventions, use that name to denote the
finite group generated by an orthonormal basis.

The Pin and Spin groups are both generated by unit vectors, and in the
case of Spin, only an even number of such vector factors can be present.
The elements of Spin @xmath are called rotors . As we will see, these
groups are intimately connected to orthogonal groups and rotations.

Throughout this section we will always assume that our scalars are real
numbers unless otherwise stated. This is reasonable both from a
geometric viewpoint and from the fact that e.g. many complex groups can
be represented by groups embedded in real geometric algebras.
Furthermore, we assume that @xmath is nondegenerate so that we are
working with a vector space of type @xmath . The corresponding groups
associated to this space will be denoted Spin @xmath etc.

### 4.1 Group actions on @xmath

In order to understand how groups embedded in a geometric algebra are
related to more familiar groups of linear transformations, it is
necessary to study how groups in @xmath can act on the vector space
@xmath itself and on the embedded underlying vector space @xmath . The
following are natural candidates for such actions.

###### Definition 4.2.

Using the geometric product, we have the following natural actions:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the (vector space) endomorphisms of @xmath .

Note that @xmath and @xmath are algebra homomorphisms while Ad and
@xmath are group homomorphisms. These actions give rise to canonical
representations of the groups embedded in @xmath . The twisted adjoint
action takes the graded structure of @xmath into account and will be
seen to play a more important role than the normal adjoint action in
geometric algebra. Using the expansion ( 2.32 ) one can verify that
@xmath is always an outermorphism, while in general @xmath is not. Note,
however, that these actions agree on the subgroup of even elements
@xmath .

###### Remark.

We note that, because the algebra @xmath is assumed to be
finite-dimensional, left inverses are always right inverses and vice
versa . This can be seen as follows. First note that the left and right
actions are injective. Namely, assume that @xmath . Then @xmath and in
particular @xmath . Suppose now that @xmath for some @xmath . But then
@xmath , so that @xmath is a right inverse to @xmath . Now, using the
dimension theorem

  -- -------- --
     @xmath   
  -- -------- --

with @xmath , we can conclude that @xmath is also a left inverse to
@xmath . Hence, @xmath , and @xmath .

Let us study the properties of the twisted adjoint action. For @xmath we
obtain

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

and if @xmath is orthogonal to @xmath ,

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

Hence, @xmath is a reflection in the hyperplane orthogonal to @xmath .
For a general versor @xmath we have

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

i.e. the twisted adjoint representation (restricted to act only on
@xmath which is clearly invariant) gives a homomorphism from the versor
group into the group of orthogonal transformations ,

  -- -------- --
     @xmath   
  -- -------- --

We have the following fundamental theorem regarding the orthogonal
group.

###### Theorem 4.1 (Cartan-DieudonnÃ©).

Every orthogonal map on a non-degenerate space @xmath is a product of
reflections. The number of reflections required is at most equal to the
dimension of @xmath .

For a constructive proof which works well for arbitrary signatures, see
[ 23 ] .

###### Corollary.

@xmath is surjective.

###### Proof.

We know that any @xmath can be written @xmath for some invertible
vectors @xmath , @xmath . But then @xmath , where @xmath . ∎

### 4.2 The Lipschitz group

We saw above that the twisted adjoint representation maps the versor
group onto the group of orthogonal transformations of @xmath . The
largest group in @xmath for which @xmath forms a representation on
@xmath , i.e. leaves @xmath invariant, is per definition the Lipschitz
group @xmath . We saw from ( 4.3 ) that @xmath .

We will now introduce an important function on @xmath , traditionally
called the norm function ,

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

The name is a bit misleading since @xmath is not even guaranteed to take
values in @xmath . For some special cases of algebras, however, it does
act as a natural norm and we will see that it can be extended in many
lower-dimensional algebras where it will act as a kind of determinant.
Our first main result for this function is that it acts as a determinant
on @xmath . This will help us prove that @xmath .

###### Lemma 4.2.

Assume that @xmath is nondegenerate. If @xmath and @xmath for all @xmath
then @xmath must be a scalar.

###### Proof.

Using Proposition 2.8 we have that @xmath for all @xmath . This means
that, for a @xmath -blade, @xmath whenever @xmath . The nondegeneracy of
the scalar product implies that @xmath must have grade 0. ∎

###### Theorem 4.3.

The norm function is a group homomorphism @xmath .

###### Proof.

First note that if @xmath then also @xmath and @xmath , hence @xmath and
@xmath .

Now take @xmath . Then @xmath for all @xmath and therefore

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

This means that @xmath , or @xmath . By Lemma 4.2 we find that @xmath .
The homomorphism property now follows easily, since for @xmath ,

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

Finally, because @xmath , we must have that @xmath is nonzero. ∎

###### Lemma 4.4.

The homomorphism @xmath has kernel @xmath .

###### Proof.

We first prove that @xmath is orthogonal for @xmath . Note that, for
@xmath ,

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

Then, since @xmath , we have that @xmath .

Now, if @xmath then @xmath for all @xmath and by Lemma 4.2 we must have
@xmath . ∎

We finally obtain

###### Theorem 4.5.

We have that @xmath .

###### Proof.

We saw earlier that @xmath . Take @xmath . By the above lemma we have
@xmath . Using the corollary to Theorem 4.1 we then find that @xmath for
some @xmath . Then @xmath and @xmath . Hence, @xmath . ∎

### 4.3 Properties of Pin and Spin groups

From the discussion above followed that @xmath gives a surjective
homomorphism from the versor, or Lipschitz, group @xmath to the
orthogonal group. The kernel of this homomorphism is the set of
invertible scalars. Because the Pin and Spin groups consist of
normalized versors ( @xmath ) we find the following

###### Theorem 4.6.

The homomorphisms

  -- -------- --
     @xmath   
  -- -------- --

are surjective with kernel @xmath .

The homomorphism onto the special orthogonal group ,

  -- -------- --
     @xmath   
  -- -------- --

follows since it is generated by an even number of reflections. @xmath
denotes the connected component of SO containing the identity. This will
soon be explained.

In other words, the Pin and Spin groups are two-sheeted coverings of the
orthogonal groups. Furthermore, we have the following relations between
these groups.

Take a unit versor @xmath . If @xmath is odd we can always multiply by a
unit vector @xmath so that @xmath and @xmath . Furthermore, when the
signature is euclidean we have @xmath for all unit versors. The same
holds for even unit versors in anti-euclidean spaces since the signs
cancel out. Hence, @xmath unless there is mixed signature. But in that
case we can find two orthogonal unit vectors @xmath such that @xmath and
@xmath . Since @xmath we then have that @xmath , where @xmath if @xmath
.

Summing up, we have that, for @xmath and any pair of orthogonal vectors
@xmath such that @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Otherwise,

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath such that @xmath . From the isomorphism @xmath we also
have the signature symmetry @xmath . In all cases,

  -- -------- --
     @xmath   
  -- -------- --

From these considerations it is sufficient to study the properties of
the rotor groups in order to understand the Pin, Spin and orthogonal
groups. Fortunately, it turns out that the rotor groups have very
convenient topological features.

###### Theorem 4.7.

The groups @xmath are pathwise connected for @xmath or @xmath .

###### Proof.

Pick a rotor @xmath , where @xmath or @xmath is greater than one. Then
@xmath with an even number of @xmath such that @xmath and an even number
such that @xmath . Note that for any two invertible vectors @xmath we
have @xmath , where @xmath . Hence, we can rearrange the vectors so that
those with positive square come first, i.e.

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

where @xmath and @xmath are so called simple rotors which are connected
to either 1 or -1. This holds because @xmath , so we can, as is easily
verified, write @xmath for some @xmath (exponentials of bivectors will
be treated shortly). Depending on the signature of the plane associated
to @xmath , i.e. on the sign of @xmath , the set @xmath forms either a
circle, a line or a hyperbola. In any case, it goes through the unit
element. Finally, since @xmath or @xmath we can connect -1 to 1 with for
example the circle @xmath , where @xmath are two orthonormal basis
elements with the same signature. ∎

Continuity of @xmath now implies that the set of rotations represented
by rotors, i.e. @xmath , forms a continuous subgroup containing the
identity. For euclidean and lorentzian signatures, we have an even
simpler situation.

###### Theorem 4.8.

The groups @xmath are simply connected for @xmath , @xmath , @xmath or
@xmath , where @xmath . Hence, these are the universal covering groups
of @xmath .

This follows because @xmath for these signatures. See e.g. [ 14 ] for
details.

This sums up the the situation nicely for higher-dimensional euclidean
and lorentzian spaces: The Pin group, which is a double-cover of the
orthogonal group, consists of two or four simply connected components.
These components are copies of the rotor group. In physics-terminology
these components correspond to time and parity reflections.

It is also interesting to relate the rotor group to its Lie algebra,
which is actually the bivector space @xmath with the usual commutator
@xmath . This follows because there is a Lie algebra isomorphism between
@xmath and the algebra of antisymmetric transformations of @xmath ,
given by

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

where @xmath is some general basis of @xmath . One verifies, by
expanding in the geometric product, that

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

Actually, this bivector Lie algebra is more general than it might first
seem. Doran and Lasenby (see [ 3 ] or [ 4 ] ) have shown that the Lie
algebra @xmath of the general linear group can be represented as a
bivector algebra. From the fact that any finite-dimensional Lie algebra
has a faithful finite-dimensional representation (Ado’s Theorem for
characteristic zero, Iwasawa’s Theorem for nonzero characteristic, see
e.g. [ 11 ] ) we have that any finite-dimensional real or complex Lie
algebra can be represented as a bivector algebra.

We define the exponential of a multivector @xmath as the usual power
series

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

Since @xmath is finite-dimensional we have the following for any choice
of norm on @xmath .

###### Proposition 4.9.

The sum in ( 4.11 ) converges for all @xmath and

  -- -------- --
     @xmath   
  -- -------- --

The following now holds for any signature.

###### Theorem 4.10.

For any bivector @xmath we have that @xmath .

###### Proof.

It is obvious that @xmath is an even multivector and that @xmath .
Hence, it is sufficient to prove that @xmath , or by Theorem 4.5 , that
@xmath . This can be done by considering derivatives of the function
@xmath for @xmath . See e.g. [ 23 ] or [ 19 ] for details. ∎

The converse is true for (anti-) euclidean and lorentzian spaces.

###### Theorem 4.11.

For @xmath , @xmath , @xmath or @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

i.e. any rotor can be written as (minus) the exponential of a bivector.
The minus sign is only required in the cases @xmath , @xmath , @xmath ,
@xmath , @xmath , @xmath , @xmath and @xmath .

The proof can be found in [ 19 ] . Essentially, it relies on the fact
that any isometry of an euclidean or lorentzian space can be generated
by a single infinitesimal transformation. This holds for these spaces
only , so that for example @xmath , where for instance @xmath , @xmath ,
cannot be reduced to a single exponential.

### 4.4 Spinors

Spinors are objects which originally appeared in physics in the early
days of quantum mechanics, but have by now made their way into other
fields as well, such as differential geometry and topology. They are
traditionally represented as elements of a complex vector space since it
was natural to add and subtract them and scale them by complex
amplitudes in their original physical applications. However, what really
characterizes them as spinors is the fact that they can be acted upon by
rotations, together with their rather special transformation properties
under such rotations. While a rotation needs just one revolution to get
back to the identity in a vector representation, it takes two
revolutions to come back to the identity in a spinor representation.
This is exactly the behaviour experienced by rotors, since they
transform vectors double-sidedly with the (twisted) adjoint action as

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

where we applied consecutive rotations represented by rotors @xmath and
@xmath . Note that the rotors themselves transform according to

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

that is single-sidedly with the left action.

This hints that rotors could represent some form of spinors in geometric
algebra. However, the rotors form a group and not a vector space, so we
need to consider a possible enclosing vector space with similar
properties. Some authors (see e.g. Hestenes [ 9 ] ) have considered so
called operator spinors , which are general even multivectors that leave
@xmath invariant under the double-sided action ( 4.12 ), i.e. elements
of

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

Since @xmath is both odd and self-reversing, it is an element of @xmath
. Therefore @xmath is only guaranteed to be a vector space for @xmath ,
where it coincides with @xmath .

For a general spinor space embedded in @xmath , we seek a subspace
@xmath that is invariant under left action by rotors, i.e. such that

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

One obvious and most general choice of such a spinor space is the whole
space @xmath . However, we will soon see from examples that spinors in
lower dimensions are best represented by another natural suggestion,
namely the set of even multivectors. Hence, we follow Francis and
Kosowsky [ 5 ] and define the space of spinors @xmath for arbitrary
dimensions as the even subalgebra @xmath .

Note that action by rotors , i.e. @xmath instead of Spin, ensures that
@xmath remains invariant under right action and @xmath remains invariant
under left action on @xmath . In lower dimensions these are invariant
under both actions, so they are good candidates for invariant or
observable quantities in physics. Also note that if @xmath then the set
of unit spinors and the set of rotors coincide, i.e.

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

This follows because then @xmath is invertible and @xmath is both odd
and self-reversing, hence a vector. Thus, @xmath lies in @xmath and is
therefore an even unit versor.

A popular alternative to the above is to consider spinors as elements in
left-sided ideals of (mostly complex) geometric algebras. This is the
view which is closest related to the original complex vector space
picture. We will see an example of how these views are related in the
case of the Dirac algebra. A motivation for this definition of spinor is
that it admits a straightforward transition to basis-independent
spinors, so called covariant spinors . This is required for a treatment
of spinor fields on curved manifolds, i.e. in the gravitational setting.
However, covariant spinors lack the clearer geometrical picture provided
by operator and even subalgebra spinors. Furthermore, by reconsidering
the definition and interpretation of these geometric spinors, it is
possible to deal with basis-independence also in this case. These and
other properties of spinors related to physics will be discussed in
Section 7 .

## 5 A study of lower-dimensional algebras

We have studied the structure of geometric algebras in general and saw
that they are related to many other familiar algebras and groups. We
will now go through a number of lower-dimensional examples in detail to
see just how structure-rich these algebras are. Although we go no higher
than to a five-dimensional base vector space, we manage to find a
variety of structures related to physics.

We choose to focus around the groups and spinors of these example
algebras. It is highly recommended that the reader aquires a more
complete understanding of at least the plane, space and spacetime
algebras from other sources. Good introductions aimed at physicists can
be found in [ 4 ] and [ 7 ] . A more mathematical treatment is given in
[ 15 ] .

### 5.1 @xmath

Since @xmath is just the field of real numbers, which should be
familiar, we start instead with @xmath , the geometric algebra of the
real line. Let @xmath be spanned by one basis element @xmath such that
@xmath . Then

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

This is a commutative algebra with pseudoscalar @xmath . One easily
finds the invertible elements @xmath by considering the norm function,
which with a one-dimensional @xmath is given by

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

For an arbitrary element @xmath then

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

When @xmath we find that @xmath has an inverse @xmath . Hence,

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

Note also that @xmath for all @xmath since the algebra is commutative.

The other groups are rather trivial in this space. Because @xmath , we
have

  -- -------- --
     @xmath   
  -- -------- --

where we write @xmath to emphasize a disjoint union. The spinors in this
algebra are just real scalars.

### 5.2 @xmath - The complex numbers

As one might have noticed from previous discussions on isomorphisms, the
complex numbers are in fact a real geometric algebra. Let @xmath span a
one-dimensional anti-euclidean space and be normalized to @xmath . Then

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

This is also a commutative algebra, but, unlike the previous example,
this is a field since every nonzero element is invertible. The norm
function is an actual norm (squared) in this case,

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

namely the modulus of the complex number. Note that the grade involution
represents the complex conjugate and @xmath as above. The relevant
groups are

  -- -------- --
     @xmath   
  -- -------- --

The spinor space is still given by @xmath .

### 5.3 @xmath

We include this as our only example of a degenerate algebra, just to see
what such a situation might look like. Let @xmath span a one-dimensional
space with quadratic form @xmath . Then

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

and @xmath . The norm function depends only on the scalar part,

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

An element is invertible if and only if the scalar part is nonzero.
Since no vectors are invertible, we are left with only the empty product
in the versor group. This gives

  -- -------- --
     @xmath   
  -- -------- --

Note, however, that for @xmath

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

so the Lipschitz group is

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

This shows that the assumption on nondegeneracy was necessary in the
discussion about the Lipschitz group in Section 4 .

### 5.4 @xmath - The plane algebra

Our previous examples were rather trivial, but we now come to our first
really interesting case, namely the geometric algebra of the euclidean
plane. Let @xmath be an orthonormal basis of @xmath and consider

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

An important feature of this algebra is that the pseudoscalar @xmath
squares to @xmath . This makes the even subalgebra isomorphic to the
complex numbers, in correspondence with the relation @xmath .

Let us find the invertible elements of the plane algebra. For
two-dimensional algebras we use the original norm function

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

since it satisfies @xmath for all @xmath . The sign relations for
involutions in Table 2.1 then require this to be a scalar, so we have a
map

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

For an arbitrary element @xmath we have

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

Furthermore, @xmath and @xmath for all @xmath . Proceeding as in the
one-dimensional case, we find that @xmath has an inverse @xmath if and
only if @xmath , i.e.

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

For @xmath in the even subspace we have @xmath , so the Clifford
conjugate acts as complex conjugate in this case. Again, the norm
function (here @xmath ) acts as modulus squared. We find that the rotor
group, i.e. the group of even unit versors, corresponds to the group of
unit complex numbers,

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

Note that, because @xmath and @xmath anticommute,

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

so a rotor @xmath represents a counter-clockwise ⁶ ⁶ 6 Assuming, of
course, that @xmath points at 3 o’clock and @xmath at 12 o’clock.
rotation in the plane by an angle @xmath . The Pin group is found by
picking for example @xmath ;

  -- -- --
        
  -- -- --

As we saw above, the spinors of @xmath are nothing but complex numbers.
We can write any spinor or complex number @xmath in the polar form
@xmath , which is just a rescaled rotor. The spinor action

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

then gives a geometric interpretation of the spinor @xmath as an
operation to rotate by an angle @xmath and scale by @xmath .

### 5.5 @xmath - The quaternions

The geometric algebra of the anti-euclidean plane is isomorphic to
Hamilton’s quaternion algebra @xmath . This follows by taking an
orthonormal basis @xmath of @xmath and considering

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

where @xmath is the pseudoscalar. We then have the classic identities
defining quaternions,

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

We write an arbitrary quaternion as @xmath . The Clifford conjugate acts
as the quaternion conjugate , @xmath .

The norm function @xmath has the same properties as in the euclidean
algebra, but in this case it once again represents the square of an
actual norm, namely the quaternion norm ,

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

Just as in the complex case then, all nonzero elements are invertible,

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

The even subalgebra is also in this case isomorphic to the complex
numbers, so the spinors and groups @xmath , Pin and Spin are no
different than in the euclidean case.

### 5.6 @xmath

This is our simplest example of a lorentzian algebra. An orthonormal
basis @xmath of @xmath consists of a timelike vector, @xmath , and a
spacelike vector, @xmath . In general, a vector (or blade) @xmath is
called timelike if @xmath , spacelike if @xmath , and lightlike or null
if @xmath . This terminology is taken from relativistic physics. The
two-dimensional lorentzian algebra is given by

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

The group of invertible elements is as usual given by

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

The pseudoscalar @xmath squares to the identity in this case and the
even subalgebra is therefore @xmath . This has as an important
consequence that the rotor group is fundamentally different from the
euclidean case,

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

This is a pair of disjoint hyperbolas passing through the points 1 and
-1, respectively. The Spin group consists of four such hyperbolas and
the Pin group of eight,

  -- -------- --
     @xmath   
  -- -------- --

The rotations that are represented by rotors of this kind are called
Lorentz boosts . We will return to the Lorentz group in the
4-dimensional spacetime, but for now note the hyperbolic nature of these
rotations,

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

Hence, a rotor @xmath transforms (or boosts ) timelike vectors by a
hyperbolic angle @xmath away from the positive spacelike direction.

The spinor space @xmath consists partly of scaled rotors, @xmath , but
there are also two subspaces @xmath of null spinors which cannot be
represented in this way. Note that such a spinor @xmath acts on vectors
as

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

so it maps the whole space into one of the two null-spaces. The action
of a non-null spinor has a nice interpretation as a boost plus scaling.

### 5.7 @xmath - The space algebra / Pauli algebra

Since the 3-dimensional euclidean space is the space that is most
familiar to us humans, one could expect its geometric algebra, the space
algebra , to be familiar as well. Unfortunately, this is generally not
the case. Most of its features, however, are commonly known but under
different names and in separate contexts. For example, using the
isomorphism @xmath from Proposition 3.3 , we find that this algebra also
appears in quantum mechanics in the form of the complex Pauli algebra .

We take an orthonormal basis @xmath in @xmath and obtain

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

where @xmath is the pseudoscalar. We write in this way to emphasize the
duality between the vector and bivector spaces in this case. This
duality can be used to define cross products and rotation axes etc.
However, since the use of such concepts is limited to three dimensions
only , it is better to work with their natural counterparts within
geometric algebra.

To begin with, we would like to find the invertible elements of the
space algebra. An arbitrary element @xmath can be written as

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

where @xmath and @xmath . Note that, since the algebra is odd, the
pseudoscalar commutes with everything and furthermore @xmath . The norm
function @xmath does not take values in @xmath in this algebra, but due
to the properties of the Clifford conjugate we have @xmath . This
subspace is, from our observation, isomorphic to @xmath and its
corresponding complex conjugate is given by @xmath . Using this, we can
construct a real-valued map @xmath by taking the complex modulus,

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

Plugging in ( 5.29 ) we obtain

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

Although @xmath takes values in @xmath , it is not a real norm ⁷ ⁷ 7
This will also be seen to be required from dimensional considerations
and the remark to Hurwitz’ Theorem in Section 6 on @xmath since there
are nonzero elements with @xmath . It does however have the
multiplicative property

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

for all @xmath , since @xmath commutes with all of @xmath . We also
observe from ( 5.31 ) that @xmath . The expression ( 5.30 ) singles out
the invertible elements as those elements ( 5.29 ) for which @xmath ,
i.e.

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

The even subalgebra of @xmath is the quaternion algebra, as follows from
the isomorphism @xmath . The rotor group is then, according to the
observation ( 4.16 ), the group of unit quaternions (note that the
reverse here acts as the quaternion conjugate),

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

where the exponentiation of the bivector algebra followed from Theorem
4.11 . The last isomorphism shows the relation to the Pauli algebra and
is perhaps the most famous representation of a Spin group. An arbitrary
rotor @xmath can according to ( 5.34 ) be written in the polar form
@xmath , where @xmath is a unit vector, and represents a rotation by an
angle @xmath in the plane @xmath (i.e. @xmath counter-clockwise around
the axis @xmath ).

The Pin group consists of two copies of the rotor group,

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

for some unit vector @xmath . The Pin group can be visualized as two
unit 3-spheres @xmath lying in the even and odd subspaces, respectively.
The odd one includes a reflection and represents the
non-orientation-preserving part of O @xmath .

As we saw above, the spinors in the space algebra are quaternions. An
arbitrary spinor can be written @xmath and acts on vectors by rotating
in the plane @xmath with the angle @xmath and scaling with @xmath . We
will continue our discussion on these spinors in Section 7 .

### 5.8 @xmath - The spacetime algebra

We take as a four-dimensional example the spacetime algebra (STA), which
is the geometric algebra of Minkowski spacetime, @xmath . This is the
stage for special relativistic physics and what is fascinating with the
STA is that it embeds a lot of important physical objects in a natural
way.

By convention, we denote an orthonormal basis of the Minkowski space by
@xmath , where @xmath is timelike and the other @xmath are spacelike.
This choice of notation is motivated by the Dirac representation of the
STA in terms of so called gamma matrices which will be explained in more
detail later. The STA expressed in this basis is

  -- -------- --
     @xmath   
  -- -------- --

where the pseudoscalar is @xmath and we set @xmath , @xmath . The form
of the STA basis chosen above emphasizes the duality which exists
between the graded subspaces. It also hints that the even subalgebra of
the STA is the space algebra ( 5.28 ). This is true from the isomorphism
@xmath , but we can also verify this explicitly by noting that @xmath
and @xmath . Hence, the timelike (positive square) blades @xmath form a
basis of a 3-dimensional euclidean space called the relative space to
@xmath . For any timelike vector @xmath we can find a similar relative
space spanned by the bivectors @xmath for @xmath . These spaces all
generate the relative space algebra @xmath . This is a very powerful
concept which helps us visualize and work efficiently in Minkowski
spacetime and the STA.

Using boldface to denote relative space elements, an arbitrary
multivector @xmath can be written

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

where @xmath , @xmath and @xmath in relative space @xmath . As usual, we
would like to find the invertible elements. Looking at the norm function
@xmath , it is not obvious that we can extend this to a real-valued
function on @xmath . Fortunately, we have for @xmath that

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

Hence, we can define a map @xmath by

  -- -------- -- --------
     @xmath      (5.38)
  -- -------- -- --------

Plugging in ( 5.36 ) into @xmath , we obtain after a tedious calculation

  -- -------- -- --------
     @xmath      (5.39)
  -- -------- -- --------

and, by ( 5.37 ),

  -- -------- -- --------
     @xmath      (5.40)
  -- -------- -- --------

We will prove some rather non-trivial statements about this norm
function where we need that @xmath for all @xmath . This is a quite
general property of this involution.

###### Lemma 5.1.

In any Clifford algebra @xmath (even when @xmath is infinite), we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Using linearity, we can set @xmath and expand @xmath in coordinates
@xmath as @xmath . We obtain

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

∎

We now have the following

###### Theorem 5.2.

@xmath for all @xmath .

###### Remark.

Note that this is not at all obvious from the expression ( 5.40 ).

###### Proof.

Using Lemma 5.1 we have that

  -- -------- -- --------
     @xmath      (5.41)
  -- -------- -- --------

Since @xmath takes values in @xmath , this must be a scalar, so that

  -- -------- -- --------
     @xmath      (5.42)
  -- -------- -- --------

where we used the symmetry of the scalar product. ∎

###### Lemma 5.3.

For all @xmath we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Take arbitrary elements @xmath and @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Comparing these expressions we find that they are equal. ∎

We can now prove that @xmath really acts as a determinant on the STA.

###### Theorem 5.4.

The norm function @xmath satisfies the product property

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Using that @xmath is a scalar and that @xmath takes values in @xmath ,
we obtain

  -- -------- --
     @xmath   
  -- -------- --

where we applied Lemma 5.1 and then Lemma 5.3 . Theorem 5.2 now gives
the claimed identity. ∎

From ( 5.38 ) we find that the group of invertible elements is given by

  -- -------- -- --------
     @xmath      (5.43)
  -- -------- -- --------

and the inverse of @xmath is

  -- -------- -- --------
     @xmath      (5.44)
  -- -------- -- --------

Note that the above theorems regarding @xmath only rely on the
commutation properties of the different graded subspaces and not on the
actual signature and field of the vector space. Therefore, these hold
for all @xmath such that @xmath , and

  -- -------- -- --------
     @xmath      (5.45)
  -- -------- -- --------

Let us now turn our attention to the rotor group of the STA. The reverse
equals the Clifford conjugate on the even subalgebra (it also
corresponds to the Clifford conjugate defined on the relative space), so
we find from ( 5.39 ) that the rotor group is

  -- -------- -- --------
     @xmath      (5.46)
  -- -------- -- --------

The last isomorphism is related to the Dirac representation of the STA,
while the exponentiation identity was obtained from Theorem 4.11 and
gives a better picture of what the rotor group looks like. Namely, any
rotor @xmath can be written @xmath for some relative vectors @xmath . A
pure @xmath corresponds to a rotation in the spacelike plane @xmath with
angle @xmath (which is a corresponding rotation also in relative space),
while @xmath corresponds to a “rotation” in the timelike plane @xmath ,
i.e. a boost in the relative space direction @xmath with velocity @xmath
times the speed of light.

Picking for example @xmath and @xmath , we obtain the Spin and Pin
groups,

  -- -------- --
     @xmath   
  -- -------- --

The Pin group forms a double-cover of the so called Lorentz group
O(1,3). Since the rotor group is connected, we find that O(1,3) has four
connected components. The Spin group covers the subgroup of proper
Lorentz transformations preserving orientation, while the rotor group
covers the connected proper orthochronous Lorentz group which also
preserves the direction of time.

The spinor space of the STA is the relative space algebra. We will
discuss these spinors in more detail later, but for now note that an
invertible spinor

  -- -------- -- --------
     @xmath      (5.47)
  -- -------- -- --------

is the product of a rotor @xmath , a duality rotor @xmath and a scale
factor @xmath .

### 5.9 @xmath - The Dirac algebra

The Dirac algebra is the representation of the STA which is most
commonly used in physics. This is due to historic reasons, since the
geometric nature of this algebra from its relation to the spacetime
algebra was not uncovered until the 1960s. The relation between these
algebras is observed by noting that the pseudoscalar in @xmath commutes
with all elements and squares to minus the identity. By Proposition 3.3
we have that the Dirac algebra is the complexification of the STA,

  -- -------- -- --------
     @xmath      (5.48)
  -- -------- -- --------

We construct this isomorphism explicitly by taking bases @xmath of
@xmath as usual and @xmath of @xmath such that @xmath and the other
@xmath . We write @xmath and @xmath , and use the convention that Greek
indices run from 0 to 3. The isomorphism @xmath is given by the
following correspondence of basis elements:

  -- -------- --
     @xmath   
  -- -------- --

The respective pseudoscalars are @xmath and @xmath . We have also noted
the correspondence between involutions in the different algebras.
Clifford conjugate in @xmath corresponds to reversion in @xmath , the
@xmath -involution becomes the @xmath -involution, while complex
conjugation in @xmath corresponds to grade involution in @xmath . In
other words,

  -- -------- -- --------
     @xmath      (5.49)
  -- -------- -- --------

We can use the correspondence above to find a norm function on @xmath .
Since @xmath was independent of the choice of field, we have that the
complexification of @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

Taking the modulus of this complex number, we arrive at a real-valued
map @xmath with

  -- -------- --
     @xmath   
  -- -------- --

In the final steps we noted that @xmath and that @xmath corresponds to
@xmath . Furthermore, since @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . The invertible elements of the Dirac algebra are then
as usual

  -- -------- -- --------
     @xmath      (5.50)
  -- -------- -- --------

and the inverse of @xmath is

  -- -------- -- --------
     @xmath      (5.51)
  -- -------- -- --------

The above strategy could also have been used to obtain the expected
result for @xmath on @xmath (with a corresponding isomorphism @xmath ):

  -- -------- -- --------
     @xmath      (5.52)
  -- -------- -- --------

We briefly describe how spinors are dealt with in this representation.
This will not be the the same as the even subspace spinors which we
usually consider. For the selected basis of @xmath we form the
idempotent element

  -- -------- -- --------
     @xmath      (5.53)
  -- -------- -- --------

Spinors are now defined as elements of the ideal @xmath and one can show
that every such element can be written as

  -- -------- -- --------
     @xmath      (5.54)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (5.55)
  -- -------- -- --------

With the standard representation of @xmath as generators of @xmath ,

  -- -------- -- --------
     @xmath      (5.56)
  -- -------- -- --------

where @xmath are the Pauli matrices which generate a representation of
the Pauli algebra, one finds that

  -- -------- -- --------
     @xmath      (5.57)
  -- -------- -- --------

Hence, these spinors can be thought of as complex column vectors. Upon a
transformation of the basis @xmath , the components @xmath will
transform according to the representation @xmath of @xmath . We will
come back to discuss these spinors in Section 7 . See [ 20 ] for more
details on this correspondence between spinors and ideals.

### 5.10 Summary of norm functions

The norm functions

  -- -------- --
     @xmath   
  -- -------- --

constructed above (where we added @xmath for completeness) all have the
product property

  -- -------- -- --------
     @xmath      (5.58)
  -- -------- -- --------

for all @xmath when @xmath is @xmath -dimensional. Because these
functions only involve products and involutions, and the proofs of the
above identities only rely on commutation properties in the respective
algebras, they even hold for any Clifford algebra @xmath with @xmath ,
respectively.

For matrix algebras, a similar product property is satisfied by the
determinant. On the other hand, we have the following theorem for
matrices.

###### Theorem 5.5.

Assume that @xmath is continuous and satisfies

  -- -------- -- --------
     @xmath      (5.59)
  -- -------- -- --------

for all @xmath . Then @xmath must be either @xmath , @xmath , @xmath or
@xmath for some @xmath .

In other words, we must have that @xmath , where @xmath is continuous
and @xmath . This @xmath is uniquely determined e.g. by whether @xmath
takes negative values, together with the value of @xmath for any @xmath
. This means that the determinant is the @xmath real-valued function on
real matrices with the product property ( 5.59 ). The proof of this
theorem can be found in the appendix.

Looking at Table 3.1 , we see that @xmath for @xmath From the above
theorem we then know that there are unique ⁸ ⁸ 8 Actually, the functions
are either @xmath or @xmath . @xmath and @xmath constructed previously
are smooth, however, so they must be equal to @xmath . continuous
functions @xmath such that @xmath and @xmath . These are given by the
determinant on the corresponding matrix algebra. What we do not know,
however, is if every one of these can be expressed in the same simple
form as @xmath , @xmath and @xmath , i.e. as a composition of products
and grade-based involutions. Due to the complexity of higher-dimensional
algebras, it is not obvious whether a continuation of the strategy
employed so far can be successful or not. It is even difficult ⁹ ⁹ 9 The
first couple of @xmath can be verified directly using a geometric
algebra package in Maple, but already for @xmath this becomes impossible
to do straight-away on a standard desktop computer. to test out
suggestions of norm functions on a computer, since the number of
operations involved grows as @xmath . We therefore leave this question
as a suggestion for further investigation.

Because of the product property ( 5.58 ), the norm functions also lead
to interesting factorization identities on rings. An example is @xmath
for quaternions,

  -- -------- -- --------
     @xmath      (5.60)
  -- -------- -- --------

This is called the Lagrange identity . These types of identities can be
used to prove theorems in number theory. Using ( 5.60 ), one can for
example prove that every integer can be written as a sum of four squares
of integers. Or, in other words, every integer is the norm (squared) of
an integral quaternion. See e.g. [ 8 ] for the proof.

Another possible application of norm functions could be in public key
cryptography and one-way trapdoor functions. We have not investigated
this idea further, however.

## 6 Representation theory

In this section we will use the classification of geometric algebras as
matrix algebras, which was developed in Section 3 , to work out the
representation theory of these algebras. Since one can find
representations of geometric algebras in many areas of mathematics and
physics, this leads to a number of interesting applications. We will
consider two main examples in detail, namely normed division algebras
and vector fields on higher-dimensional spheres.

###### Definition 6.1.

For @xmath , @xmath or @xmath , we define a @xmath -representation of
@xmath as an @xmath -algebra homomorphism

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a finite-dimensional vector space over @xmath . @xmath
is called a @xmath -module over @xmath .

Note that a vector space over @xmath or @xmath can be considered as a
real vector space together with operators @xmath or @xmath in @xmath
that anticommute and square to minus the identity. In the definition
above we assume that these operators commute with @xmath for all @xmath
, so that @xmath can be said to respect the @xmath -structure of the
space @xmath . When talking about the dimension of the module @xmath we
will always refer to its dimension as a real vector space.

The standard strategy when studying representation theory is to look for
irreducible representations.

###### Definition 6.2.

A representation @xmath is called reducible if @xmath can be written as
a sum of proper (not equal to @xmath or @xmath ) invariant subspaces,
i.e.

  -- -------- --
     @xmath   
  -- -------- --

In this case we can write @xmath , where @xmath . A representation is
called irreducible if it is not reducible.

The traditional definition of an irreducible representation is that it
does not have any proper invariant subspaces. However, because @xmath is
generated by a finite group (the Clifford group) one can verify that
these two definitions are equivalent in this case.

###### Proposition 6.1.

Every @xmath -representation @xmath of a geometric algebra @xmath can be
split up into a direct sum @xmath of irreducible representations.

###### Proof.

This follows directly from the definitions and the fact that @xmath is
finite-dimensional. ∎

###### Definition 6.3.

Two @xmath -representations @xmath , @xmath , are said to be equivalent
if there exists a @xmath -linear isomorphism @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 6.2.

Up to equivalence, the only irreducible representations of the matrix
algebras @xmath and @xmath are

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

respectively, where @xmath is the defining representation and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

This follows from the classical fact that the algebras @xmath are simple
and that simple algebras have only one irreducible representation up to
equivalence. See e.g. [ 12 ] for details. ∎

###### Theorem 6.3.

From the above, together with the classification of real geometric
algebras, follows the table of representations in Table 6.1 , where
@xmath is the number of inequivalent irreducible representations and
@xmath is the dimension of an irreducible representation for @xmath .
The cases for @xmath are obtained using the periodicity

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

We will now consider the situation when the representation space @xmath
is endowed with an inner product. Note that if @xmath is a vector space
over @xmath with an inner product, we can always find a @xmath
-invariant inner product on @xmath , i.e. such that the operators @xmath
or @xmath are orthogonal. Namely, let @xmath be an inner product on
@xmath and put

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

Then @xmath and @xmath , etc.

In the same way, when @xmath is euclidean or anti-euclidean, we can for
a representation @xmath find an inner product such that @xmath acts
orthogonally with unit vectors, i.e. such that @xmath for all @xmath and
@xmath with @xmath . We construct such an inner product by averaging a,
possibly @xmath -invariant, inner product @xmath over the Clifford
group. Take an orthonormal basis @xmath of @xmath and put

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

We then have that

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

for @xmath in @xmath . Thus, if @xmath and @xmath , we obtain

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

Hence, this inner product has the desired property. Also note that, for
@xmath , we have

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

while for @xmath ,

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

i.e. @xmath is symmetric for euclidean spaces and antisymmetric for
anti-euclidean spaces.

We are now ready for some examples which illustrate how representations
of geometric algebras can appear in various contexts and how their
representation theory can be used to prove important theorems.

### 6.1 Example I: Normed division algebras

Our first example concerns the possible dimensions of normed division
algebras. A normed division algebra is an algebra @xmath over @xmath
(not necessarily associative) with a norm @xmath such that

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

for all @xmath and such that every nonzero element is invertible. We
will prove the following

###### Theorem 6.4 (Hurwitz’ Theorem).

If @xmath is a finite-dimensional normed division algebra over @xmath ,
then its dimension is either 1, 2, 4 or 8.

###### Remark.

This corresponds uniquely to @xmath , @xmath , @xmath , and the
octonions @xmath , respectively. The proof of unicity requires some
additional steps, see e.g. [ 2 ] .

Let us first consider the restrictions that the requirement ( 6.9 ) puts
on the norm. Assume that @xmath has dimension @xmath . For every @xmath
we have a linear transformation

  -- -------- --
     @xmath   
  -- -------- --

given by left multiplication by @xmath . When @xmath we then have

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

i.e. @xmath preserves the norm. Hence, it maps the unit sphere @xmath in
@xmath into itself. Furthermore, since every element in @xmath is
invertible, we can for each pair @xmath find an @xmath such that @xmath
. Now, these facts imply a large amount of symmetry of @xmath . In fact,
we have the following

###### Lemma 6.5.

Assume that @xmath is a finite-dimensional normed vector space. Let
@xmath denote the unit sphere in @xmath . If, for every @xmath , there
exists an operator @xmath such that @xmath and @xmath , then @xmath must
be an inner product space.

###### Proof.

We will need the following fact: Every compact subgroup @xmath of @xmath
preserves some inner product on @xmath . This can be proven by picking a
Haar-measure @xmath on @xmath and averaging any inner product @xmath on
@xmath over @xmath using this measure,

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

Now, let @xmath be the group of linear transformations on @xmath which
preserve its norm @xmath . @xmath is compact in the finite-dimensional
operator norm topology, since @xmath is closed and bounded by 1.
Furthermore, @xmath is injective and therefore an isomorphism. The group
structure is obvious. Hence, @xmath is a compact subgroup of @xmath .

From the above we know that there exists an inner product @xmath on
@xmath which is preserved by @xmath . Let @xmath denote the norm
associated to this inner product, i.e. @xmath . Take a point @xmath with
@xmath and rescale the inner product so that also @xmath . Let @xmath
and @xmath denote the unit spheres associated to @xmath and @xmath ,
respectively. By the conditions in the lemma, there is for every @xmath
an @xmath such that @xmath . But @xmath also preserves the norm @xmath ,
so @xmath must also lie in @xmath . Hence, @xmath is a subset of @xmath
. However, being unit spheres associated to norms, @xmath and @xmath are
both homeomorphic to the standard sphere @xmath , so we must have that
they are equal. Therefore, the norms must be equal. ∎

We now know that our normed division algebra @xmath has some inner
product @xmath such that @xmath . We call an element @xmath imaginary if
@xmath is orthogonal to the unit element, i.e. if @xmath . Let @xmath
denote the @xmath -dimensional subspace of imaginary elements. We will
observe that @xmath acts on @xmath in a special way.

Take a curve @xmath on the unit sphere such that @xmath and @xmath .
(Note that @xmath is the tangent space to @xmath at the unit element.)
Then, because the product in @xmath is continuous,

  -- -------- -- --------
     @xmath      (6.12)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (6.13)
  -- -------- -- --------

Hence, @xmath for @xmath . If, in addition, @xmath we have that @xmath ,
so @xmath . For an arbitrary imaginary element @xmath we obtain by
rescaling

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

This motivates us to consider the geometric algebra @xmath with
quadratic form @xmath . By ( 6.14 ) and the universal property of
geometric algebras (Proposition 2.1 ) we find that @xmath extends to a
representation of @xmath on @xmath ,

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

i.e. a representation of @xmath on @xmath . The representation theory
now demands that @xmath is a multiple of @xmath . By studying Table 6.1
and taking periodicity ( 6.1 ) into account we find that this is only
possible for @xmath .

### 6.2 Example II: Vector fields on spheres

In our next example we consider the @xmath -dimensional unit spheres
@xmath and use representations of geometric algebras to construct vector
fields on them. The number of such vector fields that can be found gives
us information about the topological features of these spheres.

###### Theorem 6.6 (Radon-Hurwitz).

On @xmath there exist @xmath pointwise linearly independent vector
fields, where, if we write @xmath uniquely as

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

then

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

For example,

  -- -------- --
     @xmath   
  -- -------- --

###### Corollary.

@xmath , @xmath and @xmath are parallelizable.

###### Remark.

The number of vector fields constructed in this way is actually the
maximum number of possible such fields on @xmath . This is a much deeper
result proven by Adams [ 1 ] using algebraic topology.

Our main observation is that if @xmath is a @xmath -module then we can
construct @xmath pointwise linearly independent vector fields on @xmath
. Namely, suppose we have a representation @xmath of @xmath on @xmath .
Take an inner product @xmath on @xmath such that the action of @xmath is
orthogonal and pick any basis @xmath of @xmath . We can now define a
collection of smooth vector fields @xmath on @xmath by

  -- -------- -- --------
     @xmath      (6.18)
  -- -------- -- --------

According to the observation ( 6.8 ) this action is antisymmetric, so
that

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

Hence, @xmath for @xmath . By restricting to @xmath we therefore have
@xmath tangent vector fields. It remains to show that these are
pointwise linearly independent. Take @xmath and consider the linear map

  -- -------- -- --------
     @xmath      (6.20)
  -- -------- -- --------

Since the image of @xmath is @xmath it is sufficient to prove that
@xmath is injective. But if @xmath then also @xmath , so we must have
@xmath .

Now, for a fixed @xmath we want to find as many vector fields as
possible, so we seek the highest @xmath such that @xmath is a @xmath
-module. From the representation theory we know that this requires that
@xmath is a multiple of @xmath . Furthermore, since @xmath is a power of
2 we obtain the maximal such @xmath when @xmath , where @xmath is odd
and @xmath . Using Table 6.1 and the periodicity ( 6.1 ) we find that if
we write @xmath , with @xmath , then @xmath . This proves the theorem.

## 7 Spinors in physics

In this final section we discuss how the view of spinors as even
multivectors can be used to reformulate physical theories in a way which
clearly expresses the geometry of these theories, and therefore leads to
conceptual simplifications.

In the geometric picture provided by geometric algebra we consider
spinor fields as (smooth) maps from the space or spacetime @xmath into
the spinor space of @xmath ,

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

The field could also take values in the spinor space of a subalgebra of
@xmath . For example, a relativistic complex scalar field living on
Minkowski spacetime could be considered as a spinor field taking values
in a plane subalgebra @xmath .

In the following we will use the summation convention that matching
upper and lower Greek indices implies summation over 0,1,2,3. We will
not write out physical constants such as @xmath .

### 7.1 Pauli spinors

Pauli spinors describe the spin state of a non-relativistic fermionic
particle such as the non-relativistic electron. Since this is the
non-relativistic limit of the Dirac theory discussed below, we will here
just state the corresponding representation of Pauli spinors as even
multivectors of the space algebra. We saw that such an element can be
written as @xmath , i.e. a scaled rotor. For this spinor field, the
physical state is expressed by the observable vector (field)

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

which is interpreted as the expectation value of the particle’s spin,
scaled by the spatial probability amplitude @xmath . The vector @xmath
acts as a reference axis for the spin. The up and down spin basis states
in the ordinary complex representation correspond to the rotors which
leave @xmath invariant, respectively the rotors which rotate @xmath into
@xmath . Observe the invariance of @xmath under right-multiplication of
@xmath by @xmath . This corresponds to the complex phase invariance in
the conventional formulation.

### 7.2 Dirac-Hestenes spinors

Dirac spinors describe the state of a relativistic Dirac particle, such
as an electron or neutrino. Conventionally, Dirac spinors are
represented by four-component complex column vectors, @xmath . For a
spinor field the components will be complex-valued functions on
spacetime. Acting on these spinors are the complex @xmath -matrices
@xmath given in ( 5.56 ), which generate a matrix representation of the
Dirac algebra. The Dirac adjoint of a column spinor is a row matrix

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

where, in this context, complex conjugation is denoted by @xmath and
hermitian conjugation by @xmath . The physical state of a Dirac particle
is determined by the following 16 so called bilinear covariants :

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

Their integrals over space give expectation values of the physical
observables. For example, @xmath integrated over a spacelike domain
gives the probability ¹⁰ ¹⁰ 10 Or rather the probability multiplied with
the charge of the particle. For a large number of particles this can be
interpreted as a charge density. of finding the particle in that domain,
and @xmath , @xmath , give the current of probability. These are
components of a spacetime current vector @xmath . The quantities @xmath
describe the probability density of the particle’s electromagnetic
moment, while @xmath gives the direction of the particle’s spin ¹¹ ¹¹ 11
In the formulation below, we obtain the relative space spin vector as
@xmath . .

In Hestenes’ reformulation of the Dirac theory, we represent spinors by
even multivectors @xmath in the real spacetime algebra @xmath . Note
that both @xmath and @xmath have eight real components, so this is no
limitation. In this representation, the gamma matrices are considered as
orthonormal basis vectors of the Minkowski spacetime and the bilinear
covariants are given by

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

where @xmath , @xmath are spacetime vectors and @xmath a bivector. This
reformulation allows for a nice geometric interpretation of the Dirac
theory. Here, spinors are objects that transform the reference basis
@xmath into the observable quantities.

Since a spinor only has eight components, the bilinear covariants cannot
be independent. From ( 7.5 ) we easily find a number of relations called
the Fierz indentities ,

  -- -------- -- -------
     @xmath      (7.6)
  -- -------- -- -------

The Fierz identities also include a bunch of relations which in the case
@xmath can be derived directly from these three. In total, there are
seven degrees of freedom, given for example by the spacetime current
@xmath , the relative space direction of the spin vector @xmath (two
angles) and the so called Yvon-Takabayasi angle @xmath . The eighth
degree of freedom present in a spinor is the phase-invariance, which in
the original Dirac theory corresponds to the overall complex phase of
@xmath , while in the Dirac-Hestenes picture corresponds to a rotational
freedom in the @xmath -plane, or equivalently around the spin axis in
relative space. This is explained by the invariance of ( 7.5 ) under a
transformation @xmath .

In the null case, i.e. when @xmath , we have the additional identities

  -- -------- -- -------
     @xmath      (7.7)
  -- -------- -- -------

The geometric interpretation is that @xmath are both null vectors and
@xmath is a null bivector blade with @xmath and @xmath in its null
subspace. Hence, the remaining five degrees of freedom are given by the
direction of the plane represented by S, which must be tangent to the
light-cone (two angles), plus the magnitudes of @xmath , @xmath and
@xmath .

The equation which describes the evolution of a Dirac spinor in
spacetime is the Dirac equation, which in this representation is given
by the Dirac-Hestenes equation ,

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

where @xmath (we use the spacetime coordinate expansion @xmath ) and
@xmath is the electromagnetic potential vector field. Here, @xmath again
plays the role of the complex imaginary unit @xmath .

Another interesting property of the STA is that the electomagnetic field
is most naturally represented as a bivector field in @xmath . We can
write any such bivector field as @xmath , where @xmath and @xmath are
relative space vector fields. In the context of relativistic
electrodynamics, these are naturally interpreted as the electric and
magnetic fields, respectively. Maxwell’s equations are compactly written
as

  -- -------- -- -------
     @xmath      (7.9)
  -- -------- -- -------

in this formalism, where @xmath is the source current. The physical
quantity describing the energy and momentum present in an
electromagnetic field is the Maxwell stress-energy tensor which in the
STA formulation can be interpreted as a map @xmath , given by

  -- -------- -- --------
     @xmath      (7.10)
  -- -------- -- --------

For example, the energy of the field @xmath relative to the @xmath
-direction is @xmath ).

Rodrigues and Vaz [ 25 ] , [ 26 ] have studied an interesting
correspondence between the Dirac and Maxwell equations. With the help of
the following theorem, they have proved that the electromagnetic field
can be obtained from a spinor field satisfying an equation similar to
the Dirac equation. This theorem also serves to illustrate how
efficiently computations can be performed in the STA framework.

###### Theorem 7.1.

Any bivector @xmath can be written as

  -- -------- --
     @xmath   
  -- -------- --

for some (nonzero) spinor @xmath

###### Proof.

Take any bivector @xmath . Note that

  -- -------- -- --------
     @xmath      (7.11)
  -- -------- -- --------

for some @xmath and @xmath . We consider the cases @xmath and @xmath
separately.

If @xmath then @xmath and @xmath are not both zero and we can apply a
duality rotation of @xmath into

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

i.e. such that @xmath and @xmath . Hence, we can select an orthonormal
basis @xmath of the relative space, aligned so that @xmath and @xmath ,
where @xmath etc. Consider now a boost @xmath of angle @xmath in the
direction orthogonal to both @xmath and @xmath . Using that

  -- -------- -- --------
     @xmath      (7.13)
  -- -------- -- --------

and likewise for @xmath , we obtain

  -- -------- -- --------
     @xmath      (7.14)
  -- -------- -- --------

where we also noted that @xmath and @xmath . Since @xmath we can choose
@xmath and obtain @xmath , where @xmath . Finally, some relative space
rotor @xmath takes @xmath to our timelike target blade (relative space
vector) @xmath , i.e.

  -- -------- -- --------
     @xmath      (7.15)
  -- -------- -- --------

Summing up, we have that @xmath , where

  -- -------- -- --------
     @xmath      (7.16)
  -- -------- -- --------

When @xmath we have that both @xmath and @xmath . Again, we select an
orthonormal basis @xmath of the relative space so that @xmath and @xmath
. Note that

  -- -------- -- --------
     @xmath      (7.17)
  -- -------- -- --------

Thus, @xmath . Using that @xmath can be obtained from @xmath with some
relative space rotor @xmath , we have that @xmath , where

  -- -------- -- --------
     @xmath      (7.18)
  -- -------- -- --------

The case @xmath can be achieved not only using @xmath , but also with
e.g. @xmath . ∎

Note that we can switch @xmath for any other non-null reference blade,
e.g. @xmath .

###### Remark.

In the setting of electrodynamics, where @xmath is an electromagnetic
field, we obtain as a consequence of this theorem and proof the
following result due to Rainich, Misner and Wheeler. If we define an
extremal field as a field for which the magnetic (electric) field is
zero and the electric (magnetic) field is parallel to one coordinate
axis, the theorem of Rainich-Misner-Wheeler says that: “At any point of
Minkowski spacetime any nonnull electromagnetic field can be reduced to
an extremal field by a Lorentz transformation and a duality rotation.”

The reformulation of the Pauli and Dirac theory observables above
depended on the choice of fixed reference bases @xmath and @xmath . When
a different basis @xmath is selected, we cannot apply the same spinor
@xmath in ( 7.2 ) since this would in general yield an @xmath . This is
not a problem in flat space since we can set up a globally defined field
of such reference frames without ambiguity. However, in the covariant
setting of a curved manifold, i.e. when gravitation is involved, we
cannot fix a certain field of reference frames, but must allow a
variation in these and equations which transform covariantly under such
variations. In fibre bundle theory this corresponds to picking different
sections of an orthonormal frame bundle. We therefore seek a formulation
of spinor that takes care of this required covariance. Rodrigues, de
Souza, Vaz and Lounesto [ 20 ] have considered the following definition.

###### Definition 7.1.

A Dirac-Hestenes spinor (DHS) is an equivalence class of triplets @xmath
, where @xmath is an oriented orthonormal basis of @xmath , @xmath is an
element in @xmath , and @xmath is the representative of the spinor in
the basis @xmath . We define the equivalence relation by @xmath if and
only if @xmath and @xmath . The basis @xmath should be thought of as a
fixed reference basis and the choice of @xmath is arbitrary but fixed
for this basis. We suppress this choice and write just @xmath for the
spinor @xmath .

Note that when for example @xmath for some basis @xmath we now have the
desired invariance property @xmath for some other basis @xmath . Hence,
@xmath is now a completely basis independent object which in the Dirac
theory represents the physical and observable local current produced by
a Dirac particle.

The definition above allows for the construction of a covariant
Dirac-Hestenes spinor field. The possibility of defining such a field on
a certain manifold depends on the existence of a so called spin
structure on it. Geroch [ 6 ] has shown that in the spacetime case, i.e.
when the tangent space is @xmath , this is equivalent to the existence
of a globally defined field of time-oriented orthonormal reference
frames. In other words, the principal @xmath -bundle of the manifold
must be trivial. We direct the reader to [ 20 ] for a continued
discussion.

We end by mentioning that other types of spinors can be represented in
the STA as well. See e.g. [ 5 ] for a discussion on Lorentz, Majorana
and Weyl spinors.

## 8 Summary and discussion

We have seen that a vector space endowed with a quadratic form naturally
embeds in an associated geometric algebra. This algebra depends on the
signature and dimension of the underlying vector space, and expresses
the geometry of the space through the properties of its multivectors. By
introducing a set of products, involutions and other operations, we got
access to the rich structure of this algebra and could identify certain
significant types of multivectors, such as blades, rotors, and spinors.
Blades were found to represent subspaces of the underlying vector space
and gave a geometric interpretation to multivectors and the various
algebraic operations, while rotors connected the groups of
structure-respecting transformations to corresponding groups embedded in
the algebra. This enabled a powerful encoding of rotations using
geometric products and allowed us to identify candidates for spinors in
arbitrary dimensions.

The introduced concepts were put to practice when we worked out a number
of lower-dimensional examples. These had obvious applications in
mathematics and physics. Norm functions were found to act as
determinants on the respective algebras and helped us find the
corresponding groups of invertible elements. We noted that the
properties of such norm functions also lead to totally non-geometric
applications in number theory.

We also studied the relation between geometric algebras and matrix
algebras, and used the well-known representation theory of such algebras
to work out the corresponding representations of geometric algebras. The
dimensional restrictions of such representations led to proofs of
classic theorems regarding normed division algebras and vector fields on
spheres.

Throughout our examples, we saw that complex structures appear naturally
within real geometric algebras and that many formulations in physics
which involve complex numbers can be identified as structures within
real geometric algebras. Such identifications also resulted in various
geometric interpretations of complex numbers. This suggests that,
whenever complex numbers appear in an otherwise real or geometric
context, one should ask oneself if not a real geometric interpretation
can be given to them.

The combinatorial construction of Clifford algebra which we introduced
mainly served as a tool for understanding the structure of geometric or
Clifford algebras and the behaviour of, and relations between, the
different products. This construction also expresses the generality of
Clifford algebras in that they can be defined and find applications in
general algebraic contexts. Furthermore, it gives new suggestions for
how to proceed with the infinite-dimensional case. Combinatorial
Clifford algebra has previously been applied to simplify proofs in graph
theory [ 24 ] .

Finally, we considered examples in physics and in particular
relativistic quantum mechanics, where the representation of spinors as
even multivectors in the geometric algebra of spacetime led to
conceptual simplifications. The resulting picture is a rather classical
one, with particles as fields of operations which rotate and scale
elements of a reference basis into the observable expectation values.
Although this is a geometric and conceptually powerful view, it is
unfortunately not that enlightening with respect to the quantum
mechanical aspects of states and measurement. This requires an
operator-eigenvalue formalism which of course can be formulated in
geometric algebra, but sort of breaks the geometric picture. The
geometric view of spinors does fit in the context of quantum field
theory, however, since spinor fields there already assume a classical
character. It is not clear what conceptual simplifications that
geometric algebras can bring to other quantum mechanical theories than
the Pauli and Dirac ones, since most realistic particle theories are
formulated in infinite-dimensional spaces. Doran and Lasenby [ 4 ] have
presented suggestions for a multi-particle formulation in geometric
algebra, but it still involves a fixed and finite number of particles.

Motivated by the conceptual simplifications of Dirac theory brought by
the spacetime algebra, one can argue about the geometric significance of
all particles. The traditional classification of particles in terms of
spin quantum numbers relies on the complex representation theory of the
(inhomogeneous) Lorentz group. There are complex (or rather
complexified) representations of the STA-embedded scalar, spinor
(through the Dirac algebra), and vector fields, corresponding to spin 0,
@xmath , and 1, respectively. Coincidentally, the fundamental particles
that have been experimentally verified all have spin quantum numbers
@xmath or 1, corresponding to spinor fields and vector fields.
Furthermore, the proposed Higgs particle is a scalar field with spin 0.
Since all these types of fields are naturally represented within the STA
it then seems natural to me that exactly these spins have turned up.

## Acknowledgements

I would like to thank my supervisor Lars Svensson for many long and
interesting discussions, and for giving me a lot of freedom to follow my
own interests in the subject of geometric algebra. I am also grateful
for his decision to give introductory lectures to first year students
about geometric algebra and other mathematical topics which for some
reason are considered controversial. This is what spawned my interest in
mathematics in general and geometric algebra in particular.

## Appendix: Matrix theorems

In order to avoid long digressions in the text, we have placed proofs to
some, perhaps not so familiar, theorems in this appendix.

In the following theorem we assume that @xmath is an arbitrary
commutative ring and

  -- -------- --
     @xmath   
  -- -------- --

i.e. @xmath denotes the @xmath :th column in @xmath . If @xmath and
@xmath we let @xmath denote the @xmath -matrix minor obtained from
@xmath by deleting the rows and columns not in @xmath and @xmath .
Further, let @xmath denote the rank of @xmath , i.e. the highest integer
@xmath such that there exists @xmath as above with @xmath and @xmath .
By renumbering the @xmath :s we can without loss of generality assume
that @xmath .

###### Theorem A.1 (Basis minor).

If the rank of @xmath is @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

then every @xmath is a linear combination of @xmath .

###### Proof.

Pick @xmath and @xmath and consider the @xmath -matrix

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath . Expanding @xmath along the bottom row for fixed @xmath we
obtain

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where the @xmath are independent of the choice of @xmath (but of course
dependent on @xmath ). Hence,

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

and similarly for all @xmath . ∎

The following shows that the factorization @xmath is a unique property
of the determinant.

###### Theorem A.2 (Uniqueness of determinant).

Assume that @xmath is continuous and satisfies

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

for all @xmath . Then @xmath must be either @xmath , @xmath , @xmath or
@xmath for some @xmath .

###### Proof.

First, we have that

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

so @xmath and @xmath must be either 0 or 1. Furthermore,

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

for all @xmath , which implies that @xmath if @xmath and @xmath if
@xmath . We can therefore assume that @xmath and @xmath .

Now, an arbitrary matrix @xmath can be written as

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

where @xmath is on reduced row-echelon form (as close to the identity
matrix as possible) and @xmath are elementary row operations of the form

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

Because @xmath , we must have @xmath . This gives, since

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

that @xmath and

  -- -- -- -----
           (9)
  -- -- -- -----

In particular, we have @xmath and of course @xmath .

If @xmath is invertible, then @xmath . Otherwise, @xmath must contain a
row of zeros so that @xmath for some @xmath . But then @xmath and @xmath
. When @xmath is invertible we have @xmath and @xmath , i.e. @xmath and
@xmath . Hence,

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

We thus have that @xmath is completely determined by its values on
@xmath , @xmath and @xmath . Note that we have not yet used the
continuity of @xmath , but it is time for that now. We can split @xmath
into three connected components, namely @xmath , @xmath and @xmath ,
where the determinant is less than, equal to, and greater than zero,
respectively. Since @xmath and @xmath , we have by continuity of @xmath
that

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

on these parts. Using that @xmath , we have @xmath and @xmath where the
sign depends on ( 11 ). On @xmath we have a continuous map @xmath such
that

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

Forming @xmath , we then have a continuous map @xmath such that

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

By extending linearity from @xmath to @xmath and @xmath by continuity,
we must have that @xmath for some @xmath . Hence, @xmath . Continuity
also demands that @xmath .

It only remains to consider @xmath . We have @xmath and @xmath , i.e.

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

Proceeding as above, @xmath is linear, so that @xmath for some @xmath ,
hence @xmath . One can verify that the following identity holds for all
@xmath :

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

This gives @xmath and, using ( 14 ),

  -- -- -- ------
           (16)
  -- -- -- ------

which requires @xmath .

We conclude that @xmath is completely determined by @xmath , where
@xmath and @xmath , plus whether @xmath takes negative values or not.
This proves the theorem. ∎