### 4.4.3 Sensitivity

To test the sensitivity of the Bump Hunter, we generate some specific
new physics signal, pass it through the full CDF detector simulation,
and inject it gradually on top of pseudo-data pulled from the Standard
Model background, until the Bump Hunter identifies a discovery-level
bump.

##### 120 GeV Higgs in association with @xmath

The pseudo-signal use for this test contains a Standard Model Higgs of
mass 120 GeV, allowed to decay to @xmath , which has branching ratio 68%
[ 84 ] . The associated @xmath decays to @xmath or @xmath or @xmath plus
neutrino, with total branching ratio @xmath .

About 6500 signal events are required to obtain the first bump beyond
discovery threshold. Events passing selection criteria are distributed
in several final states, and 15 of them make it to the @xmath final
state, producing the bump in Fig. 4.20 .

Compensating for the branching ratio, we find that the required cross
section of @xmath to have this 5 @xmath level discovery would be about
14.4 pb, which is @xmath 90 times larger than the predicted Standard
Model cross section.

##### @xmath at mass 250 GeV

Pseudo-signal of a 250 GeV @xmath boson was generated, where @xmath may
decay to @xmath , where @xmath can be @xmath , @xmath , or @xmath . The
first discovery-level bump caused after injecting about 700 events of
this pseudo-signal. 55 events end up in the 1e+1e- final state, and form
the bump shown in Fig. 4.21 .

With 700 injected events the significance found is 3.7 @xmath , which is
higher than the discovery threshold of 3 @xmath . That is because the
pseudo-signal is injected in bunches of 100 events, so the actual
requirement is between 600 and 700 events. Dividing this number of
generated events by our integrated luminosity shows that we would need
the cross section times branching ratio of this signal to be
approximately 0.325 pb.

##### @xmath at mass 500 GeV

For this test we generated @xmath events of mass 500 GeV, where the
heavy boson decays to a @xmath pair. Injecting 5000 such events causes
simultaneously two significant bumps in the @xmath final state; one is
in the transverse mass between @xmath and the second highest @xmath jet
( @xmath ), with significance 3 @xmath ; the other is in the transverse
mass of the third highest @xmath ( @xmath ) and @xmath , with
significance 3.2 @xmath . The latter is shown in Fig. 4.22 .

In another instance, after injecting 4600 different pseudo-signal
events, a 3.3 @xmath effect after trials factor was created in the same
final state ( @xmath ), but this time in the variable @xmath , where one
would more easily interpret the excess as due to resonant production of
@xmath . That is shown in Fig. 4.22 as well.

With discovery cost of approximately 4800 events, the required cross
section is approximately 2.4 pb.

### 4.5 Summary of second round with 2 fb@xmath

Vista and Sleuth search for outliers, representing significant
discrepancies between data and Standard Model prediction. Unfortunately,
the result obtained is that no signficant outliers have been found
either in the total number of events in the Vista exclusive final
states, or in Sleuth ’s search of the @xmath tails. Disregarding effects
from tuning corrections to the data, Sleuth ’s @xmath provides a
rigorous statistical calculation of the likelihood that the most
discrepant Sleuth final state seen would have arisen purely by chance
from the Standard Model prediction and correction model constructed
within Vista .

Vista ’s correction model does not explicitly include some sources of
systematic uncertainty, including those associated with parton
distribution functions and showering parameters in the event generators
used; these sources of uncertainty are included implicitly, in that they
would be considered if necessary in the event of a possible discovery.
Other uncertainties related to the modeling of the CDF detector response
and object identification criteria are determined as part of Vista but
are not included in the calculation of @xmath . For the correction model
used, Sleuth finds @xmath .

The Bump Hunter, a new algorithm for identification of mass resonances,
did not find any significant mass bumps either, except for one that is
attributed to Pythia not modeling perfectly parton showering.

Although the Vista correction model could presumably be improved further
to show even better agreement with Standard Model prediction, finding
@xmath indicates that even the most discrepant @xmath tail is not of
statistical interest. The correction model used is thus good enough
(even without considering effect of systematic uncertainties on the
Sleuth final states) to conclude this search for outliers using Vista
and Sleuth in 2 fb @xmath .

This analysis does not prove that there is no new hint of physics buried
in these data; merely that this search does not find any.

## Chapter 5 Grand Summary and Conclusion

This thesis presents the first model-independent search for new physics
of such scope.

The Standard Model was implemented using a simplified set of
corrections.

New physics was sought that would cause significant discrepancies in (a)
populations of exclusive final states, (b) shapes of kinematic
distributions, (c) mass spectra, and (d) high- @xmath events.

The search was first conducted in 1 fb @xmath of CDF II data, revealing
no ground on which to support a discovery claim. It was then repeated in
2 fb @xmath of data, improved and enhanced with the Bump Hunter, an
algorithm to locate narrow resonances due to new massive particles.

Unfortunately and surprisingly, even with 2 fb @xmath the result was
null, in the sense that no new physics could be claimed with the
findings. The discrepancies seen were attributed mainly to the
difficulty in modeling soft radiated parton showers with Pythia . This
issue was suspected to be problematic, but no other analysis had
illustrated so clearly its repercussion.

Although no single analysis can guarantee that new physics is nowhere in
the data, it is highly informative that in a search of this scope
nothing exploitable was found. This is complemented and consented by the
numerous searches, dedicated to specific signals, which so far have
failed too to reveal what lies beyond the Standard Model.

Even with a null result, the value of this technique is great in
providing an overview of all data, even those nobody ever considers. It
can make a big difference at the later stages of the LHC, or in any
experiment where there is a proliferation of data, and a fairly accurate
theoretical prediction analogous to what our event generators and
detector simulation provide.

## Appendix A Correction Model Details

Some aspects of the correction model are fixed, rather than dynamically
adjusted by the global fit, which is viewed as just a tool to provide
reasonable values for some parameters of the correction model. Not every
parameter needs to be determined by a fit, as long as it is reasonable
or estimated beforehand, through a MC study for instance.

Implementation details of the correction model will be described in this
chapter in some extra detail.

### a.1 Fake rate physics

The following facts begin to build a unified understanding of fake rates
for electrons, muons, taus, and photons. This understanding is woven
throughout the correction model, and significantly informs and
constrains the Vista correction process. Explicit constraints derived
from these studies are provided in Appendix A.3 . The underlying
physical mechanisms for these fakes lead to simple and well justified
relations among them.

Table A.1 shows the response of the CDF detector simulation,
reconstruction, and object identification algorithms to single
particles. Using a single particle gun, @xmath particles of each type
shown at the left of the table are shot with @xmath GeV into the CDF
detector, uniformly distributed in @xmath and in @xmath . The resulting
reconstructed object types are shown at the top of the table, labeling
the columns. The first four entries on the diagonal at upper left show
the efficiency for reconstructing electrons and muons ¹ ¹ 1 The electron
and muon efficiencies shown in this table are different from the
correction factors 0025 and 0027 in Table 4.2 , which show the ratio of
the object efficiencies in the data to the object identification
efficiencies in CDFsim . . The fraction of electrons misidentified as
photons (top row, seventh column) is seen to be roughly equal to the
fraction of photons identified as electrons or positrons (fifth row,
first and second columns), and measures the number of radiation lengths
in the innermost regions of the CDF tracker. The fraction of @xmath
mesons identified as electrons or muons, primarily through semileptonic
decay, are shown in the four left columns, eleventh through fourteenth
rows. Other entries provide similarly useful information, most easily
comprehensible from simple physics.

The transverse momenta of the objects reconstructed from single
particles are displayed in Fig. A.1 . The relative resolutions for the
measurement of electron and muon momenta are shown in the first four
histograms on the diagonal at upper left. The histograms in the left
column, sixth through eighth rows, show that single neutral pions
misreconstructed as electrons have their momenta well measured, while
single charged pions misreconstructed as electrons have their momenta
systematically undermeasured, as discussed below. The histogram in the
top row, second column from the right, shows that electrons
misreconstructed as jets have their energies systematically
overmeasured. Other histograms in Fig. A.1 contain similarly relevant
information, easily overlooked without the benefit of this study, but
understandable from basic physics considerations once the effect has
been brought to attention.

Here and below @xmath denotes a quark fragmenting to @xmath carrying
nearly all of the parent quark’s energy, and @xmath denotes a parent
quark or gluon being misreconstructed in the detector as @xmath .

The probability for a light quark jet to be misreconstructed as an
@xmath can be written

  -- -------- -------- -------- -- -------
     @xmath   @xmath               (A.1)
                       @xmath      
                       @xmath      
                       @xmath      
  -- -------- -------- -------- -- -------

A similar equation holds for a light quark jet faking an @xmath .

The probability for a light quark jet to be misreconstructed as a @xmath
can be written

  -- -------- -------- -------- -- -------
     @xmath   @xmath               (A.2)
                       @xmath      
  -- -------- -------- -------- -- -------

Here @xmath denotes pion decay-in-flight, and @xmath denotes kaon
decay-in-flight; other processes contribute negligibly. A similar
equation holds for a light quark jet faking a @xmath .

The only non-negligible underlying physical mechanisms for a jet to fake
a photon are for the parent quark or gluon to fragment into a photon or
a neutral pion, carrying nearly all the energy of the parent quark or
gluon. Thus

  -- -------- -------- -------- -- -------
     @xmath   @xmath               (A.3)
                       @xmath      
  -- -------- -------- -------- -- -------

Up and down quarks and gluons fragment nearly equally to each species of
pion; hence

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (A.4)
                       @xmath      
  -- -------- -------- -------- -- -------

where @xmath denotes fragmentation into any pion carrying nearly all of
the parent quark’s energy. Fragmentation into each type of kaon also
occurs with equal probability; hence

  -- -------- -------- -------- -- -------
     @xmath   @xmath               (A.5)
                       @xmath      
  -- -------- -------- -------- -- -------

where @xmath denotes fragmentation into any kaon carrying nearly all of
the parent quark’s energy.

Pythia contains a parameter that sets the number of string fragmentation
kaons relative to the number of fragmentation pions. The default value
of this parameter, which has been tuned to LEP I data, is 0.3; for every
1 up quark and every 1 down quark, 0.3 strange quarks are produced.
Strange particles are produced perturbatively in the hard interaction
itself, and in perturbative radiation, at a ratio larger than 0.3:1:1.
This leads to the inequality

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

where @xmath and @xmath are as defined above.

The probability for a jet to be misreconstructed as a tau lepton can be
written

  -- -------- -- -------
     @xmath      (A.7)
  -- -------- -- -------

where @xmath denotes the probability for a jet to fake a 1-prong tau,
and @xmath denotes the probability for a jet to fake a 3-prong tau. For
1-prong taus,

  -- -------- -------- -------- -- -------
     @xmath   @xmath               (A.8)
                       @xmath      
  -- -------- -------- -------- -- -------

Similar equations hold for negatively charged taus.

Figure A.4 shows the probability for a quark (or gluon) to fake a
one-prong tau, as a function of transverse momentum. Using fragmentation
functions tuned on LEP 1 data, Pythia predicts the probability for a
quark jet to fake a one-prong tau to be roughly four times the
probability for a gluon jet to fake a one-prong tau. This difference in
fragmentation is incorporated into Vista ’s treatment of jets faking
electrons, muons, taus, and photons. The Vista correction model includes
such correction factors as the probability for a jet with a parent quark
to fake an electron ( 0033 and 0034 ) and the probability for a jet with
a parent quark to fake a muon ( 0035 ); the probability for a jet with a
parent gluon to fake an electron or muon is then obtained by dividing
the values of these fitted correction factors by four.

This effect is investigated using fake one-prong taus reconstructed in
Pythia dijet samples.

Figure A.5 shows that the reconstructed fake tau has about @xmath of the
@xmath of the prominent generated particle, defined to be the generated
particle carrying the greatest @xmath and being within a cone of @xmath
centered on the reconstructed tau. The @xmath of the misreconstructed
tau is on average more undermeasured if the generated parton is a gluon
than if it is a quark. This reduction in the @xmath of the fake tau is
implemented in Vista when a jet is made to fake a @xmath during the
misreconstruction process.

Figure A.6 shows the remaining generated @xmath to be carried by neutral
particles: mostly @xmath ’s, followed by @xmath ’s and @xmath ’s
decaying to photons or to three neutral pions. The @xmath of the fake
tau is determined by the track and reconstructed @xmath ’s.

The physical mechanism underlying the process whereby an incident photon
or neutral pion is misreconstructed as an electron is a conversion in
the material serving as the support structure of the silicon vertex
detector. This process produces exactly as many @xmath as @xmath ,
leading to

  -- -------- -- -------
     @xmath      
     @xmath      (A.9)
  -- -------- -- -------

where @xmath is an electron or positron.

From Fig. A.1 , the average @xmath of electrons reconstructed from
25 GeV incident photons is @xmath GeV. The average @xmath of electrons
reconstructed from incident 25 GeV neutral pions is @xmath GeV.

The charge asymmetry between @xmath and @xmath in Table A.1 arises
because @xmath can capture on a nucleon, producing a hyperon ( @xmath ),
which @xmath does not produce, due to baryon number and strangeness
conservation. Among the products of the hyperon decay are neutral pions,
which decay electromagnetically and deposit in the electromagnetic
calorimeter the energy needed to have a fake @xmath . The absense of
this process in @xmath interaction reduces the @xmath relative @xmath by
roughly a factor of two.

The physical process primarily responsible for @xmath is inelastic
charge exchange

  -- -------- -- --------
     @xmath      
     @xmath      (A.10)
  -- -------- -- --------

occurring within the electromagnetic calorimeter. The charged pion
leaves the “electron’s” track in the CDF tracking chamber, and the
@xmath produces the “electron’s” electromagnetic shower. No true
electron appears at all in this process, except as secondaries in the
electromagnetic shower originating from the @xmath .

The average @xmath of reconstructed “electrons” originating from a
single charged pion is @xmath GeV, indicating that the misreconstructed
“electron” in this case is measured to have on average only 75% of the
total energy of the parent quark or gluon. This is expected, since the
recoiling nucleon from the charge exchange process carries some of the
incident pion’s momentum.

An additional small loss in energy for a jet misreconstructed as an
electron, photon, or muon is expected since the leading @xmath , @xmath
, @xmath , or @xmath takes only some fraction of the parent quark’s
energy.

The cross sections for @xmath and @xmath , proceeding through the
isospin @xmath conserving and @xmath independent strong interaction, are
roughly equal. The corresponding particles in the two reactions are
related by interchanging the signs of their @xmath -components of
isospin.

The probability for a 25 GeV @xmath to decay to a @xmath can be written

  -- -------- -------- -------- -- --------
     @xmath   @xmath               (A.11)
                       @xmath      
  -- -------- -------- -------- -- --------

The probability for the pion to decay within the tracking volume is

  -- -------- -- --------
     @xmath      (A.12)
  -- -------- -- --------

where @xmath GeV / 140 MeV @xmath is the pion’s Lorentz boost, the
proper decay length of the charged pion is @xmath meters, and the radius
of the CDF tracking volume is @xmath meters, giving @xmath . The
probability for the pion to decay within the calorimeter volume is

  -- -------- -- --------
     @xmath      (A.13)
  -- -------- -- --------

where @xmath meters is the nuclear interaction length for charged pions
on lead or iron and the path length through the calorimeter is @xmath
meters, leading to @xmath . Summing the contributions from decay within
the tracking volume and decay within the calorimeter volume, @xmath .

The primary physical mechanism by which a jet fakes a photon is for the
parent quark or gluon to fragment into a leading @xmath carrying nearly
all the momentum. The highly boosted @xmath decays within the beam pipe
to two photons that are sufficiently collinear to appear in the
preshower, electromagnetic calorimeter, and shower maximum detector as a
single photon. Thus

  -- -------- -- --------
     @xmath      (A.14)
  -- -------- -- --------

An immediate corollary is that the misreconstructed “photon” carries the
energy of the parent quark or gluon, and is well measured.

Since @xmath , it follows from Eq. A.4 and Table A.1 that the conversion
contribution to @xmath is @xmath , and the charge exchange contribution
is @xmath :

  -- -------- -------- -------- -------- --------
     @xmath   @xmath                     
              @xmath   @xmath            
     @xmath   @xmath                     (A.15)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The number of @xmath events in data is 0.9 times the number of @xmath
events. This charge asymmetry arises from @xmath and @xmath in Table A.1
. Quantitatively,

  -- -------- -- --------
     @xmath      (A.16)
  -- -------- -- --------

where 0.9 is the sum of 0.75 from Eq. A.15 and @xmath from Eq. A.6 , and
0.2 is twice @xmath . From @xmath and @xmath in Table A.1 , @xmath and
@xmath , predicting @xmath , in reasonable agreement with the ratio of
the observed number of events in the @xmath and @xmath final states.

The number of @xmath events observed in CDF Run II is 1.1 times the
number of @xmath events observed. This charge asymmetry arises from
@xmath and @xmath in Table A.1 .

The physical mechanism by which a prompt photon fakes a tau lepton is
for the photon to convert, producing an electron or positron carrying
most of the photon’s energy, which is then misreconstructed as a tau.
The probability for this to occur is equal for positively and negatively
charged taus,

  -- -------- -- --------
     @xmath      (A.17)
  -- -------- -- --------

and is related to previously defined quantities by

  -- -------- -- --------
     @xmath      (A.18)
  -- -------- -- --------

where @xmath denotes the fraction of produced photons that are
reconstructed as electrons, @xmath denotes the fraction of produced
electrons that are reconstructed as electrons, and hence @xmath is the
fraction of produced photons that pair produce a single leading
electron.

Note @xmath from Table A.1 , as expected, with value of @xmath
determined by the amount of material in the inner detectors and the
tightness of isolation criteria. A hard bremsstrahlung followed by a
conversion is responsible for electrons to be reconstructed with
opposite sign; hence

  -- -------- -------- -- -- --------
     @xmath   @xmath         
     @xmath   @xmath         (A.19)
  -- -------- -------- -- -- --------

where the factor of @xmath comes because the material already traversed
by the @xmath will not be traversed again by the @xmath . In particular,
track curvature mismeasurement is not responsible for erroneous sign
determination in the central region of the CDF detector.

From knowledge of the underlying physical mechanisms by which jets fake
electrons, muons, taus, and photons, the simple use of a reconstructed
jet as a lepton or photon with an appropriate fake rate applied to the
weight of the event needs slight modification to correctly handle the
fact that a jet that has faked a lepton or photon generally is measured
more accurately than a hadronic jet. Rather than using the momentum of
the reconstructed jet, the momentum of the parent quark or gluon is
determined by adding up all Monte Carlo particle level objects within a
cone of @xmath about the reconstructed jet. In misreconstructing a jet
in an event, the momentum of the corresponding parent quark or gluon is
used rather than the momentum of the reconstructed jet. A jet that fakes
a photon then has momentum equal to the momentum of the parent quark or
gluon plus a fractional correction equal to @xmath to account for
leakage out of the cone of @xmath , and a further smearing of @xmath ,
reflecting the electromagnetic resolution of the CDF detector. The
momenta of jets that fake photons are multiplied by an overall factor of
1.12, and jets that fake electrons, muons, or taus are multiplied by an
overall factor of @xmath . These numbers are determined by the @xmath ,
@xmath , and @xmath final states. The distributions most sensitive to
these numbers are the missing energy and the jet @xmath .

A @xmath quark fragmenting into a leading @xmath hadron that then decays
leptonically or semileptonically results in an electron or muon that
shares the @xmath of the parent @xmath quark with the associated
neutrino. If all hadronic decay products are soft, the distribution of
the momentum fraction carried by the charged lepton can be obtained by
considering the decay of a scalar to two massless fermions. Isolated and
energetic electrons and muons arising from parent @xmath quarks in this
way are modeled as having @xmath equal to the parent @xmath quark @xmath
, multiplied by a random number uniformly distributed between 0 and 1.

### a.2 Additional background sources

This appendix provides additional details on the estimation of the
Standard Model prediction.

#### a.2.1 Cosmic ray and beam halo muons

There are four dominant categories of events caused by cosmic ray muons
penetrating the detector: @xmath , @xmath , @xmath , and @xmath . There
is negligible contribution from cosmic ray secondaries of any particle
type other than muons.

A cosmic ray muon penetrating the CDF detector whose trajectory passes
within 1 mm of the beam line and within @xmath cm of the origin may be
reconstructed as two outgoing muons. In this case the cosmic ray event
is partitioned into the final state @xmath . If one of the tracks is
missed, the cosmic ray event is partitioned into the final state @xmath
. The standard CDF cosmic ray filter, which makes use of drift time
information in the central tracking chamber, is used to reduce these two
categories of cosmic ray events.

CDF data events with exactly one track (corresponding to one muon) and
events with exactly two tracks (corresponding to two muons) are used to
estimate the cosmic ray muon contribution to the final states @xmath and
@xmath after the cosmic ray filter. This sample of events is used as the
SM background process cosmic @xmath . The cosmic @xmath sample does not
contribute to the events passing the analysis offline trigger, whose
cleanup cuts require the presence of three or more tracks.

The remaining two categories are @xmath and @xmath , resulting from a
cosmic ray muon that penetrates the CDF electromagnetic or hadronic
calorimeter and undergoes a hard bremsstrahlung in one calorimeter cell.
Such an interaction can mimic a single photon or a single jet,
respectively. The reconstruction algorithm infers the presence of
significant missing energy balancing the “photon” or “jet.” If this
cosmic ray interaction occurs during a bunch crossing in which there is
a @xmath interaction producing three or more tracks, the event will be
partitioned into the final state @xmath or @xmath .

CDF data events with fewer than three tracks are used to estimate the
cosmic ray muon contribution to the final states @xmath and @xmath .
These samples of events are used as SM background processes cosmic
@xmath and cosmic @xmath for the modeling of this background,
corresponding to offline triggers requiring a photon with @xmath GeV, or
a jet with @xmath GeV (prescaled) or @xmath GeV (unprescaled),
respectively. These samples do not contribute to the events passing the
analysis offline trigger, whose cleanup cuts require three or more
tracks. The contribution of these events is adjusted with correction
factors that are listed as cosmic @xmath and cosmic @xmath “ @xmath
-factors” in Table 4.2 , but which are more properly understood as
reflecting the number of bunch crossings with zero @xmath interactions
(resulting in zero reconstructed tracks) relative to the number of bunch
crossings with one or more interactions (resulting in three or more
reconstructed tracks).

The cosmic ray muon contribution to the final states @xmath and @xmath
is uniform as a function of the CDF azimuthal angle @xmath . Consider
the CDF detector to be a thick cylindrical shell, and consider two
arbitrary infinitesimal volume elements at different locations in the
material of the shell. Since the two volume elements have similar
overburdens, the number of cosmic ray muons with @xmath GeV penetrating
the first volume element is very nearly the same as the number of cosmic
ray muons with @xmath GeV penetrating the second volume element. Since
the material of the CDF calorimeters is uniform as a function of CDF
azimuthal angle @xmath , it follows that the cosmic ray muon
contribution to the final states @xmath and @xmath should also be
uniform as a function of @xmath . In particular, it is noted that the
@xmath dependence of this contribution depends solely on the material
distribution of CDF calorimeter, which is uniform in @xmath , and has no
dependence on the distribution of the horizon angle of the muons from
cosmic rays.

The final states @xmath and @xmath are also populated by beam halo
muons, traveling horizontally through the CDF detector in time with a
bunch. A beam halo muon can undergo a hard bremsstrahlung in the
electromagnetic or hadronic calorimeters, producing an energy deposition
that can be reconstructed as a photon or jet, respectively. These beam
halo muons tend to lie in the horizontal plane and outside of the
Tevatron ring, as if centrifugally hurled away from the beam; they
horizontally penetrate the CDF detector along @xmath at @xmath and
@xmath , hence at @xmath .

Fig. A.7 shows the @xmath and @xmath final states, in which events come
primarily from cosmic ray and beam halo muons.

#### a.2.2 Multiple interactions

In order to estimate event overlaps, consider an interesting event
observed in final state C, which looks like an overlap of two events in
the final states A and B. An example is C= e+e-4j , A= e+e- and B= 4j .
It is desired to estimate how many C events are expected from the
overlap of A and B events, given the observed frequencies of A and B.

Let @xmath be the instantaneous luminosity as a function of time @xmath
; let

  -- -------- -- --------
     @xmath      (A.20)
  -- -------- -- --------

denote the total integrated luminosity; and let

  -- -------- -- --------
     @xmath      (A.21)
  -- -------- -- --------

be the luminosity-averaged instantaneous luminosity. Denote by @xmath
the time interval of 396 ns between successive bunch crossings. The
total number of effective bunch crossings @xmath is then

  -- -------- -- --------
     @xmath      (A.22)
  -- -------- -- --------

Letting @xmath and @xmath denote the number of observed events in final
states A and B, it follows that the number of events in the final state
C expected from overlap of A and B is

  -- -------- -- --------
     @xmath      (A.23)
  -- -------- -- --------

Overlap events are included in the SM background estimate, although
their contribution is generally negligible.

#### a.2.3 Intrinsic @xmath

Significant discrepancy is observed in many final states containing two
objects o1 and o2 in the variables @xmath (o1,o2) , uncl @xmath , and
@xmath . These discrepancies are ascribed to the sum of two effects: (1)
an intrinsic Fermi motion of the colliding partons within the proton and
anti-proton, and (2) soft radiation along the beam axis. The sum of
these two effects appears to be larger in Nature than predicted by
Pythia with the parameter tunes used for the generation of the samples
employed in this analysis. This discrepancy is well known from previous
studies at the Tevatron and elsewhere, and affects this analysis
similarly to other Tevatron analyses.

The @xmath and @xmath electroweak samples used in this analysis have
been generated with an adjusted Pythia parameter that increases the
intrinsic @xmath . For all other generated Standard Model events, the
net effect of the Fermi motion of the colliding partons and the soft
non-perturbative radiation is hypothesized to be described by an overall
“effective intrinsic @xmath ,” and the center of mass of each event is
given a transverse kick. Specifically, for every event of invariant mass
@xmath and generated summed transverse momentum @xmath , a random number
@xmath is pulled from the probability distribution

  -- -------- -------- -------- -- --------
     @xmath   @xmath               (A.24)
                       @xmath      
  -- -------- -------- -------- -- --------

where @xmath evaluates to unity if true and zero if false; @xmath is a
Gaussian function of @xmath with center at @xmath and width @xmath ;
@xmath is the width of the core of the double Gaussian; and @xmath is
the width of the second, wider Gaussian. The event is then boosted to an
inertial frame traveling with speed @xmath with respect to the lab
frame, in a direction transverse to the beam axis, where @xmath is the
invariant mass of all reconstructed objects in the event, along an
azimuthal angle pulled randomly from a uniform distribution between 0
and @xmath . The momenta of identified objects are recalculated in the
lab frame. Sixty percent of the recoil kick is assigned to unclustered
momentum in the event. The remaining forty percent of the recoil kick is
assumed to disappear down the beam pipe, and contributes to the missing
transverse momentum in the event. This picture, and the particular
parameter values that accompany this story, are determined primarily by
the uncl @xmath and @xmath distributions in highly populated two-object
final states, including the low- @xmath @xmath final state, the high-
@xmath @xmath final state, and the final states @xmath , @xmath , and
@xmath .

Under the hypothesis described, reasonable although imperfect agreement
with observation is obtained. The result of this analysis supports the
conclusions of previous studies indicating that the effective intrinsic
@xmath needed to match observation is quite large relative to naive
expectation. That the data appear to require such a large effective
intrinsic @xmath may be pointing out the need for some basic improvement
to our understanding of this physics.

### a.3 Global fit

This section describes the construction of the global @xmath used in the
Vista global fit.

#### a.3.1 The @xmath

The bins in the CDF high- @xmath data sample are labeled by the index
@xmath , where each value of @xmath represents a phrase such as “this
bin contains events with three objects: one with @xmath GeV and @xmath ,
one with @xmath GeV and @xmath , and one with @xmath GeV and @xmath ,”
and each value of @xmath represents a phrase such as “this bin contains
events with three objects: an electron, muon, and jet, respectively.”
The reason for splitting @xmath into @xmath and @xmath is that a jet can
fake an electron (mixing the contents of @xmath ), but an object with
@xmath cannot fake an object with @xmath (no mixing of @xmath ). The
term corresponding to the @xmath bin takes the form of Eq. 3.1 , where
@xmath is the number of data events observed in the @xmath bin, @xmath
is the number of events predicted by the Standard Model in the @xmath
bin, @xmath is the Monte Carlo statistical uncertainty on the Standard
Model prediction in the @xmath bin, and @xmath is the statistical
uncertainty on the prediction in the @xmath bin. To legitimize the use
of Gaussian errors, only bins containing eight or more data events are
considered. The Standard Model prediction @xmath for the @xmath bin can
be written in terms of the introduced correction factors as

  -- -------- -- -------- -- --------
     @xmath                  (A.25)
                             
                 @xmath      
                 @xmath      
                 @xmath      
                 @xmath      
  -- -------- -- -------- -- --------

where @xmath is the Standard Model prediction for the @xmath bin; the
index @xmath is the Cartesian product of the two indices @xmath and
@xmath introduced above, labeling the regions of the detector in which
there are energy clusters and the identified objects corresponding to
those clusters, respectively; the index @xmath is a dummy summation
index; the index @xmath labels Standard Model background processes, such
as dijet production or @xmath +1 jet production; @xmath is the initial
number of Standard Model events predicted in bin @xmath from the process
labeled by the index @xmath ; @xmath is the probability that an event
produced with energy clusters in the detector regions labeled by @xmath
that are identified as objects labeled by @xmath would be mistaken as
having objects labeled by @xmath ; and @xmath represents the probability
that an event produced with energy clusters in the detector regions
labeled by @xmath that are identified as objects labeled by @xmath would
pass the trigger.

The quantity @xmath is obtained by generating some number @xmath (say
@xmath ) of Monte Carlo events corresponding to the process @xmath . The
event generator provides a cross section @xmath for this process @xmath
. The weight of each of these Monte Carlo events is equal to @xmath .
Passing these events through the CDF simulation and reconstruction, the
sum of the weights of these events falling into the bin @xmath is @xmath
.

#### a.3.2 @xmath

The term @xmath in Eq. 3.2 reflects constraints on the values of the
correction factors determined by data other than those in the global
high- @xmath sample. These constraints include @xmath -factors taken
from theoretical calculations and numbers from the CDF literature when
use is made of CDF data external to the Vista high- @xmath sample. The
constraints imposed are:

-   The luminosity ( 0001 ) is constrained to be within 6% of the value
    measured by the CDF Čerenkov luminosity counters.

-   The fake rate @xmath ( 0039 ) is constrained to be @xmath , from the
    single particle gun study of Appendix A.1 .

-   The fake rate @xmath ( 0032 ) plus the efficiency @xmath ( 0026 )
    for electrons in the plug is constrained to be within 1% of unity.

-   Noting @xmath corresponds to correction factor 0039 , @xmath , and
    @xmath , and taking @xmath and @xmath from the single particle gun
    study of Appendix A.1 , the fake rate @xmath ( 0038 ) is constrained
    to @xmath .

-   The @xmath -factors for dijet production ( 0018 and 0019 ) are
    constrained to @xmath and @xmath in the kinematic regions @xmath GeV
    and @xmath GeV, respectively, where @xmath is the transverse
    momentum of the scattered partons in the @xmath process in the
    colliding parton center of momentum frame.

-   The inclusive @xmath -factor for @xmath jets ( 0004 – 0007 ) is
    constrained to @xmath [ 85 , 86 ] .

-   The inclusive @xmath -factor for @xmath jets ( 0008 – 0010 ) is
    constrained to @xmath [ 87 ] .

-   The inclusive @xmath -factors for @xmath and @xmath production (
    0011 – 0014 and 0015 – 0017 ) are subject to a 2-dimensional
    Gaussian constraint, with mean at the NNLO/LO theoretical values [
    88 ] , and a covariance matrix that encapsulates the highly
    correlated theoretical uncertainties, as discussed in Appendix A.4 .

-   Trigger efficiency correction factors are constrained to be less
    than unity.

-   All correction factors are constrained to be positive.

#### a.3.3 Covariance matrix

This section describes the correction factor covariance matrix @xmath .
The inverse of the covariance matrix is obtained from

  -- -------- -- --------
     @xmath      (A.26)
  -- -------- -- --------

where @xmath is defined by Eq. 3.2 as a function of the correction
factor vector @xmath , vector elements @xmath and @xmath are the @xmath
and @xmath correction factors, and @xmath is the vector of correction
factors that minimizes @xmath . Numerical estimation of the right hand
side of Eq. A.26 is achieved by calculating @xmath at @xmath and at
positions slightly displaced from @xmath in the direction of the @xmath
and @xmath correction factors, denoted by the unit vectors @xmath and
@xmath . Approximating the second partial derivative

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

leads to

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (A.27)
                                @xmath   
                                @xmath   
                                @xmath   
  -- -------- -------- -------- -------- --------

for appropriately small steps @xmath and @xmath away from the minimum.
The covariance matrix @xmath is calculated by inverting @xmath . The
diagonal element @xmath is the variance @xmath of the @xmath correction
factor, and the correlation @xmath between the @xmath and @xmath
correction factors is @xmath . The variances of each correction factor,
corresponding to the diagonal elements of the covariance matrix, are
shown in Table 4.2 . The correlation matrix obtained is shown in Table
A.2 .

### a.4 Correction factor values

This section provides notes on the values of the Vista correction
factors obtained from a global fit of Standard Model prediction to data.
The correction factors considered are numbers that can in principle be
calculated a priori , but whose calculation is in practice not feasible.
These correction factors divide naturally into two classes, the first of
which reflects the difficulty of calculating the Standard Model
prediction to all orders, and the second of which reflects the
difficulty of understanding from first principles the response of the
experimental apparatus.

The theoretical correction factors considered are of two types. The
difficulty of calculating the Standard Model prediction for many
processes to all orders in perturbation theory is handled through the
introduction of @xmath -factors, representing the ratio of the true all
orders prediction to the prediction at lowest order in perturbation
theory. Uncertainties in the distribution of partons inside the
colliding proton and anti-proton as a function of parton momentum are in
principle handled through the introduction of correction factors
associated with parton distribution functions, but there are currently
no discrepancies to motivate this.

Experimental correction factors correspond to numbers describing the
response of the CDF detector that are precisely calculable in principle,
but that are in practice best constrained by the high- @xmath data
themselves. These correction factors take the form of the integrated
luminosity, object identification efficiencies, object misidentification
probabilities, trigger efficiencies, and energy scales.

#### a.4.1 @xmath-factors

For nearly all Standard Model processes, @xmath -factors are used as an
overall multiplicative constant, rather than being considered to be a
function of one or more kinematic variables. The spirit of the approach
is to introduce as few correction factors as possible, and to only
introduce correction factors motivated by specific discrepancies.

###### 0001.

The integrated luminosity of the analysis sample has a close
relationship with the theoretically determined values of inclusive
@xmath and @xmath production at the Tevatron. Figure A.8 shows the
variation in calculated inclusive @xmath and @xmath @xmath -factors
under changes in the assumed parton distribution functions. Each point
represents a different @xmath and @xmath inclusive cross section
determined using modified parton distribution functions. The use of 16
bases to reflect systematic uncertainties results in 32 black dots in
Fig. A.8 . The uncertainties in the @xmath and @xmath cross sections due
to variations in the renormalization and factorization scales are nearly
100% correlated; varying these scales affects both the @xmath and @xmath
inclusive cross sections in the same way. The uncertainties in the
parton distribution functions and the choice of renormalization and
factorization scales represent the dominant contributions to the
theoretical uncertainty in the total inclusive @xmath and @xmath cross
section calculations at the Tevatron. The term in @xmath that reflects
our knowledge of the theoretical prediction of the inclusive @xmath and
@xmath cross sections explicitly acknowledges this high degree of
correlation.

Theoretical constraints on all other @xmath -factors are assumed to be
uncorrelated with each other, not because the uncertainties of these
calculations are indeed uncorrelated, but rather because the
correlations among these computations are poorly known.

###### 0002, 0003.

The cosmic @xmath and cosmic @xmath backgrounds are estimated using
events recorded in the CDF data with one or more reconstructed photons
and with two or fewer reconstructed tracks. The use of events with two
or fewer reconstructed tracks is a new technique for estimating these
backgrounds. These correction factors are primarily constrained by the
number of events in the Vista @xmath and @xmath final states. The values
are related to (and consistent with) the fraction of bunch crossings
with one or more inelastic @xmath interactions, complicated slightly by
the requirement that any jet falling in the final state @xmath has at
least 5 GeV of track @xmath within a cone of 0.4 relative to the jet
axis.

###### 0004, 0005, 0006, 0007.

The NLOJET++ calculation of the @xmath inclusive @xmath -factor
constrains the cross section weighted sum of the @xmath , @xmath ,
@xmath , and @xmath correction factors to @xmath [ 85 , 86 ] .

###### 0008, 0009, 0010.

The DIPHOX calculation of the inclusive @xmath cross section at NLO
constrains the weighted sum of these correction factors to @xmath [ 87 ]
. From Table 4.2 , the @xmath @xmath -factor ( 0009 ) appears
anomalously large. Figure A.9 shows a calculation of this @xmath @xmath
-factor using NLOJET++ [ 85 , 86 ] as a function of summed transverse
momentum. The NLO correction to the LO prediction is found to be large,
and not manifestly inconsistent with the value for this @xmath -factor
determined from the Vista fit. The cross section for @xmath production
has not been calculated at NLO.

###### 0011, 0012, 0013, 0014.

These correction factors correspond to @xmath -factors for @xmath
production in association with zero, one, two, and three or more jets,
respectively. A linear combination of these correction factors is
constrained by the requirement that the inclusive @xmath production
cross section is consistent with the NNLO calculation of Ref. [ 89 ] .
The values of these correction factors, and their trend of decreasing as
the number of jets increases, depends heavily on the choice of
renormalization and factorization scales. The individual correction
factors are not explicitly constrained by a NLO calculation.

###### 0015, 0016, 0017.

These correction factors correspond to @xmath -factors for @xmath
production in association with zero, one, and two or more jets,
respectively. A linear combination of these correction factors is
constrained by the requirement that the inclusive @xmath production
cross section is consistent with the NNLO calculation of Ref. [ 89 ] .

###### 0018, 0019.

The two @xmath -factors for dijet production correspond to two bins in
@xmath , the @xmath of the hard two to two scattering in the parton
center of mass frame. These correction factors are constrained by a NLO
calculation [ 90 ] , and show expected behavior as a function of @xmath
.

###### 0020, 0021.

The two @xmath -factors for 3-jet production, corresponding to two bins
in @xmath , are unconstrained by any NLO calculation, but show
reasonable behavior as a function of @xmath .

###### 0022, 0023.

The @xmath -factors for 4-jet production, corresponding to two bins in
@xmath , are unconstrained by any NLO calculation, but show reasonable
behavior as a function of @xmath .

###### 0024.

The @xmath -factor for the production of five or more jets, constrained
primarily by the Vista low- @xmath @xmath final state, is found to be
close to unity.

#### a.4.2 Identification efficiencies

The correction factors in this section, although billed as
“identification efficiencies,” are truly ratios of the identification
efficiency in the data relative to the identification efficiency in
CDFsim . A correction factor value of unity indicates a proper modeling
of the overall identification efficiency by CDFsim ; a correction factor
value of 0.5 indicates that CDFsim overestimates the overall
identification efficiency by a factor of two.

###### 0025.

The central electron identification efficiency scale factor is close to
unity, indicating the central electron efficiency measured in data is
similar (to within 1%) to the central electron efficiency in the CDF
detector simulation. This reflects an emphasis within CDF on tuning the
detector simulation for central electrons. The determination of this
correction factor is dominated by the Vista final states @xmath and
@xmath , where one of the electrons has @xmath .

###### 0026.

The plug electron identification efficiency scale factor is several
percent less than unity, indicating that the CDF detector simulation
slightly overestimates the electron identification efficiency in the
plug region of the CDF detector. The determination of this correction
factor is dominated by the Vista final states @xmath and @xmath , where
one of the electrons has @xmath .

###### 0027, 0028.

To reduce backgrounds hypothesized to arise from pion and kaon decays in
flight with a substantially mismeasured track, a very good track fit in
the CDF tracker is required. Partially due to this tight track fit
requirement, CDF muon identification efficiencies in the regions @xmath
and @xmath are overestimated in the CDF detector simulation by over 10%.
The determination of the identification efficiencies @xmath is dominated
by the Vista final states @xmath and @xmath .

###### 0029.

The central photon identification efficiency scale factor is determined
primarily by the number of events in the Vista final states @xmath and
@xmath . The uncertainty on this correction factor is highly correlated
with the uncertainties on the @xmath @xmath -factor, the @xmath fake
rate, and the @xmath @xmath -factor.

###### 0030.

The plug photon identification efficiency scale factor is determined
primarily by the number of events in the Vista final state @xmath . The
uncertainty on this correction factor is highly correlated with the
uncertainty on the plug @xmath fake rate.

###### 0031.

The @xmath -jet identification efficiency is determined to be consistent
with the prediction from CDFsim .

#### a.4.3 Fake rates

###### 0032.

The fake rate @xmath for electrons to be misreconstructed as photons in
the plug region of the detector is added on top of the significant
number of electrons misreconstructed as photons by CDFsim .

###### 0033.

In Vista , the contribution of jets faking electrons is modeled by
applying a fake rate @xmath to Monte Carlo jets. Vista represents the
first large scale Tevatron analysis in which a completely Monte Carlo
based modeling of jets faking electrons is employed. Significant
understanding of the physical mechanisms contributing to this fake rate
has been achieved, as summarized in Appendix A.1 . Consistency with this
understanding is required; for example, @xmath . The value of this
correction factor is determined primarily by the number of events in the
Vista final state @xmath , where the electron is identified in the
central region of the CDF detector. It is notable that this fake rate is
independent of global event properties, and that a consistent
simultaneous understanding of the @xmath , @xmath , @xmath , and @xmath
final states is obtained.

###### 0034.

The value of the fake rate @xmath in the plug region of the CDF detector
is roughly one order of magnitude larger than the corresponding fake
rate @xmath in the central region of the detector, consistent with an
understanding of the relative performance of the detector in the central
and plug regions for the identification of electrons. This correction
factor is determined primarily by the number of events in the Vista
final state @xmath , where the electron is identified in the plug region
of the CDF detector.

###### 0035.

In Vista , the contribution of jets faking muons is modeled by applying
a fake rate @xmath to Monte Carlo jets. Vista represents the first large
scale Tevatron analysis in which a completely Monte Carlo based modeling
of jets faking muons is employed. The value obtained from the Vista fit
is seen to be roughly one order of magnitude smaller than the fake rate
@xmath in the central region of the detector, consistent with our
understanding of the physical mechanisms underlying these fake rates, as
described in Appendix A.1 . The value of this correction factor is
determined primarily by the number of events in the Vista final state
@xmath .

###### 0036.

The fake rate @xmath has @xmath dependence explicitly imposed. The
number of tracks inside a typical jet, and hence the probability that a
secondary vertex is (mis)reconstructed, increases with jet @xmath . The
values of these correction factors are consistent with the mistag rate
determined using secondary vertices reconstructed on the other side of
the beam axis with respect to the direction of the tagged jet [ 91 ] .
The value of this correction factor is determined primarily by the
number of events in the Vista final states @xmath and @xmath .

###### 0037, 0038.

The fake rate @xmath decreases with jet @xmath , since the number of
tracks inside a typical jet increases with jet @xmath . The values of
these correction factors are determined primarily by the number of
events in the Vista final state @xmath .

###### 0039, 0040.

The fake rate @xmath is determined separately in the central and plug
regions of the CDF detector. The values of these correction factors are
determined primarily by the number of events in the Vista final states
@xmath and @xmath . The value obtained for 0039 is consistent with the
value obtained from a study using detailed information from the central
preshower detector. The fake rate determined in the plug region is
noticeably higher than the fake rate determined in the central region,
as expected.

#### a.4.4 Trigger efficiencies

###### 0041.

The central electron trigger inefficiency is dominated by not correctly
reconstructing the electron’s track at the first online trigger level.

###### 0042.

The plug electron trigger inefficiency is due to inefficiencies in
clustering at the second online trigger level.

###### 0043, 0044.

The muon trigger inefficiencies in the regions @xmath and @xmath derive
partly from tracking inefficiency, and partly from an inefficiency in
reconstructing muon stubs in the CDF muon chambers.
The value of these corrections factors are consistent with other trigger
efficiency measurements made using additional information [ 92 ] .

#### a.4.5 Energy scales

The Vista infrastructure also allows the jet energy scale to be treated
as a correction factor. At present this correction factor is not used,
since there is no discrepancy requiring it.

To understand the effect of introducing such a correction factor, a jet
energy scale correction factor is added and constrained to @xmath ,
reflecting the jet energy scale determination at CDF [ 50 ] . The fit
returns a value with a very small error, since this correction factor is
highly constrained by the low- @xmath @xmath , @xmath , @xmath , and
@xmath final states. Assuming perfectly correct modeling of jets faking
electrons, as described in Appendix A.1 , this is a correct energy scale
error. The inclusion of additional correction factor degrees of freedom
to reflect possible imperfections in this modeling of jets faking
electrons increases the energy scale error. The interesting conclusion
is that the jet energy scale (considered as a lone free parameter) is
very well constrained by the large number of dijet events; adjustment to
the jet energy scale must be accompanied by simultaneous adjustment of
other correction factors (such as the dijet @xmath -factor) in order to
retain agreement with data.

### a.5 Sleuth details

This appendix elaborates on the Sleuth partitioning rule, and on the
minimum number of events required for a final state to be considered by
Sleuth .

#### a.5.1 Partitioning

Table A.3 lists the Vista final states associated with each Sleuth final
state.

#### a.5.2 Minimum number of events

This section expands on a subtle point in the definition of the Sleuth
algorithm: for purely practical considerations, only final states in
which three or more events are observed in the data are considered.

Suppose @xmath ; then in computing @xmath all final states with @xmath
must be considered and accounted for. (A final state with @xmath , on
the other hand, counts as only @xmath final states, since the fraction
of hypothetical similar experiments in which @xmath in this final state
is equal to the fraction of hypothetical similar experiments in which
one or more events is seen in this final state, which is @xmath .) This
is a large practical problem, since it requires that all final states
with @xmath be enumerated and estimated, and it is difficult to do this
believably.

To solve this problem, let Sleuth consider only final states with at
least @xmath events observed in the data. The goal is to be able to find
@xmath . There will be some number @xmath of final states with expected
number of events @xmath , writing @xmath explicitly as a function of
@xmath ; thus @xmath must be chosen to be sufficiently large that all of
these @xmath final states can be enumerated and estimated. The time cost
of simulating events is such that the integrated luminosity of Monte
Carlo events is at most 100 times the integrated luminosity of the data;
this practical constraint restricts @xmath . The number of Sleuth
Tevatron Run II final states with @xmath is @xmath .

For small @xmath , keeping the first term in a binomial expansion yields
@xmath , where @xmath is the smallest @xmath found in any final state.
From the discussion above, the computation of @xmath from @xmath can
only be justified if @xmath ; if otherwise, final states with @xmath
will need to be accounted for. Thus @xmath can be confidently computed
only if @xmath .

Solving this inequality for @xmath and inserting values from above,

  -- -------- -- --------
     @xmath      (A.28)
  -- -------- -- --------

A believable trials factor can be computed if @xmath .

At the other end of the scale, computational strength limits the maximum
number of events Sleuth is able to consider to @xmath . Excesses in
which the number of events exceed @xmath are expected to be identified
by Vista ’s normalization statistic.

#### a.5.3 @xmath, population and @xmath

Sleuth estimates @xmath for a given final state by producing
pseudo-data, i.e. @xmath values that are distributed according to the
Standard Model prediction. It then scans all @xmath tails, finds the
smallest @xmath and compares it to the @xmath from the actual data. That
is repeated with many different distributions of pseudo-data, until the
fraction of more interesting pseudo-data distributions (which is @xmath
) is determined with 5% relative uncertainty.

In each pseudo-data distribution that is produced, the population of
pseudo-data is randomly distributed according to a Poisson distribution,
whose mean is the Standard Model predicted total population ( @xmath )
for the final state.

Each examined @xmath tail has a @xmath that is not taking into account
the statistical uncertainty in the background ( @xmath ) contained in
the tail. The same is true for both data and pseudo-data, therefore the
effect in the final @xmath is negligible.

Regardless of the particular shape of an expected @xmath distribution,
@xmath in pseudo-data follows the same distribution. Therefore, @xmath
depends only on the @xmath observed in data, and on the overall expected
population; the larger the population, the bigger the average number of
considered @xmath tails in pseudo-data, therefore the larger the @xmath
. The dependence of @xmath on @xmath and @xmath is shown in Fig. A.10 .
The advantage of having tabulated this dependence, is that then one does
not have to produce pseudo-data repeatedly to estimate @xmath ; he can
simply read it from Fig. A.10 , for a given @xmath and @xmath . This
technique makes the execution of Sleuth incredibly fast, allowing for
studies such as sensitivity tests, projections to different luminosity,
propagation of systematic uncertainties to @xmath , and frequent
assessment of the @xmath excesses in data.

## Appendix B Correction Model Details, reflecting the 2 fb@xmath
analysis

### b.1 Details on Event Selection

Although specific online triggers are not explicitly required, it is
still possible to identify the primary online triggers which feed this
analysis. These are:

-    electron_central_18

-    muon_central_18

-    photon_25_iso

-    jet20

-    jet100

-   susy dilepton triggers: electron_central_8_&_track8 cem4_cmup4
    cem4_cmx4 cem4_pem8 cmup4_pem8 cmx4_pem8 dielectron_central_4
    dimuon_cmup4_cmx4 dimuon_cmupcmup4

-   susy dilepton triggers muon_cmup8_&_track8 and muon_cmx8_&_track8
    (introduced in run number 200274, roughly 600 pb @xmath into Run II)

-   hadronic ditau trigger (introduced roughly 300 pb @xmath into
    Run II)

The following datasets were used:

-   HighPt Central Electron stream: bhel0d, bhel0h, bhel0i, bhel0j

-   HighPt CMUP and CMX muon stream: bhmu0d, bhmu0h, bhmu0i, bhmu0j

-   HighPt Photon stream: cph10d, cph10h, cph10i, cph10j

-   SUSY dilepton stream: edil0d, edil0h, edil0i, edil0j

-   Ditau stream: etau0d, etau0h, etau0i, etau0j

-   Jet20 stream: gjt10d, gjt10h, gjt10i, gjt10j

-   Jet100 stream: gjt40d, gjt40h, gjt40i, gjt40j

### b.2 Details on Particle Identification

This section contains tables of information related to particle
identification. Electron identification is described in Tables B.1 and
B.2 ; muon identification in Tables B.3 , B.4 , B.5 , and B.6 ; tau
identification in Table B.7 ; and photon identification in Tables B.8
and B.9 . Standard fiducial criteria apply. Standard CDF SecVtx
algorithm is used to identify @xmath -jets.

Jets are identified using the JetClu [ 49 ] clustering algorithm with
cone size @xmath , unless the event contains one or more jets with
@xmath GeV and no leptons or photons, in which case cones of @xmath are
used.  Jets with @xmath GeV are required to have at least 5 GeV of track
@xmath within the cone.

### b.3 Vista: Single Particle Gun Results

Tables B.10 and B.11 show the response of the CDF detector simulation,
reconstruction, and particle identification algorithms to single
particles in the central and plug regions respectively, with all changes
to particle identification criteria discussed in section 4.2.1 . We use
a single particle gun to shoot @xmath particles of each type, with
@xmath GeV, uniformly distributed in @xmath and @xmath . The types of
generated particles label the rows, while the resulting reconstructed
objects label the columns of each table. Table B.12 shows a similar
study with @xmath particles at @xmath GeV. These results are not
directly used in the analysis, but provide a sensible cross-check for
the used fake rates and identification efficiencies.

It should be noted that the number of photons reconstructed as electrons
decreased compared to the last round of this analysis. As expected, the
number of electrons which were identified with the wrong charge has
decreased proportionately, as well as the number of @xmath reconstructed
as electrons. All these are results of making the conversion filter
tighter, by removing the lower @xmath threshold that was previously
required when looking for sibling tracks coming from conversion.

Figures B.1 and B.2 show the @xmath distributions of the reconstructed
object (column label), resulting from the initial particle (row label),
for the central and plug region of the detector respectively. We note
that the @xmath resolution of reconstructed @xmath s has worsened,
consistently with obtaining @xmath from calorimeter @xmath rather than
visible momentum.

### b.4 Fake Rates

It would take too many Monte Carlo events to acquire enough statistics
of rare fake processes. To overcome this difficulty, we apply our own
multiplicative fake rates on reconstructed objects, when they are
reconstructed more often than the objects thay may fake. Specifically,
we apply fake rates for jets or b-tagged jets faking electrons, muons,
photons, @xmath s, jets faking b-tagged jets, and photons faking
electrons. Note that other fake processes are not neglected – they are
handled by CDFSim. In the interest of simplicity, we try to keep our
fake rates as simple as possible. There is generally one overall
coefficient for the fake rate, and this value is usually obtained from
the Vista fit to the data. In some cases however, to better model the
true fake process, we need to introduce additional modulations as a
function of @xmath or location within the detector ( @xmath or @xmath ).
This section details all the special modulations applied for Vista fake
rates. Generally, we show a modulating function, which multiplies the
appropriate correction factor value to obtain the true fake rate
applied. If not shown here, the fake rate is treated as being constant.

Figures B.4 and B.5 show the relative fake rate for jets to fake
electrons as a function of @xmath and @xmath . These functions of @xmath
and @xmath are multiplied by overall correction factors which represent
a crude average fake rate over the appropriate region. These shaped
functions are meant to model more fine details in fake rates than the
overall average can contain. In addition to @xmath and @xmath
dependence, for plug electrons there is a dependance on the @xmath ,
shown in Figure B.3 . Figures B.6 , B.7 , and B.8 show the electron
@xmath , electron @xmath and @xmath distribution from data in the 1e+1j
final state, where almost all events come from QCD dijet production
where one of the jets fakes an electron. This serves as the dominant
control region for determining variations in jet to electron fake rate.

Figures B.9 and B.10 show the fake rate variation for jets to fake muons
as a function of @xmath and @xmath . The fake rate is higher in CMX than
in CMU and CMP. The muon @xmath , @xmath , and @xmath distributions in
the 1j1mu+ final state are shown in Fig. B.11 , B.12 , and B.13 . These
serve as the dominant control regions determining these fake rates.

Figures B.14 , B.15 , and B.16 show the jet to photon fake rates as
functions of @xmath , @xmath , and @xmath . Detector geometry features
are analogous to those exhibited in the jet to electron fake rate. The
photon @xmath , @xmath , and @xmath distributions in the 1j1ph final
state are shown in Fig. B.17 , B.18 , and B.19 . This is one of the
dominant control regions determining the jet to photon fake rates.
Unlike the previous two cases, this final state is dominated by real
@xmath +jet production, rather than the fake process, which contributes
about 35% to this final state.

The variation in jet faking b-jet rate is shown in B.20 , as a function
of @xmath . This shape is consistent with the one measured by the
b-tagging group. Before comparing absolute values, however, it should be
noted that this Vista fake rate includes contributions from charm quarks
to fake b, which is not usually included in the b-tagging mistag rate.
When we accounted for the expected relative contribution of charmed
quarks in our ’denominator jets’, we found values consistent with the
mistag rates. The b jet @xmath distribution is shown in Fig. B.21 and
B.22 , for the 1b1j high @xmath and 1b1j low @xmath final states. These
are the dominant control regions determining the mistag rates.

The jet to @xmath relative fake rate is given in Fig. B.23 . This shape
is then multiplied by the function @xmath and the jet to @xmath fake
rate correction factor to obtain the final fake rate. The shape is
consistent with previous studies of the jet to @xmath fake rate. The
@xmath @xmath distributions in the 1j1tau+ low- @xmath , 1j1tau+ high-
@xmath , and 1tau+1tau- final states are shown in Fig. B.24 , B.25 , and
B.26 . These serve as the dominant control regions determining the jet
to @xmath fake rate.

Figure B.27 shows the relative fake rate for photons to fake electrons
as a function of @xmath . Fig. B.28 and B.29 show the electron @xmath
and @xmath distributions in the 1e+1ph final state. This final state is
the dominant control region determining the photon to electron fake
rate. However, this underlying process does not contribute very much to
the background in this final state and, as a result, the photon to
electron fake rate is not as well constrained as other fake rates. Fig.
B.30 and B.31 show the photon @xmath and @xmath distributions in this
same final state. As a general comment, this final state is a
particularly good example of how well-modelled our fake backgrounds are,
since the background contributing to this final state is a mixture of
various different fake processes.

### b.5 Correction Factors

#### b.5.1 Comparison with first round

The correction factor values obtained in the second round (v02)
(corresponding to 2 fb @xmath ) are here compared with the correction
factor values obtained in the first round (v01) (corresponding to
927 pbb @xmath ). The numerical values can be found in Table B.13 ;
analysis of the changes is provided below.

###### 5001.

The integrated luminosity of the sample has of course increased with
respect to v01. The present integrated luminosity obtained from the fit
is again consistent with the luminosity obtained from the CLC
measurement.

###### 5102.

This cosmic photon “ @xmath -factor” has increased due to requiring that
this background satisfies the same good run list that are required for
the data and by requiring that these events contain at least one
reconstructed photon. As a result the number of events in this
background has been decreased prompting this @xmath -factor to increase
accordingly.

###### 5103.

This cosmic jet “ @xmath -factor” has decreased due to the cut on the
second jet in jet final states, as described in Sec. 4.2.3 . The cut
removes events in which the leading jet is due to a cosmic ray, and the
other jets are due to the underlying event. As a result of this removal,
the kfactor for this background has been reduced.

###### 5121--5132.

The @xmath -factors for photon + jet production and diphoton production
is consistent with values obtained in v01.

###### 5151--5153.

The @xmath -factors for Z + jet production is consistent with values
obtained in v01.

###### 5141--5144.

Motivated by a mistake in the modelling of the inoperational period of
the keystone and miniskirt portions of the muon detector, we switched
from the MadEvent W+jets Monte Carlo sample to the standard Top Group
Alpgen W+jets sample. These @xmath -factors were changed to correspond
to Alpgen cross sections.

###### 5161--5169.

In v01 of this analysis we used @xmath , despite the fact that @xmath .
It is logically more consistent to chose @xmath , so this is what is
done in v02. The result of this modification is that @xmath -factors for
processes with one or more jets have increased.

###### 5170,5171.

These two @xmath -factors for heavy flavor multijet production have been
introduced.

###### 5211,5212.

The central electron identification efficiency is consistent with value
obtained in v01. The phoenix electron identification efficiency scale
factor has changed reflecting our change to the phoenix electron
identification criteria.

###### 5213.

The muon identification efficiency scale factor has changed due to our
change to the muon identification criteria, and the correction to the
modelling of the inoperational period of the keystone and miniskirt
portions of the muon detector.

###### 5216,5217,5219.

The identification efficiencies @xmath in the central and plug regions,
and @xmath in the central region are consistent with values obtained in
v01.

###### 5245.

The fake rate @xmath has been removed after the change to the plug
electron and photon identification. It was found to be unnecessary. This
vanished correction factor is not listed in Table B.13 .

###### 5246.

The fake rate @xmath in the plug has been promoted to a correction
factor from a fixed value of 0.005. This value increased significantly
due to a redefinition of plug photons into electrons in the 1e+1ph final
state. This was motivated by the fact that this plug photon was much
more likely to have been an electron. We have removed this renaming
procedure for the current version of the analysis.

###### 5256,5257.

The fake rates @xmath in the central and plug regions have decreased by
roughly 13% and 6%, respectively, due to our improved conversion
removal. In v01 we required a candidate conversion track to have @xmath
GeV; in v02 we make no transverse momentum requirement on the candidate
converstion track. The change to the fake rate in the plug region is
also affected by our change to the phoenix electron identification.

###### 5261.

The fake rate @xmath is consistent with the value obtained in v01.

###### 5273.

The fake rate @xmath is consistent with the value obtained in v01.

###### 5285.

A different @xmath dependence has been imposed for the fake rate @xmath
in v02 than applied in v01 and a dependence on the generated sumPt has
also been applied. As a result of not being careful about proper
normalizations of those functions, this number is not directly
comparable to the one from v01.

###### 5292.

The value obtained for the fake rate @xmath in the central region is
consistent with the value obtained in v01.

###### 5293.

The fake rate @xmath in the plug has decreased to due our correction to
the plug photon identification criteria.

###### 5401.

The central electron trigger efficiency has been found to increase to
unity in the current version of the analysis, because we now allow an
event to pass on any online trigger. As a consequence, it is no longer
appropriate to constrain this trigger efficiency to the Joint Physics
value for the CEM trigger. We now simply fix the central electron
trigger efficiency to @xmath and it is no longer a correction factor.
This vanished correction factor is not listed in Table B.13 .

###### 5402.

The plug electron trigger efficiency is consistent with the value from
v01.

###### 5403.

We have combined the CMUP and CMX trigger efficiencies due to the fact
that they were very close to each other from v01 of the analysis. The
value in v02 of the analysis is consistent with the values from v01.

## Appendix C Risk of Being Ad Hoc

### c.1 Introduction

Here follows a general discussion, not so much about the actual SM
implementation in this analysis, but about concerns such as bias, not
being “blind”, and how these factors affect the meaning of a null
result.

In a search for new physics, especially a model-independent one, it is
necessary to construct the Standard Model (SM) prediction. Then, one can
test whether the data ( @xmath ) are consistent with it.

By definition, the data follow the true law of nature. Denote the true
theory by @xmath . If there is physics beyond the SM, then @xmath . If
new physics is to be observed, the p.d.f. of at least one observable
quantity needs to differ adequately from that predicted by the SM.

Having the data events distributed according to @xmath , one has the
freedom to test their consistency with any conceivable theory. However,
what is really interesting, is how well the data agrees with the SM,
rather than some arbitrary model, not necessarily well motivated. We
could, for example, construct a model agreeing bin by bin with the data.
Imagine for instance having a dedicated @xmath -factor ¹ ¹ 1 @xmath
-factors are corrections to the cross sections of processes. Typically,
cross sections are calculated to leading-order, or
next-to-leading-order, and rarely to an even higher order. @xmath
-factors are meant to correct such approximate calculations to the
infinite-order cross section, which is incalculable, therefore @xmath
-factors are inferred from the data. per final state; then we would be
able to adjust this elastic pseudo-theory to match any combination of
populations across final states. That data-obeying model would be
consistent with @xmath . Then, by construction, testing the quality of
the fit would confirm the null hypothesis, namely that data agree with
the constructed model. The hypothesis test itself would be perfectly
legitimate, and its outcome would be correct, yet completely
uninteresting, since nobody is interested in that absurd model anyway.

The problem then begins with the realization that the truly interesting
hypothesis, the SM, is itself not known exactly; one needs information
about correction factors, such as fake rates, @xmath -factors,
efficiencies etc. Different values of such parameters result in
different “SM” predictions.

Let’s assume there are only two observable quantities, @xmath and @xmath
(Fig. C.1 ). For example, @xmath and @xmath could be the populations of
events in two final states. Depending on the values of some correction
factors (like @xmath -factors etc.), the prediction of the SM
implementation can be centered anywhere in some locus. In this case, the
allowed locus is represented by a one-dimensional solid line; in
general, the locus may be higher-dimensional.

The correction factors have some true values, which may be unknown. The
true Standard Model prediction is located at the “SM” point, which
corresponds to the true values of the correction factors. Ideally, that
is the SM we would like to compare to the data.

When the work to construct the SM prediction begins, one has no
adjustments made yet, which results in some prediction centered on, say,
point @xmath . One sees then the data ² ² 2 Whether he sees all, or
part, or only some aspect of them will be discussed later. , which are
by definition near point @xmath , and notices the discrepancies in
@xmath and @xmath . Since he has applied no corrections yet, he can not
be confident that the current prediction is the real SM. The SM has been
successful so far, therefore to rule it out one needs convincing
evidence. To be convincing, he needs to be conservative; he must exploit
any source of systematic uncertainty that he can identify in order to
correct the prediction in a direction that brings it closer to the data.
Unfortunately, there is no prescription how to do that correctly.

There are some obvious sources of uncertainty: @xmath -factors
reflecting the fact that it is not possible to calculate the
infinite-order cross section of SM processes, uncertainties in the exact
probability by which a particle may be misidentified, uncertainty in the
integrated luminosity etc. For specific discrepancies that are not
accounted for by such obvious uncertainties, one needs to become more
imaginative to identify what may be causing them, but it is important to
not invent false corrections. It requires judgment to make well
motivated adjustments instead of ad hoc corrections that hide the signal
of potentially new physics. The locus, represented by the solid line in
Fig. C.1 , is meant to represent the possible predictions that can be
derived by making well motivated corrections, whereas points out of the
locus represent the results of poorly motivated corrections.

Suppose that throughout the process one makes well motivated
corrections. Then his prediction should drift along the locus from point
@xmath to point @xmath , which gives the best agreement with the
observed data in @xmath and @xmath simultaneously. Even though @xmath ,
he will need to stop at @xmath and not proceed towards the actual SM
point. That is because he has no way to know if he has reached the
actual SM to stop there; his only guidance is the data and his judgment.
To be conservative, he would have to bring the prediction as close to
point @xmath as allowed, but not closer – that is point @xmath . The
wrong thing to do would be to introduce extraneous, poorly motivated
corrections that would drive one from @xmath to a prediction like @xmath
, namely out of the locus. That would be the result of ad hoc treatment
of discrepancies, which in its extreme limit would result in a model as
uninteresting as the data-obeying model mentioned earlier.

What can safeguard one from constructing the prediction of some poorly
motivated model? Only prudence and an over-constrained system that
limits systematic uncertainties, making it harder to deviate from the SM
locus. The risk of implementing an ad hoc model remains, unless all
systematic uncertainties shrunk to zero, in which ideal case the locus
would shrink into just the true SM point. However, there are some
“blind” approaches that, as will be argued, create the illusion of
safety against erring, or the sensation that information is generated
out of nothing, by using the data in “clever” ways, i.e. by not seeing
all of them at the same time.

### c.2 Blind to signal region

In some cases (not in this analysis) one may presume that the new
physics will be affecting @xmath but not @xmath . That is clearly an
assumption, which in many cases can be motivated. @xmath is then treated
as “signal region”, and @xmath as “control region”. Adjusting the
correction model to achieve maximal agreement with the data in @xmath is
legitimate, since the premise is that the SM should distribute @xmath as
@xmath does. That leads (if everything is done correctly) to a SM
implementation with p.d.f. centered on @xmath .

There is nothing wrong in defining control and signal regions. Clearly,
when interpreting the result of the comparison of the data with @xmath
one needs to remember that @xmath is not the globally best fitting model
(that would be @xmath ). Furthermore, @xmath is not necessarily the true
SM, but is the model that best fits the control region. Indicative of
the value of such results is the fact what is “signal” region in one
analysis can be “control” in another. Depending on what one defines as
“signal” and “control”, the result may vary from agreement to
disagreement with the data. Although these results can be valid, they
are convincing only if the initial premise is accepted.

Unfortunately, staying “blind” in @xmath does not guarantee that the
final model will not be an absurd and ad hoc one. For example, a human
error may lead one from @xmath to @xmath or @xmath . Apart from a human
error that may occur during the development of the correction model,
opening the box (e.g. looking at the measured @xmath ) often makes
people question the correctness of their implemented model, especially
in the event of a discrepancy with the data. In that phase of
reconsideration, one may even accidentally change his background model
from @xmath to @xmath , so the notion of “blindness” is questionable,
unless no discrepancy is seen. Therefore, as in the non-blind analysis
case, prudence and an over-constrained system that limits systematic
uncertainties, making it harder to deviate from the SM locus, can
prevent testing the goodness of a worthless model (like @xmath , @xmath
or @xmath ).

### c.3 Blind to part of the data

Another approach considered “blind” is to split the whole data set (
@xmath ) in two parts ( @xmath , @xmath ), assigning for example every
third event to @xmath and the rest to @xmath . Then, @xmath can be used
to develop the correction model, and @xmath is only revealed in the end,
to check how well it is fitted by the derived background model.

The supposed advantage of this approach is that @xmath is independent
from @xmath . So, if agreement is observed between @xmath and the
background model, that supposedly can not be due to a biased model, as
the background model was developed knowing nothing about @xmath . Though
psychologically reassuring, this impression of safety is false.

Obviously, all data come from the same distribution @xmath , therefore
there is no reason why @xmath would be distributed any differently than
@xmath , apart from random statistical fluctuations, which actually
become bigger when @xmath and @xmath have smaller populations.

If one makes wrong judgments in the way he uses @xmath , then there are
two possibilities: If one observes agreement between the background
model and @xmath , that only means that @xmath didn’t fluctuate too
differently than @xmath . On the other hand, if one observes
disagreement, that would only be due to (rare) statistical fluctuation
of @xmath with respect to @xmath . In other words, if one makes the
wrong use of @xmath the result is as uninformative as it would be if he
had used the whole @xmath in a wrong way.

Furthermore, even if one is very prudent and has an over-constrained
system with small systematic uncertainties, still splitting the data
makes the situation worse. Having less data in @xmath to constrain the
correction factors makes the locus where SM could be larger, therefore
it is more likely to end up with a correction model farther away from
the actual SM, simply due to larger systematic uncertainties.
Furthermore, having a smaller number of data in @xmath reduces
statistical power, making it harder to observe a real effect that may
appear in the measured @xmath and @xmath .

In summary, splitting @xmath in two does not secure one from
implementing wrongly his theoretical prediction. If one can make proper
use of @xmath , then he can also make proper use of the whole @xmath ,
which would offer the advantage of smaller uncertainties.

### c.4 Summary

To summarize, there is no way to be sure that the null hypothesis
compared to the data is the SM, rather than some other uninteresting
one. However, there is reason to hope that what was tested in this
analysis is the agreement of the data with a model that at least is
possible to be the SM, namely belongs to the SM locus determined by well
motivated systematic uncertainties. Certainly, the tested model is
biased to agree with the data more than the SM may actually agree ³ ³ 3
Think of the analogy given by points “SM” and @xmath in Fig. C.1 . ,
since the best fitting choice of correction parameters was made, but
that is inevitable, since the SM is assumed correct until proof of the
contrary. The hope that the implemented background model is not far from
the actual SM is based on the fact that the correction model is
significantly over-constrained by examining not just a couple of
observables, but thousands. After all, human errors are always possible,
but the best effort was made to eliminate them. Well motivated
corrections usually fix several problems at once, while mistaken
adjustments tend to fix one problem but cause other. Our global approach
allowed us to distinguish the former from the latter, by monitoring
simultaneously so many observables before and after the adjustments.

## Appendix D Nomenclature

-    Barrel Muon system. Often synonymous to IMU

-    Collider Detector at Fermilab

-    Charge Exchange Injection

-    Central Electromagnetic calorimeter

-    Conseil Européen pour la Recherche Nucléaire

-    Central Electromagnetic Showermax detector

-    Central Hadronic calorimeter

-    Cabibbo Kobayashi Maskawa

-    Cerenkov Luminosity Counter

-    Central Muon Upgrade

-    A muon that has both CMU and a CMP hits

-    Central Muon Detector

-    Central Muon Extension

-    Central Outer Tracker

-    Central Preshower detector

-    Central Processor Unit

-    Charge Parity

-    Consumer Server Logger

-    Data Acquisition

-    Deep Inelastic Scattering

-    Electromagnetic

-    Event Builder

-    Electroweak

-    Electroweak Symmetry Breaking

-    Feynman Computing Center

-    First In First Out

-    Fermi National Accelerator Laboratory

-    Gauge Mediated Supersymmetry Breaking

-    Grand Unification Theory

-    Identification

-    Intermediate Muon system

-    Intermediate Silicon Layer

-    Kolmogorov Smirnov

-    Layer 0 of the Silicon Detector

-    Large Hadron Collider

-    leading order

-    Monte Carlo

-    Missing Transverse Energy

-    Minimum Ionizing Particle

-    Main Injector

-    Parton Distribution Function

-    Probability Density Function

-    Plug Electromagnetic calorimeter

-    Plug Electromagnetic Showermax detector

-    Plug Hadronic calorimeter

-    “Phoenix”, referring to forward tracks reconstructed from silicon
    hits

-    Pontecorvo Maki Nakagawa Sakata

-    Photomultiplier

-    Quantum Chromodynamics

-    Radio Frequency

-    Scanner CPU

-    Standard Model of elementary particles

-    Supergravity

-    Supersymmetry

-    Silicon Vertex system

-    Silicon Vertex Detector

-    Trigger Supervisor

-    Ultraviolet

-    Virtual Machine Environment, a standard mainframe operating system

-    VME Readout Buffer (or Board)

-    Endwall Hadronic calorimeter

-    Wavelength Shifting (optic fiber)

-    Extrapolation to Central Electromagnetic Showermax

-    Extremely Fast Tracker

-    Extrapolation Unit