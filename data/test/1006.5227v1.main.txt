##### Acknowledgements. I am greatly indebted to my supervisor Aram
Harrow for his teaching, support and inspiration throughout my PhD and
for this I greatly thank him. I would also like to thank the whole
quantum information group at Bristol and members of the Computer Science
department for their support and help. In particular, I thank Mick
Bremner, Raphaël Clifford, Toby Cubitt, Richard Jozsa, Will Matthews,
Ashley Montanaro, Ben Sach, Dan Shepherd and Andreas Winter. I also
thank my family and friends for their encouragement and interest and
members of the Hornstars, Dr Doctor, Cattle Market and Les Rosbifs for
welcome distraction from this thesis. I also acknowledge funding from
the ARO grant ASTQIT and the EPSRC grant QIP-IRC.

###### Contents

-    1 Brief Introduction to Quantum Mechanics
-    2 Preliminaries
    -    2.1 Pauli Matrices
    -    2.2 The Symmetric Group and Permutation Operators
    -    2.3 Asymptotic Notation
    -    2.4 Norms and Superoperator Norms
        -    2.4.1 Norms
        -    2.4.2 Superoperator Norms
-    3 Previous Publications
-    I Quantum Pseudo-randomness
    -    4 Random Unitaries
    -    5 @xmath -designs
        -    5.1 @xmath -wise Independence
        -    5.2 Exact Designs
            -    5.2.1 State designs
            -    5.2.2 Unitary designs
        -    5.3 Approximate @xmath -designs
            -    5.3.1 Approximate state designs
            -    5.3.2 Approximate unitary designs
            -    5.3.3 Constructions
    -    6 Introduction: Pseudo-random Quantum Circuits
        -    6.1 Random Circuits
    -    7 Preliminaries
        -    7.1 Pauli expansion
        -    7.2 Random Circuits as @xmath -designs
    -    8 Summary of Results
    -    9 Analysis of the Moments
        -    9.1 @xmath
        -    9.2 @xmath
        -    9.3 Moments for General Universal Random Circuits
    -    10 Convergence
        -    10.1 First Moments Convergence
        -    10.2 Second Moments Convergence
        -    10.3 Markov Chain Analysis
            -    10.3.1 Decomposition
            -    10.3.2 log-Sobolev Constant
        -    10.4 Convergence Proof
    -    11 Tight Analysis for the @xmath Case
        -    11.1 Second Moments Convergence
        -    11.2 Markov Chain of Coefficients
        -    11.3 Proof of Theorem 3.6.4
        -    11.4 Proof of Theorem 3.6.5
    -    12 Main Result
    -    13 Conclusions
    -    14 Proofs
        -    14.1 Zero chain mixing time proofs
            -    14.1.1 Asymmetric Simple Random Walk
            -    14.1.2 Waiting Time
            -    14.1.3 Phase 1
            -    14.1.4 Phase 2
            -    14.1.5 Phase 3
        -    14.2 Moment Generating Function Calculations
        -    14.3 Mixing Times
    -    15 Introduction
        -    15.1 Quantum Expanders
        -    15.2 Main Result
    -    16 Proof of Theorem 4.1.5
        -    16.1 Proof overview
        -    16.2 Action of a Classical @xmath -TPE
            -    16.2.1 Turning Inequality Constraints into Equality
                Constraints.
        -    16.3 Fixed Points of a Quantum Expander
        -    16.4 Fourier Transform in the Matrix Element Basis
        -    16.5 Proof of Lemma 4.2.2
    -    17 Conclusions
    -    18 Review of Applications
        -    18.1 Quantum Cryptography
        -    18.2 Measurement
        -    18.3 Average Gate Fidelity
        -    18.4 Data Hiding
        -    18.5 Decoupling and Evolution of Black Holes
        -    18.6 Applications for Larger @xmath
    -    19 Derandomising Large Deviation Bounds
        -    19.1 Introductory Problem: Entanglement of a 2-design
        -    19.2 Main Results
        -    19.3 Optimality of Results
    -    20 Main Technique
        -    20.1 Step 1: Concentration for uniform randomness
        -    20.2 Step 2: A bound on the moments
        -    20.3 Step 3: A concentration bound for a @xmath -design
    -    21 Application 1: Entropy of a @xmath -design
    -    22 Application 2: @xmath -designs and Statistical Mechanics
    -    23 Application 3: Using @xmath -designs for Measurement-Based
        Quantum Computing
    -    24 Conclusions
    -    25 Proof of Theorem 5.2.3
-    II Quantum Learning
    -    26 Introduction
    -    27 The Pauli and Clifford Groups and the Gottesman-Chuang
        Hierarchy
    -    28 Learning Gottesman-Chuang Operations
        -    28.1 Learning Pauli Operations
        -    28.2 Learning Clifford Operations
        -    28.3 Learning Gottesman-Chuang Operations
    -    29 Learning Unitaries Close to @xmath Elements
    -    30 Clifford Testing
    -    31 Conclusions and Further Work
    -    32 Proof of Lemma 6.4.4
    -    33 Miscellaneous Lemmas

### Chapter \thechapter Introduction

Landauer famously said that information is physical [ Lan92 ] . A
corollary of this is that computation is a physical process. It is
simply the evolution of a physical state, governed by the laws of
physics. A classical computer is therefore an information processor
where the physical evolution is restricted to that of classical physics.
A quantum computer is more general: quantum evolution is allowed. One
might therefore reasonably expect that quantum computers are more
powerful. It might be that the extra possibilities allowed by quantum
evolution allow states to be processed more efficiently to speed up the
computation. Determining which problems a quantum computer can solve
faster than a classical computer is the central problem in the theory of
quantum computation.

Significant progress has already been made in answering this question.
Shor’s algorithm [ Sho94 ] shows that factoring of integers is possible
in polynomial time on a quantum computer. In contrast, it is not known
if factoring is possible in polynomial time on a classical computer.
Also, Grover’s unstructured search algorithm [ Gro97 ] allows a marked
item in an unsorted database to be found using only the square root of
the time required on a classical computer. Finding other algorithms and
provable separations between quantum and classical computation is an
important area of current research.

This thesis makes some progress towards finding such new algorithms. In
classical computer science, randomness and pseudo-randomness have been
key tools in the development of new and faster algorithms. In the first
part of this thesis, we discuss applications and constructions of
quantum analogues of these pseudo-random objects. Whilst we do not come
up with new algorithms based on these, we hope that in the future the
tools we build will find application in this area. In the second part of
this thesis, we discuss problems in the theory of machine learning,
which is an area in which quantum computers could outperform their
classical counterparts.

A side theme in this thesis is the idea that computational complexity
must be considered in physical models. The converse of our opening
statement is also true: physical systems store and process information;
they are computers. Therefore physical systems that could solve problems
that are provably difficult do not exist in nature. This can rule out
models that provide too much computational power.

In Part I , we discuss quantum pseudo-randomness. We introduce the
subject in Chapter I and provide motivation from the classical computer
science literature. The main idea is to use pseudo-randomness instead of
full randomness to decrease the amount of randomness required. This is
desirable in classical computing because random bits are expensive to
produce. In quantum computing random bits can be obtained by measurement
but uniformly random unitaries and states (formally defined in Section 4
) cannot be produced efficiently so pseudo-randomness is necessary if
efficiency is desired. In classical computing random bits are often
saved by limiting dependence, for example by using @xmath -wise
independent random variables. These are variables where the distribution
of any @xmath variables is the same as for fully independent random
variables but dependencies become apparent when observing more than
@xmath of the variables. We discuss a quantum analogue of this known as
a @xmath -design.

In Chapter I we show that short random quantum circuits (see Section 1
for background on quantum circuits) are 2-designs, giving an efficient
method for producing a 2-design. Then in Chapter I we provide an
efficient @xmath -design construction for all @xmath , giving the first
construction for @xmath . In order to do this, we present an efficient
construction of a quantum @xmath -tensor product expander, which is a
quantum analogue of a classical tensor product expander which in turn is
a generalisation of the standard expander used in classical computer
science. We then summarise known applications of @xmath -designs in
Chapter I as well as providing our own to show that @xmath -designs
exhibit measure concentration which in some cases is almost as strong as
for uniformly random unitaries.

In Part II we turn to problems in learning theory. In particular, we
consider the problem of identifying a given black box unitary with as
few queries as possible. We find an algorithm with optimal asymptotic
query complexity to identify an unknown unitary from the Clifford group
(defined in Chapter II ). We also show how this can be done if the
unitary only approximately implements a Clifford and we also present a
testing algorithm to determine if a given operation is close to a
Clifford or far from every Clifford.

#### 1 Brief Introduction to Quantum Mechanics

We now briefly mention some key concepts in quantum mechanics and define
some notation. For a more complete introduction see [ NC00 ] .

The state of a @xmath -dimensional quantum system is represented by a
vector in the complex space @xmath . If @xmath , we call the system a
qubit and often we will take @xmath and say the system has @xmath
qubits.

We will normally use Dirac notation for quantum states. We write column
vectors as @xmath , with the associated conjugate row vector as @xmath .
The inner product between two states is written as @xmath . We will
write @xmath for the projector @xmath . States can also be probabilistic
mixtures of pure states. If the state is @xmath with probability @xmath
then it has density matrix @xmath .

It is often convenient to break the space up into different components,
for example the system and its environment. Mathematically, systems are
combined by using the tensor product . The combined state of system
@xmath in state @xmath and system @xmath in state @xmath is written
@xmath . This leads to the phenomenon of entanglement , which is when
the combined state cannot be written in this product form. For example,
the state @xmath cannot be written in the product form @xmath and so is
entangled. To find the state of a subsystem @xmath from a density matrix
@xmath , we take the partial trace . Write @xmath . Then the reduced
state is @xmath .

Measurement of a quantum system can be written mathematically in terms
of a POVM (positive operator valued measure). This is a set of positive
semi-definite operators @xmath such that @xmath . Then the measurement
outcomes are the labels @xmath and outcome @xmath occurs with
probability @xmath if the state being measured is @xmath .

The evolution of a closed quantum system is unitary. That is, the
quantum state at a later time @xmath is related by a unitary to the
initial quantum state: @xmath . If the system of interest is part of
some larger system then the dynamics need not be unitary. The most
general form of evolution can be written in the Kraus decomposition:
@xmath where @xmath are any operators normalised so that @xmath .

Finally we mention that it is often convenient to think of unitary
evolution as a quantum circuit, built up of smaller elementary unitary
gates. This is in direct analogy to the use of circuits in classical
computing. Classically, a NAND gate suffices to produce any other gate
so all classical circuits can be made up of just NAND gates. Similarly,
there exist sets of unitary gates from which any unitary can be built.
An example is the following three gates:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

We often seek to build a family of circuits that act on @xmath qubits
for all @xmath , where the gates are chosen from some elementary set,
such as that above. We say that the circuits are efficient if the number
of gates grows only polynomially with @xmath .

#### 2 Preliminaries

Here we define some notation and concepts that are used throughout this
thesis.

##### 2.1 Pauli Matrices

We will often use the Pauli matrices:

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (2.1)
  -- -------- -------- -- -------

We can extend these to matrices on @xmath qubits by taking tensor
products. Let @xmath and @xmath where @xmath is the value at the @xmath
position in the string @xmath . We will sometimes use the alternative
notation of @xmath . We will refer to @xmath as Pauli matrices on @xmath
qubits. There are @xmath Pauli matrices and they are orthogonal i.e.
@xmath . Note also that @xmath , the identity. Also, Pauli matrices
either commute or anticommute.

The Pauli matrices form an orthogonal basis for matrices in @xmath .
Therefore any such matrix @xmath can be written in the form @xmath ,
with @xmath . Sometimes we will choose a different normalisation for the
Pauli coefficients @xmath but will make this clear from the context.

##### 2.2 The Symmetric Group and Permutation Operators

The symmetric group is the group of all permutations. Let @xmath be the
symmetric group on @xmath objects. Then for @xmath define the
corresponding permutation operator

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

to be the matrix that permutes the basis states @xmath according to
@xmath .

On the other hand, if we have @xmath @xmath -dimensional systems then
for @xmath define the subsystem permutation operator @xmath by

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

Now we present two useful lemmas about subsystem permutation operators.

###### Lemma 2.1.

Let @xmath be a cycle of length @xmath in @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

since @xmath . Evaluate the sum using the resolution of the identity to
get the result. ∎

A simple example of this Lemma is that

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where @xmath is the swap operator.

We also work out the Pauli expansion of the swap operator. To stress
that this result does not depend on the choice of orthogonal basis we
prove it in full generality.

###### Lemma 2.2.

The swap operator @xmath on two @xmath -dimensional systems can be
written as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath form a Hermitian orthogonal basis with @xmath .

###### Proof.

Expand @xmath in the basis and use Lemma 2.1 :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The given sum has the correct coefficients in the basis therefore @xmath
. ∎

##### 2.3 Asymptotic Notation

We will use the following standard asymptotic notation.

###### Definition 2.3.

@xmath if there exists @xmath such that @xmath for all @xmath .

###### Definition 2.4.

@xmath if there exists @xmath such that @xmath for all @xmath .

###### Definition 2.5.

@xmath if @xmath and @xmath .

###### Definition 2.6.

@xmath if @xmath .

##### 2.4 Norms and Superoperator Norms

###### 2.4.1 Norms

We will make heavy use of Schatten @xmath -norms:

###### Definition 2.7.

For @xmath a @xmath matrix, the Schatten @xmath -norm is given by

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where @xmath are the singular values of @xmath .

In particular, @xmath , @xmath and @xmath .

These norms satisfy the following simple relationships:

  -- -------- -------- -- -------
     @xmath   @xmath      (2.6)
     @xmath   @xmath      (2.7)
     @xmath   @xmath      (2.8)
  -- -------- -------- -- -------

###### 2.4.2 Superoperator Norms

Just as state norms can be used to bound the distinguishability of
states, superoperator norms bound how easy it is to tell different
superoperators apart. We start with the 1-norm:

###### Definition 2.8.

The 1-norm of a superoperator @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

The main problem with this definition is that the 1-norm is not stable
under tensoring with the identity i.e. there exist channels with @xmath
, where @xmath is the identity channel on @xmath dimensions. This means
that some channels are easier to distinguish by inputting entangled
states. If the norm is to measure the distinguishability of channels it
should take this into account. To overcome this problem, the diamond
norm is defined:

###### Definition 2.9 ([Ksv02]).

The diamond norm of a superoperator @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

If follows immediately that @xmath . Also it is shown in [ KSV02 ] that
the diamond norm satisfies @xmath for all channels @xmath and dimensions
@xmath and that the dimension @xmath in the supremum can be taken to be
the same as the dimension of the system @xmath acts on. Operationally,
the diamond norm of the difference between two quantum operations tells
us the largest possible probability of distinguishing the two operations
if we are allowed to have them act on part of an arbitrary, possibly
entangled, state.

We will also use the 2-norm:

###### Definition 2.10.

The 2-norm of a superoperator @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

In [ van02 ] Appendix C, the following relationships between the
superoperator norms are proven:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.9)
     @xmath   @xmath      (2.10)
     @xmath   @xmath      (2.11)
     @xmath   @xmath      (2.12)
  -- -------- -------- -- --------

#### 3 Previous Publications

The majority of this thesis has been published previously and some is
work in collaboration.

Chapter I is joint work with Aram Harrow and is available as “Random
Quantum Circuits are Approximate 2-designs”, Communications in
Mathematical Physics, Volume 291, Number 1, Pages 257-302. It is also
available as a pre-print: arXiv:0802.1919.

Chapter I is also joint work with Aram Harrow and is available as
“Efficient Quantum Tensor Product Expanders and @xmath -Designs”,
Proceedings of RANDOM 2009, LNCS, Volume 5687, Pages 548-561. It is also
available as a pre-print: arXiv:0811.2597.

Chapter I from Section 19 onwards is available as “Large deviation
bounds for @xmath -designs”, Proceedings of the Royal Society A, Volume
465, Number 2111, Pages 3289-3308. It is also available as a pre-print:
arXiv:0903.5236.

Chapter II is available as “Learning and Testing Algorithms for the
Clifford Group”, Physical Review A, Volume 80, Number 5, Page 052314. It
is also available as a pre-print: arXiv:0907.2833.

## Part I Quantum Pseudo-randomness

### Chapter \thechapter Introduction to Quantum Pseudo-randomness

Randomness is an important resource in both classical and quantum
computing. It has applications in virtually all areas of computer
science, including algorithms, cryptography and networking. Randomness
can improve efficiency or, as in the case of cryptography, allow us to
perform tasks that we would not be able to do with deterministic
resources.

An example of an algorithm where a randomised algorithm is faster than
any known deterministic algorithm is polynomial identity testing. Here,
the task is to determine if two polynomials are identically equal. By
evaluating the polynomials on random inputs, identity testing can be
done in polynomial time whereas no polynomial time deterministic
algorithm is known.

Also, a commonly used randomised algorithm is that of randomised
quicksort. In quicksort, a pivot element is chosen and elements smaller
than this are placed to the left and larger elements to the right. Then
these two parts are sorted recursively. However, the choice of pivot
element greatly affects the run-time of the algorithm. If chosen poorly
(for example so that there is only one element smaller than the pivot),
the algorithm runs in @xmath time. If chosen well, the algorithm runs in
@xmath time. Choosing the pivot element randomly will be a good choice
on average, giving expected run-time @xmath [ MR95 ] . However, this
run-time can be achieved deterministically using deterministic median
finding [ BFP @xmath 72 ] but in practice the randomised method is more
efficient.

As another example, many primality testing algorithms are randomised
because of their simplicity, even though a deterministic polynomial-time
algorithm is now known. Also, in the field of communication complexity,
separations between deterministic and randomised algorithms can be
proven. The deterministic complexity of evaluating the equality function
(to determine if Alice and Bob’s strings are equal) is @xmath , whereas
the randomised complexity is @xmath [ KN96 ] . As yet another example of
randomness in classical computer science, in networking a random delay
is often inserted after a collision so the nodes wait different times so
are likely to avoid another collision.

In this part, we seek to extend some of these gains of using randomness
to quantum computing. We wish to find applications of randomness to find
new quantum algorithms and constructions.

Besides the computer science applications, there are also physical
reasons for studying randomness in quantum mechanics. Some systems can
be modelled as interacting randomly and it is interesting to ask what
the limiting state (or distribution on states) is for such a system.
Also of great interest is how quickly the system reaches this stationary
state. If the time taken grows too quickly with the size of the system
(for example, exponentially) then for any system apart from the most
trivial, the stationary state will never be reached and will not be seen
in physical systems. However, if the time is small (for example, a small
polynomial), then the stationary state can be reached quickly and will
be observed in real systems. It is in problems like this that physicists
must consider the computer science aspects of their models. We study
problems of this kind in Chapters I and I .

#### 4 Random Unitaries

In quantum computing, operations are unitary gates and randomness is
often used in the form of random unitary operations. Random unitaries
have algorithmic uses (e.g. [ Sen05 ] ), cryptographic applications
(e.g. [ AS04 , HLSW04 ] ) and applications to fundamental quantum
protocols (e.g. [ BHL @xmath 05 , HHL04 ] ). For information-theoretic
applications, it is often convenient to use unitary matrices drawn from
the uniform distribution on the unitary group, also known as the Haar
measure. This measure is the unique unitarily invariant measure i.e. the
only measure @xmath on the unitary group @xmath where @xmath for all
functions @xmath and unitaries @xmath . For random states, we write the
unitarily invariant measure on @xmath -dimensional states as @xmath .
This can be thought of as a Haar distributed unitary applied to any
fixed pure state. It is also known as the Fubini-Study metric.

However, in both classical and quantum computing, obtaining random bits
is often expensive, and so it is often desirable to minimise their use.
For example, in classical computing, expanders (discussed in Chapter I )
and @xmath -wise independent functions (see Section 5 ) have been
developed for this purpose and have found wide application. We will
spend a great deal of time exploring quantum analogues of these: quantum
expanders and @xmath -designs.

In addition to randomness being expensive, there is an even more
pressing problem when using random unitaries and states. An @xmath
-qubit unitary is defined by @xmath real parameters, and so cannot even
be approximated efficiently using a subexponential amount of time or
randomness. So any application that requires a random unitary cannot be
efficient. Instead, we will seek to construct efficient pseudo-random
ensembles of unitaries which resemble the Haar measure for certain
applications. For example, a @xmath -design (often referred to as a
@xmath -design, or a @xmath -design), as mentioned above, is a
distribution on unitaries which matches the first @xmath moments of the
Haar distribution. @xmath -designs have found many uses which are
explored in Chapter I .

In Section 5 , we formally define @xmath -designs and summarise known
constructions. Then in Chapter I we show that, for a natural model of a
random quantum circuit, the distribution quickly converges to that of a
2-design. This gives an efficient approximate 2-design construction and
also has physical applications. In Chapter I , we provide an efficient
construction of a unitary @xmath -design for any @xmath (although there
are restrictions on the dimension, see later). Then in Chapter I , we
discuss applications of designs, including to derandomising
constructions that use large deviation bounds.

Parts of this chapter have been published previously in [ HL09b , HL09a
, Low09a ] and parts are joint work with Aram Harrow.

#### 5 @xmath-designs

A unitary @xmath -design is a distribution of unitaries that gives the
same expectations of polynomials of degree at most @xmath as the Haar
measure. This is just like Gaussian quadrature, where integrals of
polynomials are calculated by sums. Gaussian quadrature says that there
exist sample points @xmath and weights @xmath so that for all
polynomials @xmath of degree at most @xmath ,

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

for some fixed limits @xmath and @xmath . This allows the integrals to
be calculated much more efficiently. A unitary @xmath -design is the
same, except the polynomial is on elements of unitary matrices from the
unitary group rather than numbers on the real line. The @xmath refers to
the degree of the polynomial. We will also discuss state designs, where
the function is on coefficients of states rather than unitaries.

##### 5.1 @xmath-wise Independence

@xmath -designs can also be thought of as a quantum analogue of @xmath
-wise independence. A sequence of random variables @xmath is @xmath
-wise independent if, for any subset of size @xmath ,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

As a simple example of how this can save randomness, consider the set

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

If an element is chosen uniformly at random from this set, the
probability distribution of the values of any two bits is the same as if
all three bits were chosen independently. This is therefore a 2-wise
independent set, and saves one bit of randomness. In general, if @xmath
, an exponential saving in randomness can be made in this way. Efficient
constructions of exactly @xmath -wise independent sets are known [ ABI86
] and more efficient approximate constructions are given in [ NN90 ] .

A related concept is that of @xmath -wise independent permutations.
These are sets of permutations with the property that, when a
permutation is chosen randomly from this set and applied to @xmath
points, the distribution of the positions of any @xmath points is the
same as if a uniformly random permutation was applied. For example, a
random cyclic shift is a 1-wise independent permutation. Again, an
exponential saving of randomness is possible [ KNR09 ] .

We seek to construct quantum @xmath -designs to achieve a similar saving
of randomness for quantum algorithms. We now formally define @xmath
-designs.

##### 5.2 Exact Designs

We will use the following notation to distinguish the measure we are
using. Write @xmath for the expectation with @xmath meaning the
expectation when @xmath is chosen from the measure @xmath . If the
measure is the Haar measure in dimension @xmath we will write @xmath .
We use the same subscripts for probabilities so @xmath denotes the
probability when @xmath is chosen from the Haar measure, etc.. When
considering random states, we will write @xmath , etc..

###### 5.2.1 State designs

A @xmath -design is an ensemble of states such that, when one state is
chosen from the ensemble and copied @xmath times, it is
indistinguishable from a uniformly random state. The state @xmath
-design definition we use is due to Ambainis and Emerson [ AE07 ] :

###### Definition 5.1 ([Ae07], Definition 1).

An ensemble of quantum states @xmath is a state @xmath -design if

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

We can evaluate the integral on the right hand side:

###### Lemma 5.2.

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

where @xmath is the projector onto the symmetric subspace of @xmath
@xmath -dimensional spaces.

###### Proof.

The standard proof (see e.g. [ GW98 ] or [ BBD @xmath 97 ] ) involves
showing that @xmath commutes with all elements of an irreducible
representation (irrep) of the unitary group that acts on the symmetric
subspace so by Schur’s lemma must be proportional to the projector onto
the symmetric subspace. However, here we give an alternative proof that
introduces a technique we will use later.

By the unitary invariance of the Haar measure, @xmath commutes with
@xmath for all unitaries @xmath . By Schur-Weyl duality (see e.g. [ GW98
] ), this implies that the integral is a linear combination of subsystem
permutation operators. Therefore we have

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

However, the integral is invariant under permutations so @xmath must be
the same for all permutations @xmath . Using @xmath and finding the
normalisation by taking the trace (the dimension of the symmetric
subspace is @xmath ) proves the result. ∎

We will also state equivalent definitions of designs in terms of
polynomials of matrix elements of the unitary or coefficients of the
state. First we must define what we mean by the degree of a polynomial:

###### Definition 5.3.

A monomial in elements of a matrix @xmath or state @xmath is of degree
@xmath if it contains @xmath conjugated elements and @xmath unconjugated
elements. We call it balanced if @xmath and will simply say a balanced
monomial has degree @xmath if it is degree @xmath . A balanced
polynomial is of degree @xmath if it is a sum of balanced monomials of
degree at most @xmath , with at least one monomial with degree equal to
@xmath .

So that, in this definition, @xmath is a balanced monomial of degree
@xmath and @xmath is a monomial of degree @xmath and is unbalanced. For
the state @xmath , @xmath is a balanced monomial of degree @xmath .

We can then define state @xmath -designs in terms of monomials:

###### Definition 5.4 ([Ae07], Definition 3).

An ensemble of quantum states @xmath is a state @xmath -design if, for
all balanced monomials @xmath of degree at most @xmath ,

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

This is an equivalent definition to Definition 5.1 :

###### Lemma 5.5 ([Ae07], Theorem 5).

The state design definitions 5.1 and 5.4 are equivalent.

###### Proof.

Firstly, we only need to prove the result for @xmath of degree exactly
@xmath , since by partial tracing this implies the result for any
smaller @xmath .

Each entry in the matrix @xmath is the expectation of a monomial of
degree @xmath , with the state chosen from the design. Further, the
corresponding entry in @xmath is the expectation of the same monomial
but with the state chosen from the Haar measure. If the ensemble of
states satisfies Definition 5.4 then these are equal, so the ensemble
also satisfies Definition 5.1 .

On the other hand, for every balanced monomial of degree @xmath , there
is an entry in @xmath equal to its expectation. Therefore, if the
ensemble of states satisfies Definition 5.1 then it also satisfies
Definition 5.4 . ∎

###### 5.2.2 Unitary designs

Consider having @xmath @xmath -dimensional systems in any initial state.
A unitary @xmath -design is an ensemble of unitaries such that when a
unitary is randomly selected from it and applied to each of the @xmath
systems, the overall state is indistinguishable from choosing a
uniformly random unitary. This can be seen as a generalisation of state
designs in that any column of a unitary @xmath -design is a state @xmath
-design. Formally, we have:

###### Definition 5.6.

Let @xmath be an ensemble of unitary operators. Define

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

Then the ensemble is a unitary @xmath -design if @xmath for all @xmath
matrices @xmath (not necessarily physical states).

For convenience we have defined this for all matrices @xmath although it
is equivalent to only require equality for physical states, since all
matrices can be obtained from linear combinations of physical states.

Like state designs, unitary designs can also be defined in terms of
polynomials:

###### Definition 5.7 ([Dcel06]).

@xmath is a unitary @xmath -design if, for all balanced monomials @xmath
of degree @xmath ,

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

Again, these definitions are equivalent:

###### Lemma 5.8.

The unitary design definitions 5.6 and 5.7 are equivalent.

###### Proof.

The proof is very similar to the state design case. Again, we only
consider monomials of degree @xmath since by partial tracing this
implies the result for smaller @xmath .

Consider matrices @xmath of the form @xmath in Definition 5.6 . Then
each element of @xmath is a balanced monomial of degree @xmath and, for
some choice of indices in @xmath , each balanced monomial of degree
@xmath appears. ∎

##### 5.3 Approximate @xmath-designs

While exact designs have desirable properties, it is often much easier
to construct approximate designs which, for many applications, are
sufficient. Also, approximate designs can have fewer unitaries than
exact designs. For example, it was shown in [ AMTd00 ] that @xmath
unitaries are necessary and sufficient for an exact unitary 1-design.
However, an approximate 1-design can be implemented with only @xmath
unitaries which gives almost a factor of 2 saving in random bits.

###### 5.3.1 Approximate state designs

Our approximate state design definition is as follows:

###### Definition 5.9.

@xmath is an @xmath -approximate state @xmath -design if

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

@xmath appears because it is the dimension of the symmetric subspace. In
[ AE07 ] , a similar definition was proposed but with the additional
requirement that the ensemble also forms a 1-design (exactly), i.e.

  -- -------- --
     @xmath   
  -- -------- --

This requirement was necessary there only so that a suitably normalised
version of the ensemble would form a POVM. We will not use it.

By taking the partial trace one can show that a @xmath -design is a
@xmath -design for @xmath . Thus approximate @xmath -designs are always
at least approximate 1-designs.

###### 5.3.2 Approximate unitary designs

We have many choices to make when defining an approximate design. Here
we give four definitions which are convenient in different contexts. In
Lemma 5.14 we show that they are all equivalent, up to polynomial
dimension factors.

If the unitary design is considered a quantum channel that applies a
random unitary from the distribution to the input, then a relevant
measure is the diamond norm difference between the approximate design
and an exact design. Because the diamond norm is related to the
distinguishability of channels, having a low diamond norm distance means
that it is difficult to detect that an approximate design was given
rather than exact. One approximate design definition is therefore:

###### Definition 5.10 (Diamond111We name the definitions to help
distinguish them, See Chapter I).

@xmath is an @xmath -approximate unitary @xmath -design if

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

where @xmath and @xmath are defined in Definition 5.6 .

In [ DCEL06 ] , they consider approximate twirling, which is implemented
using an approximate 2-design. They give an alternative definition of
closeness which is more convenient for this application:

###### Definition 5.11 (Twirl, [Dcel06]).

@xmath is an @xmath -approximate twirl if

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

The maximisation is over channels @xmath and @xmath is the dimension.

In Chapter I , unitary designs are constructed from quantum tensor
product expanders. A quantum @xmath -TPE is defined as an ensemble
@xmath of unitaries such that

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

for @xmath and @xmath (the motivation for this definition is explained
in Chapter I ). From this a natural @xmath -design definition follows:

###### Definition 5.12 (TRACE, See Chapter I).

@xmath is an @xmath -approximate unitary @xmath -design if

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

In Theorem 15.3 we prove the simple result that a unitary design can be
constructed by iterating the TPE.

We will also need a definition in terms of monomials:

###### Definition 5.13 (MONOMIAL, See Chapter I).

@xmath is an @xmath -approximate unitary @xmath -design if, for all
balanced monomials @xmath of degree @xmath ,

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

We would now like to show that all these definitions are equivalent. By
equivalent, we mean that, if @xmath is an @xmath -approximate unitary
design by one definition, then it is an @xmath -approximate unitary
design by any other definition, where @xmath .

###### Lemma 5.14.

Definitions 5.10 (DIAMOND), 5.12 (TRACE) and 5.13 (MONOMIAL) are all
equivalent. Also Definition 5.11 (TWIRL) is equivalent to the other
definitions for an approximate 2-design only.

###### Proof.

To prove this, we will consider yet another possible definition
(OPERATOR-2-NORM):

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

Note that this is equivalent to

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

which is the same as Definition 5.12 (TRACE) except the norm is the
@xmath -norm rather than the 1-norm. We shall prove that the other
@xmath -design definitions are equivalent to this. We then show that
Definition 5.11 (TWIRL) is equivalent to Definition 5.13 (MONOMIAL) for
@xmath . We use notation @xmath to mean that if @xmath is an @xmath
-approximate unitary @xmath -design according to definition A then it is
a @xmath -approximate unitary @xmath -design according to definition B.
If @xmath we omit the superscript.

A diagram showing the different parts to the proof is given in Figure 1
. We remark that direction 2 is unneeded but is included since it
provides tighter bounds and has a simple proof.

1.  OPERATOR-2-NORM @xmath TRACE:

    Use the equivalence between Equations 5.17 and 5.18 and that

      -- -------- --
         @xmath   
      -- -------- --

2.  TRACE @xmath OPERATOR-2-NORM:

    Use

      -- -------- --
         @xmath   
      -- -------- --

    and the equivalence between Equations 5.17 and 5.18 .

3.  MONOMIAL @xmath OPERATOR-2-NORM:

    Choose any @xmath and write it as @xmath . Then

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
      -- -------- -------- --

    using the fact that the 2-norm squared is the sum of the squares of
    the matrix elements. Now, we have a bound on the matrix elements of
    @xmath from Definition 5.13 (MONOMIAL):

      -- -------- --
         @xmath   
      -- -------- --

    so

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
      -- -------- -------- --

4.  OPERATOR-2-NORM @xmath DIAMOND:

    This follows from the superoperator norm relationship given in Eqn.
    2.12 .

5.  DIAMOND @xmath OPERATOR-2-NORM:

    This uses the operator norm inequalities @xmath and Eqn. 2.9 .

6.  TRACE @xmath MONOMIAL:

    Let @xmath be a balanced monomial of degree @xmath and write it as

      -- -------- --
         @xmath   
      -- -------- --

    Then let @xmath . Then @xmath and @xmath . Now we use the fact that
    for any operator @xmath

      -- -------- -- --------
         @xmath      (5.19)
      -- -------- -- --------

    to rewrite the TRACE definition:

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
                  @xmath   
                  @xmath   
      -- -------- -------- --

7.  MONOMIAL @xmath TWIRL (for @xmath ):

    Write @xmath in the Kraus decomposition as

      -- -------- -- --------
         @xmath      (5.20)
      -- -------- -- --------

    with

      -- -------- -- --------
         @xmath      (5.21)
      -- -------- -- --------

    Let @xmath . Then the @xmath matrix element of @xmath is

      -- -------- -- --------
         @xmath      (5.22)
      -- -------- -- --------

    From Definition 5.13 (MONOMIAL) we have that

      -- -------- --
         @xmath   
      -- -------- --

    (treating expectation as an operator). This implies that

      -- -------- -- --------
         @xmath      (5.23)
      -- -------- -- --------

    Now, @xmath and @xmath and, taking the trace of the normalisation
    condition Eqn. 5.21 we find

      -- -------- --
         @xmath   
      -- -------- --

    So we find Eqn. 5.23 is upper bounded by

      -- -------- --
         @xmath   
      -- -------- --

    Using the fact that the 2-norm squared is the sum of the squares of
    the matrix elements we find that

      -- -------- --
         @xmath   
      -- -------- --

    Using @xmath (Eqn. 2.12 ) we prove the result.

8.  TWIRL @xmath MONOMIAL for @xmath :

    Let @xmath where @xmath . Let @xmath . Then @xmath and @xmath are
    the Kraus operators of a valid channel, provided @xmath , which we
    assume for now. Further, let

      -- -------- -- --------
         @xmath      (5.24)
      -- -------- -- --------

    where @xmath is the channel with Kraus operators @xmath and @xmath .
    Now let

      -- -------- -- --------
         @xmath      (5.25)
      -- -------- -- --------

    We see that

      -- -------- -- --------
         @xmath      (5.26)
      -- -------- -- --------

    Now, from Definition 5.11 (TWIRL) and the triangle inequality (using
    @xmath ), we have

      -- -------- -- --------
         @xmath      (5.27)
      -- -------- -- --------

    This implies that each matrix element is small i.e.

      -- -------- -- --------
         @xmath      (5.28)
      -- -------- -- --------

    Now let @xmath . We do not have to choose a physical state since the
    diamond-norm bound is true for all matrices. This gives us

      -- -------- -- --------
         @xmath      (5.29)
      -- -------- -- --------

    as required.

    For @xmath , we also assume that @xmath since if not, just take
    @xmath and @xmath and swap the labels. Here take @xmath and @xmath
    and consider @xmath .∎

We remark that other types of approximate definitions are possible. For
cryptographic uses, a computationally secure approximate design may be
sufficient, rather than the information theoretic security discussed
above. A computationally secure approximate design would be nearly
indistinguishable from an exact design in polynomial time. Applications
and constructions of such objects remain open problems.

###### 5.3.3 Constructions

Here we summarise the known constructions of unitary and state designs.
We will say that a @xmath -design construction is efficient if the
effort required to sample a state or unitary from the design is
polynomial in @xmath and @xmath . Note that we do not require the number
of states or unitaries to be polynomial because, even for approximate
designs, an exponential number is required. Rather, the number of random
bits needed to specify an element of the design should be @xmath .

We start with state design constructions since these have been studied
far more than unitary designs. Firstly, exact efficient state 1-designs
are trivial: simply choose a random state from any basis. Numerous
examples of exact efficient state 2-design constructions are known (e.g.
[ Bar02 ] ). Hayashi et al. [ HHH06 ] give an inefficient construction
of state @xmath -designs for any @xmath and @xmath but general exact
constructions are not efficient in @xmath and @xmath . However, Ambainis
and Emerson provide an efficient approximate construction for any @xmath
with @xmath . Aaronson [ Aar09 ] also gives an efficient approximate
construction.

Less is known about efficient constructions for unitary designs. It is
straightforward to prove that the Pauli matrices form an exact 1-design
and in [ DLT02 , Dan05 ] it is shown that the Clifford group (see
Chapter II for a definition) forms an exact 2-design although no
efficient exact sampling method is known. However, an approximate
sampling method is given in [ DLT02 ] and a more efficient approximate
2-design construction is given in [ DCEL06 ] . The structure of unitary
2-designs is considered in [ GAE07 ] , providing lower bounds on the
number of unitaries in the design.

In Chapter I we give the first efficient approximate unitary @xmath
-design construction for @xmath . The construction works in @xmath time
for @xmath . Through Lemma 5.14 , the construction is efficient for all
the equivalent definitions above. We also conjecture in Chapter I that
random quantum circuits of length @xmath are approximate unitary @xmath
-designs although we only prove this for @xmath .

### Chapter \thechapter Random Quantum Circuits

#### 6 Introduction: Pseudo-random Quantum Circuits

Random circuits are a natural object to consider when looking at the
complexity of random operations. They are circuits where the gates and
their positions are chosen randomly from some given distribution. If the
gate set that the random circuit chooses from is universal then, as we
show below, the random circuit will converge to the uniform Haar
measure. The advantage of considering a random circuit rather than a
random unitary on the whole system is it is naturally efficient to
implement, for polynomial length circuits. Random circuits of some fixed
length are also a new measure on the unitary group which, as we show
later, reproduces some of the properties of the Haar measure for
polynomial length. As well as the computer science aspects, this has
applications in physics since randomly interacting systems could be
modelled as a random circuit. These systems will only reach their
equilibrium if the random circuit converges quickly. Thus proving
convergence of the random circuit shows that some physical systems will
have some properties of Haar random systems after evolving for a short
amount of time.

We consider a general class of random circuits where a series of
two-qubit gates are chosen from a universal gate set. We give a
framework for analysing the @xmath moments of these circuits. Our
conjecture, based on an analogous classical result [ BH08 ] , is that a
random circuit on @xmath qubits of length @xmath is an approximate
@xmath -design. While we do not prove this, we instead give a tight
analysis of the @xmath case. We find that in a broad class of natural
random circuit models (described in Section 6.1 ), a circuit of length
@xmath yields an @xmath -approximate 2-design. The approximate design
definition used in this section is the diamond-norm definition given in
Definition 5.10 and, through Lemma 5.14 , applies to the alternative
definitions given above. Moreover, our results also apply to random
stabiliser circuits, meaning that a random stabiliser circuit of length
@xmath will be an @xmath -approximate 2-design. This both simplifies the
construction and tightens the efficiency of the approach of [ DLT02 ] ,
which constructed @xmath -approximate 2-designs in time @xmath using
@xmath elementary quantum gates.

##### 6.1 Random Circuits

The random circuit we will use is the following. Choose a 2-qubit gate
set that is universal on @xmath (or on the stabiliser subgroup of @xmath
). One example of this is the set of all one qubit gates together with
the controlled-NOT gate. Another is simply the set of all of @xmath .
Then, at each step, choose a random pair of qubits and apply a gate from
the universal set chosen uniformly at random. For the @xmath case, the
distribution will be the Haar measure on @xmath . One such circuit is
shown in Fig. 2 for @xmath qubits. This is based on the approach used in
[ ODP07 , DOP07 ] but our analysis is both simpler and more general.

Since the universal set can generate the whole of @xmath in this way,
such random circuits can produce any unitary. Further, since this
process converges to a unitarily invariant distribution and the Haar
distribution is unique, the resulting unitary must be uniformly
distributed amongst all unitaries [ ELL05 ] . Therefore this process
will eventually converge to a Haar distributed unitary from @xmath .
This is proven rigorously in Lemma 9.7 . However, since a Haar unitary
cannot be produced in polynomial time, this process will not converge in
polynomial time. We address this problem by considering only the
lower-order moments of the distribution and showing these are nearly the
same for random circuits as for Haar-distributed unitaries. This claim
is formally described in Theorem 8.1 .

This chapter is organised as follows. In Section 7 we explain how a
random circuit could be used to construct a @xmath -design. We then
summarise the results of this chapter in Section 8 . In Section 9 we
work out how the state evolves after a single step of the random
circuit. We then extend this to multiple steps in Section 10 and prove
our general convergence results. A key simplification will be (following
[ ODP07 ] ) to map the evolution of the second moments of the quantum
circuit onto a classical Markov chain. We then prove a tight convergence
result for the case where the gates are chosen from @xmath in Section 11
. This section contains most of the technical content of the chapter.
Using our bounds on mixing time we put together the proof that random
circuits yield approximate unitary 2-designs in Section 12 . Section 13
concludes with some discussion of applications.

The majority of this chapter, with the exception of Section 11.4 , has
been published previously as [ HL09b ] and is joint work with Aram
Harrow.

#### 7 Preliminaries

##### 7.1 Pauli expansion

Much of the following will be done in the Pauli basis. In this chapter,
we choose the normalisation so that @xmath is written in the Pauli basis
as

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

With this normalisation,

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

which is 1 for pure @xmath . In general,

  -- -------- --
     @xmath   
  -- -------- --

with equality if and only if @xmath is pure. Note also that @xmath is
equivalent to @xmath .

This notation is extended to states on @xmath qubits by treating @xmath
as a function of @xmath strings from @xmath . Thus a state @xmath on
@xmath qubits is written as

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

##### 7.2 Random Circuits as @xmath-designs

If a random circuit is to be an approximate @xmath -design then Eqn.
5.12 must be satisfied where the unitaries in @xmath are the different
possible random circuits. We can think of this as applying the random
circuit not once but @xmath times to @xmath different systems.

Suppose that applying @xmath random gates yields the random circuit
@xmath . If @xmath acts on an @xmath -qubit state @xmath , then the
resulting state is

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

For this to be a @xmath -design, the expectation over all choices of
random circuit should match the expectation over Haar-distributed @xmath
.

We are now ready to state our main results. Our results apply to a large
class of gate sets which we define below:

###### Definition 7.1.

Let @xmath be a discrete ensemble of elements from @xmath . Define an
operator @xmath by

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

where @xmath . More generally, we can consider continuous distributions.
If @xmath is a probability measure on @xmath then we can define @xmath
by analogy as

  -- -------- -- -------
     @xmath      (7.6)
  -- -------- -- -------

Then @xmath (or @xmath ) is @xmath -copy gapped if @xmath (or @xmath )
has only @xmath eigenvalues with absolute value equal to @xmath .

For any discrete ensemble @xmath , we can define a measure @xmath .
Thus, it suffices to state our theorems in terms of @xmath and @xmath .
We also remark that the @xmath -copy gapped property is the same as the
@xmath -tensor product expander property for any non-zero gap as defined
in Chapter I .

The condition on @xmath in the above definition may seem somewhat
strange. We will see in Section 9 that when @xmath there is a @xmath
-dimensional subspace of @xmath that is acted upon trivially by any
@xmath . Additionally, when @xmath is the Haar measure on @xmath then
@xmath is the projector onto this space. Thus, the @xmath -copy gapped
condition implies that vectors orthogonal to this space are shrunk by
@xmath .

We will see that @xmath is @xmath -copy gapped in a number of important
cases. First, we give a definition of universality that can apply not
only to discrete gates sets, but to arbitrary measures on @xmath .

###### Definition 7.2.

Let @xmath be a distribution on @xmath . Suppose that for any open ball
@xmath there exists a positive integer @xmath such that @xmath . Then we
say @xmath is universal [for @xmath ].

Here @xmath is the @xmath -fold convolution of @xmath with itself; i.e.

  -- -------- --
     @xmath   
  -- -------- --

When @xmath is a discrete distribution over a set @xmath , Definition
7.2 is equivalent to the usual definition of universality for a finite
set of unitary gates.

###### Theorem 7.3.

The following distributions on @xmath are @xmath -copy gapped:

-    Any universal gate set. Examples are @xmath itself, any entangling
    gate together with all single qubit gates, or the gate set
    considered in [ ODP07 ] .

-    Any approximate (or exact) unitary @xmath -design on 2 qubits, such
    as the uniform distribution over the 2-qubit Clifford group, which
    is an exact 2-design.

###### Proof.

-   This is proven in Lemma 9.7 .

-   This follows straight from Definition 5.6 .∎

#### 8 Summary of Results

###### Theorem 8.1.

Let @xmath be a 2-copy gapped distribution and @xmath be a random
circuit on @xmath qubits obtained by drawing @xmath random unitaries
according to @xmath and applying each of them to a random pair of
qubits. Then there exists @xmath (depending only on @xmath ) such that
for any @xmath and any @xmath , @xmath is an @xmath -approximate unitary
2-design according to either Definition 5.10 (DIAMOND) or Definition
5.11 (TWIRL).

To prove Theorem 8.1 , we show that the second moments of the random
circuits converge quickly to those of a uniform Haar distributed
unitary. For @xmath a circuit as in Theorem 8.1 , write @xmath for the
Pauli coefficients of @xmath . Then write @xmath where @xmath is a
circuit of length @xmath . Then we have

###### Lemma 8.2.

Let @xmath and @xmath be as in Theorem 8.1 . Let the initial state be
@xmath with @xmath and @xmath (for example the state @xmath for any pure
state @xmath ). Then there exists a constant @xmath (possibly depending
on @xmath ) such that for any @xmath

-   -- -------- -- -------
         @xmath      (8.1)
      -- -------- -- -------

    for @xmath .

-   -- -------- -- -------
         @xmath      (8.2)
      -- -------- -- -------

    for @xmath or, when @xmath is the uniform distribution on @xmath or
    its stabiliser subgroup, @xmath .

We can then extend this to all states by a simple corollary:

###### Corollary 8.3.

Let @xmath , @xmath and @xmath be as in Lemma 8.2 . Then, for any
initial state @xmath , there exists a constant @xmath (possibly
depending on @xmath ) such that for any @xmath

-   -- -------- -- -------
         @xmath      (8.3)
      -- -------- -- -------

    for @xmath .

-   -- -------- -- -------
         @xmath      (8.4)
      -- -------- -- -------

    for @xmath .

By the diamond-norm definition of an approximate design (Definition 5.10
), we only need convergence in the 2-norm (Eqn. 8.3 ), which is implied
by 1-norm convergence (Eqn. 8.4 ) but weaker. However, Definition 5.11
(TWIRL), which requires the map to be close to the twirling operation,
requires 1-norm convergence (i.e. Eqn. 8.4 ). Thus, Theorem 8.1 for
Definition 5.10 (DIAMOND) follows from Corollary 8.3 (i) and Theorem 8.1
for Definition 5.11 (TWIRL) follows from Corollary 8.3 (ii). Theorem 8.1
is proved in Section 12 and Corollary 8.3 in Section 10 .

We note that we do not need to separately prove the result for
Definition 5.11 (TWIRL) since the result follows from the equivalence of
the @xmath -design definitions (Lemma 5.14 ). However, we include the
proof since, if our bounds were improved to show convergence in @xmath
time, if we simply applied Lemma 5.14 , this would only imply that
@xmath time was needed for Definition 5.11 (TWIRL).

We also emphasise that, in the course of proving Lemma 8.2 , we prove
that the eigenvalue gap (defined in Section 10.3 ) of the Markov chain
that gives the evolution of the @xmath terms is @xmath . It is easy to
show that this bound is tight for some gate sets.

Related work: Here we compare our work with other related results and
efficient constructions of approximate unitary 2-designs.

-   The uniform distribution over the Clifford group on @xmath qubits is
    an exact 2-design [ DLT02 ] . Moreover, [ DLT02 ] described how to
    sample from the Clifford group using @xmath classical gates and
    @xmath quantum gates. Our results show that applying @xmath random
    two-qubit Clifford gates also achieve an @xmath -approximate
    2-design (although not necessarily a distribution that is within
    @xmath of uniform on the Clifford group).

-   Dankert et al. [ DCEL06 ] gave a specific circuit construction of an
    approximate 2-design. To achieve small error in the sense of
    Definition 5.10 (DIAMOND), their circuits require the same @xmath
    gates that our random circuits do. However, when we use Definition
    5.11 (TWIRL), the circuits from [ DCEL06 ] only need @xmath gates
    while we only show that random circuits of length @xmath suffice.

-   The closest results to our own are in the papers by Oliveira et al.
    [ ODP07 , DOP07 ] , which considered a specific gate set (random
    single qubit gates and a controlled-NOT) and proved that the second
    moments converge in time @xmath . Our strategy of analysing random
    quantum circuits in terms of classical Markov chains is also adapted
    from [ ODP07 , DOP07 ] . In Section 9 , we generalise this approach
    to analyse the @xmath moments for arbitrary @xmath .

    Our main results extend the results of [ ODP07 , DOP07 ] to a larger
    class of gate sets and improve their convergence bounds. Some of
    these improvements have been conjectured by [ Zni07 ] , where the
    author presented numerical evidence in support of them.

-   An algorithmic application of random circuits was given in [ HH08 ]
    , where they were used to construct a new class of superpolynomial
    quantum speedups. In that paper, random circuits of length @xmath
    were used in order to guarantee that they were so-called
    “dispersing” circuits. Our results immediately imply that circuits
    of length @xmath would instead suffice. We believe that this could
    be further improved with a specialised argument, since [ HH08 ]
    assumed that the input to the random circuit was always a
    computational basis state.

#### 9 Analysis of the Moments

In order to prove our results, we need to understand how the state
evolves after each step of the random circuit. In this section we
consider just one step and a fixed pair of qubits. Later on we will
extend this to prove convergence results for multiple steps with random
pairs of qubits drawn at every step. We consider first the Haar
distribution over the full unitary group and then will discuss the more
general case of any 2-copy gapped distribution.

In this section, we work in general dimension @xmath and with a general
Hermitian orthogonal basis @xmath . Later we will take @xmath to be
either 4 or @xmath and the @xmath to be Pauli matrices. However, in this
section we keep the discussion general to emphasise the potentially
broader applications.

Fix an orthonormal basis for @xmath Hermitian matrices: @xmath ,
normalised so that @xmath . Let @xmath be the identity. We need to
evaluate the quantity

  -- -------- -- -------
     @xmath      (9.1)
  -- -------- -- -------

where the expectation is over Haar distributed @xmath . We will need
this quantity in two cases. Firstly, for @xmath , these are the moments
obtained after applying a uniformly distributed unitary so we know what
the random circuit must converge to. Secondly, for @xmath , this tells
us how a random @xmath gate acts on any chosen pair.

Call the quantity in Eqn. 9.1 @xmath (we use bold to indicate a @xmath
-tuple of coefficients; take @xmath ) and write it in the @xmath basis
as

  -- -------- -- -------
     @xmath      (9.2)
  -- -------- -- -------

Here, @xmath is the coefficient in the Pauli expansion of @xmath and we
define @xmath as the matrix with entries equal to @xmath . We have left
off the usual normalisation factor because, as we shall see, with this
normalisation @xmath is a projector. Inverting this, we have

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (9.3)
  -- -------- -------- -- -------

Note that @xmath is real since @xmath and the basis are Hermitian.

We can gain all the information we need about the Haar integral in Eqn.
9.1 with the following observations:

###### Lemma 9.1.

@xmath commutes with @xmath for any unitary @xmath .

###### Proof.

Follows from the invariance of the Haar measure on the unitary group. ∎

###### Corollary 9.2.

@xmath is a linear combination of permutations from the symmetric group
@xmath .

###### Proof.

This follows from Schur-Weyl duality (see e.g. [ GW98 ] ). ∎

From this, we can prove that @xmath is a projector and find its
eigenvectors.

###### Theorem 9.3.

@xmath is symmetric, i.e. @xmath .

###### Proof.

Follows from the invariance of the trace under cyclic permutations. ∎

###### Theorem 9.4.

@xmath is an eigenvector of @xmath with eigenvalue @xmath for any
subsystem permutation operator @xmath i.e.

  -- -------- --
     @xmath   
  -- -------- --

Further, any vector orthogonal to this set has eigenvalue @xmath .

###### Proof.

For the first part,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      (9.4)
  -- -------- -------- -- -------

Writing @xmath in the @xmath basis, we find

  -- -------- --
     @xmath   
  -- -------- --

Therefore Eqn. 9.4 becomes

  -- -------- --
     @xmath   
  -- -------- --

For the second part, consider any vector @xmath which is orthogonal to
the permutation operators (we can neglect the complex conjugate because
@xmath is real in this basis), i.e.

  -- -------- -- -------
     @xmath      (9.5)
  -- -------- -- -------

for any permutation @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

which is zero since @xmath is a linear combination of permutations and
@xmath is orthogonal to this by Eqn. 9.5 . ∎

###### Theorem 9.5.

@xmath , i.e. @xmath .

###### Proof.

Using Eqn. 9.3 ,

  -- -------- --
     @xmath   
  -- -------- --

From Corollary 9.2 , @xmath is a linear combination of permutations.
This implies, using Theorem 9.4 that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

as required. ∎

###### Corollary 9.6.

@xmath is a projector so has eigenvalues @xmath and @xmath .

We now evaluate @xmath and @xmath for the cases of @xmath and @xmath
since these are the cases we are interested in for the remainder of the
chapter.

##### 9.1 @xmath

The @xmath case is clear: the random unitary completely randomises the
state. Therefore all terms in the expansion are set to zero apart from
the identity i.e.

  -- -------- -- -------
     @xmath      (9.6)
  -- -------- -- -------

##### 9.2 @xmath

For @xmath , there are just two permutation operators, identity @xmath
and swap @xmath . Therefore there are just two eigenvectors with
non-zero eigenvalue ( @xmath ). In normalised form, taking them to be
orthogonal, their components are

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We will now prove three properties of @xmath that we need:

1.  @xmath if @xmath or @xmath .

    ###### Proof.
    Consider the function @xmath with @xmath . This function has zero
    overlap with the eigenvectors @xmath and @xmath so it goes to zero
    when acted on by @xmath . Therefore @xmath . The claim follows from
    the symmetry property (Theorem 9.3 ). ∎

    With this we will write @xmath .

2.  @xmath .

    ###### Proof.
    Let @xmath act on eigenvector @xmath . ∎

3.  @xmath for @xmath .

    ###### Proof.
    Let @xmath act on the input @xmath . This has zero overlap with
    @xmath and overlap @xmath with @xmath . ∎

Therefore we have

  -- -------- -- -------
     @xmath      (9.7)
  -- -------- -- -------

Since @xmath , we have

  -- -------- -- -------
     @xmath      (9.8)
  -- -------- -- -------

Therefore the terms @xmath with @xmath are set to zero. Further, the sum
of the diagonal coefficients @xmath is conserved. This allows us to
identify this with a probability distribution (after renormalising) and
use Markov chain analysis. To see this, write again the starting state

  -- -------- --
     @xmath   
  -- -------- --

with state after application of any unitary @xmath

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

as required, where @xmath is the swap operator and we have used Lemmas
2.2 and 2.1 .

##### 9.3 Moments for General Universal Random Circuits

We now consider universal distributions @xmath that in general may be
different from the uniform (Haar) measure on @xmath . Our main result in
this section will be to show that a universal distribution on @xmath is
also 2-copy gapped. In fact, we will phrase this result in slightly more
general terms and show that a universal distribution on @xmath is also
@xmath -copy gapped for any @xmath . Universality (Definition 7.2 )
generalises in the obvious way to @xmath , whereas when we say that
@xmath is @xmath -copy gapped, we mean that

  -- -------- -- -------
     @xmath      (9.9)
  -- -------- -- -------

where @xmath , with the expectation taken over @xmath for @xmath or over
the Haar measure for @xmath .

The reason Eqn. 9.9 represents our condition for @xmath to be @xmath
-copy gapped is as follows: Observe that @xmath and @xmath are unitarily
related, so the definition of @xmath -copy gapped could equivalently be
given in terms of @xmath . We have shown above that @xmath (and thus
@xmath ) has all eigenvalues equal to @xmath or @xmath i.e. it is a
projector. By contrast, @xmath may not even be Hermitian. However, we
will prove below that all eigenvectors of @xmath with eigenvalue 1 are
also eigenvectors of @xmath with eigenvalue 1. Thus, Eqn. 9.9 will imply
that @xmath , just as we would expect for a gapped random walk.

We would like to show that Eqn. 9.9 holds whenever @xmath is universal.
This result was proved in [ AK62 ] (and was probably known even earlier)
when @xmath had the form @xmath . Here we show how to extend the
argument to any universal @xmath .

###### Lemma 9.7.

Let @xmath be a distribution on @xmath . Then all eigenvectors of @xmath
with eigenvalue 1 are eigenvectors of @xmath with eigenvalue 1.
Additionally, if @xmath is universal then @xmath is @xmath -copy gapped
for any positive integer @xmath (cf. Eqn. 9.9 ).

In particular, if @xmath this Lemma implies that @xmath is 2-copy gapped
(cf. Theorem 7.3 ).

###### Proof.

Let @xmath be the fundamental representation of @xmath , where the
action of @xmath is simply @xmath itself. Let @xmath be its dual
representation, where @xmath acts as @xmath . The operators @xmath and
@xmath act on the space @xmath . We will see that @xmath is completely
determined by the decomposition of @xmath into irreducible
representations (irreps). Suppose that the multiplicity of @xmath in
@xmath is @xmath , where the @xmath ’s are the irrep spaces and @xmath
the corresponding representation matrices. In other words

  -- -------- -------- -- --------
     @xmath   @xmath      (9.10)
     @xmath   @xmath      (9.11)
  -- -------- -------- -- --------

Here @xmath indicates that the two sides are related by conjugation by a
fixed ( @xmath independent) unitary.

Let @xmath denote the trivial irrep: i.e. @xmath and @xmath for all
@xmath . We claim that @xmath whenever @xmath and the expectation is
taken over the Haar measure. To show this, note that @xmath commutes
with @xmath for all @xmath and thus, by Schur’s Lemma, we must have
@xmath for some @xmath . However, by the translation-invariance of the
Haar measure we have @xmath for all @xmath . Since @xmath , we cannot
have @xmath for all @xmath and so it must be that @xmath .

Thus, if we write @xmath and @xmath using the basis on the RHS of Eqn.
9.11 , we have

  -- -------- -- --------
     @xmath      (9.12)
  -- -------- -- --------

where @xmath is a projector onto the trivial irrep. On the other hand,

  -- -------- -- --------
     @xmath      (9.13)
  -- -------- -- --------

Thus, every eigenvector of @xmath with eigenvalue one is also fixed by
@xmath . For the remainder of the space, the direct sum structure means
that

  -- -------- -- --------
     @xmath      (9.14)
  -- -------- -- --------

Note that this maximisation only includes @xmath with @xmath . This is
because non-trivial one-dimensional irreps of @xmath have the form
@xmath for some non-zero integer @xmath . Under the map @xmath , such
irreps pick up a phase of @xmath . However, @xmath is invariant under
@xmath . Thus @xmath cannot contain any non-trivial one-dimensional
irreps.

Now suppose by contradiction that there exists @xmath with @xmath and

  -- -------- --
     @xmath   
  -- -------- --

(We do not need to consider the case @xmath , since @xmath for all
@xmath and @xmath obeys the triangle inequality.) Indeed, the triangle
inequality further implies that there exists a unit vector @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath with @xmath .

By the above argument we can assume that @xmath . Since @xmath is
irreducible, it cannot contain a one-dimensional invariant subspace,
implying that there exists @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath . Since @xmath is continuous, there exists an open ball
@xmath around @xmath such that @xmath for all @xmath . Define @xmath .

Now we use the fact that @xmath is universal to find an @xmath such that
@xmath . Next, observe that @xmath . Taking the absolute value of both
sides yields

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

a contradiction. We conclude that @xmath . ∎

#### 10 Convergence

In the previous section we saw that iterating any universal gate set on
@xmath eventually converges to the uniform distribution on @xmath .
Since the set of all two-qubit unitaries is universal on @xmath , this
implies that random circuits eventually converge to the Haar measure. In
this section, we turn to proving upper bounds on this convergence rate,
focusing on the first two moments.

Let @xmath be the matrix with @xmath (with @xmath ) acting on qubits
@xmath and @xmath and the identity on the others. Then, if the pair
@xmath is chosen at step @xmath , we can find the expected coefficients
at step @xmath by multiplying by @xmath . In general, a random pair is
chosen at each step. So

  -- -------- -- --------
     @xmath      (10.1)
  -- -------- -- --------

where @xmath are the expected coefficients at step @xmath . We can think
of this evolution as repeated application of the matrix

  -- -------- -- --------
     @xmath      (10.2)
  -- -------- -- --------

For @xmath , the key idea of Oliveira et al. [ ODP07 ] was to map the
evolution of the @xmath coefficients to a Markov chain. The @xmath
coefficients with @xmath just decay as each qubit is chosen and can be
analysed directly.

However, we can only map the @xmath coefficients to a probability
distribution when they are non-negative, which is not the case for
general states. Most of the rest of the chapter is dedicated to proving
Lemma 8.2 , which only applies to states with @xmath and normalised so
their sum is @xmath . Corollary 8.3 then extends this to all states:

###### Proof of Corollary 8.3.

Lemma 8.2 still applies to the @xmath terms with @xmath . Therefore we
just need to show how to apply Lemma 8.2 to states that initially have
some negative @xmath terms.

For the @xmath terms, Lemma 8.2 says that the random walk starting with
any initial probability distribution converges to uniform in some
bounded time @xmath . Let @xmath be the coefficients after @xmath steps
of the walk starting at a particular point @xmath (i.e. @xmath ). Now,
for any starting state @xmath , let the initial coefficients be @xmath .
Then, by linearity, we can write the expected coefficients after @xmath
steps @xmath as

  -- -------- -- --------
     @xmath      (10.3)
  -- -------- -- --------

for @xmath .

We can now prove convergence rates for the expected coefficients @xmath
:

-   For the 2-norm, we have from Lemma 8.2 that for @xmath

      -- -------- -- --------
         @xmath      (10.4)
      -- -------- -- --------

    for any @xmath . Note that the normalisation for the @xmath terms
    with @xmath has changed from Lemma 8.2 since we are neglecting the
    @xmath term here. Now

      -- -------- --
         @xmath   
         @xmath   
         @xmath   
         @xmath   
         @xmath   
         @xmath   
         @xmath   
      -- -------- --

    where the first inequality is the Cauchy-Schwarz inequality.
    Therefore for @xmath , the 2-norm distance from stationarity for the
    @xmath terms is at most @xmath . Choose @xmath such that @xmath to
    obtain the result.

-   For the 1-norm, Lemma 8.2 says that for @xmath

      -- -------- -- --------
         @xmath      (10.5)
      -- -------- -- --------

    We can then proceed much as for the 2-norm case:

      -- -------- --
         @xmath   
         @xmath   
         @xmath   
         @xmath   
         @xmath   
         @xmath   
      -- -------- --

    Therefore for @xmath , the 1-norm distance from stationarity for the
    @xmath terms is at most @xmath .∎

We now proceed to prove Lemma 8.2 . Firstly, we will consider the simple
case of @xmath to prove this process forms a 1-design as this will help
us to understand the more complicated case of @xmath .

##### 10.1 First Moments Convergence

Recall that @xmath and we wish to evaluate the moments of the
coefficients. So for the first moments to converge, we want to know
@xmath .

For @xmath , the @xmath random circuit uniformly randomises each pair
that is chosen. More precisely, a pair of sites @xmath are chosen at
random and all the coefficients with @xmath or @xmath are set to zero.
Thus we get an exact 1-design when all sites have been hit. For other
gate sets, the terms do not decay to zero but decay by a factor
depending on the gap of @xmath . Call the gap @xmath ; for @xmath @xmath
and for others @xmath and @xmath is independent of @xmath . Therefore
once each site has been hit @xmath times the terms have decayed by a
factor @xmath .

For a bound like the mixing time (see Section 10.3 for definition), we
want to bound the quantity @xmath where @xmath is the Pauli coefficient
after applying the random circuit @xmath . We also want 2-norm bounds,
so we bound @xmath too. We will in fact find bounds on

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

which are stronger.

A standard problem in the theory of randomised algorithms is the coupon
collector problem . If a magazine comes with a free coupon, which is
chosen uniformly randomly from @xmath different types, how many
magazines should you buy to have a high probability of getting all
@xmath coupons? It is not hard to show that @xmath samples (magazines)
have at least a @xmath probability of including all @xmath coupons.
Using this, we expect all sites to be hit with probability at least
@xmath after @xmath steps. This argument can be made precise in this
context by bounding the non-identity coefficients. We find, as expected,
that the sum is small after @xmath steps:

###### Lemma 10.1.

After @xmath steps

  -- -------- --
     @xmath   
  -- -------- --

and after @xmath steps,

  -- -------- -- --------
     @xmath      (10.6)
  -- -------- -- --------

###### Proof.

At each step, a pair of sites is chosen at random and any terms with
non-identity coefficients for this pair decay by a factor @xmath . For
example, the term @xmath decays whenever the first site is chosen. Thus
the probability of each term decaying depends on the number of zeroes.
We start with the 1-norm bound.

Suppose the circuit applied after @xmath steps is @xmath . Consider
@xmath for any @xmath with @xmath non-zeroes. Since the state @xmath is
physical, @xmath so @xmath . Now, in each step, if any site is chosen
where @xmath is non-zero, this term decays by a factor @xmath . This
occurs with probability @xmath , the probability of choosing a pair
where at least one site is non-zero. Therefore

  -- -------- --
     @xmath   
  -- -------- --

where the expectation is over the circuit applied at step @xmath . If we
iterate this @xmath times we find

  -- -------- --
     @xmath   
  -- -------- --

where the expectation here is over all random circuits for the @xmath
steps. We now sum over all @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the number of non-zeroes in @xmath . For the 1-norm
bound, we can simply bound @xmath to give @xmath so

  -- -------- --
     @xmath   
  -- -------- --

where we have used the binomial theorem. Now let @xmath . This gives

  -- -------- --
     @xmath   
  -- -------- --

For the 2-norm bound,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where we have used @xmath . We find after @xmath steps that

  -- -------- --
     @xmath   
  -- -------- --

##### 10.2 Second Moments Convergence

Firstly, the @xmath terms for @xmath decay in a similar way to the
non-identity terms in the 1-design analysis. In fact, the proof of Lemma
10.1 carries over almost identically to this case to give

###### Lemma 10.2.

After @xmath steps

  -- -------- --
     @xmath   
  -- -------- --

and after @xmath steps

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Instead of the number of zeroes governing the decay rate, we need to
count the number of places where @xmath and @xmath differ. This gives

  -- -------- --
     @xmath   
  -- -------- --

where now @xmath is the number of differing sites. There are @xmath
states that differ in @xmath places so we find

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath to make this @xmath . The 2-norm bound follows in the same
way as for Lemma 10.1 . ∎

We now need to prove the @xmath terms converge quickly. We have seen
above that the sum of the terms @xmath is conserved and, for the
purposes of proving Lemma 8.2 , we assume the sum is @xmath and @xmath
for all @xmath .

To illustrate the evolution, consider the simplest case when the gates
are chosen from @xmath . We have evaluated @xmath in Section 9.2 for
@xmath for this case. Translated into coefficients this yields the
following update rule, where we have written it for the case when qubits
1 and 2 are chosen:

  -- -------- -- --------
     @xmath      (10.7)
  -- -------- -- --------

The key idea of Oliveira et al. [ ODP07 ] was to map the evolution of
the @xmath coefficients to a Markov chain. We can apply this here to
get, on state space @xmath , the evolution:

1.  Choose a pair of sites uniformly at random.

2.  If the state is @xmath it remains @xmath .

3.  Otherwise, choose the state uniformly at random from @xmath .

This is the correct evolution since, if the initial state is distributed
according to @xmath , the final state is distributed according to @xmath
.

The evolution for other gate sets will be similar, but the states will
not be chosen uniformly randomly in the third step. However, the state
@xmath will remain @xmath and the stationary distribution on the other
15 states is the same. We will find the convergence times for general
gate sets and then consider the @xmath gate set since we can perform a
tight analysis for this case.

##### 10.3 Markov Chain Analysis

Before finding the convergence rate for our problem, we will briefly
introduce the basics of Markov chain mixing time analysis. All of these
standard results can be found in [ MT06 ] and references therein.

A process is Markov if the evolution only depends on the current state
rather than the full state history. Therefore the evolution of the state
can be thought of as a matrix, the transition matrix , acting on a
vector which represents the current distribution. We will only be
interested in discrete time processes so the state after @xmath steps is
given by the @xmath power of the transition matrix acting on the initial
distribution.

We say a Markov chain is irreducible if it is possible to get from one
state to any other state in some number of steps. Further, a chain is
aperiodic if it does not return to a state at regular intervals. If a
chain is both irreducible and aperiodic then it is said to be ergodic .
A well known result of Markov chain theory is that all ergodic chains
converge to a unique stationary distribution. In matrix language this
says that the transition matrix @xmath has eigenvalue @xmath with no
multiplicity and all other eigenvalues have absolute value strictly less
than 1. We will also need the notion of reversibility . A Markov chain
is reversible if the time reversed chain has the same transition matrix,
with respect to some distribution. This condition is also known as
detailed balance :

  -- -------- -- --------
     @xmath      (10.8)
  -- -------- -- --------

It can be shown that a reversible ergodic Markov chain is only
reversible with respect to the stationary distribution. So above @xmath
is the stationary distribution of @xmath . An immediate consequence of
this is that for a chain with uniform stationary distribution, it is
reversible if and only if it is symmetric (i.e. @xmath ). Note also that
reversible chains have real eigenvalues, since they are similar to the
symmetric matrix @xmath (using the similarity transform @xmath ).

With these definitions and concepts, we can now ask how quickly the
Markov chain converges to the stationary distribution. This is normally
defined in terms of the 1-norm mixing time. We use (half the) 1-norm
distance to measure distances between distributions:

  -- -------- -- --------
     @xmath      (10.9)
  -- -------- -- --------

We assume all distributions are normalised so then @xmath . We can now
define the mixing time:

###### Definition 10.3.

Let @xmath be the stationary distribution of @xmath . Then if @xmath is
ergodic the mixing time @xmath is

  -- -------- -- ---------
     @xmath      (10.10)
  -- -------- -- ---------

We will also use the (weaker) 2-norm mixing time (note this is not the
same as @xmath in [ MT06 ] ):

###### Definition 10.4.

Let @xmath be the stationary distribution of @xmath . Then if @xmath is
ergodic the 2-norm mixing time @xmath is

  -- -------- -- ---------
     @xmath      (10.11)
  -- -------- -- ---------

Unless otherwise stated, when we say mixing time we are referring to the
1-norm mixing time.

There are many techniques for bounding the mixing time, including
finding the second largest eigenvalue of @xmath . This gives a good
measure of the mixing time because components parallel to the second
largest eigenvector decay the slowest. We have (for reversible ergodic
chains)

###### Theorem 10.5 (see [Mt06], Corollary 1.15).

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath where @xmath is the second largest eigenvalue
and @xmath is the smallest. @xmath is known as the gap .

If the chain is irreversible, it may not even have real eigenvalues.
However, we can bound the mixing time in terms of the eigenvalues of the
reversible matrix @xmath where @xmath . In this case we have ( [ MT06 ]
, Corollary 1.14)

  -- -------- -- ---------
     @xmath      (10.12)
  -- -------- -- ---------

where now @xmath is the gap of the chain @xmath . Note that for a
reversible chain @xmath and @xmath so the bounds are approximately the
same.

This can also be converted into a 2-norm mixing time bound:

  -- -------- -- ---------
     @xmath      (10.13)
  -- -------- -- ---------

To bound the gap, we will use the comparison theorem in Theorem 10.6
below. In this Theorem, we are thinking of the Markov chain as a
directed graph where the vertices are the states and there are edges for
allowed transitions (i.e. transitions with non-zero probability). For
irreducible chains, it is possible to make a path from any vertex to any
other; we call the path length the number of transitions in such a path
(which will in general depend on the choice of path).

###### Theorem 10.6 (see [Mt06], Theorem 2.14).

Let @xmath and @xmath be two Markov chains on the same state space
@xmath with the same stationary distribution @xmath . Then, for every
@xmath with @xmath define a directed path @xmath from @xmath to @xmath
along edges in @xmath and let its length be @xmath . Let @xmath be the
set of all such paths. Then

  -- -------- --
     @xmath   
  -- -------- --

for the gaps @xmath and @xmath where

  -- -------- --
     @xmath   
  -- -------- --

For example, when comparing 1-dimensional random walks there is no
choice in the paths; they must pass through every point between @xmath
and @xmath . Further, the walk can only progress one step at a time so
(without loss of generality, for reversible chains) let @xmath to give

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (10.14)
  -- -------- -------- -- ---------

A generalisation of the comparison theorem involves constructing flows,
which are weighted sets of paths between states. This can give a tighter
bound since bottlenecks are averaged over. This gives a modified
comparison theorem:

###### Theorem 10.7 ([Ds93], Theorem 2.3).

Let @xmath and @xmath be two Markov chains on the same state space
@xmath with the same stationary distribution @xmath . Then, for every
@xmath with @xmath , construct a set of directed paths @xmath from
@xmath to @xmath along edges in @xmath . We define the flow function
@xmath which maps each path @xmath to a real number in the interval
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Again, let the length of each path be @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

for the gaps @xmath and @xmath where

  -- -------- -- ---------
     @xmath      (10.15)
  -- -------- -- ---------

Note that we recover the comparison theorem when there is just one path
between each @xmath and @xmath .

Yet another generalisation is to allow general length functions instead
of simply counting the edges. This only appears in the literature as a
comparison to the chain @xmath although it can easily be generalised to
allow comparison with any chain.

###### Theorem 10.8 ([Kah96], Proposition 1).

Let @xmath be a Markov chain on the state space @xmath with stationary
distribution @xmath . Then, for every @xmath define a directed path
@xmath from @xmath to @xmath along edges in @xmath and let its length be

  -- -------- -- ---------
     @xmath      (10.16)
  -- -------- -- ---------

for any positive length function @xmath , defined on the edges of the
path. Let @xmath be the set of all such paths. Then

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

###### 10.3.1 Decomposition

For some Markov chains, it is easier to consider different parts of the
chain separately to prove convergence results. This allows, for example,
different convergence techniques to be used on different parts of the
chain. The separate parts are combined using the decomposition theorem:

###### Theorem 10.9 ([Mr00], Theorem 4.2).

Let @xmath be the transition matrix for a reversible Markov chain with
state space @xmath and stationary distribution @xmath . Then let @xmath
be disjoint subsets of @xmath such that @xmath . Let

  -- -------- -- ---------
     @xmath      (10.17)
  -- -------- -- ---------

Further let @xmath and

  -- -------- -- ---------
     @xmath      (10.18)
  -- -------- -- ---------

Then

  -- -------- -- ---------
     @xmath      (10.19)
  -- -------- -- ---------

where @xmath is the gap of @xmath , @xmath for @xmath and @xmath for
@xmath .

###### 10.3.2 log-Sobolev Constant

We will need tighter, but more complicated, mixing time results to prove
the tight result for the @xmath case. We use the log-Sobolev constant:

###### Definition 10.10.

The log-Sobolev constant @xmath of a chain with transition matrix @xmath
and stationary distribution @xmath is

  -- -------- --
     @xmath   
  -- -------- --

The mixing time result is:

###### Lemma 10.11 (see [Ds96], Theorem 3.7’).

The mixing time of a finite, reversible, irreducible Markov chain is

  -- -------- -- ---------
     @xmath      (10.20)
  -- -------- -- ---------

where @xmath is the Sobolev constant, @xmath is the smallest value of
the stationary distribution, @xmath is the gap and @xmath is the size of
the state space.

Further, the comparison theorem (Theorem 10.6 ) works just the same to
give

  -- -------- --
     @xmath   
  -- -------- --

We will need one more result, due to Diaconis and Saloff-Coste:

###### Lemma 10.12 ([Ds96], Lemma 3.2).

Let @xmath , @xmath , be Markov chains with gaps @xmath and Sobolev
constants @xmath . Now construct the product chain @xmath . This chain
has state space equal to the product of the spaces for the chains @xmath
and at each step one of the chains is chosen at random and run for one
step. Then @xmath has spectral gap given by:

  -- -------- --
     @xmath   
  -- -------- --

and Sobolev constant:

  -- -------- --
     @xmath   
  -- -------- --

##### 10.4 Convergence Proof

We now prove the Markov chain convergence results to show that the
@xmath terms converge quickly. We have already shown that the @xmath
terms with @xmath converge quickly and that there is no mixing between
these terms and the @xmath terms. Therefore, in this section, we remove
such terms from @xmath .

We want to prove the Markov chain with transition matrix (Eqn. 10.2 )

  -- -------- --
     @xmath   
  -- -------- --

converges quickly. Firstly, we know from Section 9.3 that @xmath has two
eigenvectors with eigenvalue @xmath . The first is the identity state (
@xmath ) and the second is the uniform sum of all non-identity terms (
@xmath ). From now on, we remove the identity state. This makes the
chain irreducible. Since we know it converges, it must be aperiodic also
so the chain is ergodic and all other eigenvalues are strictly between
@xmath and @xmath .

We show here that the gap of this chain, up to constants, does not
depend on the choice of 2-copy gapped gate set. In the second half of
the chapter we find a tight bound on the gap for the @xmath case which
consequently gives a tight bound on the gap for all universal sets.

Since the stationary distribution is uniform, the chain is reversible if
and only if @xmath is a symmetric matrix. A sufficient condition for
@xmath to be symmetric is for @xmath to be symmetric. We saw in Theorem
9.3 that for the @xmath gate set case @xmath is symmetric. In fact, the
proof works identically to show that @xmath is symmetric for any gate
set, provided the set is invariant under Hermitian conjugation. However,
2-copy gapped gate sets do not necessarily have this property so the
Markov chain is not necessarily reversible. We will find equal bounds
(up to constants) for the gaps of both @xmath (if @xmath is symmetric)
and @xmath (if @xmath is not symmetric) below:

###### Theorem 10.13.

Let @xmath be any 2-copy gapped distribution of gates. If @xmath is
invariant under Hermitian conjugation then let @xmath be the eigenvalue
gap of the resulting Markov chain matrix @xmath . Then

  -- -------- -- ---------
     @xmath      (10.21)
  -- -------- -- ---------

where @xmath is the eigenvalue gap of the @xmath chain. If @xmath is not
invariant under Hermitian conjugation then let @xmath be the eigenvalue
gap of the resulting Markov chain matrix @xmath . Then

  -- -------- -- ---------
     @xmath      (10.22)
  -- -------- -- ---------

###### Proof.

We will use the comparison method with flows (Theorem 10.7 ). Firstly
consider the case where @xmath is closed under Hermitian conjugation
i.e. @xmath is symmetric.

We will compare @xmath to the @xmath chain, which we call @xmath .
Recall that this chain chooses a pair at random and does nothing if the
pair is @xmath and chooses a random state from @xmath otherwise.

To apply Theorem 10.7 , we need to construct the flows between
transitions in @xmath . We will choose paths such that only one pair is
modified throughout. For example (with @xmath ), the transition @xmath
is allowed in @xmath . To construct a path in @xmath , we need to find
allowed transitions between these two paths in @xmath . @xmath may not
include the transition @xmath directly, however, @xmath is irreducible
on this subspace of just two pairs. This means that a path exists and
can be of maximum length @xmath if it has to cycle through all
intermediate states (in fact, since @xmath is symmetric the maximum path
length is @xmath ; all that is important here is that it is constant).
For example, the transitions @xmath might be allowed. Then we could
choose the full path to be @xmath . In this case we have chosen the path
to involve transitions pairing sites 1 and 2. However, we could equally
well have chosen any pairing; we could pair the first site with any of
the others. We can choose 3 paths in this way. For this example, the
flow we want to choose will be all 3 of these paths equally weighted. We
now use this idea to construct flows between all transitions in @xmath
to prove the result.

Let @xmath and let @xmath be the Hamming distance between the states (
@xmath gives the number of places at which @xmath and @xmath differ).
There are two cases where @xmath :

1.  @xmath . Here we must choose a unique pairing, specified by the two
    sites that differ. Make all transitions in @xmath using this pair
    giving just one path.

2.  @xmath . For this case, choose all possible pairings of the changing
    site that give allowed transitions in @xmath . For each pairing,
    construct a path in @xmath modifying only this pair. If the
    differing site is initially non-zero then there are @xmath such
    pairings; if the differing site is initially zero then there are
    @xmath pairings where @xmath is the number of zeroes in the state
    @xmath .

All the above paths are of constant length since we have to (at most)
cycle through all states of a pair. We must now choose the weighting
@xmath for each path such that

  -- -------- -- ---------
     @xmath      (10.23)
  -- -------- -- ---------

where @xmath is the set of all paths from @xmath to @xmath constructed
above. We choose the weighting of each path to be uniform. We just need
to calculate the number of paths in @xmath to find @xmath :

1.  @xmath . There is just one path so @xmath .

2.  @xmath . If the differing site is initially non-zero then @xmath and
    there are @xmath paths so @xmath . If the differing site is
    initially zero then @xmath and there are @xmath paths so @xmath .

So for all paths, @xmath . We now just need to know how many times each
edge @xmath in @xmath is used to calculate @xmath :

  -- -------- -- ---------
     @xmath      (10.24)
  -- -------- -- ---------

where

  -- -------- -- ---------
     @xmath      (10.25)
  -- -------- -- ---------

We have cancelled the factors of @xmath because the stationary
distribution is uniform. We have also ignored the lengths of the paths
since they are all constant.

To evaluate @xmath , we need to know how many paths pass through each
edge @xmath . We again consider the two possibilities separately:

1.  @xmath . Suppose @xmath and @xmath differ at sites @xmath and @xmath
    . Firstly, we need to count how many transitions from @xmath to
    @xmath in @xmath could use this edge, and then how many paths for
    each transition actually use the edge.

    To find which @xmath and @xmath could use the edge, note that @xmath
    and @xmath must differ at sites @xmath , @xmath or both.
    Furthermore, the values at the sites other than @xmath and @xmath
    must be the same as for @xmath (and therefore @xmath ). There is a
    constant number of @xmath pairs that satisfy this condition. Now,
    for each @xmath pair satisfying this, paths that use this edge must
    use the pairing @xmath for all transitions. Since in the paths we
    have chosen above there is a unique path from @xmath to @xmath for
    each pairing, there is at most one path for each @xmath pair that
    uses edge @xmath .

    For @xmath , @xmath so @xmath is a constant for this case.

2.  @xmath . Let there be @xmath pairings that give allowed transitions
    in @xmath between @xmath and @xmath . As above, each pairing gives a
    constant number of paths. So the numerator is @xmath . Further,
    @xmath . So again @xmath is constant.

Combining, @xmath is a constant so the result is proven for the case
@xmath is symmetric.

We now turn to the irreversible case. We now need to bound the gap of
@xmath . This chain selects two (possibly overlapping) pairs at random
and applies @xmath to one of them and @xmath to the other. We can use
the above exactly by choosing @xmath to perform the transitions above
and @xmath to just loop the states back to themselves. By aperiodicity
(the greatest common divisor of loop lengths is @xmath ), we can always
find constant length paths that do this. ∎

Now we need to know the gap of the @xmath chain. We can, by a simple
application of the comparison theorem, show it is @xmath . However, in
the second half of this chapter we show it is @xmath . This gives us
(using Theorem 10.5 ):

###### Corollary 10.14.

The Markov chain @xmath has mixing time @xmath and 2-norm mixing time
@xmath .

We conjecture that the mixing time (as well as Lemma 10.2 ) can be
tightened to @xmath , which is asymptotically the same as for the @xmath
case:

###### Conjecture 10.15.

The second moments for the case of general 2-copy gapped distributions
have 1-norm mixing time @xmath .

It seems likely that an extension of our techniques in Section 11 could
be used to prove this.

Combining the convergence results we have proved our general result
Lemma 8.2 :

###### Proof of Lemma 8.2.

Combining Corollary 10.14 (for the @xmath terms) and Lemma 10.2 (for the
@xmath , @xmath terms) proves the result. ∎

We have now shown that the first and second moments of random circuits
converge quickly. For the remainder of the chapter we prove the tight
bound for the gap and mixing time of the @xmath case and show how mixing
time bounds relate to the closeness of the 2-design to an exact design.
Only for the @xmath case is the matrix @xmath a projector so in this
sense the @xmath random circuit is the most fundamental. While we expect
the above mixing time bound is not tight, we can prove a tight mixing
time result for the @xmath case. However, using our definition of an
approximate @xmath -design, the gap rather than the mixing time governs
the degree of approximation.

#### 11 Tight Analysis for the @xmath Case

We have already found tight bounds for the first moments in Lemma 10.1 :
just set @xmath .

##### 11.1 Second Moments Convergence

We need to prove a result analogous to Lemma 10.2 for the terms @xmath
where @xmath . We already have a tight bound for the 2-norm decay, by
setting @xmath into Lemma 10.2 . We tighten the 1-norm bound:

###### Lemma 11.1.

After @xmath steps

  -- -------- -- --------
     @xmath      (11.1)
  -- -------- -- --------

###### Proof.

We will split the random circuits up into classes depending on how many
qubits have been hit. Let @xmath be the random variable giving the
number of different qubits that have been hit. We can work out the
distribution of @xmath and bound the sum of @xmath for each outcome.

Firstly we have, after @xmath steps,

  -- -------- --
     @xmath   
  -- -------- --

Now, for each qubit hit, each coefficient which has @xmath and @xmath
differing in this place is set to zero. So after @xmath have been hit,
there are only (at most) @xmath terms in the sum in Eqn. 11.1 . As
before, the state is a physical state, @xmath so @xmath so @xmath if
there are at most @xmath non-zero terms in the sum. Therefore we have,
after @xmath steps,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now, let @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where the last line follows from the binomial theorem. ∎

This, combined with the mixing time result we prove below, completes the
proof that the second moments of the random circuit converge in time
@xmath .

##### 11.2 Markov Chain of Coefficients

The Markov chain acting on the coefficients is reducible because the
state @xmath is isolated. However, if we remove it then the chain
becomes irreducible. The presence of self loops implies aperiodicity
therefore the chain is ergodic. We have already seen that the chain
converges to the Haar uniform distribution (in Section 6.1 ) therefore
the stationary state is the uniform state @xmath . Further, since the
chain is symmetric and has uniform stationary distribution, the chain
satisfies detailed balance (Eqn. 10.8 ) so is reversible. We now turn to
obtaining bounds on the mixing time of this chain.

We want to show that the full chain converges to stationarity in time
@xmath . To prove this, we will construct another chain called the zero
chain. This is the chain that counts the number of zeroes in the state.
Since it is the zeroes that slow down the mixing, this chain will
accurately describe the mixing time of the full chain.

###### Lemma 11.2.

The zero chain has transition matrix P on state space (we count non-zero
positions) @xmath .

  -- -------- -- --------
     @xmath      (11.2)
  -- -------- -- --------

for @xmath .

###### Proof.

Suppose there are @xmath zeroes (so there are @xmath non-zeroes). Then
the only way the number of zeroes can decrease (i.e. for @xmath to
increase) is if a non-zero item is paired with a zero item and one of
the @xmath (out of @xmath ) new states is chosen with no zeroes. The
probability of choosing such a pair is @xmath so the overall probability
is @xmath .

The number of zeroes can increase only if a pair of non-zero items is
chosen and one of the @xmath states is chosen with one zero. The
probability of this occurring is @xmath .

The probability of the number of zeroes remaining unchanged is simply
calculated by requiring the probabilities to sum to @xmath . ∎

We see that the zero chain is a one-dimensional random walk on the line.
It is a lazy random walk because the probability of moving at each step
is @xmath . However, as the number of zeroes decreases, the probability
of moving increases monotonically:

  -- -------- -- --------
     @xmath      (11.3)
  -- -------- -- --------

###### Lemma 11.3.

The stationary distribution of the zero chain is

  -- -------- -- --------
     @xmath      (11.4)
  -- -------- -- --------

###### Proof.

This can be proven by multiplying the transition matrix in Lemma 11.2 by
the state Eqn. 11.4 . Alternatively, it can be proven by counting the
number of states with @xmath zeroes. There are @xmath ways of choosing
which sites to make non-zero and each non-zero site can be one of three
possibilities: 1, 2 or 3. The total number of states is @xmath , which
gives the result. ∎

Below we will prove the following theorem:

###### Theorem 11.4.

The zero chain mixes in time @xmath .

We prove this using direct arguments about the convergence of the random
walk. However, we also include a less complex method that only bounds
the gap:

###### Theorem 11.5.

The zero chain has gap @xmath .

This only implies the mixing time is @xmath which is weaker than Theorem
11.4 , although still sufficient to prove our main result Theorem 8.1 ,
using a modification of Corollary 11.7 to show that the full chain
mixing time is @xmath .

Knowing the gap allows us to easily work out the 2-norm mixing time:

###### Theorem 11.6.

The zero chain has 2-norm mixing time @xmath .

###### Proof.

Use the bound on the gap in Theorem 11.5 and Eqn. 10.13 . ∎

Before proving Theorem 11.4 , we will show how the mixing time of the
full chain follows from this.

###### Corollary 11.7.

The full chain mixes in time @xmath .

###### Proof.

Once the zero chain has approximately mixed, the distribution of zeroes
is almost correct. We need to prove that the distribution of non-zeroes
is correct after @xmath steps too.

Once each site of the full chain has been hit, meaning it is chosen and
paired with another site so not both equal zero, the chain has mixed.
This is because, after each site has been hit, the probability
distribution over the states is uniform. When the zero chain has
approximately mixed, a constant fraction of sites are zero so the
probability of hitting a site at each step is @xmath . By the coupon
collector argument, each site will have been hit with probability at
least @xmath in time time @xmath . Once the zero chain has mixed to
@xmath , we can run the full chain this extra number of steps to ensure
each site has been hit with high probability. Since the mixing of the
zero chain only increases with time, the distance to stationarity of the
full chain is now @xmath . We make this formal below.

After @xmath steps, the number of zeroes is @xmath -close to the
stationary distribution @xmath by Theorem 11.4 and only gets closer with
more steps since the distance to stationarity decreases monotonically.
The stationary distribution Eqn. 11.4 is approximately a Gaussian peaked
at @xmath with @xmath variance. This means that, with high probability,
the number of non-zeroes is close to @xmath . We will in fact only need
that there is at least a constant fraction of non-zeroes; with
probability at least @xmath there will be at least @xmath .

To prove the mixing time, we run the chain for time @xmath so the zero
chain mixes to @xmath . Then run for @xmath additional steps. Let @xmath
be the event that site @xmath is hit at step @xmath . Let @xmath and
@xmath . We want to show @xmath is close to 1, or, in other words, that
all sites are hit with high probability. Further let @xmath be the
random variable giving the number of non-zeroes at step @xmath .

If at step @xmath site @xmath is non-zero then the event @xmath occurs
if the qubit is chosen, which occurs with probability @xmath . If,
however, it was zero then it must be paired with a non-zero thing for
@xmath to hold. Conditioned on any history with @xmath , this
probability is @xmath . In particular, we can condition on not having
previously hit @xmath and the bound does not change. Combining we have

  -- -------- --
     @xmath   
  -- -------- --

Then, after @xmath extra steps,

  -- -------- --
     @xmath   
  -- -------- --

which, using the union bound, gives

  -- -------- --
     @xmath   
  -- -------- --

Now, since the zero chain has mixed to @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

so

  -- -------- --
     @xmath   
  -- -------- --

Now, choose @xmath so that @xmath where @xmath . Choose @xmath and
@xmath so that @xmath is @xmath . Now, using the bound on @xmath , we
can write the state @xmath after @xmath steps as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the stationary distribution and @xmath is any other
distribution. Using this,

  -- -------- --
     @xmath   
  -- -------- --

We now apply Lemma 14.13 to show that after @xmath steps the distance to
stationarity of the full chain is @xmath . ∎

##### 11.3 Proof of Theorem 3.6.4

We will now proceed to prove Theorem 11.4 . We present an outline of the
proof here; the details are in Section 14.1 .

Firstly, note that by the coupon collector argument, the lower bound on
the time is @xmath . We need to prove an upper bound equal to this.
Intuition says that the mixing time should take time @xmath because the
walk has to move a distance @xmath and the waiting time at each step is
proportional to @xmath which sums to @xmath , provided each site is not
hit too often. We will show that this intuition is correct using
Chernoff bound and log-Sobolev (see later) arguments.

We will first work out concentration results of the position after some
number of accelerated steps. The zero chain has some probability of
staying still at each step. The accelerated chain is the zero chain
conditioned on moving at each step. We define the accelerated chain by
its transition matrix:

###### Definition 11.8.

The transition matrix for the accelerated chain is

  -- -------- -- --------
     @xmath      (11.5)
  -- -------- -- --------

We use the accelerated chain in the proof to firstly prove the
accelerated chain mixes quickly, then to bound the waiting time at each
step to obtain a mixing time bound for the zero chain.

To prove the mixing time bound, we will split the walk up into three
phases. We will split the state space into three (slightly overlapping)
parts and the phase can begin at any point within that space. So each
phase has a state space @xmath , an entry space @xmath and an exit
condition @xmath . We say that a phase completes successfully if the
exit condition is satisfied in time @xmath for an initial state within
the entry space. When the exit condition is satisfied, the walk moves
onto the next phase.

The phases are:

1.  @xmath for some constant @xmath with @xmath . @xmath (i.e. it can
    start anywhere) and @xmath is satisfied when the walk reaches @xmath
    . For this part, the probability of moving backwards (gaining
    zeroes) is @xmath so the walk progresses forwards at each step with
    high probability. This is proven in Lemma 14.6 . We show that the
    waiting time is @xmath in Lemma 14.7 .

2.  @xmath for some constant @xmath with @xmath . @xmath and @xmath is
    satisfied when the walk reaches @xmath . Here the walk can move both
    ways with constant probability but there is a @xmath forward bias.
    Here we use a monotonicity argument: the probability of moving
    forward at each step is

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
                  @xmath   
      -- -------- -------- --

    If we model this random walk as a walk with constant bias equal to
    @xmath we will find an upper bound on the mixing time since mixing
    time increases monotonically with decreasing bias. Further, the
    waiting time at @xmath stochastically dominates the waiting time at
    @xmath for @xmath . The true bias decreases with position so the
    walk with constant bias spends more time at the early steps. Thus
    the position of this simplified walk is stochastically dominated by
    the position of the real walk while the waiting time stochastically
    dominates the waiting time of the real walk.

3.  @xmath and @xmath . @xmath is satisfied when this restricted part of
    the chain has mixed to distance @xmath . Here the bias decreases to
    zero as the walk approaches @xmath but the moving probability is a
    constant. We show that this walk mixes quickly by bounding the
    log-Sobolev constant of the chain.

Showing these three phases complete successfully will give a mixing time
bound for the whole chain.

We now prove in Section 14 that the phases complete successfully with
probability at least @xmath :

###### Lemma 11.9.

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 11.10.

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

###### Lemma 11.11.

  -- -------- --
     @xmath   
  -- -------- --

We can now finally combine to prove our result:

###### Proof of Theorem 11.4.

The stationary distribution has exponentially small weight in the tail
with lots of zeroes. We show that, provided the number of zeroes is
within phase 3, the walk mixes in time @xmath . We also show that if the
number of zeroes is initially within phase 1 or 2, after @xmath steps
the walk is in phase 3 with high probability. We can work out the
distance to the stationary distribution as follows.

Let @xmath be the probability of failure. This is the sum of the error
probabilities in Lemmas 11.9 , 11.10 and 11.11 . The key point is that
@xmath . Then after @xmath steps (the sum of the number of steps in the
3 phases), the state is equal to @xmath where @xmath is the state in the
phase 3 space and @xmath is any other distribution, which occurs if any
one of the phases fails. Since the distance to stationarity in phase 3
is @xmath , @xmath , where @xmath is the stationary distribution on the
state space of phase 3. In Lemma 14.11 we show that @xmath where @xmath
. Since @xmath is exponentially small in this range, @xmath is
exponentially small in @xmath . Now use the triangle inequality to find

  -- -------- -- --------
     @xmath      (11.6)
  -- -------- -- --------

Since the chain in phase 3 has mixed to @xmath , the first term is
@xmath . We can evaluate @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

So now,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath . We are free to choose @xmath : choose it to be @xmath so
that @xmath is @xmath . So now the running time to get a distance @xmath
is @xmath . We then apply Lemma 14.13 to obtain the result.

This concludes the proof of Theorem 11.4 so Corollary 11.7 is proved. ∎

We have now proven Lemma 8.2 and consequently Corollary 8.3 . We are now
ready to show how Theorem 8.1 follows, but first give the alternative
proof that the zero chain gap is @xmath . The remainder of the proof of
Theorem 8.1 is in Section 12 .

##### 11.4 Proof of Theorem 3.6.5

Here we prove that the gap of the zero chain is @xmath . While this can
be deduced from Theorem 11.4 and provides weaker mixing time bounds,
this bound on the gap is sufficient to prove our main result so we
present it as a simpler alternative proof.

We use the method of decomposition (Theorem 10.9 ), whereby the Markov
chain is split up into disjoint state spaces. This works well here
because, for the first part of the walk with many zeroes, the walker
remains stationary most of the time whereas when there is a constant
fraction of zeroes, the walker moves on most steps. Using the
decomposition method allows us to use different techniques in these
different regimes.

We therefore divide the walk up into two parts, @xmath and @xmath ,
which are shown in Figure 3 . The chain @xmath is the chain that links
the two parts, according to the decomposition theorem, Theorem 10.9 .

-   @xmath : Let @xmath have state space @xmath . This chain has
    transition matrix

      -- -------- -- --------
         @xmath      (11.7)
      -- -------- -- --------

    and stationary distribution

      -- -------- -- --------
         @xmath      (11.8)
      -- -------- -- --------

    where

      -- -------- -- --------
         @xmath      (11.9)
      -- -------- -- --------

-   @xmath : Let @xmath be on state space @xmath . This chain has
    transition matrix

      -- -------- -- ---------
         @xmath      (11.10)
      -- -------- -- ---------

    and stationary distribution

      -- -------- -- ---------
         @xmath      (11.11)
      -- -------- -- ---------

    where

      -- -------- -- ---------
         @xmath      (11.12)
      -- -------- -- ---------

We will take @xmath where @xmath (we could in principle just have @xmath
but this restriction makes the calculations simpler; see Lemma 11.12 for
the origin of the upper bound on @xmath ). Note that @xmath is the same
as phase 3 used in the direct mixing time proof (up to relabelling
@xmath to @xmath ). Therefore we already have, from the proof of Lemma
14.11 , that the gap @xmath of @xmath is @xmath . To find the gap of the
whole zero chain we need to find the gaps @xmath and @xmath . An
ingredient to proving this is an exponential bound on the tail of the
stationary distribution:

###### Lemma 11.12.

  -- -------- -- ---------
     @xmath      (11.13)
  -- -------- -- ---------

and for @xmath , @xmath .

###### Proof.

Use @xmath and @xmath to prove the bound. When @xmath , @xmath is
exponentially small. @xmath is the solution to @xmath . ∎

From this we can bound the gap of @xmath :

###### Lemma 11.13.

The gap of @xmath is @xmath .

###### Proof.

We first need to work out the transition matrix for @xmath . From the
definition of @xmath in the decomposition theorem,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We can find the two diagonal elements using the fact that the transition
matrix is stochastic. Because the zero chain is reversible, @xmath is
also reversible and by direct calculation of the eigenvalues has gap

  -- -------- -- ---------
     @xmath      (11.14)
  -- -------- -- ---------

However, we can remove the modulus signs since, for @xmath large enough,
@xmath . This is because, using reversibility and @xmath

  -- -------- -- ---------
     @xmath      (11.15)
  -- -------- -- ---------

Using Lemma 11.12 we find that @xmath for @xmath and @xmath . Using
@xmath , we find that for @xmath large enough, @xmath .

Now we need to show that @xmath . Again using Lemma 11.12 , we find
@xmath . We just need a bound on @xmath . First we bound @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

using @xmath for @xmath . For @xmath , we have @xmath so overall @xmath
, proving the bound on the gap. ∎

###### Lemma 11.14.

The gap of @xmath is @xmath .

###### Proof.

We use the comparison method with length functions as stated in Theorem
10.8 . The length function we choose is, for @xmath , @xmath for some
constant @xmath satisfying @xmath .

Let

  -- -------- -- ---------
     @xmath      (11.16)
  -- -------- -- ---------

Then, according to Theorem 10.8 , @xmath where @xmath . We need to find
an upper bound for @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the inequality comes from @xmath . Now let @xmath then, plugging
in the value of @xmath , we find

  -- -------- --
     @xmath   
  -- -------- --

Now, @xmath so showing @xmath is sufficient the prove the bound we
require. We evaluate @xmath recursively:

Firstly,

  -- -------- --
     @xmath   
  -- -------- --

Then evaluate the sum:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Combining,

  -- -------- --
     @xmath   
  -- -------- --

or

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , @xmath is constant in this range, proving the result. ∎

We can now combine the results to prove the bound on the zero chain gap:

###### Proof of Theorem 11.5.

Using Lemmas 11.14 , 14.11 and 11.13 together with Theorem 10.9 proves
the result. ∎

#### 12 Main Result

We will now show how the mixing time results imply that we have an
approximate 2-design.

###### Proof of Theorem 8.1:.

We will go via the 2-norm since this gives a tight bound when working
with the Pauli operators. We write @xmath in the Pauli basis as usual
(as Eqn. 7.3 ) and note that @xmath is not necessarily a physical state
so the coefficients may not be real.

  -- -------- -------- --
     @xmath            
              @xmath   
                       
              @xmath   
  -- -------- -------- --

Now, write (for @xmath ) @xmath . We get

  -- -------- --
              
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

where the first equality comes from the orthogonality of the Pauli
operators under the Hilbert-Schmidt inner product. This proves the
result for the diamond norm, Definition 5.10 . For the distance measure
defined in Definition 5.11 (TWIRL), the argument in [ DCEL06 ] can be
used together with the 1-norm bound to prove the result. ∎

#### 13 Conclusions

We have proved tight convergence results for the first two moments of a
random circuit. We have used this to show that random circuits are
efficient approximate 1- and 2-unitary designs. Our framework readily
generalises to @xmath -designs for any @xmath and the next step in this
research is to prove that random circuits give approximate @xmath
-designs for all @xmath .

We have shown that, provided the random circuit uses gates from a
universal gate set that is also universal on @xmath , the circuit is
still an efficient 2-design. We also see that the random circuit with
gates chosen uniformly from @xmath is the most natural model. We note
that the gates from @xmath can be replaced by gates from any approximate
2-design on two qubits without any change to the asymptotic convergence
properties.

Finally, random circuits are interesting physical models in their own
right. The original purpose of [ ODP07 ] was to answer the physical
question of how quickly entanglement grows in a system with random two
party interactions. Lemma 8.2 (i) shows that @xmath steps suffice (in
contrast to @xmath which they prove) to give almost maximal entanglement
in such a system.

#### 14 Proofs

##### 14.1 Zero chain mixing time proofs

###### 14.1.1 Asymmetric Simple Random Walk

We will use some facts about asymmetric simple random walks i.e. a
random walk on a 1D line with probability @xmath of moving right at each
step and probability @xmath of moving left.

The position of the walk after @xmath steps is tightly concentrated
around @xmath :

###### Lemma 14.1.

Let @xmath be the random variable giving the position of a random walk
after @xmath steps starting at the origin with probability @xmath of
moving right and probability @xmath of moving left. Let @xmath . Then
for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The standard Chernoff bound for @xmath variables @xmath gives, with
@xmath equal to @xmath with probability p and for @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

For our case, set @xmath to give the desired result. ∎

This result is for a walk with constant bias. We will need a result for
a walk with varying (but bounded from below) bias:

###### Lemma 14.2.

Let @xmath be the random variable giving the position of a random walk
after @xmath steps starting at the origin with probability @xmath of
moving right and probability @xmath of moving left at step @xmath . Let
@xmath . Then for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath be a random variable equal to @xmath with probability @xmath
and @xmath with probability @xmath . Then let @xmath be a random
variable equal to @xmath with probability @xmath and @xmath with
probability @xmath . Let @xmath and @xmath . Then following the standard
Chernoff bound derivation (for @xmath ),

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We can then, as above, set @xmath . The calculation is similar for the
bound on @xmath . ∎

From Lemma 14.1 we can prove a result about how often each site is
visited. If the walk runs for @xmath steps the walk is at position
@xmath with high probability so we might expect from symmetry that each
site will have been visited about @xmath times. Below is a weaker
concentration result of this form but is strong enough for our purposes.
It says that the amount of time spent @xmath is about @xmath .

###### Lemma 14.3.

For @xmath and integer @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the indicator function.

###### Proof.

Let @xmath . From Lemma 14.1 ,

  -- -------- --
     @xmath   
  -- -------- --

for @xmath and

  -- -------- --
     @xmath   
  -- -------- --

for @xmath .

Then the quantity to evaluate is

  -- -------- --
     @xmath   
  -- -------- --

We use a standard trick to split this into two mutually exclusive
possibilities and then bound the probabilities separately. Write

  -- -------- -- --------
     @xmath      (14.1)
  -- -------- -- --------

We can bound the first term:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The second term similarly:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The last fact we need about asymmetric simple random walks is a bound on
the probability of going backwards. If @xmath then we expect the walk to
go right in the majority of steps. The probability of going left a
distance @xmath is exponentially small in @xmath . This is a well known
result, often stated as part of the gambler’s ruin problem:

###### Lemma 14.4 (See e.g. [Gw86]).

Consider an asymmetric simple random walk that starts at @xmath and has
an absorbing barrier at the origin. The probability that the walk
eventually absorbs at the origin is @xmath if @xmath and @xmath
otherwise.

This result is for infinitely many steps. If we only consider finitely
many steps, the probability of absorption must be at most this.

###### 14.1.2 Waiting Time

From above we saw that the probability of moving is at least @xmath when
at position @xmath . The length of time spent waiting at each step is
therefore stochastically dominated by a geometric distribution with
parameter @xmath . The following concentration result will be used to
bound the waiting time (in our case @xmath ):

###### Lemma 14.5.

Let the waiting time at each site be @xmath , the total waiting time
@xmath and @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

By Markov’s inequality for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The @xmath are independent so

  -- -------- --
     @xmath   
  -- -------- --

Summing the geometric series we find

  -- -------- --
     @xmath   
  -- -------- --

provided @xmath for all @xmath . Therefore @xmath is of the form @xmath
where @xmath . With this,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

We are free to choose @xmath within its range to optimise the bound.
However, for simplicity, we will choose @xmath . From Lemma 14.12 ,

  -- -------- --
     @xmath   
  -- -------- --

The result follows, using the inequality @xmath . ∎

###### 14.1.3 Phase 1

Here we prove that phase 1 completes successfully with high probability.
The bias here is large so the walk moves right every time with high
probability:

###### Lemma 14.6.

The probability that the accelerated chain moves right at each step,
starting from @xmath for @xmath steps, is at least

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The probability of moving right at each step is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Let @xmath . Provided @xmath this probability is close to one.
Therefore, with high probability, the walk moves to @xmath in @xmath
steps. Using Lemma 14.5 the waiting time can be bounded:

###### Lemma 14.7.

Let @xmath be the waiting time during phase 1. Let @xmath be the event
that the walk moves right at each step. Then

  -- -------- -- --------
     @xmath      (14.2)
  -- -------- -- --------

where @xmath .

###### Proof.

This follows directly from Lemma 14.5 , since each site is hit exactly
once. ∎

We now combine these two lemmas to prove that phase 1 completes
successfully with high probability:

###### Proof of Lemma 11.9.

In Lemma 14.6 , we show that in @xmath accelerated steps, the walk moves
right at each step with probability @xmath . Call this event @xmath .
Then @xmath . Lemma 14.7 shows that the waiting time @xmath is bounded
with high probability (choosing @xmath ):

  -- -------- --
     @xmath   
  -- -------- --

Then we can bound the probability of phase 1 completing successfully:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

###### 14.1.4 Phase 2

Phase 2 starts at @xmath and finishes when the walk has reached @xmath
for some constant @xmath . We show that, with high probability, this
also takes time @xmath . The probability of moving right during this
phase is at least @xmath . We first define some constants that we will
derive bounds in terms of. Let @xmath be a constant @xmath . Let @xmath
and @xmath . Finally let @xmath for some @xmath (which will be the
number of accelerated steps). Then, with high probability, the walk will
have passed @xmath after @xmath steps:

###### Lemma 14.8.

Let @xmath be the position of the walk at accelerated step @xmath ,
where @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath . Then from Lemma 14.2 ,

  -- -------- --
     @xmath   
  -- -------- --

Now let @xmath and use

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

to complete the proof. ∎

We now prove a bound on the waiting time:

###### Lemma 14.9.

Let @xmath be the waiting time in phase 2. Then, assuming the walk does
not go back beyond @xmath ,

  -- -- -- --------
           (14.3)
  -- -- -- --------

###### Proof.

Let @xmath where @xmath is the position of the walk at accelerated step
@xmath ( @xmath ). We want to bound (w.h.p.) the waiting time @xmath of
@xmath steps of the accelerated walk.

Define the event @xmath to be

  -- -------- -- --------
     @xmath      (14.4)
  -- -------- -- --------

If @xmath occurs, no sites have been hit too often and the walk has not
gone back further than @xmath . It is important that we also use the
restriction that @xmath because the waiting time grows the longer the
walk moves back. However, it is very unlikely that the walk will go
backwards (even to @xmath ).

We now define some more notation to bound the waiting time. Let @xmath
be a tuple of positions and let @xmath be the number of times that
@xmath appears in @xmath and let @xmath . Then we have @xmath .

As we said above, the waiting time at @xmath stochastically dominates
the waiting time at @xmath for @xmath . In other words,

  -- -------- -- --------
     @xmath      (14.5)
  -- -------- -- --------

where @xmath means that @xmath stochastically dominates @xmath . Now
write the waiting time for all steps

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (14.6)
  -- -------- -------- -- --------

where @xmath .

If event @xmath occurs, we can put some bounds on @xmath . We find that,
for all @xmath ,

  -- -------- -- --------
     @xmath      (14.7)
  -- -------- -- --------

and @xmath for @xmath . Now let @xmath be such that @xmath and @xmath
for @xmath . Then

  -- -------- -- --------
     @xmath      (14.8)
  -- -------- -- --------

Now we introduce the relation @xmath :

###### Definition 14.10.

Let @xmath and @xmath be @xmath -tuples. Then @xmath if

  -- -------- -- --------
     @xmath      (14.9)
  -- -------- -- --------

for all @xmath with equality for @xmath .

Note that this is like majorisation, except the elements of the tuples
are not sorted. Using this, we find that @xmath (Using @xmath for all
@xmath .)

If we combine Equations 14.5 and 14.6 we find that @xmath if @xmath .
Roughly speaking, this is simply saying that the waiting time is larger
if the earlier sites are hit more often. But since for all @xmath that
satisfy @xmath , @xmath , we have @xmath provided @xmath occurs. We will
simplify further by noting that @xmath where @xmath for @xmath and zero
elsewhere. Therefore

  -- -------- --
     @xmath   
  -- -------- --

We can bound this by applying Lemma 14.5 . Let @xmath . From Lemma 14.5
,

  -- -------- -- ---------
     @xmath      (14.10)
  -- -------- -- ---------

where @xmath . However, we want a bound on @xmath . The same reasoning
as in Lemma 14.5 bounds this as

  -- -------- -- ---------
     @xmath      (14.11)
  -- -------- -- ---------

Therefore

  -- -- -- ---------
           (14.12)
  -- -- -- ---------

To complete the proof, we just need to find @xmath . We can bound it
using the union bound and Lemma 14.3 :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now, for any events @xmath and @xmath

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and set @xmath and @xmath to obtain the result. ∎

We now combine these two lemmas to prove that phase 2 completes
successfully with high probability:

###### Proof of Lemma 11.10.

Phase 2 can fail if:

-   The walk does not reach @xmath . The probability of this is bounded
    by Lemma 14.8 :

      -- -------- --
         @xmath   
      -- -------- --

    This follows from setting @xmath and @xmath .

-   The waiting time is too long. This probability is bounded by Lemma
    14.9 :

      -- -------- --
         @xmath   
      -- -------- --

-   The walk gets back to @xmath . This is bounded by Lemma 14.4 :

      -- -------- --
         @xmath   
      -- -------- --

So, using the union bound we can bound the overall probability of
failure:

  -- -------- --
     @xmath   
  -- -------- --

###### 14.1.5 Phase 3

This phase starts at @xmath . We show that this mixes quickly using
log-Sobolev arguments.

###### Lemma 14.11.

The zero chain on the restricted state space @xmath where @xmath for
@xmath has mixing time @xmath .

###### Proof.

We restrict the Markov chain to only run from @xmath by adjusting the
holding probability at @xmath , @xmath . Construct the chain @xmath with
transition matrix

  -- -------- -- ---------
     @xmath      (14.13)
  -- -------- -- ---------

where @xmath is the transition matrix of the full zero chain. This chain
then has stationary distribution

  -- -------- -- ---------
     @xmath      (14.14)
  -- -------- -- ---------

where @xmath . To see this, first note that the distribution is
normalised. We want to show that

  -- -------- -- ---------
     @xmath      (14.15)
  -- -------- -- ---------

When @xmath we are required to prove that @xmath . This follows from the
reversibility of the unrestricted zero chain, using @xmath . For @xmath
, Eqn. 14.15 is satisfied simply because @xmath is the stationary
distribution of @xmath and related by a constant factor to @xmath .

We can now prove this final mixing time result, making use of Lemma
10.12 . Let @xmath be the chain that uniformly mixes site @xmath . This
converges in one step and has a log-Sobolev constant independent of
@xmath ; call it @xmath . Let @xmath be the chain that chooses a site at
random and then uniformly mixes that site. This is the product chain of
the @xmath so, by Lemma 10.12 , has gap @xmath and Sobolev constant
@xmath . We can construct the zero chain for this and find its Sobolev
constant.

The Sobolev constant is defined (Definition 10.10 ) in terms of a
minimisation over functions on the state space. For the chain @xmath we
can write

  -- -------- --
     @xmath   
  -- -------- --

If we restrict the infimum to be over functions @xmath with @xmath for
@xmath and @xmath containing the same number of zeroes then we obtain
the Sobolev constant for the zero-Q chain, @xmath , which is the chain
which counts the number of zeroes in the full chain Q. Since taking the
infimum over less functions cannot give a smaller value,

  -- -------- --
     @xmath   
  -- -------- --

We can now compare this chain to the zero- @xmath chain. The stationary
distributions are the same. The transition matrix for the zero- @xmath
chain is

  -- -------- --
     @xmath   
  -- -------- --

Then construct @xmath by restricting the space to only run from @xmath
in exactly the same way as @xmath is constructed from @xmath . @xmath
has the same stationary distribution as @xmath . Now we can perform the
comparison. From Eqn. 10.3 :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Therefore @xmath . Exactly the same argument applies to show the gap is
@xmath so the mixing time is (from Eqn. 10.20 ) @xmath . ∎

Now we can prove that phase 3 completes successfully with high
probability:

###### Proof of Lemma 11.11.

In Lemma 14.11 , we show that after @xmath steps the chain mixes to
distance @xmath . We just need to show that the walk goes back to @xmath
with small probability. This follows from Lemma 14.4 . ∎

##### 14.2 Moment Generating Function Calculations

The following lemma is needed in the moment generating function
calculations.

###### Lemma 14.12.

For Integer @xmath ,

  -- -------- -- ---------
     @xmath      (14.16)
  -- -------- -- ---------

###### Proof.

From expanding the @xmath functions, Eqn. 14.16 becomes

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

We then proceed by induction. @xmath and by the inductive hypothesis

  -- -------- --
     @xmath   
  -- -------- --

It is easy to show that @xmath and the result follows. ∎

##### 14.3 Mixing Times

We find bounds for the mixing time above that are valid with high
probability. Below we turn these into full mixing time bounds.

###### Lemma 14.13.

If after @xmath steps the state @xmath of a random walk satisfies

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the stationary distribution and @xmath is @xmath then
the number of steps required to be at most a distance @xmath from
stationarity is

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath be the slowest mixing initial state. Then, after @xmath steps
we have at worst the state

  -- -------- --
     @xmath   
  -- -------- --

and if we repeat @xmath times @xmath becomes @xmath . So to get a
distance @xmath , @xmath .

Now we evaluate the mixing time:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

### Chapter \thechapter Quantum Tensor Product Expanders and an
Efficient Unitary Design Construction

#### 15 Introduction

In this chapter, we give an efficient construction of a unitary @xmath
-design on @xmath qubits for any @xmath up to @xmath . We will do this
by first finding an efficient construction of a quantum @xmath -copy
tensor product expander (k-TPE), which can then be iterated to produce a
@xmath -design. We will therefore need to understand some of the theory
of expanders before presenting our construction.

Classical expander graphs have the property that a marker executing a
random walk on the graph will have a distribution close to the
stationary distribution after a small number of steps. We consider a
generalisation of this, known as a @xmath -tensor product expander (TPE)
and due to [ HH09 ] , to graphs that randomise @xmath different markers
carrying out correlated random walks on the same graph. This is a
stronger requirement than for a normal ( @xmath ) expander because the
correlations between walkers (unless they start at the same position)
must be broken. We then generalise quantum expanders in the same way, so
that the unitaries act on @xmath copies of the system. We give an
efficient construction of a quantum @xmath -TPE which uses an efficient
classical @xmath -TPE as its main ingredient. We then give as a key
application the first efficient construction of a unitary @xmath -design
for any @xmath .

While randomised constructions yield @xmath -designs (by a modification
of Theorem 5 of [ ABW09 ] ) and @xmath -TPEs (when the dimension is
polynomially larger than @xmath [ HH09 ] ) with near-optimal parameters,
these approaches are not efficient. Previous efficient constructions of
@xmath -designs were known only for @xmath , and no efficient
constant-degree, constant-gap quantum @xmath -TPEs were previously
known, except for the @xmath case corresponding to quantum expanders [
BT07 , AS04 , Har08 , GE08 ] .

In Section 15.1 , we will define quantum expanders and other key terms.
Then in Section 15.2 we will describe our main result which will be
proved in Section 16 . In this chapter, we will use @xmath to denote the
dimension rather than @xmath to be consistent with the rest of the
quantum expander literature.

This chapter has been published previously as [ HL09a ] and is joint
work with Aram Harrow.

##### 15.1 Quantum Expanders

We will only consider @xmath -regular expander graphs here. We can think
of a random walk on such a graph as selecting one of @xmath permutations
of the vertices randomly at each step. We construct the permutations as
follows. Label the vertices from @xmath to @xmath . Then label each edge
from @xmath to @xmath so that each edge label appears exactly once on
the incoming and outgoing edges of each vertex. This gives a set of
@xmath permutations. Choosing one of these permutations at random (for
some fixed probability distribution) then defines a random walk on the
graph.

We now define a classical @xmath -TPE:

###### Definition 15.1 ([Hh09]).

Let @xmath be a probability distribution on @xmath with support on
@xmath permutations. Then @xmath is a classical @xmath -TPE if

  -- -------- -- --------
     @xmath      (15.1)
  -- -------- -- --------

with @xmath . Here @xmath means the expectation over @xmath drawn
according to @xmath and @xmath means the expectation over @xmath drawn
uniformly from @xmath .

Setting @xmath recovers the usual spectral definition of an expander.
Note that a @xmath -TPE is also a @xmath -TPE for any @xmath . The
largest meaningful value of @xmath is @xmath , corresponding to the case
when @xmath describes a Cayley graph expander on @xmath .

The degree of the map is @xmath and the gap is @xmath . Ideally, the
degree should be small and gap large. To be useful, these should
normally be independent of @xmath and possibly @xmath . We say that a
TPE construction is efficient if it can be implemented in @xmath steps.
There are known constructions of efficient classical TPEs. The
construction of Hoory and Brodsky [ BH08 ] provides an expander with
@xmath and @xmath with efficient running time. An efficient TPE
construction is also known, due to Kassabov [ Kas05 ] , which has
constant degree and gap (independent of @xmath and @xmath ).

Similarly, we define a quantum @xmath -TPE:

###### Definition 15.2 ([Hh09]).

Let @xmath be a distribution on @xmath , the group of @xmath unitary
matrices, with @xmath . Then @xmath is a quantum @xmath -TPE if

  -- -------- -- --------
     @xmath      (15.2)
  -- -------- -- --------

with @xmath . Here @xmath means the expectation over @xmath drawn from
the Haar measure.

Again, normally we want @xmath and @xmath to be constants and setting
@xmath recovers the usual definition of a quantum expander. Note that an
equivalent statement of the above definition is that, for all @xmath ,

  -- -------- -- --------
     @xmath      (15.3)
  -- -------- -- --------

A natural application of this is to make an efficient unitary @xmath
-design. The definition we use here is the same as for a @xmath -TPE,
except with closeness in the 1-norm rather than the @xmath -norm. This
is given in Definition 5.12 (TRACE).

We can make an @xmath -approximate unitary @xmath -design from a quantum
@xmath -TPE with @xmath overhead:

###### Theorem 15.3.

If @xmath is a quantum @xmath -TPE then iterating the map @xmath times
gives an @xmath -approximate unitary @xmath -design according to
Definition 5.12 (TRACE) with @xmath unitaries.

###### Proof.

Iterating the TPE @xmath times gives

  -- -------- --
     @xmath   
  -- -------- --

This implies that

  -- -------- --
     @xmath   
  -- -------- --

We take @xmath such that @xmath to give the result. ∎

###### Corollary 15.4.

A construction of an efficient quantum @xmath -TPE yields an efficient
approximate unitary @xmath -design, provided @xmath . Further, if @xmath
and @xmath are constants, the number of unitaries in the design is
@xmath .

Our approach to construct an efficient quantum @xmath -TPE will be to
take an efficient classical @xmath -TPE and mix it with a quantum
Fourier transform. The degree is thus only larger than the degree of the
classical expander by one. Since the quantum Fourier transform on @xmath
requires @xmath time, it follows that if the classical expander is
efficient then the quantum expander is as well. The main technical
difficulty is to show for suitable values of @xmath that the gap of the
quantum TPE is not too much worse than the gap of the classical TPE.

A similar approach to ours was first used in [ HH09 ] to construct a
quantum expander (i.e. a 1-TPE) by mixing a classical 2-TPE with a
phase. However, regardless of the set of phases chosen, this approach
will not yield quantum @xmath -TPEs from classical @xmath -TPEs for any
@xmath .

##### 15.2 Main Result

Let @xmath and define the @xmath -dimensional Fourier transform to be

  -- -------- -- --------
     @xmath      (15.4)
  -- -------- -- --------

Define @xmath to be the distribution on @xmath consisting of a point
mass on @xmath . Our main result in this chapter is that mixing @xmath
with a classical @xmath -TPE yields a quantum @xmath -TPE for
appropriately chosen @xmath and @xmath .

###### Theorem 15.5.

Let @xmath be a classical @xmath -TPE, and for @xmath , define @xmath .
Suppose that

  -- -------- -- --------
     @xmath      (15.5)
  -- -------- -- --------

Then @xmath is a quantum @xmath -TPE where

  -- -------- -- --------
     @xmath      (15.6)
  -- -------- -- --------

The bound in Eqn. 15.6 is optimised when @xmath , in which case we have

  -- -------- -- --------
     @xmath      (15.7)
  -- -------- -- --------

This means that any constant-degree, constant-gap classical @xmath -TPE
gives a quantum @xmath -TPE with constant degree and gap. If the the
classical TPE is efficient then the quantum TPE is as well. Using
Corollary 15.4 , we obtain approximate unitary @xmath -designs with
polynomial-size circuits.

Unfortunately the construction does not work for all dimensions; we
require that @xmath , so that @xmath is lower-bounded by a positive
constant. However, in applications normally @xmath is fixed. An
interesting open problem is to find a construction that works for all
dimensions, in particular a @xmath expander. (Most work on @xmath TPEs
so far has focused on the @xmath case [ BG06 ] .) We suspect our
construction may work for @xmath as large as @xmath for a small constant
@xmath . On the other hand, if @xmath then the gap in our construction
drops to zero.

#### 16 Proof of Theorem 4.1.5

##### 16.1 Proof overview

First, we introduce some notation. Define @xmath and @xmath . These are
both projectors onto spaces which we label @xmath and @xmath
respectively. Since @xmath , it follows that @xmath is a projector onto
the space @xmath . We also define @xmath and @xmath .

The idea of our proof is to consider @xmath a proxy for @xmath ; if
@xmath is small enough then this is a reasonable approximation. Then we
can restrict our attention to vectors in @xmath , which we would like to
show all shrink substantially under the action of our expander. This in
turn can be reduced to showing that @xmath maps any vector in @xmath to
a vector that has @xmath amplitude in @xmath . This last step is the
most technically involved step of the chapter, and involves careful
examination of the different vectors making up @xmath .

Thus, our proof reduces to two key Lemmas. The first allows us to
substitute @xmath for @xmath while keeping the gap constant.

###### Lemma 16.1 ([Hh09] Lemma 1).

Let @xmath be a projector and let @xmath and @xmath be operators such
that @xmath , @xmath , @xmath , @xmath and @xmath . Assume @xmath . Then
for any @xmath , @xmath . Specifically,

  -- -------- -- --------
     @xmath      (16.1)
  -- -------- -- --------

We will restrict to @xmath , or equivalently, subtract the projector
@xmath from each operator. Thus we have @xmath , @xmath and @xmath .
According to Definition 15.1 , we have the bound

  -- -------- -- --------
     @xmath      (16.2)
  -- -------- -- --------

It will remain only to bound @xmath .

###### Lemma 16.2.

For @xmath ,

  -- -------- -- --------
     @xmath      (16.3)
  -- -------- -- --------

Combining Eqn. 16.2 , Lemma 16.2 and Lemma 16.1 now completes the proof
of Theorem 15.5 .

##### 16.2 Action of a Classical @xmath-Tpe

We start by analysing the action of a classical @xmath -TPE. (We
consider @xmath -TPEs rather than general @xmath -TPEs since our quantum
expander construction only uses these.) The fixed points are states
which are unchanged when acted on by @xmath copies of any permutation
matrix. Since the same permutation is applied to all copies, any equal
indices will remain equal and any unequal indices will remain unequal.
This allows us to identify the fixed points of the classical expander:
they are the sums over all states with the same equality and difference
constraints. For example, for @xmath (corresponding to a 2-TPE), the
fixed points are @xmath and @xmath (all off-diagonal entries equal to
1). In general, there is a fixed point for each partition of the set
@xmath into at most @xmath non-empty parts. If @xmath , which is the
only case we consider, the @xmath Bell number @xmath gives the number of
such partitions (see e.g. [ Sta86 ] ).

We now write down some more notation to further analyse this. If @xmath
is a partition of @xmath , then we write @xmath . We will see that
@xmath projects onto a space spanned by vectors labelled by partitions.
For a partition @xmath , say that @xmath if and only if elements @xmath
and @xmath are in the same block. Now we can write down the fixed points
of the classical expander. Let

  -- -------- -- --------
     @xmath      (16.4)
  -- -------- -- --------

This is a set of tuples where indices in the same block of @xmath are
equal and indices in different blocks are not equal. The corresponding
state is

  -- -- -- --------
           (16.5)
  -- -- -- --------

where @xmath . Note that the @xmath form a partition @xmath and thus the
@xmath form an orthonormal basis for @xmath . This is because, when
applying the same permutation to all indices, indices that are the same
remain the same and indices that differ remain different. This implies
that

  -- -------- -- --------
     @xmath      (16.6)
  -- -------- -- --------

To evaluate the normalisation, use @xmath where @xmath is the falling
factorial @xmath and @xmath is the number of blocks in @xmath . We will
later find it useful to bound @xmath with

  -- -------- -- --------
     @xmath      (16.7)
  -- -------- -- --------

We will also make use of the refinement partial order:

###### Definition 16.3.

The refinement partial order @xmath on partitions @xmath is given by

  -- -------- -- --------
     @xmath      (16.8)
  -- -------- -- --------

For example, @xmath . Note that @xmath implies that @xmath .

###### 16.2.1 Turning Inequality Constraints into Equality Constraints.

In the analysis, it will be easier to consider just equality constraints
rather than both inequality and equality constraints as in @xmath .
Therefore we make analogous definitions:

  -- -------- -- --------
     @xmath      (16.9)
  -- -------- -- --------

and

  -- -- -- ---------
           (16.10)
  -- -- -- ---------

Then @xmath . For @xmath , indices in the same block are equal, as with
@xmath , but indices in different blocks need not be different.

We will need relationships between @xmath and @xmath . First, observe
that @xmath can be written as the union of some @xmath sets:

  -- -------- -- ---------
     @xmath      (16.11)
  -- -------- -- ---------

To see this, note that for @xmath , we have @xmath , but we may also
have an arbitrary number of additional equalities between @xmath ’s in
different blocks. The (unique) partition @xmath corresponding to these
equalities has the property that @xmath is a refinement of @xmath ; that
is, @xmath . Thus for any @xmath there exists a unique @xmath such that
@xmath . Conversely, whenever @xmath , we also have @xmath because each
inclusion is achieved only be relaxing constraints.

Using Eqn. 16.11 , we can obtain a useful identity involving sums over
partitions:

  -- -------- -- ---------
     @xmath      (16.12)
  -- -------- -- ---------

Additionally, since both sides in Eqn. 16.12 are degree @xmath
polynomials and are equal on @xmath points (we can choose any @xmath in
Eqn. 16.12 with @xmath ), it implies that @xmath as an identity on
formal polynomials in @xmath .

The analogue of Eqn. 16.11 for the states @xmath and @xmath is similar
but has to account for normalisation factors. Thus we have

  -- -------- -- ---------
     @xmath      (16.13)
  -- -------- -- ---------

We would also like to invert this relation, and write @xmath as a sum
over various @xmath . Doing so will require introducing some more
notation. Define @xmath to be 1 if @xmath and 0 if @xmath . This can be
thought of as a matrix that, with respect to the refinement ordering,
has ones on the diagonal and is upper-triangular. Thus it is also
invertible. Define @xmath to be the matrix inverse of @xmath , meaning
that for all @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath if @xmath and @xmath otherwise. Thus, if we rewrite Eqn.
16.13 as

  -- -------- -- ---------
     @xmath      (16.14)
  -- -------- -- ---------

then we can use @xmath to express @xmath in terms of the @xmath as

  -- -------- -- ---------
     @xmath      (16.15)
  -- -------- -- ---------

This approach is a generalisation of inclusion-exclusion known as Möbius
inversion, and the function @xmath is called the Möbius function (see
Chapter 3 of [ Sta86 ] for more background). For the case of the
refinement partial order, the Möbius function is known:

###### Lemma 16.4 ([Rot64], Section 7).

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the number of blocks of @xmath in the @xmath block of
@xmath .

We can use this to evaluate sums involving the Möbius function for the
refinement order.

###### Lemma 16.5.

  -- -------- -- ---------
     @xmath      (16.16)
  -- -------- -- ---------

where @xmath is arbitrary and @xmath is the rising factorial @xmath .

###### Proof.

Start with @xmath to obtain

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

using Eqn. 16.12 . Then use Möbius inversion and @xmath to prove the
result. ∎

We will mostly be interested in the special case @xmath :

###### Corollary 16.6.

  -- -------- -- ---------
     @xmath      (16.17)
  -- -------- -- ---------

Using @xmath and the fact that @xmath for all @xmath , we obtain a bound
on the total number of partitions:

###### Corollary 16.7.

The Bell numbers @xmath satisfy @xmath .

##### 16.3 Fixed Points of a Quantum Expander

We now turn to @xmath , the space fixed by the quantum expander. As in
Chapter I , the only operators on @xmath to commute with @xmath for all
@xmath are linear combinations of subsystem permutations. The equivalent
statement for @xmath is that the only states invariant under all @xmath
are of the form

  -- -------- -- ---------
     @xmath      (16.18)
  -- -------- -- ---------

for some permutation @xmath . Since @xmath projects onto the set of
states that is invariant under all @xmath , it follows that @xmath is
equal to the span of the states in Eqn. 16.18 .

Now we relate these states to our previous notation.

###### Definition 16.8.

For @xmath , define the partition corresponding to @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Then the state in Eqn. 16.18 is simply @xmath , and so

  -- -------- -- ---------
     @xmath      (16.19)
  -- -------- -- ---------

Note that the classical expander has many more fixed points than just
the desired @xmath . The main task in constructing a quantum expander
from a classical one is to modify the classical expander to decay the
fixed points that should not be fixed by the quantum expander.

##### 16.4 Fourier Transform in the Matrix Element Basis

Since we make use of the Fourier transform, we will need to know how it
acts on a matrix element. We find

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- -- ---------
     @xmath      (16.20)
  -- -------- -- ---------

We will also find it convenient to estimate the matrix elements @xmath .
The properties we require are proven in the following lemmas.

###### Lemma 16.9.

Choose any @xmath . Let @xmath and @xmath . Call the free indices of
@xmath @xmath for @xmath . Then let @xmath where @xmath is a @xmath
matrix with entries in @xmath which depends on @xmath (but not @xmath ).
Then

  -- -------- -- ---------
     @xmath      (16.21)
  -- -------- -- ---------

where @xmath is the indicator function.

###### Proof.

Simply perform the @xmath sum in

  -- -------- -- ---------
     @xmath      (16.22)
  -- -------- -- ---------

###### Lemma 16.10.

@xmath is real and positive.

###### Proof.

Since all entries in the sum in Eqn. 16.21 are nonnegative and at least
one ( @xmath ) is strictly positive, Lemma 16.9 implies the result. ∎

###### Lemma 16.11.

If @xmath and @xmath then

  -- -------- -- ---------
     @xmath      (16.23)
  -- -------- -- ---------

###### Proof.

We prove first the special case when @xmath , but @xmath is arbitrary.
Recall that @xmath implies that @xmath . Now the LHS of Eqn. 16.23
equals

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

as desired. To prove Eqn. 16.23 we repeat this argument, interchanging
the roles of @xmath and @xmath and use the fact that @xmath is symmetric
in @xmath and @xmath . ∎

###### Lemma 16.12.

  -- -------- -- ---------
     @xmath      (16.24)
  -- -------- -- ---------

###### Proof.

Here, there are two cases to consider. The simpler case is when @xmath .
Here we simply apply the inequality

  -- -------- --
     @xmath   
  -- -------- --

to Eqn. 16.22 , and conclude that @xmath .

Next, we would like to prove that

  -- -------- -- ---------
     @xmath      (16.25)
  -- -------- -- ---------

Here we use Lemma 16.11 with @xmath and @xmath , the maximally refined
partition. Note that @xmath and @xmath . Thus

  -- -------- --
     @xmath   
  -- -------- --

establishing Eqn. 16.25 . ∎

###### Lemma 16.13.

If @xmath then @xmath . If, for any @xmath , @xmath with @xmath , either
condition isn’t met (i.e. either @xmath or there does not exist @xmath
such that @xmath ) then

  -- -------- -- ---------
     @xmath      (16.26)
  -- -------- -- ---------

for @xmath .

###### Proof.

In Lemma 16.14 , we introduce the @xmath matrix @xmath with the property
that

  -- -------- -- ---------
     @xmath      (16.27)
  -- -------- -- ---------

for all @xmath and @xmath where @xmath and @xmath are the free indices
of @xmath and @xmath . This is similar to the matrix @xmath introduced
in Lemma 16.9 except only the free indices of @xmath are considered.

For @xmath , Lemma 16.14 implies that @xmath , or equivalently @xmath
for all @xmath . Using @xmath , @xmath .

Otherwise we have @xmath with @xmath . For all these, Lemma 16.14
implies that @xmath is nonzero (for @xmath , no entries in @xmath can be
@xmath or @xmath so @xmath is equivalent to @xmath ). Fix an @xmath for
which the @xmath row of @xmath is nonzero. We wish to count the number
of @xmath such that @xmath . Assume that each @xmath divides @xmath and
is nonnegative; if not, we can replace @xmath with @xmath by a suitable
change of variable for @xmath .

Now choose an arbitrary @xmath such that @xmath . For any values of
@xmath , there are @xmath choices of @xmath such that @xmath . Thus,
there are @xmath choices of @xmath such that @xmath . Substituting this
into Eqn. 16.21 (which we can trivially modify to apply for @xmath
rather than just @xmath ), we find that

  -- -------- --
     @xmath   
  -- -------- --

thus establishing Eqn. 16.26 . ∎

###### Lemma 16.14.

Let @xmath be the matrix such that @xmath for all @xmath and @xmath
where @xmath and @xmath are the free indices of @xmath and @xmath . Then
@xmath if and only if @xmath for some @xmath .

###### Proof.

We first consider @xmath for the “if” direction. Note that for any
@xmath , we have

  -- -------- -- ---------
     @xmath      (16.28)
  -- -------- -- ---------

This implies that @xmath . Now, choose any @xmath and @xmath . Then for
any @xmath and @xmath , @xmath . This means Eqn. 16.28 holds for this
case so @xmath also.

On the other hand, suppose that @xmath . We will argue that this implies
the existence of a permutation @xmath such that @xmath , thus
establishing the “only if” direction.

Let @xmath (resp. @xmath ) denote the @xmath block of @xmath (resp.
@xmath ). Then

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is defined to be

  -- -------- --
     @xmath   
  -- -------- --

If @xmath then for each @xmath we have

  -- -------- -- ---------
     @xmath      (16.29)
  -- -------- -- ---------

Denote the meet of @xmath and @xmath , @xmath to be the greatest lower
bound of @xmath and @xmath , or equivalently the unique partition with
the fewest blocks that satisfies @xmath and @xmath . The blocks of
@xmath are simply all of the nonempty sets @xmath , for @xmath and
@xmath . Thus, Eqn. 16.29 implies that each block of @xmath contains an
equal number of indices from @xmath as it does from @xmath . This
implies the existence of a permutation @xmath such that @xmath is
contained in a single block of @xmath for each @xmath . Equivalently
@xmath , implying that @xmath and @xmath . ∎

##### 16.5 Proof of Lemma 4.2.2

###### Proof.

We would like to show that, for any unit vector @xmath , @xmath . Our
strategy will be to calculate the matrix elements of @xmath in the
@xmath and @xmath bases. While the @xmath states are orthonormal, we
will see that the @xmath matrix elements are easier to calculate. We
then use Möbius functions to express @xmath in terms of @xmath .

Consider the matrix @xmath . It has @xmath unit eigenvalues,
corresponding to the @xmath -dimensional space @xmath . Call the @xmath
largest eigenvalue @xmath . We bound @xmath with

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (16.30)
  -- -------- -------- -- ---------

We divide the terms in Eqn. 16.30 into four types.

1.  The leading-order contribution comes from the @xmath terms of the
    form @xmath for @xmath . We bound them with the trivial upper bound

      -- -------- -- ----------
         @xmath      (16.31a)
      -- -------- -- ----------

    (which turns out to be nearly tight). We will then show that the
    remaining terms are all @xmath .

2.  If @xmath then

      -- -------- -------- -- ----------
         @xmath   @xmath      
                  @xmath      
                  @xmath      (16.31b)
      -- -------- -------- -- ----------

    where in the last line we have used the fact that @xmath .

3.  If @xmath then we will show that

      -- -------- -- ----------
         @xmath      (16.31c)
      -- -------- -- ----------

4.  If @xmath but either @xmath or there is no @xmath satisfying @xmath
    , then we will show that

      -- -------- -- ----------
         @xmath      (16.31d)
      -- -------- -- ----------

To establish these last two claims, we will find it useful to express
@xmath in terms of the various @xmath states.

Lemmas 16.12 and 16.13 can now be used together with the Möbius function
to bound @xmath . First, suppose @xmath . Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by Lemma 16.12 . Then using by Corollary 16.6 we find

  -- -------- -------- -- ---------
     @xmath   @xmath      (16.32)
              @xmath      
  -- -------- -------- -- ---------

In the last step, we have assumed that @xmath , so that @xmath for any
@xmath . We have also made use of the fact that (still assuming @xmath )
Eqn. 16.32 is maximised when @xmath , and in particular, when one of
@xmath , @xmath is equal to @xmath and the other is equal to 1.

A similar analysis applies to the pairs @xmath with @xmath , but with
@xmath . In this case,

  -- -------- -------- -- ---------
     @xmath   @xmath      
              @xmath      (16.33)
  -- -------- -------- -- ---------

We now use Lemmas 16.13 and 16.12 to bound each of the two terms. For
the first term, we use Eqn. 16.26 to upper bound it with @xmath . For
each choice of @xmath and @xmath in the second sum, we have @xmath .
Thus we can upper bound the absolute value of the second term in Eqn.
16.33 with

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

We combine the two terms and square to establish Eqn. 16.31d .

We now put together the components from Eqn. 16.31 to upper bound Eqn.
16.30 , and find that

  -- -------- --
     @xmath   
  -- -------- --

implying that @xmath . This concludes the proof of Lemma 16.2 . ∎

#### 17 Conclusions

We have shown how efficient quantum tensor product expanders can be
constructed from efficient classical tensor product expanders. This
immediately yields an efficient construction of unitary @xmath -designs
for any @xmath . Unfortunately our results do not work for all
dimensions; we require the dimension @xmath to be @xmath . While tighter
analysis of our construction could likely improve this, our construction
does not work for @xmath . Constructions of expanders for all dimensions
remains an open problem.

### Chapter \thechapter Applications of Designs

In this chapter we first survey known applications of designs from a
wide variety of areas. Then we present new results applying designs to
derandomise some large deviation bounds.

#### 18 Review of Applications

As we have already discussed, random unitaries and random states have
many applications. For some of these applications a design is sufficient
since only the first few moments of the distribution are required to be
equal to those of the Haar measure.

##### 18.1 Quantum Cryptography

The first applications we discuss are to quantum cryptography. In
classical cryptography, the one-time pad is the most basic operation
that perfectly encrypts a message using a key that is the same length as
the message. In quantum cryptography the analogue is a quantum operation
@xmath such that for all input states @xmath , @xmath with the
requirement that given a secret key Bob can decode Alice’s message
perfectly. Since all states are encoded to @xmath Eve cannot learn
anything about the message without knowing the key. In this section all
logs will be taken to base @xmath .

If @xmath is the identity, then the map @xmath is a unitary 1-design.
Therefore using @xmath bits of key to label the Pauli operators provides
a quantum one-time pad. In fact, in [ AMTd00 ] it is shown that @xmath
bits of key are also necessary for a quantum one-time pad. Therefore
this unitary 1-design is of optimal size.

It is interesting to note that @xmath bits of key are required rather
than just @xmath for the classical one-time pad. This is related to the
fact that quantum states allow superdense coding [ BW92 ] , which allows
two classical bits to be sent per qubit. Although it is not possible to
use a shorter key for exact encryption, it would be desirable to shorten
the key if we can tolerate Eve learning a small amount of information
about the message. In [ AS04 ] , they consider closeness in the 1-norm,
and set @xmath . They define a map @xmath as an @xmath -approximate
quantum encryption scheme if for all @xmath

  -- -------- -- --------
     @xmath      (18.1)
  -- -------- -- --------

Thus we see an @xmath -approximate unitary 1-design according to (for
example) Definition 5.10 (DIAMOND) suffices. In [ AS04 ] , they present
an efficient construction that satisfies Eqn. 18.1 with @xmath bits of
key. While this does not immediately provide an @xmath -approximate
1-design according to any of our definitions with only @xmath bits of
key, Eqn. 18.1 is a valid definition of an @xmath -approximate unitary
1-design. This key length was further improved by Dickinson and Nayak [
DN06 ] to @xmath and their construction is efficient.

A stronger definition for @xmath -approximate encryption was given in [
HLSW04 ] . They define a map @xmath to be an @xmath -approximate quantum
encryption scheme if for all @xmath

  -- -------- -- --------
     @xmath      (18.2)
  -- -------- -- --------

This implies the 1-norm bound in Eqn. 18.1 but a dimension factor is
lost when converting the other way. This could be used as yet another
approximate 1-design definition. However, there are no known efficient
constructions of such @xmath -norm randomising maps. In [ HLSW04 ] they
provide an inefficient randomised construction with key length @xmath .
Their method is to show that with non-zero probability random unitaries
suffice.

This result was improved by Aubrun in [ Aub09 ] to reduce the key length
to @xmath . The method is the same as [ HLSW04 ] except the analysis is
tighter. Aubrun also makes a step towards finding an efficient
construction by showing that the unitaries can be Pauli matrices, which
can be implemented efficiently, although the sampling is still
inefficient.

Besides cryptographic applications, it is shown in [ HLSW04 ] that
@xmath -norm randomising maps can be used to hide correlations from
local operations and classical communication (LOCC) and have
applications to data hiding (see later) and locking of classical
correlations [ DHL @xmath 04 ] , whereby classical correlations can be
hidden but unlocked by a very short key.

The last cryptography example we give is that of non-malleable
encryption given in [ ABW09 ] . Here the authors not only consider
hiding information from Eve but they also require that she cannot change
the message. Of course, Eve could always replace the message with some
fixed state or do nothing, so according to [ ABW09 ] , an encryption
scheme is non-malleable if these (or a convex combination) are the only
operations Eve can perform on the encoded data. The main result of this
paper is that a unitary 2-design is necessary and sufficient. They then
show, as do Gross et al. [ GAE07 ] , that a 2-design requires at least
@xmath unitaries i.e. the key must be at least @xmath bits long. Even
for approximate encryption (which can be seen as an approximate
2-design) the key length is essentially the same.

##### 18.2 Measurement

In some cases a random measurement is a good choice but cannot be
performed efficiently. One example of such a result is:

###### Theorem 18.1 (Sen, [Sen05]).

Let @xmath and @xmath be any mixed states with @xmath for a sufficiently
large constant @xmath . Here, @xmath is the rank of the state @xmath .
Then

  -- -------- -- --------
     @xmath      (18.3)
  -- -------- -- --------

where @xmath is an orthonormal basis picked from the Haar measure. Here,
@xmath is the probability distribution of outcomes according to the POVM
@xmath .

Since a large 1-norm distance between probability distributions means
the distributions are easily distinguishable, this result places a lower
bound on the distinguishability of the states @xmath and @xmath in terms
of their 2-norm distance.

In [ AE07 ] , Ambainis and Emerson show that a POVM made from a state
4-design achieves the bound in Eqn. 18.3 . In fact, an @xmath
-approximate state 4-design suffices, provided that @xmath . To ensure
the POVM is suitably normalised, we insist here that the approximate
4-design is also an exact 1-design rather than an @xmath -approximate
1-design, which is all that Definition 5.9 ensures.

In [ IR06 ] , Iblisdir and Roland consider a slightly different
measurement problem for which a random measurement achieves the best
outcome. The setting is that Alice chooses a random pure state (the
authors only consider the case that Alice’s system is 2-dimensional
i.e. a single qubit) from the Haar measure and makes @xmath copies of
it. Bob then has to find a state with high overlap with the given state.
The POVM that achieves the optimum is [ MP95 ]

  -- -------- -- --------
     @xmath      (18.4)
  -- -------- -- --------

From Lemma 5.2 , the average of this is the projector onto the symmetric
subspace of @xmath qubits. While this is not the identity, no other
outcomes are possible because the input state is symmetric. The states
in the POVM can be replaced by a state @xmath -design and in [ IR06 ]
the authors present a construction of a state @xmath -design for all
@xmath , although only for one qubit.

##### 18.3 Average Gate Fidelity

When implementing a quantum operation, we would like to know how far the
actual operation is from the desired. One way of measuring this is the
average gate fidelity [ Nie02 ] :

  -- -------- -- --------
     @xmath      (18.5)
  -- -------- -- --------

where @xmath is the operation implemented and @xmath is the desired
unitary. We see immediately, following [ DCEL06 ] , that the integrand
is a balanced polynomial of degree 2 so the Haar measure on states can
be replaced by a state 2-design. We can even use an approximate design
if the average fidelity only needs to be known approximately. By
repeatedly sampling from the design we can obtain an estimate of the
average to @xmath accuracy efficiently whereas naively sampling random
states will not be efficient.

##### 18.4 Data Hiding

Data hiding was introduced by Terhal, DiVincenzo and Leung [ TDL01 ,
DLT02 ] as a fundamentally quantum concept. The setting is that Alice
and Bob share a quantum state which contains secret bits. However, the
state is chosen so that if they can only communicate using LOCC then
they cannot learn this secret bit. To encode one secret bit, the “hider”
constructs one of two orthogonal mixed states @xmath and @xmath and
hands half to Alice and the other half to Bob. @xmath is the state with
@xmath random Bell pairs chosen subject to the constraint that the
number of singlets is even. @xmath is the same state except with an odd
number of singlets. The parameter @xmath controls the degree of
security.

The way that designs help here is in the construction of these states
using minimal resources. The authors show that @xmath can be obtained
from twirling any initial pure state of the form @xmath ² ² 2 By unitary
invariance of the Haar measure, the choice of @xmath does not affect the
resultant state. :

  -- -------- -- --------
     @xmath      (18.6)
  -- -------- -- --------

@xmath can be created from @xmath .

The authors consider replacing the Haar integral with a sum over a
unitary 2-design. If errors can be tolerated then an approximate
2-design can be used and the state can be prepared efficiently.

##### 18.5 Decoupling and Evolution of Black Holes

For various tasks in quantum Shannon theory, it is desirable to decouple
a system from the environment. In [ HHYW07 ] and [ ADHW06 ] , it is
shown that for most random unitaries applied to the system the resulting
overall state is close to a product state.

The setting is that there is a system @xmath with two parts @xmath and
@xmath . The environment is @xmath . Let the initial state be @xmath and
let

  -- -------- -- --------
     @xmath      (18.7)
  -- -------- -- --------

Then we have

###### Theorem 18.2 ([Adhw06], Theorem 4.2).

  -- -------- -- --------
     @xmath      (18.8)
  -- -------- -- --------

where @xmath , etc..

The proof uses the 2-norm squared, which is a polynomial of degree 2 in
the matrix elements of the random unitary. Therefore the same result
holds when @xmath is selected from a unitary 2-design instead and, as
above, an approximate design can be used to allow an efficient
implementation. This allows the encoding circuits in [ ADHW06 ] to be
made efficient although unfortunately the decoding circuits are still
inefficient.

Decoupling has also been used in the study of the evolution of black
holes. While many aspects of quantum gravity are not understood, some
attempts have been made to understand how black holes leak information.
Two examples are by Hayden and Preskill [ HP07 ] and Sekino and Susskind
[ SS08 ] . We concentrate on the approach in [ HP07 ] here. The idea is
that Alice wishes to destroy some quantum information by throwing it
into a black hole. However, Bob has been watching it and storing the
Hawking radiation emitted. The question they ask is how long does Bob
have to wait before he can recover Alice’s information.

Imagine that Alice’s information is maximally entangled with a system
@xmath held by Charlie. Should Bob acquire a state from the emitted
radiation that is maximally entangled with @xmath then we say he has
successfully recovered Alice’s information. Decoupling is used because,
if what remains of the black hole after some evaporation is uncorrelated
with @xmath , then the emitted radiation must be maximally entangled
with @xmath and Bob has succeeded. We therefore require that the
evolution of the black hole produces a decoupling unitary. If the
evolution is random then, using Theorem 18.2 , this will likely happen,
provided enough radiation has been emitted. In fact, if Bob holds a
system that is maximally entangled with the black hole’s internal state
before Alice throws in her message, then he can recover her state with
fidelity @xmath by reading in only the @xmath qubits emitted after Alice
deposits her information, where @xmath is the number of qubits in
Alice’s message.

This model is not physically realistic because most unitaries cannot be
implemented efficiently so the black hole would take far too long to
apply the decoupling unitary. However, as we said above, only a 2-design
is required. In fact, in [ HP07 ] they consider the case that the
evolution of a black hole is a local random quantum circuit. This is
similar to the random circuits discussed in Chapter I except they assume
that the unitaries are only applied to nearest-neighbour qubits. Should
the random circuit converge to a 2-design quick enough (as Hayden and
Preskill conjecture) then the evolution will be sufficiently fast for
Bob to find Alice’s state. While our results do not prove this they
could readily be extended to cover the local case considered here.

##### 18.6 Applications for Larger @xmath

So far we have only used @xmath -designs for @xmath . However, the
higher @xmath is the more similar a @xmath -design is to a random
unitary. In the next section we consider replacing random unitaries with
@xmath -designs in large deviation bounds, thus finding applications for
larger @xmath .

#### 19 Derandomising Large Deviation Bounds

The remainder of this chapter has been published previously as [ Low09a
] .

There are many results in quantum information theory that show generic
properties of states or unitaries (e.g. [ HLW06 , HLSW04 ] ). Often,
these results say that, with high probability, a random state or unitary
has some property, for example high entropy. However, as we have seen
above, neither random unitaries nor random states can be implemented
efficiently. This limits the usefulness of such results since no
physical systems will behave truly randomly. To make such results more
physically relevant, it would be desirable to show that these properties
are generic properties of unitaries from some natural distribution that
can be implemented efficiently. Only then could we conclude that we
would expect to see such properties in natural systems.

In many cases, the generic properties of unitaries are desirable but
randomised constructions given by the large deviation bounds are
inefficient. We would like to come up with distributions which can be
implemented efficiently that have similar generic properties. One
example where the best known construction is an inefficient randomised
one is the @xmath -norm randomising map (see Section 18.1 ). Another
example is locking of classical correlations [ DHL @xmath 04 , HLSW04 ]
, which is a quantum phenomenon whereby a small amount of communication
can greatly enhance the classical correlation between two parties. To
prove the randomised constructions, the authors show that, with some
non-zero probability, random unitaries have the required property.
However, there are no known efficient constructions of unitaries with
these properties. If, on the other hand, we could show that unitaries
drawn randomly from a set that can be implemented efficiently have the
property with non-zero probability, we could move an important step
closer to finding efficient constructions. (It would not actually
provide an efficient construction unless we could find an efficient
sampling method.) In fact, for the case of @xmath -norm randomisation,
this was done by Aubrun in [ Aub09 ] .

In this section we continue the theme of replacing the Haar measure with
a @xmath -design. The reason for using @xmath -designs is two-fold.
Firstly, because the first @xmath moments are the same we would expect
similar (although weaker) measure concentration results. Secondly, for
@xmath (when the design is on @xmath qubits), we might expect to be able
to implement the @xmath -design efficiently (i.e. in @xmath time).
Indeed, for @xmath , we can use the construction from Chapter I ,
provided we allow for approximate designs. However, in the applications
we consider here we can always make the approximation good enough to
make the error negligible.

Not only can @xmath -designs be constructed efficiently, they may even
be the product of generic dynamics. In Chapter I , we show that random
quantum circuits quickly converge to a 2-design for a quite general
model of such circuits. We also conjecture in Chapter I that random
circuits give @xmath -designs for @xmath and @xmath in polynomial time.
If a physical system can be accurately modelled by a random circuit
then, assuming this conjecture, the naturally occurring states will be
@xmath -designs rather than fully random states.

We now summarise some related results in this area. Smith and Leung [
SL06 ] and Dahlsten and Plenio [ DP06 ] found large deviation bounds for
stabiliser states. They showed that, in certain regimes, stabiliser
states are very likely to have large entanglement. Stabiliser states are
state 2-designs so our results can be seen as a generalisation of this
to @xmath -designs for @xmath and to other problems. There are also some
recent classical results related to the present work. Alon and Nussboim
[ AN08 ] consider replacing full randomness with @xmath -wise
independence, a classical analogue of @xmath -designs, in random graph
theory. They show that @xmath -wise independent random graphs with
@xmath ( @xmath is the number of vertices) have similar generic
properties to fully random graphs.

In the remainder of this chapter, unless otherwise stated, we will use
the definition of an @xmath -approximate unitary design given in terms
of monomials, as in Definition 5.13 . Using the tensor product expander
construction of Chapter I together with Lemma 5.14 gives an efficient
construction for @xmath for this definition.

##### 19.1 Introductory Problem: Entanglement of a 2-design

We now illustrate our main idea by showing a large deviation bound for
the entanglement of a 2-design, but in a different way to [ SL06 , DP06
] .

It has been known for a long time that random states are highly
entangled across any bipartition [ Pag93 , FK94 , San95 ] . Further, in
[ HLW06 ] , it is shown that random unitaries generate almost maximally
entangled states with high probability. However, generating random
states is inefficient so it is an interesting question to ask if random
efficiently obtainable states are highly entangled.

Let the system be @xmath , where we label the two systems @xmath and
@xmath . Let the dimensions be @xmath and @xmath and @xmath . Let the
overall initial state be any fixed pure state @xmath . Then consider
applying a random unitary @xmath to @xmath to get the state @xmath .
Then the von Neumann entropy @xmath of the reduced state @xmath is close
to @xmath (the maximal) with high probability:

###### Theorem 19.1 ([Hlw06] Theorem 3.3).

Let @xmath . Then for unitaries chosen from the Haar measure

  -- -------- -- --------
     @xmath      (19.1)
  -- -------- -- --------

where @xmath and @xmath .

Now, consider choosing the unitary from a 2-design instead. Later on
(Lemma 21.1 ), we show that @xmath . Since purity is a polynomial of
degree @xmath , it does not matter if we take the expectation over the
Haar measure or the 2-design. We now apply Markov’s inequality:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Using the bound @xmath and some manipulations (the details are in
Section 21 ), this can be written as

  -- -------- -- --------
     @xmath      (19.2)
  -- -------- -- --------

where @xmath is as in Theorem 19.1 . This bound is much weaker than the
bound in Theorem 19.1 and, in particular, does not show stronger
concentration as @xmath increases. Later in the chapter, we will show
that choosing unitaries from a @xmath -design with larger @xmath will
give a much stronger bound that does give sharp concentration results
for large @xmath .

##### 19.2 Main Results

We will now state our main results.

Our most general result is:

###### Theorem 19.2.

Let @xmath be a polynomial of degree @xmath . Let @xmath where @xmath
are monomials and let @xmath . Suppose that @xmath has probability
concentration

  -- -------- -- --------
     @xmath      (19.3)
  -- -------- -- --------

and let @xmath be an @xmath -approximate unitary @xmath -design. Then

  -- -------- -- --------
     @xmath      (19.4)
  -- -------- -- --------

for integer @xmath with @xmath .

We therefore take a bound for Haar random unitaries of the form Eqn.
19.3 and turn it into a bound for @xmath -designs. Often, we will use
Levy’s Lemma (Lemma 20.2 ) to give the initial concentration bound in
Eqn. 19.3 . In this case, @xmath (provided the Lipschitz constant (see
later) is constant).

We then apply this to entropy, as a generalisation of Section 19.1 . We
go via the 2-norm since the entropy function is not a polynomial. We
find

###### Theorem 19.3.

Let @xmath be a @xmath -approximate unitary @xmath -design on dimension
@xmath with @xmath . Let @xmath and @xmath and @xmath . Then

  -- -------- -- --------
     @xmath      (19.5)
  -- -------- -- --------

where @xmath and @xmath is the exponential function base 2.

We choose a @xmath -design for @xmath since this is (up to constants)
the largest @xmath for which we have an efficient unitary @xmath -design
construction (using the construction of Chapter I ).

We then move on to apply our results to ideas in statistical mechanics
from Popescu et al. [ PSW06 ] . In this paper, the authors show that,
for almost all pure states of the universe, any subsystem is very close
to the canonical state, which is the state obtained by assuming a
uniform distribution over all allowed states of the universe (defined in
Eqn. 22.2 ). This could be achieved if the dynamics of the universe
produced a random unitary, but this would take exponential time in the
size of the universe. We show that the random unitary can be replaced by
a @xmath -design, showing that the canonical state can be reached in
polynomial time:

###### Theorem 19.4.

Let @xmath be the canonical state of the system (defined in Eqn. 22.2 )
and @xmath be the state after choosing a unitary from an @xmath
-approximate @xmath -design. Let @xmath be the dimension of the
universe’s Hilbert space subject to the arbitrary constraint @xmath
(normally this will be a total energy constraint). Then for @xmath ,
@xmath

  -- -------- -- --------
     @xmath      (19.6)
  -- -------- -- --------

Finally, we use results from [ GFE09 ] to show that most states in an
@xmath -approxi-mate state @xmath -design on @xmath qubits are useless
for measurement-based quantum computing, in the sense that any
computation using such states could be simulated efficiently on a
classical computer. We do this, following [ GFE09 ] , by showing that
the states are so entangled that the measurement outcomes are
essentially random.

##### 19.3 Optimality of Results

An important question is how close our results are to optimal, in terms
of their scaling with dimension @xmath . In Theorem 19.2 , we will
normally have @xmath so for @xmath constant, we obtain polynomial
bounds, rather than the exponential bounds for full randomness. This is
to be expected:

###### Theorem 19.5.

Let @xmath be an @xmath -approximate unitary @xmath -design. Suppose
also that it is discrete i.e. contains a finite number @xmath of
unitaries. Let @xmath be any function on matrix elements of @xmath and
@xmath be any constant. Then either @xmath for all @xmath in @xmath or
for some @xmath

  -- -------- -- --------
     @xmath      (19.7)
  -- -------- -- --------

where @xmath is the probability of choosing the least probable unitary
from @xmath . If the probability is uniform, @xmath .

###### Proof.

There exists at least one @xmath such that @xmath for some @xmath ; the
probability of selecting one such @xmath is at least @xmath . ∎

###### Corollary 19.6.

Our results are polynomially related to the optimal (i.e. the optimal
bounds can be obtained by raising ours to a constant power).

###### Proof.

Our results apply for any design, so must obey the bound in Theorem 19.5
for all designs. The unitary design construction we use (from Chapter I
using Lemma 5.14 ) has @xmath hence the bounds cannot scale better than
this. ∎

We can also almost recover the tail bound for full randomness in Theorem
19.2 . Suppose for simplicity that we have an exact design (i.e. @xmath
), so that

  -- -------- --
     @xmath   
  -- -------- --

The optimal @xmath is @xmath , which gives

  -- -------- --
     @xmath   
  -- -------- --

So our result allows us to interpolate from Markov’s inequality, which
gives weak bounds, all the way to full Haar randomness and is within a
polynomial correction of optimal for the full range.

The remainder of the chapter is organised as follows. In Section 20 we
present our main technique for finding large deviation bounds for @xmath
-designs. We then apply this to entropy in Section 21 , to ideas in
statistical mechanics in Section 22 and to using @xmath -designs for
measurement-based quantum computing in Section 23 . We then conclude in
Section 24 .

#### 20 Main Technique

The main idea in this chapter can be summarised in three steps. Let
@xmath be a balanced polynomial of degree @xmath in the matrix elements
of a unitary @xmath . Then to get a concentration bound on @xmath when
@xmath is chosen from a @xmath -design:

1.  Find some measure concentration result for @xmath when the unitaries
    are chosen uniformly at random from the Haar measure. Normally
    @xmath will be the expectation of @xmath .

2.  Use this to bound the moments @xmath for some integer @xmath .

3.  Then use Markov’s inequality and the fact that for a (approximate)
    @xmath -design the moments are (almost) the same as for uniform
    randomness. We then optimise the bound for @xmath , which will often
    involve setting @xmath close to the maximum, @xmath .

We will now work through each of these steps and finish with a proof of
Theorem 19.2 .

##### 20.1 Step 1: Concentration for uniform randomness

For the first step, we will often start with Levy’s Lemma. This states,
roughly speaking, that slowly varying functions in high dimensions are
approximately constant. We quantify ‘slowly varying’ by the Lipschitz
constant:

###### Definition 20.1.

The Lipschitz constant @xmath (with respect to the Euclidean norm) for a
function @xmath is

  -- -------- -- --------
     @xmath      (20.1)
  -- -------- -- --------

Then we have Levy’s lemma:

###### Lemma 20.2 (Levy, see e.g. [Led01]).

Let @xmath be an @xmath -Lipschitz function on @xmath with mean @xmath .
Then

  -- -------- -- --------
     @xmath      (20.2)
  -- -------- -- --------

where @xmath can be taken to be @xmath .

##### 20.2 Step 2: A bound on the moments

Levy’s Lemma says that @xmath is close to its mean. This means that
@xmath should be small. We will bound the moments for slightly more
general concentration results:

###### Lemma 20.3.

Let @xmath be any random variable with probability concentration

  -- -------- -- --------
     @xmath      (20.3)
  -- -------- -- --------

(Normally @xmath will be the expectation of @xmath , although the bound
does not assume this.) Then

  -- -- -- --------
           (20.4)
  -- -- -- --------

for any @xmath .

###### Proof.

This proof is based on the proof of an analogous result by Bellare and
Rompel [ BR94 ] , Lemma A.1.

Note that, for any random variable @xmath ,

  -- -------- -- --------
     @xmath      (20.5)
  -- -------- -- --------

Therefore

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where in the last line we used the assumed large deviation bound Eqn.
20.3 . To evaluate this integral, use the change of variables @xmath to
get

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

##### 20.3 Step 3: A concentration bound for a @xmath-design

Now we show how to obtain a measure concentration result for polynomials
when the unitaries are selected from an approximate @xmath -design. We
first show that the moments of @xmath for @xmath a polynomial are close
to the Haar measure moments:

###### Lemma 20.4.

Let @xmath be a balanced polynomial of degree @xmath and @xmath be any
constant. Let @xmath where each @xmath is a monomial. Let @xmath . Then
for @xmath an integer with @xmath and @xmath an @xmath -approximate
@xmath -design,

  -- -------- -- --------
     @xmath      (20.6)
  -- -------- -- --------

###### Proof.

For simplicity, we assume that @xmath and @xmath are real. Our proof
easily generalises to the complex case.

Firstly we calculate @xmath using the multinomial theorem:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We now calculate @xmath :

  -- -- -------- --
        @xmath   
        @xmath   
        @xmath   
        @xmath   
        @xmath   
  -- -- -------- --

Now we can simply apply Markov’s inequality to prove Theorem 19.2 .

###### Proof of Theorem 19.2.

Apply Markov’s inequality and Lemmas 20.3 and 20.4 :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We finish this section with two remarks. Firstly, provided @xmath (the
sum of the absolute value of all the coefficients) is at most
polynomially large in @xmath , we can choose @xmath to be polynomially
small to cancel this at no change to the asymptotic efficiency.
Secondly, when applying the theorem we will optimise the choice of
@xmath (and normally choose @xmath ). Often @xmath and the optimal
choice of @xmath is often @xmath as well. However, we will not take
@xmath so large because we can only implement an efficient @xmath
-design for @xmath .

#### 21 Application 1: Entropy of a @xmath-design

We now apply the above to show that most unitaries in a @xmath -design
generate large amounts of entropy across any bipartition, provided the
dimensions are sufficiently far apart. This means that, for any initial
state, for most choices of a unitary from a @xmath -design applied to
the state, the resulting state will be highly entangled. We go via the
purity of the reduced density matrix, since the entropy function is not
a polynomial.

We will call the two systems @xmath (the ‘system’) and @xmath (the
‘environment’) and calculate the purity of the reduced state. That the
purity, @xmath , is a balanced polynomial of degree 2 is easily seen by
noting that the trace is linear and the reduced state is squared.
However, we should check that there are not too many terms or terms with
large coefficients. To do this, we should calculate @xmath to apply
Theorem 19.2 .

There is a general method for calculating @xmath which we will use.
Write @xmath for monomials @xmath . To evaluate @xmath , calculate
@xmath where @xmath is the matrix with all entries equal to @xmath (so
that @xmath ) and replace @xmath with @xmath . Using this here we find

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We now calculate the expected purity:

###### Lemma 21.1.

The expected purity of the reduced state is @xmath , where @xmath is the
dimension of subsystem @xmath and @xmath is the dimension of subsystem
@xmath .

###### Proof.

We have

  -- -- -- --------
           (21.1)
  -- -- -- --------

where @xmath is swap acting between systems @xmath and @xmath . By
linearity of the trace, we can commute the @xmath through and use @xmath
to find

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Working out the higher moments in this way is difficult (although has
been done in [ Gir07 ] ) so we use Levy’s Lemma and Lemma 20.3 . To use
Levy’s Lemma, all we have to do is find the Lipschitz constant for the
purity:

###### Lemma 21.2.

The Lipschitz constant for purity is @xmath .

###### Proof.

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Now we use @xmath to find

  -- -------- --
     @xmath   
  -- -------- --

using the fact that the purity is upper bounded by 1. ∎

###### Lemma 21.3.

For @xmath and @xmath an integer with @xmath and @xmath an @xmath
-approximate @xmath -design,

  -- -------- -- --------
     @xmath      (21.2)
  -- -------- -- --------

###### Proof.

We use the fact that von Neumann entropy is lower bounded by the Renyi
2-entropy i.e. @xmath :

  -- -------- -- --------
     @xmath      (21.3)
  -- -------- -- --------

Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

using Theorem 19.2 in the last line. ∎

We have written this in a more convenient form in Theorem 19.3 which is
proved in Section 25 . This is to be compared with the Haar random
version Theorem 19.1 . As expected, we have @xmath appearing in the
exponent rather than @xmath . Note also that our bound does not work
well for @xmath . In fact, in this case, we do not get a bound that
improves with dimension. In order to achieve such a bound in this regime
a different technique will be necessary.

#### 22 Application 2: @xmath-designs and Statistical Mechanics

We can also apply these ideas to partially derandomise some of the
arguments on the foundations of statistical mechanics in [ PSW06 ] . In
this paper, the authors develop the idea that the uncertainty in
statistical mechanics comes from entanglement rather than the
traditional assumption of the principle of equal a priori probabilities.
They consider the universe being in a pure quantum state and that the
uncertainty in the state of a subsystem comes from the entanglement
between this system and the rest of the universe.

The setting is that there is an arbitrary global linear constraint
@xmath . Often this will be a total energy constraint although this is
not assumed. Let the Hilbert space of states satisfying @xmath be @xmath
. Then let the system and environment Hilbert spaces be @xmath and
@xmath respectively. Then

  -- -------- -- --------
     @xmath      (22.1)
  -- -------- -- --------

Let the dimensions be @xmath , @xmath and @xmath and let @xmath . Note
that @xmath , unlike in the above where we took @xmath . Normally we
will have @xmath . The principle of equal a priori probabilities says
that the state of the universe is @xmath which implies the subsystem
state is the canonical state, given by

  -- -------- -- --------
     @xmath      (22.2)
  -- -------- -- --------

The main result of [ PSW06 ] (the ‘principle of apparently equal a
priori probabilities’) is that, for almost all pure states of the
universe, the subsystem state is almost exactly the canonical state.

###### Theorem 22.1 (Theorem 1 of [Psw06]).

For a randomly chosen state @xmath and arbitrary @xmath , the distance
between the reduced density matrix of the system @xmath and the
canonical state @xmath (Eqn. 22.2 ) is given probabilistically by

  -- -------- -- --------
     @xmath      (22.3)
  -- -------- -- --------

where @xmath and @xmath .

This result gives compelling evidence to replace the principle of equal
a priori probabilities with the principle of apparently equal a priori
probabilities, but it does not address the problem of how the system
reaches this state. It will take an extremely (exponentially) long time
for the universe to reach a random pure state, in contrast to the
observed fact that thermalisation occurs quickly. Here, we show that for
almost all unitaries in a @xmath -design applied to the universe, the
subsystem state is close to the canonical state. Since these unitaries
can be implemented and sampled efficiently, this means that equilibrium
could be reached quickly to match observations.

We are now ready to show that a @xmath -design gives a small @xmath .
First, we have to modify Lemma 20.3 slightly:

###### Lemma 22.2.

Let @xmath be any non-negative random variable with probability
concentration

  -- -------- -- --------
     @xmath      (22.4)
  -- -------- -- --------

where @xmath . Then

  -- -------- -- --------
     @xmath      (22.5)
  -- -------- -- --------

for any @xmath .

The proof is very similar to the proof of Lemma 20.3 .

Now we state and prove the main result in this section:

###### Theorem 22.3.

Let @xmath be an @xmath -approximate unitary @xmath -design. Then

  -- -------- -- --------
     @xmath      (22.6)
  -- -------- -- --------

In particular, with @xmath , @xmath ,

  -- -------- -- --------
     @xmath      (22.7)
  -- -------- -- --------

Again, we need @xmath to be polynomially smaller than @xmath to obtain
non-trivial bounds.

###### Proof.

We go via the 2-norm and use Lemmas 22.2 and 20.4 .

We have from Theorem 22.1 that

  -- -------- -- --------
     @xmath      (22.8)
  -- -------- -- --------

where @xmath . Since @xmath ,

  -- -------- -- --------
     @xmath      (22.9)
  -- -------- -- --------

We now apply Lemma 22.2 to get

  -- -------- -- ---------
     @xmath      (22.10)
  -- -------- -- ---------

So for @xmath , using Markov’s inequality and Lemma 20.4 (with @xmath )
on the polynomial @xmath :

  -- -------- -- ---------
     @xmath      (22.11)
  -- -------- -- ---------

Here, we used an estimate of @xmath , the sum of the moduli of the
coefficients:

  -- -------- -- ---------
     @xmath      (22.12)
  -- -------- -- ---------

which we obtain via a similar calculation to that in Section 21 .

Now we go back to the 1-norm, using @xmath to get

  -- -------- -------- -- ---------
     @xmath   @xmath      (22.13)
              @xmath      (22.14)
  -- -------- -------- -- ---------

To obtain the result in Eqn. 22.6 , we just use @xmath and set @xmath .

To prove the simplified version, first use, as in Section 21 , that
@xmath for @xmath . This is implied by @xmath . We then set @xmath to
find

  -- -------- -- ---------
     @xmath      (22.15)
  -- -------- -- ---------

Then, using @xmath , with @xmath , we obtain the simplified result Eqn.
22.7 . ∎

#### 23 Application 3: Using @xmath-designs for Measurement-Based
Quantum Computing

Here we apply our ideas to partially derandomise some results of Gross,
Flammia and Eisert in [ GFE09 ] and Bremner, Mora and Winter in [ BMW09
] . The main result in these two papers is that most states do not offer
any advantage over classical computation when used in the
measurement-based quantum computing (MBQC) model. In MBQC, a classical
computer is given access to a large quantum state on which it can do
single qubit measurements. Some states allow for universal quantum
computation whereas others do not add any extra power to the classical
computer. These results are concerned with the question of
characterising which states do and do not work. Showing that random
states do not give any speed up shows that useful states for MBQC are
not generic and so must be carefully constructed.

While the results in these two papers are similar, we will concentrate
on the methods from [ GFE09 ] since their methods are simpler to apply
here. They prove their result by showing that most states are very
entangled in the geometric measure (see Definition 23.1 ). They then use
this to show that the measurement outcomes of even the best possible
measurement scheme are almost completely random. In fact, the state
could be thrown away and the measurement outcomes replaced with random
numbers to solve the computational problem just as efficiently. This
shows that you can classically simulate any quantum computation that
uses these highly entangled states. The measure of entanglement they use
is the geometric measure:

###### Definition 23.1.

The geometric measure of entanglement of a state @xmath is [ Shi95 ,
BL01 ]

  -- -------- -- --------
     @xmath      (23.1)
  -- -------- -- --------

where @xmath is the set of all product states.

They show that any MBQC using a state @xmath with @xmath can be
efficiently simulated classically. They then show that

###### Theorem 23.2 ([Gfe09], Theorem 2).

For @xmath ,

  -- -------- -- --------
     @xmath      (23.2)
  -- -------- -- --------

This shows that most states are useless. We partially derandomise this
result to show that most states in an @xmath -approximate ( @xmath can
be taken as a constant) state @xmath -design have high geometric measure
of entanglement and thus are useless in the same way.

We could apply our technique and use Theorem 19.2 but in this case, it
is simpler to directly bound the probability using Markov’s inequality.

###### Lemma 23.3.

  -- -------- -- --------
     @xmath      (23.3)
  -- -------- -- --------

where @xmath is chosen from an @xmath -approximate state @xmath -design
@xmath , @xmath and a positive integer and @xmath is any fixed state.

###### Proof.

We prove this bound directly using Markov’s inequality:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We now prove the main result in this section:

###### Theorem 23.4.

For @xmath randomly drawn from an @xmath -approximate state @xmath
-design with @xmath

  -- -------- -- --------
     @xmath      (23.4)
  -- -------- -- --------

In particular, for @xmath , @xmath and @xmath ,

  -- -------- -- --------
     @xmath      (23.5)
  -- -------- -- --------

We note that this bound is almost the same as in Theorem 23.2 . It only
works for slightly larger deviations from @xmath , which is why we
obtain a slightly better probability bound. Note also that we can obtain
an exponential bound in @xmath (not @xmath ) because the design is
exponentially large in @xmath .

###### Proof.

This proof closely mirrors the proof of Theorem 2 in [ GFE09 ] . We use
the idea of a @xmath -net. @xmath is a @xmath -net on product states if

  -- -------- -- --------
     @xmath      (23.6)
  -- -------- -- --------

In [ GFE09 ] , it is shown that such a net exists with @xmath . We then
proceed by showing that most states in the state design have small
overlap with every state in the net using the union bound and Lemma 23.3
. Finally, since every state is close to one in the net, we can show
that most states in the design have small overlap with every product
state.

We now formalise the above. Using Lemma 23.3 and the union bound,

  -- -------- -- --------
     @xmath      (23.7)
  -- -------- -- --------

Now, we need to bound

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

We now claim that

  -- -------- -- --------
     @xmath      (23.8)
  -- -------- -- --------

To prove this claim, let @xmath be the state that achieves the supremum
on the left hand side, and let @xmath be the state closest to it in the
@xmath -net. It is shown in [ GFE09 ] that this implies for any @xmath

  -- -------- -- --------
     @xmath      (23.9)
  -- -------- -- --------

Therefore

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

This implies that the supremum over all states in the net must be at
least @xmath to prove the claim.

We can now finish the proof. Set @xmath in Eqn. 23.8 and use Eqn. 23.7
with @xmath to find

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Combining this with the arguments of [ GFE09 ] shows that most states in
a state @xmath -design on @xmath qubits are useless for MBQC. This shows
that even many efficiently preparable states are useless.

#### 24 Conclusions

We have seen how to turn large deviation bounds for Haar-random
unitaries into bounds for @xmath -designs. The main technique was
applied to show that unitaries from @xmath -designs generate large
amounts of entanglement. Then we showed that, if the dynamics of the
universe produced a @xmath -design, the entanglement generated would be
sufficient to reproduce the principle of equal a priori probabilities.
Finally we showed that most states in sufficiently large state designs
are useless for measurement-based quantum computing, in the sense that
computation using them can be efficiently simulated classically.

However, there are other bounds for which our technique does not work.
Since we cannot obtain exponential bounds for polynomially sized
designs, our technique cannot directly derandomise some bounds. Some
results, for example showing that the @xmath -norm of the reduced state
of a random pure state is close to @xmath [ HHL04 ] , are proven by
using an @xmath -net of states and the union bound. Since the @xmath
-net is exponentially large, exponentially small bounds are required. We
do not know how to apply our idea to results of this kind and still have
@xmath . (Note that we could cope with the @xmath -net in Section 23
since it was just a net on product states which is considerably
smaller.)

It is also possible that our ideas could be used to completely
derandomise some constructions (e.g. locking [ HLSW04 , DHL @xmath 04 ]
). If we could show that unitaries drawn from a @xmath -design work with
non-zero probability, and come up with an efficient sampling method,
then we could obtain efficient randomised constructions.

#### 25 Proof of Theorem 5.2.3

Here we prove the more convenient form of Lemma 21.3 stated as Theorem
19.3 .

###### Proof of Theorem 19.3.

Firstly, we will write the left hand side of Eqn. 21.2 in a more useful
way. Using @xmath , we find

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , following the notation in [ HLW06 ] . This means

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

We now simplify the right hand side. Let @xmath . For @xmath , we have
@xmath . We shall also assume that @xmath . This gives us (using @xmath
)

  -- -------- -- --------
     @xmath      (25.1)
  -- -------- -- --------

Now, one can easily show (e.g. by induction on @xmath ) that

  -- -------- -- --------
     @xmath      (25.2)
  -- -------- -- --------

for @xmath . We use this for @xmath and @xmath . The condition is then
@xmath , which we shall assume (we will set @xmath later). We now obtain

  -- -------- -- --------
     @xmath      (25.3)
  -- -------- -- --------

We will now take @xmath , so that the two terms are the same. @xmath is
@xmath so this remains efficient. Now

  -- -------- -- --------
     @xmath      (25.4)
  -- -------- -- --------

Assuming that @xmath , we should take @xmath as large as possible up to
@xmath , when the right hand side is maximised. We then find the result
after further simplification. ∎

## Part II Quantum Learning

### Chapter \thechapter Learning and Testing Algorithms for the Clifford
Group

#### 26 Introduction

A central problem in quantum computing is to determine an unknown
quantum state from measurements of multiple copies of the state. This
process is known as quantum state tomography (see [ NC00 ] and
references therein). By making enough measurements, the probability
distributions of the outcomes can be estimated from which the state can
be inferred. A related problem is that of quantum process tomography,
where an unknown quantum evolution is determined by applying it to
certain known input states. There are several methods for doing this,
including what are known as Standard Quantum Process Tomography [ CN97 ,
PCZ97 ] and Ancilla Assisted Process Tomography [ DLP01 , Leu03 ] .
These methods work by using state tomography on the output states for
certain input states.

However, all these procedures share one important downside: the number
of measurements required increases exponentially with the number of
qubits. This already presents problems even with systems achievable with
today’s technology, for which complete tomographical measurements can
take hours (e.g. [ HHR @xmath 05 ] ) making tomography of larger systems
unfeasible. Unfortunately this exponential cost is necessary to
determine a completely unknown state or process, since there are
exponentially many parameters to measure. To make tomography feasible
for larger systems, we need to find a restriction that requires fewer
measurements, ideally polynomially many.

One way to improve the measurement, or query, complexity is to assume
some prior knowledge of the process. For example, suppose the process
was known to be one of a small number of unitaries, then the task is
just to decide which. This is the approach we take here. As a simple
example, consider being given a black box implementing an unknown Pauli
matrix. By applying this to half a maximally entangled state, the Pauli
can be identified with one query. This is essentially superdense coding
[ BW92 ] and is explained in Section 28.1 . Indeed, if the black box
performed a tensor product of arbitrary Paulis on @xmath qubits then it
too can be identified with just one query.

We extend this to work for elements of the Clifford group (the
normaliser of the Pauli group; see Definition 27.1 ) and show that any
member of the Clifford group can be learnt with @xmath queries, which we
show is optimal. The Clifford group is an important subgroup of the
unitary group that has found uses in quantum error correction and fault
tolerance [ CRSS97 , Sho96 , Got98 ] .

Then generalising further, we show that elements of the Gottesman-Chuang
hierarchy [ GC99 ] (see Definition 27.2 ), also known as the @xmath
hierarchy, can also be learnt efficiently. As the level @xmath
increases, the set @xmath includes more and more unitaries so this
implies ever larger sets can be learnt, although the number of queries
scales exponentially with @xmath . Our methods also work if the unitary
is known to be close to a Clifford (or any element of @xmath for some
known @xmath ) rather than exactly a Clifford.

We also give a Clifford testing algorithm, which determines whether an
unknown unitary is close to a Clifford or far from every Clifford. This
is an extension of the Pauli testing algorithm given in [ MO08 ] .
Indeed, our results are closely related to results in [ MO08 ] and we
use some of the algorithms presented there as ingredients. Our results
can also be compared with [ Aar07 ] , which contains methods to
approximately learn quantum states. Another related result is that of
Aaronson and Gottesman [ AG09 ] , which provides a method of learning
stabiliser states with linearly many copies.

We only consider query complexity although, at least for the Clifford
group results, our methods are computationally efficient too.

The rest of the chapter is organised as follows. In Section 27 , we
define the Pauli and Clifford groups and the Gottesman-Chuang hierarchy.
In Section 28 we present our algorithm for exact learning of Clifford
and @xmath elements. In Section 29 we show how to find the closest
element of @xmath to an unknown unitary. In Section 30 we present our
Clifford testing algorithm and then conclude in Section 31 .

This chapter has been published previously as [ Low09b ] .

#### 27 The Pauli and Clifford Groups and the Gottesman-Chuang Hierarchy

Firstly, we define the Pauli group. Call the set of all Pauli matrices
on @xmath qubits @xmath . We then have @xmath . We write matrices in the
Pauli basis using the normalisation @xmath . To make @xmath into a
group, the Pauli group @xmath , we must include each matrix in @xmath
with phases @xmath .

We can now define the Clifford group:

###### Definition 27.1 (The Clifford group).

The Clifford group is the normaliser of the Pauli group i.e.

  -- -------- --
     @xmath   
  -- -------- --

Then the Gottesman-Chuang hierarchy is a generalisation:

###### Definition 27.2 (The Gottesman-Chuang hierarchy [Gc99]).

Let @xmath be the Pauli group @xmath . Then level @xmath of the
hierarchy is defined recursively:

  -- -------- --
     @xmath   
  -- -------- --

By definition, @xmath is the Clifford group @xmath . For @xmath , @xmath
is no longer a group but contains a universal gate set, whereas @xmath
and @xmath are not universal.

#### 28 Learning Gottesman-Chuang Operations

Before we give our algorithm for learning Gottesman-Chuang operations,
we present a simple method for learning Pauli operations, which we use
as the main ingredient.

##### 28.1 Learning Pauli Operations

This is due to [ MO08 ] and is in fact identical to the
superdense-coding protocol [ BW92 ] .

###### Theorem 28.1 ([Mo08], Proposition 20).

Pauli operations can be identified with one query and in time @xmath .

###### Proof.

Apply the operator @xmath to half of the maximally entangled state

  -- -------- --
     @xmath   
  -- -------- --

For different choices of @xmath , the resulting states are orthogonal so
can be perfectly distinguished:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The time complexity @xmath comes from the preparation and measurement
operations. ∎

##### 28.2 Learning Clifford Operations

We can now present our algorithm for learning Clifford operations to
illustrate our main idea for learning unitaries in the Gottesman-Chuang
hierarchy. We will use the fact that knowing how a unitary acts by
conjugation on all elements of @xmath identifies it uniquely (up to
phase):

###### Lemma 28.2.

Knowing @xmath for all @xmath uniquely determines @xmath , up to global
phase.

###### Proof.

The Pauli matrices form a basis for all @xmath matrices so knowing the
action of @xmath on the Paulis is enough to determine the action of
@xmath on any matrix up to phase. The phase cannot be determined because
action by conjugation does not reveal the phase. ∎

Now let @xmath where @xmath ( @xmath ) is the matrix with @xmath (
@xmath ) acting on qubit @xmath and trivially elsewhere. We think of
this as a set of generators for @xmath since each element of @xmath can
be written as a product of elements of @xmath , up to phase. Using this,
knowledge of how @xmath acts on elements of @xmath is sufficient to
determine the action on all of @xmath :

###### Lemma 28.3.

@xmath for any @xmath can be calculated from knowledge of @xmath for
each @xmath .

###### Proof.

Let @xmath for @xmath where @xmath is a phase. Then

  -- -------- --
     @xmath   
  -- -------- --

With these definitions and observations, we can now present the Clifford
learning algorithm.

###### Theorem 28.4.

Given oracle access to an unknown Clifford operation @xmath and its
conjugate @xmath , @xmath can be determined exactly (up to global phase)
with @xmath queries to @xmath and @xmath to @xmath . The algorithm runs
in time @xmath .

###### Proof.

From the definition of the Clifford group, @xmath for all @xmath . Note
that @xmath is not necessarily a Pauli operator in @xmath because there
is a phase of @xmath (complex phases are not allowed because @xmath is
Hermitian). Determining which Pauli operator and phase for every @xmath
would be sufficient to learn @xmath using Lemma 28.2 . But from Lemma
28.3 , we only need to know @xmath for each @xmath .

Let @xmath and @xmath , where @xmath . Knowing just @xmath and @xmath is
enough to specify @xmath up to a Pauli correction factor @xmath which
gives the phases @xmath and @xmath . Choosing @xmath that anticommutes
with @xmath flips the sign of @xmath and similarly for @xmath . We now
present the algorithm:

1.  Apply @xmath and @xmath for each @xmath and use Theorem 28.1 to
    determine @xmath and @xmath . This uses @xmath queries to both
    @xmath and @xmath .

2.  Let @xmath be such that @xmath and @xmath i.e. the phases are all
    @xmath . Then, choosing a phase for @xmath , we can write @xmath
    where

      -- -------- -- --------
         @xmath      (28.1)
      -- -------- -- --------

    Then implement @xmath to determine @xmath using Theorem 28.1 . This
    uses one query to @xmath . We can now calculate the phases @xmath
    and @xmath .

To work out the time complexity, note that in step 1 the @xmath time
Pauli learning algorithm is called @xmath times. Then for step 2, the
Clifford @xmath can be implemented in @xmath time using for example
Theorem 10.6 of [ NC00 ] . ∎

We now show that this algorithm is optimal, in terms of number of
queries, up to constant factors:

###### Lemma 28.5.

Any method of learning a Clifford gate requires at least @xmath queries.

###### Proof.

Each application of the gate @xmath can give at most @xmath bits of
mutual information about @xmath . This follows from the optimality of
superdense coding [ BW92 ] . The Clifford group (modulo global phase) is
of size [ CRSS98 ] @xmath . To identify an element with @xmath queries,
we therefore need

  -- -------- -- --------
     @xmath      (28.2)
  -- -------- -- --------

which implies @xmath . ∎

It is unfortunate that access to @xmath is also required, but we do not
know a method with optimal query complexity that works without @xmath .
There are however methods that use @xmath queries that do not use @xmath
. The result of [ HW06 ] can be used to show that @xmath queries to
@xmath are sufficient, by distinguishing the states @xmath for different
Cliffords @xmath and where @xmath is the maximally entangled state. We
can use Lemma 29.4 to show that these states are far apart in the
distance measure used in [ HW06 ] , allowing us to apply their result.

##### 28.3 Learning Gottesman-Chuang Operations

Theorem 28.4 can easily be generalised to learning any operation from
the @xmath hierarchy:

###### Theorem 28.6.

Given oracle access to an unknown operation @xmath and its conjugate
@xmath , @xmath can be determined exactly (up to phase) with @xmath
queries to @xmath and @xmath to @xmath .

###### Proof.

The proof is by induction. The base case is for the Paulis and is proven
in Theorem 28.1 . Then, to learn @xmath , we assume we have a learning
algorithm for members of @xmath . Apply @xmath for each @xmath . These
operations are elements of @xmath so use the learning algorithm for
@xmath to determine these up to phase. Then use the last step of Theorem
28.4 to determine the phases.

We now determine the number of queries to @xmath and @xmath . Let @xmath
be the number of queries to @xmath and @xmath the number of queries to
@xmath . We have the recurrences

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (28.3)
  -- -------- -------- -- --------

and

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (28.4)
  -- -------- -------- -- --------

which have solutions @xmath and @xmath (with @xmath ). ∎

#### 29 Learning Unitaries Close to @xmath Elements

Here we suppose that we are given a unitary that is known to be close to
an element of @xmath for some given @xmath . We present a method for
finding this element. But first we must define our distance measure.

We would like our distance measure to not distinguish between unitaries
that differ by just an unobservable global phase. We define a ‘distance’
@xmath below with this property. However, firstly define the distance
@xmath to be a normalised 2-norm distance:

###### Definition 29.1.

For @xmath and @xmath @xmath matrices,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

We have chosen the normalisation so that @xmath . We now define our
phase invariant ‘distance’:

###### Definition 29.2.

For @xmath and @xmath @xmath matrices,

  -- -------- --
     @xmath   
  -- -------- --

This is not a true distance since @xmath does not imply @xmath , but
that @xmath and @xmath are the same up to a phase so the difference is
unobservable. From the 2-norm definition, we can show:

###### Lemma 29.3.

  -- -------- -- --------
     @xmath      (29.1)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (29.2)
  -- -------- -- --------

From this we can easily see that @xmath with equality if and only if
@xmath and @xmath are orthogonal. Further note that by the unitary
invariance of the 2-norm, both @xmath and @xmath are unitarily invariant
and from the triangle inequality for the 2-norm they both obey the
triangle inequality.

Our approximate learning method will find the unique closest element of
@xmath to @xmath . In order to guarantee uniqueness, the distance must
be upper bounded:

###### Lemma 29.4.

If @xmath for some @xmath then @xmath is unique up to phase.

The proof is in Section 32 .

###### Theorem 29.5.

Given oracle access to @xmath and @xmath and @xmath such that @xmath for
some @xmath with

  -- -------- -- --------
     @xmath      (29.3)
  -- -------- -- --------

then @xmath can be determined with probability at least @xmath with

  -- -------- --
     @xmath   
  -- -------- --

queries.

###### Proof.

By Lemma 29.4 , @xmath is unique up to phase. We now prove the Theorem
by induction.

For @xmath , use Proposition 21 of [ MO08 ] to learn the closest Pauli
operator. This works by repeating the Pauli learning method Theorem 28.1
and taking the majority vote. This uses @xmath queries to succeed with
probability at least @xmath .

Now for the inductive step. Assume we have a learning algorithm for
level @xmath . Then for @xmath , let @xmath for @xmath . By Lemma 33.1 ,
we have @xmath . Use the learning algorithm for level @xmath to
determine @xmath up to phase for all @xmath . Then to find the phases we
use the same method as before: implement any @xmath with @xmath for any
(known) choice of phase. Then @xmath for some Pauli operator @xmath . We
can determine @xmath by implementing @xmath and using the @xmath
learning algorithm since

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (29.4)
  -- -------- -------- -- --------

Now we calculate the success probabilities and number of queries. There
are @xmath calls to the algorithm at lower levels, which all succeed
with probability at least @xmath . So at this level the success
probability is at least @xmath . So to succeed with probability at least
@xmath we must replace @xmath with @xmath . Then the overall number of
queries is

  -- -------- -- --------
     @xmath      (29.5)
  -- -------- -- --------

We remark that there is only @xmath overhead (for constant @xmath and
@xmath ) over the exact learning algorithm of Theorem 28.6 .

#### 30 Clifford Testing

Here we present an efficient algorithm to determine whether an unknown
unitary operation is close to a Clifford or far from every Clifford.
Whereas the previous results allow us to find the Clifford operator
close to the given black box unitary, in this section we are concerned
with determining how far the given unitary is from any Clifford. We do
not measure this directly, but provide an algorithm of low query
complexity that decides if the given unitary is close to a Clifford or
far from all. This type of algorithm is known in computer science as a
property testing algorithm and has many applications, including the
theory of probabilistically checkable proofs [ ALM @xmath 98 ] . The
result in this section could be extended to work for any level of the
Gottesman-Chuang hierarchy although for simplicity we only present the
version for Cliffords.

The key ingredient to our method will be a way of estimating the Pauli
coefficients:

###### Lemma 30.1 (Lemma 23 of [Mo08]).

For any @xmath and unitary @xmath , the Pauli coefficients @xmath can be
estimated to within @xmath with probability @xmath using @xmath queries.

This is a generalisation of Theorem 28.1 and the method is similar.
Instead of there being only one possible outcome, now the probability of
obtaining the outcome corresponding to @xmath is estimated. This
probability is equal to @xmath .

###### Theorem 30.2.

Given oracle access to @xmath and @xmath with the promise that for
@xmath either

1.   CLOSE: there exists @xmath such that @xmath or

2.   FAR: for all @xmath , @xmath and there exists @xmath such that
    @xmath

holds then there is a @xmath algorithm that determines which with
probability at least @xmath .

###### Proof.

In both cases, we have that @xmath for some @xmath , which ensures that
@xmath is unique (using Lemma 29.4 , since @xmath ) and can be found
using Theorem 29.5 with @xmath queries. Then the algorithm is:

1.  For each @xmath , measure the Pauli coefficient of @xmath in @xmath
    (i.e. measure @xmath ) to precision @xmath using Lemma 30.1 .

2.  If all the coefficients are found to have modulus at least @xmath
    then output CLOSE else output FAR .

This works because, for the two possibilities CLOSE and FAR :

1.  Using Lemma 33.1 , @xmath implies that for all @xmath

      -- -------- -- --------
         @xmath      (30.1)
      -- -------- -- --------

    Since we will only apply @xmath for @xmath we restrict this to only
    the generators to find that for all @xmath

      -- -------- -- --------
         @xmath      (30.2)
      -- -------- -- --------

    giving

      -- -------- -- --------
         @xmath      (30.3)
      -- -------- -- --------

    for every generator @xmath . We need a bound on the non-squared
    coefficients, which follows directly:

      -- -------- -- --------
         @xmath      (30.4)
      -- -------- -- --------

    Therefore when measuring the coefficients to precision @xmath , all
    results will give at least @xmath .

2.  Using the contrapositive of Lemma 33.2 , @xmath implies that there
    exists @xmath such that

      -- -------- -- --------
         @xmath      (30.5)
      -- -------- -- --------

    Using the contrapositive of Lemma 33.3 this in turn implies there
    exists @xmath such that

      -- -------- -- --------
         @xmath      (30.6)
      -- -------- -- --------

    which means that for at least one @xmath , @xmath will have a small
    overlap with @xmath i.e. there exists @xmath such that

      -- -------- -- --------
         @xmath      (30.7)
      -- -------- -- --------

    The @xmath returned by the application of Theorem 29.5 is such that
    @xmath is positive, which justifies inserting the absolute value
    signs above when using @xmath rather than @xmath . This implies that
    at least one coefficient will be found to be less than @xmath when
    measuring to precision @xmath .∎

#### 31 Conclusions and Further Work

We have shown how to exactly identify an unknown Clifford operator in
@xmath queries, which we show is optimal. This is then extended to cover
elements of the @xmath hierarchy and for unitaries that are only known
to be close to @xmath operations. The key to the Clifford learning
algorithm is to apply @xmath and then find the resulting Pauli operator.

A way of extending this idea could be to learn unitaries from larger
sets. Suppose @xmath is a set of unitaries with the property that for
every @xmath , @xmath is a linear combination of a constant number of
Paulis. Then @xmath can be learnt in the same way as above, using the
quantum Goldreich-Levin algorithm of [ MO08 ] , which can efficiently
find which Paulis have large overlap with an input unitary. However, we
have not been able to find interesting sets @xmath other than the
Clifford group with this property.

We also presented a Clifford testing algorithm, which determines whether
a given black-box unitary is close to a Clifford or far from every
Clifford. This can be seen as a quantum generalisation of quadratic
testing, just as Pauli testing can be seen as a quantum generalisation
of linearity testing. Property testing of this form is used to prove the
PCP theorem [ ALM @xmath 98 ] so these quantum testing results could
potentially be useful in proving a quantum PCP theorem. It would also be
interesting to strengthen the testing method in Theorem 30.2 to remove
the @xmath difference between the close and far conditions.

Finally, it would be interesting to see if it is possible to remove the
requirement to have access to @xmath . However, using both @xmath and
@xmath is the key to our method so we do not know if a method without
@xmath is possible with low query complexity.

#### 32 Proof of Lemma 6.4.4

###### Proof of Lemma 29.4.

The proof is by induction. The base case is for @xmath when we have the
Pauli group. Without loss of generality, assume @xmath is a Pauli
operator with no phase. Let @xmath .

Expand @xmath in the Pauli basis:

  -- -------- -- --------
     @xmath      (32.1)
  -- -------- -- --------

Since @xmath is unitary, we have @xmath . By Lemma 29.3 ,

  -- -------- -- --------
     @xmath      (32.2)
  -- -------- -- --------

which implies

  -- -------- -- --------
     @xmath      (32.3)
  -- -------- -- --------

Now, suppose for contradiction that there exists @xmath with @xmath and
@xmath . Then by the above, @xmath . But there is also the constraint
@xmath which combined give

  -- -------- -- --------
     @xmath      (32.4)
  -- -------- -- --------

which is false by assumption. This implies @xmath , which proves the
base case.

To prove the inductive step, again assume for contradiction that there
exist @xmath with @xmath and @xmath and @xmath . Then there exists
@xmath with

  -- -------- -- --------
     @xmath      (32.5)
  -- -------- -- --------

Here, @xmath .

Using Lemma 33.1 , @xmath and @xmath .

Now there are two cases. Firstly, suppose we can choose @xmath such that
@xmath . Then @xmath and @xmath are not equivalent up to phase so, using
the inductive hypothesis, we must have

  -- -------- -- --------
     @xmath      (32.6)
  -- -------- -- --------

or

  -- -------- -- --------
     @xmath      (32.7)
  -- -------- -- --------

which is again false by assumption.

For the other case, @xmath for all @xmath . This implies that @xmath for
some Pauli @xmath . Then we have

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (32.8)
  -- -------- -------- -- --------

which by unitary invariance gives

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      (32.9)
  -- -------- -------- -- --------

But we proved that this is impossible in this range of @xmath in the
@xmath proof above. ∎

#### 33 Miscellaneous Lemmas

Here we prove some miscellaneous lemmas used earlier in the chapter.

The first lemma says that for two close operators @xmath and @xmath ,
@xmath is close to @xmath for all Paulis @xmath :

###### Lemma 33.1.

If @xmath then for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Let @xmath and @xmath . Then we simply apply the triangle inequality for
@xmath and unitary invariance:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The next lemma is a converse to this:

###### Lemma 33.2.

If for all @xmath

  -- -------- -- --------
     @xmath      (33.1)
  -- -------- -- --------

then

  -- -------- -- --------
     @xmath      (33.2)
  -- -------- -- --------

###### Proof.

If @xmath then @xmath . Since this is true for all @xmath , we can take
the average of this over the whole of @xmath and use the fact that for
any @xmath matrix @xmath @xmath (the Paulis are a 1-design ) to find

  -- -------- -- --------
     @xmath      (33.3)
  -- -------- -- --------

which simplified gives

  -- -------- -- --------
     @xmath      (33.4)
  -- -------- -- --------

giving the desired result. ∎

Now we show how to go from distances for just the generators @xmath to
distances for the whole of @xmath :

###### Lemma 33.3.

If for all @xmath

  -- -------- -- --------
     @xmath      (33.5)
  -- -------- -- --------

then for all @xmath

  -- -------- -- --------
     @xmath      (33.6)
  -- -------- -- --------

###### Proof.

The proof is by induction on the number of generators required to make
@xmath , using the triangle inequality for @xmath . ∎