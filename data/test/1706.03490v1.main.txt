## 1 Introduction

Consider the following set-up applicable throughout the thesis. Let
@xmath be an independent and identically distributed sequence of paired
random elements, defined on a probability space @xmath . It is assumed
that, each pair of random elements @xmath takes values in some product
space @xmath . Furthermore, throughout the thesis we let @xmath denote
the simultaneous distribution and let @xmath and @xmath denote the
marginal distributions on @xmath and @xmath respectively. That is,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are the coordinate projections onto the marginal
spaces @xmath and @xmath respectively (see section 7.1 for further
details on product spaces). The purpose of this thesis is to answer the
following problem in a set-up as general as possible.

###### Problem (The Non-Parametric Independence Problem).

Suppose that we are given a finite collection of paired sample points
@xmath , where each pair @xmath is a realization of @xmath . Given this
collection of samples, how can we without restricting @xmath to a
specific parametric class of distributions, draw inference on whether to
reject the null-hypothesis of independence

  -- -------- --
     @xmath   
  -- -------- --

in favor of the alternative hypothesis of dependence

  -- -------- --
     @xmath   
  -- -------- --

A solution to the above problem was proposed by Gábor J. Székely, Maria
L. Rizzo and Nail K. Bakirov, in the widely cited article ”Measuring and
Testing Dependence by Correlation of Distances” from 2007, published in
The Annals of Statistics; [ SRB07 ] . In this article, a solution to the
above problem is proposed, in the case that both @xmath and @xmath are
finite-dimensional Euclidean spaces. This is done by introducing the
so-called distance covariance measure between two random vectors @xmath
and @xmath , with simultaneous distribution @xmath on @xmath . This
distance covariance measure, is given by

  -- -------- --
     @xmath   
  -- -------- --

a weighted @xmath difference between the characteristic functions of
@xmath and @xmath . The distance covariance measure @xmath is easily
seen to be zero if and only if @xmath . They furthermore introduce a
plug-in estimator of this distance covariance measure, based on
empirical characteristic functions. Hereafter they showed, that the
estimator possesses asymptotic properties that allow the construction of
an asymptotically consistent test of independence.

In 2013, Russell Lyons published the article ”Distance covariance in
metric spaces” in The Annals of Probability; [ Lyo13 ] . This article
proposes a solution to the above problem, under the weaker assumption
that the marginal spaces @xmath and @xmath are so-called metric spaces
of strong negative type. This is done by introducing another so-called
distance covariance measure (a generalization of @xmath ; see theorem
4.5 ) between the random Borel elements @xmath and @xmath with
simultaneous distribution @xmath . This distance covariance measure, is
equivalently (see eq. 5 ) given by

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath and @xmath are independent copies of @xmath . An important
but non-trivial property of this distance covariance measure, is that it
can be used as a direct indicator of independence. That is, @xmath if
and only if @xmath , whenever @xmath and @xmath are metric spaces of
strong negative type (a superset of separable Hilbert spaces; see
theorem 3.27 ). Russell Lyons then introduces a plug-in estimator for
the distance covariance measure and show that it possesses asymptotic
properties that can be used to construct an asymptotically consistent
test of independence.
In this thesis we will answer the non-parametric independence problem
using the theory developed in [ Lyo13 ] . This thesis is therefore
essentially best described as, a very detailed exposition of discoveries
made by Russell Lyons. The original article leaves a surprisingly large
amount of details to the reader, therefore it has not been easy or
without problems to make this thesis.

Some of the mathematical concepts and constructions needed to understand
and describe the theory of distance covariance in metric spaces, were at
the beginning unknown to me. So in order to keep the thesis
self-contained, appendices have been added to introduce these concepts
in a degree which suffices for our needs.

In writing this thesis I also stumbled upon several discrepancies in the
original article, ranging from negligible to serious. Whenever the
non-negligible discrepancies are met, I have explicitly added remarks
explaining the problems and how they are solved. I am grateful that
Russell Lyons has taken the time to both confirm problems, and in the
case of lemma 3.22 (lemma 3.8 in [ Lyo13 ] ) providing a smart
workaround idea that yielded the new and to some extend quite different
proof.

We will now provide a brief overview of the content of the following
sections.
Section 2 We construct the so-called distance covariance measure @xmath
. This so-called measure @xmath , is formally a real-valued functional
with domain given by a space of sufficiently nice Borel probability
measures on the product space @xmath of metric spaces. From the
definition of @xmath it is easily realized that @xmath implies @xmath .
However, the converse implication which would render the distance
covariance measure a direct indicator of independence, is not true for
general metric spaces @xmath and @xmath . Section 3 To answer the
question regarding which metric spaces would yield the converse
implication mentioned above, we define metric spaces of negative and
strong negative type. Metric spaces of negative type, are metric spaces
that can be isometrically embedded into Hilbert spaces. If both marginal
metric spaces @xmath and @xmath are of negative type, then we show that
the functional @xmath has an alternative representation in terms of the
isometric embeddings. This alternative representation leads us to the
definition of metric space of strong negative type. The essential
property of these spaces are, if both @xmath and @xmath are metric
spaces of strong negative type, then dcov( θ )=0⇔ θ = μ × ν . It is
furthermore shown that, when disregarding the unimportant singleton
spaces, it is necessary for the marginal metric spaces to be of strong
negative type in order to have the implication @xmath . This section is
concluded with a theorem identifying all separable Hilbert spaces as
metric spaces of strong negative type. Section 4 This section is
dedicated to proving some properties and bounds on the functional @xmath
. We also establish the connection between the distance covariance in
Euclidean spaces from [ SRB07 ] and the distance covariance measure in
metric spaces. That is, if @xmath and @xmath are finite-dimensional
Euclidean spaces, then @xmath , proving that the distance covariance
measure in metric spaces indeed is a generalization of the former.
Section 5 Section 5 is divided into three subsections. In section 5.1 ,
we introduce two different estimators for @xmath . It is seen that,
@xmath is a so-called regular functional, and one may recall that such
functionals are the building blocks of the so-called @xmath - and @xmath
-statistic estimators. Our choice of estimators for @xmath are therefore
given by such estimators. In section 5.2 , we show that these estimators
are both strongly consistent and if scaled correctly also possess rather
complicated asymptotic distributions. In section 5.3 , we formally
describe the statistical models for which the assymptotic properties
from section 5.2 yield asymptotically consistent tests of independence.
These tests turns out to have non-traceable rejection thresholds, so we
end this last section by describing how one may reasonably bootstrap the
rejection thresholds.

## 2 Distance covariance in metric spaces

As mentioned in the introduction the main objective of this thesis is to
establish a measure of dependence that can be used to create an
asymptotically consistent statistical test for independence. In this
section we will construct the so-called distance covariance measure
@xmath of a probability measure @xmath on a product space @xmath , which
can be used to directly establish whether or not the probability measure
@xmath is in fact given by the product of its marginals @xmath . That
is, we will construct a functional

  -- -------- --
     @xmath   
  -- -------- --

with the desired property, that whenever the marginal spaces @xmath and
@xmath are sufficiently nice, @xmath if and only if @xmath . Here @xmath
is the space of all Borel probability measures on @xmath with sufficient
integrability. Exactly what this sufficient integrability entails, is
the content of the first definition below.

Note that @xmath is not a measure in the usual sense, nevertheless we
will still refer to it as the distance covariance measure rather than
functional. Before proceeding, we make an initial restriction on what
kind of marginal spaces @xmath and @xmath we will consider. This
restriction is the content of the following universal assumption of this
thesis:

###### Assumption 2.1.

Every metric space @xmath and @xmath considered in this thesis is
assumed separable.

As we shall see later, this restriction on the marginal spaces is not
sufficient for the distance covariance measure to have the desired
property. This is indeed solved by assuming that the marginal metric
spaces are of strong negative type, which is the focus of attention in
section 3 . Before continuing we present a short remark on the above
assumption.

###### Remark 2.2.

In the article of Russell Lyons [ Lyo13 ] , it is nowhere stated that we
move beyond the realm of general metric spaces. This is an obvious error
in the article as one has to require as a minimum, that the metric
spaces considered have cardinality less than or equal to the continuum.

The reason for this, is that in order to define the distance covariance,
we need that the metrics on our marginal spaces are jointly measurable,
i.e. @xmath needs to be @xmath -measurable. Due to Nedomas pathology
(see prop. 21.8 [ Sch96 ] or example 6.4.3 [ Bog07a ] ) we get that,
every metric @xmath on space with cardinality strictly greater than the
continuum @xmath , is not jointly measurable (the diagonal is not
measurable). An example of such a space could be @xmath endowed with the
discrete metric, since @xmath .

This problem is of course eliminated by the assumption of separability
of @xmath , which implies that @xmath but also implies that @xmath (see
theorem 7.2 ), rendering @xmath jointly measurable, since it is
continuous. There are indeed other places in this thesis, that utilize
the separability of the considered metric spaces. Some of these are
lemma 3.10 which uses that @xmath , furthermore in theorem 4.4 and lemma
5.8 where we explicitly use the separability.

In personal communication with Russell Lyons he acknowledges the
problems, and agrees with me that this discrepancy is best solved by
only considering separable metric spaces.

Throughout the thesis we will use a variety of Borel measureas on our
metric spaces, so we start by defining some commonly used spaces of
measures.
In order to do so we need to define moments of measures. For any @xmath
we say that a finite signed Borel measure on @xmath has finite @xmath
’th moment if

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath , where @xmath is the total variation of @xmath and
@xmath is the Jordan-Hahn decomposition. We may also note that, if the
above holds for some @xmath , then it holds for all @xmath . In order to
see this, note that, if there is an @xmath such that @xmath , then for
any @xmath the @xmath -inequality allows for the following finite bound

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
                       
              @xmath   
  -- -------- -------- --

where @xmath when @xmath and @xmath when @xmath . We may also note that
in the case that @xmath and @xmath is a probability measure on @xmath ,
the above definition of moments coincides with the regular definition of
moments of random vectors. That is, if @xmath then choose @xmath and
note that

  -- -------- --
     @xmath   
  -- -------- --

###### Definition 2.3.

Let @xmath and @xmath be two metric space. We define the following
spaces
@xmath The space of all finite signed measures on @xmath . @xmath The
space of all finite signed measures on @xmath with finite @xmath ’th
moment. @xmath The space of all probability measures on @xmath . @xmath
The space of all finite signed measures on @xmath that assigns the
entire space @xmath to zero. That is, @xmath for all @xmath . @xmath The
space of all finite signed measures on @xmath . @xmath The space of all
finite signed measures @xmath on @xmath for which it holds that @xmath
and @xmath @xmath The space of all probability measures on @xmath .
@xmath The space of all probability measures on @xmath that has
non-degenerate marginal distributions. That is, the marginal
distributions are not concentrated on a singleton or equivalently not
Dirac measures. Whenever we put both a subscript and superscript it
denotes the intersection. For example, @xmath is the space of
probability measures with finite 1st moment.

We may furthermore note that @xmath . Whenever we consider a measure
@xmath then we indirectly assume that @xmath is the marginal measure on
@xmath and @xmath is the marginal measure on @xmath , i.e. @xmath and
@xmath . Also note that if @xmath then @xmath and @xmath , since @xmath
.
It turns out that some of these spaces are in fact @xmath -vector
spaces, if we define some sensible addition and multiplication on them.
This fact is not important for the definition of @xmath , but it will
later play a very important part in the further analysis of the distance
covariance measure.

###### Lemma 2.4.

For any metric space @xmath , it holds that @xmath is an @xmath -vector
space, and @xmath is a linear subspace of @xmath . If furthermore @xmath
is yet another metric space, then @xmath is a @xmath -vector space and
@xmath is a linear subspace of @xmath .

###### P 1.

On @xmath - the space of finite signed Borel measures - we define scalar
multiplication and addition of these measures by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , @xmath and @xmath . It is obvious that @xmath is closed
under any finite linear combination and satisfies every other axiom of
vector spaces, meaning that @xmath is a vector space. The question is
now, if the subset of signed measures with finite first moments @xmath ,
indeed is a linear subspace. Since the zero measure (maps every
measurable set to zero) clearly has a finite first moment (rendering
@xmath non-empty), we note that it suffices to show that @xmath is a
measure with finite first moment for all @xmath and @xmath , in order to
prove that @xmath is a linear subspace of @xmath . Hence we see that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath and that the Lebesgue integral is monotone in
measure, when the integrand is non-negative.
The inequality @xmath follows by standard arguments, but in order to
keep the thesis self-contained we show it regardless. First note that if
@xmath and @xmath are two positive measures and @xmath , then @xmath and
@xmath (see p. 88 [ Fol99 ] ). In our setup, we have that @xmath and as
a consequence @xmath . Now note that the total variation measure @xmath
equivalently can be stated as the expression @xmath for any @xmath ,
where the supremum is over all mutually disjoint sequences @xmath in
@xmath with @xmath (see section A.1 [ Sok14 ] or p. 177 [ Bog07b ] ),
proving the wanted inequality.
The proof for @xmath and @xmath follows by analogous arguments. That is,
for @xmath and @xmath we have that @xmath and that @xmath . Hence

  -- -------- --
     @xmath   
  -- -------- --

since @xmath is a @xmath -vector space and @xmath by definition of
@xmath . A similar derivation follows for the @xmath projection, so
@xmath .

Now that we have defined the important spaces of measures we are almost
ready to define the distance covariance measure. The distance covariance
measure is defined in terms of integrals of certain mappings, hence we
start by proving that these are sufficiently integrable. Before doing
this, we need to establish some common ground, on how to define the
integral of a mapping with respect to the product of two finite signed
measures.
Recall the integral of a measurable mapping @xmath with respect to a
signed measure @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

whenever @xmath , where @xmath is the Jordan-Hahn decomposition and
@xmath is the total variation of @xmath . We remind the reader that one
constructs the product measure of two signed measures @xmath and @xmath
by utilizing the Jordan-Hahn decomposition theorem.

If we let @xmath and @xmath , then we define the product measure @xmath
directly by its Jordan-Hahn decomposition. That is, as the difference
between the mutually singular measures @xmath and @xmath . To see that
they are mutually singular simply realize that the first measure is
concentrated on @xmath and the other is concentrated on @xmath , where
@xmath and @xmath are the disjoint decompositions of the spaces given by
the Jordan-Hahn decomposition of the marginal measures. Hence @xmath is
the finite signed measure given by

  -- -------- --
     @xmath   
  -- -------- --

and the above decomposition is equal to its Jordan-Hahn decomposition.
We say that a @xmath -measurable mapping @xmath is integrable with
respect to @xmath , written @xmath , if @xmath is integrable with
respect to all the above product measures. The integral of @xmath with
respect to @xmath is defined in the natural way as the sum and
difference of the four regular product integrals. In terms of
integrability conditions, it suffices to check that @xmath , since

  -- -------- --
     @xmath   
  -- -------- --

which is seen by using Tonelli’s theorem and then successively making an
upper bound for the inner and then the outer integral, by changing
integration measures to @xmath and @xmath . That is, if @xmath then
@xmath . In the case of an integrable mapping @xmath , we have the
following Fubini theorem for the product integral of signed measures

  -- -------- --
     @xmath   
  -- -------- --

which is seen be utilizing Fubini’s theorem for the marginal integrals
with respect to @xmath . For further details on the integration with
respect to the product of finite signed measures, we refer the reader to
section 3.3 of [ Bog07b ] .

###### Lemma 2.5.

If @xmath is a metric space, then @xmath for any two measures @xmath .

###### P 2.

By the above remark if suffices to show that @xmath . This is easily
seen by using the triangle inequality: @xmath for any @xmath . Thus for
some @xmath , we get

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath is finite and @xmath for both @xmath , since
they are finite signed measures with finite first moment.

This lemma allows for the definition of the mappings relevant for the
distance covariance measure.

###### Definition 2.6.

Let @xmath be a metric space. For any measure @xmath we may define the
@xmath -integrable and continuous mapping @xmath given by

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

and the mapping @xmath by

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

Lastly for any @xmath we may define the @xmath -modified ”distance”
@xmath by

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

The quotation sign in ”distance”, signifies that it is not a real
metric. To see the continuity of @xmath for all @xmath , we note that
@xmath is continuous and that by applying the reverse triangle
inequality @xmath which tends to zero as @xmath for any @xmath , proving
continuity of @xmath .
We may note that the @xmath -modified distance @xmath lies within @xmath
for any two @xmath , but as shown below it actually possess stronger
integrability than @xmath .

###### Lemma 2.7.

For any metric space @xmath and any @xmath , we have that @xmath

###### P 3.

First of all @xmath is the composition of @xmath -measurable mappings,
hence it is itself jointly measurable. By the triangle inequality @xmath
we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath , by Fubini’s theorem and the fact that @xmath since it
is a probability measure. This also implies that @xmath for all @xmath .
We also have that

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

which is seen by integrating on both sides of the triangle inequality
with respect to @xmath of different arguments. Hence with @xmath , all
the above inequalities yield that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and @xmath (by symmetry) for all @xmath . Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by the above inequalities, Fubini’s theorem and lemma 2.5 .

Having established the square integrability of the mapping @xmath we
define the distance covariance functional in the following way

###### Definition 2.8 (Distance Covariance).

For any two metric spaces @xmath and @xmath , we define the distance
covariance measure as @xmath given by

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath with marginal probability measures @xmath and @xmath .

We stress that @xmath is indeed well-defined for any @xmath by the
Cauchy-Schwarz inequality. Simply note that

  -- -------- --
     @xmath   
  -- -------- --

by Fubini’s theorem and lemma 2.7 . By analougus aruguments for @xmath ,
we conclude that the mappings @xmath . Hence Cauchy-Schwarz inequality
yields that @xmath , proving that @xmath is well-defined.
It is immediately seen from the above definition, that if @xmath then

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

by Fubini’s theorem. But it is not readily apparent what is needed in
order to get the converse statement: if @xmath , then @xmath . As
mentioned previously this implication does not hold for general metric
spaces. The next section is dedicated to understand when it does.
We may also derive a representation of @xmath in terms of conditional
expectations of random variables. Let @xmath and @xmath be metric spaces
and let @xmath and @xmath be random Borel elements defined on a common
probability space @xmath with distribution @xmath and @xmath
respectively, i.e. @xmath . Now let @xmath and @xmath be independent
copies of @xmath and note that @xmath . Thus we may realize that

  -- -------- -------- -- -----
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (5)
              @xmath      
     @xmath   @xmath      
              @xmath      
  -- -------- -------- -- -----

In the case when @xmath and @xmath are sufficiently integrable, these
linear combinations are actually orthogonal projections onto certain
subspaces of @xmath . In the appendix on @xmath - and @xmath -statistics
( section 7.2.1 ) we create formulas for the orthogonal projection onto
certain spaces, which we use to prove the Hoeffding decomposition of
@xmath -statistics. For our purpose right here it suffices to consider
the space @xmath consisting of all mappings in @xmath that have the form
@xmath for some measurable map @xmath and for which it holds that

  -- -------- --
     @xmath   
  -- -------- --

with a similar space @xmath constructed for the @xmath variables. Now if
@xmath we have that @xmath and their orthogonal projection onto the
spaces @xmath and @xmath are given by

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

respectively, where we used the projection formula from lemma 7.3 . Thus
we may say that, if @xmath , then @xmath is given as the expectation of
@xmath projected onto @xmath multiplied by @xmath projected onto @xmath
. This observation is not used in the next chapters, but we feel it was
a connection worth mentioning.

## 3 Metric spaces of negative and strong negative type

In this section we will examine what restriction on our marginal metric
spaces @xmath and @xmath allows for the distance covariance measure to
be used as a direct indicator of independence. That is, for which
marginal metric spaces @xmath and @xmath are we allowed to conclude that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath with marginals @xmath and @xmath . The answer to this
problem is: if we only consider spaces for which the independence
problem is indeed valid, then it is necessary and sufficient to assume
that both @xmath and @xmath are what is called metric spaces of strong
negative type.
The procedure for showing this is as follows. We define a certain subset
of all metric spaces, called metric spaces of negative type. It turns
out that whenever both marginal metric spaces @xmath and @xmath are of
negative type, one may derive a Hilbert space representation of the
distance covariance measure given by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , and some mapping @xmath with values in the tensor
product of two Hilbert spaces @xmath and @xmath . This Hilbert space
representation will also be essential in section 4 , where we prove that
the distance covariance measure in metric spaces (as we defined it)
coincides with the distance covariance measure from [ SRB07 ] , when the
marginal spaces are assumed to be finite-dimensional Euclidean spaces.

If we restrict ourselves to an even smaller set of metric spaces,
so-called metric spaces of strong negative type, then we get that the
mapping @xmath is an injective linear transformation. Since @xmath ,
this of course entails that @xmath . Hence we get that, if both marginal
spaces are of strong negative type, then the distance covariance measure
can be used as a direct indicator of independence. On the other hand, if
we disregard the unimportant (from an independence perspective)
singleton-spaces, then it is also necessary that both marginal spaces
are of strong negative type, in order for the distance covariance
measure to be a direct indicator of independence.
The above procedure is split into the next two subsections. In section
3.1 , we define metric spaces of negative type and prove the alternative
Hilbert space representation of the distance covariance measure. In
section 3.2 , we define metric spaces of strong negative type and prove
the above mentioned implication of injectivity, which yields the wanted
property of the distance covariance measure. In section 3.2 , we also
identify all separable Hilbert spaces as metric spaces of strong
negative type, and we furthermore prove that it is necessary for the
marginals spaces to be of strong negative type in order for the distance
covariance measure to be used as a direct indicator of independence.

### 3.1 Metric spaces of negative type

Metric spaces of negative type are defined in [ Lyo13 ] in the same way
we do below, but without mention of negative definite kernels. The
concept of negative definite kernels is presented in ”Harmonic Analysis
on Semigroups” by Christian Berg et al. [ BCR84 ] and here a connection
between negative definite kernels and positive definite kernels (defined
below) is established. This connection turns out to be useful, since the
concept of positive definite kernels is central in the theory of
reproducing kernel Hilbert spaces. By utilizing this connection between
negative definite kernels and reproducing kernel Hilbert spaces, we can
establish a more structured (yet longer) presentation of the properties
of metric spaces of negative type, than the one presented in [ Lyo13 ] .

###### Definition 3.1 (Metric spaces of negative type).

A metric space @xmath is said to be of negative type if the metric
mapping @xmath is a negative definite kernel. That is, if

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , @xmath and @xmath with @xmath .

One of the main objectives of this section is to provide an equivalence
between metric spaces of negative type and the isometric embeddability
of @xmath into a Hilbert space. This turns out to be a very helpful
equivalence, because it is indeed these isometric embeddings that allow
for the derivation of the alternative Hilbert space representation of
the distance covariance measure.
But before we continue with proving the existence of previously
mentioned isometric embeddings, we establish a connection between inner
product spaces and negative definite kernels - a connection found in [
WW75 ] .

###### Theorem 3.2.

Let @xmath be a normed @xmath -vector space and let @xmath denote the
naturally induced metric. Then @xmath is an inner product space if and
only if @xmath is a negative definite kernel. That is, if and only if
the semi-metric space @xmath is of negative type.

###### P 4.

First note that, if @xmath is a metric, then @xmath is in general not a
metric but rather a semi-metric, which by definition means that it
satisfies all conditions of metrics except the triangle inequality. Let
@xmath and @xmath denote the inner product and its naturally induced
norm on @xmath .
If @xmath is an inner product space, we see that @xmath . Hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath and @xmath with @xmath , proving that @xmath is a
negative definite kernel. Conversely assume that @xmath is a negative
definite kernel. We know that @xmath is an inner product space if and
only if the norm satisfies the parallelogram law

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (cf. theorem 6.9 [ HN01 ] ). Thus fix @xmath and let
@xmath , @xmath and @xmath . After reduction we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

is less than or equal to zero. Rearranging this equation and dividing
with @xmath on both sides, we get that

  -- -------- --
     @xmath   
  -- -------- --

and, if we let @xmath , then we get that @xmath . Now this holds for any
@xmath , hence we may also let @xmath and @xmath and note that

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

proving the reverse inequality. We conclude that @xmath for any @xmath ,
proving that @xmath is an inner product space.

Now we return to our main topic of this section - to prove that metric
spaces are of negative type if and only if they can be isometrically
embedded into a Hilbert space. As mentioned above there is a connection
between metric spaces of negative type and positive definite kernels.
Thus we start by defining positive definite kernels and thereafter
establish this connection.

###### Definition 3.3.

Let @xmath be a non-empty set. A symmetric mapping @xmath is called a
positive definite kernel if

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , @xmath and @xmath .

Note that the definition of positive definite kernels can be
equivalently represented in terms of positive semi-definite matrices.
Let @xmath be a symmetric mapping and let @xmath denote the symmetric
@xmath dimensional real matrix with entries given by @xmath for any
@xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . Hence we get that @xmath is a positive definite kernel
if and only if @xmath is a positive semi-definite matrix for all @xmath
and @xmath .
With the terminology in order, we are now ready to prove an equivalence
between a metric space being of negative type and a certain mapping
being a positive definite kernel.

###### Theorem 3.4.

A metric space @xmath is of negative type if and only if @xmath given by

  -- -- --
        
  -- -- --

for some @xmath , is a positive definite kernel.

###### P 5.

Fix @xmath , @xmath and consider any @xmath with @xmath and @xmath .
Note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

using that the latter double sum can be written as two sums multiplied
by @xmath . This especially entails that, if @xmath is a positive
definite kernel, then @xmath is a metric space of negative type. Now let
@xmath and set @xmath such that @xmath , note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

proving that @xmath is a positive definite kernel, when @xmath is a
metric space of negative type.

As mentioned above, positive definite kernels are intertwined with the
theory of reproducing kernel Hilbert spaces, so we summarize some useful
relations from this theory.

###### Lemma 3.5.

If @xmath is a separable topological space and @xmath is a continuous
positive definite kernel, then there exist a separable @xmath -Hilbert
space @xmath and a mapping @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . We say that @xmath is the reproducing kernel of the
reproducing kernel Hilbert space @xmath .

###### P 6.

Consider the real linear subspace

  -- -------- --
     @xmath   
  -- -------- --

and note that any @xmath has some representation by a finite linear
combination @xmath and @xmath and for any such elements we define

  -- -------- --
     @xmath   
  -- -------- --

We may note that @xmath defined by the above relation is a well-defined
mapping. To see this, note that

  -- -------- --
     @xmath   
  -- -------- --

Hence if for any two representations, @xmath we have that

  -- -------- --
     @xmath   
  -- -------- --

so it evidently does not depend on the specific representation of @xmath
, and by symmetry of @xmath it is also independent of the representation
of @xmath . By the symmetry of @xmath , we also have symmetry of @xmath
and for @xmath and @xmath and note that

  -- -------- --
     @xmath   
  -- -------- --

which is seen by taking a finite linear combination representation of
@xmath and using the above definition followed by a separation of the
terms. We also note that

  -- -------- --
     @xmath   
  -- -------- --

by the assumption that @xmath is a positive definite kernel. Hence
@xmath is a semi-inner product and thus by the
Cauchy-Bunyakowsky-Schwarz inequality (cf. proposition 1.4 [ Con90 ] )
we have that @xmath . As a consequence we get

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , hence @xmath . Conversely by the same considerations as
above we have that @xmath , proving that @xmath . We conclude that
@xmath is a inner product on @xmath , that is @xmath is an inner product
space.

Let @xmath denote the completion of @xmath with respect to the inner
product induced metric. By proposition 1.9 [ Con90 ] , we can extend the
inner product @xmath on @xmath to an inner product @xmath on @xmath in
the following way. First one constructs the completion @xmath and a
linear isometric embedding @xmath as done in section 7.5 . The
proposition from [ Con90 ] now states that there exists an inner product
@xmath on @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . We especially have that @xmath is an @xmath -Hilbert
space and we define @xmath by @xmath , then we have that

  -- -------- --
     @xmath   
  -- -------- --

As regards the separability of @xmath , we note that since @xmath is
separable it has a countable dense subset @xmath . We claim that the
countable collection of maps

  -- -------- --
     @xmath   
  -- -------- --

is dense in @xmath . To see this, let @xmath and note that @xmath has
representation @xmath for some @xmath . Since @xmath is dense in @xmath
there exists a sequence @xmath such that @xmath and since @xmath is
dense in @xmath there exists a sequence @xmath such that @xmath . Now
define the sequence @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . Note that if @xmath we have that @xmath is a countable
dense set in @xmath , rendering @xmath separable. By writing the
difference in norm as a linear combination of inner products one may
realize that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

where we used the continuity of @xmath . Thus @xmath is a separable
inner product space and since @xmath is the completion of @xmath , we
get that @xmath is a separable @xmath -Hilbert space (cf. theorem 3.25.
[ AB06 ] ).

The above lemma now allows us to establish a condition on metric spaces
to be of negative type in terms of the isometric embeddability into
Hilbert spaces.
First let us define what we mean by an isometric embedding between
spaces, and show some important properties of such mappings. For any two
metric spaces @xmath and @xmath we say that a map @xmath is an isometry
if it satisfies @xmath and if such a mapping exists, we say that @xmath
is isometrically embeddable into @xmath . Whenever we do not specify the
metric for the isometric embeddings, it is equipped with the natural
metric. For example, if we have a metric space @xmath and a Hilbert
space @xmath , then we say that @xmath is an isometric embedding if

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

for all @xmath . We will apply the convention that, whenever we mention
a isometric embedding from a metric space to a Hilbert space, it should
be understood as in eq. 6 . Note that the isometric mapping is
automatically injective and continuous.
An important property of such isometric embeddings into Hilbert spaces
is that they are Pettis integrable with respect to every measure in
@xmath . A fact that is used in the main theorem below about the
equivalence between metric spaces of negative type and their
embeddability into Hilbert spaces, hence we briefly explain what we mean
by Pettis integrable.

A thorough introduction to the Pettis integral of Hilbert space valued
mappings can be found in section 7.3 . For the reader who is somewhat
familiar with the Pettis integral the following one sentence recap can
be given: If @xmath is a @xmath -Hilbert space valued mapping, which is
Pettis integrable with respect to a finite positive measure @xmath on
@xmath , then the Pettis integral of @xmath over @xmath with respect to
@xmath , denoted @xmath , is defined to be the unique element in @xmath
satisfying

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the continuous dual space of @xmath , that is

  -- -------- --
     @xmath   
  -- -------- --

Note that above and for the remainder of this thesis we let @xmath
denote either @xmath or @xmath , and in any given scenario we only use
@xmath whenever the result or statement holds for both @xmath and @xmath
respectively. E.g. a @xmath -Hilbert space is either an @xmath -Hilbert
space (real Hilbert space) or a @xmath -Hilbert space (complex Hilbert
space).

###### Lemma 3.6.

Any isometric embedding @xmath into a @xmath -Hilbert space is Pettis
integrable with respect to both Jordan-Hahn decompositions @xmath and
@xmath , for any @xmath .

###### P 7.

Let @xmath be any finite signed Borel measure on @xmath with finite
first moment. By theorem 7.28 is suffices to show that @xmath is @xmath
-scalarly integrable. That is, it suffices to show that @xmath for all
@xmath . Measurability : @xmath is an @xmath -isometry, that is

  -- -------- --
     @xmath   
  -- -------- --

from which continuity and hence @xmath -measurability follows. By the
same reasoning we get that every function @xmath is continuous and hence
@xmath -measurable, so composition @xmath is indeed @xmath -measurable.
Integrability: Note that for any @xmath and @xmath we have that @xmath
and @xmath , where @xmath denotes the operator norm on @xmath . Thus we
get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath , since @xmath has finite first moment and its total
variation is finite (cf. Corollary 3.1.2 [ Bog07b ] ). One might note
that only half moments of @xmath were needed in order to ensure
integrability.

For our purpose it does not suffice with the Pettis integral with
respect to positive measures. Hence we will define the Pettis integral
with respect to signed measures in the same way as one defines the
Lebesgue integral with respect to signed measures. That is, for a finite
signed measure @xmath on @xmath with Jordan-Hahn decompositions @xmath
and @xmath and a Hilbert space valued mapping @xmath , we define the
Pettis integral of @xmath over @xmath with respect to @xmath by

  -- -------- --
     @xmath   
  -- -------- --

whenever both Pettis integrals exist. We may also realize that the
unique defining property of the Pettis integral agrees with this
definition. The linearity of @xmath allows us to say

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and as a consequence the Pettis integral of @xmath over @xmath with
respect to a finite signed measure @xmath is the unique element in
@xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

Uniqueness is realized in the following way: assume for contradiction
that

  -- -------- --
     @xmath   
  -- -------- --

for two @xmath with @xmath . Now note that @xmath separates points in
@xmath for any Hilbert space @xmath (cf. theorem 3.4 [ Rud91 ] ),
meaning that @xmath for all @xmath implies that @xmath . This is a
contradiction, so we have that @xmath , proving uniqueness.
Having defined all necessary Pettis integral terminology needed, we may
now state one of the most important theorems of this section, namely the
previously mentioned equivalence between a metric space of negative type
and its isometric embeddability into a Hilbert space.

###### Theorem 3.7.

Let @xmath be a metric space. Then the following statements are
equivalent

-   @xmath is a metric space of negative type.

-    There exists an isometric embedding @xmath into a separable @xmath
    -Hilbert space.

-    There exists an isometric embedding @xmath into a separable @xmath
    -Hilbert space.

###### P 8.

2) @xmath 3) : To fully understand the equivalence between 2) and 3) we
encourage the reader to understand how the realification of a complex
Hilbert space and the complexification of a real Hilbert space are
constructed (see section 7.4 ).

Nevertheless if @xmath is an isometric embedding into a separable @xmath
-Hilbert space, then we may note that @xmath is an isometric embedding
into a separable @xmath -Hilbert space, where the complexification map
@xmath into the complexification @xmath of @xmath is an isometry.
Conversely if @xmath is an isometric embedding into a separable @xmath
-Hilbert space, then we may note that @xmath is an isometric embedding
into a separable @xmath -Hilbert space, where the realification map
@xmath into the realification @xmath of @xmath is an isometry.
1) @xmath 2) : Assume that @xmath is of negative type. For some @xmath
we define the function @xmath by

  -- -------- --
     @xmath   
  -- -------- --

By theorem 3.4 @xmath is a positive definite kernel (since @xmath is).
Furthermore @xmath is separable and @xmath is continuous, by the
continuity of @xmath . Hence by lemma 3.5 we know that there exists a
separable @xmath -Hilbert space @xmath and a mapping @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

We realize that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving that there exists an isometric embedding @xmath into a separable
@xmath -Hilbert space.
2) @xmath 1) : Assume that there is an an isometric embedding @xmath
into a separable @xmath -Hilbert space. Let @xmath , @xmath and @xmath
with @xmath . Define a signed measure @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath is not necessarily the Jordan-Hahn decomposition, but
we know that @xmath and @xmath , thus @xmath , by the proof of lemma 2.4
. Hence @xmath for any @xmath , so @xmath . Thus @xmath is well-defined
and equals

  -- -------- --
     @xmath   
  -- -------- --

where we have used that @xmath , see proof of lemma 3.9 for further
explanation. Therefore it suffices to show that @xmath . Since @xmath is
an isometric embedding we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by integrability of each term. This is seen by noting that @xmath
(expand with triangle inequality around @xmath to get integrable upper
bound @xmath ) and that @xmath (by Cauchy-Schwarz inequality @xmath ,
where the upper bound is integrable by proof of lemma 3.6 ). It holds
that @xmath (i.e. @xmath ) since @xmath , and therefore we get

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath is Pettis integrable with respect to any @xmath (see
lemma 3.6 ) so the Pettis integral @xmath exists. By the unique defining
property of the Pettis integral we can derive that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath for every fixed @xmath and that @xmath are
continuous linear mappings ( @xmath is a @xmath -Hilbert space). Hence

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

This concludes the proof. As a closing remark, we may add that eq. 7
holds for general finite signed measures @xmath with finite first moment
and @xmath , i.e. for any @xmath .

The above theorem is crucial for our further development. Because if we
assume that @xmath and @xmath are metric spaces of negative type, then
this theorem is mainly responsible for the derivation of the alternative
Hilbert space representation of the distance covariance measure @xmath .
The next order of business is to derive this alternative representation
and this is done through the analysis of some rather abstract maps from
@xmath to a tensor product of Hilbert spaces. By lemma 3.6 we may define
the following mean embedding map, which is extensively used throughout
the remainder of the thesis.

###### Definition 3.8.

For any isometric embedding @xmath into a @xmath -Hilbert space, we
define the mean embedding of @xmath as the map @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath .

We may also note that when @xmath , that is a probability measure with
finite first moment then @xmath if @xmath (see definition 7.32 ). That
is why we call @xmath the mean embedding map of @xmath . We will later
use that this mapping is linear, so we start by showing this property.

###### Lemma 3.9.

For any isometric embedding @xmath into a @xmath -Hilbert space, it
holds that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath . Thus if @xmath is a @xmath -Hilbert space
then @xmath is linear, and if @xmath is a @xmath -Hilbert space then
@xmath is linear if we view it as a map into the realification @xmath .

###### P 9.

We refer the reader to appendix section 7.4 for the construction of the
realification of a complex Hilbert space.
For any two signed measures @xmath and @xmath , we have to show that
@xmath That is, we need to show that

  -- -------- --
     @xmath   
  -- -------- --

By the unique defining property of the Pettis integral with respect to
finite signed measures, we know that is suffices to show that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . For any @xmath we use the linearity of @xmath and the
unique defining property for each of the individual Pettis integrals to
rewrite the right-hand side

  -- -------- --
     @xmath   
  -- -------- --

This equality indeed holds, and follows from standard integration theory
with respect to signed measures, proving that @xmath is a linear map.
To keep the thesis self-contained we sketch the proof of this equality.
It holds by the standard approach of showing that the equality holds for
characteristic, hence simple functions, followed by an approximation of
measurable functions by simple functions. Lastly one goes to the limit
of these approximations by for example Lebesgue’s dominated convergence
theorem for signed measures.

To summarize our previous findings: if both @xmath and @xmath are metric
spaces of negative type, then we know that there exist two isometric
embeddings @xmath and @xmath into two separable @xmath -Hilbert spaces
@xmath and @xmath .

Hence the idea is now to construct the tensor product map of these two
embeddings @xmath and show that it is Pettis integrable with respect to
any measure in @xmath . An introduction to the construction of the
tensor product of Hilbert spaces can be found in section 7.5 . This
allows for the construction of a mean embedding map @xmath in the same
fashion as we constructed @xmath . The purpose of constructing @xmath ,
is as mentioned in the introduction to section 3 , that we can represent
the distance covariance measure @xmath in terms of this Hilbert space
mapping .
First we prove a lemma, which guarantees sufficient integrability needed
to construct the mean embedding map @xmath .

###### Lemma 3.10.

Let @xmath and @xmath be any two isometric embeddings into two separable
Hilbert spaces with the same scalar field. Then @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

is Pettis integrable with respect to any @xmath .

###### P 10.

Note @xmath is a Hilbert space, and its construction can be found in
section 7.5 . This construction requires that both Hilbert spaces have
the same scalar field, which is why we insist that @xmath and @xmath
have the same scalar field.
The mapping @xmath (with slight abuse of notation) given by @xmath is
realized to be the composition @xmath , where @xmath maps products into
the embedding of simple tensors in @xmath by @xmath (formally @xmath ,
see section 7.5 ) and @xmath given by @xmath .
By lemma 7.43 we have that @xmath is @xmath -measurable and @xmath ,
@xmath are both continuous and hence measurable ( @xmath ), and as a
consequence the bundle map @xmath is @xmath -measurable (cf. theorem
13.10 [ Sch05 ] ). We conclude that the composition @xmath is indeed
@xmath -measurable.
By lemma 7.30 it now suffices to show that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , for which we know that @xmath and @xmath for any @xmath
. Assume that @xmath and note that

  -- -------- --
     @xmath   
  -- -------- --

where the upper bound is square integrable with respect to @xmath (see
proof of lemma 3.6 ). Similarly we also get that @xmath is square
integrable with respect to @xmath . Thus we may conclude that @xmath and
@xmath . Recall from section 7.5 that the inner product @xmath on @xmath
satisfies the following equality for simple tensor products

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by the Cauchy-Schwarz inequality, proving that @xmath is Pettis
integrable with respect to any @xmath .

###### Remark 3.11.

In the above lemma, it is essential that the two Hilbert spaces @xmath
and @xmath are separable if we are to prove measurability of @xmath by
utilizing lemma 7.43 . Hence if we did not make the universal assumption
of only considering separable metric spaces, then theorem 3.7 would only
guarantee isometric embeddings into general Hilbert spaces. As a
consequence it would not be sufficient to assume, that both marginal
metric spaces are of negative type, in order for the Hilbert space
representation of the distance covariance measure to hold, since this is
constructed on the premise that @xmath is measurable. I do not postulate
that measurability does not hold when @xmath or @xmath are
non-separable, only that I failed to show this. Thus, this is yet
another part of the thesis, where we directly use separability of the
marginal metric spaces.

A consequence of the above lemma is that the extension of the class of
mean embedding mappings @xmath to tensor product of isometric embeddings
is well-defined.

###### Definition 3.12.

For any two isometric embeddings @xmath and @xmath into two separable
Hilbert spaces with the same scalar field, we may define the mapping
@xmath by the Pettis integral

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath .

Furthermore, this mean embedding map of the tensor product of isometries
exhibits the same linear properties as the previously defined mean
embedding map. Again this property is not needed for the derivation of
the Hilbert space representation of the distance covariance measure, but
it will be crucial when analysing the representation to find out for
which marginal metric spaces we have the implication @xmath .

###### Corollary 3.13.

If @xmath and @xmath are isometric embeddings into two separable Hilbert
spaces with the same scalar field @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath . Thus, if @xmath , then @xmath is linear, and
if @xmath , then @xmath is linear when viewed as a map into the
realification @xmath .

###### P 11.

The proof follows by arguments identical to those of lemma 3.9 .

Before proceeding with the proof of the alternative representation of
the distance covariance measure in terms of the Hilbert space valued
mapping @xmath , we state a lemma which will help facilitate the
derivation of this representation This lemma especially implies that, if
both @xmath and @xmath are metric spaces of negative type as witnessed
by isometric embeddings @xmath and @xmath respectively, then @xmath and
@xmath have a Hilbert space representation in terms of @xmath and @xmath
respectively.

###### Lemma 3.14.

Let @xmath be a isometric embedding into a separable @xmath -Hilbert
space and let @xmath . It holds that

-   @xmath ,

-   @xmath ,

-   @xmath

for all @xmath .

###### P 12.

Let @xmath and let @xmath be a random Borel element in @xmath defined on
some probability space @xmath with @xmath . Now note that @xmath and
hence

  -- -------- --
     @xmath   
  -- -------- --

(see definition 7.32 ). Thus, using that @xmath is an isometric
embedding, we get

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we used that each term is integrable. Since @xmath is a continuous
linear mapping for any @xmath , the unique defining property of the
Pettis integral yields that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Hence @xmath and

  -- -------- --
     @xmath   
  -- -------- --

proving @xmath , which implies @xmath when inserting into @xmath .
Lastly using the above proven equalities we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Furthermore for any @xmath , the linearity in the first argument and
additivity in the second argument of @xmath yield

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

As a consequence we have that

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

allowing us to conclude that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and in the case that @xmath is a @xmath -Hilbert spaces we have that

  -- -------- --
     @xmath   
  -- -------- --

which is what we wanted to prove.

Now we are ready to prove the alternative representation of @xmath under
the assumption that both @xmath and @xmath are metric spaces of negative
type.

###### Theorem 3.15.

Let @xmath and @xmath have negative type as witnessed by the isometric
embeddings @xmath and @xmath into two separable Hilbert spaces with the
same scalar field. If @xmath have marginals @xmath and @xmath , then it
holds that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

###### P 13.

As in [ Lyo13 ] we show this for the case that @xmath and @xmath are
separable @xmath -Hilbert spaces for simplicity. First note, by lemma
3.14 identity @xmath and the property of @xmath on simple tensors, we
have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath and @xmath for any @xmath and @xmath . The mapping @xmath
given by @xmath is Pettis integrable with respect to @xmath so @xmath is
well-defined. This is seen by a simple replication of the arguments of
lemma 3.10 with the slight adjustment

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

which is still integrable with respect to @xmath , since each term is.
Using that the inner product @xmath is a linear and continuous mapping
when fixing one of its arguments (hence an element of @xmath ), we get
by the defining property of the Pettis integral that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now we simply need to show that @xmath has the wanted representation. To
that end, we may expand @xmath is the following way

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by the linearity of the Pettis integral (cf. corollary 7.29 ), since
each Pettis integral exists (integrability follows from the same bounds
as above). Now note that since @xmath and @xmath are Hilbert spaces we
know that there exist orthonormal bases @xmath and @xmath for @xmath and
@xmath respectively, where @xmath is at most infinitely countable (see
remark 5.9 ). By theorem 7.44 we have that @xmath is an orthonormal
basis for @xmath . As a consequence (cf. theorem 6.26 (2) [ HN01 ] ) we
can expand elements of @xmath in terms of this basis in the following
way

  -- -------- --
     @xmath   
  -- -------- --

where the equality is meant with respect to @xmath -norm convergence.
But for any @xmath we have by the unique defining property of the Pettis
integral

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus the (possibly countably infinite) series converges in @xmath -norm
to both @xmath and @xmath , so we conclude that they coincide. That is

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and by analogous arguments we also get that @xmath . Finally for any
@xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

hence by the unique defining property of the Pettis integral we conclude
that @xmath . Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath is linear (cf. corollary 3.13 ) and that
@xmath , which is seen by an expansion in terms of the orthonormal
basis. That is,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by the unique defining property of the Pettis integral for any @xmath ,
so they coincide by the same arguments as above. We conclude that

  -- -------- --
     @xmath   
  -- -------- --

This marks an end of this section, as we proved the alternative
representation of the distance covariance measure, we set out to find.

### 3.2 Metric spaces of strong negative type

In the previous section we showed that, if @xmath and @xmath where of
negative type, then

  -- -------- --
     @xmath   
  -- -------- --

for some isometric embeddings @xmath and @xmath . In this section we
will define a subclass of negative type metric spaces called metric
spaces of strong negative type. We will show that, if both marginal
spaces @xmath and @xmath are of this so-called strong negative type,
then there exist isometric embeddings @xmath and @xmath into Hilbert
spaces, such that the corresponding mean embedding maps are injective on
a certain class of measures. This injective property is then used to
prove that the linear mean embedding of the tensor product @xmath given
by @xmath is injective on the whole of @xmath . Now since @xmath for any
@xmath , we will argue that this injectivity together with the
alternative representation of @xmath stated above, yield the converse
implication needed to use the distance covariance metric as a direct
indicator of independence. That is, if @xmath and @xmath are metric
spaces of strong negative type, then we show that @xmath if and only if
@xmath for any @xmath with marginals @xmath and @xmath .

In this section we will furthermore show that, if at least one of the
marginal spaces @xmath and @xmath are not of strong negative type (and
non-singleton spaces), then there exists a probability measure in @xmath
for which @xmath but @xmath . This renders @xmath unusable as a direct
indicator of independence in metric spaces that are not of strong
negative type.

Lastly since the definition of a metric space of strong negative type is
rather abstract and not easily recognizable, we will prove a theorem
that identifies every separable Hilbert space as a metric space of
strong negative type. That is, we prove that the distance covariance
measure can directly determine whether random elements with values in
two separable Hilbert spaces are independent or not.
The definition of metric spaces of strong negative type is given in
terms of properties of @xmath , so before we continue we will present a
lemma, connecting the previously analysed class of negative type metric
spaces, with the mapping @xmath .

###### Lemma 3.16.

If @xmath is a metric space of negative type, then

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

###### P 14.

We initially note that @xmath is well-defined for any @xmath , since
@xmath is well-defined on @xmath , which is a vector space by lemma 2.4
.
Let @xmath and let @xmath and @xmath be two independent i.i.d. sequences
of random elements in @xmath defined on a common probability space
@xmath such that @xmath and @xmath . This especially entails that @xmath
is an i.i.d. sequence of random elements in @xmath . Fix @xmath and
consider the @xmath -sample empirical measures @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and note that for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

hence @xmath since @xmath for all @xmath . Thus lemma 2.5 yields that
@xmath . Let @xmath and @xmath . Suppressing the @xmath -notation,
Fubini’s theorem gives that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath , since @xmath is a metric space of negative type and
@xmath . Now define @xmath and note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

is recognized as an @xmath -sample V-statistic (see section 7.2.3 ) with
symmetric kernel @xmath of degree @xmath , given by

  -- -------- --
     @xmath   
  -- -------- --

The kernel @xmath is symmetric in the following sense

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

By the strong law of large numbers for @xmath -statistics ( theorem 7.21
) we get that

  -- -------- --
     @xmath   
  -- -------- --

if @xmath and @xmath . To this end, note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath by lemma 2.5 . By the triangle inequality we get that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath , hence for some @xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We conclude that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus we have a non-positive sequence @xmath , that for some @xmath
converges to the constant @xmath , allowing us to conclude that

  -- -------- --
     @xmath   
  -- -------- --

which proves the claim.

Now we state the definition of metric spaces of strong negative type.

###### Definition 3.17 (Metric spaces of strong negative type).

A metric space @xmath of negative type, is of strong negative type if

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

The next order of business is to prove an equivalence between the above
definition and injectivity of mean embedding maps @xmath on the subspace
@xmath . Recall from theorem 3.7 that @xmath is a metric space of
negative type if and only if there exist isometric embeddings into both
a separable @xmath -Hilbert space and a separable @xmath -Hilbert space.

###### Lemma 3.18.

The following statements are equivalent

-   @xmath is a metric space of strong negative type.

-    There exists an isometric embedding @xmath into a separable @xmath
    -Hilbert space, which induces a mean embedding map @xmath that is
    injective on @xmath .

-    There exists an isometric embedding @xmath into a separable @xmath
    -Hilbert space, which induces a mean embedding map @xmath that is
    injective on @xmath .

###### P 15.

First note that, for any isometric embedding @xmath into a separable
@xmath -Hilbert space, eq. 7 yields that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . Now consider any two probability measures @xmath . By
lemma 2.4 we have that @xmath is a vector space, so @xmath . This signed
measure is obviously an element of @xmath , so the above applies, i.e.

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

By lemma 3.9 we have that @xmath is a linear map on @xmath (when viewing
it as a map into the realification @xmath if @xmath is a @xmath -Hilbert
space), so @xmath . This allows us to conclude that

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

1) @xmath 2) : Assume that @xmath is a metric space of strong negative
type. Since @xmath is especially of negative type theorem 3.7 yields
that there exists an isometric embedding @xmath into a @xmath -Hilbert
space. By definition 3.17 and eq. 10 we have for any two probability
measures @xmath that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is injective on @xmath . Conversely assume that
@xmath is an isometric embedding into a @xmath -Hilbert space such that
@xmath is injective on @xmath . Then for any two probability measures
@xmath , eq. 10 yields that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is a metric space of strong negative type.
1) @xmath 3) : Start by invoking theorem 3.7 to get an isometric
embedding @xmath into a @xmath -Hilbert space. Then the result follows
by arguments identical to the previous equivalence.

This lemma is indeed very crucial for the following work. If we can
identify a single isometric embedding into a separable @xmath -Hilbert
space, which induces a mean embedding map that is injective on @xmath ,
then @xmath is a metric space of strong negative type. This is the
primary tool used, when we show that every separable Hilbert space is of
strong negative type. In the case of finite-dimensional separable
Hilbert spaces we identifying an isometric embedding into a separable
@xmath -Hilbert space @xmath , that induces a mean embedding map that is
injective on @xmath . Furthermore, infinite-dimensional separable
Hilbert spaces are similarly shown to be of strong negative type; by
identifying an isometric embedding into a separable @xmath -Hilbert
space, which induces a mean embedding map that is injective on @xmath .

However, if we assume that @xmath is a metric space of strong negative
type, then we know that there exists an isometric embedding @xmath into
a separable @xmath -Hilbert space that induces a mean embedding map
@xmath that is injective on @xmath . This turns out to be very important
in the following lemmas/theorems used to prove that, if both marginal
metric spaces are of strong negative type, then @xmath . To be perfectly
clear on what is important about this: we only need to consider @xmath
-Hilbert space valued isometric embeddings, a fact which greatly reduces
to complexity of the following proofs (e.g. we do not have to consider
Pettis integration of Hilbert space valued mappings with respect to
complex measures.).
Since we do not have to bother with embeddings into @xmath -Hilbert
spaces, the following statements are only concerned with embeddings into
@xmath -Hilbert spaces, even though they may hold in both cases. The
following lemma yields another domain on which our mean embeddings are
injective.

###### Lemma 3.19.

If an isometric embedding @xmath into a separable @xmath -Hilbert space
induces a injective mean embedding map @xmath , then @xmath is also
injective

###### P 16.

Note that @xmath and that the lemma does not state that @xmath is
injective on @xmath , but that @xmath is injective when restricted the
different domain @xmath .
Consider any isometric embedding @xmath into a separable @xmath -Hilbert
space which induces a mean embedding map @xmath that is injective on
@xmath . We obviously have that @xmath is a @xmath -vector space, and
since @xmath is a linear map ( lemma 3.9 ) it is injective if and only
if the kernel @xmath only contains the zero measure. Thus fix @xmath
such that @xmath , and note that it suffices to show that @xmath .

The Jordan-Hahn decomposition theorem yields that @xmath where @xmath
are non-negative finite singular measures on @xmath , so it suffices to
show that @xmath . Let @xmath . If @xmath , then the non-negativity of
@xmath implies that @xmath . So it only remains to check the case where
@xmath . We note that @xmath , are probability measures satisfying that
@xmath . By the linearity of @xmath we have that

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is injective on probability measures, we conclude that
@xmath . As a consequence @xmath , proving that @xmath is injective on
@xmath .

This above lemma can now be used in conjunction with lemma 3.18 to prove
the following result.

###### Theorem 3.20.

If @xmath is a metric space of strong negative type, then there exists
an isometric embedding @xmath into a separable @xmath -Hilbert space
which induces a mean embedding map @xmath that is injective on the whole
domain @xmath . This isometric embedding might be different from the one
guaranteed to exist by theorem 3.7 .

###### P 17.

Assume that @xmath is a metric space of strong negative type. By lemma
3.18 we know that there exists an isometric embedding @xmath into a
separable @xmath -Hilbert space, which induces a mean embedding map
@xmath that is injective on @xmath . By lemma 3.19 we get that the mean
embedding map @xmath is injective on @xmath .
If @xmath is injective on the whole of @xmath we are done. On the other
hand, if @xmath is not injective on the whole on @xmath , then we can
construct another isometric embedding into a different @xmath -Hilbert
space inducing a mean embedding map, which is.
To see how this is done we assume that @xmath is not injective on @xmath
. Since @xmath is linear and linear maps are injective if and only if
the kernel only contains the zero element (in our case the zero
measure), we may conclude that @xmath . That is, there exists at least
one non-zero measure @xmath such that @xmath .

Next we will realize that every measure in @xmath has a distinct
measurement on the entire space @xmath . That is, for any two distinct
finite signed measures @xmath , it holds that @xmath . To see this,
assume for contradiction that there are two distinct finite signed
measures @xmath with @xmath . But note that @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

by linearity and the injectivity of @xmath on @xmath , a contradiction.
Now we turn our attention to the construction of the @xmath -Hilbert
space @xmath and isometric embedding @xmath into @xmath , which induces
a mean embedding map, that is injective on the whole of @xmath . Let
@xmath be the direct sum of @xmath and @xmath . This space is an @xmath
-Hilbert space given by the Cartesian product of @xmath and @xmath with
addition and scalar multiplication operations working coordinate-wise.
That is,

  -- -------- --
     @xmath   
  -- -------- --

with addition and scalar multiplication defined by @xmath and @xmath ,
@xmath . Furthermore we define an inner product on @xmath , @xmath given
by @xmath . This is easily seen to be an inner product: symmetry,
linearity in the first argument and positive-definiteness are all
inherited by the same properties of the marginal inner product spaces.
Lastly, @xmath is also complete with respect to the naturally induced
metric (cf. section 1.6 [ Con90 ] ) rendering it a @xmath -Hilbert
space.

Separability of @xmath is realized by noting that, if @xmath and @xmath
are countable dense sets in @xmath and @xmath respectively, then @xmath
is countable and dense in @xmath . More specifically for any @xmath
there exist sequences @xmath with @xmath and @xmath with @xmath , hence
the sequence @xmath satisfies @xmath , proving that @xmath is dense in
@xmath .
Now we construct a candidate for @xmath and subsequently show that it is
indeed an isometric embedding of @xmath into @xmath , which induces an
injective mean embedding map @xmath .

Let @xmath be given by @xmath , and note that for any @xmath

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving that @xmath is an isometric embedding into an @xmath -Hilbert
space. Now note that the linear map @xmath given by the Pettis integral
of @xmath with respect to the argument, satisfies that @xmath is the
unique element in @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Fix @xmath and note that Riesz’s representation theorem yields that
there exists a unique @xmath such that @xmath , hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
                       
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus

  -- -------- --
     @xmath   
  -- -------- --

By the linearity of @xmath we have that it is injective if and only if
the kernel @xmath only contains the zero element of @xmath , i.e. the
zero measure. But we note that the zero element of @xmath is @xmath ,
hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We previously showed that every element of @xmath has a distinct measure
on the entire space. Therefore only one measure in @xmath measures the
entire space @xmath to zero. We easily realize that the only measure in
@xmath that assigns the entire space to zero is the zero measure. That
is, @xmath , proving that @xmath is an injective mapping. We conclude
that there exists an @xmath -Hilbert space isometric embedding @xmath ,
which induces a mean embedding map @xmath that is injective on the whole
of @xmath .

Hence we have that, if both @xmath and @xmath are metric spaces of
strong negative type, then we know that there exist two isometric
embeddings @xmath and @xmath into two separable @xmath -Hilbert spaces
that induce mean embeddings @xmath and @xmath that are injective. Lemma
3.22 below will furthermore show that, if this is indeed the case, then
the tensor mean embedding @xmath is injective. From here it is easy to
prove that @xmath , using the alternative representation of the distance
covariance.
The proof of lemma 3.22 will utilize a specific continuous linear map,
hence we start by proving that such a map is indeed unique and
well-defined.

###### Lemma 3.21.

Let @xmath be the tensor product of two separable @xmath -Hilbert
spaces. For any @xmath , there exists a unique continuous and linear map
@xmath , such that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath .

###### P 18.

Since @xmath and @xmath are separable Hilbert spaces we know that there
exist two orthonormal bases @xmath and @xmath for @xmath and @xmath
respectively, where @xmath and @xmath are two at most infinitely
countable index sets (see remark 5.9 ). By theorem 7.44 , we have that
an orthonormal basis for @xmath is given by @xmath . Now for any @xmath
, we define the mapping @xmath by letting

  -- -- --
        
  -- -- --

and extending by linearity to @xmath (finite linear combinations of
elements of @xmath ). That is, for any @xmath , there exist @xmath and
@xmath , @xmath with @xmath , @xmath and a family of @xmath -scalars
@xmath such that @xmath , and then let

  -- -------- --
     @xmath   
  -- -------- --

We equip @xmath with the inner product @xmath (and induced norm @xmath )
given by the restriction of @xmath to @xmath . Now recall that for a
finite sum of mutually orthogonal elements it holds that @xmath , hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by Cauchy-Schwarz’s inequality. Each of the latter factors can be
bounded from above by @xmath ; by noting that @xmath , by Parseval’s
identity since @xmath is an orthonormal basis for @xmath . Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath for any @xmath and that @xmath are normalized.
This proves that @xmath is a bounded linear map. By the bounded linear
transformation (BLT) theorem (see theorem 5.19 [ HN01 ] ) there exists a
unique bounded linear extension of @xmath , to the closure of the
original domain @xmath (seen as a subset of @xmath ). Since @xmath is a
basis for @xmath , we know that @xmath . Hence the theorem gives a
unique bounded linear map @xmath , with the property that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . Assume without loss of generality, that @xmath and
@xmath are infinite-dimensional, such that the respective bases are
countably infinite and enumerated by the natural numbers. Any @xmath and
@xmath , may be expanded in terms of the bases @xmath and @xmath . Thus
we realize (see section 7.5 ) that

  -- -------- --
     @xmath   
  -- -------- --

The linearity and continuity of @xmath yield that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

which is what we wanted to show.

###### Lemma 3.22.

If @xmath and @xmath are metric spaces of strong negative type, then
there exist two isometric embeddings @xmath and @xmath into two @xmath
-Hilbert spaces, such that the mean embedding of the tensor map @xmath
is injective.

###### P 19.

Since @xmath is a metric space of strong negative type we invoke theorem
3.20 to get an isometric embedding @xmath into a separable @xmath
-Hilbert space, which induces a mean embedding @xmath that is injective.
Furthermore since @xmath is a metric space of strong negative type we
invoke theorem 3.7 to say that there exists an isometric embedding
@xmath into a separable @xmath -Hilbert space. Now translate this
embedding, such that the translation has the origin of @xmath in its
image. That is define a new embedding @xmath for some @xmath . This is
obviously still an isometric embedding in the same fashion as @xmath is,
but it indeed contains the origin of @xmath in its images.

Now since the @xmath is of strong negative type lemma 3.18 yields that
there exists an isometric embedding @xmath with mean embedding map that
is injective on @xmath . Now note that, for any @xmath eq. 9 gives that

  -- -------- --
     @xmath   
  -- -------- --

yielding that @xmath is injective, since @xmath is (note that this
actually proves that if a metric space is of strong negative type, then
all mean embeddings of isometric embeddings are injective in this
manner). Lemma 3.19 now yield that @xmath is injective. By the proof of
theorem 3.20 we get, that @xmath is an isometric embedding into @xmath ,
which induces a mean embedding @xmath that is injective.
This explicit construction is necessary, since we later need that @xmath
is constructed in the above manner in terms of @xmath , which contains
the origin of @xmath in its image. More specifically we will use that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and that @xmath contains the origin of @xmath .
Since @xmath is linear it suffices to show that @xmath . Hence consider
any @xmath . That is, a @xmath with @xmath and note that we have to show
that @xmath .

Step 1): Reformulating the problem.
It suffices to show that

  -- -------- --
     @xmath   
  -- -------- --

since such sets form an intersection stable generator for @xmath . Fix
an arbitrary set @xmath and define the finite signed measure (cf. lemma
7.47 ) @xmath by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . The problem has now been reduced to showing that @xmath
.

For any @xmath , it holds that @xmath , hence

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Now note that @xmath , which by definition means that @xmath , so the
above inequality also yields that @xmath . We especially know @xmath is
Pettis integrable with respect to such measures (see lemma 3.6 ),
meaning that @xmath for all @xmath . We also note that @xmath is Pettis
integrable with respect to @xmath , since it is jointly measurable and

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . Hence the Pettis integrals @xmath and @xmath exist, but
we also have that @xmath , so

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath ; by lemma 7.47 . By the unique defining property of the
Pettis integral we get that

  -- -------- --
     @xmath   
  -- -------- --

The linearity and injectivity of @xmath , yield that @xmath if and only
if @xmath , which by the unique defining property of the Pettis integral
and the above equality happens if and only if

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . In the remainder of this proof we use the Riesz
representation theorem to uniquely connect every @xmath with @xmath , by
the identity @xmath for all @xmath .
Now for any @xmath , we define the finite signed measure ( lemma 7.47 )
@xmath by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . Note that @xmath does not necessarily have finite first
moment for all @xmath , rendering the original proof in [ Lyo13 ]
incorrect . We realize that, it suffices to show that @xmath for all
@xmath .
Step 2.1): Proving that @xmath for all @xmath .
Recall the continuous linear map @xmath from lemma 3.21 and that @xmath
is Pettis integrable with respect to @xmath . Lemma 7.31 yields that the
composition @xmath is Pettis integrable with respect to @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , by the linearity of @xmath and the assumption that
@xmath .
Fix @xmath , where @xmath is the image of @xmath . and note that there
exists a @xmath such that @xmath . Recall from eq. 8 , that

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

for any @xmath . Let @xmath , @xmath and @xmath . Equation 11 together
with the initial construction of @xmath , yield that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath by construction of @xmath ; by the triangle inequality and
the reverse triangle inequality. Now note that for any @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

so @xmath . A consequence of the above mentioned Pettis integrability of
@xmath is that,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . By lemma 7.47 we have that @xmath and

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . By the unique defining property of the Pettis integral
we now have that

  -- -------- --
     @xmath   
  -- -------- --

so @xmath by the linearity and injectivity of @xmath on @xmath .
Now fix @xmath . For any fixed @xmath we obviously have that @xmath is
linear, but it is indeed also continuous. To see this, note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by the Cauchy-Schwarz inequality, proving that @xmath is a bounded
linear map since @xmath . Since @xmath can be written as @xmath for
@xmath and @xmath for some @xmath , we get that

  -- -------- --
     @xmath   
  -- -------- --

since @xmath . Hence @xmath , since @xmath for all @xmath .
Now fix @xmath . Note that any point in the closure can be written as
the limit of elements in @xmath . That is, @xmath for some sequence
@xmath . By the continuity of @xmath we get that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , proving that @xmath for all @xmath .
Step 2.1): Proving that @xmath for all @xmath .
Simply note that, since @xmath is a closed linear subspace of @xmath ,
we have that @xmath (cf. corollary 6.15 [ HN01 ] ). That is, for any
@xmath there exist unique elements @xmath and @xmath , such that @xmath
. Hence

  -- -------- --
     @xmath   
  -- -------- --

Lastly, note that since @xmath , then @xmath for any @xmath . Thus we
have that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . We conclude that @xmath , hence @xmath , for any @xmath
. Which is what we wanted to show.

###### Remark 3.23.

Lemma 3.22 is lemma 3.8 in [ Lyo13 ] . However the proof is vastly
different from the original, since the original proof erroneously used
that @xmath is a measure with finite first moment for every @xmath ,
without proving so. In personal communication with Russell Lyons, he
acknowledges that the original proof is indeed not correct. However, he
took a look at the proof again and provided me with the idea that
resulted in the above proof, where we circumvent the problem with @xmath
not necessarily having first moment for all @xmath .

We have now proved every essential lemma, which we will use to prove the
main result of this section.

###### Theorem 3.24.

Let @xmath and @xmath be metric spaces of strong negative type. For any
@xmath with marginals @xmath and @xmath , it holds that

  -- -------- --
     @xmath   
  -- -------- --

###### P 20.

Since @xmath and @xmath are metric spaces of strong negative type, lemma
3.22 yields that there exist isometric embeddings @xmath and @xmath into
separable @xmath -Hilbert spaces such that the linear mapping @xmath is
injective. Let @xmath and note that by theorem 3.15 we have that

  -- -------- --
     @xmath   
  -- -------- --

If @xmath then we obviously have that @xmath . For the converse
statement, note that if @xmath then @xmath . Furthermore, as @xmath we
get that @xmath , since @xmath is a vector space. By the linearity and
injectivity of @xmath we see that the kernel @xmath only contains the
zero measure, hence

  -- -------- --
     @xmath   
  -- -------- --

which concludes the proof.

This concludes the most important part of this section - namely that the
distance covariance measure can be used as a direct measure of
independence, in the case where both marginal metric spaces are of
strong negative type.
An important question to ask is whether metric spaces of strong negative
type is the smallest class of metric spaces where distance covariance
works as a direct measure of independence.

The answer to this question is a partial yes. If we only consider metric
spaces consisting of two or more points, then the answer is yes by
theorem 3.26 below. On the other hand, if one of the marginal spaces
consist only of a singleton then distance covariance can be used as a
direct measure of independence regardless of the properties of the other
marginal metric space. However such scenarios are not important for the
independence problem, because every measure in @xmath is a product
measure as explained by the below remark.

###### Remark 3.25.

Note that in independence testing, we are only interested in metric
spaces @xmath and @xmath which consist of two or more points because, if
one of the spaces consists only of a singleton, then the null-hypothesis
is always satisfied. That is, if @xmath is a space consisting only of a
singleton @xmath , then any probability measure @xmath satisfies the
null-hypothesis @xmath . This is seen by noting that any metric on
@xmath generates the trivial Borel sigma algebra @xmath . As a
consequence we have that @xmath and @xmath , for any @xmath . This
proves that @xmath since @xmath for all @xmath and @xmath (intersection
stable generator for @xmath ).

As mentioned above the distance covariance measure cannot directly be
used to verify independence if the (non-singleton) marginal metric
spaces are not of strong negative type. To this end, we simply need to
show that the distance covariance measure is flawed as a direct
indicator of independence in metric spaces with at least two points that
is not of strong negative type.

###### Theorem 3.26.

Consider two metric spaces @xmath and @xmath both consisting of two or
more points. If at least one of these metric spaces is not of strong
negative type, then there exists a measure @xmath with marginals @xmath
and @xmath , such that @xmath but

  -- -------- --
     @xmath   
  -- -------- --

###### P 21.

Assume without loss of generality, that @xmath is not of strong negative
type, meaning that there exist two distinct probability measures @xmath
such that @xmath . Fix any such @xmath and two distinct elements @xmath
(possible since @xmath is a non-singleton space). Now define the measure
@xmath by

  -- -------- --
     @xmath   
  -- -------- --

The marginals of this measure are easily seen to be @xmath and @xmath .
Since @xmath we know that there exists a set @xmath such that @xmath .
Let @xmath such that @xmath and note that

  -- -------- --
     @xmath   
  -- -------- --

but

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath . Now it suffices to show that this measure
satisfies that @xmath and to this end note that @xmath , where we
denoted @xmath , which allows us to say that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We also have that @xmath and @xmath , hence

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Now note that by using the symmetry of @xmath and @xmath , Fubini’s
theorem yields that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now by direct calculation we may derive all of the following equalities

  -- -------- --
     @xmath   
  -- -------- --

and as a consequence we get that

  -- -------- --
     @xmath   
  -- -------- --

proving that the distance covariance measure is zero, yet @xmath .

Having established that it is sufficient and necessary to assume strong
negativity of the (non-singleton) marginal sample spaces, in order for
the distance covariance to be used as a direct indicator of
independence, we will now explore what kind of metric spaces are indeed
of strong negative type.
It is not easy to directly verify that a metric space is of strong
negative type by the definition, so the last agenda of this section is
to find a subclass of strong negative type metric spaces, with easily
verifiable conditions or at the very least are more well-known. To this
end, we have the following theorem, which tells us that every separable
Hilbert space is a metric space of strong negative type.

###### Theorem 3.27.

Every separable Hilbert space is a metric space of strong negative type.

###### P 22.

We prove this theorem in the case that the Hilbert space @xmath is an
@xmath -Hilbert space, but we note that the arguments are similar for
@xmath -Hilbert spaces, by using the intermediate space @xmath instead
of @xmath used below. Alternatively, instead of considering @xmath , we
could start by embedding @xmath into its realification @xmath with the
additive isometric isomorphism @xmath (see section 7.4 ) and repeating
the steps below. That is, we could use the embedding scheme @xmath and
continue as below.
First we need some initial considerations about the @xmath space. Recall
that @xmath for @xmath is the separable Hilbert space of square summable
@xmath -length real sequences. That is,

  -- -------- --
     @xmath   
  -- -------- --

with coordinate-wise addition and @xmath -scalar multiplication when
equipped with the inner product @xmath is a separable @xmath -Hilbert
space (cf. example 21.2 [ Sch05 ] ). Also note that if @xmath in @xmath
, then @xmath in @xmath for all @xmath , so the coordinate projections
are continuous. If @xmath is an @xmath -dimensional separable @xmath
-Hilbert space then there exists a linear isometric homeomorphism @xmath
. Any orthonormal basis for @xmath has cardinality @xmath , hence we may
enumerate such a basis by @xmath . Now note that

  -- -------- --
     @xmath   
  -- -------- --

is a well-defined mapping from @xmath to @xmath by Bessel’s inequality
@xmath . It is furthermore linear by the linearity of @xmath and the
coordinate-wise addition and scalar multiplication on @xmath . We also
note that, if @xmath , then @xmath for all @xmath . This implies that
@xmath , since @xmath is am orthonormal basis (cf. theorem 6.26 (a) [
HN01 ] ). This proves that @xmath is injective. Surjectivity of @xmath
follows by noting that for any @xmath then @xmath . This holds since
@xmath converges if and only if @xmath by the orthogonality of the terms
and Lemma 6.23 [ HN01 ] . Hence @xmath , since @xmath , where we used
the normality of @xmath and that @xmath . Now fix any @xmath and note
that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is surjective, hence a bijective linear
transformation (isomorphism). By Parseval’s identity it furthermore
holds that @xmath , for any @xmath , proving that @xmath is a linear
isometry - Thus @xmath is an linear isometric isomorphism, and as a
consequence also a linear isometric homeomorphism.
We now consider finite dimensional and infinite dimensional separable
Hilbert spaces separately, since arguments from the case of finite
dimensional Hilbert spaces, will be used later to prove that the
distance covariance measure in metric spaces (which we have derived)
coincides with the distance covariance measure in Euclidean spaces
introduced in .
Finite dimensional separable Hilbert spaces:
If @xmath is a finite dimensional @xmath -Hilbert space, then @xmath is
isometric homeomorphic to @xmath for some @xmath . This is seen by
noting @xmath given by @xmath , is an obvious linear isometric
isomorphism. The composition @xmath is therefore a linear isometric
homeomorphism.
Let @xmath be given by @xmath (zero in zero) and define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the Lebesgue measure on @xmath and @xmath is an
appropriate constant (chosen below). This map is called the Fourier
embedding in [ Lyo13 ] . That @xmath follows from the fact that @xmath ,
hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the fifth equality and constant @xmath are found in lemma 1 [
SRB07 ] . We may also note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

As a consequence of the same lemma used above, we also get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath . Hence if we set @xmath we get that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is an isometric embedding into a @xmath -Hilbert
space, which proves that @xmath is a metric space of negative type by
theorem 3.7 . We need to show that @xmath is of strong negative type,
and by lemma 3.18 it suffices to show that the mean embedding map @xmath
is injective on @xmath . To this end, let @xmath and note that @xmath is
the unique element in @xmath satisfying

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by Fubini’s theorem, where @xmath is the characteristic function
corresponding to the push-forward measure @xmath on @xmath (see section
7.6 for more details on characteristic functions). The application of
Fubini’s theorem is justified by noting that @xmath ) is a @xmath
-finite measure and that @xmath , implying that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used Cauchy-Schwarz’s inequality and that @xmath has finite
first moment. We furthermore used that @xmath is the zero element of
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

To see the @xmath -finitesness, note that @xmath given by @xmath for
@xmath , is a exhausting sequence of Borel sets with @xmath . This is
seen by noting that @xmath for all @xmath (recall that we set @xmath ),
such that @xmath , proving that @xmath is a @xmath -finite measure.

This proves that @xmath in @xmath .
Assume for contradiction that @xmath is not injective on @xmath . Then
there exists two distinct Borel probability measures @xmath such that
@xmath in @xmath , which by the above derivation happens if and only if
@xmath for @xmath -almost all @xmath . Since @xmath for all @xmath we
may conclude that @xmath for @xmath -almost all @xmath . By theorem 7.46
item creftype 3 and creftype 6 we have that @xmath , since their
characteristic functions coincide.

Now recall that @xmath is a isometric homeomorphism, i.e. it has a
well-defined inverse @xmath that is continuous and measurable. As a
consequence we have that @xmath and the pre-image of the inverse
coincides with the image @xmath , such that @xmath for any @xmath .
Hence for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

a contradiction. We conclude that @xmath is injective on @xmath ,
proving that @xmath is a metric space of negative type.
Infinite dimensional separable Hilbert spaces:
Let @xmath be a infinite dimensional separable @xmath -Hilbert space. We
will show that @xmath equipped with the naturally induced metric @xmath
is of strong negative type by showing that there exists an isometric
embedding @xmath , into a separable @xmath -Hilbert space, which induces
an injective mean embedding map @xmath , where @xmath is a probability
measure on @xmath and @xmath is the Lebesgue measure on @xmath . This
will indeed imply that @xmath is a metric space of strong negative type
by lemma 3.18 .
The isometric embedding we are going to construct requires the
definition of another map, which we start by defining. Let @xmath be an
independent and identically standard normal distributed sequence of
random variables defined on some probability space @xmath . Let @xmath
denote its natural embedding into @xmath equipped with the product
@xmath -algebra @xmath (see section 7.1 ). @xmath is obviously
measurable and we may denote its law @xmath on @xmath . For any @xmath
one may show that

  -- -------- --
     @xmath   
  -- -------- --

by the use of characteristic functions and Levy’s continuity theorem.
But we also have that @xmath , so @xmath by uniqueness of limits. Hence,
@xmath @xmath -almost surely. We may also note that @xmath is
half-normal distributed, so

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

where @xmath . Let @xmath be given by

  -- -------- --
     @xmath   
  -- -------- --

and note by the above convergence in distribution we have that for fixed
@xmath , @xmath converges in @xmath for @xmath -almost all @xmath .
Furthermore let @xmath denote the restriction of @xmath to @xmath . Also
note that @xmath and @xmath are respectively @xmath - and @xmath
-measurable mappings, since they are given by the limit supremum of
measurable mappings (coordinate projections are continuous, hence
measurable). In general, when we write @xmath we mean the restriction of
@xmath to @xmath .
Let @xmath denote the linear isometric homeomorphism defined in the
beginning of the proof. We define our embedding @xmath into the
separable @xmath -Hilbert space @xmath by

  -- -------- --
     @xmath   
  -- -------- --

To see that @xmath for all @xmath , we simply note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

which is finite, where @xmath . Furthermore by similar reasoning

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where @xmath , proving that @xmath is an isometric embedding. It remains
to be shown that the mean embedding map @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is injective on @xmath . That is, @xmath implies @xmath for any two
measures @xmath . Thus fix @xmath with @xmath and denote @xmath . By the
same arguments as in the above proof for finite-dimensional Hilbert
spaces it suffices to show that @xmath as measures on @xmath .
Now we will show that these measures on @xmath are uniquely determined
by their finite-dimensional distributions pushed forward to @xmath by
dual mappings, what this exactly entails will become clear below.

First we show that @xmath . Note that @xmath , which follows by noting
that all coordinate projections @xmath are continuous mappings, by the
definition of the product topology that induced the Borel @xmath
-algebra @xmath . As a consequence they are also Borel measurable,
rendering the map @xmath defined by @xmath Borel measurable. Now note
that @xmath for any @xmath and @xmath , so @xmath . We now show that
@xmath by showing each is included in the other. A generator for the
trace @xmath -algebra @xmath is given by @xmath (see section 7.1 ) and
all these sets are open in @xmath since @xmath is continuous for all
@xmath . This proves that @xmath , since we have inclusion of their
generators. For the converse we show that every open ball in @xmath lies
in @xmath , proving that @xmath since the open balls in @xmath generate
@xmath . To this end, note that every open ball in @xmath has the form
@xmath for some @xmath and @xmath . We realize that @xmath is Borel
measurable, hence the above set becomes @xmath Thus @xmath .

Let @xmath and @xmath denote the extensions of @xmath and @xmath to
@xmath given by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath . Since @xmath , it suffices to show that @xmath .
Furthermore it is well know that probability measures of @xmath are
uniquely determined by their finite dimensional distributions meaning
that it suffices to show that @xmath for all @xmath . This equality can
be expressed through equality of their respective characteristic
functions, see theorem 7.46 item creftype 3 and creftype 6 . We note
that

  -- -------- --
     @xmath   
  -- -------- --

showing that it suffices to show that @xmath on @xmath for all @xmath
and @xmath , where we use Riesz’s representation theorem to uniquely
connect dual mappings @xmath with a corresponding element @xmath such
that @xmath .

Now we show that it suffices to prove that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath in order for @xmath , which is exactly what we
meant by saying that probability measures on @xmath are uniquely
determined by their finite dimensional distributions pushed forward to
@xmath by dual mappings. Simply note for any @xmath , @xmath and @xmath
it holds that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving the claim, since @xmath is an intersection stable generator for
@xmath .
Hence with a little manipulation of the above representation, we see
that it suffices to show that the following cdfs coincide. That is,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , @xmath and @xmath .
To this end we need a property implied by the assumption that @xmath .
Note that @xmath is the unique element in @xmath satisfying

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath , where we used Riesz’s representation theorem and
Fubini’s theorem, which is justified since

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

by Cauchy-Schwarz’s inequality and that @xmath has finite first moment.
We conclude that the mean embedding @xmath is the map in @xmath given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Let @xmath denote the left shift operator, i.e. @xmath for any @xmath .
For any @xmath and @xmath , we write @xmath and @xmath such that @xmath
. Note that @xmath , since @xmath is @xmath -measure preserving ( @xmath
is stationary). Furthermore we note that @xmath , such that @xmath .
Since we assumed that @xmath , Tonelli’s theorem yields that for any
@xmath

  -- -------- -------- --
     @xmath            
              @xmath   
              @xmath   
  -- -------- -------- --

Hence for every @xmath there exists a @xmath -almost sure set @xmath ,
such that for every @xmath there exists a @xmath -almost everywhere set
@xmath , such that for every @xmath there exists a @xmath -almost
everywhere set @xmath , with the following properties. For any @xmath it
holds that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath            
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

First we show that for any @xmath this statement can be strengthened to
all @xmath . Fix any @xmath and @xmath and let @xmath . Note that we can
find a sequence @xmath such that @xmath . Now let @xmath and @xmath be
defined on some probability space @xmath , and note that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for @xmath by continuity of the inner product, so we also have
convergence in distribution. Hence also point-wise convergence of the
cdfs for every continuity point of the limit distribution cdfs. Let
@xmath and @xmath denote the corresponding discontinuity points of the
limit cdfs, which are at most countably infinite ( @xmath -nullsets).
Thus for every @xmath in the @xmath -almost everywhere set

  -- -------- --
     @xmath   
  -- -------- --

we have that

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

So we have two mappings that are cadlag in @xmath (they are cdfs) which
coincide @xmath -almost all @xmath . By trivial @xmath -arguments
(similar to those below) they must coincide for all @xmath .

We conclude that for every @xmath there exists a @xmath -almost sure set
@xmath such that for all @xmath it holds that

  -- -------- -------- -- ------
              @xmath      (13)
     @xmath   @xmath      
  -- -------- -------- -- ------

for all @xmath and @xmath
With this in mind, we fix @xmath . Note for any @xmath we have that
@xmath and this entails that @xmath for all @xmath and

  -- -------- --
     @xmath   
  -- -------- --

Furthermore since @xmath is an linear isometry, an application of the
abstract change of variable theorem gives us that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for all @xmath , since @xmath (finite first moment). As a consequence
the Lebesgue dominated convergence theorem yields that @xmath , implying
that there exists a @xmath (dependent on @xmath ) such that

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

Now let @xmath be the set, given by @xmath and note that @xmath , since
@xmath is @xmath -measurable. Furthermore

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (15)
  -- -------- -------- -- ------

where we used Markov’s inequality, Tonelli’s theorem, eq. 12 and eq. 14
. For any @xmath we denote the section set @xmath . By theorem 3.4.1 [
Bog07b ] we have that @xmath is @xmath -measurable and

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then we have that @xmath . This is in contradiction with eq.
15 , hence @xmath . Thus there exists a @xmath -positive probability set
@xmath such that @xmath for all @xmath . Now note that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is non-empty. Hence there exists a @xmath such that

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

and

  -- -------- -------- -- ------
              @xmath      (17)
     @xmath   @xmath      
  -- -------- -------- -- ------

for all @xmath and @xmath . Moreover these two properties also implies
that

  -- -------- --
     @xmath   
  -- -------- --

To see this, note that we may take any sequence @xmath such that @xmath
. It holds that @xmath for any @xmath , so continuity from below and eq.
17 (with @xmath and @xmath ) yield that

  -- -------- -------- -- ------
     @xmath   @xmath      (18)
              @xmath      
              @xmath      
              @xmath      
  -- -------- -------- -- ------

Thus by eqs. 18 , 17 and 16 we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Now note that, when suppressing @xmath , we get

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath and @xmath . We note that the expression after the first
strict inequality is interchangeable in @xmath for all @xmath ; by eq.
17 . As a consequence we have that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath . Note that @xmath depends on @xmath , but for
any @xmath it holds that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath , so the inequalities also hold for any @xmath
. Now fix @xmath and note that for any @xmath we may choose @xmath ,
implying that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , @xmath and @xmath . Since @xmath and @xmath are càdlàg
functions, we may let @xmath and get that

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

for all @xmath and @xmath . Now fix @xmath , and let @xmath and @xmath
be the sets of discontinuities of @xmath and @xmath on @xmath
respectively. For any @xmath , eq. 19 yields that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

For any @xmath there exists a sequence @xmath such that @xmath , hence
@xmath for all @xmath , since there is at most a countable number of
discontinuities of càdlàg functions over a finite interval (e.g. @xmath
). As a consequence of right-continuity of the cumulative distribution
functions we get that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath for all @xmath . Note @xmath and @xmath were
arbitrarily chosen, so we conclude that @xmath for all @xmath and @xmath
. We conclude that @xmath is injective on @xmath , such that @xmath is
of strong negative type.

This concludes the section on distance covariance in metric spaces of
strong negative type.

## 4 Properties of distance covariance in metric spaces

In this section, we will derive some rudimentary properties of the
distance covariance measure in metric spaces. These properties include
absolute bounds on the distance covariance measure, and when these
bounds are attained. We will also show that, when the marginal spaces
are finite-dimensional Euclidean spaces, then our distance covariance
measures coincide with the squared distance covariance measure from [
SRB07 ] .
We stress that distance covariance measure cannot be used to measure any
kind of dependence degree. It only serves as a direct indicator of
independence or the alternative in metric spaces of strong negative
type.
As previously, let @xmath and @xmath be separable metric spaces and
@xmath be random Borel elements with values in @xmath and @xmath , with
simultaneous distribution @xmath and marginal distributions @xmath and
@xmath . Now recall the alternative representation of @xmath from
section 2 given by

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is an independent copy of @xmath . Similarly we may define
@xmath . By reasoning similar to that of the derivation of the above
representation we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is an independent copy of @xmath .

###### Theorem 4.1.

For any random element @xmath with marginals @xmath and @xmath , it
holds that

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, if both metric spaces @xmath and @xmath are of negative
type, then

  -- -------- --
     @xmath   
  -- -------- --

###### P 23.

Recall that by lemma 2.7 we have that @xmath and vice versa for @xmath .
As we did below definition 2.8 we may view @xmath and @xmath as mappings
from @xmath and say that @xmath . This allows us to use Cauchy-Schwarz’
inequality to get

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Utilizing Tonelli’s theorem on both integrals and the fact that the
@xmath -coordinates in the first integral and @xmath -coordinates in the
second integral are superfluous, the last expression equals

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is an independent copy of @xmath and @xmath is an
independent copy of @xmath . This proves the first inequality of the
theorem.
Recall the two inequalities from eq. 4 in lemma 2.7 : @xmath and @xmath
. These inequalities show that @xmath , and therefore we have that

  -- -------- --
     @xmath   
  -- -------- --

Thus Fubini’s theorem yields that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and analogously @xmath . Thus using the representation from above and
expanding, we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Since @xmath we have that @xmath , proving that the first three terms
have an upper bound of zero. The fifth and sixth term was shown above to
have expectation zero. Thus using linearity of the expectation (all
individual terms are integral)

  -- -------- -------- --
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

where we have used that @xmath , so @xmath . By similar arguments we
also get that @xmath , proving the second inequality of the theorem.
As regards the last inequality, assume that @xmath and @xmath are of
negative type. By theorem 3.7 there exist isometric embeddings @xmath
and @xmath into Hilbert spaces. Then by the representation of @xmath
found in theorem 3.15 , we have that

  -- -------- --
     @xmath   
  -- -------- --

As seen in the above theorem, we have that

  -- -------- --
     @xmath   
  -- -------- --

Hence, if either @xmath or @xmath , then @xmath . Thus for metric spaces
of strong negative type, we might have that information only about
@xmath or @xmath , would be sufficient to conclude that @xmath . There
is only one scenario where this is possible, and that is when the
marginal distributions are degenerate (concentrated on a singleton),
which automatically implies independence.

However, as we shall see in the below theorem, this is also the case for
arbitrary separable metric spaces. This next theorem also entails that
@xmath can only attain the upper bound @xmath if both @xmath and @xmath
are concentrated at two points in @xmath and @xmath respectively.
Before stating the above mentioned theorem, we need to prove a lemma
regarding the support of Borel probability measures on separable metric
spaces.

###### Lemma 4.2.

Every Borel probability measure @xmath on a separable metric space has
support of full measure. That is, @xmath and as a consequence @xmath
since @xmath .

###### P 24.

The support @xmath of a Borel probability measure @xmath on a metric
space @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the set of all open neighbourhoods of @xmath . Hence we
also have that the complement of the support of @xmath is given by

  -- -- --
        
  -- -- --

where @xmath are the open sets of @xmath (this latter representation
coincides with the definition in [ Bog07a ] p. 77). We obviously have
that @xmath is an open cover of @xmath and by separability (see [ Bil99
] section M3) of @xmath we know that it has a countable sub-cover @xmath
. Hence by the countable sub-additivity of @xmath we get that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath has full measure.
We recall from section 7.1 that @xmath equipped with the product
topology is a metrizable topological space - the maximum metric @xmath
given by @xmath induces the product topology. By theorem 7.2 it also
holds that @xmath is separable and that @xmath , implying that @xmath is
a Borel probability measure on the separable metric space @xmath , so we
also have that @xmath .
Lastly we show that @xmath . The inclusion @xmath easily follows from
contraposition. Let @xmath and assume without loss of generality that
@xmath . Then there exists an @xmath such that @xmath . We furthermore
have that @xmath with @xmath , proving that @xmath .

The converse inclusion @xmath follows by noting that for any @xmath we
have @xmath that @xmath . Hence fix @xmath and note that for any open
neighbourhood @xmath , the definition of open sets in metric spaces,
yields there exists a @xmath such that the open ball @xmath . It is
obvious by the definition of the maximum metric that @xmath , hence we
have that

  -- -------- --
     @xmath   
  -- -------- --

since @xmath and @xmath , proving that @xmath .

###### Remark 4.3.

The assumption that @xmath is a separable metric space is essential to
the above proof. In fact there exist Borel probability measures on a
topological space which have no support; see example 7.1.3 [ Bog07a ] .
Whether or not the topological space considered in example 7.1.3 [
Bog07a ] is metrizable is not investigated further, but it serves as an
indicator that we might run into further trouble if we did not restrict
ourselves to separable metric spaces.

###### Theorem 4.4.

Let @xmath be a metric space. For any random element @xmath we have that

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

###### P 25.

Let @xmath be an independent copy of @xmath and recall that @xmath .
First equivalence: Since @xmath is separable we have by lemma 4.2 that
@xmath , and as a consequence we have that

  -- -------- --
     @xmath   
  -- -------- --

To see this note that

  -- -------- --
     @xmath   
  -- -------- --

The latter set for which the equality must hold may obviously be
intersected with another almost sure set at no cost. That is, for @xmath
-almost all @xmath . But for contradiction assume that there exists an
@xmath where @xmath . We recall by definition of the support of @xmath
(see previous lemma) that @xmath for every open neighbourhood @xmath of
@xmath . The function @xmath is continuous since @xmath and @xmath are
continuous. Hence there exists a @xmath such that @xmath for all @xmath
. But since @xmath is an open neighbourhood of @xmath we have that
@xmath , proving that @xmath with positive probability - a
contradiction.
Assume that @xmath is degenerate. That is, @xmath almost surely or
equivalently @xmath , for some @xmath . We obviously have that @xmath
for all @xmath and @xmath . Thus

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , so @xmath .
Conversely if @xmath or equivalently @xmath for all @xmath , then we
have that @xmath , proving that @xmath for all @xmath . As a
consequence, we have that @xmath , hence @xmath for all @xmath . In
other words, the distance between any two points in the non-empty
support @xmath is zero, proving that @xmath is a singleton or
equivalently that @xmath is degenerate.
Second equivalence: First note that by the above proof we have that, if
@xmath is concentrated on a single point, then @xmath . Additionally
note that, if @xmath is concentrated on two points @xmath or
equivalently @xmath , then by direct calculation we see that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where we used the symmetry of @xmath and assumed that @xmath and @xmath
for some @xmath .
For the converse, assume for contradiction that @xmath and @xmath .
Recall from the proof of theorem 4.1 that @xmath where the only upper
bound we used was

  -- -------- --
     @xmath   
  -- -------- --

Hence we have @xmath if and only if @xmath almost surely. In other
words, we have equality if and only if @xmath for all @xmath with @xmath
. Now fix any @xmath with @xmath and note that

  -- -------- --
     @xmath   
  -- -------- --

so we must have that @xmath for @xmath -almost all @xmath . As a
consequence, it must especially hold that @xmath for all @xmath , by
continuity of @xmath . Thus

  -- -------- --
     @xmath   
  -- -------- --

and since we assumed that @xmath , there exist three such points. That
is, there exist three distinct points @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

By inserting the second equation in the first, we get that

  -- -------- --
     @xmath   
  -- -------- --

a contradiction, proving that @xmath .

Now to the last item on the agenda of this section, namely proving that
the distance covariance measure in metric spaces coincides with the
distance covariance from [ SRB07 ] , when the marginal spaces are
finite-dimensional Euclidean spaces.

###### Theorem 4.5.

Let @xmath have marginals @xmath and @xmath for some @xmath . It then
holds that the square root of the distance covariance measure in metric
spaces coincides with the distance covariance in Euclidean spaces from [
SRB07 ] . That is,

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath , @xmath and @xmath are the characteristic functions
corresponding to the probability measures @xmath , @xmath and @xmath
respectively. @xmath is the Lebesgue measure on @xmath and @xmath for
any @xmath .

###### P 26.

First note that @xmath and @xmath are indeed separable Hilbert spaces
for any @xmath and therefore theorem 3.27 yields that they are of strong
negative type. As a consequence of theorem 4.1 we have that @xmath , so
@xmath is indeed well-defined.
In the proof of theorem 3.27 we saw that with @xmath be given by @xmath
(set @xmath ), then @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is a well-defined isometric embedding into the @xmath -Hilbert space
@xmath . We also saw that the mean embedding of @xmath was given by
@xmath for any @xmath . We define the isometric embedding on the other
marginal space in an identical fashion, @xmath given by @xmath which has
mean embedding given by @xmath for any @xmath . Now note that @xmath and
@xmath are separable since @xmath and @xmath are separable (cf. theorem
4.13 [Bre10]). Hence we know that the map @xmath taking simple tensors
from @xmath to @xmath by

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath , extends uniquely to a unitary isomorphism of
@xmath onto @xmath (see p. 51 [RS72] and theorem 7.16 [Fol95]). That is,
we have that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

is an isometry satisfying

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and it has a well-defined inverse @xmath . Now note that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath . Hence we get that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath and @xmath . For notational simplicity in the following
arguments, we define the maps @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Now fix any @xmath and note that by the Riesz representation theorem,
there exists a unique @xmath such that @xmath . As a consequence we have
that the mean embedding @xmath fulfils

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Since @xmath was arbitrarily chosen, the unique defining property of the
Pettis integral yields that @xmath . In an identical fashion we deduce
that @xmath . It obviously holds that @xmath for any @xmath , and since
@xmath is an isometry, theorem 3.15 yields

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

which is what we wanted to show.

###### Remark 4.6.

The above theorem becomes rather trivial, if we assume that @xmath . In
this case we have that

  -- -------- --
     @xmath   
  -- -------- --

by the Cauchy-Schwarz inequality. Now note that (see next section)

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath are mutually independent random elements with distribution
@xmath . By expanding, we see that it is the expectation of a sum of
individually integrable terms (use triangle inequality to get terms on
the above form). As a consequence, we may split up the expectation into
a sum of expectations. By tirelessly reducing the expression of 16
expectations, one gets that

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

where @xmath and @xmath are mutually independent random elements with
distribution @xmath . Hence in the case that @xmath and @xmath are
finite-dimensional Euclidean spaces we get that @xmath coincides with,
the square of the distance covariance @xmath from [ SRB07 ] , and the
square of the Brownian distance covariance @xmath from [ SR09 ] (compare
with the expressions in theorem 7 and 8 in [ SR09 ] ).

With this remark, we end the section on basic properties of the distance
covariance measure.

## 5 Asymptotic consistent tests of independence

In the previous sections we defined the distance covariance measure

  -- -------- --
     @xmath   
  -- -------- --

and showed that it can be used as a direct indicator of independence,
whenever the marginal spaces @xmath and @xmath are metric spaces of
strong negative type. That is,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . However as @xmath is only know when @xmath is, we can
not directly use it in the non-parametric independence problem stated in
the introduction of this thesis.
Let us recall the probabilistic set-up and the non-parametric
independence problem :
@xmath and @xmath denotes two generic metric spaces, and @xmath is an
independent and identically distributed sequence of random Borel
elements, defined on a probability space @xmath . It is assumed that,
each pair of random elements @xmath takes values in the product space
@xmath , such that

  -- -------- --
     @xmath   
  -- -------- --

for each @xmath . Now suppose that we are given a finite collection of
paired sample points @xmath , where each pair @xmath is a realization of
@xmath . Given this collection of samples how can we, without
restricting @xmath to a specific parametric class of distributions, draw
inference on whether to reject the null-hypothesis of independence

  -- -------- --
     @xmath   
  -- -------- --

in favor of the alternative hypothesis of dependence

  -- -------- --
     @xmath   
  -- -------- --

In this section, we will finally provide an answer this question by
constructing estimators of the distance covariance measure @xmath and
utilizing their asymptotic properties to create asymptotically
consistent tests of independence.

The asymptotic properties of the estimators holds for general separable
metric spaces, but when constructing the asymptotically consistent tests
of independence we will restrict the both marginal spaces to be metric
spaces of strong negative type (e.g. separable Hilbert spaces), since we
need to utilize that @xmath .
The construction of these asymptotically consistent tests of
independence, is split up into the three following subsections:
Section 5.1 We introduce two different estimators for @xmath , which
will yield two different statistical tests of independence. It is seen
that, @xmath is a so-called regular functional, and one may recall that
such functionals are the building blocks of the so-called @xmath - and
@xmath -statistic estimators. Our choice of estimators for @xmath are
therefore given by a @xmath - and a @xmath -statistic estimator. Section
5.2 We show that the estimators from section 5.1 are both strongly
consistent and if scaled correctly also possess rather complicated
asymptotic distributions, under certain moment conditions of the
underlying distribution @xmath . Section 5.3 We formally describe the
statistical models for which the asymptotic properties from section 5.2
yield asymptotically consistent tests of independence. These tests turns
out to have non-traceable rejection thresholds, so we end this last
section by describing how one may reasonably bootstrap the rejection
thresholds.

### 5.1 Estimators for the distance covariance measure

This section is dedicated to defining two different estimators of the
distance covariance measure. First we derive an alternative
representation of @xmath , and to that extent define @xmath , @xmath and
@xmath by

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath .

###### Lemma 5.1.

For any @xmath it holds that @xmath and that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath and @xmath .

###### P 27.

First let @xmath denote either @xmath or @xmath and let @xmath be
arguments in the corresponding space. Note that the two first
inequalities of lemma 7.48 yield

  -- -------- --
     @xmath   
  -- -------- --

The fact that @xmath is @xmath -measurable is seen by noting that @xmath
is the product of sums where each term is measurable. Using the above
upper bound of @xmath we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used Tonelli’s theorem in the second equality, abstract change
of variable in the third and lemma 2.5 to bound the last expression. As
regards to the two equalities one can easily show, using similar
arguments as above, that the two integrals exists. Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

using linearity, Fubini’s theorem and that @xmath is a probability
measure. Analogous arguments yield the equality for @xmath .

An immediate consequence of the above lemma is that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

that is, @xmath is a regular functional with kernel @xmath of degree
@xmath . Regular functionals are the building blocks of @xmath - and
@xmath -statistics so it seems quite intriguing to create our estimators
using such statistics.

###### Remark 5.2.

The kernel @xmath is in general not symmetric. To see this, let @xmath
be equipped with the Euclidean metric. By insertion we see that

  -- -------- --
     @xmath   
  -- -------- --

but if we permutate the third and fourth argument pairs, we get that

  -- -------- --
     @xmath   
  -- -------- --

proving that is @xmath is not symmetric.

Now we may define the estimators of @xmath . Before doings so, recall
that the random empirical measure @xmath of @xmath , based on the first
@xmath samples @xmath of our sample sequence, is defined in the usual
way as

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath

###### Definition 5.3 (Estimators for the distance covariance measure).

We define the empirical distance covariance as the (in general biased)
plug-in estimator @xmath . It is easily seen that this estimator is a
V-statistic with non-symmetric kernel @xmath of degree 6 given by

  -- -------- --
     @xmath   
  -- -------- --

In addition to the V-statistic estimator, we may also consider the
corresponding U-statistic with kernel @xmath . That is, for a sample
size of @xmath , the unbiased estimator given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

###### Remark 5.4.

In [ Lyo13 ] , Russell Lyons only considers the V-statistic plug-in
estimator @xmath . We introduce a second estimator given by the above
U-statistic. This new estimator was deviced after discovering that the
original moment assumptions in [ Lyo13 ] were insufficient to guarantee
strong consistency of @xmath (explained in detail in the next section).
As we shall see later, the U-statistic estimator @xmath is guaranteed to
be strongly consistent, under weaker moment conditions than those for
@xmath .We refer the reader to Sections 7.2.3 and 7.2.2 for quick
introductions to the theory of U- and V-statistics. We will however note
that the theory for U- and V-statistics typically works from the outset
of symmetric kernels.
In the case of the above U-statistics we can write it in the regular
form with a symmetric kernel (see section 7.2.2 for explanation). That
is,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the symmetrized version of @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

with @xmath being the set of all permutations of @xmath . So instead of
working with the U-statistic @xmath with an (in general) non-symmetric
kernel, we can work with @xmath with symmetric kernel for which most
theorems regarding U-statistics are formulated.
Likewise in the case of V-statistics we may note that, for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

by the abstract change of variable theorem, where @xmath . This shows
that the integral of @xmath with respect to @xmath is invariant under
permutations of the integrand’s arguments. By Minkowski’s inequality
this especially implies integrability of @xmath with respect to @xmath ,
but it also shows that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath . Since @xmath for any @xmath (it is a finitely supported
probability measure) we get that

  -- -------- --
     @xmath   
  -- -------- --

which proves that instead of working with the V-statistic @xmath with an
(in general) non-symmetric kernel, we can work with @xmath with
symmetric kernel for which most theorems regarding V-statistics are
formulated.

Furthermore, the @xmath -statistic can easily be rewritten in a form
similar to the estimator from [ SRB07 ] . To this end, note that it
obviously also hold that @xmath for any @xmath , so by utilizing the
expression from eq. 20 we have that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for any @xmath .

### 5.2 Asymptotic properties of the estimators

Now we start by justifying the choice of the above estimators for
distance covariance. It turns out both estimators are strongly
consistent and we are able to derive asymptotic distributions under the
null-hypothesis, which allows for the construction of asymptotically
consistent statistical tests for independence.

###### Theorem 5.5 (Strong consistency of estimators).

If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

If it furthermore holds that @xmath for any @xmath and @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

###### P 28.

We start by showing the almost sure convergence of @xmath . By the above
remark 5.4 it suffices to show that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the symmetrized version of @xmath defined in remark 5.4
. By the strong law of large numbers for V-statistics - theorem 7.21 -
it suffices to show that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . Since @xmath is the sum of all permutations of the
given indices the above integrability conditions are especially
satisfied if

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath ; by the sub-additivity of @xmath for @xmath . Hence let
@xmath and set @xmath . Note that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for some @xmath and @xmath , where we used the first two inequalities of
lemma 7.48 . In all terms above we either have that the indices of the
random elements are distinct or coincide. Lets consider any of the above
terms separately. If the indices are distinct the assumption of @xmath
is sufficient for finiteness since @xmath ; e.g. @xmath . However, in
the case that they coincide the additional assumption that

  -- -------- --
     @xmath   
  -- -------- --

implies that the @xmath ’th moments are finite, since @xmath whenever
one or more indices coincide. We conclude that under the assumptions of
the theorem, the claimed almost sure convergence of the V-statistic
hold.
As regards to the almost sure convergence of the U-statistic, we also
note that by the above remark 5.4 , we have to show almost sure
convergence

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the symmetrized version of @xmath mentioned before. The
strong law of large numbers for U-statistics - theorem 7.19 - yields the
wanted almost sure convergence if

  -- -------- --
     @xmath   
  -- -------- --

We note that in the case of U-statistics opposed to V-statistics, we
only need integrability of the kernel when all arguments are independent
and this is the reason for weaker moment assumptions. By the triangle
inequality, it suffices to show that each term of @xmath is integrable.
That is,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . We note that for any such @xmath the arguments in the
above expectation are mutually independent copies of @xmath implying
that their simultaneous distribution is given by the six-fold product
measure @xmath . Hence for any @xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

where we applied lemma 5.1 to ensure finiteness. We conclude that the
claimed almost sure convergence of the U-statistic hold.

###### Remark 5.6.

The almost sure convergence of the empirical distance covariance @xmath
in the previous theorem is proposition 2.6 in [ Lyo13 ] by Russell
Lyons. In that paper the almost sure convergence is claimed to hold
whenever @xmath , i.e. the same conditions for which we showed the
almost sure convergence of the U-statistic estimator. In [ Lyo13 ] it is
proved that @xmath , after which it is stated that the almost sure
convergence follows. The weakest conditions (that I am aware of) under
which the SLLN for V-statistics applies are those of [ GZ92 ] (see
theorem 7.21 ).

However I am unable to verify the conditions of theorem 7.21 under the
sole assumption that @xmath . The problem lies within showing sufficient
integrability of the kernel @xmath whenever two or more indices
coincide. In [ Lyo13 ] @xmath is bounded from above by using the
triangle inequality on the factors @xmath and @xmath . The two
inequalities for @xmath used in [ Lyo13 ] are a subset of all such
inequalities, which are as follows

  -- -- --
        
  -- -- --

where @xmath or @xmath and @xmath are elements in the corresponding
metric space (see lemma 7.48 ). It is easy to see that, whenever the
arguments in @xmath have distinct indices, then the 1st and 2nd (as we
used above) or the 12th and 15th (as Lyons used) inequality used on
@xmath and @xmath respectively yield independent factors. Hence allowing
us to conclude finiteness of @xmath whenever the marginal distributions
of @xmath and @xmath have finite first moments. However, in the case
that the indices are not distinct, e.g. when @xmath , the above
inequalities do not yield independent factors. To see this, note that
any combination of the above inequalities used on @xmath and @xmath
gives upper bounds dependent on @xmath and @xmath respectively. That is,
the upper bounds for @xmath and @xmath are dependent on @xmath or @xmath
and @xmath or @xmath respectively. Since @xmath we get that all possible
combinations of the above inequalities result in upper bound factors
that are mappings of @xmath and @xmath respectively. Since @xmath and
@xmath are in general not independent we get upper bounds which possibly
consists of dependent factors. I have been unsuccessful in resolving
this matter, hence I assumed the ad-hoc condition @xmath .

The ad-hoc condition is obviously satisfied if @xmath , which in the
case when @xmath are equipped with the Euclidean metric is equivalent to
the existence of the covariance @xmath . By the virtue of Cauchy-Schwarz
inequality this stronger integrability condition is also satisfied if
@xmath
In personal communication with Russell Lyons he acknowledges that his
original conditions are insufficient and recommended that they should be
replaced by second moments. However, as seen above the slightly weaker
ad-hoc condition which we used suffices.

The next order of business is to show results concerning the asymptotic
distribution of our estimators under the null-hypothesis, when the
sample size @xmath tends to infinity. Before we proceed with this, we
introduce some lemmas which will facilitate the following theorem
regarding the asymptotic distributions of the estimators.

###### Lemma 5.7.

For any @xmath satisfying the null-hypothesis @xmath , we have that the
kernel @xmath and its symmetrized version @xmath are square integrable
with respect to @xmath . That is, we have that @xmath .

###### P 29.

First note that under the null hypothesis we can factorize the following
expectation

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

These two factors are finite, and one can realize this by either using
equality one and two from lemma 7.48 on both @xmath and @xmath .
Alternatively, we note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath , such that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by Minkowski’s inequality, where we used lemma 2.7 for finiteness of
each of the four terms. This combined with analogous arguments for the
finiteness of @xmath , proves that @xmath .
As regards the symmetrized version @xmath , we get by Minkowski’s
inequality that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we used that all of the terms in the sum over all permutations
@xmath of @xmath are identically equal to the @xmath -norm of @xmath .
Hence both @xmath and @xmath are square integrable kernels under the
null-hypothesis.

The limit distribution of @xmath - and @xmath -statistics in the case of
non-degenerate kernels is given by a normal distribution. However, as we
shall see in the following lemma, the kernel for both the @xmath - and
@xmath -statistic estimators for distance covariance measure, is
degenerate of order 1, under the null-hypothesis. An implication of this
is that, rather than having a nice normal distribution as a limit, we
instead get rather complex limit distributions called Gaussian chaos
distributions.

###### Lemma 5.8.

For any @xmath satisfying the null-hypothesis @xmath , it holds that

  -- -------- --
     @xmath   
  -- -------- --

and as a consequence we have that the symmetric kernel @xmath is @xmath
-degenerate of order @xmath .

###### P 30.

Assume that @xmath such that the marginals @xmath and @xmath are
non-degenerate. As a consequence @xmath and the (so-called @xmath
-canonical) mappings @xmath for @xmath from section 7.2.4 become @xmath
and then recursively

  -- -------- --
     @xmath   
  -- -------- --

for @xmath . Here the subscript functions @xmath are conditional
expectations

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for @xmath .
We start by showing that the first @xmath -canonical mapping is
identically zero, i.e. @xmath for all @xmath . Note that @xmath and that
@xmath is the symmetrized version of @xmath . Hence all terms of @xmath
are given by @xmath with @xmath in argument number @xmath or @xmath
while @xmath will be placed in one of the @xmath possible permutation of
the remaining arguments. The placement of the random elements @xmath in
the remaining arguments does not matter since @xmath are independent and
identically distributed. That is, we realize that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is the set of all permutations of @xmath . Let @xmath be
given by @xmath for all @xmath and @xmath . We see that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and similarly @xmath for all @xmath . For @xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and similarly @xmath for all @xmath . For @xmath and @xmath we get
mirrored expressions of the cases @xmath and @xmath , all resulting in
zero. We conclude that @xmath for all @xmath and @xmath . Hence @xmath ,
which means that the kernel @xmath is at least @xmath -degenerate of
first order. As a consequence we have that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . With @xmath denoting the Dirac measure at @xmath , we
may realize that for any @xmath

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The latter sum vanishes since every term is zero.

To see this, note that under the null-hypothesis @xmath , the fact that
@xmath , allows Fubini’s theorem to factorize the integral of @xmath
into two integrals of @xmath and @xmath with respect to measures
depending on the specific choice of permutation @xmath . What we will
realize is that in any of the permutations in the latter sum, one of the
factor integrals will always be zero. The arguments are trivial so, if
the reader can take the fact that the latter sum vanishes at face value,
then the following wall-of-text can be skipped.

To that extent, we may note that the permutations in question ( @xmath
for which it does not hold that @xmath ) will at most allow one of the
Dirac measures to act on argument 1 or 2 of the integrand @xmath .

First we consider permutations where @xmath or @xmath , that is the
cases where one of the Dirac measures acts on argument 1 or 2. Assume
that @xmath such that @xmath or @xmath will act on the first argument of
@xmath . To further clarify, note that since @xmath such that @xmath ,
we know that the Dirac measure not acting on argument 1 must act on
arguments @xmath or @xmath . If this latter Dirac measure acts on
arguments 3 or 4 we have that the factor integral with integrand @xmath
becomes @xmath . On the other hand, if it instead acts on argument 5 or
6, then the factor integral with integrand @xmath becomes @xmath . By
similar arguments one can realize that one of the two factor integrals
is also always zero, if we instead assume that @xmath , e.g. in the case
that @xmath and the other Dirac measure acts on 3 or 4 then the factor
integral with integrand @xmath becomes @xmath .

It remains to be shown that one of the factor integrals is always zero
in the case that both Dirac measures act on argument number 3,4,5 or 6.
It suffices to consider two different scenarios: Either both Dirac
measures acts on the same argument pair @xmath or @xmath or both Dirac
measures acts on different arguments - one from each pair @xmath and
@xmath . If both act on the same argument pair @xmath or @xmath we get
that the factor integral with integrand @xmath or @xmath becomes zero
respectively, since they would equal @xmath or @xmath . Lastly, if both
Dirac measures act on different argument pairs, one from each pair
@xmath and @xmath we still get zero. Assume that @xmath acts on argument
3, while @xmath acts on argument 5 or 6. Then the factor integral of
@xmath becomes @xmath and if @xmath instead acted on argument 4 the
factor integral would become @xmath . Interchanging @xmath with @xmath
in the above considerations, one obtains that one of the factor
integrals are zero in the remaining cases (simply interchange @xmath
with @xmath in the above expressions).
Thus we have that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Now note that in each of the above sums there are @xmath identical terms
and that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

implying that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

proving the wanted equality.
Hence it only remains to be shown that the kernel @xmath is degenerate
or order 1. By definition 7.13 we need to show that @xmath almost surely
and @xmath with positive probability. We have already shown that @xmath
, meaning that @xmath is degenerate of at least order 1, so it only
remains to be shown that @xmath with positive probability, as to
guarantee that @xmath is exactly degenerate of order 1. Note that by the
equalities shown above

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and for contradiction assume that @xmath almost surely. By the
independence @xmath under the null-hypothesis we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

This shows that at least one of the two factors must be zero almost
surely, i.e. @xmath almost surely or @xmath almost surely. Assume
without loss of generality that @xmath almost surely. By the proof of
theorem 4.4 this implies that @xmath is degenerate, which is a
contradiction. We conclude that @xmath with positive probability,
proving that @xmath is degenerate of order 1.

Before proceeding with the theorem regarding the asymptotic distribution
of the estimators, we will continue with a remark containing thorough
explanations and analysis of the limiting distribution.

###### Remark 5.9.

Assume that the null-hypothesis is satisfied. Now define the linear
operator @xmath by

  -- -------- --
     @xmath   
  -- -------- --

For notational simplicity denote @xmath with norm @xmath induced by the
inner product @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

First we show that the obviously linear map @xmath is in fact an
operator between @xmath and @xmath . Under the null-hypothesis @xmath we
have that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

by Tonelli’s theorem and lemma 2.7 . The proof of lemma 2.7 can easily
be adjusted to show that also @xmath and @xmath for all @xmath and
@xmath . This especially implies that @xmath for all @xmath since @xmath
(formally when looking at @xmath as an element in @xmath we consider its
equivalent class, but this will not create any confusion). Hence the
square integrability of @xmath for any @xmath follows by noting that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by the Cauchy-Schwarz inequality, proving that @xmath is indeed a linear
operator on @xmath . In fact we have that @xmath is a bounded linear
operator, since @xmath , by the above inequality. Since @xmath is a
Hilbert space we know that it has an orthonormal basis @xmath for some
index-set @xmath (cf. theorem 6.29 [ HN01 ] ). Furthermore it holds that
@xmath is separable, since @xmath is a separable metric space and @xmath
is a Borel measure on it (cf. theorem 4.13 [ Bre10 ] ). Since a
separable Hilbert space admits a countable orthonormal basis, we
conclude that @xmath is either finite or countably infinite (cf.
proposition 2.3.8 [ Sun98 ] ). Note that the bounded linear operator
@xmath is a Hilbert-Schmidt operator, if the Hilbert-Schmidt norm @xmath
(definition 1 section 10.6 [ DS63 ] ). Since @xmath is at most countably
infinite we can use Tonelli’s theorem to interchange the summation and
integration in the following way

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where in the fourth equality we used Parseval’s identity. We conclude
that the integral operator @xmath is a Hilbert-Schmidt operator and
hence also compact (cf. theorem 6 section 10.6 [ DS63 ] ). Moreover the
integral operator is self-adjoint

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus @xmath is a self-adjoint compact linear operator on a separable
Hilbert space, and by the Hilbert-Schmidt theorem (see theorem 6.2.3. [
EMT04 ] or theorem 8.94 [ RR06 ] ) the set of non-zero eigenvalues
counted according to multiplicity @xmath of @xmath is either finite or
countably infinite. Furthermore the eigenvalues may be indexed in
absolute descending order @xmath and they possess the property that
@xmath . The set of corresponding eigenfunctions @xmath , i.e. @xmath ,
may be assumed orthonormal. Lastly, the theorem also states that the
orthonormal set @xmath is actually a orthonormal basis for @xmath .
Assume without loss of generality that there are infinitely many
non-zero eigenvalues and note that since @xmath is a self-adjoint
Hilbert-Schmidt integral operator on @xmath with kernel @xmath , it
satisfies the conditions of exercise 56 [ DS63 ] , which then states
that @xmath where the convergence of the series happens in @xmath . That
is,

  -- -------- --
     @xmath   
  -- -------- --

The eigenfunctions @xmath obviously satisfy the following properties

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

@xmath -almost surely, hence @xmath almost surely. Thus when expanding
@xmath we get

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where we used that @xmath for @xmath . Since @xmath we get that

  -- -------- --
     @xmath   
  -- -------- --

That is, the sequence of non-zero eigenvalues @xmath of @xmath repeated
according to multiplicity is square summable.
Now, for an independent and identically distributed sequence @xmath of
standard normal distributed random variables, define @xmath for all
@xmath , and let

  -- -------- --
     @xmath   
  -- -------- --

denote the pointwise limit as @xmath tends to infinity. This pointwise
limit is welldefined and almost surely finite by Khinchin-Kolmogorov’s
convergence theorem. That is, @xmath converges almost surely and in
@xmath , since @xmath and @xmath has mean zero and finite variance. This
also entails that the pointwise limit

  -- -------- --
     @xmath   
  -- -------- --

is almost surely finite. Thus the following limit distribution of @xmath
and @xmath are well-defined distributions on @xmath . By a standard
convolution argument, we also see that the distribution is absolutely
continuous with respect to the Lebesgue measure, hence the corresponding
cumulative distribution function is continuous.

###### Theorem 5.10 (Limiting distribution of estimators under the null
hypothesis).

If @xmath satisfies the null-hypothesis @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

as @xmath . Where @xmath is a sequence of independent and identically
standard normal distributed random variables, and @xmath are the
eigenvalues counted with multiplicity of the linear operator @xmath
given by

  -- -------- --
     @xmath   
  -- -------- --

###### P 31.

The convergence of the U-statistics follows quite effortlessly from the
well-documented limit theorem of U-statistics with 1st order degenerate
kernel - see theorem 7.18 . The convergence in distribution of the
V-statistics is a little more complicated, since this is not a theorem
explicitly found in the literature we have referenced. Such limit
theorems can be found in e.g. [ Bor96 ] , where the limit distribution
is stated in terms of multiple stochastic integrals. In order to avoid
the theory of multiple stochastic integrals, we can with a little more
work derive the limit distribution of @xmath , using various
decomposition theorems and asymptotic properties of U-statistics.
First we show the wanted convergence in distribution of the scaled
U-statistic @xmath . Under the null-hypothesis this is a centered
U-statistic and since @xmath (by lemma 5.1 ) is a symmetric kernel with
@xmath -degeneracy of first order, we get that

  -- -------- --
     @xmath   
  -- -------- --

as @xmath tends to infinity, by theorem 7.18 , where @xmath are the
eigenvalues of

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by lemma 5.8 . Let @xmath be all the non-zero eigenvalues of @xmath
counted according to its multiplicity and let @xmath be the
corresponding eigenfunctions descriped in the above remark 5.9 . We note
that @xmath , so if we enumerate @xmath for all @xmath , every non-zero
eigenvalue for @xmath repeated according to multiplicity will be given
by @xmath . This is easily seen by observing that if a non-zero
eigenvalue of @xmath is missing from the list @xmath , then there will
also be missing a non-zero eigenvalue of @xmath in the list @xmath - a
contradiction. Remark 5.9 also showed that @xmath is almost surely
convergent, hence

  -- -------- --
     @xmath   
  -- -------- --

almost surely, proving the wanted convergence in distribution of the
scaled U-statistic @xmath .
Now we will show the claimed convergence in distribution of the scaled
V-statistics @xmath . We note that @xmath under the null-hypothesis, is
a centered V-statistic with symmetric kernel @xmath of degree @xmath .
Hence by lemma 7.16 we decompose it into a linear combination of six
V-statistics. The last four of these we furthermore decompose into a
linear combination of U-statistics using lemma 7.17 . That is,

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (21)
  -- -------- -------- -- ------

where @xmath is defined in section 7.2.4 (or the previous theorem) and
@xmath (which is also found in section 7.2.4 ), is a symmetric kernel of
degree @xmath defined by

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath , e.g. @xmath and @xmath We used the
convention that the superscript is read first, i.e. @xmath .
We need to find the limiting distribution of @xmath and we do this by
multiplying @xmath on both sides of eq. 21 and showing that the right
hand side converges in distribution to the claimed limiting
distribution. In the proof of lemma 5.8 we showed that @xmath , so more
specifically we only need to prove that

-   @xmath , as @xmath .

-   @xmath , as @xmath , for all @xmath and @xmath .

which will yield the wanted convergence of @xmath ; by Slutsky’s
theorem.
(1) : By the identity of @xmath in lemma 5.8 we have that

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (22)
  -- -------- -------- -- ------

by the symmetry @xmath . Now note that under the null-hypothesis @xmath
by the triangle inequality, and

  -- -------- --
     @xmath   
  -- -------- --

Hence the last term in eq. 22 converges almost surely

  -- -------- --
     @xmath   
  -- -------- --

by the regular strong law of large numbers. By Slutsky’s theorem it now
suffices to show the following convergence in distribution

  -- -------- --
     @xmath   
  -- -------- --

and to that end Slutsky’s theorem also yields that it suffices to show
the convergence in distribution of the expression in question multiplied
by a factor that tends to one in probability,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Now note that @xmath is a completely @xmath -degenerate (cf. corollary
7.15 ) symmetric kernel of degree 2 with @xmath , hence

  -- -------- --
     @xmath   
  -- -------- --

by theorem 7.18 , where @xmath are the eigenvalues of @xmath counted
according to multiplicity. We conclude that the convergence in
distribution of (1) holds.
(2) : First we note that the factor multiplied with the U-statistics has
different asymptotic properties depending on the @xmath ’s and @xmath
’s. We have that

  -- -------- --
     @xmath   
  -- -------- --

Hence for any @xmath it suffices to show that

-   [leftmargin=+.6in]

-   If @xmath then @xmath .

-   If @xmath then @xmath .

-   If @xmath then @xmath .

(2.1): We note that for any @xmath

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

and realize that the factor @xmath tends to zero much slower than @xmath
which is the normalization factor on regular U-statistic type sums.
Hence the regular SLLN for U-statistics is insufficient for our purpose.
Luckily it turns out that @xmath is a degenerate kernel which comes to
our aid as centered U-statistics with degenerate kernel are, under
certain conditions, guaranteed to converge to zero much faster than
regular centered U-statistics. This SLLN for centered U-statistics with
degenerate kernels can be found in [ GZ92 ] and is also stated in the
appendix under theorem 7.20 .
Fix @xmath and note that by corollary 7.15 the kernel @xmath of degree
@xmath is completely degenerate, i.e. degenerate of order @xmath or has
rank @xmath . The reader is encouraged to read the conditions and
statement of theorem 7.20 - the SLLN for centered U-statistics with
degenerate kernels. Firstly we note that the order of normalization
@xmath in eq. 23 lies within allowed interval @xmath . Hence by theorem
7.20 we have that

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

if @xmath . Since @xmath it suffices to show that @xmath to ensure the
convergence in eq. 24 . By examining the recursive nature of @xmath we
see that it entirely consists of linear combinations of @xmath for
@xmath with argument spanning over all subsets of @xmath of cardinality
@xmath . Hence by Minkowski’s inequality it sufficies to show that all
of the aforementioned terms of the linear combination are square
integrable. We note that any such @xmath -cardinality subset has
distribution @xmath , so we only need to show that @xmath for all @xmath
. To this extend we simply note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by Jensen’s conditional inequality ( @xmath is convex). By lemma 5.1 we
have that the right-hand side is finite, so convergence in probability
stated in eq. 24 holds.
To conclude the wanted convergence it suffices to show that @xmath . By
similar considerations as we initially did with the above square
integrability of @xmath regarding the recursive nature of @xmath , we
note that it suffices to show that @xmath for all @xmath . Thus note
that for any @xmath , that @xmath is a conditional expectation of @xmath
given @xmath , such that

  -- -------- --
     @xmath   
  -- -------- --

proving that the claimed convergence in statement (2.1) holds.
(2.2): We will show this by using theorem 7.19 - the regular strong law
of large numbers for U-statistics, to establish that @xmath and
hereafter showing that @xmath for all @xmath , proving the wanted
convergence.
Fix @xmath and note that in order to apply the SLLN for U-statistics it
suffices to show that @xmath , since the kernel @xmath is symmetric.
Recall

  -- -------- --
     @xmath   
  -- -------- --

and note that any solution to @xmath with @xmath will consist of @xmath
and @xmath for some @xmath . Now for any sequence @xmath we define the
projection onto the @xmath -first without the @xmath ’th coordinate as

  -- -------- --
     @xmath   
  -- -------- --

e.g. @xmath . Since @xmath is a symmetric mapping we can always move the
two identical arguments up to the first two argument positions, that is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Using the fact that @xmath are independent and identically distributed
we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by the triangle inequality and linearity of the expectation. As argued
in (2.1) we have that @xmath is a linear combination of @xmath for
@xmath with arguments spanning over all sublists of @xmath of
cardinality @xmath . Whenever those sublists of cardinality @xmath have
only one occurrence of @xmath the square integrability of @xmath shown
in (2.1) implies integrability in particular. Hence the only terms of
the aforementioned linear combination needing attention are those, where
the sublist of cardinality @xmath have both occurrences of @xmath .
Again by the i.i.d. property of @xmath the particular composition of
these ordered sublists is not important, implying that we only need to
show that @xmath . The first of these expectation is finite since @xmath
. For @xmath we have that

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where we used the triangle inequality for integrals and Tonelli’s
theorem. Now this upper bound is easily seen finite, by using the
triangle inequality on all @xmath terms of @xmath . That is, we get
finiteness if @xmath , for all @xmath where all but two indices are
distinct. We can actually show even stronger integrability, which
becomes useful in the proof of statement (2.3). To this extend take any
indices @xmath and note that under the null-hypothesis the expectation
factorizes

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath and @xmath , where we used the triangle inequality to say
that @xmath with a similar inequality for @xmath . Thus we have argued
that the SLLN for U-statistics applies and it remains to be shown that
the limit, given by the expectation of the kernel, is zero. That is,

  -- -------- --
     @xmath   
  -- -------- --

By the above discussion about @xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

so it suffices to show that the last expectation is zero. Hence we note
that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

where in the last equality we used theorem 7.9 since @xmath . This
concludes the proof of statement (2.2).
(2.3): Fix @xmath and let @xmath . We realize that the wanted
convergence @xmath , holds if we can show that the conditions of the
SLLN for U-statistics are satisfied. Since @xmath is a symmetric kernel
of degree @xmath , we only need to show that the kernel is integrable,
i.e. @xmath . Hence note that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

where @xmath for all @xmath . By similar considerations as we have done
previously, we may note that any of the above terms @xmath can be
written as a linear combination of @xmath with arguments spanning over
all ordered sublists of @xmath with cardinality @xmath (i.e. @xmath
-size sublists) for all @xmath .
Consider any of the terms in the above sum: @xmath for some @xmath with
@xmath , and realize that the term is finite by the triangle inequality,
if all individual terms in its linear combination have finite
expectation. Thus for any @xmath we fix an arbitrary ordered sublist of
@xmath of cardinality @xmath . We note that this ordered sublist can be
written as @xmath for some @xmath . It furthermore holds that @xmath for
some @xmath . Thus we may establish that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The kernel @xmath is the symmetrized version of @xmath , that is it is a
linear combination of @xmath with arguments spanning over every possible
permutation of the list @xmath . We realize that it suffices to show
that @xmath for any @xmath , which was done in the proof of statement
(2.2) above. We conclude that the wanted convergence in statement (2.3)
holds. Hence we have argued that statements (2.1), (2.2) and (2.3) hold,
implying the wanted convergence in distribution of our @xmath
-statistics.

###### Remark 5.11.

As regards the limit distribution of @xmath , we note that it indeed
differs from the claimed limit distribution from Theorem 2.7 [ Lyo13 ] .
In the proof of that theorem, it is stated (without proof) that @xmath ,
and we have been unable to prove this equality. In case that the
equality holds we obviously have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

such that

  -- -------- --
     @xmath   
  -- -------- --

almost surely, showing why @xmath in [ Lyo13 ] .
Let us try to examine a possible way to arrive at the above equality. If
@xmath is a trace class operator, i.e. @xmath , then the trace of @xmath
is given by @xmath . Under certain conditions a trace class operator has
trace given by integral of the kernel over the diagonal; see for example
exercise 49 in [ DS63 ] or [ Cas16 ] . In the affirmative of the
previous conditions we have that @xmath . However, we have not even been
successful in affirming that @xmath is of trace class.

Furthermore exercise 49 in gives conditions for which @xmath is of trace
class and has trace given be the integral of kernel over the diagonal.
This exercise specifically requires that @xmath is a composition of two
Hilbert-Schmidt integral operators, i.e. our kernel needs to satisfy

  -- -------- --
     @xmath   
  -- -------- --

for two Hilbert-Schmidt integral operator kernels @xmath and @xmath . We
have not been able to prove such a factorization, so we are not able to
justify the conditions of this exercise. In the proof of theorem 2.7 [
Lyo13 ] there is a reference to [ Ser09 ] and within this book there is
a remark on p. 227 stating a similar trace formula. This remark refers
to exercise 49 in , hence this might be what motivated the equality [
Lyo13 ] (only speculation).

In personal communication with Russell Lyons he acknowledges that it is
not evident that @xmath is of trace class, so it remains an open
problem.

With this remark, we end this subsection about the asymptotic properties
of our estimators.

### 5.3 Asymptotically consistent tests of independence

In this section we will discuss how to construct an asymptotically
consistent statistical test of independence, using the theory derived in
the previous sections. The tests we construct have rejection thresholds
given by quantiles of unknown distributions, so we will finally show how
these thresholds can be consistently bootstrapped.

#### 5.3.1 Statistical models and specification of tests

A statistical test of significance level @xmath is said to be
asymptotically consistent at level @xmath if (1) the probability of
rejecting a true hypothesis (Type I error) tends to @xmath and (2) the
probability of failing to reject a wrong hypothesis (Type II error)
tends to zero as the sample size tends to infinity.
First we present the general setup of the statistical models in which we
can test the null-hypothesis against its general alternative, using the
theory of distance covariance in metric spaces examined in the previous
sections.

###### Definition 5.12.

Let @xmath and @xmath be separable metric spaces of strong negative type
and consider the following three statistical models

-    The first statistical model is given by the sample space @xmath and
    the non-parametric family of probability measures @xmath .

-    The second statistical model is given by the sample space @xmath
    and the non-parametric family of probability measures @xmath given
    by the subset of @xmath such that every @xmath satisfies @xmath for
    some @xmath and @xmath .

-    The third statistical model is given by the sample space @xmath and
    the non-parametric family of probability measures @xmath .

Having established the statistical models we now focus on devising an
asymptotically consistent statistical test, which can test the
null-hypothesis

  -- -------- --
     @xmath   
  -- -------- --

For the first statistical model we will construct a statistical test
with test statistic given by the @xmath -statistic estimator of @xmath
and for the second statistical model we will construct a statistical
test with test statistic given by the @xmath -statistic estimator of
@xmath . However, note that the three models are nested, @xmath . Hence,
every test that is asymptotically consistent in the first model is also
asymptotically consistent in the second model, and every test that is
asymptotically consistent in the second model is also asymptotically
consistent in the third model.

For all statistical models we assume that @xmath are independent pairs
of random elements with values in @xmath , all defined on a common
probability space @xmath , such that each pair has simultaneous
probability distribution @xmath .

As previously, we denote the first @xmath sample pairs by @xmath but now
we also let @xmath and @xmath denote the probability distributions on
@xmath of the limiting variables of the scaled estimators @xmath and
@xmath respectively. That is,

  -- -------- --
     @xmath   
  -- -------- --

and let @xmath and @xmath denote the respective cumulative distribution
functions. Here @xmath is a sequence of independent and identically
standard normal distributed random variables, and @xmath are the
eigenvalues counted with multiplicity of the linear operator @xmath
given by

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 5.13.

Consider the following two statements

-    For any fixed significance level @xmath , we have that the
    statistical test that rejects the null-hypothesis if

      -- -------- --
         @xmath   
      -- -------- --

    is an asymptotically consistent test of independence at level @xmath
    .

-    For any fixed significance level @xmath , we have that the
    statistical test that rejects the null-hypothesis if

      -- -------- --
         @xmath   
      -- -------- --

    is an asymptotically consistent test of independence at level @xmath
    .

Statement 1) is true in all three of the considered statistical models,
but statement 2) is only guaranteed to be true in the second and third
statistical model.

###### P 32.

Let us consider test 1) in the first statistical model. Let the test
statistic based on the first @xmath sample pairs @xmath be given by the
scaled @xmath -statistic estimator of the distance covariance measure.
That is, the @xmath ’th test statistic is given by

  -- -------- --
     @xmath   
  -- -------- --

for every @xmath . Under the null-hypothesis @xmath , theorem 5.10
yields that

  -- -------- --
     @xmath   
  -- -------- --

where the limiting distribution @xmath is a well-defined probability
distribution on @xmath with continuous distribution function (see remark
5.9 ). On the other hand, if @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

by theorem 5.5 , theorem 4.1 and theorem 3.24 , implying that @xmath .
Thus we realize that large values of our test statistic @xmath are in
disagreement with the null-hypothesis. It is therefore reasonable to
devise a test that rejects the null-hypothesis if the test statistic is
observed to be larger than a certain threshold. If we let this threshold
be the @xmath -quantile @xmath of the limit distribution @xmath , then
we see that

  -- -------- --
     @xmath   
  -- -------- --

under the assumption that the null-hypothesis @xmath is true. This is
seen by noting that the above convergence in distribution implies
convergence of the cumulative distribution functions in every point
(since the limit distribution has a continuous cdf.). Thus the
probability of rejecting the null-hypothesis, even though it is true, is
asymptotically @xmath . Furthermore we see that

  -- -------- --
     @xmath   
  -- -------- --

under the assumption that the null-hypothesis is false. This follows
from the fact that the above almost sure convergence implies convergence
in probability towards infinity. Thus the probability of accepting the
null-hypothesis, even though it is false, is asymptotically zero. We
conclude that for any level @xmath , the test that rejects the
null-hypothesis if

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the @xmath -quantile of @xmath , is an asymptotically
consistent test at level @xmath in the first statistical model.

The asymptotically consistency of the test proposed in 2) , follows by
identical arguments. However, the @xmath -statistic estimator is only
guaranteed to be strongly consistent in the second and third models,
because of the additional moment condition from theorem 5.5 . Thus the
test in 2) is only guaranteed to be asymptotically consistent in the
second and third statistical models.

At a first glance one might think we devised statistical tests that are
directly usable in practice, but unfortunately one may realize that the
proposed thresholds for rejection depends on the specific underlying
distribution @xmath . That is, the eigenvalues @xmath of the integral
operator @xmath are dependent on the specific choice of @xmath . Thus
without knowing @xmath we cannot find the eigenvalues @xmath
analytically and as a consequence we have no idea how the @xmath and
@xmath distributions behave and we especially do not know where the
@xmath -quantiles are located.

#### 5.3.2 Bootstrapping of test thresholds

Fortunately for us Miguel A. Arcones and Evarist Giné proved in 1992 [
AG92 ] that the limiting distribution of both degenerate @xmath - and
@xmath -statistics can be consistently bootstrapped. However one needs
to be careful when doing this, since the naive bootstrap approach of
simply sampling with replacement from the empirical distribution and
inserting into @xmath and @xmath fails to be consistent in general. In
our case the @xmath - and @xmath -statistic estimators are both @xmath
-degenerate of order 1 (see lemma 5.8 ) and [ AG92 ] proves consistency
of a bootstrapping approach which utilizes that the asymptotic
distribution of such degenerate statistics is solely determined by the
leading terms in the Hoeffding decomposition. In the proof of theorem
5.10 we only saw this this for the @xmath -statistic estimator since we
referred to the literature for the @xmath -statistics estimator.
Nevertheless, we saw that the specific asymptotic distribution @xmath of
@xmath was derived solely from the decomposition term @xmath , as every
other decomposition term converged to zero. This is the reason for the
bootstrapping approach proposed in [ AG92 ] , instead samples with
replacement from the empirical distribution and inserts these samples
into @xmath - and @xmath -statistics with empirically modified kernels
based on @xmath .

We go into detail on how to bootstrap the limit distribution @xmath , of
our scaled @xmath -statistics @xmath under the null-hypothesis, but
refer the reader to [ AG92 ] for a similar approach for limit the
distribution @xmath of our scaled @xmath -statistics @xmath .
To this end, let @xmath be an i.i.d. sequence defined on a common
probability space @xmath such that each pair is distributed according to
a @xmath that satisfies the null-hypothesis @xmath . Furthermore let
@xmath denote the @xmath ’th empirical measure given a realization
@xmath . We assume that @xmath denotes i.i.d. random elements in @xmath
with distribution function @xmath for any @xmath and @xmath . Recall
@xmath defined in definition 7.8 , and note that it can be written as

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The previously mentioned empirically modified version of @xmath is given
by the above expression, but where we interchange the true distribution
@xmath with the realized empirical distribution @xmath . That is, the
empirically modified version of @xmath is given by

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The bootstrap consistency theorem of [ AG92 ] (theorem 2.4) states that,
if the symmetric kernel @xmath satisfies the integrability condition

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

As we have argued before, @xmath is the symmetrized version of @xmath so
Minkowski’s inequality (or see below if exponent is less than one)
yields that the above integrability holds if @xmath for any @xmath . In
the case that all indices are distinct, we note that the requirement is
square integrability of @xmath with respect to @xmath , which is
guaranteed by lemma 5.1 . Hence denote @xmath and fix any indices @xmath
such that @xmath . We see that

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for some @xmath and @xmath , where we used Minkowski’s inequality, the
first two inequalities of lemma 7.48 and that @xmath (if @xmath create
similar upper bounds by the inequality @xmath ). From this we see, it is
sufficient that @xmath in order to guarantee that the bootstrap
consistency theorem holds.

These arguments entail that we are only guaranteed to have convergence
in distribution of the bootstrap statistics towards the @xmath
distribution in the third statistical model where @xmath . Note that we
do not state, that the integrability condition is not satisfied in the
first and second statistical models, but that the above upper bounds are
only sufficiently tight in third statistical model.
Now let us describe the heuristics behind bootstrap approach to
approximate the @xmath -quantile of the @xmath distribution. Let @xmath
denote the cumulative distribution function of the random variable
@xmath for any @xmath and @xmath . Since @xmath has a continuous
cumulative distribution function @xmath , the bootstrap consistency
theorem yields that

  -- -------- --
     @xmath   
  -- -------- --

for @xmath -almost all @xmath . This is of course equivalent to the
convergence of the quantile functions (lemma 21.2 [ VdV00 ] ), i.e.

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

for @xmath -almost all @xmath . Now the bootstrap approach for
approximating @xmath makes the approximation @xmath for any realization
@xmath and @xmath , which is deemed reasonable if @xmath is large by the
above quantile convergence.

Hence fix @xmath and @xmath (denoting the given sample-size) and note
that we have reduced the problem of finding the rejection threshold to
finding the quantile @xmath . Let @xmath be independent copies of @xmath
i.i.d. random variables @xmath each distributed according to the @xmath
’th empirical measure @xmath given the realization @xmath . By the
Glivenko-Cantelli theorem we have that

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

almost surely. Hence for large @xmath we may reasonably approximate the
unknown distribution function @xmath by @xmath . Since we know the
empirical measure @xmath we may generate realizations of

  -- -------- --
     @xmath   
  -- -------- --

for some arbitrarily large @xmath . Based on these samples we may
calculate

  -- -------- --
     @xmath   
  -- -------- --

and find corresponding the @xmath -quantile @xmath of the resulting
empirical distribution. We say that this quantile @xmath approximates
the true @xmath -quantile of the @xmath distribution, through the above
reasoning of the approximations @xmath , for some large @xmath .
We can summarize this approach in the following bootstrap and test
algorithm, where we are given empirical samples @xmath assumed to be a
realization of @xmath .

-   Choose a large @xmath and sample with replacement @xmath times from
    the observed empirical distribution @xmath placing @xmath point-mass
    at @xmath for all @xmath , yielding an @xmath array of samples

      -- -------- --
         @xmath   
      -- -------- --

-   For each @xmath calculate

      -- -------- --
         @xmath   
      -- -------- --

    and denote the resulting empirical cdf by @xmath

-   Calculate the corresponding empirical @xmath -quantile @xmath and
    reject the null-hypothesis if

      -- -------- --
         @xmath   
      -- -------- --

This concludes the last section of the thesis. We have provided a
solution to the non-parametric independence problem, by constructing
asymptotically consistent statistical tests for testing the
null-hypothesis, and it was argued how one reasonably can bootstrap
approximate the rejection thresholds of the aforementioned tests.

## 6 Summary and future work

Summary: In this thesis we proposed a solution to the non-parametric
independence problem, whenever the marginal metric spaces were separable
and of strong negative type. We did this by introducing the distance
covariance measure in metric spaces @xmath . In order to ensure that
@xmath is well-defined, we made the additional restriction only to
consider marginal metric spaces that are separable.

The distance covariance measure is however, not a direct indicator of
independence, for general marginal metric spaces. Thus we embarked on
searching for further conditions on the marginal metric spaces, which
would guarantee this property. To this end, we showed that whenever the
marginal metric spaces are of negative type, we can represent the
distance covariance measure in terms of mean embeddings of certain
isometries into separable Hilbert spaces. This representation resulted
in the definition of the subset of negative type metric spaces, called
metric spaces of strong negative type. With some effort we were able to
show, that the distance covariance measure is a direct indicator of
independence, whenever the marginal spaces are metric spaces of strong
negative type. Additionally, it was shown that every separable Hilbert
space is a metric space of strong negative type.

Then we constructed two estimators for the distance covariance measure,
a @xmath -statistic estimator as in [ Lyo13 ] , but also a new one given
by the corresponding @xmath -statistic. We proved, that both estimators
are strongly consistent and possesses well-defined asymptotic
distributions. We also argued that the moment conditions in [ Lyo13 ]
for strong consistency of the @xmath -statistic estimators are not
sufficient. However, our @xmath -statistic estimator only needed the
weaker moment assumptions in order to guarantee strong consistency. As
regards to the asymptotic distribution of the @xmath -statistics, it is
still unresolved whether the asymptotic distribution indeed can be
written as in [ Lyo13 ] .

Nevertheless the aforementioned asymptotic properties of the estimators
was combined with the developed theory of the distance covariance
measure, to construct statistical tests of independence. These tests
were guaranteed to be asymptotically consistent under certain
conditions, one of which was that the marginal spaces must be of strong
negative type. Lastly as the tests were constructed with rejection
thresholds given by non-traceable quantiles, we argued that they could
be reasonably bootstrapped.
Future work: There are a few things, which would be very interesting to
explore and examine. First of all, it would be interesting to do a
simulation experiment, to see how the two different statistical tests
compare to each other. For example, it would be interesting to examine
the statistical power ” @xmath ” of the two tests for varying sample
sizes, to see how many sample points each test would need to yield a
reasonably low frequency of Type II errors. It could also be of interest
to actually apply the statistical tests, to real sample-data with values
in a non-Euclidean space. E.g. functional data where each realization is
seen as a sample-path with valued in a @xmath -space.

In [ SSG @xmath 13 ] , published in The Annals of Statistics, it is
stated that the theory of distance covariance in metric spaces extends
to semi-metric spaces. They furthermore state an equivalence between
independence testing using distance covariance in semi-metric spaces and
something called the Hilbert-Schmidt independence criterion. Since we
did not have the time to pursue these claims, it could serve as a very
interesting continuation of the thesis.

## 7 Appendix

### 7.1 Product spaces - metrics, topologies and @xmath-algebras

Let @xmath for some @xmath , or @xmath and consider any family of
measurable spaces @xmath . The (Cartesian) product space @xmath has the
following representations.

-    If @xmath then @xmath that is the set of all ordered @xmath
    -tuples, with @xmath for all @xmath .

-    If @xmath then @xmath that is the set of all infinite sequences
    @xmath , with @xmath for all @xmath .

For any @xmath we define the coordinate projection @xmath by @xmath for
any @xmath . Furthermore for any @xmath and @xmath with @xmath , we
define the simultaneous coordinate projection @xmath by

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

and for simplicity we may also denote this simultaneous coordinate
projection by @xmath whenever it is clear that @xmath with @xmath .

###### Definition 7.1 (Product @xmath-algebra).

We define the product @xmath -algebra @xmath on @xmath as the smallest
@xmath -algebra making every coordinate projection measurable . That is

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

It is fairly easy to show that the above @xmath -algebra is also
generated by

  -- -------- --
     @xmath   
  -- -------- --

when @xmath with @xmath for all @xmath (see proposition 1.3 and 1.4 [
Fol99 ] ). It is also evident that the simultaneous coordinate
projections @xmath defined in eq. 25 are @xmath -measurable. Moreover
when @xmath we have an intersection stable generator for @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

(see [ SRN15 ] lemma 2.3.2 for proof).
Let @xmath be a family of metric spaces. Whenever a norm or metric is
introduced for a space, we always work with the corresponding metric
topology @xmath and the Borel @xmath -algebra @xmath induced by these,
unless otherwise stated.
When considering the product space @xmath we always equip it with the
product topology @xmath , unless we introduce a metric in which case the
above comment applies. The product topology is defined as the topology
generated by (smallest topology containing)

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

that is the above family of sets is a subbase for the product topology
on @xmath . In other words the product topology is the smallest/coarsest
topology making all coordinate projections continuous. We may also note
that the Borel @xmath -algebra of the product space @xmath (which we
also write as @xmath always contains the product Borel @xmath -algebra,
that is

  -- -------- --
     @xmath   
  -- -------- --

To see this simply note that @xmath . By the remark below definition 7.1
yields that the former family of sets is a generator for the product
Borel @xmath -algebra. Hence

  -- -------- --
     @xmath   
  -- -------- --

A natural question is whether the Borel @xmath -algebra @xmath induced
by product topology coincides with the product @xmath -algebra @xmath .
Nice properties follow if we indeed have equality, for instance every
continuous function becomes measurable with respect to the product
@xmath -algebra. Unfortunately, equality does not always hold, as shown
in example 6.4.3 [ Bog07a ] . Though for some sufficiently nice spaces
the equality does indeed hold. Below we prove that they coincide in the
case of the case where all marginal spaces are separable.
We note that if @xmath is finite then @xmath is metrizable, in the sense
that the maximum/product metric @xmath given by @xmath is a metric on
@xmath which induces product topology (see remark below next theorem).
Hence for finite @xmath it is evident that we have convergence in @xmath
if and only if we have convergence in @xmath of each coordinate, and
realize that this implies that every coordinate projection is
continuous.

###### Theorem 7.2.

If every metrizable topological space in the family @xmath is separable,
then @xmath is separable and the Borel @xmath -algebra @xmath induced by
the product topology coincides with the product @xmath -algebra @xmath .

###### P 33.

We prove it for @xmath for some @xmath , but the proof when @xmath is a
countably infinite index set follows by analogous steps (see for example
Lemma 1.2 [ Kal97 ] ).
First we show separability of @xmath : Let @xmath be a countable dense
subset of @xmath , for each @xmath . Note that @xmath is the Cartesian
product of @xmath countable sets, hence itself countable. Now fix an
arbitrary @xmath and note that since @xmath is dense, there exists a
sequence @xmath in @xmath converging to @xmath , for all @xmath .

Now construct the sequence @xmath in @xmath by setting @xmath for every
@xmath . Lastly, note that since convergence in @xmath is equivalent to
convergence in each coordiante, we by construction of @xmath have that
@xmath since @xmath , for all @xmath . Thus every point in @xmath is a
limit point of the dense subset @xmath , proving separability.

As regards the claim about the @xmath -algebras it suffices to show that
@xmath and that @xmath for any two generators @xmath and @xmath such
that @xmath and @xmath respectively. Since the open sets of @xmath
generate @xmath , we get by the remark below definition 7.1 , that the
generator @xmath can be choosen as

  -- -------- --
     @xmath   
  -- -------- --

but by eq. 27 this is exactly the family of sets generating the product
topology. Hence @xmath must be a subset of the corresponding Borel
@xmath -algebra @xmath .

Conversely, let @xmath be the entire family of open sets in @xmath and
note that since @xmath is separable, we know that it has a countable
base (cf. Theorem M3 [ Bil99 ] ) given by

  -- -------- --
     @xmath   
  -- -------- --

Now realize that each set in the above base is a finite intersection of
sets in @xmath since @xmath is open in @xmath for any @xmath and @xmath
, and therefore @xmath . Lastly, we note that the countability of the
base implies that any open set in @xmath , i.e. any element of @xmath is
a countable union of elements in @xmath . Since @xmath contains
countable unions of its members we conclude that, @xmath .

Moreover we have that a metrizable topological space is completely
determined by its convergent sequences (see [ Fra65 ] ). Furthermore if
@xmath are metrizable topological spaces then @xmath is a metrizable
topological space if @xmath (see corollary 7.3 [ Dug66 ] ) and in
general we have that the product spaces have coordinatewise convergence,
i.e. @xmath in @xmath if and only if @xmath in @xmath for all @xmath
(see lemma 43.3 ). A consequence of these facts is: If @xmath is a
family of metric spaces and @xmath is a metric on the product space
@xmath for which it holds that @xmath if and only @xmath for all @xmath
then @xmath induces the product topology on @xmath .

### 7.2 U- and V-statistics

In this section we introduce @xmath - and @xmath -statistics. We prove
the Hoeffding decomposition theorem for @xmath -statistics, and we will
define various mappings used in the thesis. Lastly, we draw on the
litterature to state some asymptotic properties of @xmath - and @xmath
-statistics, which we use in the thesis.

#### 7.2.1 Hoeffding decomposition

Let @xmath be a family of non-empty subspaces of a Hilbert space @xmath
each mutually orthogonal to each other, that is @xmath for @xmath . If
all orthogonal projections @xmath for @xmath exist for some @xmath then

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the sum of subspaces defined by

  -- -------- --
     @xmath   
  -- -------- --

This is easily established by an induction argument. For two spaces, let
@xmath and @xmath with @xmath and @xmath for all @xmath . Note that
@xmath and for every @xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

since @xmath and @xmath because @xmath , proving that the projection
onto the sum space @xmath is given by the sum of the projections by
standard equivalence of orthogonal projections. Now assume that @xmath
for some @xmath and note that @xmath such that by the above arguments
@xmath . Hence by the induction assumption we have that @xmath , proving
the general claim by induction. We will use this property so the next
step is to define some mutually orthogonal spaces.
Consider independent random elements @xmath in the measurable space
@xmath defined on some probability space @xmath , and let @xmath .
Furthermore let @xmath denote the number of elements in the set @xmath
and let @xmath denote the subset of @xmath where each element can be
written in the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath with

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

for all @xmath with @xmath (With the convention that conditioning on an
empty index set, is the conditioning on the trivial @xmath -algebra
@xmath , resulting in every random variable in @xmath for @xmath having
zero expectation). Furthermore it is interpreted that @xmath is the set
of (almost surely) constant functions, and it is easily verified that
@xmath is indeed a subspace of @xmath for any @xmath .
Now we show the family @xmath of subspaces are mutually orthogonal. Thus
consider any two distinct sets @xmath , and take any @xmath and @xmath .
If @xmath then by the mutual independence of the @xmath ’s we have that

  -- -------- --
     @xmath   
  -- -------- --

where we used that since @xmath so one of them has at least one element
which by the above remark yields a zero expectation. If @xmath then
since @xmath we have that @xmath or @xmath or both. Without loss of
generality we may assume that @xmath (otherwise interchange @xmath with
@xmath below) and by removing redundant information on the conditioning
(that is @xmath if @xmath ) we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used eq. 28 . This proves that @xmath is a family of mutually
orthogonal subspaces of @xmath .

###### Lemma 7.3.

Let @xmath be an arbitrary square integrable random variable and let
@xmath .

-    The orthogonal projection onto @xmath exists and is given by

      -- -------- -- ------
         @xmath      (29)
      -- -------- -- ------

-    If @xmath for all @xmath , then @xmath .

-    For any measurable map @xmath such that @xmath , it holds that
    @xmath .

###### P 34.

Let @xmath be any non-empty subset and @xmath any subset hereof. Now
note that by the independence of @xmath we may remove redundant
information as follows

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we also used that only the smallest conditioning @xmath -algebra
remains in an iterated conditional expectation. With @xmath defined in
and @xmath being a proper subset we have that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

First realize when summing over all possible subsets @xmath , we exactly
hit all conditioning indexes of the form @xmath at least once in the
sum. Hence we may instead sum over @xmath and change the conditional
expectation to @xmath , but in order to do this we must count how many
different subsets @xmath result in the same @xmath . Or to put it
differently when considering any subset @xmath which and how many
subsets @xmath result in @xmath .

This only holds for sets of the form @xmath , for any @xmath . To see
this note that in order for @xmath it is necessary that @xmath , so it
can only be true for @xmath of the the form @xmath for some set @xmath .
Let @xmath such that @xmath is non-empty, but then as @xmath we get

  -- -------- --
     @xmath   
  -- -------- --

so as stated above we need to restrict @xmath to be a subset of @xmath
in order for @xmath to fulfil @xmath .

In general for each @xmath we can chose @xmath to consist of @xmath
elements of @xmath , and there are exactly @xmath distinct ways of
choosing @xmath distinct elements from @xmath . Hence by using the
binomial formula we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now realize that for any subset @xmath with @xmath we have that @xmath
and thus by removing redundant information from the conditioning we get

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by using the above. It is furthermore not hard to realize that @xmath
can be written as a measurable function composed with @xmath , which
satisfies

  -- -------- --
     @xmath   
  -- -------- --

by Minkowski’s inequality, proving that @xmath .
Hence in order to check that @xmath is indeed the projection of @xmath
onto @xmath it remains to verify that @xmath for all @xmath . Take any
@xmath and note that @xmath for some @xmath . Then by using the
bilinearity of inner products we get

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Now realize that @xmath is the orthogonal projection of @xmath onto the
closed linear subspace @xmath of all @xmath -measurable mappings, and
therefore by [ Sch05 ] Corollary 21.6(ii), we get that @xmath , implying
that the first term is zero. As to the second term we simply note that
for any @xmath

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

by the defining property of random variables in @xmath . We conclude
that @xmath and that @xmath , proving that @xmath is indeed the
orthogonal projection of @xmath onto @xmath .

As regards the orthogonal projection @xmath when @xmath the formula
still holds. In that case we have that the projection onto @xmath is
given by @xmath , by the above mentioned convention about conditioning
on the empty set. Lastly, we can identify @xmath , since mappings that
are measurable with respect to the trivial @xmath -algebra are constant
and vice versa. Now we know that the orthogonal projection of @xmath
onto @xmath is given by the conditional expectation @xmath conditioning
on the trivial @xmath -algebra, which by Theorem 22.4 (xiii) in [ Sch05
] coincides with @xmath , proving that the formula holds.
As regards the two last claims, assume @xmath for all @xmath and note
that if @xmath then if @xmath then @xmath , implying that @xmath , so
the assertion holds for @xmath . Here we used that @xmath which is
easily seen by using the above formula for the projection onto @xmath
spaces. Now assume that the assertion also holds for any @xmath with
@xmath hence by induction we are done if it holds for @xmath with @xmath
. The induction assumption implies that every term with @xmath for all
@xmath , hence

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

But note that the assumption @xmath implies that @xmath . Now using that
@xmath , we get @xmath , which proves the claim.
The very last claim can be verified by checking that @xmath or
equivalently

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath . Fix any such @xmath and note that @xmath for all @xmath
implying that @xmath for all @xmath . Hence @xmath for all @xmath , or
equivalently @xmath for all @xmath , which by the above claim implies
that @xmath . But since @xmath is @xmath -measurable it follows that
@xmath .

The following theorem called the Hoeffding decomposition theorem gives
an explicit representation of any symmetric square-integrable mapping in
terms of its projections onto the above mentioned subspaces. This
decomposition theorem will later allow us to decompose U-statistics
(defined next section) in a beneficial way.

###### Theorem 7.4 (The Hoeffding decomposition).

Let @xmath be a symmetric measurable mapping such that @xmath . Then we
have the following decomposition holds almost surely

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where @xmath is a symmetric function given by

  -- -------- --
     @xmath   
  -- -------- --

An important thing to note is that for all @xmath with identical
cardinality the projections @xmath is given by a fixed function with
arguments @xmath .

###### P 35.

First note that by the integrability condition we have that @xmath , by
lemma 7.3 (3). Hence @xmath is identical to it’s projection onto @xmath
. By the mutual orthogonality of the spaces in @xmath that projection is
given by the sum of projections onto @xmath for @xmath . Each of these
subspace projections can be expressed by formula in lemma 7.3 (i). Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

almost surely, where the mapping @xmath is a @xmath -almost everywhere
unique mapping satisfying that it is a conditional expectation of @xmath
given @xmath . In order words @xmath almost surely.
Now fix any @xmath with @xmath and @xmath . By the symmetry of @xmath ,
we have that @xmath for any permutation @xmath of @xmath , so we may
change the order of the arguments. Hence with @xmath

  -- -------- --
     @xmath   
  -- -------- --

which by similar arguments as in Corollary 2.2.4 [ RNH14 ] implies that
for @xmath -almost all @xmath that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Hence every mapping @xmath for @xmath with identical cardinality
coincides @xmath -almost everywhere with @xmath . This allows us to
change summation indexes in the following way

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

almost surely. Now realize that this is the form as stated in the
theorem, that is

  -- -------- --
     @xmath   
  -- -------- --

almost surely, where @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

which by the symmetry of @xmath is itself a symmetric function.

#### 7.2.2 U-statistics

Let @xmath be a index set and consider a family of probability
distributions @xmath on a measurable space @xmath and a functional
@xmath . In the terminology of [ KB13 ] we assume that @xmath is a
regular functional, that is there exists a mapping (refereed to as the
kernel) @xmath that is @xmath -integrable for all @xmath , such that

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be a sequence of independent and identically distributed
random variables each with distribution @xmath and let @xmath for all
@xmath . We want to establish an estimator for @xmath and the obvious
one is to simply estimate @xmath by @xmath but in the case that we have
@xmath observations there is unused samples. Hence we propose the
unbiased estimator for @xmath given by the arithmetic mean

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

where @xmath and the summation is over all @xmath possible @xmath
-permutations @xmath of @xmath . We note that one may replace each term
with the arithmetic mean of all @xmath @xmath -permutations of @xmath .
That is

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

where @xmath is the set of all permutations of @xmath .

To see this, fix any @xmath -permutation @xmath of @xmath and define
@xmath and note that this only contains @xmath one time for each
permutation @xmath of @xmath and nothing else. Now consider the
expression @xmath , the sum of @xmath over all possible @xmath
-permutations @xmath of @xmath . This sum consists solely of terms of
the form @xmath for @xmath being a @xmath -permutation on @xmath . For
any fixed @xmath -permutation @xmath of @xmath , we shall count how many
times @xmath occurs in the sum we consider. We note that @xmath only
occurs in @xmath whenever @xmath is a permutation of @xmath and in the
affirmative it occurs only once. There are @xmath terms in the sum
@xmath such that @xmath is a permutation of @xmath . Thus for every
@xmath -permutation @xmath of @xmath the term @xmath appears @xmath
times, hence we conclude that

  -- -------- --
     @xmath   
  -- -------- --

proving that the equality in eq. 31 is valid.
Now define the symmetrized version of @xmath by

  -- -------- --
     @xmath   
  -- -------- --

such that

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is a symmetric mapping, it holds that for each @xmath ,
there will be @xmath terms in the above sum which are permutations of
@xmath contributing the same amount. Hence we may write

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Thus we may without loss of generality restrict the concept of these
unbiased estimators to symmetric kernels and define U-statistics as
follows.

###### Definition 7.5.

Suppose that the kernel @xmath for the regular functional @xmath is
symmetric. Then the unbiased estimator @xmath for the parameter @xmath ,
based on the first @xmath -samples @xmath for @xmath , called the
U-statistic with symmetric kernel @xmath of degree @xmath , is given by

  -- -------- --
     @xmath   
  -- -------- --

If the kernel @xmath is non-symmetric, then we define @xmath by eq. 30
and note that @xmath , where @xmath is the symmetrized version of @xmath
.

###### Remark 7.6 (Numerical considerations for a given @xmath-sample).

Say we are given an @xmath -sample @xmath and want to calculate @xmath .
If @xmath initially was a symmetric mapping we obviously have that
@xmath and the definition of @xmath reduces ( @xmath ) the number of
computations of @xmath compared to the representation eq. 30 , since we
only need to deal with @xmath summands. In the case that @xmath is
non-symmetric the representation doesn’t matter, both have @xmath
summands.

In the further analysis of U-statistics we need to define the following
mappings

###### Definition 7.7.

For any kernel @xmath we define the mappings @xmath by

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath , e.g. @xmath and @xmath .

Recall that the kernel @xmath is a @xmath -integrable mappings so the
conditional expectations are well-defined and we especially have that
@xmath is a conditional expectation of @xmath given @xmath . That is,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for @xmath -almost all @xmath , by similar arguments as in Corollary
2.2.4 of [ RNH14 ] .
Having defined the conditional expectations we will now introduce some
rather tedious and (for the moment) unintuitive recursively defined
mappings.

###### Definition 7.8.

For any symmetric kernel @xmath we recursively define the mappings
@xmath by

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

for @xmath . With the convention that @xmath such that @xmath .

We may note that the mappings @xmath defined above, are themselves
symmetric kernels for all @xmath . Furthermore one can show that they
possess the following properties.

###### Theorem 7.9.

For any symmetric kernel @xmath it holds that

-   @xmath for all @xmath and @xmath .

-   @xmath for all @xmath .

###### P 36.

See [ Lee90 ] theorem 2 in section section 1.6

We will henceforth drop the first parenthesis and apply the convention
that the superscript is always read first, i.e. the first equality will
now be written as @xmath . These recursively defined mappings @xmath
turns out to exactly be the orthogonal projections of @xmath onto the
spaces @xmath defined in section 7.2.1 , whenever the orthogonal
projections exists. We stress that the orthogonal projections of @xmath
exist if @xmath , but to define @xmath it suffices that @xmath . We will
now show the equality of the recursively defined mappings and orthogonal
projections in conjunction with the so-called Hoeffding decomposition of
U-statistics.

###### Theorem 7.10 (The Hoeffding decomposition of U-statistics).

Assuming that @xmath is a symmetric kernel of degree @xmath , then the
orthogonal projection of the U-statistics @xmath onto the space @xmath ,
defined in section 7.2.1 , exists. As a consequence we arrive at the
following decomposition of the U-statistic into a linear combination of
U-statistics with kernels of lower degrees:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

almost surely, where the mapping @xmath is a orthogonal projection
mapping of @xmath onto @xmath , that is, @xmath and it is given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where these last two equalities also hold if @xmath .

###### P 37.

Using the Hoeffding decomposition - theorem 7.4 - on the square
integrable (Minkowski’s inequality) mapping

  -- -------- --
     @xmath   
  -- -------- --

for some measurable mapping @xmath , we get that @xmath can be written
as the projection onto the space @xmath in the following way

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Note that for any @xmath then @xmath since @xmath for all @xmath ,
implying that @xmath , so these terms does not contribute to the above
summation. Hence we can for starters remove the terms for @xmath without
changing anything. Now for any two @xmath and @xmath with @xmath we have
that

  -- -------- --
     @xmath   
  -- -------- --

This is seen by inspecting the representation of @xmath in the Hoeffding
decomposition ( theorem 7.4 ) and noting that since @xmath is i.i.d.
then @xmath for any @xmath , implying that the projections are given by
identical functions @xmath composed with @xmath . Now for any fixed
@xmath with @xmath all @xmath that does not contain @xmath yields a
zero, but on the other hand as we argued above, every partitioning that
does contain @xmath yields the same projection term. When summing over
all @xmath -partitionings @xmath we have exactly @xmath terms which
contains @xmath , implying that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath as per above mentioned convention (see
explicit formula for @xmath in theorem 7.4 ) is the conditioning on the
trivial @xmath -algebra @xmath . Conditioning on the trivial @xmath
-algebra is simply the regular expectation, that is @xmath . The above
binomial coefficient factors can be rewritten as

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and as a consequence we get that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

is itself is a @xmath -sample U-statistic of degree @xmath with
symmetric kernel @xmath .
Using the explicit formula for @xmath found in theorem 7.4 we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now realize that we can re-index (reverse the order of summation) the
outer sum over with @xmath to get

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we in the last equality used the identity given in equation (11)
in section 1.6 of [ Lee90 ] . This identity is proved by rewriting
@xmath is terms of integrals followed by further manipulations. The
specific steps are notation heavy and does not provide further insight
into the nature of the mappings @xmath , hence we refer the reader to [
Lee90 ] section 1.6 for the proof of the identity. Thus we also have
that

  -- -------- --
     @xmath   
  -- -------- --

which is what we wanted to show.

###### Corollary 7.11.

The above decomposition of @xmath holds even though @xmath , but the
geometric property that @xmath are projection mappings does no longer
hold.

###### P 38.

See page 8-9 in [ Bor96 ] or Lemma @xmath in section 5.1.5 of [ Ser09 ]
.

#### 7.2.3 V-statistics

Consider the exact same set-up as in the above section on U-statistics.
That is, we have a sequence of independent and identically distributed
random elements @xmath in a measurable space @xmath . We want to
estimate

  -- -------- --
     @xmath   
  -- -------- --

for @xmath and @xmath a kernel.
We will now construct an estimator for @xmath based on the @xmath first
samples @xmath

###### Definition 7.12.

The in general biased estimator @xmath for the parameter @xmath , based
on the first @xmath -samples @xmath , called the V-statistic with kernel
@xmath of degree @xmath , is given by

  -- -------- -------- --
     @xmath            
              @xmath   
  -- -------- -------- --

where @xmath is the random empirical measure of @xmath based on @xmath

#### 7.2.4 Various results and definitions

###### Definition 7.13 (Degeneracy).

Let @xmath be a symmetric kernel. The rank of @xmath or the
corresponding U- or V-statistic is defined as the smallest integer
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

almost surely and

  -- -------- --
     @xmath   
  -- -------- --

with positive probability. We say the the kernel is @xmath -degenerate
of order @xmath and if @xmath it is called non-degenerate and in the
case that @xmath we say that it is completely degenerate.

In the literature there is (at least) two non-equivalent ways of
defining degeneracy of kernels. The above definition of degeneracy of
kernels coincide with that of [ Bor96 ] , [ KB13 ] and [ GZ92 ] , which
allows for stronger results than authors who defines degeneracy as
below. For example in [ GZ92 ] we have a SLLN for degenerate
U-statistics, which has weaker convergence conditions than the square
integrability required to define degeneracy, using the definition from [
Lee90 ] , [ Ser09 ] and [ VdV00 ] . They define the degeneracy of
kernels that has second moment @xmath , in the following way. If the
second moment exists then the following constants are well-defined

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath and @xmath . Then they define the kernel @xmath to be
degenerate of order @xmath if

  -- -------- --
     @xmath   
  -- -------- --

For consistency - since we use results from literature defining
degeneracy in both ways - we show that definitions are equivalent under
the assumption of square integrability of the kernel.

###### Lemma 7.14.

If @xmath then the above two definitions of degenerate kernels are
equivalent.

###### P 39.

Assume that @xmath and that @xmath is @xmath -degenerate of order @xmath
, that is

  -- -------- --
     @xmath   
  -- -------- --

almost surely and @xmath with positive probability. Thus by the
recursive nature of @xmath (formally by an induction argument as below)
we may realize that @xmath almost surely implying that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . By the definition of @xmath we see that

  -- -------- --
     @xmath   
  -- -------- --

Thus @xmath is with positive probability not equal to its mean, hence we
have that @xmath . Furthermore

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by Jensen’s conditional inequality, proving that @xmath .
Conversely if @xmath then we start by inductively showing that @xmath .
We obviously have that @xmath almost surely, showing the induction
basis. Now for the inductive step assume that @xmath almost surely for
all @xmath and note that this also holds for any @xmath ’element subset
@xmath of @xmath . Thus

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

but since @xmath we have that @xmath implying that the above difference
vanishes, proving that @xmath almost surely. By induction we now have
that @xmath . As argued above these stay almost surely zero for any such
independent and identically distributed arguments. Hence the double sum
of @xmath vanishes, such that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

almost surely. Since @xmath we have that @xmath is non-degenerate and
hence different from its mean @xmath with positive probability. Thus
@xmath with positive probability.

###### Corollary 7.15.

For any non-zero symmetric kernel @xmath it holds that @xmath defined in
definition 7.8 are complete degenerate kernels for all @xmath .

###### P 40.

This is an immediate consequence of theorem 7.9

In the thesis we are going to use the following decomposition theorems
of V-statistics which are similar to the above proven Hoeffding
decomposition of @xmath -statistics.

###### Lemma 7.16.

A centered V-statistic with symmetric kernel @xmath of degree @xmath can
be decomposed into a linear combination of V-statistics. That is,

  -- -------- --
     @xmath   
  -- -------- --

###### P 41.

See section 1.3 in [ Bor96 ] .

###### Lemma 7.17.

A V-statistic with symmetric kernel @xmath of degree @xmath can be
decomposed into a linear combination of U-statistics. That is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a symmetric and measurable mapping given by

  -- -------- --
     @xmath   
  -- -------- --

with @xmath .

###### P 42.

See section 1.3 in [ Bor96 ] or theorem 1 of section 4.2 in [ Lee90 ] .

#### 7.2.5 Asymptotic results for U- and V-statistics

In the following let @xmath be a measurable space and let @xmath be an
i.i.d. sequence of random elements in with values in @xmath and
corresponding distribution @xmath .

###### Theorem 7.18 (Asymptotic distribution of degenerate
U-Statistics).

Let @xmath be a symmetric and measurable kernel of degree @xmath . If
@xmath is @xmath -degenerate of order 1 and @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

as @xmath tends to infinity, where @xmath are independent and
identically standard normal distributed, and @xmath are the real
eigenvalues (counting algebraic multiplicity) of the operator @xmath
given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

That is @xmath consists of every @xmath for which there exists a @xmath
such that

  -- -------- --
     @xmath   
  -- -------- --

as a mapping in @xmath

###### P 43.

See corollary 4.2.2 [ Bor96 ] , theorem 4.3.1 ( @xmath ) / corollary
4.4.2 [ KB13 ] or theorem 5.5.2 ( @xmath ) [ Ser09 ] .

###### Theorem 7.19 (Strong Law of Large Number for U-statistics).

Assume that @xmath is a symmetric and measurable function. If

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -- --
        
  -- -- --

###### P 44.

See theorem 3.1 [ Bor96 ] or theorem 3.1.2 [ KB13 ] or as first proved
in [ Hoe61 ] .

###### Theorem 7.20 (Strong Law of Large Numbers for Degenerate
U-statistics).

Let @xmath be a symmetric and measurable kernel which is @xmath
-degenerate of order @xmath , and let @xmath . If @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

That is,

  -- -------- --
     @xmath   
  -- -------- --

###### P 45.

See theorem 2 [ GZ92 ] .

###### Theorem 7.21 (Strong Law of Large Number for V-statistics).

Assume that @xmath is a symmetric and measurable function. If

  -- -------- --
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

###### P 46.

see proposition on page 274 in [ GZ92 ] or proposition 2.3 in [ KY02 ] .

### 7.3 Integration of Hilbert space valued mappings

This section is an short introduction into Pettis integration and is
inspired by the construction method of the Pettis integral seen in [
Rya13 ] and [ SG05 ] . The goal is to establish some theory which allows
for the integration of Hilbert space valued mappings @xmath , where
@xmath is a finite measure space @xmath is a @xmath -Hilbert space,
where the scalar field @xmath is either @xmath or @xmath .
The weak Pettis integral was introduced by Billy James Pettis in [ Pet38
] for Banach space valued mappings but we will restrict ourself to
Hilbert space valued mappings, since it suffices for our purpose in this
thesis. There are different notions of integrals with values in Hilbert
spaces. Two of these integrals are the strong Bochner integral and the
above mentioned weak Pettis integral.

The Bochner integral which allows for integration of Banach space valued
mappings and strong reefers to the fact that the integral is constructed
as the limit of integrals of simple mappings which approximates the
integrand. That a mapping has such a sequence of approximating simple
mappings is called being strongly measurable. A theorem called the
Pettis measurability theorem yields equivalence between being strongly
measurable and being both weakly measurable (defined below) and @xmath
-essentially separably valued (see [ Rya13 ] section 2.3). However the a
mapping only needs to be weakly measurable in order to define its Pettis
integral.

This section on Pettis integration was written before restricting the
thesis to marginal spaces that are separable (and as a consequence the
Hilbert spaces of attention are separable). This is why, even though we
actually have strong measurability of the mappings we intend to
integrate, we still use the Pettis integral. As we see in the main part
of the thesis, the Pettis integral suffices for our needs (in fact one
can show that the Pettis and Bochner integral coincide in our cases).

The name weak comes from the fact that we only impose weak conditions on
the integral. Unlike the Bochner integral (which is constructed as the
limit of simple mappings), the weak Pettis integral of a sufficiently
nice integrand @xmath over @xmath with respect to @xmath is defined as
the unique element @xmath which satisfies

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath (see below). This weak uniquely determining property
seems to be a reasonable starting requirement for the integral, since it
also holds for the Bochner integral and in some sense also conforms with
the interpretation that integrals are related to sums which exhibit
similar properties. In this explanation of the Pettis integral we
already claimed uniqueness and existence of the Pettis integral element
in @xmath , and this is essentially what the remaining part of this
section sets out to prove.
For the rest of the section, assume that @xmath is a finite measure
space, @xmath is a @xmath -Hilbert space and @xmath is some mapping. The
arguments for @xmath -Hilbert spaces follows similarly.
We also let @xmath denote its continuous dual space, that is

  -- -------- --
     @xmath   
  -- -------- --

is the space of all linear mapping from @xmath to @xmath . Now we define
the measurability and integrability conditions our integrands should
possess in order for the construction of the Pettis integral to be
successful.

###### Definition 7.22 (Weakly measurable mappings).

A mapping @xmath is said to be weakly @xmath -measurable (or scalarly
measurable) if @xmath is @xmath -measurable, for all @xmath .

###### Definition 7.23 (Scalar integrable mappings).

A weakly @xmath -measurable mapping @xmath is called scalarly @xmath
-integrable if @xmath , for all @xmath .

We prove the existence of the Pettis integral for a class of mappings,
by proving the existence of the Dunford integral and then establishing a
link between them. That is, for a suitable mapping @xmath we prove the
existence of the Dunford integral of @xmath over @xmath with respect to
@xmath , written as @xmath , which is an element in @xmath . Then we
introduce the natural injective embedding @xmath and if @xmath we say
that @xmath is Pettis integrable over @xmath with respect to @xmath and
define the Pettis integral as the element in @xmath which maps to the
Dunford integral via @xmath . Thus the first order of business is to
prove the existence of the Dunford integral.
Before proceeding to show the existence of the Dunford integral, we have
to establish which topology we equip @xmath with before we even start to
talk about its continuous dual. In general we equip every space of
continuous linear mappings between normed vector spaces @xmath and
@xmath with the operator norm. This norm is denoted @xmath and
equivalently defined by either of the following expressions

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

It is well known that this is indeed a norm on the subspace of
continuous linear mappings @xmath , it in fact makes @xmath a Banach
space (It is important that it is complete for later arguments). Thus
the operator norm of a linear mapping is finite (that is, a linear map
is bounded) if and only if the linear map is continuous. To see the only
if part simply note that if @xmath there exists a @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , proving continuity. As a last remark about the operator
norm; one should realize that @xmath for any @xmath , which trivially
follows from the second of the equivalent definitions of @xmath .
Now assume that @xmath is scalarly @xmath -integrable. For any
measurable set @xmath define the linear mapping @xmath by letting

  -- -------- --
     @xmath   
  -- -------- --

Furthermore we define the linear integral operator @xmath by letting

  -- -------- --
     @xmath   
  -- -------- --

Now if @xmath is continuous it is an element of @xmath and then we
define @xmath and denote it the Dunford integral of @xmath over @xmath
with respect to @xmath . By linearity of @xmath it suffices to show that
@xmath is continuous in zero. Thus note; for any @xmath that

  -- -------- --
     @xmath   
  -- -------- --

which tends to zero when @xmath tends to zero, if @xmath . That is, if
@xmath is a continuous map then @xmath is a linear continuous map.
@xmath is indeed continuous and one can realize this by utilizing the
closed graph theorem described below.

###### Theorem 7.24 (Closed graph theorem).

Assume that @xmath and @xmath are Banach spaces and @xmath is a linear
map such that, whenever @xmath and @xmath are Cauchy sequences, then
@xmath . Then @xmath is a continuous mapping.

###### P 47.

See theorem 2.15 and its remark in [ Rud91 ] for proof.

Now to use this theorem we simply assume that @xmath and @xmath are
Cauchy sequences with limits, say @xmath and @xmath . By the above
Closed graph theorem it suffices to show that @xmath in @xmath in order
to prove that @xmath is continuous. Now recall that @xmath -convergence
implies convergence in @xmath -measure which in turn implies that there
exists a subsequence @xmath converging @xmath -almost everywhere to
@xmath . On the other hand we note that @xmath implying that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

tends to zero as @xmath tends to infinity. We have established
point-wise convergence of @xmath to @xmath but we also showed the
existence of a subsequence hereof converging @xmath -almost everywhere
to @xmath . Hence we must have that the limits coincide @xmath -almost
everywhere, that is @xmath for @xmath -almost all @xmath .
Identification of mappings up to @xmath -almost everywhere equality in
@xmath now implies that @xmath in @xmath , so @xmath is indeed a linear
and continuous map and we may conclude that @xmath . By the above
arguments we therefore have that @xmath is linear and continuous
implying that @xmath .

###### Definition 7.25 (Dunford integral).

Assume that @xmath is scalarly @xmath -integrable mapping. For every set
@xmath we denote the Dunford integral of @xmath over @xmath with respect
to @xmath by @xmath and define it as the unique functional in @xmath
which maps

  -- -------- --
     @xmath   
  -- -------- --

Now we are close to defining the Pettis integral of @xmath with respect
to @xmath . The above Dunford integral is an element in @xmath but we
want an element in @xmath . Before proceeding we introduce the natural
isometric embedding @xmath , which we are going to use in determining
which element of @xmath we define as the Pettis integral. In the general
theory for Banach spaces @xmath is Pettis integrable if the image of
@xmath contains the Dunford integral of @xmath , and one defines the
Pettis integral as the element which is mapped to the Dunford integral
by @xmath . But as we shall see this is always the case whenever the
value space of @xmath is a Hilbert space.
Define the linear evaluation map @xmath by letting @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . That @xmath for all @xmath follows by noting that
@xmath is clearly linear and continuity in zero (and by linearity:
everywhere) follows from the inequality @xmath .

###### Lemma 7.26.

The evaluation mapping @xmath is an isometric isomorphism. That is
@xmath is a bijective mapping satisfying

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

###### P 48.

First we show that @xmath is isometric embedding (satisfy the above
equation). The last inequality before this lemma yields that

  -- -------- --
     @xmath   
  -- -------- --

Conversely by the corollary to theorem 3.3 (Hahn-Banach theorem) [ Rud91
] there exists an @xmath with @xmath and @xmath . Thus we get that

  -- -------- --
     @xmath   
  -- -------- --

Hence @xmath for all @xmath , and by linearity of @xmath we get that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , proving that @xmath is an isometric (hence also
injective and continuous) embedding of @xmath into @xmath . It remains
to be shown that @xmath is a surjective mapping. In our scenario, with
the value space being a Hilbert space, it is well known that the natural
embedding @xmath into the double continuous dual space is surjective -
but in general - spaces possessing this property are called reflexive.

In an effort to keep the thesis self-contained we sketch a proof for the
fact that every Hilbert space is reflexive. Note that by Reisz
representation theorem (see [ Sch05 ] ) we get that @xmath given by
@xmath is a bijection and the inverse especially fulfils that

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

for any @xmath and @xmath . It can easily be verified that @xmath
defined by

  -- -------- --
     @xmath   
  -- -------- --

is indeed a inner product which agrees with the operator norm on @xmath
, making it a Hilbert space. Yet again Reisz representation theorem
yields that @xmath given by @xmath is a bijection. The important thing
to note is that @xmath is surjective and

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath and @xmath , proving that @xmath is surjective.

Hence we know that there always exists a unique @xmath such that @xmath
which leads us to the following definition of the Pettis integral.

###### Definition 7.27 (Pettis integral).

If @xmath is scalarly @xmath -integrable we say that @xmath is Pettis
integrable with respect to @xmath and for any @xmath there exists a
unique element @xmath such that @xmath . We denote this element @xmath
and call it the Pettis integral of @xmath over @xmath with respect to
@xmath .

For simplicity lets make en equivalent definition of the Pettis integral
which circumvents using the Dunford integral but rather its defining
property.

###### Theorem 7.28.

If @xmath is scalarly @xmath -integrable and @xmath then the Pettis
integral @xmath is the unique element satisfying

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

###### P 49.

Recall that the Dunford integral @xmath is the unique element in @xmath
satisfying

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . But @xmath is the unique element in @xmath satisfying
@xmath . Combining these two characterisations we get that the Pettis
integral @xmath is the unique element in @xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

###### Corollary 7.29.

The Pettis integral exhibits the same linearity property as the Lebesgue
integral. That is, if @xmath and @xmath are both Pettis integrable over
@xmath with respect to @xmath then for any @xmath in the scalar field of
@xmath it holds that

  -- -------- --
     @xmath   
  -- -------- --

###### P 50.

This is a consequence of the above theorem. Simply note that by the
unique defining property of the Pettis integral

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The latter condition is easily verified by the linearity of @xmath .
That is,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

###### Lemma 7.30.

Any @xmath -measurable mapping @xmath , if Pettis integrable with
respect to @xmath , if

  -- -------- --
     @xmath   
  -- -------- --

###### P 51.

By theorem 7.28 is suffices to show that @xmath is @xmath -scalarly
integrable. That is, it suffices to show that @xmath for all @xmath .
Measurability : @xmath is an @xmath -measurable mapping and any @xmath
is continuous and hence @xmath -measurable. We conclude that the
composition @xmath indeed is @xmath -measurable, for any @xmath .
Integrability: Note that for any @xmath we have that @xmath , and since
@xmath for all @xmath , we get that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

if @xmath , proving that @xmath is Pettis integrable with respect to
@xmath .

###### Lemma 7.31.

If @xmath is Pettis integrable with respect to @xmath and @xmath is a
continuous linear map, then @xmath is Pettis integrable with respect to
@xmath and

  -- -------- --
     @xmath   
  -- -------- --

###### P 52.

First we show that @xmath is indeed Pettis integrable. Note that if
@xmath is scalarly @xmath -integrable then one simply hote that for any
@xmath then @xmath , hence

  -- -------- --
     @xmath   
  -- -------- --

proving that also @xmath is Pettis integrable with respect to @xmath .
Now for the identity note for any @xmath we have that @xmath , hence

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , proving the identity by the unique defining property of
the Pettis integral.

We may also extend the notions of expectations and variance to Hilbert
space valued random elements. Let @xmath be a random Borel element in a
Hilbert space @xmath , defined on some probability space @xmath .

###### Definition 7.32.

If @xmath is Pettis integrable with respect to @xmath (e.g. if @xmath ,
by above lemma), then we define the expectation of @xmath as the Pettis
integral of @xmath with respect to @xmath , that is

  -- -------- --
     @xmath   
  -- -------- --

and if @xmath then we define the variance of @xmath as the positive real
number

  -- -------- --
     @xmath   
  -- -------- --

This allows for a different notation of Pettis integrals with respect to
probability measures. For any probability space @xmath , we may
construct a random element @xmath such that @xmath . Then for any
Hilbert space valued mapping @xmath , that is Pettis integrable with
respect to @xmath , we have that

  -- -------- --
     @xmath   
  -- -------- --

This is easily seen, as @xmath fulfils the unique defining property of
the Pettis integral. That is

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath .

### 7.4 Complexification and Realification of a Hilbert space

#### 7.4.1 Complexification of a real Hilbert space

Assume that we have a @xmath -Hilbert space @xmath . We will now
associate a complex Hilbert space @xmath to @xmath , which will be
useful for our analysis of metric spaces of negative and strong negative
type. We follow the general procedure for complexification of real
vector spaces presented in [ Rom05 ] , and modify it to Hilbert spaces.

###### Definition 7.33.

We define the complexification @xmath of @xmath as the complex vector
space of ordered pairs in @xmath with coordinatewise addition

  -- -------- --
     @xmath   
  -- -------- --

and scalar multiplication

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath . Note that we can use notation alike to the
complex numbers by denoting @xmath by @xmath , and under this notation
we may write @xmath . With this notation addition and scalar
multiplication resembles those of the complex numbers.

We can also extend the original inner product @xmath on @xmath to an
inner product @xmath on @xmath , in the following way

###### Theorem 7.34.

The mapping @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is an inner product on @xmath

###### P 53.

Since @xmath is a @xmath -inner product we obviously have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

i.e. we have conjugate symmetry. Further more note that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
                       
  -- -------- -------- --

proving linearity in the first argument. Lastly we need to show
finite-definiteness of @xmath , and this follows from the symmetry and
finite-definiteness of @xmath . Note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving that @xmath is an inner product on @xmath .

###### Theorem 7.35.

The inner product space @xmath is complete and therefore a Hilbert
space.

###### P 54.

First note that @xmath is complete, meaning that every Cauchy sequence
converges. Now consider an arbitrary Cauchy sequence @xmath in @xmath
and note that

  -- -------- --
     @xmath   
  -- -------- --

by the equality in the above proof. Using this we see that

  -- -------- --
     @xmath   
  -- -------- --

which tends to zero as @xmath since @xmath is Cauchy, and we realize
that this happens if and only if both of the addends on the right hand
side converge to zero. Thus @xmath and @xmath are both Cauchy sequences
in @xmath . As a consequence they have limits @xmath and @xmath in
@xmath , and we note that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath converges to @xmath as @xmath tends to infinity. We
conclude that every Cauchy sequence in @xmath converges in @xmath ,
proving that @xmath is complete. Hence @xmath is a complete inner
product space, i.e. a Hilbert space.

Lastly we introduce a mapping which will be used in the main sections.

###### Definition 7.36.

We define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and call it the complexification map.

We easily see that the complexification map is injective and satisfies
the following properties

  -- -------- --
     @xmath   
     @xmath   
              
  -- -------- --

so cpx is additive and we may ”pull out” real scalars, so cxp is almost
a linear map if we disregard the fact that linearity of maps are only
defined for maps between vector spaces with the same scalar field.

###### Theorem 7.37.

The complexification map @xmath is an isometric embedding and

  -- -------- --
     @xmath   
  -- -------- --

for every @xmath

###### P 55.

Recall that @xmath and note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving that @xmath is an isometric embedding from the real Hilbert
space @xmath into the corresponding complexification Hilbert space
@xmath . It further more holds that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for every @xmath .

###### Theorem 7.38.

If @xmath is a separable Hilbert space, then @xmath is a separable
Hilbert space.

###### P 56.

Since @xmath is separable, we know that there exists a countable dense
subset @xmath . Note that @xmath is countable and for any @xmath there
exists a sequence @xmath such that @xmath and @xmath , since @xmath is
dense in @xmath . Thus

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is indeed separable.

#### 7.4.2 Realification of a complex Hilbert space

Let @xmath be a @xmath -Hilbert space. The realification @xmath of
@xmath is given by @xmath where we simply ignore the possibility of
scalar multiplying elements with complex scalars and only keep scalar
multiplication over @xmath . Let @xmath denote the identification map
between these spaces.

###### Theorem 7.39.

The mapping @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is an inner product of @xmath .

###### P 57.

We obviously have that @xmath and

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , also since @xmath preserves addition we also have that

  -- -------- --
     @xmath   
  -- -------- --

Lastly since @xmath has the property of positive-definiteness, so has
@xmath . That is

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 7.40.

@xmath is a @xmath -Hilbert space and if @xmath is separable, then
@xmath is separable.

###### P 58.

The inner product space @xmath is actually a @xmath -Hilbert space. To
see this note that @xmath is Cauchy in @xmath if and only if @xmath
Cauchy in @xmath , since @xmath for all @xmath . Furthermore if @xmath
converges to @xmath in @xmath we also have that @xmath converges to
@xmath in @xmath , proving that @xmath is complete. This furthermore
implies that if @xmath is separable, then @xmath is also separable.

We may note that the identification map @xmath is an additive isometric
since @xmath and @xmath . Lastly it is bijective, rendering @xmath an
additive isometric isomorphism, hence also a homeomorphism

### 7.5 Tensor product of Hilbert spaces

In this section we are going to construct the tensor product of two
Hilbert spaces. This tensor product turns out to be a new Hilbert space,
which we can use in the theory of metric spaces of negative and strong
negative type. The following construction approach is found in [ RS72 ]
, but we try to prove things more carefully.

We can only construct the tensor product of two Hilbert spaces with the
same scalar field, so consider two @xmath -Hilbert spaces @xmath and
@xmath , both with the same scalar field @xmath or @xmath . Since we in
the theory of metric spaces of negative type, may have two Hilbert
spaces with different scalar fields on our hands, the approach is to
complexify [realify] (see section 7.4 ) the real [complex] Hilbert
spaces and carry on with the following construction.
For any @xmath we define the map @xmath , as the tensor product of
@xmath and @xmath , given by

  -- -------- --
     @xmath   
  -- -------- --

Its easily realized that @xmath is a conjugate bilinear map(anti-linear
in both arg.), with the following properties

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

any we may note that @xmath is the zero mapping if and only if @xmath or
@xmath are zero elements of @xmath and @xmath respectively.
Now denote @xmath , the collection of conjugate bilinear mappings from
@xmath to @xmath , that can be written as a finite linear combinations
of tensor products. That is

  -- -------- --
     @xmath   
  -- -------- --

where scalar multiplication and summation of maps is defined in the
regular fashion @xmath for any maps @xmath and scalar @xmath .
For any two tensor products @xmath and @xmath we define

  -- -------- --
     @xmath   
  -- -------- --

and extend by linearity in first argument and conjugate linearity in the
second argument. That is by letting

  -- -------- --
     @xmath   
  -- -------- --

for arbitrary linear combinations of tensor products. That we need
conjugate linearity in the second argument follows from the fact that
@xmath , using that @xmath and @xmath are @xmath -inner products. Note
that we have not said anything about @xmath being a map on @xmath , this
requires that @xmath is invariant to how one represents the bilinear
mappings of @xmath , which is among other things what we show in the
following lemma.

###### Lemma 7.41.

@xmath is a @xmath -vector space and @xmath is a well-defined mapping
making @xmath a @xmath -inner product space.

###### P 59.

With the above mentioned definition of scalar multiplication and
summation of maps, we see that @xmath is closed under summation and
scalar multiplication. For any @xmath and @xmath then for some @xmath
and @xmath , @xmath , @xmath , @xmath , @xmath , @xmath we have that

  -- -------- --
     @xmath   
  -- -------- --

Hence we obviously see that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath . Furthermore the zero-element of
@xmath is the (obviously bilinear) zero mapping @xmath . All other
axioms for vector spaces are easily realized, so we conclude that @xmath
is a @xmath -vector space.
By the extension of @xmath to finite linear combinations of tensor
products above, we have that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

proving that @xmath is sesquilinear (sesqui meaning one-and-a-half, the
same as conjugate linear), where @xmath and

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

with @xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath ,
@xmath and @xmath .
In order for @xmath to be a well-defined mapping on @xmath , we must
show that no matter what finite linear combination we express @xmath and
@xmath in, then @xmath stay the same. Let @xmath be two finite linear
combination representations of @xmath and let @xmath be two finite
linear combination representations of @xmath . Then we note that @xmath
and @xmath , hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving that @xmath is a well-defined mapping on @xmath . Thus we have
proved that @xmath is a well-defined mapping of sesquilinear form, and
it only remains to be shown that @xmath possess the
positive-definiteness property, i.e. @xmath with @xmath , in order to
confirm that it is a inner product on @xmath .
Let @xmath , and assume that a finite linear combination representation
is given by

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath , @xmath , @xmath and @xmath . Consider the following
subspaces

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and note that there exists two orthonormal bases @xmath and @xmath with
@xmath for the @xmath and @xmath respectively (reduce to independent
vectors and utilize Gram-Schmidt procedure). Now note that for any
@xmath , we can represent @xmath and @xmath in terms of linear
combinations of the basis elements for every @xmath , yielding

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath . Hence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and note by the orthogonality that each term is zero if @xmath or @xmath
and for those terms where @xmath and @xmath the inner products become
@xmath . Hence

  -- -------- --
     @xmath   
  -- -------- --

This proves that @xmath and we have that

  -- -------- --
     @xmath   
  -- -------- --

but note that @xmath and since @xmath and @xmath are orthonormal bases
then @xmath , proving that @xmath if and only if every factor @xmath is
zero. We conclude that

  -- -------- --
     @xmath   
  -- -------- --

proving that @xmath is an inner product on @xmath .

First some notes about how the completion of @xmath and hence also the
tensor product is constructed: Let @xmath be the inner product induces
norm on @xmath , and let @xmath be the @xmath -vector space of all
Cauchy sequences

  -- -------- --
     @xmath   
  -- -------- --

with scalar multiplication and addition defined by @xmath and @xmath and
zero element @xmath . Define the semi-norm @xmath by

  -- -------- --
     @xmath   
  -- -------- --

and note that the limit always exists and is an element of @xmath since
@xmath is Cauchy in @xmath which in complete. This is a semi-norm since
there may be Cauchy sequences @xmath with @xmath . But if we define
@xmath then @xmath is a norm on the quotient @xmath -vector space @xmath
given by the space of equivalence classes @xmath , which identifies
elements @xmath if @xmath . This @xmath -vector space has scalar
multiplication and addition defined by @xmath and @xmath . That is, we
have the following normed @xmath -vector space

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . Note that @xmath is well-defined on @xmath by this
definition, since for any two @xmath then @xmath and hence @xmath ,
proving that @xmath .
We can now define the linear (by the scalar multiplication and addition
defined on the space @xmath and @xmath ) operator @xmath by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the constant Cauchy sequence @xmath . By the linearity
of @xmath we see that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , proving that @xmath is an isometry between @xmath and
@xmath . In other words we have that @xmath is isometrically embeddable
via @xmath into its completion @xmath .

###### Definition 7.42.

We define the tensor product of @xmath and @xmath by @xmath , i.e. the
@xmath -Banach space given by the completion of @xmath with respect to
the induced norm. Whenever a simple tensor @xmath is presented it will
henceforth, without mention, represent the corresponding element in the
completion @xmath .

By proposition 1.9 [ Con90 ] there exists an inner product @xmath on
@xmath with induced norm coinciding with @xmath rendering @xmath a
@xmath -Hilbert space. This inner product furthermore satisfies @xmath
for any @xmath . This especially entails that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath and @xmath .
The following proof is only true for separable Hilbert spaces @xmath and
@xmath .

###### Lemma 7.43.

If @xmath and @xmath are separable Hilbert spaces, then the map @xmath
is a @xmath -measurable.

###### P 60.

Fist note that the map in question if the composition @xmath , where
@xmath is given by the simple tensor, that is @xmath for any @xmath .
The isometric embedding into the completion @xmath is continuous and
therefore measurable with respect to the Borel @xmath -algebra induced
by the respective norms, i.e. @xmath -measurable. Thus it suffices to
show that @xmath is @xmath -measurable. Since @xmath and @xmath are
separable spaces we note that @xmath and therefore it suffices to show
that @xmath is a continuous mapping. Fix any point @xmath and note that

  -- -------- --
     @xmath   
  -- -------- --

Thus for any @xmath , let @xmath and notice that for any @xmath (see
section 7.1 regarding that the maximum metric is a metric generating the
product topology on @xmath ) we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving continuity of @xmath in every point @xmath , which concludes the
proof.

###### Theorem 7.44.

@xmath is an orthonormal basis for @xmath if @xmath and @xmath are
orthonormal bases for @xmath and @xmath respectively. Furthermore since
@xmath and @xmath are both separable Hilbert spaces we know that @xmath
and @xmath are either finite or countably infinite index sets.

###### P 61.

That any orthonormal basis for a separable Hilbert space is at most
countably infinite, follows from standard Hilbert space theory, the
arguments can also be found in remark 5.9 . Assume without loss of
generality that both Hilbert spaces @xmath and @xmath are infinite
dimensional. We know that every Hilbert space has an orthonormal basis,
so fix any two arbitrary orthonormal bases @xmath and @xmath of @xmath
and @xmath respectively. Denote @xmath and note that it suffices to show
that @xmath . To see this note that in the affirmative, then @xmath and
@xmath , where the overline denotes the closure with respect to the
topology on @xmath . The fact that @xmath , which follows by showing
that @xmath is dense in @xmath (omitted, trivial @xmath -proof) and the
fact that the closure of a dense set is equal to the space itself.

Since @xmath is the space of finite linear combinations of elements
@xmath for @xmath , @xmath , it suffices to show that @xmath for any
@xmath and @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

Assume without loss of generality that both @xmath and @xmath are
infinite dimensional Hilbert spaces, rendering the orthonormal bases
@xmath and @xmath countably infinite, so we may enumerate them by the
natural numbers. Thus fix @xmath and @xmath and note that since @xmath
and @xmath are orthonormal bases for @xmath and @xmath respectively, we
get that

  -- -------- --
     @xmath   
  -- -------- --

where we equalities are understood as convergence in the norms on @xmath
and @xmath . Now realize that @xmath , since it converges in the
topology of @xmath . This is seen by noting that such a series converge
if and only if

  -- -------- --
     @xmath   
  -- -------- --

converges, but we note that this equals

  -- -------- --
     @xmath   
  -- -------- --

by the orthonormality of the bases and the inequality @xmath known as
Bessel’s inequality. With @xmath and @xmath , linearity and the triangle
inequality yields that

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

which converge to zero when @xmath and @xmath tends to infinity, by the
basis representation of @xmath and @xmath above and the fact that @xmath
(reverse triangle inequality). Since @xmath can be written as a limit of
elements in @xmath it must lie in the closure @xmath .

### 7.6 Characteristic functions of random elements in @xmath

We will briefly introduce the theory of characteristic functions of
random elements in @xmath -dimensional Euclidean spaces. To this end,
let @xmath denote the regular inner product on @xmath , and let @xmath
be some probability space. We refer to [ Fol99 ] for the theory
concerning integration of complex valued functions and to [ Sch05 ] for
measure and integration theory.
Let @xmath be a measurable mapping (random vector) and let @xmath denote
the push-forward measure on @xmath , i.e. @xmath .

###### Definition 7.45.

The characteristic function @xmath of @xmath (or the law @xmath on
@xmath ) is defined as

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath . For a Borel probability measures @xmath on @xmath we
denote the corresponding characteristic function by @xmath

We note that the characteristic function is always well-defined by
realizing that @xmath .
Now lets prove some properties of characteristic functions of random
vectors. Note these properties equally apply to probability measures on
@xmath , in the sense that we can think of the identity mapping @xmath
as the random vector with sample space @xmath . .

###### Theorem 7.46 (Properties of the characteristic function).

Let @xmath and @xmath be random elements in @xmath and @xmath
respectively, then the following holds

1.   [label=(0)., ref=(0)]

2.  @xmath , @xmath and @xmath for all @xmath .

3.   The mapping @xmath is uniformly continuous.

4.   If @xmath : @xmath for all @xmath if and only if @xmath .

5.  @xmath for all @xmath if and only if @xmath .

6.   If @xmath then @xmath for all @xmath .

7.   The conditions of creftype 3 and creftype 4 are equivalent to
    requiring that they hold almost everywhere with respect to the
    Lebesgue measure on @xmath and @xmath respectively.

8.   For any @xmath , let @xmath and @xmath be a linear mapping then
    @xmath has characteristic function given by @xmath for all @xmath .

9.  @xmath has a symmetric distribution around @xmath if and only if
    @xmath .

###### P 62.

(1): It is trivial that @xmath since @xmath . Furthermore let @xmath and
note that by the triangle inequality for complex valued integrals we
have that

  -- -------- --
     @xmath   
  -- -------- --

and as regards the last claim simply note that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

since cosine is even, sinus is odd and @xmath is linear.
(2): The proof proceeds analogously to the one-dimensional case: By the
continuity of the map @xmath for any @xmath , we get that for any
sequence @xmath with @xmath

  -- -------- --
     @xmath   
  -- -------- --

as @xmath tends to infinity by Lebesgue’s dominated convergence theorem.
Hence for any @xmath there exists a @xmath such that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for all @xmath with @xmath , proving uniform continuity of the
characteristic function.
(3): see [ Dud02 ] theorem 9.5.1.
(4): If follows rather trivially by noting that @xmath on @xmath
allowing us to utilize Fubini’s theorem to conclude that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath . Only if follows after realizing that by the above we
have that the distribution @xmath on @xmath has characteristic function
@xmath for @xmath coinciding with the characteristic function of @xmath
. Hence by (2) we have that @xmath proving independence.
(5): Simply note that for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

(6): The result follows from an easy application of creftype 1 . Assume
for contradiction that @xmath @xmath -almost everywhere and that there
exists a @xmath such that @xmath . Let @xmath and note that by the
continuity of @xmath there exists a @xmath such that @xmath , where
@xmath denotes the open @xmath -ball of @xmath . However we now have
that @xmath ↯. The equivalence for creftype 4 follows exactly similarly.
(7): Let @xmath and simply note that @xmath for any @xmath .
(8): By definition @xmath is symmetric around @xmath if @xmath , and if
this is the case then creftype 3 and creftype 7 yields that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

for any @xmath , where @xmath is the identity mapping, proving that
@xmath . For the converse realize that if @xmath then by the same
arguments as above we get that

  -- -------- --
     @xmath   
  -- -------- --

for any @xmath , which by creftype 3 proves that @xmath .

### 7.7 Miscellaneous

###### Lemma 7.47.

If @xmath and @xmath then the set function @xmath given by

  -- -------- --
     @xmath   
  -- -------- --

is a finite signed measure. If furthermore @xmath and @xmath is
measurable then @xmath and

  -- -------- --
     @xmath   
  -- -------- --

###### P 63.

We obviously have that @xmath and for any disjoint sequence of sets
@xmath we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by the dominated convergence theorem for signed measures, since @xmath .
Hence @xmath is a signed measure, but it furthermore holds that @xmath
for any @xmath , proving that @xmath is a finite signed measure. Now
note that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and as a consequence

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

by the same reasoning as in lemma 2.4 . So @xmath if @xmath , and as we
shall see this is indeed the case if @xmath . If @xmath is measurable
there exists a sequence of positive simple mappings @xmath such that
@xmath point-wise with @xmath for all @xmath . By the monotone
convergence theorem we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

proving that @xmath if @xmath . Hence if @xmath is measurable and @xmath
then there exists a sequence of simple mappings @xmath such that @xmath
point-wise with @xmath for all @xmath . By the dominated convergence
theorem for signed measures we have that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we used that @xmath .

###### Lemma 7.48.

It holds that

  -- -- --
        
  -- -- --

###### P 64.

Initially recall that for any metric

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

by the triangle inequality.
Fix @xmath in @xmath or @xmath and note that if @xmath then

  -- -------- --
     @xmath   
  -- -------- --

Now we create upper bounds for @xmath by using the triangle inequality
on the positive terms

-   Expanding first term with

    -   @xmath as an intermediate point

          -- -------- -------- --
             @xmath   @xmath   
                      @xmath   
                      @xmath   
          -- -------- -------- --

    -   @xmath as an intermediate point

          -- -------- -------- --
             @xmath   @xmath   
                      @xmath   
                      @xmath   
          -- -------- -------- --

-   Expanding fourth term with

    -   @xmath as an intermediate point

          -- -------- -------- --
             @xmath   @xmath   
                      @xmath   
                      @xmath   
          -- -------- -------- --

    -   @xmath as an intermediate point

          -- -------- -------- --
             @xmath   @xmath   
                      @xmath   
                      @xmath   
          -- -------- -------- --

Thus if @xmath then

  -- -------- --
     @xmath   
  -- -------- --

If @xmath then by using the above inequalities we get

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

proving that in general

  -- -- --
        
  -- -- --

for @xmath or @xmath .
