# Chapter 1 Introduction \lettrine

[lines=3]The nature of dark matter (DM) remains one of the major
unsolved problems in physics. Originally inferred through its
gravitational influence on galaxies and clusters, a rich body of
evidence has accumulated over the last four decades firmly establishing
its existence. All of the evidence, however, comes from inferring dark
matter’s presence solely through its gravitational effects. Many open
questions remain: Does dark matter consist of a fundamental particle? If
so, what is its mass? Could there be an entire dark sector, akin to the
Standard Model (SM)? How does dark matter interact with the SM? The
quest to answer these questions drives a huge collective effort that
draws from a rich body of theoretical and experimental work, as well as
major input from computational and numerical studies. We are currently
at the dawn of a data-driven era in astrophysics and cosmology—a large
number of ongoing and forthcoming experiments, both in the lab and in
the sky, combined with an increasingly open approach to data
availability, offer great potential in elucidating the nature of dark
matter.

Dark matter plays a central role in many subfields of particle physics,
astrophysics and cosmology. Understanding its nature and interactions
would have far reaching consequences in those fields by providing major
insights into fundamental physics beyond the Standard Model as well as
elucidating the evolution of our Universe and the formation of
structures within it.

This introduction is organized as follows. In Sec. 1.1 , I will
summarize the large body of evidence pointing to the existence of dark
matter, occasionally touching upon relevant historical developments. In
Sec. 1.2 , I will describe possible explanations for the particle nature
of dark matter and various detection schemes, focusing on DM thermally
produced in the early Universe and specifically Weakly Interacting
Massive Particles (WIMPs). Section 1.3 will focus on the effort to
detect and characterize WIMPs through their astrophysical signatures, in
particular using gamma-ray data. I will briefly summarize the
theoretical and experimental tools available to us in these searches.
Finally, in Sec. 1.4 , I will describe the organization of the rest of
this thesis. This chapter partially draws from a number of excellent
review articles on the topic which the reader is referred to for further
details. Refs. [ 1 , 2 ] provide recent, comprehensive reviews of dark
matter physics. Ref. [ 3 ] reviews indirect detection, which will be the
main focus of this thesis. Finally, Ref. [ 4 ] provides a thorough
overview of the history of the field.

### 1.1 Evidence for Dark Matter

Although the study of dark matter had its inception and development in
the 20th century, the interplay between theory and observation in making
the unknown knowable goes back much earlier. For example, the
Aristotelian view of an immutable Universe with the Earth at its center
offered a clean framework that did not call for additional celestial
objects, and was the orthodox viewpoint until Renaissance astronomers
conclusively refuted it with observations. Galileo was able to leverage
new technological developments and make observations that arguably
played the largest role in this. After pioneering the development of the
telescope, he was able to understand the make-up of the Milky Way as
consisting of individual stars rather than diffuse clouds, observe
Saturn’s rings and discover Jupiter’s four largest moons. These
observations are very much in the spirit of modern dark matter
searches—demonstrating that the Universe can contain invisible forms of
matter, and that scientific inquiry and technological developments can
play a big role in revealing them to us.

Evidence for some yet-unknown form of matter started piling up in the
early 19th century. In 1922, Dutch astronomer Jacobus Kapteyn wrote down
for the first time a predictive model for the distribution of matter in
the Milky Way, describing the stars as particles in a virialized system
[ 5 ] and using this model to obtain the local matter density in terms
of the observed stellar mass. Kapteyn’s student Jan Oort [ 6 ] and
others [ 7 ] were able to derive estimates for the local matter density,
in some cases seeing excesses above the observed luminous mass.
Astronomers during this time reckoned with the existence of missing
matter in the Universe, in some cases explicitly using the term dark
matter [ 5 ] and positing that it could potentially be accounted for by
the extrapolation of the stellar luminosity function down to very faint
stars [ 6 ] .

In 1933, Swiss-American astronomer Fritz Zwicky studied redshift data
for galaxy clusters collected by Hubble and Humason [ 8 ] , using
estimates of the velocity dispersions in eight galaxies within the Coma
cluster to estimate its mass through the virial theorem [ 9 ] . Zwicky
obtained a theoretical prediction for the dispersion by using the number
of observed galaxies, average mass of a galaxy and its extent, finding a
value of @xmath 80 km s @xmath . This was in stark conflict with the
observed line-of-sight velocity dispersion of @xmath 1000 km s @xmath .
Although Zwicky’s work made use of an estimate of the Hubble constant
that was a factor of @xmath 8 too big compared to the current accepted
value, the large discrepancy between the observed and expected values
pointed to the existence of unaccounted-for matter in the Coma system.
Zwicky himself concluded that “If this would be confirmed, we would get
the surprising result that dark matter is present in much greater amount
than luminous matter.” An analysis of the Virgo cluster by Sinclair
Smith in 1936 again pointed to a very high mass-to-light ratio in that
system. In either case, the astronomers put forward potential
explanations in terms of diffuse clouds of internebular material [ 10 ]
.

Although this presented a conundrum, there was widespread consensus
within the astronomical community that more information would be needed
to understand what was going on. Historically, velocity rotation
curves—the circular velocity profiles of stars in a galaxy as a function
of the distance from the galactic center—did the most to convince the
scientific community of the existence of large amounts of non-luminous
matter in galaxies. The basic idea here is as follows. Standard
Newtonian theory dictates that the circular velocity of stars is given
by @xmath , where @xmath is the radial distance, @xmath the mass
enclosed within radius @xmath and @xmath the universal gravitational
constant. In the region beyond the galactic disk (which defines the
observed extent of a given galaxy), we expect the enclosed mass to be
constant, and consequently the circular velocity to fall as @xmath .
Measurements started in the late 1930s with Babcock’s observations of
the rotation curve of M31 (Andromeda) out to about 20 kpc from its
center [ 11 ] . Technological advancements over the next few decades
enabled more accurate measurements. In the 1970s, Kent Ford, Vera Rubin
and others observed in galaxies such as M31 and M33 as well as the Milky
Way the approximate flattening of rotation curves at distances extending
well beyond the baryonic disk [ 12 , 13 ] . The implications of these
observations for the missing mass problem were realized soon after [ 14
, 15 ] . Flat rotation curves indicated that the mass contained in a
galaxy continues to increase as @xmath beyond the extent of the visible
matter, in the form of unobserved “dark” matter whose density can be
inferred to roughly scale as @xmath . The left panel of Fig. 1.1 shows
the measured rotation curves for the Milky Way compiled in Ref. [ 16 ]
compared with theoretical expectations from bulge- and disk-like
components (blue and green lines, respectively) inferred from baryonic
matter, as well as an additional dark matter component from a spherical,
isothermal dark matter halo (red line). The rotation curve for the
baryonic-only component (disk + bulge) is shown as the dashed yellow
line, and the total rotation curve including the dark halo is shown as
the solid yellow line. It can clearly be seen that the additional dark
halo component is required to match the observed data at larger radii
@xmath kpc. The descriptions of the individual components shown are
provided in Ref. [ 16 ] .

While astrophysical observations played a significant role historically
in motivating the study of dark matter, modern cosmological data
provides substantial evidence supporting its existence in our Universe.
@xmath CDM, a phenomenological framework often referred to as the
standard model of cosmology, contains dark energy ( @xmath ) and cold
dark matter (CDM) as essential ingredients. It is able to account for a
plethora of cosmological observations, including the existence and
structure of the cosmic microwave background (CMB) radiation,
large-scale distribution of matter, accelerating expansion of the
Universe and relic elemental abundances [ 17 , 18 ] . In particular, the
CMB, which is the imprint of photons that decoupled from the
baryon-photon fluid in the Universe about 370,000 years ago and have
been free-streaming ever since, provides irrefutable evidence for
(non-baryonic) dark matter. The primary relevant observable is the
angular scale of inhomogeneities in the temperature distribution (the
@xmath angular power spectrum) of the CMB. The power spectrum largely
consists of a set of peaks, each indicating an angular scale with a
particularly large contribution to the temperature fluctuations. The
leading physical effect behind these are acoustic oscillations in the
baryon-photon fluid during photon decoupling. Early on, photons and
baryons were electromagnetically coupled, and non-baryonic dark matter
was responsible for generating gravitational potential wells that could
pull in the baryon-photon fluid. The photon pressure acting against
these wells gave rise to a tower of acoustic modes, imprinted in the CMB
as characteristic peaks. While the detailed physics is somewhat nuanced
^(*) ^(*) * See Wayne Hu’s CMB tutorials for an excellent introduction:
http://background.uchicago.edu/index.html . , the relative heights of
these peaks can provide information about the energy content of our
Universe, including the relative composition of baryonic and
non-baryonic (dark) matter. Very heuristically, the position of the
first peak provides information about the curvature of the universe (and
hence how much total “stuff” there is in it), while the second peak
tells us how much of the matter is baryonic (ordinary matter). The third
peak and its relative height can shed insights into the abundance of
non-baryonic dark matter. Historically, the WMAP satellite, while not
able to fully resolve the third peak, was already able to conclusively
say that dark matter makes up the majority of the matter budget in the
Universe, finding the baryon density @xmath and cold dark matter density
@xmath [ 19 ] . Since then, Planck has been able to precisely measure
eight peaks of the @xmath spectrum, finding @xmath and @xmath when
additionally including the CMB @xmath -mode polarization auto- and
cross-spectra ( @xmath and @xmath ). The right panel of Fig. 1.1 shows
the Planck @xmath spectrum [ 20 ] along with the best-fit theoretical
predictions (solid blue line), as well as predictions for a slightly
altered cosmology @xmath and @xmath with a reduced dark matter density
(dashed blue line), where striking differences from the measured
spectrum can be seen.

The above classes of observational evidence or the existence of DM are
by no means exhaustive—many other observations over a large range of
scales support the existence of dark matter, including observations of
the distribution of galaxies on large scales [ 22 ] , weak [ 23 ] and
strong lensing [ 24 , 25 ] of background galaxies by foreground
structure, and observations of merging clusters [ 26 ] .

### 1.2 (Particle) Nature of Dark Matter

Although there exists a great deal of evidence for the existence of dark
matter, its nature largely remains a mystery. These days, it is often
implicitly assumed that when people are talking about detecting dark
matter, say at a Xenon direct detection experiment or in gamma-ray data,
they are referring to a dark matter particle . As touched upon above,
this has by no means always been the case—early usage and references to
dark matter usually referred to the existence of generic dark objects
that would be too faint to be observed, such as dim stars or
internebular material [ 10 ] . The transition in usage was a result of
sociological changes within the particle physics and astrophysics
communities, bringing the two closer after the missing mass problem had
been firmly accepted in the 1970s. All evidence amassed since then is
consistent with dark matter being a fundamental particle, or even the
existence of an entire dark sector consisting of many particles with a
rich set of properties and interactions. It should be noted however that
there exist alternatives to particle dark matter that seek to explain
the dynamical observations suggesting the existence of missing mass in
the Universe. In particular, MOdified Newtonian Dynamics (MOND) [ 27 ,
28 , 29 ] posits an alteration of Newtonian gravitation on larger scales
and is successful in explaining the observed rotation curves as well as
the empirical Tully-Fisher relation between the intrinsic luminosities
and angular velocities of spiral galaxies [ 30 ] . While having some
observational success, MOND and related theories [ 31 ] are (arguably)
less successful at explaining observations on cluster and cosmological
scales. See the reviews in Refs. [ 32 , 33 ] for further details.

Within the Standard Model, neutrinos—by virtue of being stable (or very
long-lived), electrically neutral particles that do not interacting
strongly—contain some of the essential attributes for a particle dark
matter candidate, and were considered a promising DM candidate from
early on. Cosmological effects of neutrinos were explored throughout the
1960s and 1970s, pioneered by the work of Zeldovich and others [ 34 , 35
] , and implications of massive neutrinos for the missing mass observed
on (super-)galactic scales were discussed in the the late 1970s [ 36 ,
37 ] . Early simulations during the 1980s eventually showed that hot
(relativistic) and cold (non-relativistic) particle dark matter would
lead to very different outcomes for structure formation: in the former
case leading to formation and collapse of larger structures (known as
“top-down” structure formation), where in the latter case overdensities
would seed larger structures, leading to hierarchical (known as
“bottom-up”) structure formation. Neutrinos, by virtue of being very
light thermal relics, would be extremely relativistic during structure
formation and, combined with these simulations, early surveys of the
local Universe were able to quickly discount them as dark matter
candidates [ 38 ] . Nevertheless, neutrinos served as a gateway to
understanding how potential new particles could affect observations on
galactic, cluster and cosmological scales.

With no reason to be confined to the Standard Model, people turned to
theories beyond the Standard Model that could explain DM. Supersymmetry
(SUSY) posits that nature may contain a spacetime symmetry relating
bosons and fermions, requiring that for every boson there must exist a
fermion with the same quantum numbers (and vice versa) [ 39 , 40 ] .
This leads to the prediction of several new electrically neutral
particles that are uncharged under the strong force. If some of these
were stable, they could have played an important role in the history of
our Universe and could conceivably make up (some portion of) the dark
matter [ 41 ] . Supersymmetry took its modern form in a paper by
Dimopolous and Georgi, who introduced the Minimal Supersymmetric
Standard Model (MSSM) [ 42 ] . Here, superpartners of the @xmath boson,
photon and two Higgses mix to form four particles, known today as
neutralinos. Neutralinos have arguably been the most-discussed
(particle) dark matter candidate [ 43 ] , in part because
supersymmetry—able to achieve gauge coupling unification and to solve
the electroweak hierarchy problem—is motivated in its own right
independent of the dark matter problem, and the existence of a viable DM
candidate within SUSY is often seen as a desirable bonus.

Outside of SUSY, there is no shortage of viable particle DM candidates,
including but not limited to axions [ 44 , 45 ] , sterile neutrinos [ 46
, 47 ] , light (sub-GeV) dark matter [ 48 , 49 ] and fuzzy dark matter [
50 ] . Such a wealth of possibilities exists in part because the most
general observational constraints on the properties of particle DM are
relatively mild. For example, the mass of the dominant DM component has
only been constrained with @xmath orders of magnitude. In particular,
observations constrain @xmath eV for bosonic dark matter [ 51 ] and
@xmath keV for fermionic dark matter [ 52 ] . This is obtained from
observations of DM halos around dwarf galaxies, imposing the requirement
for particles to occupy a minimum phase-space volume according to the
uncertainty principle for bosons and the Pauli exclusion principle for
fermions. An upper limit of @xmath 10 @xmath GeV comes from searches for
microlensing signatures of MACHOS (Massive Astrophysical Compact Halo
Objects) in our Galaxy [ 53 ] .

#### 1.2.1 Thermal Dark Matter and WIMPs

Assumptions about dark matter’s role in the cosmological history of the
Universe can further impose constraints on its particle properties. A
specific scenario is that of thermal dark matter, where it is assumed
that dark matter particles were in equilibrium with the thermal bath of
matter and radiation in the early Universe. The cooling and expansion of
the Universe reduced its density and consequently suppressed its
interaction rates. DM fell out of chemical equilibrium (a process known
as freeze-out) when the forward process in @xmath (where @xmath is a DM
particle) could no longer be maintained, establishing the DM relic
density. The turning off of the elastic process @xmath , known as
kinetic decoupling, set a scale after which the DM could free-stream
(see [ 54 ] for further details).

There are several general arguments that apply to dark matter particles
in thermal equilibrium with the Standard Model in the early Universe. As
already mentioned in the context of Standard Model neutrinos, thermal
relics that are sufficiently relativistic at decoupling (corresponding
to light particle masses) would strongly suppress structure formation at
small scales [ 54 ] , and the DM mass is accordingly constrained to be
@xmath keV from measurements of the power spectrum in the non-linear
regime [ 55 ] . Unitarity arguments place an upper bound of @xmath TeV
on the mass of a stable particle that was once in thermal equilibrium
with the SM [ 56 ] , although this is model-dependent and assumes that
there are no states heavier than the DM. Additionally, a weak-scale
self-annihilation cross section of @xmath cm @xmath s @xmath and GeV–TeV
particle masses can reproduce the observed DM density through thermal
freeze-out in the early Universe (see Refs. [ 57 , 41 , 1 ] for further
details). This fact holds for a large variety of electroweak-scale DM
candidates, including those naturally arising from SUSY [ 43 , 41 ] ,
and combined with the theoretical arguments for the existence of new
physics at electroweak scales these particles—known as Weakly
Interacting Massive Particles (WIMPs)—have been the dominant particle
dark matter paradigm over the last three decades and have motivated an
extensive search program.

Searches for WIMPs are generally organized into three categories
depending on the experimental detection paradigm. Direct detection
experiments look for the energy deposited when dark matter particles
recoil against nuclei through the process @xmath , where @xmath is a DM
particle. While the flux of WIMPs through a terrestrial detector can be
large, the expected deposited energies and interaction rates would be
very small, requiring large amounts of target material and exquisite
control over backgrounds [ 1 ] . Direct detection experiments have been
able to set very strong limits on WIMP scenarios [ 58 , 59 ] and have
been able to exclude several attractive baseline models [ 60 ] . The
second class of searches involves production of WIMPs at particle
colliders like the Large Hadron Collider (LHC) through the process
@xmath , usually in association with additional visible particles
emitted by initial or intermediate SM particles that can used to detect
the event along with the missing energy characterizing the WIMP.
Dedicated collider searches can also target specific scenarios, such as
neutralino production [ 61 ] . See Ref. [ 62 ] for a recent review of
collider searches for dark matter.

The final strategy and the focus of this thesis is indirect detection,
which looks for the annihilation of DM particles into SM particles
through the process @xmath by looking for its signature in astrophysical
data. The nature of the SM particles depends on the specific DM model
and interaction properties considered. The basic idea behind indirect
detection is that annihilation processes will be taking place at higher
rates in regions of the Universe that have more dark matter, leading to
an excess in production of SM particles from those regions. These would
then cascade onto photons, electrons, positrons, (anti)protons and
neutrinos, some of which could eventually reach us and be detected with
appropriate telescopes.

It is worth noting that the WIMP scenario, while well-motivated, relies
on several assumptions that can easily be relaxed [ 63 ] . The
possibility of the DM relic density set by annihilations into heavier
states (“Forbidden” DM) [ 64 , 63 ] or @xmath annihilations of Strongly
Interacting Massive Particles (SIMPs) [ 65 , 66 ] are representative
examples where relatively small modifications to the WIMP paradigm can
lead to very different ranges of allowed masses and cross sections. See
Refs. [ 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 ,
80 ] for further examples of such scenarios.

### 1.3 Indirect Detection of Annihilating Dark Matter

As noted above, for thermal WIMP scenarios where the DM can
self-annihilate, the late-time DM abundance is set by the coupling of
the DM particle to the Standard Model. In this case the DM would have an
electroweak-scale cross section around @xmath cm @xmath s @xmath and a
particle mass of @xmath GeV–TeV). When DM particles in this mass range
annihilate to SM particles, the resulting photons fall dominantly in the
gamma-ray energy range. This regime is well-probed by gamma-ray
telescopes, including the Fermi Large Area Telescope ( Fermi -LAT) [ 81
] , data from which will be used in the analyses presented in this
thesis. Terrestrial gamma-ray observatories such as HAWC [ 82 ] ,
H.E.S.S. [ 83 ] , MAGIC [ 84 ] , VERITAS [ 85 ] and the upcoming CTA [
86 ] can typically achieve better sensitivity at higher photon energies
(and correspondingly higher DM masses @xmath GeV) due to their much
larger effective area. In certain cases ( e.g. leptonic final states),
experiments like AMS-02 can be sensitive probes of DM annihilation via
observations of charged cosmic ray spectra. See Ref. [ 3 ] for a
comprehensive recent review of indirect dark matter searches.

#### 1.3.1 Tools for Indirect Detection

A major challenge for indirect detection searches is to calculate the
expected dark matter annihilation flux from a given astrophysical target
or source population. The basic prescription for doing so is as follows.
If we denote the DM (particle) number density at coordinate @xmath
(parameterized by the angle away from the Galactic plane @xmath and
line-of-sight distance from us @xmath ) by @xmath and the
velocity-averaged self-annihilation cross section by @xmath , then the
annihilation rate per particle is given by

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath is the DM density and @xmath its particle mass. The
annihilation rate in a volume element @xmath is given by multiplying
this quantity by the number of particles in the volume:

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

The factor of @xmath in the denominator is to avoid double counting
since two particles are involved in the annihilation process. The
observed annihilation flux (in units of photons cm @xmath s @xmath ) is
obtained by inserting the area factor @xmath and integrating over the
desired volume:

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

where the photon energy spectrum @xmath gives the number of photons
produced per annihilation for a given 2-body final state, and can be
obtained with parton shower tools like Pythia8 [ 87 ] or from tabulated
values for certain specific cases [ 88 ] . While there are many
possibilities for the annihilation final states, the resulting spectra
can be broadly classed into a few categories: (i) Annihilation directly
to photons, which would show up as a spectral line and allow for bump
hunts. However, since DM is not expected to be electrically charged,
such interactions would generically be loop-suppressed. (ii)
Annihilation to gauge bosons or quarks and their subsequent
hadronization, which would produce pions that would dominantly decay to
photons. This would result in a broad continuum photon spectrum. (iii)
Annihilation to electrons and muons, which would produce photons through
final-state radiation and/or radiative decays. This would result in a
narrower spectrum and suppressed rate compared to (ii) . Annihilation to
taus, which have both hadronic and leptonic decays, would result in a
spectrum intermediate to (ii) and (iii) . As a benchmark and for
comparison purposes, limits in the literature are often presented for
annihilation into @xmath -quarks ( @xmath ).

The annihilation cross section can be taken out of the integral, and the
annihilation flux factorizes as

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

where @xmath encapsulates the particle physics assumptions, and @xmath
is the so-called @xmath -factor, which captures the astrophysical
dependence of the flux. Objects with higher @xmath -factors over some
localized region typically make for more interesting indirect detection
targets. However, a high @xmath -factor by itself does not guarantee a
good annihilation target, since the figure of merit is the
signal-to-noise ratio. This must additionally be balanced with how well
the systematic uncertainties on the potential signal, astrophysical
backgrounds and Galactic foregrounds can be accounted for and
controlled.

#### 1.3.2 Sources of Gamma Rays from Annihilating Dark Matter

An important ingredient in indirect detection is the accurately
characterization of the DM signal and its associated uncertainties. This
often involves input from astrophysics, observations at other
wavelengths and @xmath -body simulations. Given the typically sizable
systematic uncertainties in both signal and background modeling, it is
crucial to have the ability to probe the same DM parameter space using
multiple complementary targets and search strategies. The following
sources have been and continue to be used as gamma-ray targets in
annihilation searches:

-   Milky Way dwarf galaxies : Dwarf spheroidal satellite galaxies
    (dSphs) of the Milky Way are expected to be dark matter dominated
    and thus to have relatively low expected astrophysical backgrounds.
    As such, dSphs have traditionally been considered excellent targets
    for DM annihilation searches. There have been about 45 dSphs
    candidates discovered recently by surveys like optical SDSS and DES
    (see Ref. [ 89 ] and references therein), and searches for gamma-ray
    emission from these have been able to place strong constraints on
    annihilation scenarios, excluding thermal WIMPs at masses below
    @xmath GeV at 95% confidence level for the case of annihilation into
    the @xmath final state [ 89 , 90 ] . However, the relevant @xmath
    -factors are far from well-characterized—assumptions about e.g. ,
    the dSph halo shape [ 91 , 92 ] and stellar membership criteria used
    to infer the halo properties [ 93 , 94 ] can lead to significant
    uncertainties on the predicted annihilation signal and the
    corresponding annihilation limit. Figure 1.2 (top right) shows a map
    of the inferred @xmath -factors of dSphs considered in Ref. [ 89 ] .
    As in that study, the dSphs are assumed to be point-like since the
    shape of the corresponding DM halos is not very well constrained.

-   The Milky Way halo : Because of its proximity to us, the DM halo
    surrounding our own Galaxy is the brightest source of DM emission in
    the sky. Figure 1.2 (top left) shows the expected annihilation
    @xmath -factor for the smooth component of the Milky Way halo (see
    caption for further details).

    Searches in the inner Galaxy ( @xmath ), where the signal is
    expected to be the brightest, have yielded an excess emission whose
    spatial and spectral properties can be consistent with those of a DM
    annihilation signal ( e.g. , a @xmath 40 GeV WIMP annihilating to
    @xmath with an approximately thermal cross section), often called
    the Galactic Center Excess [ 95 , 96 , 97 , 98 , 99 , 100 , 101 ] .
    This region of the sky is however plagued by the presence of
    substantial and difficult-to-characterize Galactic foregrounds,
    which complicates the interpretation of any signal and/or constraint
    from it. In addition, recent results based on analyzing the
    statistics of photons in the region [ 102 , 103 ] (see also Ch. 2 )
    indicate that the excess is more consistent with emission from an
    unresolved population of point sources rather than a dark matter
    signal, which is expected to be more diffuse in nature. There is
    also some evidence that the morphology of the excess emission
    preferentially traces the stellar overdensity in the Galactic bulge
    [ 104 , 105 , 106 ] , suggesting association with an underlying
    stellar population.

    Another class of searches focus on looking for DM emission from the
    Milky Way halo over larger regions of the sky at higher Galactic
    latitudes ( @xmath ), where the signal is still appreciable but
    Galactic foregrounds are much lower. These studies necessitate being
    able to accurately characterize the Galactic foreground emission
    over larger regions of the sky, and a careful consideration of
    potential foreground mismodeling effects yields stringent limits,
    excluding thermal WIMPs at masses below @xmath GeV at 95% confidence
    level for the case of annihilation into @xmath [ 107 ] .

-   Galactic substructure : By definition, hierarchical bottom-up
    structure formation implies the existence of substructure
    (“subhalos”) within galactic DM halos, and these have the potential
    to be attractive DM annihilation targets. Unlike the dwarf galaxies
    mentioned above, low-mass subhalos with virial mass @xmath M @xmath
    would be mostly dark and have highly suppressed stellar activity [
    108 , 109 ] . This makes it difficult to localize them and look for
    their gamma-ray emission. Figure 1.2 (bottom left) shows a simulated
    realization of @xmath -factors for Galactic substructure (subhalos)
    following the prescription in [ 110 ] (see caption for further
    details).

    Traditional searches rely on assuming that the emission from
    unassociated gamma-ray sources detected by Fermi is coming from DM
    annihilation in individual subhalos, and comparing this to
    expectations from @xmath -body simulations [ 111 , 112 , 113 , 114 ]
    . The bright source in the top right corner of the substructure map
    in Fig. 1.2 , for example, would likely show up as a resolved
    unassociated source in Fermi point source catalogs such as 3FGL [
    115 ] .

    An orthogonal approach is to study the statistics of photons coming
    from DM annihilation within dim subhalos. While these subhalos may
    not be detectable individually, their collective emission could be
    detected statistically as a heightened level of “clumpiness” in the
    photon map. Statistical methods described in Chs. 2 and 3 of this
    thesis can be applied to search for such signals structure in
    gamma-ray data, and this approach is currently a topic of ongoing
    study.

-   Extragalactic galaxies and clusters : Searches for DM annihilation
    in extragalactic targets have traditionally been complicated by the
    difficulty in characterizing the DM properties of extragalactic
    halos and the presence of potentially significant astrophysical
    emission. Searches for emission from individual, nearby clusters [
    116 ] ; the integrated, isotropic emission from background halos [
    117 , 118 , 119 , 120 ] ; and cross-correlation between gamma ray
    emission and catalogs of galaxies or large-scale structure [ 121 ,
    122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 ] have yielded
    constraints on DM annihilation properties. These searches typically
    do not attain sensitivity to thermal WIMPs for realistic
    astrophysical assumptions. Chapter 4 of this thesis focuses on
    developing methods to systematically characterize the dark matter
    emission and associated uncertainties from a large number of nearby
    extragalactic galaxies and clusters [ 131 ] . Figure 1.2 (bottom
    right) shows the extragalactic @xmath -factor map derived using this
    prescription and the group catalogs from Refs. [ 132 ] and [ 133 ] .
    Chapter 5 presents a search for gamma-ray emission using this map,
    which results in stringent limits on annihilating DM and excludes
    thermal WIMPs at masses below @xmath GeV at 95% confidence level for
    the case of annihilation into @xmath [ 134 ] .

#### 1.3.3 Template Methods for Gamma-Ray Searches

Data from gamma-ray detectors such as Fermi -LAT is typically a series
of sky maps, representing the number of photons binned spatially as well
as in energy. Figure 1.3 shows a subset of a typical Fermi -LAT dataset.
In analyzing such data within the context of dark matter indirect
detection, the challenge lies in have contributions from large-scale
structures such as the smooth Galactic halo as well as point/extended
sources like dwarf galaxies, from various astrophysical backgrounds. The
most common technique for characterizing the various potential sources
that contribute to gamma-ray data is Poissonian template fitting, which
is briefly described here; a detailed description will be given in Ch. 4
.

A template is a spatial map which traces the modeled contribution of a
particular source or class of sources to the data, e.g. the expected
emission from the diffuse Galactic foreground or resolved astrophysical
point sources. Figure 1.4 shows some templates commonly used in Fermi
gamma-ray analyses (see caption for descriptions). Templates for DM
emission can be constructed as described in Secs. 1.3.1 and 1.3.2 .

Within a single energy bin, if we denote the value of a given template
@xmath in pixel @xmath by @xmath , then the total expected counts in
pixel @xmath is given by

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

where @xmath represents the signal and background model parameters
@xmath , which in this case are the normalizations of the corresponding
templates. The observed data in pixel @xmath should therefore be a
Poisson realization of the sum of modeled components. It follows that
the likelihood function for the parameters @xmath given the data @xmath
is a product over all pixels in the region-of-interest of the Poisson
probabilities associated with observing @xmath counts in each pixel
@xmath :

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

With the likelihood in hand, we can quantify the contribution of various
components using conventional inference methods, e.g. obtaining
posterior distributions within a Bayesian framework or building up a
likelihood surfaces using frequentist profile likelihood techniques. The
latter is more commonly used in DM searches—typically, we are more
interested in the parameters associated with the DM model ( e.g. its
particle mass @xmath and annihilation cross section @xmath , which are
in 1-to-1 correspondence with the normalization of the DM template) than
those corresponding to the astrophysical backgrounds. A likelihood
surface @xmath for the signal parameters corresponding to a given DM
model @xmath can be obtained by maximizing the likelihood with respect
to the background parameters at each signal parameter point. This can be
generalized to the cases of analyzing several energy bins and/or
stacking multiple sources ( e.g. several extragalactic halos), where the
total likelihood would be given by the product of the individual
likelihoods.

For inferring dark matter properties, a log-likelihood difference test
statistic (TS) can be defined for a given mass @xmath as

  -- -------- -------- -------- -------
     @xmath   @xmath            (1.7)
                       @xmath   
  -- -------- -------- -------- -------

where @xmath corresponds to the null signal hypothesis. Wilks’ theorem
guarantees that in the asymptotic limit of a large sample size, the TS
is @xmath -distributed, allowing us to discover (if we’re lucky) or
exclude a DM signal in the data to a desired statistical significance in
accordance with @xmath statistics. A TS value of @xmath , for example,
corresponds to exclusion at a confidence level of 95%. Modified versions
of this statistical procedure will be used in Chs. 4 and 5 to look for
DM annihilation in extragalactic galaxies and clusters.

A fundamental limitation of Poissonian template fitting is that while
resolved point sources can be either modeled with templates or masked,
this is not possible for dim, sub-threshold point sources that cannot be
detected individually. Depending on their spatial distribution, emission
from these unresolved point sources is typically absorbed by other
extended templates, e.g. isotropic (in the case of extragalactic
sources) or Galactic dark matter (in the case of an approximately
spherically symmetric population of unresolved sources in the Galactic
center). Chapter 2 will be dedicated to extending traditional Poissonian
template fitting methods to statistically account for the presence of
unresolved point sources in the data.

### 1.4 Thesis Organization

The rest of this thesis is organized as follows. Chapter 2 describes the
implementation of a novel statistical method, first introduced in Ref. [
102 ] , which leverages the “clumpiness” of photons associated with
populations of unresolved point sources (PSs) in astronomical datasets
to derive their contribution and properties. In Ch. 3 , this method is
applied to the gamma-ray sky at higher latitudes as seen by Fermi to
characterize the contribution of PSs to the extragalactic gamma-ray sky
over three order of magnitude in energy, from 2 to 2000 GeV. Chapter 4
poses the question “what is the best way to look for annihilating dark
matter in extragalactic sources?” and attempts to answer it by
constructing a pipeline to robustly map out the distribution of dark
matter outside the Milky Way using galaxy group catalogs. Uncertainties
involved in inferring various dark matter parameters are discussed in
detail. In Ch. 5 , this framework is then applied to Fermi data and
existing group catalogs to search for annihilating dark matter in
extragalactic galaxies and clusters.

[]

## Chapter 2 Non-Poissonian Template Fitting: Fundamentals and Code

This chapter is based on an edited version of NPTFit: A code package for
Non-Poissonian Template Fitting , Astron.J. 153 (2017) no.6, 253
[arXiv:1612.03173] with Nicholas Rodd and Benjamin Safdi [ 142 ] .

### 2.1 Introduction \lettrine

[lines=3]Astrophysical point sources (PSs), which are defined as sources
with angular extent smaller than the resolution of the detector, play an
important role in virtually every analysis utilizing images of the
cosmos. It is useful to distinguish between resolved and unresolved PSs;
the former may be detected individually at high significance, while
members of the latter population are by definition too dim to be
detected individually. However, unresolved PSs—due to their potentially
large number density—can be a leading and sometimes pesky source of flux
across wavelengths. Recently, a novel analysis technique called the
non-Poissonian template fit (NPTF) has been developed for characterizing
populations of unresolved PSs at fluxes below the detection threshold
for finding individually-significant sources [ 143 , 102 ] . The
technique expands upon the traditional fluctuation analysis technique
(see, for example, [ 144 , 145 ] ), which analyzes the aggregate
photon-count statistics of a data set to characterize the contribution
from unresolved PSs, by additionally incorporating spatial information
both for the distribution of unresolved PSs and for the potential
sources of non-PS emission. In this work, we present a code package
called NPTFit for numerically implementing the NPTF in python and cython
.

The most up-to-date version of the open-source package NPTFit may be
found at

https://github.com/bsafdi/NPTFit

and the latest documentation at

http://nptfit.readthedocs.io .

The NPTF generalizes traditional astrophysical template fits. Template
fitting is useful for pixelated data sets consisting of some number of
photon counts @xmath in each pixel @xmath , and it typically proceeds as
follows. Given a set of model parameters @xmath , the mean number of
predicted photon counts @xmath in the pixel @xmath may be computed. More
specifically, @xmath , where @xmath is an index of the set of templates
@xmath , whose normalizations and spatial morphologies may depend on the
parameters @xmath . These templates may, for example, trace the
gas-distribution or other extended structures that are expected to
produce photon counts. Then, the probability to detect @xmath photons in
the pixel @xmath is simply given by the Poisson distribution with mean
@xmath . By taking a product of the probabilities over all pixels, it is
straightforward to write down a likelihood function as a function of
@xmath .

The NPTF modifies this procedure by allowing for non-Poissonian
photon-count statistics in the individual pixels. That is, unresolved PS
populations are allowed to be distributed according to spatial
templates, but in the presence of unresolved PSs the photon-count
statistics in individual pixels, as parameterized by @xmath , no longer
follow Poisson distributions. This is heuristically because we now have
to ask two questions in each pixel: first, what is the probability,
given the model parameters @xmath that now also characterize the
intrinsic source-count distribution of the PS population, that there are
PSs within the pixel @xmath , then second, given that PS population,
what is the probability to observe @xmath photons?

It is important to distinguish between resolved and unresolved PSs. Once
a PS is resolved—that is once its location and flux is known—that PS may
be accounted for by its own Poissonian template. Unresolved PSs are
different because their locations and fluxes are not known. When we
characterize unresolved PSs with the NPTF, we characterize the entire
population of unresolved sources, following a given spatial
distribution, based on how that population modifies the photon-count
statistics.

The NPTF has played an important role recently in addressing various
problems in gamma-ray astroparticle physics with data collected by the
Fermi -LAT gamma-ray telescope. ^(*) ^(*) * http://fermi.gsfc.nasa.gov/
The NPTF was developed to address the excess of gamma rays observed by
Fermi at @xmath GeV energies originating from the inner regions of the
Milky Way [ 96 , 97 , 146 , 147 , 148 , 149 , 99 , 150 , 95 , 98 , 151 ,
100 , 106 , 152 ] . The GeV excess, as it is commonly referred to, has
received a significant amount of attention due to the possibility that
the excess emission arises from dark matter (DM) annihilation. However,
it is well known that unresolved PSs may complicate searches for
annihilating DM in the Inner Galaxy region due to, for example, the
expected population of dim pulsars [ 150 , 153 , 154 , 155 , 156 , 157 ,
158 , 159 , 160 ] . In [ 102 ] (see also [ 161 ] ) it was shown, using
the NPTF, that indeed the photon-count statistics of the data prefer a
PS over a smooth DM interpretation of the GeV excess. The same
conclusion was also reached by [ 103 ] using an unrelated method that
analyzes the statistics of peaks in the wavelet transformation of the
Fermi data.

In the case of the GeV excess, there are multiple PS populations that
may contribute to the observed gamma-ray flux and complicate the search
for DM annihilation. These include isotropically distributed PSs of
extragalactic origin, PSs distributed along the disk of the Milky Way
such as supernova remnants and pulsars, and a potential spherical
population of PSs such as millisecond pulsars. Additionally, there are
various identified PSs that contribute significantly to the flux as well
as a variety of smooth emission mechanisms such as gas-correlated
emission from pion decay and bremsstrahlung. The power of the NPTF is
that these different source classes may be given separate degrees of
freedom and constrained by incorporating the spatial morphology of their
various contributions along with the difference in photon-count
statistics between smooth emission and emission from unresolved PSs.
Although the origin of the GeV excess is still not completely settled,
as even if the excess arises from PSs as the NPTF suggests the source
class of the PSs remains a mystery at present, the NPTF has emerged as a
powerful tool for analyzing populations of dim PSs in complicated data
sets with characteristic spatial morphology.

The NPTF and related techniques utilizing photon-count statistics have
also been used recently to study the contribution of various source
classes to the extragalactic gamma-ray background (EGB) [ 145 , 162 ,
163 , 164 , 165 ] . ^(†) ^(†) † The complementary analysis strategy of
probabilistic catalogues has also been applied to this problem [ 166 ] .
In these works it was shown that unresolved blazars would predominantly
show up as PS populations under the NPTF, while other source classes
such as star-forming galaxies would show up predominantly as smooth
emission. For example, in [ 165 ] (described in Ch. 3 ) it was shown
using the NPTF that blazars likely account for the majority of the EGB
from @xmath 2 GeV to @xmath 2 TeV. These results set strong constraints
on the flux from more diffuse sources, such as star-forming galaxies,
which has significant implications for, among other problems, the
interpretation of the high-energy astrophysical neutrinos observed by
IceCube [ 167 , 168 , 169 , 170 ] (see, for example, [ 171 , 172 ] ).
This is because certain sources that contribute gamma-ray flux at Fermi
energies, such as star forming galaxies and various types of active
galactic nuclei, may also contribute neutrino flux observable by
IceCube.

Another promising application of the NPTF is to searches of annihilating
dark matter from a population of subhalos in our Galaxy. Annihilation
emission from Milky Way subhalos would be characterized by three
distinctive features: their spatial distribution, energy spectrum, and
non-Poissonian photon-count distribution. These three features taken
together can be used to effectively distinguish subhalos from more
standard extragalactic sources. This approach is quite different from
traditional subhalo searches that look for resolved subhalo candidates
in the Fermi point-source catalog [ 112 , 113 , 114 ] . When the
spectrum of an isolated source resembles DM, it is difficult to confirm
the exotic nature of the emission [ 173 , 111 ] . The NPTF-based
proposal relies on looking for a population of subhalos, rather than
isolated objects, and is therefore less sensitive to the variations
between individual sources.

The NPTF originates from the older fluctuation analysis technique, which
is sometimes referred to as the @xmath analysis. This technique has been
used extensively to study the flux of unresolved X-ray sources [ 174 ,
175 , 176 , 177 , 144 ] . In these early works, the photon-count
probability distribution function (PDF) was computed numerically for
different PS source-count distributions using Monte Carlo (MC)
techniques. The fluctuation analysis was first applied to gamma-ray data
in [ 145 ] , ^(‡) ^(‡) ‡ The fluctuation analysis has more recently been
applied to both gamma-ray [ 178 ] and neutrino [ 179 ] datasets. and in
that work the authors developed a semi-analytic technique utilizing
probability generating functions for calculating the photon-count PDF.
The code package presented in this work uses this formalism for
efficiently calculating the photon-count PDF. The specific form of the
likelihood function for the NPTF, while reviewed in this work, was first
presented in [ 102 ] . The works [ 102 , 161 , 165 ] utilized an early
version of to perform their numerical analyses.

The code package has a python interface, though the likelihood
evaluation is efficiently implemented in cython [ 180 ] . The
user-friendly interface allows for an arbitrary number of PS and smooth
templates. The PS templates are characterized by pixel-dependent
source-count distributions @xmath , where @xmath is the spatial template
tracking the distribution of point sources on the sky and @xmath is the
pixel-independent source-count distribution. The distribution @xmath
quantifies the number of sources @xmath that contributes flux between
@xmath and @xmath in the pixel @xmath . The @xmath are parameterized as
multiply broken power-laws, with an arbitrary number of breaks. The code
is able to account for both an arbitrary exposure map (accounting for
the pointing strategy of an instrument) as well as an arbitrary point
spread function (PSF, accounting for the instrument’s finite angular
resolution) in translating between flux @xmath (in units of photons cm
@xmath s @xmath ) and photon counts @xmath .

NPTFit has a built-in interface with MultiNest [ 181 , 182 ] , which
efficiently implements nested sampling of the posterior distribution and
Bayesian evidence for the user-specified model, given the specified data
and instrument response function, in the Bayesian framework [ 183 , 184
, 185 ] . The interface handles the Message Passing Interface (MPI), so
that inference may be performed efficiently using parallel computing. A
basic analysis package is provided in order to facilitate easy
extraction of the most relevant data from the posterior distribution and
quick plotting of the MultiNest output. The preferred format of the data
for NPTFit is HEALPix [ 186 ] (a nested equal-area pixelation scheme of
the sky), although the the code is also able to handle non- HEALPix data
arrays. Note that the code package may also be used to simply extract
the NPTF likelihood function so that NPTFit may be interfaced with any
numerical package for Bayesian or frequentist inference.

A large set of example Jupyter [ 187 ] notebooks and python files are
provided to illustrate the code. The examples utilize 413 weeks of
processed Fermi Pass 8 data in the UltracleanVeto event class collected
between August 4, 2008 and July 7, 2016 in the energy range from 2 to 20
GeV. We restrict this dataset to the top quartile as graded by PSF
reconstruction in order to reduce cosmic-ray contamination and further
apply the standard quality cuts DATA_QUAL==1 && LAT_CONFIG==1 , as well
as restricting the zenith angle to be less than @xmath . This data is
made available in the code release. Moreover, the example notebooks
illustrate many of the main results in [ 102 , 161 , 165 ] .

In addition to the above, the base NPTFit code makes use of the python
packages corner [ 188 ] , matplotlib [ 189 ] , mpmath [ 190 ] , GSL [
191 ] and numpy [ 192 ] .

The rest of this chapter is organized as follows. Section 2.2 outlines
in more detail the framework of the NPTF. Sections 2.3 and 2.4 describe
further details behind the mathematical framework of the NPTF. Section
2.5 highlights the key classes and features in the NPTFit code package
and usage instructions. In Sec. 2.6 we present an example of how to
perform an NPTF scan using NPTFit , looking at the Galactic Center with
Fermi data to reproduce aspects of the main results of [ 102 ] . We
conclude in Sec. 2.7 .

### 2.2 The Non-Poissonian Template Fit

In this section we review the NPTF, which was first presented in [ 102 ]
and described in more detail in [ 161 , 165 ] (see also [ 145 , 143 ,
162 , 164 ] and Ch. 3 ). The NPTF is used to fit a model @xmath with
parameters @xmath to a data set @xmath consisting of counts ( i.e. ,
number of photon) @xmath in each pixel @xmath . The likelihood function
for the NPTF is then simply

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath gives the probability of drawing @xmath counts in the given
pixel @xmath , as a function of the parameters @xmath . The main
computational challenge, of course, is in computing these probabilities.

It is useful to divide the model parameters into two different
categories: the first category describes smooth templates, while the
second category describes PS templates. We describe each category in
turn, starting with the smooth templates.

For most applications, the data has the interpretation of being a
two-dimensional pixelated map consisting of an integer number of counts
in each pixel. The smooth templates may be used to predict the mean
number of counts @xmath in each pixel @xmath :

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

Above, @xmath is an index over templates and @xmath denotes the mean
contribution of the @xmath template to pixel @xmath for parameters
@xmath . In principle, @xmath may describe both the spatial morphology
as well as the normalization of the templates. However, in the current
implementation of the code, the Poissonian model parameters simply
characterize the overall normalization of the templates: @xmath . Here,
@xmath is the normalization parameter and @xmath is the @xmath template,
which takes values over all pixels @xmath and is independent of the
model parameters. The superscript @xmath implies that the template is a
counts templates, which is to be contrasted with a flux template, for
which we use the symbol @xmath . The two are related by the exposure map
of the instrument @xmath : @xmath . In the case where we only have
smooth, Poissonian templates, the probabilities are then given by the
Poisson distribution:

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

In the presence of unresolved PS templates, the probabilities @xmath are
no longer Poissonian functions of the model parameters @xmath . Each PS
template is characterized by a pixel-dependent source-count distribution
@xmath , which describes the differential number of sources per pixel
per unit flux interval. In this work, we model the source-count
distribution by a multiply broken power-law:

  -- -- -- -------
           (2.4)
  -- -- -- -------

Above, we have parameterized the source-count distribution with an
arbitrary number of breaks @xmath , denoted by @xmath with @xmath , and
@xmath indices @xmath with @xmath . The spatial dependence of the
source-count distribution is accounted for by the overall factor @xmath
, where @xmath is the pixel-independent normalization, which is a
function of the model parameters, and @xmath is a template describing
the spatial distribution of the PSs. More precisely, the number of
sources @xmath (and the total PS flux @xmath ) in pixel @xmath , for a
fixed set of model parameters @xmath , follows the template @xmath . On
the other hand, the locations of the flux breaks and the indices are
taken to be fixed between pixels. ^(§) ^(§) § In principle, the breaks
and indices could also vary between pixels. However, in the current
version of NPTFit , only the number of sources (and, accordingly, the
total flux) is allowed to vary between pixels.

To summarize, a PS template described by a broken power-law with @xmath
breaks has @xmath model parameters describing the locations of the
breaks, the power-law indices, and the overall normalization. For
example, if we take a single break then the PS model parameters may be
denoted as @xmath . Additionally, a spatial template @xmath must be
specified, which describes the distribution of the number of sources
(and total flux) with pixel @xmath .

Notice that when we discussed the Poissonian templates we used the
counts templates @xmath and talked directly in terms of counts @xmath ,
while so far in our discussion of the unresolved PS templates we have
used the point source distribution template @xmath and written the
source-count distribution @xmath in terms of flux @xmath . Of course as
the total flux from a distribution of point sources is also proportional
to the template @xmath , it can be thought of as a flux template,
however conceptually it is being used to track the distribution of the
sources rather than the flux they produce. For this reason we have
chosen to distinguish the two. Moreover, in the presence of a
non-trivial PSF, @xmath should also be smoothed by the PSF to account
for the instrument response function. That is, @xmath is a template for
the observed counts taking into account the details of the instrument,
while @xmath ( @xmath ) is a map of the physical point sources (flux),
which is independent of the instrument. In photon-counting applications,
the exposure map @xmath often has units of @xmath and flux has units of
@xmath .

For the unresolved PS templates, we also need to convert the
source-count distribution from flux to counts. This is done by a simple
change of variables:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

which implies that for a non-Poissonian template the spatial dependence
of @xmath is given by @xmath . This inverse exposure scaling may seem
surprising, but it is straightforward to confirm that the mean number of
counts in a given pixel, @xmath , is given by @xmath , as expected, up
to pixel independent factors.

As an important aside, the template @xmath used by the Poissonian models
needs to be smoothed by the PSF. Incorporating the PSF into the
unresolved PS models, on the other hand, is more complicated and is not
accomplished simply by smoothing the spatial template. Indeed, @xmath
should remain un-smoothed by the PSF when used for non-Poissonian scans.
Accounting for PSF effects in the non-Poissonian likelihood will be
described in detail in Sec. 2.3.2 .

In the remainder of this section we briefly overview the mathematic
framework behind the computation of the @xmath with NPTFit ; however,
details of the algorithms used to calculate these probabilities in
practice, along with more in-depth explanations, are given in Secs. 2.3
and 2.4 . We use the probability generating function formalism,
following [ 145 ] , to calculate the probabilities. For a discrete
probability distribution @xmath , with @xmath , the generating function
is defined as:

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

from which we can recover the probabilities:

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

The key feature of generating functions exploited here is that the
generating function of a sum of two independent random variables is
simply the product of the individual generating functions.

The probability generating function for the smooth templates, as a
function of @xmath , is simply given by

  -- -- -- -------
           (2.8)
  -- -- -- -------

The probability generating function for an unresolved PS template, on
the other hand, takes a more complicated form (derived in Sec. 2.3 ):

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

where

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

Above, @xmath is a function that takes into account the PSF, which we
describe in more detail in Sec. 2.3 . In the presence of a non-trivial
PSF, the flux from a single source is smeared among pixels. The
distribution of flux fractions among pixels is described by the function
@xmath , where @xmath is the flux fraction. By definition @xmath equals
the number of pixels which, on average, contain between @xmath and
@xmath of the flux from a PS; the distribution is normalized such that
@xmath . If the PSF is a @xmath -function, then @xmath .

Putting aside the PSF correction for the moment, the @xmath have the
interpretation of being the average number of @xmath -count PSs within
the pixel @xmath , given the distribution @xmath . The generating
function for @xmath @xmath -count sources is simply @xmath (see [ 145 ]
or Sec. 2.3 ), which then leads directly to ( 2.9 ). The PSF correction,
through the distribution @xmath , incorporates the fact that PSs only
contribute some fraction of their flux within a given pixel.

### 2.3 Mathematical Foundations of NPTFit

In this section we present the mathematical foundation of the NPTF and
the evaluation of the non-Poissonian likelihood in more detail that what
was shown in Sec. 2.2 . Note that many of the details presented in this
section have appeared in the earlier works of [ 145 , 143 , 102 ] ,
however we have reproduced these here in order to have a single clear
picture of the method.

The remainder of this section is divided as follows. Firstly we outline
how to determine the generating functions for the Poissonian and
non-Poissonian case. We then describe how we account for finite PSF
corrections.

#### 2.3.1 The (non-)Poissonian Generating Function

There are two reasons why the evaluation of the Poissonian likelihood
for traditional template fitting can be evaluated rapidly. The first of
these is that the functional form of the Poissonian likelihood is
simple. Secondly, and more importantly, is the fact that if we have two
discrete random variables @xmath and @xmath that follow Poisson
distributions with means @xmath and @xmath , then the random variable
@xmath again follows a Poisson distribution with mean @xmath . This
generalizes to combining an arbitrary number of random Poisson
distributed variables and is why we were able to write @xmath in Sec.
2.2 . This fact is not true when combining arbitrary random variables,
and in particular if we add in a template following non-Poissonian
statistics.

An elegant solution to this problem was introduced in [ 145 ] , using
the method of generating functions. As we are always dealing with
pixelized maps containing discrete counts (of photons or otherwise), for
any model of interest there will always be a discrete probability
distribution @xmath , the probability of observing @xmath counts. In
terms of these, we then define the probability generating function as
in ( 2.6 ). The property of probability generating functions that make
them so useful in the present context is as follows. Consider two random
processes @xmath and @xmath , with generating functions @xmath and
@xmath , that follow arbitrary and potentially different statistical
distributions. Then the generating function of @xmath is simply given by
the product @xmath . In this subsection we will derive the appropriate
form of @xmath for Poissonian and non-Poissonian statistics.

To begin with, consider the purely Poissonian case. Here and throughout
this section we consider only the likelihood in a single pixel; the
likelihood over a full map is obtained from the product of the
pixel-based likelihoods. Then for a Poisson distribution with an
expected number of counts @xmath in a pixel @xmath :

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

Note that the variation of the @xmath across the full map will be a
function of the model parameters, such that @xmath . In order to
simplify the notation in this section however, we leave the @xmath
dependence implicit. Given the @xmath values, we then have:

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.12)
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --------

From this form, it is clear that if we have two Poisson distributions
with means @xmath and @xmath , the product of their generating functions
will again describe a Poisson distribution, but with mean @xmath .

Next we work towards the generating function in the non-Poissonian case.
At the outset, we let @xmath denote the average number of sources in a
pixel @xmath that emit exactly @xmath counts. In terms of this, the
probability of finding @xmath @xmath -count sources in this pixel is
just a draw from a Poisson distribution with mean @xmath , i.e.

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

Given this, the probability to find @xmath counts from a population of
@xmath -count sources is

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

We can then use this to derive the non-Poissonian @xmath -count
generating function as follows:

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.15)
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --------

However this is just the generating function for @xmath -count sources,
to get the full non-Poissonian generating function we need to multiply
this over all values of @xmath . Doing so we arrive at

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.16)
                       @xmath   
  -- -------- -------- -------- --------

justifying the form given in Sec. 2.2 . Again recall for the full
likelihood we can just multiply the pixel based likelihoods and that
@xmath .

So far we have said nothing of how to determine @xmath , the average
number of @xmath -count source in pixel @xmath . This value depends on
the source-count distribution @xmath , which specifies the distribution
of sources as a function of their expected number of counts, @xmath . Of
course the physical object is @xmath , where @xmath is the flux. This
distinction was discussed in Sec. 2.2 , and can be implemented in NPTFit
to arbitrary precision. Nevertheless @xmath does not fully determine
@xmath —we need to account for the fact that a source that is expected
to give @xmath photons could Poisson fluctuate to give @xmath . As such
any source can in principle contribute to @xmath , and so integrating
over the full distribution we arrive at:

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

An important part of implementing the NPTF in a rapid manner, which is a
central feature of NPTFit , is the analytic evaluation of the integral
in this equation. In order to do this, we need to have a specific form
of the source-count distribution. For this purpose, we allow the source
count distribution to be a multiply broken power-law and evaluate the
integral for any number of breaks.

Putting the evaluation of the integral aside for the moment then, we
have arrived at the full non-Poissonian generating function:

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.18)
              @xmath   @xmath   
  -- -------- -------- -------- --------

Contrasting this with Eq. ( 2.12 ), we see that whilst the Poissonian
likelihood is specified by a single number @xmath , the non-Poissonian
likelihood is instead specified by a distribution @xmath .

In the case of multiple PS templates, we should multiply the independent
probability generating functions. However, this is equivalent to summing
the @xmath parameters. This is how multiple PS templates are
incorporated into the NPTFit code:

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

where the sum over @xmath is over the contributions from individual PS
templates.

#### 2.3.2 Correcting For a Finite Point Spread Function

The next factor to account for is the fact that in any realistic dataset
there will be a non-zero PSF. Here, we closely follow the discussion in
[ 145 ] . The PSF arises due to the inability of an instrument to
perfectly reconstruct the original direction of the photon, neutrino, or
quantity making up the counts. In practice, a finite PSF means that a
source in one pixel can contribute counts to nearby pixels as well. To
implement this correction, we modify the calculation of @xmath given in
Eq. ( 2.18 ), which accounts for the distribution of sources as a
function of @xmath and the fact that each one could Poisson fluctuate to
give us @xmath counts. The finite PSF means that in addition to this, we
also need to draw from the distribution @xmath , that determines the
probability that a given source contributes a fraction of its flux
@xmath in a given pixel. Once we know @xmath , this modifies our
calculation of @xmath in Eq. ( 2.18 )—now a source that is expected to
contribute @xmath counts, will instead contribute @xmath , where @xmath
is drawn from @xmath . As such we arrive at the result in ( 2.10 ).

In NPTFit we determine @xmath using Monte Carlo. To do this we place a
number of PSs appropriately smeared by the PSF at random positions on a
pixelized sphere. Then integrating over all pixels we can determine the
fraction of the flux in each pixel @xmath , @xmath , defined such that
@xmath . Note in practice one can truncate this sum at some minimal
value of @xmath without impacting the argument below. From the set
@xmath , we then denote by @xmath the number of fractions for @xmath
point sources that fall within some range @xmath . From these
quantities, we may determine @xmath as

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

which is normalized such that @xmath . From this definition we see that
the case of a vanishing PSF is just @xmath - i.e. the flux is always
completely in the pixel with the PS.

### 2.4 NPTFit: Algorithms

The generating-function formalism for calculating the probabilities
@xmath is described at the end of Sec. 2.2 and in more detail in Sec.
2.3 . In particular—given the generating function @xmath —we are
instructed to calculate the probabilities by taking @xmath derivatives
as in ( 2.7 ). However, taking derivatives is numerically costly, and so
instead we have developed recursive algorithms for computing these
probabilities. In the same spirit, we analytically evaluate the @xmath
parameters defined in ( 2.10 ) for the multiply-broken source-count
distribution in order to facilitate a fast evaluation of the NPTF
likelihood function. In this section, we overview these methods that are
essential to making NPTFit a practical software package.

In general we may write the full single pixel generating function for a
model containing an arbitrary number of Poissonian and non-Poissonian
templates as:

  -- -------- -- --------
     @xmath      (2.21)
  -- -------- -- --------

where we have defined

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

Above, @xmath represents the average number of @xmath -count source in
pixel @xmath . The remaining task is to efficiently calculate the
probabilities @xmath , which are formally defined in terms of
derivatives through ( 2.7 ). Nevertheless, derivatives are slow to
implement numerically, so we instead use a recursion relation to
determine @xmath in terms of @xmath .

To begin with, note that

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

For the rest of this discussion, we suppress the pixel index @xmath ,
though one should keep in mind that this process must be performed
independently in every pixel. From ( 2.23 ), we can immediately write
down

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.24)
              @xmath   @xmath   
  -- -------- -------- -------- --------

Given @xmath and @xmath , we may write our recursion relation for @xmath
as

  -- -------- -- --------
     @xmath      (2.25)
  -- -------- -- --------

which as mentioned requires the knowledge of all @xmath . To derive (
2.25 ), we first define

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

Then, for example,

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

From here to determine @xmath we simply need @xmath more derivatives.
Using the generalized Leibniz rule, we have

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.28)
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --------

Then setting @xmath and recalling the definition of @xmath , this yields

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.29)
                       @xmath   
  -- -------- -------- -------- --------

as claimed.

To calculate the @xmath in a pixel @xmath , we need to calculate the
@xmath and the sum @xmath . We may calculate these expressions
analytically using the general source-count distribution in ( 2.4 ). To
calculate the sums, we make use of the relation

  -- -------- -------- -------- --------
     @xmath   @xmath            (2.30)
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --------

Finiteness of the total flux, and also the probabilities, requires
@xmath and @xmath . However, both the integral and @xmath , appearing in
the last line above, may be divergent individually if @xmath . In this
case, we analytically continue in @xmath , evaluate the contributions
individually, and then sum the two expressions to get a result that is
finite across the whole range of allowable parameter space.

### 2.5 NPTFit: Orientation

NPTFit implements the NPTF, as described above, in python . In this
section we give a brief orientation to the code package and its main
classes. A more thorough description of the code and its uses is
available in the online documentation .

#### class NPTFit.nptfit.Nptf

This is the main class used to set up and perform non-Poissonian and
Poissonian template scans. It is initialized by

⬇

nptf = NPTF ( tag = ’Untagged’ , work_dir = None )

with keywords

  ---------- ------------ ------------------ ------
  Argument   Default      Purpose            type
  tag        ’Untagged’   Label of scan      str
  work_dir   None         Output directory   str
  ---------- ------------ ------------------ ------

.

If no work_dir is specified, the code will default to the current
directory. This is the directory where all output is stored. Specifying
a tag will create an additional folder, with that name, within the
work_dir for the output.

The data, exposure map, and templates are loaded into the nptfit . NPTF
instance after initialization (see the example in Sec. 2.6 ). The data
and exposure map are loaded by

⬇

nptf . load_data ( data , exposure )

Here, data and exposure are 1-D numpy arrays. The recommended format for
these arrays is the HEALPix format, so that all pixels are equal area,
although the code is able to handle arbitrary data and exposure arrays
so long as they are of the same length. The templates are added by

⬇

nptf . add_template ( template , key ,

units = ’counts’ )

Here, template is a 1-D numpy array of the same length as the data and
exposure map, key is a string that will be used to refer to the template
later on, and units specifies whether the template is a counts template
(keyword ’counts’ ) or a flux template (keyword ’flux’ ) in units @xmath
. The default, if unspecified, is units = ’counts’ . The template should
be pre-smoothed by the PSF if it is going to be used for a Poissonian
model. If the template is going to be used for a non-Poissonian model,
either choice for units is acceptable, though in the case of ’counts’
the template should simply be the product of the exposure map times the
flux template and not smoothed by the PSF.

The user also has the option of loading in a mask that reduces the
region of interest (ROI) to a subset of the pixels in the data,
exposure, and template arrays. This is done through the command

⬇

nptf . load_mask ( mask )

where mask is a boolean numpy array of the same length as the data and
exposure arrays. Pixels in mask should be either True or False ; by
convention, pixels that are True will be masked, while those that are
False will not be masked. Note if performing an analysis with
non-Poissonian templates, regions where the exposure map is identically
zero should be explicitly masked.

Afterwards, Poissonian and non-Poissonian models may be added to the
instance using the available templates. An arbitrary number of
Poissonian and non-Poissonian models may be added to the scan. Moreover,
each non-Poissonian model may be specified in terms of a multiply broken
power law with a user-specified number of breaks, as in ( 2.4 ).

Poissonian models are added sequentially using the syntax

⬇

nptf . add_poiss_model ( template_name , model_tag , prior_range =[],
log_prior = False , fixed = False , fixed_norm =1.0)

where the keywords are

  --------------- --------- ----------------------- -------------------
  Argument        Default   Purpose                 type
  template_name   -         key of template         str
  model_tag       -         LaTeX-ready label       str
  prior_range     []        Prior [min, max ]       [ float , float ]
  log_prior       False     Log/linear-flat prior   bool
  fixed           False     Is template fixed       bool
  fixed_norm      1.0       Norm if fixed           float
  --------------- --------- ----------------------- -------------------

Any of the model parameters may be fixed to a user specified value
instead of floated in the scan. For those parameters that are floated in
the scan, a prior range needs to be specified along with whether or not
the prior is flat or log-flat. Note that if log_prior = True , then the
prior range is set with respect to @xmath of the linear prior range.
^(¶) ^(¶) ¶ More complicated priors will be incorporated in future
releases of NPTFit . For example, if we want to scan the normalization
of a template over the range from @xmath with a log-flat prior, then we
would set log_prior = True and prior_range = [-1,1] . In this case, it
might make sense to label the model with model_tag = ’$\log_{10}A$’ to
emphasize that the actual model parameter is the log of the
normalization; this label will appear in various plots made using the
provided analysis class for visualizing the posterior.

The non-Poissonian models are added with a similar syntax:

⬇

nptf . add_non_poiss_model ( template_name , model_tag , prior_range
=[], log_prior = False , dnds_model = ’specify_breaks’ , fixed_params =
None , units = ’counts’ )

The template_name keyword is the same as for the Poissonian models. The
rest of the keywords are

  -------------- ------------------ ------------------------------------- --------------------------
  Argument       Default            Purpose                               type
  model_tag      -                  LaTeX-ready label                     [ str , str , ...]
  prior_range    []                 Prior [[min, max], …]                 [[ float , float ], ...]
  log_prior      [ False ]          Log/linear-flat prior                 [ bool , bool , ...]
  dnds_model     ’specify_breaks’   How to specify multiple breaks        str
  fixed_params   None               Fix certain parameters                [[ int , float ], ...]
  units          ’counts’           ’flux’ or ’counts’ units for breaks   str
  -------------- ------------------ ------------------------------------- --------------------------

The syntax for adding non-Poissonian models is that the model parameters
are specified by @xmath for a broken power-law with @xmath breaks. As
such, the model_tag , prior_range , and log_prior are now arrays where
each entry refers to the respective model parameter. The code
automatically determines the number of breaks by the length of the
model_tag array. The arrays prior_range and log_prior should only
include entries for model parameters that will be floated in the scan.
Any model parameter may be fixed using the fixed_params array, with the
syntax such that fixed_params = [[ i , c_i ],[ j , c_j ]] would fix the
@xmath model parameter to @xmath and the @xmath to @xmath , where the
parameter indexing starts from 0.

The units keyword determines whether the priors for the breaks in the
source-count distribution (and also the fixed parameters, if any are
given) will be specified in terms of ’flux’ or ’counts’ . The relation
between flux and counts varies between pixels if the exposure map is
non-trivial. For this reason, it is more appropriate to think of the
breaks in the source-count distribution in terms of flux. The keyword
’counts’ still specifies the breaks in the source-count distribution in
terms of flux, with the relation between counts and flux given through
the mean of the exposure map @xmath : @xmath .

The dnds_model keyword has the options ’specify_breaks’ and
’specify_relative_breaks’ . If ’specify_breaks’ is chosen, which is the
default, then the breaks are the model parameters. If instead
’specify_relative_breaks’ is chosen, the full set of model parameters is
given by @xmath . Here, @xmath is the highest break and the lower breaks
are determined by @xmath . Note that the prior ranges for the @xmath ’s
should be between @xmath and @xmath (for linear flat), since @xmath .

After setting up a scan, the configuration is finished by executing the
command

⬇

nptf . configure_for_scan ( f_ary =[1.0], df_rho_div_f_ary =[1.0], nexp
=1)

For a purely Poissonian scan, none of the keywords above need to be
specified. For non-Poissonian scans, f_ary and df_rho_div_f_ary
incorporate the PSF correction. In particular, f_ary is a discretized
list of @xmath values between @xmath and @xmath , while df_rho_div_f_ary
is a discretized list of @xmath at those @xmath values. A class is
provided for computing these lists; it is described later in this
section. If no keywords are given for these two arrays they default to
the case of a @xmath -function PSF.

The keyword nexp , which defaults to @xmath , is related to the exposure
correction in the calculation of the source-count distribution @xmath
from @xmath . In many applications, it is computationally too expensive
to perform the mapping in ( 2.5 ) in each pixel. The overall
pixel-dependent normalization factor @xmath factorizes from many of the
internal computations, and as a result this contribution to the exposure
correction is performed in every pixel. However, it is useful to perform
the mapping from flux to counts, which should be performed uniquely in
each pixel @xmath , using the mean exposure within small sub-regions.
Within a given sub-region, we map flux to counts using @xmath , where
the mean is taken over all pixels in the sub-region. The number of
sub-regions is given by nexp , and all sub-regions have approximately
the same area. As nexp approaches the number of pixels, the
approximation becomes exact; however, for many applications the
approximation converges for a relatively small number of exposure
regions. We recommend verifying, in any application, that results are
stable as nexp is increased.

After configuring the NPTF instance, the log-likelihood may be
extracted, as a function of the model parameters, in addition to the
prior range. The log-likelihood and prior range may then be used with
any external package for performing Bayesian or frequentist inference.
This is particularly useful if the user would like to combine likelihood
functions between different energy bins or otherwise add to the default
likelihood function, for example, incorporating nuisance parameters
beyond those associated with individual templates. The package MultiNest
, however, is already incorporated into the NPTF class and may be run
immediately after configuring the NPTF instance. This is done simply by
executing the command

⬇

nptf . perform_scan ( run_tag = None , nlive =500)

where nlive is an integer that specifies the number of live points used
in the sampling of the posterior distribution. MultiNest recommends an
nlive @xmath 500-1000, though the parameter defaults to @xmath if
unspecified for quick test runs. Additional MultiNest arguments may be
passed as a dictionary through the optional pymultinest_options keyword
(see the online documentation for more details). The optional keyword
run_tag is used to create a sub-folder for the MultiNest output with
that name.

After a scan has been run (or if a scan has been run previously and
saved), the results may be loaded through the command

⬇

nptf . load_scan ( run_tag = None )

The MultiNest chains, which give a discretized view of the posterior
distribution, may then be accessed through, for example, nptf . samples
. An instance of the PyMultiNest analyzer class may be accessed through
nptf . a . A small analysis package, described later in this section, is
also provided for performing a few common analyses.

#### class NPTFit.psf_correction.PSFCorrection

This is the class used to construct the arrays f_ary and
df_rho_div_f_ary for the PSF correction. An instance of PSFCorrection is
initialized through

⬇

pc_inst = PSFCorrection . PSFCorrection ( psf_dir = None , num_f_bins
=10, n_psf =50000, n_pts_per_psf =1000, f_trunc =0.01, nside =128,
psf_sigma_deg = None , delay_compute = False )

with keywords

  --------------- --------- ----------------------------------------------------------- -------
  Argument        Default   Purpose                                                     type
  psf_dir         None      Where PSF arrays are stored                                 str
  num_f_bins      10        Number of linear-spaced points in f_ary                     int
  n_psf           50000     Number of MC simulations for determining df_rho_div_f_ary   int
  n_pts_per_psf   1000      Number of points drawn for each MC simulation               int
  f_trunc         0.01      Minimum @xmath value                                        float
  nside           128       HEALPix parameter for size of map                           int
  psf_sigma_deg   None      Standard deviation @xmath of 2-D Gaussian PSF               float
  delay_compute   False     If True , PSF not Gaussian and will be specified later      bool
  --------------- --------- ----------------------------------------------------------- -------

Note that the arrays f_ary and df_rho_div_f_ary depend both on the PSF
of the detector as well as the pixelation of the data; at present the
PSFCorrection class requires the pixelation to be in the HEALPix
pixelation.

The keyword psf_dir points to the directory where the f_ary and
df_rho_div_f_ary will be stored; if unspecified, they will be stored to
the current directory. The f_ary consists of num_f_bins entries linear
spaced between @xmath and @xmath . The PSF correction involves placing
many ( n_psf ) PSFs at random positions on the HEALPix map, drawing
n_pts_per_psf points from each PSF, and then looking at the distribution
of points among pixels. The larger n_psf and n_pts_per_psf , the more
accurate the computation of df_rho_div_f_ary will be. However, the
computation time of the PSF arrays also increases as these parameters
are increased.

By default the PSFCorrection class assumes that the PSF is a 2-D
Gaussian distribution:

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

Here, @xmath describes the spread of arriving counts with angular
distance @xmath away from the arrival direction. The parameter
psf_sigma_deg denotes @xmath in degrees. Upon initializing PSFCorrection
with psf_sigma_deg specified, the class automatically computes the array
df_rho_div_f_ary and stores it in the psf_dir with a unique name related
to the keywords. If such a file already exists in the psf_dir , then the
code will simply load this file instead of recomputing it. After
initialization, the relevant arrays may be accessed by pc_inst . f_ary
and pc_inst . df_rho_div_f_ary .

The PSFCorrection class can also handle arbitrary PSF functions. In this
case, the class should be initialized with delay_compute = True . Then,
the user should manually set the function pc_inst . psf_r_func to the
desired function @xmath . This function will be discretized with pc_inst
. psf_samples points out to pc_inst . sample_psf_max degrees from @xmath
. These two quantities also need to be manually specified. The user also
needs to set pc_inst . psf_tag to a string that will be used for saving
the PSF arrays. After these four attributes have been set manually by
the user, the PSF arrays are computed and stored by executing pc_inst .
make_or_load_psf_corr () .

#### def NPTFit.create_mask.make_mask_total

This function is used to make masks that can then be used to reduce the
data and templates to a smaller ROI when performing the scan. While
these masks can always be made by hand, this function provides a simple
masking interface for maps in the HEALPix format. The make_mask_total
function can mask pixels by latitude, longitude, and radius from any
point on the sphere. See the online documentation for more specific
examples.

#### class NPTFit.dnds_analysis.Analysis

The analysis class may be used to extract useful information from the
results of an NPTF performed using MultiNest . The class also has
built-in plotting features for making many of the most common types of
visualizations for the parameter posterior distribution. An instance of
the analysis class can be instantiated by

⬇

an = Analysis ( nptf , mask = None , pixarea =0.)

where nptf is itself an instance of the NPTF class that already has the
results of a scan loaded. The keyword arguments mask and pixarea are
optional. The user should specify a mask if the desired ROI for the
analysis is different that that used in the scan. The user should
specify a pixarea if the data is not in the HEALPix format. The code
will still assume the pixels are equal area with area pixarea , which
should be specified in sr.

After initialization, the intensities of Poissonian and non-Poissonian
templates, respectively, may be extracted from the analysis class by the
commands

⬇

an . return_intensity_arrays_poiss ( comp )

and

⬇

an . return_intensity_arrays_non_poiss (

comp )

Here, comp refers to the template key used by the Poissonian or
non-Poissonian model. The arrays returned give the mean intensities of
that model in the ROI in units of @xmath , assuming the exposure map was
in units of cm @xmath s. The arrays computed over the full set of
entries in the discretized posterior distribution output by MultiNest .
Thus, these intensity arrays may be interpreted as the 1-D posteriors
for the intensities. For additional keywords that may be used to
customize the computation of the intensity arrays, see the online
documentation .

The source-count distributions may also be accessed from the analysis
class. Executing

⬇

an . return_dndf_arrays ( comp , flux )

will return the discretized 1-D posterior distribution for @xmath at
flux @xmath for the PS model with template key comp . Note that the mean
is computed over pixels @xmath in the ROI.

The 1-D posterior distributions for the individual model parameters may
be accessed by

⬇

A_poiss_post = an . return_poiss_parameter_posteriors (

comp )

for Poissonian models, and

⬇

A_non_poiss_post , n_non_poiss_post , Sb_non_poiss_post = an .
return_non_poiss_parameter_posteriors ( comp )

for non-Poissonian models. Here A_poiss_post is a 1-D array of the
discretized posterior distribution for the Poissonian template
normalization parameter. Similarly, A_non_poiss_post is the posterior
array for the non-Poissonian normalization parameter. The arrays
n_non_poiss_post and Sb_non_poiss_post are 2-D, where—for example—
n_non_poiss_post = [ n_1_array , n_2_array , ...] and n_1_array is a 1-D
array for the posterior for @xmath .

Another useful piece of information that may be extracted from the scan
is the Bayesian evidence:

⬇

l_be , l_be_err = an . get_log_evidence ()

returns the log of the Bayesian evidence along with the uncertainty on
this estimate based on the resolution of the MCMC.

For information on the plotting capabilities in the analysis class, see
the online documentation or the example in the following section.

### 2.6 NPTFit: An Example

In this section we give an example for how to perform an NPTF using
NPTFit . Many more examples are available in the online documentation .
This particular example reproduces aspects of the main results of [ 102
] , which found evidence for a spherical population of unresolved
gamma-ray PSs around the Galactic Center. The example uses the
processed, public Fermi data made available with the release of the
NPTFit package. The data set consists of 413 weeks of Fermi Pass 8 data
in the UltracleanVeto event class (top quartile of events as ranked by
PSF) from 2 to 20 GeV. The map is binned in HEALPix with @xmath . The
data, along with the exposure map and background templates, may be
downloaded from

http://hdl.handle.net/1721.1/105492 .

In the example we will perform an NPTF on the sub-region where we mask
the Galactic plane at latitude @xmath and mask pixels with angular
distance greater than @xmath from the Galactic Center. We also mask
identified PSs in the 3FGL PS catalog [ 115 ] at 95% containment using
the provided PS mask, which is added to the geometric mask. We include
smooth templates for diffuse gamma-ray emission in the Milky Way (using
the Fermi p6v11 diffuse model), isotropic emission (which can also
absorb instrumental backgrounds), and emission following the Fermi
bubbles, which are taken to be uniform in flux following the spatial
template in [ 140 ] . We also include a dark matter template, which
traces the line of sight integral of the square of a canonical NFW
density profile.

We additionally include point source (non-Poissonian) models for the DM
template, as well as for a disk template which corresponds to a doubly
exponential thin-disk source distribution with scale height 0.3 kpc and
radius 5 kpc. The source-count distributions for these are parameterized
by singly-broken power laws, each described by four parameters @xmath .

#### 2.6.1 Setting Up the Scan

We begin the example by loading in the relevant modules, described in
the previous section, that we will need to setup, perform, and analyze
the scan.

⬇

import numpy as np

# module for performing scan

from NPTFit import nptfit

# module for creating the mask

from NPTFit import create_mask as cm

# module for determining the PSF correction

from NPTFit import psf_correction as pc

# module for analyzing the output

from NPTFit import dnds_analysis

Next, we create an instance of the NPTF class, which is used to
configure and perform a scan.

⬇

n = nptfit . NPTF ( tag = ’GCE_Example’ )

We assume here that the supplementary Fermi data has been downloaded to
a directory ’fermi_data’ . Then, we may load in the data and exposure
maps by

⬇

fermi_data = np . load ( ’fermi_data/fermidata_counts.npy’ ). astype (
int )

fermi_exposure = np . load ( ’fermi_data/fermidata_exposure.npy’ )

n . load_data ( fermi_data , fermi_exposure )

Importantly, note that the exposure map has units of cm @xmath s. Next,
we use the create_mask class to generate our ROI mask, which consists of
both the geometric mask and the PS mask loaded in from the ’fermi_data’
directory:

⬇

pscmask = np . array ( np . load ( ’fermi_data/fermidata_pscmask.npy’ ),
dtype = bool )

mask = cm . make_mask_total ( band_mask = True , band_mask_range = 2,
mask_ring = True , inner = 0, outer = 30, custom_mask = pscmask )

n . load_mask ( mask )

The templates may also be loaded in from this directory,

⬇

dif = np . load ( ’fermi_data/template_dif.npy’ )

iso = np . load ( ’fermi_data/template_iso.npy’ )

bub = np . load ( ’fermi_data/template_bub.npy’ )

gce = np . load ( ’fermi_data/template_gce.npy’ )

dsk = np . load ( ’fermi_data/template_dsk.npy’ )

These templates are counts map (i.e. flux maps times the exposure map)
that have been pre-smoothed by the PSF (except for the disk-correlated
template labeled dsk ). We then add them to our NPTF instance with
appropriately chosen keywords:

⬇

n . add_template ( dif , ’dif’ )

n . add_template ( iso , ’iso’ )

n . add_template ( bub , ’bub’ )

n . add_template ( gce , ’gce’ )

n . add_template ( dsk , ’dsk’ )

# remove the exposure correction for PS templates

rescale = fermi_exposure / np . mean ( fermi_exposure )

n . add_template ( gce / rescale , ’gce_np’ , units = ’PS’ )

n . add_template ( dsk / rescale , ’dsk_np’ , units = ’PS’ )

Note that templates ’gce_np’ and ’dsk_np’ intended to be used in
non-Poissonian models should trace the underlying PS distribution,
without exposure correction, and are added with the keyword units = ’PS’
.

#### 2.6.2 Adding Models

Now that we have loaded in all of the external data and templates, we
can add models to our NPTF instance. First, we add in the Poissonian
models,

⬇

n . add_poiss_model ( ’dif’ , ’$A_\mathrm{dif}$’ , False , fixed = True
, fixed_norm =14.88)

n . add_poiss_model ( ’iso’ , ’$A_\mathrm{iso}$’ , [0,2], False )

n . add_poiss_model ( ’gce’ , ’$A_\mathrm{gce}$’ , [0,2], False )

n . add_poiss_model ( ’bub’ , ’$A_\mathrm{bub}$’ , [0,2], False )

All Poissonian models are taken to have linear priors, with prior ranges
for the normalizations between 0 and 2. However, the normalization of
the diffuse background has been fixed to the value @xmath , which is
approximately the correct normalization in these units for this
template, in order to provide an example of this syntax. Next, we add in
the two non-Poissonian models:

⬇

n . add_non_poiss_model ( ’gce_np’ , [ ’$A_\mathrm{gce}^\mathrm{ps}$’ ,
’$n_1^\mathrm{gce}$’ , ’$n_2^\mathrm{gce}$’ , ’$S_b^{(1),
\mathrm{gce}}$’ ], [[-6,1],[2.05,30],[-2,1.95],[0.05,40]], [ True ,
False , False , False ])

n . add_non_poiss_model ( ’dsk_np’ , [ ’$A_\mathrm{dsk}^\mathrm{ps}$’ ,
’$n_1^\mathrm{dsk}$’ , ’$n_2^\mathrm{dsk}$’ , ’$S_b^{(1),
\mathrm{dsk}}$’ ], [[-6,1],[2.05,30],[-2,1.95],[0.05,40]], [ True ,
False , False , False ])

We have added in the models for disk-correlated and NFW-correlated (line
of sight integral of the the NFW distribution squared) unresolved PS
templates. Each of these models takes singly-broken power-law
source-count distributions. In this configuration, the normalization
parameters are taken to have a log-flat prior while the indices and
breaks are taken to have linear priors (relevant for the Bayesian
posterior sampling). The units of the breaks are specified in terms of
counts.

#### 2.6.3 Configure Scan with PSF Correction

In this energy range and with this data set, the PSF may be modeled by a
2-D Gaussian distribution with @xmath . From this, we are able to
construct the PSF-correction arrays: ^(∥) ^(∥) ∥ For an example of how
to construct these arrays with a more complicated, non-Gaussian PSF
function, see the online documentation .

⬇

pc_inst = pc . PSFCorrection ( psf_sigma_deg =0.1812)

f_ary , df_rho_div_f_ary = pc_inst . f_ary , pc_inst . df_rho_div_f_ary

These arrays are then passed into the NPTF instance when we configure
the scan:

⬇

n . configure_for_scan ( f_ary , df_rho_div_f_ary , nexp =1)

Note that since our ROI is relatively small and the exposure map does
not change significantly over the region, we have a single exposure
region with nexp =1 .

#### 2.6.4 Performing the Scan With MultiNest

We perform the scan using MultiNest with nlive =500 as an example to
demonstrate the basic features and conclusions of this analysis while
being able to perform the scan in a reasonable amount of time on a
single processor, although ideally nlive should be set to a higher value
for more reliable results:

⬇

n . perform_scan ( nlive =500)

#### 2.6.5 Analyzing the Results

Now, we are ready to analyze the results of the scan. First we load in
relevant modules:

⬇

import corner

import matplotlib . pyplot as plt

and then we load in the results of the scan (configured as above),

⬇

n . load_scan ()

The chains, giving a discretized view of the posterior distribution, may
be accessed simply through the attribute n . samples . However, we will
analyze the results by using the analysis class provided with NPTFit .
We make an instance of this class simply by

⬇

an = dnds_analysis . Analysis ( n )

##### Making Corner Plots

Corner (or triangle) plots are a simple and quick way of visualizing
correlations in the posterior distribution. Such plots may be generated
through the command

⬇

an . make_triangle ()

which leads to the plot in Fig. 2.1 .

##### Plotting Source-count Distributions

The source-count distributions for NFW- and disk-correlated point source
models may be plotted with

⬇

an . plot_source_count_median ( ’dsk’ , smin =0.01, smax =1000, nsteps
=1000, color = ’cornflowerblue’ , spow =2, label = ’Disk’ )

an . plot_source_count_band ( ’dsk’ , smin =0.01, smax =1000, nsteps
=1000, qs =[0.16,0.5,0.84], color = ’cornflowerblue’ , alpha =0.3, spow
=2)

an . plot_source_count_median ( ’gce’ , smin =0.01, smax =1000, nsteps
=1000, color = ’forestgreen’ , spow =2, label = ’GCE’ )

an . plot_source_count_band ( ’gce’ , smin =0.01, smax =1000, nsteps
=1000, qs =[0.16,0.5,0.84], color = ’forestgreen’ , alpha =0.3, spow =2)

along with the following matplotlib plotting options.

⬇

plt . yscale ( ’log’ )

plt . xscale ( ’log’ )

plt . xlim ([5 e -11,5 e -9])

plt . ylim ([2 e -13,1 e -10])

plt . tick_params ( axis = ’x’ , length =5, width =2, labelsize =18)

plt . tick_params ( axis = ’y’ , length =5, width =2, labelsize =18)

plt . ylabel ( ’$F^2 dN/dF$ [counts/cm$^2$/s/deg$^2$]’ , fontsize =18)

plt . xlabel ( ’$F$ [counts/cm$^2$/s]’ , fontsize =18)

plt . title ( ’Galactic Center NPTF’ , y =1.02)

plt . legend ( fancybox = True )

plt . tight_layout ()

This is shown in Fig. 2.2 . Contribution from both NFW- and
disk-correlated PSs may be seen, with NFW-correlated sources
contributing dominantly at lower flux values. In that figure, we also
show a histogram of the detected 3FGL sources within the relevant energy
range and region, with vertical error bars indicating the 68% confidence
interval from Poisson counting uncertainties only. ^(**) ^(**) ** The
data for plotting these points is available in the online documentation
. Since we have explicitly masked all 3FGL sources, we see that the
disk- and NFW-correlated PS templates contribute at fluxes near and
below the 3FGL PS detection threshold, which is @xmath @xmath counts cm
@xmath s @xmath in this case.

##### Plotting Intensity Fractions

The intensity fractions for the smooth and PS NFW-correlated models may
be plotted with

⬇

an . plot_intensity_fraction_non_poiss ( ’gce’ , bins =800, color =
’cornflowerblue’ , label = ’GCE PS’ )

an . plot_intensity_fraction_poiss ( ’gce’ , bins =800, color =
’lightsalmon’ , label = ’GCE DM’ )

plt . xlabel ( ’Flux fraction (%)’ )

plt . legend ( fancybox = True )

plt . xlim (0,6)

This is shown in Fig. 2.3 . We immediately see a preference for
NFW-correlated point sources over the smooth NFW component.

##### Further Analyses

The example above may easily be pushed further in many directions, many
of which are outlined in [ 102 ] . For example, a natural method for
performing model comparison in the Bayesian framework is to compute the
Bayes factor between two models. Here, for example, we may compute the
Bayes factor between the model with and without NFW-correlated PSs. This
involves repeating the scan described above but only adding in
disk-correlated PSs. Then, by comparing the global Bayesian evidence
between the two scans (see Sec. 2.5 for the syntax on how to extract the
Bayesian evidence), we find a Bayes factor @xmath @xmath in preference
for the model with spherical PSs.

Another straightforward generalization of the example described above is
simply to leave out the PS mask, so that the NFW- and disk-correlated PS
templates must account for both the resolved and unresolved PSs. The
likelihood evaluations take longer, in this case, since there are pixels
with higher photon counts compared to the 3FGL-masked scan. The result
for the source-count distribution from this analysis is shown in Fig.
2.4 . In this case, the disk-correlated PS template accounts for the
resolved 3FGL sources, while the NFW-correlated PS template contributes
at roughly the same flux range as in the 3FGL masked case. The Bayes
factor in preference for the model with NFW-correlated PSs over that
without—as described above—is found to be @xmath @xmath in this case.

### 2.7 Conclusion

We have presented an open-source code package for performing
non-Poissonian template fits. We strongly recommend referring to the
online documentation —which will be kept up-to-date—in addition to this
chapter accompanying the initial release. There are many way in which
NPTFit can be improved in the future. For one, the NPTFit package only
handles a single energy bin at a time. In a later version of the code we
plan to incorporate the ability to scan over multiple energy bins
simultaneously. Additionally, there are a few areas—such as the
evaluation of the incomplete gamma functions—where the cython code may
still be sped up. Such improvements to the computational cost are
relevant for analyses of large data sets with many model parameters. Of
course, we welcome additional suggestions for how we may improve the
code and better adapt it to applications beyond the gamma-ray
applications it has been used for so far.

[]

## Chapter 3 Application of Non-Poissonian Template Fitting to the
Extragalactic Gamma-Ray Background

This chapter is based on an edited version of Deciphering Contributions
to the Extragalactic Gamma-Ray Background from 2 GeV to 2 TeV ,
Astrophys.J. 832 (2016) no.2, 117 [arXiv:1606.04101] with Mariangela
Lisanti, Lina Necib and Benjamin Safdi [ 165 ] . The results of this
chapter have been presented at the following conferences and workshops:
Gamma Rays and Dark Matter in Obergurgl, Austria (December 2015), TeV
Particle Astrophysics (TeVPA) 2016 in Geneva, Switzerland (September
2016) and APS April Meeting 2017 in Washington, DC (January 2017).

### 3.1 Introduction \lettrine

[lines=3]The Extragalactic Gamma-Ray Background (EGB) is the nearly
isotropic all-sky emission that arises from sources outside of the Milky
Way. The OSO-3 [ 193 , 194 ] and SAS-2 satellites [ 195 , 196 ] were the
first to see hints of the EGB and have since been followed by EGRET [
197 , 198 ] and, most recently, the Fermi Large Area Telescope ^(*) ^(*)
* http://fermi.gsfc.nasa.gov/ [ 199 , 163 ] . The origin of the EGB
remains an open question. The dominant contributions are likely due to
blazars [ 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 , 208 , 209 ,
210 , 211 , 212 , 213 , 118 , 214 ] , star-forming galaxies (SFGs) [ 215
, 216 , 217 , 218 , 219 , 220 , 221 , 222 , 223 , 224 ] , and misaligned
active galactic nuclei (mAGN) [ 225 , 226 , 227 , 228 , 229 ] .
Understanding the relative contributions of these source components to
the EGB has taken on a new sense of importance in light of IceCube’s
observation of ultra-high-energy extragalactic neutrinos [ 167 , 168 ,
169 , 170 ] , the origin of which still remains a mystery. For instance,
the same sources that dominate the extragalactic neutrino background at
@xmath PeV energies may also contribute significantly to the EGB from
@xmath GeV–TeV energies [ 230 , 224 , 229 ] . In addition, the EGB may
harbor the imprints of more exotic physics such as dark matter
annihilation or decay [ 231 , 232 , 233 , 234 , 43 , 235 , 118 , 120 ,
117 ] , as well as contributions from truly diffuse processes such as
propagating ultra-high-energy cosmic rays [ 236 , 237 , 238 , 239 , 240
] and structure formation shocks in clusters of galaxies [ 241 , 242 ] .
Given the potential wealth of information that can be extracted from the
EGB, deciphering its constituents remains a high priority.

Most recently, Fermi presented a measurement of the EGB intensity from
100 MeV to 820 GeV [ 199 ] . The total EGB intensity is the sum of all
resolved point sources (PSs) and smooth isotropic emission . The smooth
emission, referred to as the Isotropic Gamma-Ray Background (IGRB),
arises from PSs that are too faint to be resolved individually as well
as other truly diffuse processes. It is also important to note that both
the EGB and IGRB may be contaminated by cosmic rays that are
mis-identified as gamma rays; this emission is expected to be smoothly
distributed across the sky. Of the known gamma-ray emitting PSs at high
latitudes, which are captured by Fermi’s 3FGL [ 115 ] catalog from
0.1–300 GeV and the more recent 2FHL [ 214 ] catalog from 50–2000 GeV,
the dominant source class is blazars.

In this chapter, we use the analysis method Non-Poissonian Template
Fitting (NPTF) introduced in the previous chapter, to study the source
populations that contribute to the EGB in a data-driven manner. The
method relies on photon-count statistics to illuminate the aggregate
properties of a source population, even when its constituents are not
individually resolvable [ 145 , 143 , 102 ] . This allows us to
constrain the contribution of PSs to the EGB whose flux is too dim to be
detected individually. While at very low fluxes the NPTF also loses the
ability to distinguish PSs from smooth emission, the threshold for PS
detection is lower for the NPTF than it is for other techniques that
rely on finding individually-significant sources. This is because the
NPTF only measures the aggregate properties of a PS population.

Using the NPTF, we are able to recover, for the first time, the
source-count distribution ( e.g. , flux distribution) for isotropically
distributed PSs at high Galactic latitudes, as a function of energy from
1.89 GeV to 2 TeV. This builds on previous studies that use related
methods to obtain the source-count distributions in single energy bins
from @xmath 2–12 GeV [ 162 , 102 ] and from 50–2000 GeV [ 163 ] .

The source-count distribution for a given astrophysical population
convolves information about its cosmological evolution. For a flat,
non-expanding universe, a uniformly distributed population of galaxies
has a differential source-count distribution @xmath , where @xmath is
the source flux at Earth and @xmath is the differential number of
sources [ 243 ] . This is the well-known Euclidean limit. However, the
power-law index changes when one takes the standard @xmath CDM cosmology
and more realistic assumptions for the redshift evolution of
source-dependent observables such as luminosity. Therefore, the features
of the source-count distribution—especially, its power-law indices
and/or flux breaks—encode information about the number of source classes
contributing to the EGB as well as their cosmological evolution.

These source-count distributions provide the keys for interpreting the
GeV–TeV sky. For example, they enable us to obtain the intensity
spectrum for PSs, down to a certain flux threshold, as a function of
energy. We find that while the EGB is dominated by PSs, likely blazars,
in the entire energy range from 1.89–2000 GeV, there is also room for
other source classes, which contribute flux more diffusely, to produce a
sizable fraction of the EGB. Our findings may therefore leave open the
possibility that IceCube’s PeV neutrinos [ 167 , 168 , 169 , 170 ] can
be explained by @xmath hadronic interactions in e.g. , SFGs [ 230 , 224
, 244 ] or mAGN [ 245 ] , which—as we show in Sec. 3.3 —show up as
smooth isotropic emission under the NPTF. Additionally, the high-energy
source-count distributions allow us to make predictions for the number
of blazars, which dominate the high-energy data, that will be resolved
by upcoming TeV observatories such as the Cherenkov Telescope Array
(CTA) [ 246 , 247 ] . While our analysis does not let us conclusively
identify the locations of these sources, we provide maps showing the
locations on the sky where, statistically, there are most likely to be
PSs.

This chapter is organized as follows. We begin in Sec. 3.2 by reviewing
the analysis methods. Sec. 3.3 then applies these methods to simulated
sky maps. We cannot stress the importance of these simulated data
studies enough; they are crucial for proving the stability of the
analysis methods and laying the foundation for the data results that
follow. Our data study is divided into two separate analyses for low
(1.89–94.9 GeV) and high (50–2000 TeV) energies, described in Sec. 3.4
and 3.5 , respectively. The global fits to the full energy range, as
well as their implications, are discussed in Sec. 3.6 . Further details
on the creation of the simulated data maps and supplementary analysis
plots are provided in the Appendix. The main results of this chapter are
summarized in a few key figures. In particular, the source-count
distributions for the low and high-energy analyses are shown in Figs.
3.4 , 3.7 , and 3.9 , respectively, while Fig. 3.10 presents a spectral
fit to the PS intensity from 2 GeV to 2 TeV.

### 3.2 Methodology

In this chapter, we make use of both Poissonian and non-Poissonian
template-fitting techniques. Poissonian template fitting is a standard
tool in astrophysics for decomposing a sky map into component
“templates” with different spatial morphologies. The NPTF builds upon
this technique by allowing for the addition of templates whose spatial
morphology traces the distribution of a PS population, even if the exact
position of the sources that make up that population are not known. More
precisely, in both template-fitting procedures one starts with a data
set @xmath that consists of counts @xmath in each pixel @xmath . ^(†)
^(†) † We will only work with a single energy bin at a time for
simplicity, though in principle model parameters may be shared between
energy bins. In this case, the likelihood function over the full energy
range may be written as the product of the likelihood functions in the
energy sub-bins. One then fits a model @xmath with parameters @xmath to
the data by calculating the likelihood function

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath denotes the probability of observing @xmath photons in
pixel @xmath with model parameters @xmath .

In Poissonian template fits, the probabilities @xmath are Poisson
distributions, with the model parameters @xmath only determining the
means of the distributions. That is, the mean expected number of photon
counts at each pixel @xmath may be written as

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where the sum is over template components and @xmath denotes the mean of
the @xmath component for model parameters @xmath . The @xmath may
parameterize, for example, the overall normalization of the templates or
the shapes of the templates. Then, the probability @xmath is simply
given by the Poisson distribution with mean @xmath .

In the NPTF, the situation is more complicated because we do not know
where the PSs are. As a result, if we want to calculate the probability
of observing @xmath photons in a given pixel @xmath , we must first
calculate the probability that a PS (or a collections of PSs) exists in
the vicinity of the pixel @xmath , with a given flux (or set of fluxes).
Then, for that PS population, we calculate the probability of @xmath
photons being produced in pixel @xmath . Convolving these two
calculations together leads to distinctly non-Poissonian probabilities.
In particular, the probability distributions in the presence of
unresolved PSs tend to be broader than Poisson distributions, if both
distributions have the same mean expected number of photon counts. The
intuition behind this fact is that relative to a diffuse source, a
collection of PSs leads to more “hot” pixels with many photons (where
there are PSs) and more “cold” pixels with very few photons (where there
are no PSs).

#### 3.2.1 The Templates

We include three Poissonian templates for (1) diffuse gamma-ray emission
in the Milky Way, assuming the Fermi p8r2 ( gll_iem_v06.fits )
foreground model, (2) uniform emission from the Fermi bubbles [ 140 ] ,
and (3) smooth isotropic emission. Each of these templates is associated
with a single model parameter describing its overall normalization.
Variations to the choice of foreground model and bubbles template will
be discussed in Sec. 3.4.2 .

The model parameters specific to the isotropic-PS population enter into
the source-count distribution @xmath , which we characterize as a
triply-broken power law:

  -- -- -- -------
           (3.3)
  -- -- -- -------

In particular, there are three breaks, @xmath , along with four indices,
@xmath , and the overall normalization, @xmath . ^(‡) ^(‡) ‡ Note that
the NPTF can also handle PS templates with non-trivial spatial
distribution, as was done in the Inner Galaxy analyses in [ 102 , 161 ]
, though in this work we will only consider the isotropic-PS template.
The justification for a triply-broken power law is that @xmath
designates the high-flux loss of sensitivity, beyond which @xmath cannot
be probed because no sources exist with such high flux. The break @xmath
designates the low-flux sensitivity, below which PS emission cannot be
distinguished from smooth emission. This leaves @xmath to probe any
physical break in the source-count distribution in the flux region where
the NPTF can constrain it. We have verified, however, that the results
do not change significantly if the source-count distribution is fit by a
doubly broken power law.

It is important to stress that the photon-count probabilities are
non-Poissonian in the presence of unresolved PSs because their locations
are unknown. Once we know where a PS is, we can fix its location and
describe it through a Poissonian template with a free parameter for the
overall normalization of the source. However, even resolved sources with
known locations may be characterized by the non-Poissonian template if
we do not also put down Poissonian templates at their locations. This is
the approach that we take throughout this chapter; that is, we model
both the resolved (in the 3FGL and 2FHL catalogs) and unresolved PS
populations through a single @xmath distribution, without individually
specifying the locations of any sources.

The point-spread function (PSF) must be properly accounted for in the
template-fitting procedure. The diffuse models are smoothed according to
the PSF using the Fermi Science Tools routine gtsrcmaps . The bubbles
template is smoothed with a Gaussian approximation to the PSF, with
width set to give the correct 68% containment radius in each energy bin.
We follow the prescription developed in [ 145 ] to account for the PSF
in the calculation of the non-Poissonian photon-count probabilities; for
this, we use the King function parameterization of the PSF provided with
the instrument response function for the given data set. In Sec. 3.4.2 ,
however, we show that consistent results are obtained when using a
Gaussian approximation to the PSF instead.

#### 3.2.2 Bayesian Fitting Procedure

The formalism developed in [ 145 , 143 , 102 ] (see also [ 162 ] and [
161 ] ) is used to calculate the photon-count probability distributions
in each pixel as a function of the Poissonian and non-Poissonian model
parameters @xmath . Then, Bayesian techniques are used to construct a
posterior distribution @xmath for the parameters @xmath and the
likelihood function in ( 3.1 ). We construct the posterior distribution
numerically using the MultiNest package [ 181 , 182 ] with 700 live
points, importance nested sampling and constant efficiency mode
disabled, and sampling efficiency set for model-evidence evaluation.

All prior distributions are taken to be flat except for @xmath , which
is taken to be log-flat. The prior ranges for the model parameters are
shown in Tab. 3.1 .

These prior ranges successfully reconstruct the source-count
distributions of simulated data sets, as discussed in Sec. 3.3 .
Variations to the prior ranges in Tab. 3.1 are considered in Sec. 3.4.2
.

In Tab. 3.1 , the parameter @xmath denotes the normalization of the
@xmath template, which is defined in terms of a baseline value. The
baseline value is obtained by first performing a Poissonian template fit
over 17 (10) log-spaced energy sub-bins between @xmath and @xmath GeV
(50 and 2000 GeV) for the low (high)-energy analysis. When this
procedure is applied to the low-energy analysis where the known PSs are
very bright, we mask the 300 brightest and most variable 3FGL sources,
at 95% containment. At both high and low energies, we include a PS model
constructed from the 3FGL catalog. ^(§) ^(§) § Importantly, we do not
include the PS model or mask any PSs in the NPTF analyses. The fitting
procedure then allows us to recover the normalizations for the diffuse
background, bubbles, and isotropic templates in each energy sub-bin.

The actual energy bins used for the NPTF studies presented in this study
are larger than the sub-bins described above. Therefore, the baseline
normalizations used to define the NPTF priors in the energy range @xmath
are found by applying the best-fit Poissonian normalizations from the
individual sub-bins to the corresponding templates, which are then
combined. ^(¶) ^(¶) ¶ In practice, however, this prescription for
combining the templates between energy sub-bins does not significantly
affect our results. Therefore, @xmath in the NPTF analysis implies that
the normalization of the @xmath template is the same as that computed
from the Poissonian scans. The benefit of this approach is that it
allows one to keep track of how the individual Poissonian templates
react to the addition of non-Poissonian ones. For example, the
normalization of the diffuse-background template should remain
consistent between a standard template analysis, where PSs are accounted
for by the 3FGL model, and the NPTF analysis, where PSs are accounted
for by the non-Poissonian template; indeed, we find that is the case in
all of the analyses we perform.

#### 3.2.3 Exposure Correction

While the source-count distribution @xmath is defined in terms of flux,
@xmath , with units of @xmath , the priors for the breaks in Tab. 3.1
are written in terms of counts, @xmath . To convert from flux to counts,
we multiply by the exposure of the instrument, with units of cm @xmath
s. However, the relation between flux and counts is complicated by the
fact that the exposure of the instrument varies both with energy and
position in the sky. Below, we describe how we deal with both
complications, starting first with the energy dependence.

The exposure map in the @xmath energy sub-bin is given by @xmath . To
construct the exposure map @xmath in the larger energy range from @xmath
, which contains multiple energy sub-bins, we average over the @xmath of
the individual sub-bins, weighted by a power-law spectrum @xmath , as
this is generally consistent with the isotropic spectrum over most of
our energy range. This procedure introduces a source of systematic
uncertainty in going from counts to flux, as not all source components
have an energy spectrum consistent with this spectrum. However, we have
checked that variations to this procedure—such as weighting the
exposures in the sub-bins by power laws of the form @xmath , with @xmath
varying between @xmath and @xmath ---do not significantly change the
results. ^(∥) ^(∥) ∥ We have also checked that weighting the exposures
in the sub-bins by the intensities computed from the Poissonian template
scans gives consistent results. The weighting procedure is most
important at very high energies, on the order of hundreds of GeV, where
the exposure map varies strongly across the energy sub-bins.

The breaks @xmath in Tab. 3.1 , with units of counts, are defined
relative to the mean exposure @xmath , averaged over all pixels in the
region of interest (ROI). Because the NPTF is performed at the level of
counts and not flux, we must also convert the source-count distribution
@xmath to a distribution @xmath , which is unique to each pixel @xmath :

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

Then, the photon-count probability distribution must be computed
uniquely at each pixel. In practice, however, it is numerically
expensive to perform this procedure for every pixel in the ROI. Instead,
we follow [ 162 ] and break the ROI up into @xmath regions by exposure.
Within each region, we assume that all pixels have the same exposure,
which is taken to be the mean over all pixels in the sub-region. The
likelihood function is then computed uniquely in each exposure region,
and the total likelihood function for the ROI is the product of the
likelihoods across exposure regions. In practice, we find that our
results are convergent for @xmath . We will take @xmath throughout this
study, though we have checked that our main results are consistent with
those found using @xmath .

#### 3.2.4 Data Samples

We run the NPTF analysis, as described above, on Fermi data, considering
low (1.89–94.9 GeV) and high (50–2000 GeV) energies separately. The
former is discussed in Sec. 3.4 , while the latter is the focus of Sec.
3.5 . The primary difference between the data sets used in these studies
is the data-quality cuts; moving to higher energies requires loosening
these criteria to avoid being limited by statistics. The overlap in
energy between the two studies allows us to compare the consistency of
the results when transitioning between analyses.

The low-energy study uses the Pass 8 Fermi data from @xmath August 4,
2008 to June 3, 2015. The primary studies use the top quartile of the
ultracleanveto event class (PSF3) as ranked by angular resolution,
although the top-three quartiles (PSF1--3) are also studied separately.
^(**) ^(**) ** The PSF quartiles indicate the quality of the
reconstructed photon direction, with ‘PSF3’ being the best and ‘PSF0’
being the worst. As a systematic check, we also consider the top-three
quartiles of source data. The ultracleanveto event class is the cleanest
event class released with the Pass 8 data and is recommended for studies
of the EGB. However, the source event class has an enhanced exposure and
thus may be advantageous at high energies where statistics become
limited. On the other hand, we expect the source data to have additional
cosmic-ray contamination relative to the ultracleanveto data.

The recommended ^(††) ^(††) ††
http://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data_Exploration/Data_preparation.html
event quality cuts are applied, requiring that all photons have a zenith
angle less than @xmath and satisfy “ DATA_QUAL==1 && LAT_CONFIG==1 &&
ABS(ROCK_ANGLE) @xmath .” A HEALPix [ 186 ] pixelation is used with
nside =128, which corresponds to pixels roughly @xmath to a side. We
consider four separate energy bins: @xmath , @xmath , @xmath , and
@xmath GeV.

In the low-energy analysis with ultracleanveto PSF3 data, the means of
the weighted exposure maps in the four increasing energy bins are @xmath
cm @xmath s over the region of interest with @xmath . The 68%
containment radii for the PSF, averaged over the isotropic spectra in
the energy sub-bins, are @xmath degrees. Going to PSF1–3 data, the
exposures increase to @xmath cm @xmath s, while the 68% containment
radii of the PSF degrade to @xmath degrees. Going to source data with
PSF1–3, the exposures ( @xmath cm @xmath s) increase further, while the
68% containment radii ( @xmath degrees) are essentially the same as in
the ultracleanveto case.

The high-energy analysis uses the Pass 8 Fermi data from @xmath August
4, 2008 to May 2, 2016 and all PSF quartiles of either the
ultracleanveto or source event class. The ROI is also extended to @xmath
. We include more data in the high-energy analysis as there are far
fewer photons than at lower energies. We employ the recommended
event-quality cuts as in the low-energy analysis and also choose nside
=128 HEALPix pixelation. Results are presented for the three energy bins
@xmath , and @xmath GeV. With ultracleanveto data, the weighted
exposures in the energy bins are @xmath cm @xmath s, while with source
data the exposures become @xmath cm @xmath s. For both data sets, the
68% containment radii are approximately @xmath degrees. We will also
discuss results of analyses performed over a single wide-energy bin from
@xmath GeV.

### 3.3 Simulated Data Studies

To study the behavior of the NPTF, we apply it to simulated data sets of
the gamma-ray sky. These results are crucial both for understanding
systematics associated with the NPTF as well as for interpreting the
results of the NPTF in terms of evidence for or against the existence of
these source populations.

A simulated data map can be created starting from a particular source
population that contributes to the EGB. Using a theory model for the
energy spectrum and luminosity function, the source-count distribution
for that population can be derived in a specified energy range—see Sec.
3.3.1 for further details on this procedure. The appropriate number of
sources is then drawn from this function and randomly distributed across
the sky, with counts chosen to follow the intensity spectrum. Sources
are then smeared with the appropriate Gaussian PSF to mimic the desired
Fermi data set bin-wise in energy, and Poisson counts are drawn to
obtain the simulated map for the population. This is then combined with
the simulated contribution of the p8r2 foreground model and the Fermi
bubbles, whose normalizations are determined from the Poissonian
template fits to the real data, as described in Sec. 3.2 .

For most of this section, we simulate data corresponding to the PSF3
event type (best PSF quartile) of the ultracleanveto event class and
focus on the following four energy bins: [1.89, 4.75], [4.75, 11.9],
[11.9, 30], and [30, 94.9] GeV. However, we also simulate data
corresponding to the PSF1–3 (top 3 PSF quartiles) instrument response
function to illustrate potential advantages in going to the more
inclusive data set, albeit with a slightly worse PSF. Once the simulated
data maps are created, we run them through the NPTF analysis pipeline.
First, we analyze the case where either blazars or SFGs fully account
for the EGB, and then we analyze a perhaps more realistic scenario where
both populations contribute significantly to the flux. The particular
blazar and SFG models used here are merely meant for illustration. They
are chosen as examples that span the range of possibilities between
smooth and PS isotropic contributions. As mAGN are fainter and more
numerous then blazars, they likely act similarly to SFGs in the context
of the NPTF and so we do not consider them separately here. A detailed
analysis of how the NPTF responds to the broader class of theoretical
models for these source classes is beyond the scope of this study.

#### 3.3.1 Simulating Energy-Binned Source-Count Distributions

We generate simulated maps directly from the source-count distribution
@xmath . To obtain this, we need two inputs: the gamma-ray luminosity
function, @xmath , and the source energy spectrum, @xmath [ 248 ] .
Typically, the luminosity function (LF) is given by

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is the comoving volume, @xmath is the photon spectral
index, @xmath is the redshift, @xmath is the number of sources, and
@xmath is the rest-frame luminosity for energies from 0.1–100 GeV in
units of GeV s @xmath .

The photon flux in this energy range, @xmath , is defined in terms of
the source energy spectrum,

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where the units are cm @xmath s @xmath , and @xmath GeV.

The source-count distribution is then given by

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

which can be accurately estimated as

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

where @xmath is the full-sky solid angle, @xmath is the comoving volume
slice for a given redshift and @xmath is sufficiently small. To
calculate @xmath , we need the following expression, which relates the
luminosity to the energy flux:

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath is the luminosity distance. For a given @xmath and @xmath ,
one can use ( 3.6 ) to solve for the normalization of @xmath , which can
be substituted into ( 3.9 ), along with @xmath and @xmath , to obtain
the associated value of the luminosity. The photon flux, @xmath , is
related to the photon count, @xmath , via the mean exposure @xmath ,
which is averaged over 0.1–100 GeV and the ROI. This allows us to
finally obtain @xmath from ( 3.8 ).

The procedure outlined above allows one to obtain the source-count
distributions based on models of luminosity functions and spectral
energy distributions provided in the literature. For the AGN and SFG
examples we consider in detail in this work, the luminosity functions
correspond to photon energies from 0.1–100 GeV. However, we also need
the source-count distributions in subset energy ranges corresponding to
our energy bins of interest, with @xmath GeV. We rescale the fluxes for
these individual energy bins of interest to those in the provided
0.1–100 GeV range using a procedure similar to [ 248 ] . Denoting
quantities associated with this energy bin with a prime, we can write
the new source-count distribution as

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

where @xmath is again sufficiently small—we set @xmath , and verify that
the answer is robust to this choice. Note that the integral must still
be done over @xmath (unprimed) because the luminosity function is
explicitly defined in terms of it. So, we must solve for the photon flux
over the full energy, @xmath , in terms of the value in the sub-bin,
@xmath . The two are related via a proportionality relation

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

where the exponential factor accounts for the attenuation due to
extragalactic background light (EBL) [ 249 , 250 , 251 , 252 , 253 , 254
, 255 ] . It arises from pair annihilation of high-energy gamma-ray
photons with other background photons in infrared, optical, and/or
ultraviolet, and is described by the optical depth, @xmath . We use the
EBL attenuation model from [ 256 ] .

Additionally, the expected gamma-ray spectrum can be calculated from the
luminosity function as

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

We use this equation to appropriately weight the number of photons per
energy sub-bin for the individual sources when creating simulated maps.
This ensures that the variations in PSF and exposure within the larger
energy bins used in the NPTF analyses are properly accounted for in the
simulation procedure.

#### 3.3.2 Blazars

Active galactic nuclei (AGN) are the highly luminous central regions of
galaxies where emission is dominated by accretion onto a supermassive
black hole [ 257 ] . If the black hole is spinning, then relativistic
jets may also form. Blazars are a subclass of AGN in which the jet is
oriented within @xmath of the line-of-sight [ 258 ] . The spectral
energy distribution of these objects is bimodal with a peak in the
ultraviolet due to synchrotron radiation of electrons in the jet, and
another peak in the gamma band from inverse Compton scattering of the
same electrons [ 259 , 260 , 261 , 262 ] . There is also the possibility
of a hadronic contribution to blazar gamma-ray spectra, although this is
likely to be sub-dominant [ 263 , 264 , 265 ] . Blazars may be further
classified as either BL Lacertae (BL Lacs) or Flat Spectrum Radio
Quasars (FSRQs), which are characterized by the absence or presence of
broad optical/ultraviolet emission lines, respectively.

Before Fermi , few blazars had been identified in gamma rays, and to
estimate the size of this population, one had to extrapolate based on
those observed at lower frequencies. However, Fermi brought the
discovery of many more blazars in the gamma-ray band, making it possible
to study their properties directly [ 207 , 211 , 212 , 213 , 266 , 267 ]
. Most recently, 403 blazars (with @xmath ) from the First LAT AGN
Catalog [ 268 ] were studied [ 118 ] . FSRQs and BL Lacs were considered
together in the same sample to improve statistics. We use the best-fit
luminosity and spectral energy distributions given in [ 118 ]
(specifically, the luminosity-dependent density evolution, or LDDE,
scenario) to model the blazar component in our simulated data and refer
to it as the “Blazar–1” model. Alternatively, we also consider BL Lacs
and FSRQs separately, adding up their respective contributions using the
LDDE1 model from [ 212 ] and the LDDE model from [ 211 ] , which we
refer to as the “Blazar–2” model. This model predicts a much flatter
source-count distribution below the Fermi detection threshold, with more
low-flux sources. The two source-count models approximately bracket the
current theoretical uncertainty in the faint-end slope of blazars, and
we use them to study the response of different blazar models to the
NPTF, although this is meant to be purely illustrative and by no means
exhaustive.

Figure 3.1 shows the best-fit source-count distributions recovered when
the NPTF analysis is run on the Blazar–1 simulated data map, assuming
the PSF3 instrument response function. In each panel, the dark (light)
red band is the 68% (95%) credible interval for the isotropic-PS
source-count distribution as recovered from the posterior and
constructed pointwise in flux. The red line shows the median
source-count distribution, constructed in the same way. The dashed red
curve, on the other hand, indicates the source-count distribution of the
blazar model used to generate the simulated data. A flux histogram of
the simulated PSs for the particular realization shown here is given by
the red points, with vertical error bars indicating the 68% credible
interval associated with Poisson counting statistics on the number of
sources in that bin. Notice that these error bars become large at high
fluxes because there are very few sources per flux bin.

In general, the reconstructed source-count distribution is in good
agreement with the input source-count distribution at intermediate
fluxes, with uncertainties becoming large at low and high fluxes. At
high flux, this is due to the fact that it is unlikely to draw a bright
source from the underlying source-count distribution. At low fluxes, it
is difficult to distinguish PS emission from genuinely isotropic
emission. To illustrate this point, we also mark the flux that
corresponds to a single photon on average (in the particular energy
range, region-of-interest, and event class) with the vertical dot-dashed
black line. At fluxes corresponding to counts near or below @xmath 1
photon, it is difficult to distinguish PS emission from smooth emission
with the NPTF, as evidenced by the growing uncertainties. In this
low-flux regime, we do not expect that the NPTF will be able to fully
recover the properties of the input source-count distribution.

The vertical dotted green lines in Fig. 3.1 correspond to the fluxes
above which 90%, 50%, and 10% (from left to right) of the photon counts
are accounted for, on average, by sources with larger flux. Note that in
the lowest energy bin, 90% of the flux arises from sources that
contribute more than one photon. Moving towards higher energies, a
larger fraction of the flux arises from sources that contribute less
than one photon. In all energy bins, more than 50% of the flux is
accounted for by sources that contribute more than a single photon each.

The corresponding energy spectra for the various templates are shown on
the top left panel of Fig. 3.2 . As is evident, these blazars show up as
PSs under the NPTF; indeed, the smooth isotropic flux (blue) is
sub-dominant in each energy bin. Overlaid in dashed red is the spectrum
for the simulated Blazar–1 sources. The sum of the smooth and PS
isotropic components—which is simply the EGB intensity—is consistent
with the simulated spectrum for the blazar model. The green curve shows
the median of the posterior for the galactic diffuse model spectrum. The
energy spectrum of the diffuse model is softer than that for blazars, so
that the diffuse model dominates more at low energies than at high. The
sum of the components (yellow band) is consistent with the total flux in
the simulated data (black lines) at 68% confidence.

As a contrasting example, we also simulate the Blazar–2 model, which
predicts more low-flux sources than the previous example we considered.
The best-fit source-count distributions for the Blazar–2 simulated maps
are shown in Fig. LABEL:fig:bl2dnds . Once again, we see good agreement
between the input data and the recovered source-count distribution above
the single-photon sensitivity threshold. In this case, however, the
reference model predicts a larger fraction of flux coming from sources
below this threshold. For example, about 50% of the flux comes from
sub-single photon sources in the second energy bin, and this fraction
only increases further at higher energies. The corresponding energy
spectrum is shown in the top right panel of Fig. 3.2 . As expected, an
increasing amount of flux is absorbed by the Poissonian isotropic
template. However, the EGB spectrum, shown by the purple band, is still
consistent with the input spectrum for the Blazar–2 model.

To further quantify the ability of the NPTF to reconstruct the blazar
flux as PS emission, it is convenient to consider the ratios @xmath in
each energy bin, where @xmath is the PS intensity found by the NPTF and
@xmath is the blazar intensity in the simulation. For the Blazar--1
model, we find ^(‡‡) ^(‡‡) ‡‡ Throughout this work, best-fit values
indicate the 16 @xmath , 50 @xmath , and 84 @xmath percentiles of the
appropriate posterior probability distributions.

  -- -------- --
     @xmath   
  -- -------- --

in each of the four respective energy bins, while for the Blazar–2
model, we find

  -- -------- --
     @xmath   
  -- -------- --

for the particular Monte Carlo realizations shown. ^(**) ^(**) **
Different Monte Carlo realizations are found to induce variations
consistent with the quoted statistical uncertainties, generally on the
order of 5%. For the Blazar–2 scenario, more flux goes into smooth
isotropic emission, which is why the PS fractions are correspondingly
smaller in each energy bin. Note that, in both scenarios, the fraction
of the blazar flux absorbed by the PS template decreases at higher
energies, where the photon counts become less numerous and a higher
fraction of the blazar flux is generated by sub-threshold sources. As a
result, the intensities @xmath should be interpreted as lower bounds on
the blazar flux; this intuition is validated by the fact that the ratios
@xmath tend to be less than unity.

Next, we explore whether including more quartiles of the ultracleanveto
data, as ranked by PSF, increases our ability to reconstruct the blazar
flux as PSs under the NPTF. When including more quartiles of data, there
are two competing effects that determine our ability to constrain the PS
flux: on the one hand, we increase the effective area, but on the other
hand, we worsen the angular resolution of the data set. We investigate
these effects by repeating the Monte Carlo tests described above using
the PSF1–3 instrument response function , and here we simply quote the
fractions

  -- -------- --
     @xmath   
  -- -------- --

for a generic realization of the Monte Carlo simulations for the
Blazar–2 model. The PSF1–3 event type increases our ability to
distinguish between the blazar emission and smooth emission compared to
the PSF3 event type.

#### 3.3.3 Star-Forming Galaxies

Star-forming galaxies (SFGs) like our own Milky Way are individually
fainter, though much more numerous, than blazars. The modeling of SFGs
in the gamma-ray band is highly uncertain, as Fermi has only detected
eight SFGs thus far [ 269 ] . However, SFGs could still contribute a
sizable fraction of the total flux observed by Fermi . Even though SFGs
are PSs, their flux is expected to be dominated by a large population of
dim sources degenerate with smooth isotropic emission. Under the NPTF,
therefore, we expect that the majority of their emission will be
absorbed by the smooth isotropic template. To illustrate this point, we
simulate SFGs using the luminosity function and energy spectrum from [
224 ] . In that work, input from infrared wavelengths was used to
construct a model for the infrared flux from SFGs. Then, a scaling
relation was used to convert from infrared to gamma-ray luminosities.
The contributions from quiescent and starburst SFGs were considered
separately, along with SFGs that host an AGN. Note, however, that other
models predict less emission from SFGs than this particular case—see
e.g. , [ 220 , 226 , 221 ] .

We also performed tests using simulated SFGs. We find that while the
NPTF does detect a small PS component in the first few energy bins, as
the result of a few SFGs above the sensitivity threshold of the NPTF in
those energy bins, by far most of the SFG emission is detected as smooth
isotropic emission, with the ratio @xmath in all energy bins, where
@xmath is the intensity of smooth isotropic emission. Moreover, the
intensity @xmath is consistent with the simulated EGB (SFG flux) in all
energy bins, at 68% confidence.

#### 3.3.4 Blazar and SFG combination

A perhaps more realistic scenario for testing the NPTF is to consider a
scenario where both SFGs and blazars contribute to the EGB. Therefore,
we create simulated maps that include both components and test them on
the NPTF. The recovered energy spectra for the SFG + Blazar–1 (Blazar–2)
example is shown in the bottom left (right) panel of Fig. 3.2 . In both
cases, the PS spectrum is consistent with that found in the blazar-only
simulations, which are shown in the top panels in that figure. The
reconstructed source-count distributions for these examples are not
shown, as they are consistent with those found in the blazar-only cases.

In the case of of the Blazar–1 model, the spectra of the smooth
isotropic emission and the PS emission trace the spectra of the input
SFG population and blazar population, respectively. In the case of the
Blazar–2 model, the PS flux is further below the input blazar spectrum,
as was found in the blazar-only simulations. However, the smooth
isotropic emission is further above the simulated SFG spectrum. In both
cases, the sum of the smooth isotropic emission and PS emission (EGB) is
consistent with the simulated blazar plus SFG flux.

There is, in fact, a subtle difference between the PS distribution
recovered with and without the addition of a SFG population. The
difference becomes noticeable when comparing the fractions

  -- -------- --
     @xmath   
  -- -------- --

for SFG + Blazar–1 and

  -- -------- --
     @xmath   
  -- -------- --

for SFG + Blazar–2 to the corresponding values for the blazar-only
simulations. In the simulations with SFGs, the fractions @xmath are
generally higher and have larger uncertainties. The reason for this is
that the SFG emission is degenerate with an enhanced sub-threshold
component to the PS source-count distribution.

Simulating data with the PSF1–3 instrument response function, we find
that the ratios @xmath are somewhat closer to unity than in the PSF3
case. In particular, for the SFG + Blazar–2 model simulations,

  -- -------- --
     @xmath   
  -- -------- --

The improved exposure allows the NPTF to probe lower fluxes and to
therefore recover a larger fraction of the isotropic-PS emission.

### 3.4 Low-Energy Analysis: 1.89–94.9 GeV

The findings from the previous section illustrate that the NPTF
procedure is able to set strong constraints on the PS ( e.g. , blazar)
and smooth Poissonian ( e.g. , SFGs, mAGN) contributions to the EGB. In
this section, we focus on the energy range from 1.89–94.9 GeV, and begin
by presenting the results of our benchmark analysis on the real Fermi
data. This is followed by a detailed discussion of potential systematic
uncertainties and their effects on the conclusions.

#### 3.4.1 Pass 8 ultracleanveto Data

##### Top PSF Quartile

We begin by analyzing the ultracleanveto PSF3 data for @xmath , using
the p8r2 foreground model. This is referred to as the “benchmark
analysis” throughout the text. Table 3.2 provides the best-fit
intensities for each spectral component, as a function of energy, and
the best-fit spectra are plotted in the left panel of Fig. 3.3 . The
p8r2 diffuse model is shown in green (median only), while the smooth
isotropic and isotropic-PS posteriors are shown by the blue and red
bands, respectively. The best-fit spectrum for PSs with @xmath in the
3FGL catalog [ 115 ] is shown by the dashed black line in Fig. 3.3 ; the
spectrum as plotted should be treated with care as systematic
uncertainties are not properly accounted for. In particular, the 3FGL
catalog includes sources between 0.1–300 GeV. At the high end of this
range, the spectrum is driven to a large extent by extrapolations from
lower energies, where the statistics are better. The potential errors
associated with such extrapolations are difficult to quantify and are
not shown in Fig. 3.3 . As a result, a direct comparison between the
3FGL spectrum and our results is difficult to make, especially in the
highest energy bins. For this reason, we have a dedicated NPTF study for
energies greater than 50 GeV in Sec. 3.5 . Those results are compared to
the Fermi 2FHL catalog [ 163 ] , which is explicitly constructed at
higher energies and is likely a more faithful representation of
above-threshold PSs in this regime.

The source-count distributions reconstructed from the NPTF are shown in
Fig. 3.4 , with best-fit parameters provided in Tab. 3.3 . For
comparison, the binned 3FGL source-count distributions are also plotted;
the vertical error bars represent 68% statistical uncertainties and do
not account for systematic uncertainties. A few trends are clearly
visible. First, each flux break tends to have large uncertainties. This
may be a reflection of the fact that the real source-count distribution
is not a simple triply-broken power law, but rather a more complicated
function, as in the blazar simulations of Sec. 3.3 . Therefore, the
best-fit values for each of these parameters, when viewed independently,
may be somewhat deceptive. As is evident in Fig. 3.4 , the posteriors
for the breaks and indices are distributed in such a way as to describe
a smooth concave function for @xmath .

At very high and very low flux, the uncertainties on the indices (
@xmath and @xmath , respectively) become large. At high flux, this is
simply due to the fact that there are very few sources, so the
source-count distribution falls off rapidly. At low flux, the large
uncertainties on @xmath arise from the difficulty in distinguishing the
isotropic-PS contribution from its smooth counterpart. Indeed, below the
single-photon boundary (dot-dashed black line), the NPTF analysis starts
to lose sensitivity. The posterior distributions for the slopes above
(below) the highest (lowest) break are highly dependent on the priors
and so the quoted values in Tab. 3.3 should be treated with care.

The presence of any distinctive breaks encodes information about the
number of source populations as well as their evolutionary properties.
In all energy bins, we see that the NPTF places the lowest break, @xmath
, close to the one-photon sensitivity threshold and the highest break,
@xmath , in the vicinity of the highest-flux 3FGL source (see Tab. 3.3
for the exact values). The evidence for an additional break, @xmath , at
intermediate fluxes varies depending on the energy bin. From
1.89–4.75 GeV, there is strong indication for a break at fluxes @xmath
@xmath ph cm @xmath s @xmath , with the index @xmath above the break
hardening to @xmath below the break. In the two subsequent energy bins,
up to @xmath @xmath GeV, we also find evidence that the source-count
distribution hardens as we move from high fluxes to below the second
break, with the index @xmath below the second break @xmath 1.9-2.0 in
both cases. In the last bin, the uncertainties are too large to
determine if the source-count distribution changes slope at any flux
above the lowest break @xmath .

##### Top Three PSF Quartiles

The benchmark analysis described in the previous section used only the
top quartile (PSF3) of the Pass 8 ultracleanveto data set. This
restriction selects events with the best angular resolution, but at the
price of reducing the total photon count. In Sec. 3.3 , we showed that
including the top three quartiles of the Pass 8 ultracleanveto data may
help constrain the source-count distribution at low fluxes. With that in
mind, we now investigate how the results of the benchmark analysis
change when using the PSF1–3 ultracleanveto data set.

In general, the best-fit intensities for the individual spectral
components are consistent within uncertainties with those obtained using
only the top quartile of data. The PS flux does increase slightly in
going from PSF3 to PSF1–3 in the upper energy bins due to the increased
exposure. More specifically, the ratios of the median PS intensities
measured with ultracleanveto PSF1–3 data to those measured with PSF3
data are @xmath in the four increasing energy bins. This can also be
seen in the associated spectral intensity plot (right panel of Fig. 3.3
), where the red bands are further above the 3FGL line in the last
energy bins than in the corresponding plot for the PSF3 analysis (left
panel). The intensity of the EGB is seen to increase slightly, in all
energy bins, when going from PSF3 to PSF1–3 data, potentially suggesting
additional cosmic-ray contamination with the looser photon-quality cuts,
though the increases in EGB intensities are within statistical
uncertainties.

The best-fit source-count distributions recovered by the NPTF with
PSF1–3 data are shown in Fig. 3.5 . For reference, the blue curve shows
the best-fit for the PSF3–only analysis. The most important difference
between the PSF3 and PSF1–3 results is that the source-count
distributions extend to lower flux with PSF1–3 data. This is due to the
fact that the exposure in each energy bin, averaged over the region of
interest, is larger for the top three quartiles compared to the top
quartile alone. As a result, the flux corresponding to single-photon
detection is lower (compare the vertical dot-dashed line in Fig. 3.5
with that in Fig. 3.4 ), which improves the NPTF reach. Thus, the PSF1–3
analysis is sensitive to more sub-threshold sources. Note that the same
trend was observed in the simulation tests in Sec. 3.3 in going from
PSF3 to PSF1–3 data sets.

Other than the location of the lowest break, which is lower due to the
increased exposure, all other source-count distribution parameters are
consistent, within uncertainties, between analyses. At the lowest
energy, the break at @xmath photons cm @xmath s @xmath is even more
pronounced, with an index @xmath above the break and @xmath below the
break. In the highest energy bin, the structure observed in the
source-count distribution for the benchmark analysis has smoothed out.

#### 3.4.2 Systematic Tests

The previous subsection illustrated how the results of the NPTF change
when additional ultracleanveto PSF quartiles are included in the
analysis. We also tested the stability of our analysis to variations in
the region of interest, Fermi event class, foreground modeling, Fermi
bubbles, PSF modeling, and choice of priors.

Figure 3.6 briefly summarizes the results. The EGB intensity as measured
by Fermi is shown by the gray band. To obtain this band, we use the
best-fit power-law spectrum with exponential cut-off provided in [ 199 ]
; the width of the gray band is found by varying between best-fit values
for the three foreground models considered in that paper (Models A/B/C)
and does not include statistical uncertainties, which become
increasingly important at high energies. The smooth isotropic intensity,
and thus the intensity of the EGB, is subject to large systematic
uncertainties. As expected, the variation in smooth isotropic intensity
is most pronounced when using the source event class, which contains
more cosmic-ray contamination. However, the spectrum of emission from
PSs as captured by the NPTF appears robust to all the systematic effects
considered here. This is the primary conclusion of this subsection. We
now describe in detail the systematic tests that were conducted for the
low-energy analysis.

##### Region of Interest

As a first cross-check on the stability of the results presented in Sec.
3.4.1 , we explore the effects of altering the region of interest. While
we previously defined the region of interest with @xmath , we now loosen
this constraint and consider the case @xmath . Extending the region of
interest closer to the Galactic disk increases the amount of data being
analyzed, but at the cost of potentially more contamination from diffuse
foreground emission and local PSs. As shown in Fig. 3.6 , the best-fit
intensities for the isotropic and isotropic-PS components are
equivalent, within errors, to their counterparts in the benchmark
analysis.

We also ran the NPTF on the Northern ( @xmath ) and Southern ( @xmath )
hemispheres separately. The intensities for the EGB, IGRB, and PS
components are systematically lower (higher) for the Northern (Southern)
analysis than for the benchmark case.

##### Event class

We explored the implications of broadening the ultracleanveto data set
to include the top three quartiles in Sec. 3.4.1 . Now, we consider the
implications of repeating the NPTF analysis on the source data with
PSF1–3. This event class has looser photon-quality cuts, which leads to
larger overall exposure, but significantly more cosmic-ray
contamination. In general, it is not recommended to use source data for
IGRB studies; for our purposes, however, it will be intriguing to see
how the increased photon statistics affect the recovered source-count
distribution for the PS component. As shown in Fig. 3.6 , the EGB
intensity is far larger than that recovered by the benchmark analysis
and overpredicts Fermi ’s EGB result in most energy bins. The sharp rise
in the EGB intensity can be traced to a substantial fraction of smooth
isotropic emission, which is expected for this event class at most
energies. Most importantly, the intensity of the isotropic-PS component
is consistent, within uncertainties, with that found in the benchmark
analysis. ^(*†) ^(*†) *† The recovered PS intensity is slightly larger
with source PSF1–3 data as compared to ultracleanveto PSF3 data, which
is likely due to the increased exposure in the source PSF1–3 data set.
This is a confirmation that the NPTF is able to successfully constrain
the source-count distribution even in a data set with significantly more
smooth isotropic flux.

##### Foreground Model

A potentially significant source of systematic uncertainty in the NPTF
analysis is due to mis-modeling of high-energy gamma-rays produced in
cosmic-ray propagation in the Milky Way [ 270 ] . These high-energy
photons arise from bremsstrahlung of electrons off the interstellar
medium, boosted pion decay, and inverse Compton (IC) emission off the
interstellar radiation field. Our benchmark analysis uses the associated
foreground model for the Pass 8 data set ( gll_iem_v06.fits ), denoted
here as p8r2 . The total diffuse emission in p8r2 is modeled as a linear
combination of several sources, some of which are traced by maps of gas
column densities, which serve as templates for the pion and
bremsstrahlung emission. The IC component is modeled using the GALPROP
package [ 271 ] . ^(*‡) ^(*‡) *‡ http://galprop.stanford.edu/ These
individual templates are fit to the data, and used to identify ‘extended
emission excesses’ that are identified directly and then added back into
the model [ 272 ] .

To better assess the uncertainties due to the foreground modeling, we
repeat the NPTF analysis using several other foreground models made
available by Fermi . In particular, we use the
gll_iem_v02_P6_V11_DIFFUSE.fits diffuse emission model, denoted as p6v11
, which was initially developed for the Pass 6 data set. ^(*§) ^(*§) *§
http://fermi.gsfc.nasa.gov/ssc/data/access/lat/ring_for_FSSC_final4.pdf
p6v11 is distinct from p8r2 in that it uses older gas and IC maps and
does not include templates for large-scale structure or extended
emission excesses. The Pass 7 model gal_2yearp7v6_v0.fits , denoted as
p7v6 , ^(*¶) ^(*¶) *¶
http://fermi.gsfc.nasa.gov/ssc/data/access/lat/Model_details/Pass7_galactic.html
is a compromise as it uses updated gas and IC maps and includes some
large-scale extended structures, such as Loop 1 and the Fermi bubbles.

The NPTF results using the p6v11 and p7v6 foreground models are
summarized in Fig. 3.6 .In general, we observe that the intensity of the
PS components is consistent with that for the benchmark analysis in all
energy bins. However, variations occur in the smooth isotropic
intensity. Typically, more IGRB intensity is recovered with p6v11 and
p7v6 , versus p8r2 . The differences are particularly dramatic in the
first two energy bins and are more severe for p6v11 . The net
consequence is that the EGB intensity is higher than the expected range
from Fermi . The enhancement in the isotropic component may arise from
the fact that each foreground model incorporates large-scale diffuse
structures differently—with p6v11 being the least inclusive and p8r2
being the most inclusive. We note, however, that the fit to data with
the p8r2 foreground model, from the point of view of the Bayesian
evidence, is much better than the analogous fit with the p6v11 model;
the fit with the p7v6 model is intermediate.

##### The Bubbles Template

To better understand how dependent the analysis is on the details of the
Fermi bubbles template, we simply removed the template from the
analysis. This has indiscernible effects on the final results. We see in
Fig. 3.6 that the EGB, IGRB, and PS intensities are consistent, within
uncertainties, to the corresponding values in the benchmark study.

##### Point Spread Function

The PSF can affect the photon-count distribution because it can
redistribute photons between pixels, and must therefore be properly
accounted for in the calculation of the photon-count probability
distributions. For the primary analyses presented in this work, the PSF
is modeled using a King function. However, to test the sensitivity of
the results to mis-modeling of the PSF, we have also repeated the NPTF
analysis using a two-dimensional Gaussian in the calculation of the
photon-count probability distributions, with a width set to give the
correct 68% containment radius. As shown in Fig. 3.6 , the NPTF results
remain unchanged with this substitution.

##### Priors

Our choice of priors, given in Tab. 3.1 , is carefully chosen to both
avoid biasing the posterior for the source-count distribution while at
the same time allowing breaks at both high and low flux. This is meant
to properly account for the fact that the source-count distribution is
not well constrained by the data at very high fluxes, where the mean
expected number of sources over the full region is much less than unity,
and at very low fluxes, where the mean photon-count per source is much
less than unity. Our choice of priors is further justified by the
simulated data studies, presented in Sec. 3.3 , which show that the NPTF
can successfully constrain the emission from blazar models. However, one
may still be concerned that these particular choice of priors might bias
the recovered source-count distribution in a particular way. For that
reason, we have tried many variations to the priors shown in Tab. 3.1 ,
three of which (labeled ‘Alt. priors 1–3’) are described below and shown
in Fig. 3.6 :

-    Alternate prior 1 : All priors are the same as in Tab. 3.1 , except
    for those on the breaks, which are changed to @xmath , @xmath , and
    @xmath ph for @xmath , @xmath , and @xmath , respectively.

-    Alternate prior 2 : As above, except changing the priors for the
    breaks to @xmath , @xmath , and @xmath ph, respectively.

-    Alternate prior 3 : All priors are the same as in Tab. 3.1 , except
    for that of @xmath , which is changed to @xmath .

The first two examples address the possibility that the break priors
might artificially sculpt the source-count distribution and the
recovered PS intensity, while the third example addresses how the
source-count distribution is dealt with at fluxes below the lowest
break, where the distribution is not well constrained by the data. In
many classes of blazar models, such as those considered in Sec. 3.3 ,
the index below the lowest break ( @xmath ) is greater than unity, so
that the total number of PSs @xmath @xmath diverges as the minimum flux
cut-off @xmath is taken to zero.

It is useful to know if the recovered PS intensity, @xmath , tends to
under or overshoot the simulated blazar intensity, @xmath , when using
the alternate priors. With that in mind, we run the NPTF on simulated
maps, as in Sec. 3.3 , constructed from both the SFG + Blazar–1 model as
well as the SFG + Blazar–2 model. For Alternate prior 1 , we find that

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

for the SFG + Blazar–1 and SFG + Blazar–2 models, respectively, with
ultracleanveto PSF3 instrument response function. With Alternate prior 1
, we see larger uncertainties, with the PS template capable of absorbing
more flux in particular. With Alternate prior 2 , on the other hand, we
find more noticeable differences in the medians as well as in the
uncertainties. In particular, for the SFG + Blazar–1 and SFG + Blazar–2
models, we find

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

respectively. In the Blazar–1 model case, it is important to notice that
at intermediate energies the NPTF tends to over-predict @xmath at the
@xmath 20% level. With Alternate prior 3 , the results are

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

for the Blazar–1 and Blazar–2 models. The Alternate prior 3 results are
consistently closer to unity than the first two alternate prior results.

As may be seen in Fig. 3.6 , the median values for the PS intensities
recovered from the NPTF analyses with alternate priors are generally
consistent with those found in the baseline study. The Alternate prior 3
PS intensities are slightly enhanced in all energy bins compared to the
baseline results—following our expectations from the simulation results
presented above—though the two results are consistent within statistical
uncertainties.

### 3.5 High-Energy Analysis: 50–2000 GeV

We now consider the NPTF results at high energies from 50–2000 GeV. The
number of photons available decreases when moving to higher energies, so
we loosen the restrictions on the PSF quartiles to maximize the
sensitivity potential of the NPTF. In this section, the majority of the
analyses are done using all quartiles of the ultracleanveto data, though
we also show results using all quartiles of source data. For the same
reason, we widen the ROI to @xmath rather than @xmath , although the
results are not sensitive to this cut, as we will show.

The best-fit energy spectra recovered by the NPTF analysis for the
high-energy study of ultracleanveto data is shown in the bottom right
panel of Fig. 3.7 . The fit results are compared with the best-fit
energy spectrum for sources in Fermi ’s 2FHL catalog [ 214 ] (dashed
black line). This recently-published catalog is based on 80 months of
data and focuses on hard sources in the range from 50–2000 GeV.
Statistical and systematic uncertainties are not accounted for in the
determination of the 2FHL spectrum in Fig. 3.7 ; these are likely
non-negligible, especially at the highest energies.

The best-fit source-count distributions for the three energy bins are
also shown in Fig. 3.7 , in the top row and bottom left panel. The black
points in those panels denote the 2FHL source-count distributions, with
vertical error bars indicating 68% Poisson errors. The statistical
errors on the 2FHL sources are large due to the fact that there are not
many sources. In all energy bins, the NPTF places the lowest break close
to the single-photon sensitivity threshold (vertical dot-dashed line)
and the highest break in the vicinity of the brightest 2FHL source, just
as in the low-energy analysis. Most notably in the 50–151 GeV bin, the
NPTF probes unresolved sources with fluxes nearly an order-of-magnitude
below the apparent 2FHL threshold. We find no evidence for an additional
intermediate-flux break in any of the energy bins, although it is
difficult to make conclusive statements due to the large uncertainties
in the individual source-count distributions.

We have completed a number of systematic tests of the high-energy
analyses that include looking at all quartiles of the source data,
requiring @xmath for both event classes, and using the third alternate
prior choice, with @xmath . The results are summarized in Fig. 3.8 .
Importantly, the isotropic-PS intensity is consistent across all the
tests. However, the EGB intensities recovered by the NPTF are, in
general, higher than those measured by Fermi . This discrepancy is
likely due to increased cosmic-ray contamination above @xmath 100 GeV,
as suggested by the high IGRB intensities recovered by the NPTF at these
energies. Indeed, the Fermi EGB study on Pass 7 data [ 199 ] used
dedicated event classes with specific data cuts to minimize such
contributions. Such an analysis is beyond the scope of this study, as
our primary focus is on the PS populations. We simply caution the reader
that the derived intensity for the smooth isotropic component in the
high-energy analyses is subject to potentially large contamination.

It is possible to make stronger statements about the best-fit
source-count distribution at high energies if we consider the
wide-energy bin from 50–2000 GeV. The results are shown in the left
panel of Fig. 3.9 . Due to the improved statistics, the uncertainties on
the source-count distribution are smaller than those for the three
sub-bins. Other than the low-flux sensitivity break, the NPTF finds no
preference for an additional break. The intermediate-flux break, @xmath
, is essentially unconstrained as a result, and the power-law slope
above (below) it are consistent within uncertainties: @xmath and @xmath
, respectively. We compare this result to the best-fit source-count
distribution (blue line) published by Fermi for sources in this same
energy range [ 163 ] . There are important differences between the two
analyses. In the Fermi study, simulated maps were created using several
different source-count distributions, parametrized as singly broken
power laws. The histogram of the photon-count distribution for each of
these maps, averaged over the full region of interest, was compared to
the actual data, and a fit was done to select the simulated maps that
most closely resembled the data. This method is related to but in many
ways distinct from the NPTF. The NPTF considers the difference between
Poissonian and non-Poissonian photon probability distributions at the
pixel-by-pixel level, instead of averaging the distributions over the
full region. Moreover, in our analysis we rely on semi-analytic
techniques to calculate the photon-count probability distributions as we
scan over the space of model parameters, instead of relying on Monte
Carlo samples to numerically construct these distributions. As a result,
we are able to consider source-count distributions with additional
degrees of freedom and also scan over the normalizations of all of the
background templates, which tend to be well determined given the
pixel-by-pixel nature of the fit. In contrast, the intensity of all
Poissonian models in [ 163 ] , including the smooth isotropic emission,
was kept fixed while scanning over the source-count distribution degrees
of freedom.

The cumulative source-count plot is provided in the right panel of Fig.
3.9 . Our result is in good agreement with the 2FHL sources above the
catalog sensitivity threshold @xmath @xmath ph cm @xmath s @xmath . In
the first few flux bins above this threshold, there appear to be more
2FHL sources than what is predicted by the NPTF, although the results
are still consistent within uncertainties. This may be due to the
Eddington bias [ 273 ] where extra sources are observed above threshold
due to upward statistical fluctuations from sources immediately below.

Based on the results in Fig. 3.9 , we can project the expected number of
these sources that may be observed by the Cherenkov Telescope Array
(CTA) [ 246 , 247 ] . For energies above 50 GeV, the CTA flux
sensitivity is @xmath cm @xmath s @xmath for 50 hours of observation per
field-of-view (5 @xmath detection). ^(*∥) ^(*∥) *∥
https://portal.cta-observatory.org/Pages/CTA-Performance.aspx For 250
hours total of observation time, this covers @xmath 190 deg @xmath of
sky, assuming a @xmath field-of-view. As shown in Fig. 3.9 , the NPTF
predicts a density of @xmath deg @xmath for sources above this
threshold. This translates to @xmath detected sources, more than double
what had previously been estimated for similar observing parameters [
247 ] . Relaxing the observing time per source and assuming, as in [ 163
] , that a quarter of the sky is surveyed in 240 hours at 5mCrab
sensitivity, then the NPTF predicts @xmath sources. This is lower, and
in slight tension, with the @xmath sources predicted by the Fermi study
using the blue source-count distribution illustrated in Fig. 3.9 .

### 3.6 Discussion and Conclusions

The primary focus of this chapter is to characterize the properties of
the PSs contributing to the EGB in a data-driven manner. To achieve
this, we use a novel analysis method, referred to as Non-Poissonian
Template Fitting (NPTF), which takes advantage of photon-count
statistics to distinguish diffuse and PS contributions to gamma-ray maps
with non-trivial spatial variations. We presented the NPTF results on
Fermi Pass 8 data at low (1.89–94.9 GeV, @xmath ) and high (50–2000 GeV,
@xmath ) energies. For the first time, the intensity and source-count
distributions for the isotropic PSs have been obtained as a function of
energy, up to 2 TeV. The best-fit source-count distributions probe
fluxes below the current detection threshold for the Fermi 3FGL and 2FHL
catalogs, providing information on the unresolved populations.

Through extensive studies of how the NPTF responds to simulated
populations, we have shown that the analysis procedure reproduces the
properties of input source classes. Therefore, the features of the
best-fit source-count distributions obtained from the data provide a
potential wealth of information about the source populations of the EGB.
While a detailed interpretation of the source-count distributions in
terms of particular theoretical models is beyond the scope of this
study, several important trends were observed.

In this chapter, the source-count distributions are parametrized as
triply-broken power laws in the NPTF. At all energies, a break is fit at
low (high) fluxes, below (above) which the analysis method loses
sensitivity. Of particular interest is whether an additional break,
@xmath , is preferred at intermediate flux. We find a break in the
lowest energy bin (1.89–4.75 GeV) at @xmath cm @xmath s @xmath with
slope @xmath above and @xmath below. In the subsequent two energy bins,
4.75–11.9 GeV and 11.9–30.0 GeV, there is a mild indication that the
source-count distribution hardens below the intermediate flux break,
though the change in slope is not as robust and significant as in the
lowest energy bin. At higher energies, above @xmath 30 GeV, there is no
indication that the source-count distribution changes slope at the
intermediate break. This trend is in line with the expectations from the
blazar simulations in Sec. 3.3 . For example, in both Figs. 3.1 and
LABEL:fig:bl2dnds , which show the results of the NPTF run on simulated
data with the Blazar–1 and Blazar–2 models, we find evidence for
curvature in the source-count distribution at intermediate fluxes in the
lowest energy bins, while at higher energies the recovered source-count
distribution appears as a single power law at fluxes above the
sensitivity threshold of the NPTF. In the energy bin from 50–2000 GeV
the best-fit value for @xmath is essentially unconstrained and the
slopes above and below it are consistent within uncertainties: @xmath
and @xmath .

The NPTF also provides the best-fit intensities for the isotropic-PS
populations as a function of energy. Figure 3.10 illustrates this
spectrum for analyses done using the ultracleanveto event class. The
filled red circles (open red boxes) show the results for the dedicated
low (high)-energy analysis, with PSF1–3 data used at low energies and
PSF0–3 data at high energies. For comparison, the Fermi EGB spectrum is
shown by the black line [ 199 ] . This corresponds to the best-fit
intensity using the Model A diffuse background from that study. To
illustrate the systematic uncertainty on this curve, we also plot the
spectra for diffuse models B and C (dashed and dotted, respectively).

The PS fraction, defined as @xmath , is provided in Tab. 3.4 for each
energy bin. While using the EGB intensity derived in this study
(‘Scenario A’) is the most self-consistent comparison, this may
underestimate the PS contribution above @xmath 100 GeV, where the NPTF
appears to recover too much smooth isotropic emission due to increased
cosmic-ray contamination in the data sets used, as already discussed.
Therefore, we also show the PS fractions calculated relative to the
Fermi EGB intensity from [ 199 ] for diffuse model A (‘Scenario B’). The
comparison to the EGB as measured in [ 199 ] is not fully self
consistent, since, for example, the foreground modeling and data sets in
[ 199 ] differ from those used in this study to measure @xmath .
However, the advantage of this comparison is that the Fermi analysis
uses special event-quality cuts to mitigate contamination, and thus
their measure of @xmath is likely more faithful than that presented in
this study. These results are shown in the second row of Tab. 3.4 . For
the low-energy analysis, the PS fractions are consistent, within
uncertainties, when @xmath is taken from our study or Fermi ’s. ^(***)
^(***) *** For ‘Scenario B’, the quoted uncertainties only include those
measured in this work for @xmath . For @xmath , we use the best-fit
value given in [ 199 ] . The substantial differences occur at
high-energies, where our result is systematically lower than the
fractions based on Fermi ’s EGB intensity.

In general, we find that approximately 50–70% of the EGB consists of PSs
in the energy ranges considered. To interpret these results, we use the
ratios @xmath obtained in the simulation studies of Sec. 3.3 . In that
section, we showed that the efficiency for the NPTF to recover the flux
for the Blazar–2 model (with PSF1–3) is @xmath 100% in the first energy
bin and drops to @xmath 60% in the fourth energy bin. For the Blazar–1
model, the efficiencies are consistently higher than the Blazar–2
scenario. These two blazar models are meant to illustrate extreme
scenarios, with the Blazar–1 model having a significant fraction of the
total flux arising from high-flux sources, while low-flux sources
dominate instead in the Blazar–2 case. The high efficiency of the NPTF
to recover the blazar component at low energies, combined with the PS
fractions observed in the data (Tab. 3.4 ), clearly suggests that there
is a substantial non-blazar component of the EGB up to energies @xmath
30 GeV. The interpretation of the results in the energy bin from
30.0–94.9 GeV is less clear. A proper interpretation of the results at
higher energies in terms of evidence for or against a non-blazar
component of the EGB requires dedicated blazar simulations, which we
leave to future work.

Our results tend to predict fewer PSs (and photons from PSs) where we do
overlap with previous studies. For example, a similar photon-count
analysis was used by [ 162 ] to study 1–10 GeV energies in the Pass 7
Reprocessed data. They found an @xmath 80% PS fraction at these
energies. At the lowest energies that we probe—which admittedly do not
extend down as low as @xmath 1 GeV—we only find a @xmath 54% PS fraction
(relative to Model A). Systematic uncertainties, as shown in Fig. 3.6 ,
can affect the recovered PS intensities at the @xmath level, which can
partially alleviate the tension between our results.

Above 50 GeV, the NPTF procedure predicts that @xmath of the EGB
consists of PSs, with systematic uncertainties estimated at
approximately @xmath . This fraction is smaller, and in slight tension,
with the predicted value @xmath obtained in previous work [ 163 ] . The
fact that our results suggest that there is more diffuse isotropic
emission at high energies may help alleviate the tension between [ 163 ]
and the hadronuclear ( @xmath ) interpretation of IceCube’s PeV
neutrinos [ 230 ] . Some models suggest, for example, that these
very-high-energy neutrinos are produced in hadronuclear interactions,
along with high-energy gamma-rays that would contribute to the IGRB [
230 , 224 , 244 , 229 ] . If the smooth isotropic gamma-ray spectrum (
i.e. , the non-blazar spectrum) is suppressed above 50 GeV in the Fermi
data, it could put such scenarios in tension with the data [ 171 , 274 ]
; however, that does not necessarily appear to be the case given the
results of our analysis [ 172 ] . With that said, and as already
mentioned, dedicated blazar simulations at high energies are needed to
properly interpret our results at these energies.

The PS spectrum in Fig. 3.10 is well-modeled (reduced @xmath ) as a
power law with an exponential cut-off:

  -- -- -- --------
           (3.13)
  -- -- -- --------

where @xmath GeV @xmath cm @xmath s @xmath sr @xmath , @xmath , and
@xmath GeV are the best-fit parameters. ^(*††) ^(*††) *†† Repeating the
fit using the results from the NPTF analyses with source data returns
similar results, though the PS spectrum is slightly enhanced relative to
the ultracleanveto result. In particular, with source data, we find
@xmath GeV @xmath cm @xmath s @xmath sr @xmath , @xmath , and @xmath
GeV, with reduced @xmath . Note that the fit is done taking into account
the uncertainties on the PS intensities in the energy sub-bins. The
global fit for the PS spectrum is shown in Fig. 3.10 by the red band,
which denotes the 68% credible interval. Interestingly, the index @xmath
and cut-off @xmath that we extract from the fit are very similar to the
values found in [ 199 ] , which used the same functional form to fit the
EGB spectrum. Subtracting our PS spectrum from the EGB spectral fits
gives the blue band in Fig. 3.10 . The band includes statistical
uncertainties from our global fit as well as systematic uncertainties
associated with varying between Models A-C. The blue band is an estimate
of the IGRB spectrum and we compare it to the smooth isotropic spectrum
recovered by the NPTF (blue points). Note that the two are consistent,
within the large uncertainties, below @xmath 100 GeV; above this energy,
our IGRB value is expectedly high.

The NPTF allows us to make statistical statements about the properties
of source populations contributing to the EGB, but at the expense of
identifying the precise locations of these sources. However, it is still
possible to make probabilistic statements about these locations. To do
so, we compare the observed photon count in a given pixel, @xmath , to
the mean expected value, @xmath , without accounting for PSs. To
determine @xmath we include the diffuse background, smooth isotropic
emission, and the Fermi bubbles templates, with normalizations as
determined from the NPTF. The pixel-dependent survival function is
defined as

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

where CDF is the Poisson cumulative distribution function. The smaller
the value of @xmath (or, conversely, the larger the value of @xmath ),
the more probable it is that the pixel contains a PS. Figure 3.11 shows
full-sky maps of @xmath for both low (1.89--94.9 GeV) and high
(50--2000 GeV) energies. ^(*‡‡) ^(*‡‡) *‡‡ Digital versions of these
maps are available upon request. The white circles indicate the presence
of a 3FGL (2FHL) source for the low- (high-)energy map, with the radii
proportional to the predicted photon counts for the sources. There is
good correspondence between the hottest pixels, as determined by @xmath
, and the brightest resolved sources. Pixels that are correspondingly
less “hot” tend to be associated with less-bright 3FGL (or 2FHL)
sources. Of particular interest are the hot pixels not already
identified by the published catalogs. In the region @xmath ( @xmath ) in
the low- (high-)energy analysis, these are likely the sources lending
the most weight to the NPTF below the catalog sensitivity thresholds.
While more sophisticated algorithms are needed to further refine the
candidate source locations, Fig. 3.11 provides a starting point for
identifying the spatial locations of potential new sources to help
guide, for example, future TeV gamma-ray observations and
cross-correlations with other data sets, such as the IceCube
ultra-high-energy neutrinos.

Deciphering the constituents of the EGB remains an important goal in the
study of high-energy gamma-ray astrophysics, with broad implications
extending from the production of PeV neutrinos to signals of dark matter
annihilation or decay. The Fermi LAT has already played an important
role in the discovery of many new sources in the GeV sky. By taking
advantage of the statistical properties of unresolved populations, our
results provide a glimpse at the aggregate properties of the sources
that lie below the detection threshold of these published catalogs and
suggest a wealth of detections for future observatories.

#### 3.6.1 Implication for Dark Matter Annihilation Searches

Pinning down the origin of 50-70% of the extragalactic gamma-ray sky as
being of point source origin narrows down the potential contribution of
more exotic sources such as the integrated emission of annihilating dark
matter in halos around far-away galaxies and clusters. This would lead
to an improvement in constraints on annihilating DM obtained by studying
their contribution to the isotropic gamma-ray background (IGRB), such as
those presented in [ 117 , 118 ] , potentially by a factor of a few.

There are a few drawbacks to this approach, however. The contribution of
relatively nearby halos to an annihilation signal is expected to
dominate due to the late-time clustering of matter (which boosts the
annihilation signal) as well as our favored location in the Local Group
where we are surrounded by halos and clusters of a larger size than
those around a randomly chosen place in the Universe. This fact is not
optimally taken into account in IGRB analyses. Secondly, IGRB analyses
for dark matter annihilation cannot conclusively discover a DM signal
due to the irreducible isotropic background of astrophysical origin –
only constraints on its properties are possible.

In the next part of this thesis, we will systematically build up the
best way to search for extragalactic dark matter annihilation, focusing
on emission from nearby galaxies and clusters. We will develop a
framework to characterize the distribution of nearby extragalactic dark
matter halos (Ch. 4 ) and look for this structure in Fermi data (Ch. 5
).

[]

## Chapter 4 Mapping Extragalactic Dark Matter Annihilation with Galaxy
Surveys

This chapter is based on an edited version of Mapping Extragalactic Dark
Matter Annihilation with Galaxy Surveys: A Systematic Study of Stacked
Group Searches , Phys.Rev. D97 (2018) 063005 [arXiv:1709.00416] with
Mariangela Lisanti, Nicholas Rodd, Benjamin Safdi and Risa Wechsler [
131 ] . The results of this chapter have been presented at the following
conferences and workshops: TeV Particle Astrophysics (TeVPA) 2017 in
Columbus, OH (August 2017), Dark Matter, Neutrinos and their Connection
(DA @xmath CO) in Odense, Denmark (August 2017), Workshop on Statistical
Challenges in the Search for Dark Matter in Banff, Canada (February
2018) and Recontres de Blois 2018 in Blois, France (June 2018).

### 4.1 Introduction \lettrine

[lines=3]Dark matter (DM) annihilation into visible final states remains
one of the most promising avenues for discovering non-gravitational
interactions in the dark sector. While an individual annihilation event
is rare, the probability of observing it can be maximized by searching
for excess photons in regions of high dark matter density. The center of
the Milky Way is potentially one of the brightest regions of DM
annihilation as seen from Earth, but the astrophysical uncertainties
associated with the baryonic physics at the heart of our Galaxy motivate
exploring other targets. Gamma-ray studies of DM-dominated dwarf
galaxies in the Local Group currently provide some of the most robust
constraints on the annihilation cross section [ 89 , 90 ] . However,
many more potential targets are available beyond the Local Group. This
chapter proposes a new analysis strategy to search for DM emission from
hundreds more DM halos identified in galaxy group catalogs.

A variety of methods have been used to study gamma-ray signatures of
extragalactic DM annihilation, including modeling potential
contributions to the Isotropic Gamma-Ray Background [ 231 , 232 , 233 ,
234 , 43 , 235 , 118 , 120 , 117 , 275 ] , measuring the Fermi
auto-correlation power spectrum [ 276 , 277 , 278 , 279 ] , and
cross-correlating the Fermi data with galaxy counts [ 280 , 281 , 122 ,
121 , 126 , 125 , 124 , 123 ] , cosmic shear [ 282 , 130 , 283 , 284 ,
128 , 127 , 129 ] and lensing of the Cosmic Microwave Background [ 285 ,
275 ] . These methods typically rely on using a probabilistic
distribution of the DM annihilation signal on the sky. Our approach is
more deterministic in nature. In particular, we treat a collection of
known galaxies as seeds for DM halos. The properties of each galaxy—such
as its luminosity and redshift—enable one to deduce the characteristics
of its associated halo and the expected DM-induced gamma-ray flux from
that particular direction in the sky. In this way, we can build a map of
the expected DM annihilation flux that traces the observed distribution
of galaxy groups.

In certain ways, our approach resembles that used in previous studies of
DM annihilation from individual galaxy clusters. For example, most
recently the Andromeda galaxy [ 286 ] and Virgo cluster [ 116 ] have
been the subject of dedicated study by the Fermi Collaboration. Other
work has inferred the properties of the DM halos associated with galaxy
clusters detected in X-rays [ 287 , 288 , 289 , 290 , 291 , 292 , 293 ]
. Most of these studies focused on a small number of galaxy clusters and
obtained DM sensitivities weaker than those from dwarf galaxies.

Recent advancements in the development of galaxy group catalogs allow us
to now build a full-sky map of the nearby galaxies that should be the
brightest DM gamma-ray emitters. Catalogs based primarily on the 2MASS
Redshift Survey (2MRS) [ 294 ] provide an unprecedented amount of
information regarding a group’s constituents and halo properties [ 132 ,
133 , 295 ] . This information allows us to build a list of the
brightest extragalactic DM targets on the sky and to perform a stacked
analysis for gamma-ray emission from them. A gamma-ray line search using
this methodology was recently performed by Ref. [ 296 ] . Our focus is
on continuum DM signatures, which carry considerably more complications
in terms of the treatment of astrophysical backgrounds.

In the upcoming Chapter 5 , we present results implementing a stacked
analysis of the group catalogs from Ref. [ 132 , 133 ] on Fermi data and
show explicitly that this method yields competitive sensitivity to the
dwarf searches. Here, we present the full details of the analysis method
and a thorough discussion of the systematic uncertainties involved in
deducing the DM-induced flux associated with a given galaxy group. To
fully understand these uncertainties, we apply these methods on mock
data where it is possible to compare the inferred DM properties to their
true values. For this purpose, we use the DarkSky cosmological @xmath
-body simulation [ 297 , 298 ] and an associated galaxy catalog from
Ref. [ 298 ] . We emphasize that, while we illustrate the analysis
method on gamma-ray data, it can also be applied to other wavelengths
and even other messengers, such as neutrinos.

This chapter is organized as follows. In Sec. 4.2 , we describe how to
build DM annihilation flux maps starting from a galaxy group catalog and
discuss the associated systematic uncertainties. Sec. 4.3 presents a
detailed description of the statistical methods that we follow to
implement the stacking. We show the results of applying the
limit-setting and signal recovery procedures on mock data in Sec. 4.4
and conclude in Sec. 4.5 . Appendix A provides a detailed discussion of
the @xmath -factor expressions used in the main text.

### 4.2 Tracing Dark Matter Flux with Galaxy Surveys

In this Section, we describe how to construct catalogs of extragalactic
DM targets starting from a list of galaxy groups. We begin by reviewing
the properties of the galaxy group catalogs and then describe how to
predict the DM signal from a given galaxy group and quantify the
systematic uncertainties of this extrapolation.

#### 4.2.1 Galaxy and Halo Catalogs

The approach that we use throughout this work relies on galaxy surveys
as an input. Different galaxy catalogs span a range of redshifts and
luminosities. Optimal catalogs for DM searches should cover as much of
the sky as possible (to increase statistics) and sample low redshifts (
@xmath ). The strength of the DM signal increases at lower redshifts due
to accretion of mass at late times, affecting both the halo mass
distribution and substructure [ 122 ] . In contrast, the integrated
gamma-ray flux of standard astrophysical sources, such as Active
Galactic Nuclei and star-forming galaxies, is expected to peak at higher
redshifts between @xmath 0.1 and @xmath 2 depending on the specific
source class and model for its unresolved contribution [ 122 , 126 ] .

The Two Micron All-Sky Survey Extended Sources Catalog (2MASS XSC) [ 299
, 294 ] satisfies the criteria listed above and has been used
extensively in past cross-correlation studies [ 121 , 122 , 123 , 124 ,
125 , 281 , 126 ] . The XSC is an all-sky infrared survey that consists
of approximately one million galaxies up to a limiting magnitude of
@xmath mag. Several redshift surveys based on the 2MASS XSC map the
redshifts associated with these galaxies. The 2MRS [ 294 ] , for
example, samples about 45,000 galaxies in the 2MASS XSC with redshifts
to a limiting magnitude of @xmath mag. This corresponds to a nearly
complete galaxy sample up to redshifts of @xmath , which is ideal for DM
studies.

Galaxies from large surveys such as 2MASS can be organized into group
catalogs. A group of gravitationally-bound galaxies shares a DM host
halo. The brightest galaxy in the group is referred to as the central
galaxy; the additional galaxies are bound satellites surrounded by their
own subhalos. As we will see, the total luminosity of the galaxies in
the group is a good predictor of the mass of the DM host halo. A variety
of group finders have been developed and applied to the 2MASS data set [
132 , 295 , 133 ] , using the 2MRS which adds information in the
redshift dimension. The groups in these catalogs range from cluster
scales with @xmath 190 members and associated halo masses of @xmath 10
@xmath M @xmath , down to much smaller systems with only a single
member. Galaxy group catalogs are especially relevant for the present
study, since (as will be shown) halo properties tend to be correlated
with properties of galaxy groups rather than those of individual
galaxies.

While in the upcoming Chapter 5 we use information from the 2MASS group
catalogs in the analysis of Fermi data, we focus on a catalog of
simulated galaxies and halos here. We use the DarkSky-400 cosmological
@xmath -body simulation (version ds14_i ) [ 297 , 298 ] and an
associated @xmath -band galaxy catalog. Using the code 2hot [ 300 ] ,
DarkSky-400 follows the evolution of @xmath particles (DM-only) of mass
@xmath M @xmath in a box 400 Mpc @xmath per side. Initial perturbations
are tracked from @xmath to today, assuming @xmath . The halo catalog was
generated using the Rockstar halo finder [ 301 , 298 ] . Crucially, the
simulation covers the relevant redshift space for DM studies. ^(*) ^(*)
* The snapshot of the simulation analyzed in this work is taken at
@xmath , but we will refer to distance using redshift because that is
the more appropriate language when applied to real data. In particular,
an observer at the center of the simulation box has a complete sample of
galaxies out to @xmath , with the furthest galaxies extending out to
@xmath . In our work, we only consider groups located within @xmath ,
which is the approximate redshift cutoff of the catalogs in Ref. [ 132 ,
133 , 295 ] . We include only well-resolved halos in our analysis by
imposing a lower cut-off of @xmath M @xmath on the mass of included host
halos. The associated galaxy catalog is generated using the abundance
matching technique following Ref. [ 302 , 303 ] with luminosity function
and two-point correlation measurements from the Sloan Digital Sky Survey
(SDSS). Specifically, the @xmath model from Ref. [ 298 ] is used, which
was shown to provide the best fit to SDSS two-point clustering. The
DarkSky galaxy catalog contains the same information that would be found
in, e.g. the 2MASS galaxy catalog and associated group catalogs, such as
individual galaxy luminosities and sky locations.

Figure 4.1 shows a sky map of the galaxy counts in DarkSky up to @xmath
for an observer at the center of the simulation box. It is a HEALPix [
186 ] map with resolution nside=128 . To first approximation, the
galaxies are isotropically distributed throughout the sky. However,
regions of higher and lower galaxy density are clearly visible. Note
that this is shown for a particular sky realization and placing the
observer in different parts of the DarkSky box would change the regions
of contrasting galaxy density.

#### 4.2.2 Dark Matter Annihilation Flux Map

One can predict the DM annihilation flux associated with a halo that
surrounds a given galaxy group. This requires knowing the halo’s
properties, including its mass and concentration. In this subsection, we
discuss how to determine the flux when the halo’s properties are known
exactly. Then, in the following subsection, we consider how to
generalize the results to the more realistic scenario where the halo
properties have to be inferred.

Each halo in DarkSky is fit by the Rockstar halo finder with a
Navarro-Frenk-White (NFW) distribution [ 304 ] of the form

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is the scale radius and @xmath is the normalization. The
NFW parameters are determined from the parameters that are provided for
each DM halo—specifically, its redshift @xmath , virial mass @xmath ,
virial radius @xmath , and virial concentration parameter @xmath .

In the simplest scenarios, the annihilation flux factorizes as

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath is the photon energy and @xmath ( @xmath ) encodes the
particle physics (astrophysical) dependence. The particle physics
contribution is given by

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath is the DM mass, @xmath is its annihilation cross section,
@xmath is its branching fraction to the @xmath annihilation channel,
@xmath is the photon energy distribution in this channel, which is
modeled using PPPC4DMID [ 88 ] , and @xmath is the redshift. We consider
the case of annihilation into the @xmath channel as a generic example of
a continuum spectrum. Of course, the exact limits will vary for
different spectra, and one should consider a range of final states when
applying the method to data, or use model independent-approaches (see,
e.g. , Ref. [ 305 , 306 ] ).

The @xmath -factor is defined as the integral along the line-of-sight of
the squared DM density of the observed object: ^(†) ^(†) † As defined,
the @xmath -factor has units of [ @xmath ]. This definition is
convenient for extragalactic objects, but beware because another common
definition of the @xmath -factor involves dividing out by a solid angle
to remove the units of @xmath . A detailed discussion of the units is
provided in Appendix A .

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath is the line-of-sight distance and @xmath is the so-called
boost factor. The boost factor accounts for the enhancement in the flux
due to the annihilation in DM substructure (subhalos, subhalos within
subhalos and so on…), and is usually the dominant source of systematic
uncertainty in extragalactic DM annihilation studies. For the case of
extragalactic objects, one can obtain a closed form solution that is an
excellent approximation to the integral in Eq. 4.4 , which is
proportional to

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

where @xmath is the comoving distance (a function of redshift, @xmath ),
@xmath is the critical density, and @xmath is the concentration. In our
analysis, we calculate the @xmath -factor exactly, but the scaling
illustrated in Eq. 4.5 is useful for understanding the dependence of
@xmath on the halo mass and concentration. The derivation of the @xmath
-factor expression is reviewed in detail in Appendix A , where we also
show the result for the Burkert profile.

Figure 4.1 illustrates the truth @xmath -factor map associated with
DarkSky , obtained by putting the observer in the center of the
simulation box. This map is constructed by applying Eq. 4.4 to all host
halos in the DarkSky catalog and using the boost model from Ref. [ 307 ]
to describe the contribution from substructure. Once the @xmath -factors
are known, the expected photon counts per pixel can be determined using
Eq. 4.2 and Fermi ’s exposure map. This is also shown in Fig. 4.1 ,
assuming a DM particle with @xmath GeV that annihilates to @xmath with
@xmath cm @xmath s @xmath . Not all the pixels that contain one or more
galaxies correspond to significant regions of DM annihilation. The DM
annihilation flux is largest for the most massive, concentrated, and/or
closest galaxy groups.

Note that when constructing Fig. 4.1 , we perform the angular integrals
in Eq. 4.4 as a function of angular extent, @xmath . In doing so, we
implicitly assume that the boost factor is simply a multiplicative
factor. In reality, the boost factor likely broadens the angular
profile, because the subhalo annihilation should extend further away
from the halo center. However, since the angular extent of the
annihilation in most halos is small compared to the instrument
point-spread function (PSF), we do not model this extension here. Some
nearby halos may have significantly larger angular extent, as would be
expected for the Andromeda galaxy. Nevertheless, such considerations
need to be made case by case and are discussed in detail in the next
chapter, where we choose to exclude Andromeda due to its size.

Figure 4.2 is a heatmap representing the average @xmath -factor, for a
given @xmath and @xmath , of the DarkSky halos in the above
configuration. The halos span a wide range of masses and redshifts, with
@xmath -factors averaging over several orders of magnitude from @xmath
10 @xmath . The largest @xmath -factors are observed for the most
massive, cluster-sized halos at @xmath 0.01–0.02, as well as for
less-massive halos at smaller redshifts ( @xmath ).

#### 4.2.3 Uncertainties in Halo Modeling

Now, we consider more carefully the systematic uncertainties associated
with modeling the halo properties. A halo with an NFW density profile
has a @xmath -factor dictated by its parameters as given in Eq. 4.5 . In
addition to the distance, the @xmath -factor also depends on the virial
mass and concentration. ^(‡) ^(‡) ‡ Note that uncertainties on the halo
redshift also feed into the @xmath -factor. However, we consider this
uncertainty to be subdominant for spectroscopically determined
redshifts. For nearby halos, where the relation between distance and
redshift is nontrivial, the uncertainty on the distance can be
noticeably larger, and as high as @xmath 5% [ 308 ] . Nonetheless, even
such uncertainties are considerably smaller than those associated with
the mass and concentration, and so we do not consider them. Therefore,
any uncertainty in the determination of these halo properties is
propagated through to the uncertainty on the DM annihilation flux. Up
until now, we have taken the halo mass and concentration directly from
DarkSky , but in practice these parameters need to be inferred from
properties of the observed galaxy groups.

Within DarkSky , the halo mass can be inferred from the absolute
luminosity of its associated galaxy group. We obtain a deterministic
@xmath relation following a procedure similar to that in Ref. [ 309 ] ,
which derived a phenomenological relation between the @xmath -band
galaxy luminosity and the mass of its DM halo. The left panel of Fig.
4.3 shows the true masses for the DarkSky halos, as a function of
central galaxy luminosity (green) or the total luminosity, which
includes the luminosity of the satellite galaxies (red). The DarkSky
catalog provides the associations for all galaxies, central and
satellite, so we include all satellites that are associated to the group
when calculating the total absolute luminosity. This is similar to what
is done in published group catalogs [ 132 , 133 , 295 ] , where they
account for the loss in luminosity of satellite galaxies that are
farther away.

From Fig. 4.3 , we see that the spread in the associated halo mass
increases above @xmath L @xmath , up to the brightest galaxy at @xmath L
@xmath , when the central galaxy luminosity is used. In contrast, the
spread is significantly smaller when the total luminosity is used,
making it a better predictor for the halo mass. As demonstrated in the
right panel of Fig. 4.3 , including the satellite luminosities allows
one to better reconstruct the halo mass. Therefore, we use the median
@xmath relation thus obtained as our fiducial case to infer the central
mass estimate, and we use the spread in the @xmath relation to infer the
uncertainty on the mass. Note that the @xmath relation shown in Fig. 4.3
is constructed by binning the DarkSky data in luminosity and calculating
the 16, 50, and 84 percentiles in @xmath ; different results would be
obtained by binning in @xmath and then constructing the percentiles from
the luminosity distributions. This procedure is similar to that adopted
by galaxy group catalogs to infer the halo mass [ 132 , 295 , 133 ] .
Using this @xmath relation, we can infer the halo mass and uncertainty
for each galaxy-group host halo in DarkSky .

DM halos of the same mass can have very different characteristics,
usually reflecting their distinct formation history and environment. One
such characteristic is the halo’s virial concentration @xmath . The
scale radius is the relevant quantity to compare to as it indicates an
isothermal slope for the density profile, which is required for a flat
rotation curve. The virial radius corresponds to the spherical volume
within which the mean density is @xmath times the critical density of
the Universe at that redshift. We use @xmath with @xmath in accordance
with Ref. [ 310 ] . The cosmology associated with the DarkSky simulation
is used throughout, with @xmath , @xmath and @xmath .

In general, the concentration correlates strongly with halo mass due to
the dependence of halo formation time on mass—on average, lower mass
halos tend to be more concentrated because they collapsed earlier, when
the Universe was denser. For the same reason, the concentration is
sensitive to the cosmology, which determines how early halos start to
assemble. The concentration of field halos has been extensively studied
and several concentration-mass relations have been proposed in the
literature, usually based on @xmath -body simulations or physically
motivated analytic approaches [ 316 , 313 , 139 , 312 , 311 , 314 , 317
, 318 ] . In the left panel of Fig. 4.4 , we show the median value of
the concentration-mass relation derived directly from the DarkSky
simulation, as well as the middle 68 and 95% spread. The middle 68%
scatter in the relation is typically in the range 0.14-0.19 across the
halo mass range considered. For comparison, we also show several
concentration models that are commonly used in the literature. As is
standard in the literature [ 110 , 139 ] , we model the uncertainty in
the concentration, for a given virial mass, as a log-normal distribution
around its median value.

To summarize, it is possible to infer the halo mass from the luminosity
of the galaxy group and to then obtain the concentration. The final
remaining property that is needed to solve for the @xmath -factor in Eq.
4.4 is the boost factor, which depends on the distribution and minimum
cutoff of the subhalos’ mass. The boost factor encapsulates the
complicated dependence of the subhalo mass distribution on both the
particle physics assumptions of the DM model as well as the dynamics of
the host halo formation. A variety of different boost models typically
used in the literature are illustrated in the right panel of Fig. 4.4 .
As our fiducial case, we adopt the boost model of Ref. [ 307 ] (labeled
as ‘Bartels Boost Model’), which self-consistently accounts for the
concentration-mass relation of subhalos (compared to field halos) as
well as the effects of tidal stripping. Specifically, in the subhalo
mass function @xmath , we use a minimum subhalo mass cutoff of @xmath M
@xmath and slope @xmath that varies self-consistently with host halo
mass while accounting for evolution effects (see Ref. [ 307 ] for
details).

We have now built up a framework that allows us to determine the
expected DM annihilation flux map associated with a catalog of galaxy
groups. Next, we show how to use this information to search for signals
of DM from hundreds of galaxy groups.

### 4.3 Statistical Methods

In this work, we introduce and study a statistical procedure to search
for gamma-ray signals from DM by stacking galaxy groups. All analyses
discussed here are run on mock data, which is based on the expected
astrophysical contributions to the real Fermi data set. When building
this mock data set, we include contributions from (1) the diffuse
emission, for which we use the Fermi Collaboration’s p7v6 model; (2)
isotropic emission; (3) emission from the Fermi Bubbles [ 140 ] ; and
(4) emission from point sources in the Fermi 3FGL catalog [ 115 ] . The
overall flux normalization for each component must be known a priori to
create the mock data. To obtain this, we fit spatial maps of (1)–(4)
above to the actual Fermi data. We use 413 weeks of UltracleanVeto (all
PSF quartile) Pass 8 data collected between August 4, 2008 and July 7,
2016. We break the data into 40 equally log-spaced energy bins between
200 MeV and 2 TeV, applying the recommended data cuts: zenith angle
@xmath , DATA_QUAL @xmath , and LAT_CONFIG @xmath . To minimize the
Galactic contamination in this initial fit, we mask the region @xmath as
well as the 68% containment radius for the 300 brightest and most
variable sources in the 3FGL catalog. We emphasize that these masks are
only used when creating the mock data and not in the stacked analysis.
The fitting procedure described here provides the expected astrophysical
background contribution from the real data. Monte Carlo (MC) is then
generated by summing up these contributions and taking a Poisson draw
from the resulting map. In the following discussion, we will show how
results vary over different MC realizations of the mock data as a
demonstration of Poisson fluctuations in the photon distribution.

We now describe in detail the statistical procedure we employ to
implement the stacking analysis on the mock data. We perform a
template-fitting profile likelihood analysis in a 10 @xmath
region-of-interest (ROI) around each group. Template studies amount to
describing the sky by a series of spatial maps (called templates). The
normalization of each template is proportional to its relative gamma-ray
flux. We use five templates in our study. The first four are associated
with the known astrophysical sources (1)–(4) described above. Within
@xmath of the halo center, we independently float the normalization of
each 3FGL source. ^(§) ^(§) § The results do not change when floating
all the point sources together as one combined template. This can
potentially cause problems when implemented on data, however, because
the 3FGL normalizations can be erroneous in certain energy bins.
Allowing the normalizations of the sources to float separately helps to
mitigate this potential problem. Sources outside this region may
potentially contribute within the ROI because of the tails of the Fermi
PSF. Therefore, between 10 @xmath and 18 @xmath of the halo center, we
float the sources as a single template. The fifth and final template
that we include is associated with the expected DM annihilation flux for
the halo, which is effectively a map of the @xmath -factor and is
described in Sec. 4.2 . Note that all templates have been carefully
smoothed using the Fermi PSF. The diffuse model is smoothed with the
Fermi Science Tools, whereas other templates are smoothed according to
the instrument response function using custom routines. Mismodeling the
smoothing of either the point sources or individual halos can
potentially impact the results.

A given mock data set, @xmath , is divided into 40 log-spaced energy
bins indexed by @xmath . Each energy bin is then spatially binned using
HEALPix [ 186 ] with nside =128 and individual pixels indexed by @xmath
. In this way, the full data set is reduced to a two-dimensional array
of integers @xmath describing the number of photons in energy bin @xmath
and pixel @xmath . For a given halo, indexed by @xmath , only a subset
of all the pixels in its vicinity are relevant. In particular, the
relevant pixels are those with centers within 10 @xmath of the object.
Restricting to these pixels leaves a subset of the data, which we denote
by @xmath . Template fitting dictates that this data is described with a
set of spatial templates binned in the same way as the data, which we
label as @xmath , where @xmath indexes the different templates
considered. The number of counts in a given pixel, energy bin, and
region consists of a combination of these templates:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

Here, @xmath represents the set of model parameters. For Poissonian
template fitting, these are given by the normalizations of the templates
@xmath , i.e. , @xmath . Note that the template normalizations have an
energy but not a spatial index, as the templates have an independent
degree of freedom in each energy bin as written, but the spatial
distribution of the model is fixed by the shapes of the templates
themselves. In principle, we could also remove this freedom in the
relative emission across energy bins, because we have models for the
spectra of the various background components, and in particular DM.
Nevertheless, we still allow the template normalizations to float
independently in each energy bin for the various backgrounds. This is
more conservative than assuming a model for the background spectra, and
in particular we can use the shape of the derived spectra as a check
that the dominant background components are being correctly modeled. The
spectral shape of the DM forms part of our model prediction, however,
and once we pick a final state such as annihilation to two @xmath
-quarks, we fix the relative emission between the energy bins.

As we assume that the data comes from a Poisson draw of the model, the
appropriate likelihood in energy bin @xmath and ROI @xmath is

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

Of the templates that enter this likelihood, there are some we are more
interested in than others. In particular, we care about the the DM
annihilation intensity, which we denote as @xmath . We treat the
normalizations of the templates associated with the known astrophysical
emission as nuisance parameters, @xmath . Below, we will describe how to
remove the nuisance parameters to reduce Eq. 4.7 to a likelihood profile
that depends only on the DM annihilation intensity, but for now we have
@xmath .

Importantly, the nuisance parameters have different values between ROIs,
but the DM parameters do not. This is because the DM parameters, such as
the DM mass, annihilation rate, and set of final states, are universal,
while the parameters that describe the astrophysical emission can vary
from region to region. We do, however, profile over the @xmath -factor
uncertainty in each ROI. Explicitly, each halo is given a model
parameter @xmath , which is described by a log-normal distribution
around the central value @xmath with width @xmath , both of which depend
on the object and hence ROI considered. The @xmath -factor error, @xmath
, is determined by propagating the errors associated with the mass and
concentration of a given halo. To account for this, we append the
following addition onto our likelihood as follows:

  -- -- -------- -------- -------
        @xmath            (4.8)
                 @xmath   
  -- -- -------- -------- -------

Note that this procedure does not account for any systematic
uncertainties that can bias the determination of the @xmath -factor.

The nuisance parameter @xmath can now be eliminated via the profile
likelihood—see Ref. [ 319 ] for a review. Unlike for the other nuisance
parameters, the value of @xmath does not depend on energy and so we
eliminate the energy-dependent parameters first:

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

The full implementation of the profile likelihood method as suggested by
this equation requires determining the maximum likelihood for the @xmath
template coefficients, for every value of @xmath . Nevertheless, an
excellent approximation to the profile likelihood, which is
computationally more tractable, is simply to set the nuisance parameters
to their maximum value obtained in an initial scan where all templates
are floated. ^(¶) ^(¶) ¶ The DM template is only included for energy
bins above 1 GeV. At lower energies, the large Fermi PSF leads to
confusion between the DM, isotropic and point source templates, which
can introduce a spurious preference for the DM template.

Using this approach to determine the likelihood in Eq. 4.9 , we can
build a total likelihood by combining the energy bins. Once this is
done, the likelihood depends on the full set of DM intensities @xmath ,
which are specified by a DM model @xmath , cross section @xmath , mass
@xmath , and @xmath -factor via Eq. 4.2 . Explicitly:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

and recall that unlike the other parameters on the left hand side, the
@xmath -factor not only determines the @xmath , but also enters the
likelihood through the expression in Eq. 4.8 . We emphasize that in this
equation, the DM model and mass specify the spectra, and thereby the
relative weightings of the @xmath , whereas the cross section and @xmath
-factor set the overall scale of the emission.

The remaining step to get the complete likelihood for a given halo
@xmath is to remove @xmath , again using profile-likelihood:

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

This provides the full likelihood for this object as a function of the
DM model parameters. The likelihood for the full stacked catalog is then
simply a product over the individual likelihoods:

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

Using this likelihood, we define a test statistic (TS) profile as
follows:

  -- -------- -------- -------- --------
     @xmath   @xmath            (4.13)
                       @xmath   
  -- -------- -------- -------- --------

where @xmath is the cross section that maximizes the likelihood for that
DM model and mass. From here, we can use this TS, which is always
nonpositive by definition, to set a threshold for limits on the
cross-section. When searching for evidence for a signal, we use an
alternate definition of the test statistic defined as

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

We implement template fitting with the package NPTFit [ 142 ] , which
uses MultiNest [ 181 , 182 ] by default, but we have employed Minuit [
320 ] in our analysis.

### 4.4 Analysis Results

In this Section, we present the results of our analysis on mock data
using the DarkSky galaxy catalog. We begin by describing the sensitivity
estimates associated with this study, commenting on the impact of
statistical as well as systematic uncertainties and studying the effect
of stacking a progressively larger number of halos. Then, we justify the
halo selection criteria that are used by showing that we can recover
injected signals on mock data.

#### 4.4.1 Halo Selection and Limits

We now discuss the results obtained by applying the halo inference
pipeline described in Sec. 4.2 and the statistical analysis described in
Sec. 4.3 to mock gamma-ray data. We focus on the top 1000 galaxy groups
in the DarkSky catalog, as ranked by the inferred @xmath -factors of
their associated halos, placing ourselves at the center of the
simulation box. In addition, we mask regions of the sky associated with
seven large-scale structures that are challenging to model accurately:
the Large and Small Magellanic Clouds, the Orion molecular clouds, the
galaxy NGC5090, the blazar 3C454.3, and the pulsars J1836+5925 and
Geminga. This is done here for simulated data in order to closely track
the analysis that will subsequently be performed on real Fermi data.

While we start from an initial list of 1000 galaxy groups, we do not
include all of them in the stacking procedure. A galaxy group is
excluded if:

1.  it is located within @xmath ;

2.  it is located less than @xmath from the center of another brighter
    group in the catalog;

3.  it has TS @xmath @xmath and @xmath ,

where @xmath is the best-fit cross section at any mass and @xmath is the
best-fit limit set by any halo at the specified DM mass. Note that the
second requirement is applied sequentially to the ranked list of halos,
ordered by @xmath -factor. We now explain the motivation for each of
these requirements separately. The first requirement listed above
removes groups that are located close to the Galactic plane to reduce
contamination from regions of high diffuse emission and the associated
uncertainties in modeling these. The second requirement demands that the
halos be reasonably well-separated, which avoids issues having to do
with overlapping halos and accounting for multiple DM parameters in the
same ROI. The non-overlap criterion of 2 @xmath is chosen based on the
Fermi PSF containment in the lowest energy bins used and on the largest
spatial extent of gamma-ray emission associated with the extended halos,
which collectively drive the possible overlap between nearby halos.

The final requirement excludes a galaxy group if it has an excess of at
least 3 @xmath significance associated with the DM template that is
simultaneously excluded by the other galaxy groups in the sample. This
selection is necessary because we expect that some galaxy groups will
have true cosmic-ray-induced gamma-ray emission from conventional
astrophysics in the real data, unrelated to DM. To identify these
groups, we take advantage of the fact that we are starting from a large
population of halos that are all expected to be bright DM sources in the
presence of a signal. Thus, if one halo sets a strong limit on the
annihilation rate and another halo, at the same time, has a large excess
that is severely in conflict with the limit, then most likely the large
excess is not due to DM. The worry here is that we could have
mis-constructed the @xmath -factor of the halo that gave the strong
limit, so that the real limit is not as strong as we think it is.
However, with the TS @xmath and @xmath criteria outlined above, this
does not appear to be the case. In particular, we find that the criteria
very rarely rejects halos due to statistical fluctuations. For example,
over 50 MC iterations of the mock data, @xmath halos (out of 1000)
remain after applying the TS @xmath and cross section cuts alone, and
the excluded halos tend to have lower @xmath -factors, since there the
@xmath requirement is more readily satisfied.

We expect that this selection criteria will be very important on real
data, however, where real excesses can abound. In addition, as we will
describe in the next subsection, injected signals are not excluded when
the analysis pipeline is run on mock data. In an ideal scenario, we
would attempt to understand the origin of these excesses by correlating
their emission to known astrophysics either individually or
statistically. In the present analysis, however, we take the
conservative approach of removing halos that are robustly inconsistent
with a DM signal and leave a deeper understanding of the underlying
astrophysics to future work.

We apply the procedure outlined in Secs. 4.2 and 4.3 to the mock data to
infer the 95% confidence limit on the DM annihilation cross section. The
resulting sensitivity is shown by the blue dashed line in the left panel
of Fig. 4.5 , which uses the boost factor from Ref. [ 307 ] . For
comparison, we also show the limit assuming no boost factor (red dashed
line); note that the boost factor model that we use provides a modest
@xmath improvement to the limit. Because the limit can potentially vary
over different MC realizations of the mock data, we repeat the procedure
for 100 MCs (associated with different Poisson realizations of the map);
the blue band indicates the middle 68% spread in the limit associated
with this statistical variation.

To see how the limit depends on the observer’s location within the
DarkSky simulation box, we repeat the procedure described above over
nine different locations. ^(∥) ^(∥) ∥ The nine locations we used are at
the following coordinates @xmath Mpc/h in DarkSky : @xmath , @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath . The first
listed location is our default position, and any time we use more than
one location they are selected in order from this list. At each
location, we perform 20 MCs and obtain the median DM limit. The green
band in the left panel of Fig. 4.5 denotes the middle 68% spread on the
median bounds for each of the different sky locations. In general, we
find that the results obtained by an observer at the center of the
DarkSky box are fairly representative, compared to random locations.
Note, however, that this bound does not necessarily reflect the
sensitivity reach one would expect to get with actual Fermi data. The
reason for this is that the locations probed in DarkSky do not resemble
that of the Local Group in detail. We will come back to this point
below, when we compare the @xmath -factors of the DarkSky halos to those
from galaxy catalogs that map the local Universe.

The orange line in the left panel of Fig. 4.5 shows the limit obtained
by requiring that the DM emission from the groups not overproduce the
measured isotropic gamma-ray component [ 199 ] . This should not be
compared to the published DM bounds obtained with the Fermi Isotropic
Gamma-Ray Background [ 117 ] because that study accounts for the
integrated effect of the DM annihilation flux from halos much deeper
than those we consider here. The inclusion of these halos results in a
total flux that can be greater than those from our sample by over an
order of magnitude. Nevertheless, this gives an idea of how much we gain
by resolving the spatial structure of the local DM population and
knowing the locations of the individual galaxy groups.

The right panel of Fig. 4.5 shows the effect of propagating
uncertainties associated with inferring the halo properties. The green
line indicates how the limit improves when no uncertainties are assumed,
i.e. , we can perfectly reconstruct the virial mass and concentration of
the halos. The sensitivity reach improves by roughly a factor of two in
this case. We further show the effect of individually reducing the error
on @xmath (dashed purple line) and @xmath (purple line) by 50%. The
reductions in the uncertainties provide only marginal improvements to
the overall sensitivity, still far below the level of systematic
uncertainty associated with extragalactic analyses in general.

It is interesting to study how the limit scales with the number of
halos, @xmath , included in the stacking procedure. This result is shown
in Fig. 4.6 for @xmath @xmath and @xmath GeV, for four different
observer locations in the simulation box. The dashed red line indicates
the median 95% confidence limit. The red bands are the 2.5, 16, 84 and
97.5 percentiles on the limit, obtained from 100 MC realizations of the
mock data. We observe that the limit typically improves continuously for
the first @xmath 10 halos. As more halos are included in the stacking,
the gains diminish. For some sky locations, the limit simply remains
flat; for others we see some marginal improvements in the bounds. These
results are consistent, within uncertainties, between the DM masses and
the different sky locations of the observer.

We emphasize that the scaling on @xmath can be very different on
applicaton to real data, because the distribution of @xmath -factors in
the random DarkSky locations is not representative of our own
environment in the Local Group and also some halos can have residuals
that are not related to DM but rather to mismodeling or real
cosmic-ray–induced emission from the galaxy groups. The former point is
demonstrated in Fig. 4.7 , where we histogram the top 1000 @xmath
-factors associated with the baseline DarkSky analysis (blue line/band).
For comparison, we also show the distributions corresponding to 2MRS
galaxy group catalogs, specifically the Tully et al. [ 132 , 133 ]
(green line) and the Lu et al. [ 295 ] (red line) catalogs. We see that
the distribution of @xmath -factors for the 2MRS catalogs is skewed
towards higher values compared to that from DarkSky . (Note that the
cut-off at low @xmath -factors is artificial and is simply a result of
including 1000 halos for each catalog.)

The differences in the @xmath -factor distributions can be traced to the
redshift distribution of the galaxy groups, as illustrated in Fig. 4.8 .
We see specifically that the mass function of the top 1000 DarkSky halos
in each of the random sky locations sampled is roughly consistent with
that observed in the 2MRS catalogs. In contrast, the actual catalogs
have more groups at lower @xmath than observed in the random DarkSky
locations.

While a random location in the DarkSky box does not resemble our own
Local Group, we can try to find specific locations in the simulation box
that do. Therefore, we place the observer at ten random Milky Way–like
halos in the simulation box, which have a mass @xmath M @xmath . More
specifically, we select halos with mass @xmath and at least 100 Mpc
@xmath from the box boundaries. The distribution of the top 1000 @xmath
-factors is indicated by the orange line/band in Fig. 4.7 , while the
corresponding mass and redshift distributions are shown in Fig. 4.8 . We
see that the redshift—and, consequently, @xmath -factor—distributions
approach the observations, though the correspondence is still not exact.
A more thorough study could be done assessing the likelihood that an
observer in DarkSky is located at a position that closely resembles the
Local Group. However, as our primary goal here is to outline an analysis
procedure that we can apply to actual data, we simply conclude that our
own local Universe appears to be a richer environment compared to a
random location within the DarkSky simulation box, which bodes well for
studying the actual Fermi data.

#### 4.4.2 Signal Recovery Tests

It is critical that the halo selection criteria described in the
previous section do not exclude a potential DM signal if one were
present. To verify this, we have conducted extensive tests where we
inject a signal into the mock data, pass it through the analysis
pipeline and test our ability to accurately recover its cross section in
the presence of the selection cuts. Figure 4.9 summarizes the results of
the signal injection tests for two different observer locations in the
DarkSky simulation box (top and bottom rows, respectively). We inject a
signal in the mock data that is associated with @xmath annihilation for
three different masses ( @xmath GeV) that traces the DM annihilation
flux map associated with DarkSky . The dashed line in each panel
delineates where the injected cross section, @xmath , matches the
recovered cross section, @xmath .

The green line shows the 95% one-sided limit on the cross section @xmath
found using Eq. 4.13 , with a TS threshold corresponding to @xmath . The
green band shows the 68% containment region on this limit, constructed
from twenty different MC realizations of the mock data set. Importantly,
the limit on @xmath roughly follows—but is slightly weaker than—the
injected signal, up until the maximum sensitivity is reached and smaller
cross sections can no longer be probed. This behavior is generally
consistent between the three DM masses tested and both sky locations. We
clearly see that the limit obtained by the statistical procedure never
excludes an injected signal over the entire cross section range.

Next, we consider the recovered cross section that is associated with
the maximum test statistic, TS @xmath , in the total likelihood. The
blue line in each panel of Fig. 4.9 shows the median value of @xmath
over 20 MCs of the mock data. The blue band spans the median cross
sections associated with @xmath . The inset show the median and 68%
containment region for TS @xmath as a function of the injected cross
section. The maximum test statistic is an indicator for the significance
of the DM model and as such the @xmath distributions are only influenced
by the data at high injected cross sections where TS @xmath has begun to
increase. At lower injected cross sections, the distributions for @xmath
are not meaningful.

Two issues are visible in Fig. 4.9 : (i) at high injected cross
sections, the best-fit recovered cross sections are systematically
around 1 @xmath too high, and (ii) at high DM masses and near-zero
injected cross sections, the distribution of TS @xmath deviates from the
chi-square distribution (which can be seen based on the fact that the TS
@xmath flattens out with a non-zero median value). The first issue stems
from the way we model the @xmath -factor contribution to the likelihood,
while the second arises from the approximations we make to perform the
profile likelihood in a computationally efficient manner.

### 4.5 Conclusions

In this chapter, we introduced a procedure to build a full-sky map of
extragalactic DM targets based on galaxy surveys and demonstrated this
methodology using the DarkSky cosmological simulation. Starting from the
galaxies in the DarkSky catalog, we inferred the properties of their
respective DM halos using the galaxy-halo connection. In so doing, we
identified the halos that are the brightest sources of extragalactic DM
annihilation and which act as the best annihilation targets. This
procedure allows us to account for the fact that not all galaxy groups
are expected to be bright DM emitters; the most massive, concentrated,
and/or most nearby galaxies dominate the signals. By building a map of
extragalactic DM targets, we can focus our search for DM annihilation on
the most relevant regions of sky. This philosophy contrasts with that of
cross-correlation studies, which treat all galaxies as equally good
targets for DM.

With a list of extragalactic DM halos in hand, as well as their inferred
@xmath -factors, we performed a stacked analysis to search for gamma-ray
signatures of DM annihilation in mock data. We described the likelihood
procedure for the stacking analysis in detail. There are two clear
advantages to this approach over, say, a full-sky template study. First,
focusing on smaller regions around each halo significantly reduces the
sensitivity to mis-modeling of the foregrounds. Second, uncertainties on
the predicted DM annihilation flux can be straightforwardly included in
the likelihood function. In particular, we outlined how uncertainties in
the @xmath -factors, which arise from the determination of the virial
mass and concentration, are marginalized over in the analysis.

We presented limits on the DM annihilation cross section for mock data
and, most importantly, demonstrated that the analysis procedure robustly
recovers injected signals. We found that the sensitivity improves by
nearly two orders of magnitude when the structure of extragalactic DM
emission on the sky is accounted for, rather than simply assuming an
isotropic distribution. Typically, the limit is dominated by the
brightest @xmath halos in the stacking, though this varies depending on
the location in the simulation box. The @xmath -factor distribution of
nearby groups in our own Galaxy differs from the random locations
sampled in the DarkSky box, which can change the number of halos that
dominate the limit. In actuality, one would want to continue adding
halos to the analysis—ranked starting from the brightest @xmath
-factors—until the gains in the limit are observed to level off.

One advantage of using the DarkSky simulation in this initial study is
that the truth information for all the halos is known. We can therefore
study how the DM limits improve when the virial mass and concentration
of the halos are known precisely. For this ideal scenario, we find that
that the limits improve by roughly 50% over those obtained by
marginalizing over uncertainties. This suggests that a concrete way to
improve the bounds on DM annihilation is to reduce the uncertainties on
@xmath and @xmath for the brightest halos in the catalog.

The substructure boost factor remains one of the most difficult
systematics to handle. In this work, we use recent boost factor models
that account for tidal stripping of subhalos. This boost factor changes
the limit by an @xmath factor, which is more conservative than other
models sometimes used in extragalactic DM studies. While the boost
factor enhancement is fairly modest, it is still the dominant systematic
uncertainty over the halo mass and concentration.

The analysis outlined in this chapter can be repeated on Fermi data
using published galaxy group catalogs. In particular, the Tully et al.
catalogs [ 132 , 133 ] and the Lu et al. catalog [ 295 ] provide a map
of the galaxy groups in the local Universe within @xmath . Both catalogs
are based primarily on 2MRS, but use different clustering algorithms and
halo mass determinations. Taken together, they provide a way to estimate
the systematic uncertainties associated with the galaxy to halo mapping
procedure. Previous cluster studies on data [ 287 , 288 , 289 , 290 ,
293 ] used the extended HIghest X-ray FLUx Galaxy Cluster Sample
(HIFLUGCS) [ 322 , 323 ] , which includes 106 of the brightest clusters
observed in X-ray with the ROSAT all-sky survey. These clusters cover
redshifts from @xmath ; the distribution of their @xmath -factors,
masses, and redshifts are shown in Fig. 4.7 and 4.8 . In general, the
2MRS catalogs provide a larger number of groups that should be brighter
in DM annihilation flux, so we expect a corresponding improvement in the
sensitivity to annihilation signatures.

The recent advancement of galaxy catalogs based on 2MRS and other nearby
group catalogs allows us for the first time to map out the most
important extragalactic DM targets in the nearby Universe. This, in
turn, enables us to perform a search that focuses on regions of sky
where we expect the DM signals to be the brightest outside the Local
Group. We present the complete results of such an analysis, as applied
to data, in the next chapter.

[]

## Chapter 5 A Search for Dark Matter Annihilation in Galaxy Groups

This chapter is based on an edited version of A Search for Dark Matter
Annihilation in Galaxy Groups , Phys.Rev.Lett. 120 (2018) 101101
[arXiv:1708.09385] with Mariangela Lisanti, Nicholas Rodd and Benjamin
Safdi [ 134 ] . The results of this chapter have been presented at the
following conferences and workshops: TeV Particle Astrophysics (TeVPA)
2017 in Columbus, OH (August 2017), Dark Matter, Neutrinos and their
Connection (DA @xmath CO) in Odense, Denmark (August 2017), Workshop on
Statistical Challenges in the Search for Dark Matter in Banff, Canada
(February 2018) and Recontres de Blois 2018 in Blois, France (June
2018).

### 5.1 Introduction \lettrine

[lines=3]Weakly-interacting massive particles, which acquire their
cosmological abundance through thermal freeze-out in the early Universe,
are leading candidates for dark matter (DM). Such particles can
annihilate into Standard Model states in the late Universe, leading to
striking gamma-ray signatures that can be detected with observatories
such as the Fermi Large Area Telescope. Some of the strongest limits on
the annihilation cross section have been set by searching for excess
gamma-rays in the Milky Way’s dwarf spheroidal satellite galaxies
(dSphs) [ 90 , 89 ] . In this chapter, we present competitive
constraints that are obtained using hundreds of galaxy groups within
@xmath .

Chapter 4 describes the procedure for utilizing galaxy group catalogs in
searches for extragalactic DM. Previous attempts to search for DM
outside the Local Group were broad in scope, but yielded weaker
constraints than the dSph studies. For example, limits on the
annihilation rate were set by requiring that the DM-induced flux not
overproduce the isotropic gamma-ray background [ 117 ] . These bounds
could be improved by further resolving the contribution of sub-threshold
point sources to the isotropic background [ 164 , 165 ] , or by looking
at the auto-correlation spectrum [ 276 , 276 , 278 , 279 ] . A separate
approach involves cross-correlating [ 281 , 122 , 121 , 126 , 125 , 124
, 123 ] the Fermi data with galaxy-count maps constructed from, e.g. ,
the Two Micron All-Sky Survey (2MASS) [ 324 , 299 ] . A positive
cross-correlation was detected with 2MASS galaxy counts [ 126 ] , which
could arise from annihilating DM with mass @xmath @xmath – @xmath GeV
and a near-thermal annihilation rate [ 125 ] . However, other source
classes, such as misaligned Active Galactic Nuclei, could also explain
the signal [ 124 ] .

An alternative to studying the full-sky imprint of extragalactic DM
annihilation is to use individual galaxy clusters [ 287 , 288 , 289 ,
116 , 290 , 291 , 292 , 293 , 296 , 325 ] . Previous analyses along
these lines have looked at a small number of @xmath @xmath – @xmath M
@xmath clusters whose properties were inferred from X-ray measurements [
322 , 323 ] . Like the dSph searches, the cluster studies have the
advantage that the expected signal is localized in the sky, which
reduces the systematic uncertainties associated with modeling the
foregrounds and unresolved extragalactic sources. As we will show,
however, the sensitivity to DM annihilation is enhanced—and is more
robust—when a larger number of targets are included compared to previous
studies.

Our work aims to combine the best attributes of the cross-correlation
and cluster studies to improve the search for extragalactic DM
annihilation. We use the galaxy group catalogs in Refs. [ 132 ] and [
133 ] (hereby T15 and T17, respectively), which contain accurate mass
estimates for halos with mass greater than @xmath @xmath M @xmath and
@xmath , to systematically determine the galaxy groups that are expected
to yield the best limits on the annihilation rate. The T15 catalog
provides reliable redshift estimates in the range @xmath , while the T17
catalog provides measured distances for nearby galaxies, @xmath , based
on Ref. [ 308 ] . The T15 catalog was previously used for a gamma-ray
line search [ 296 ] , but our focus here is on the broader, and more
challenging, class of continuum signatures. We search for gamma-ray flux
from these galaxy groups and interpret the null results as bounds on the
annihilation cross section.

### 5.2 Galaxy Group Selection

The observed gamma-ray flux from DM annihilation in an extragalactic
halo is proportional to both the particle physics properties of the DM,
as well as its astrophysical distribution:

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

with units of @xmath . Here, @xmath is the gamma-ray energy, @xmath is
the annihilation cross section, @xmath is the DM mass, @xmath is the
branching fraction to the @xmath annihilation channel, and @xmath is the
cosmological redshift. The energy spectrum for each channel is described
the function @xmath , which is modeled using PPPC4DMID [ 88 ] . The
@xmath -factor that appears in Eq. 5.1 encodes the astrophysical
properties of the halo. It is proportional to the line-of-sight integral
of the squared DM density distribution, @xmath , and is written in full
as

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath is the boost factor, which accounts for the enhancement due
to substructure. For an extragalactic halo, where the comoving distance
@xmath is much greater than the virial radius @xmath , the integral in
Eq. 5.2 scales as @xmath for the Navarro-Frenk-White (NFW) density
profile [ 326 ] . Here, @xmath is the virial mass, @xmath is the
critical density, and @xmath is the virial concentration, with @xmath
the scale radius. We infer @xmath using the concentration-mass relation
from Ref. [ 311 ] , which we update with the Planck 2015 cosmology [ 20
] . For a given mass and redshift, the concentration is modeled as a
log-normal distribution with mean given by the concentration-mass
relation. We estimate the dispersion by matching to that observed in the
DarkSky-400 simulation for an equivalent @xmath [ 298 ] . Typical
dispersions range from @xmath @xmath – @xmath over the halo masses
considered.

The halo mass and redshift also determine the boost factor enhancement
that arises from annihilation in DM substructure. Accurately modeling
the boost factor is challenging as it involves extrapolating the
halo-mass function and concentration to masses smaller than can be
resolved with current simulations. Some previous analyses of
extragalactic DM annihilation have estimated boost factors @xmath @xmath
– @xmath for cluster-size halos (see, for example, Ref. [ 315 ] ) based
on phenomenological extrapolations of the subhalo mass and concentration
relations. However, more recent studies indicate that the
concentration-mass relation likely flattens at low masses [ 327 , 328 ,
311 ] , suppressing the enhancement. We use the model of Ref. [ 307 ]
—specifically, the “self-consistent” model with @xmath M @xmath —which
accounts for tidal stripping of bound subhalos and yields a modest boost
@xmath @xmath for @xmath @xmath M @xmath halos. Additionally, we model
the boost factor as a multiplicative enhancement to the rate in our main
analysis, though we consider the effect of possible spatial extension
from the subhalo annihilation in App. B . In particular, we find that
modeling the boost component of the signal as tracing a subhalo
population distributed as @xmath rather than @xmath degrades the upper
limits obtained by almost an order of magnitude at higher masses @xmath
GeV while strengthening the limit by a small @xmath factor at lower
masses @xmath GeV. This is arguably a more plausible scenario, since the
spatial distribution of subhalos is expected to follow the overall shape
of the dark matter halo rather than the annihilation profile (modulo
baryonic effects).

The halo masses and redshifts are taken from the galaxy group catalog
T15 [ 132 ] , which is based on the 2MASS Redshift Survey (2MRS) [ 329 ]
, and T17 [ 133 ] , which compiles an inventory of nearby galaxies and
distances from several sources. The catalogs provide group associations
for these galaxies as well as mass estimates and uncertainties of the
host halos, constructed from a luminosity-to-mass relation. The mass
distribution is assumed to follow a log-normal distribution with
uncertainty fixed at 1% in log-space (see Ch. 4 ), which translates to
typical absolute uncertainties of 25-40%. ^(*) ^(*) * To translate,
approximately, between log- and linear-space uncertainties for the mass,
we may write @xmath , which implies that the linear-space fractional
uncertainties are @xmath . This is conservative compared to the 20%
uncertainty estimate given in T15 due to their inference procedure. The
halo centers are assumed to coincide with the locations of the brightest
galaxy in the group. We infer the @xmath -factor using Eq. 5.2 and
calculate its uncertainty by propagating the errors on @xmath and @xmath
, which we take to be uncorrelated. Note that we neglect the distance
uncertainties, which are expected to be @xmath 5% [ 308 , 133 ] , as
they are subdominant compared to the uncertainties on mass and
concentration. We compile an initial list of nearby targets using the
T17 catalog, supplementing these with the T15 catalog. We exclude from
T15 all groups with Local Sheet velocity @xmath km s @xmath ( @xmath )
and @xmath km s @xmath ( @xmath ), the former because of peculiar
velocity contamination and the latter because of large uncertainties in
halo mass estimation due to less luminous satellites. When groups
overlap between the two catalogs, we preferentially choose distance and
mass measurements from T17.

The galaxy groups are ranked by their inferred @xmath -factors,
excluding any groups that lie within @xmath to mitigate contamination
from Galactic diffuse emission. We require that halos do not overlap to
within @xmath of each other, which is approximately the scale radius of
the largest halos. The exclusion procedure is applied sequentially
starting with a halo list ranked by @xmath -factor. We manually exclude
Andromeda, the brightest halo in the catalog, because its large angular
size is not ideally suited to our analysis pipeline and requires careful
individual study [ 286 ] . As discussed later in this chapter, halos are
also excluded if they show large residuals that are inconsistent with DM
annihilation in the other groups in the sample. Starting with the top
1000 halos, we end up with 495 halos that pass all these requirements.
Of the excluded halos, 276 are removed because they fall too close to
the Galactic plane, 134 are removed by the @xmath proximity requirement,
and 95 are removed because of the cut on large residuals. Other than the
manual exclusion of Andromeda, these selection criteria are identical to
those introduced and tested in Ch. 4 in the context of simulations.

Table 5.1 lists the top five galaxy groups included in the analysis,
labeled by their central galaxy or common name, if one exists. We
provide the inferred @xmath -factor including the boost factor, the halo
mass, redshift, position in Galactic coordinates, inferred
concentration, and boost factor. Additionally, we show @xmath to
indicate the spatial extension of the halo. We find that @xmath is
typically between the 68% and 95% containment radius for emission
associated with annihilation in the halos, without accounting for spread
from the point-spread function (PSF). For reference, Andromeda has
@xmath . A complete list of the analyzed galaxy groups is provided as
Supplementary Data at https://github.com/bsafdi/DMCat .

### 5.3 Data Analysis

We analyze 413 weeks of Pass 8 Fermi data in the UltracleanVeto event
class, from August 4, 2008 through July 7, 2016. The data is binned in
26 logarithmically-spaced energy bins between 502 MeV and 251 GeV and
spatially with a HEALPix pixelation [ 186 ] with nside =128. ^(†) ^(†) †
Our energy binning is constructed by taking 40 log-spaced bins between
200 MeV and 2 TeV and then removing the lowest four and highest ten
bins, for reasons discussed in Ch. 4 The recommended set of quality cuts
are applied to the data corresponding to zenith angle less than @xmath ,
@xmath , and @xmath . ^(‡) ^(‡) ‡
https://fermi.gsfc.nasa.gov/ssc/data/analysis/documentation/Cicerone/Cicerone_Data_Exploration/Data_preparation.html
. We also mask known large-scale structures (see Ch. 4 ).

The template analysis that we perform using NPTFit [ 142 ] is similar to
that of previous dSph studies [ 90 , 89 ] and is detailed in Ch. 4 . We
summarize the relevant points here. Each region-of-interest (ROI),
defined as the @xmath area surrounding each halo center, has its own
likelihood. In each energy bin, this likelihood is the product, over all
pixels, of the Poisson probability for the observed photon counts per
pixel. This probability depends on the mean expected counts per pixel,
which depends on contributions from known astrophysical emission as well
as a potential DM signal. Note that the likelihood is also multiplied by
the appropriate log-normal distribution for @xmath , which we treat as a
single nuisance parameter for each halo and account for through the
profile likelihood method.

To model the expected counts per pixel, we include several templates in
the analysis that trace the emission associated with: (i) the projected
NFW-squared profile modeling the putative DM signal, (ii) the diffuse
background, as described by the Fermi gll_iem_v06 (p8r2) model, (iii)
isotropic emission, (iv) the Fermi bubbles [ 140 ] , (v) 3FGL sources
within @xmath to @xmath of the halo center, floated together after
fixing their individual fluxes to the values predicted by the 3FGL
catalog [ 115 ] , and (vi) all individual 3FGL point sources within
@xmath of the halo center. Note that we do not model the contributions
from annihilation in the smooth Milky Way halo because the brightest
groups have peak flux significantly (approximately an order of magnitude
for the groups in Tab. 5.1 ) over the foreground emission from Galactic
annihilation and because we expect Galactic annihilation to be subsumed
by the isotropic component.

We assume that the best-fit normalizations ( i.e. , profiled values) of
the astrophysical components, which we treat as nuisance parameters, do
not vary appreciably with DM template normalization. This allows us to
obtain the likelihood profile in a given ROI and energy bin by profiling
over them in the presence of the DM template, then fixing the
normalizations of the background components to the best-fit values and
scanning over the DM intensity. We then obtain the total likelihood by
taking the product of the individual likelihoods from each energy bin.
In order to avoid degeneracies at low energies due to the large PSF, we
only include the DM template when obtaining the best-fit background
normalizations at energies above @xmath @xmath GeV. At the end of this
procedure, the likelihood is only a function of the DM template
intensity, which can then be mapped onto a mass and cross section for a
given annihilation channel. We emphasize that the assumptions described
above have been thoroughly vetted in Ch. 4 , where we show that this
procedure is robust in the presence of a potential signal.

The final step of the analysis involves stacking the likelihoods from
each ROI. The stacked log-likelihood, @xmath , is simply the sum of the
log-likelihoods for each ROI. It follows that the test statistic for
data @xmath is defined as

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.3)
                       @xmath   
  -- -------- -------- -------- -------

where @xmath is the cross section that maximizes the likelihood for DM
model @xmath . The 95% upper limit on the annihilation cross section is
given by the value of @xmath where @xmath .

Galaxy groups are expected to emit gamma-rays from standard cosmic-ray
processes. Using group catalogs to study gamma-ray emission from cosmic
rays in these objects is an interesting study in its own right (see,
e.g. , Ref. [ 330 , 331 , 116 , 291 ] ), which we leave to future work.
For the purpose of the present analysis, however, we would like a way to
remove groups with large residuals, likely arising from standard
astrophysical processes in the clusters, to maintain maximum sensitivity
to DM annihilation. This requires care, however, as we must guarantee
that the procedure for removing halos does not remove a real signal, if
one were present.

We adopt the following algorithm to remove halos with large residuals
that are inconsistent with DM annihilation in the other groups in the
sample. A group is excluded if it meets two conditions. First, to ensure
it is a statistically significant excess, we require twice the
difference between the maximum log likelihood and the log likelihood
with @xmath to be greater than 9 at any DM mass. This selects sources
with large residuals at a given DM mass. Second, the residuals must be
strongly inconsistent with limits set by other galaxy groups.
Specifically, the halo must satisfy @xmath , where @xmath is the halo’s
best-fit cross section at any mass and @xmath is the strongest limit out
of all halos at the specified @xmath . These conditions are designed to
exclude galaxy groups where the gamma-ray emission is inconsistent with
a DM origin. This prescription has been extensively tested on mock data
and, crucially, does not exclude injected signals (see Ch. 4 ).

### 5.4 Results

Figure 5.1 illustrates the main results of the stacked analysis. The
solid black line represents the limit obtained for DM annihilating to a
@xmath final state using the fiducial boost factor model [ 307 ] , while
the dashed line shows the limit without the boost factor enhancement
(results for final states other than @xmath are presented in App. B ).
To estimate the expected limit under the null hypothesis, we repeat the
analysis by randomizing the locations of the halos on the sky 200 times,
though still requiring they pass the selection cuts described above. The
colored bands indicate the 68 and 95% containment regions for the
expected limit. The limit is consistent with the expectation under the
null hypothesis.

Figure 5.2 illustrates how the limits evolve for the @xmath final state
with @xmath GeV as an increasing number of halos are stacked. We also
show the expected 68% and 95% containment regions, which are obtained
from the random sky locations. As can be seen, no single halo dominates
the bounds. For example, removing Virgo, the brightest halo in the
catalog, from the stacking has no significant effect on the limit.
Indeed, the inclusion of all 495 halos buys one an additional order of
magnitude in the sensitivity reach.

Fig. 5.3 shows a Mollweide projection of all the @xmath -factors
inferred using the T15 and T17 catalogs, smoothed at @xmath with a
Gaussian kernel. The map is shown in Galactic coordinates with the
Galactic Center at the origin. Looking beyond astrophysical sources,
this is how an extragalactic DM signal might show up in the sky.
Although this map has no masks added to it, a clear extinction is still
visible along the Galactic plane. This originates from the
incompleteness of the catalogs along the Galactic plane.

The limit derived in this work is complementary to the published dSph
bound [ 90 , 89 ] , shown as the solid gray line in Fig. 5.1 . Given the
large systematic uncertainties associated with the dwarf analyses (see
e.g. , Ref. [ 91 ] ), we stress the importance of using complementary
targets and detection strategies to probe the same region of parameter
space. Our limit also probes the parameter space that may explain the
Galactic Center excess (GCE); the best-fit models are marked by the
orange cross [ 150 ] , blue [ 98 ] , red [ 99 ] , and orange [ 95 ]
@xmath @xmath regions. The GCE is a spherically symmetric excess of
@xmath GeV gamma-rays observed to arise from the center of the Milky Way
[ 96 , 97 , 100 , 101 ] . The GCE has received a considerable amount of
attention because it can be explained by annihilating DM. However, it
can also be explained by more standard astrophysical sources; indeed,
recent analyses have shown that the distribution of photons in this
region of sky is more consistent with a population of unresolved point
sources, such as millisecond pulsars, compared to smooth emission from
DM [ 102 , 103 , 161 , 332 ] . Because systematic uncertainties can be
significant and hard to quantify in indirect searches for DM, it is
crucial to have independent probes of the parameter space where DM can
explain the GCE. While our null findings do not exclude the DM
interpretation of the GCE, their consistency with the dwarf bounds
(which also cut into the GCE region) put it further in tension. This
does not, however, account for the fact that the systematics on the
modeling of the Milky Way’s density distribution can potentially
alleviate the tension by changing the best-fit cross section for the
GCE.

### 5.5 Conclusions

This chapter presents the results of the first systematic search for
annihilating DM in nearby galaxy groups. We introduced and validated a
prescription to infer properties of DM halos associated with these
groups, thereby allowing us to build a map of DM annihilation in the
local Universe. Using this map, we performed a stacked analysis of
several hundred galaxy groups and obtained bounds that exclude thermal
cross sections for DM annihilating to @xmath with mass below @xmath
@xmath GeV, assuming a conservative boost factor model. These limits are
competitive with those obtained from the Fermi dSph analyses and are in
tension with the range of parameter space that can explain the GCE.
Moving forward, we plan to investigate the objects with gamma-ray
excesses to see if they can be interpreted in the context of
astrophysical emission. In so doing, we can also develop more refined
metrics for selecting the optimal galaxy groups for DM studies.

We include additional results in App. B that further extends the results
presented here. There, we show limits for additional annihilation final
states and the brightest individual halos. We also show how the limits
are affected by several analysis choices, such as the inclusion of
Andromeda and Virgo, as well as a variety of systematic uncertainties. A
complete table of the galaxy groups used in this analysis, as well as
their associated properties, are provided in Supplementary Data, which
can be accessed at https://github.com/bsafdi/DMCat . The catalog
includes decay factors for all of the groups in addition to the
annihilation @xmath -factors. We emphasize that the supplementary
catalog is separate from the Fermi analysis presented here and may be
used to search for extragalactic DM annihilation and decay into neutral
cosmic rays, regardless of wavelength, messenger, and instrument.

[]

## Appendix A @xmath- and @xmath-factors for Extragalactic Sources
\lettrine

[lines=3]In this Appendix, we derive the @xmath -factor relations used
in the main text. We also derive the corresponding @xmath -factor
relations, which apply to the case of decaying DM. Although we do not
make use of the decay results in the main text, we include these results
for completeness because much of our main analysis can be extended to
the decaying case. This Appendix is broken into three subsections. In
the first of these, we detail the units and conventions used in our
definition of the @xmath - and @xmath -factors. After this, we derive an
approximate form of the astrophysics factors for different DM density
profiles and discuss the accuracy of the approximations made. We
conclude with a discussion of error propagation in the @xmath -factors.
Note that several of the details presented in these appendices have been
discussed elsewhere, see e.g. , Ref. [ 333 , 334 , 335 , 336 ] .

### a.1 Units and Conventions

#### a.1.1 Dark Matter Flux

We begin by carefully outlining the units associated with the @xmath -
and @xmath -factors. The flux, @xmath , associated with either DM
annihilation or decay factorizes into two parts:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.1)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where @xmath is the photon energy and the ‘ann.’ (‘dec.’) superscripts
denote annihilation (decay). The particle physics factors are given by:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.2)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where @xmath is the velocity-averaged annihilation cross section, @xmath
is the DM mass, Br @xmath is the branching fraction into the @xmath
channel, @xmath is the photon energy distribution associated with this
channel, and @xmath is the DM lifetime. The annihilation factor assumes
that the DM is its own antiparticle; if this were not the case, and
assuming no asymmetry in the dark sector, then the factor would be half
as large. The particle physics factors carry the following dimensions:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.3)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where ‘counts’ refers to the number of gamma-rays produced in the
interaction and the @xmath is associated with the @xmath in the particle
physics factors. Note that some references include this @xmath in the
definition of the @xmath - or @xmath -factors, but this is not the
convention that we follow here.

The @xmath - and @xmath -factors are defined as follows:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.4)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where @xmath is the subhalo boost factor. The @xmath - and @xmath
-factors carry the following units:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.5)
              @xmath   @xmath   
  -- -------- -------- -------- -------

Combining these with Eq. A.3 , we find that

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

for both the annihilation and decay case. This means that @xmath is
given in units of counts per experimental effective area [ @xmath ] per
experimental run time [ @xmath ]. In this work, we study extragalactic
objects with small angular extent. So long as each object is centered on
the region-of-interest (ROI), we expect that all of its flux will be
contained within the ROI as well. This means that the photon counts
obtained by integrating Eq. A.4 over the entire sky corresponds to the
total counts expected from that object in the ROI. The situation is
different when treating objects with a large angular extent that exceeds
the size of the ROI— e.g. , when looking for emission from the halo of
the Milky Way. In such cases, it is more common to divide the @xmath -
and @xmath -factors by the solid angle of the ROI ( @xmath ) such that
both they, and consequently @xmath , are averages rather than totals.

#### a.1.2 Halo Mass and Concentration

We briefly comment here on different mass and concentration definitions
(virial and 200) as relevant to our analysis. Boost-factor models,
concentration-mass relations, and masses are often specified in terms of
200 quantities, which must be converted to virial ones. In order to do
this, we use the fact that

  -- -------- -- -------
     @xmath      (A.7)
  -- -------- -- -------

for the NFW profile [ 304 ] , where @xmath is the normalization of the
density profile, @xmath is the critical density, @xmath is the
concentration parameter, and @xmath is the critical overdensity. For
virial quantities, @xmath with @xmath in accordance with Ref. [ 310 ] ,
while for 200 quantities, @xmath . Therefore, Eq. A.7 can be equated
between the 200 and virial quantities and solved numerically to convert
between definitions of the concentration.

For different mass definitions, we have

  -- -------- -- -------
     @xmath      (A.8)
  -- -------- -- -------

where the concentration definitions on the right-hand side depend on
@xmath and @xmath and may have to be converted between each other and we
have suppressed the redshift dependence for clarity. Solving this
numerically, we can convert between the two mass definitions.

### a.2 Approximate @xmath- and @xmath-factors

For an extragalactic DM halo, the astrophysical factors in Eq. A.4 can
be approximated as:

  -- -------- -------- -------- -------
     @xmath   @xmath            (A.9)
              @xmath   @xmath   
  -- -------- -------- -------- -------

where the integrals are performed in a coordinate system centered on the
halo, and @xmath is the comoving distance, which is a function of
redshift for a given cosmology. The aim of this subsection is to derive
Eq. A.9 from Eq. A.4 and to quantify the error associated with this
approximation.

To handle the @xmath - and @xmath -factors simultaneously, we consider
the following integral over all space:

  -- -------- -- --------
     @xmath      (A.10)
  -- -------- -- --------

with @xmath . Here, @xmath is playing the role of a radius in a
spherical coordinate system centered on the Earth. Therefore, we can
rewrite the measure as

  -- -------- -- --------
     @xmath      (A.11)
  -- -------- -- --------

Next, we transform to a coordinate system (denoted by primed quantities)
that is centered at the origin of the halo described by @xmath . Because
this change of coordinates is only a linear translation, it does not
induce a Jacobian and @xmath . Assuming that the Earth is located at a
position @xmath from the halo center and the DM interaction occurs at
position @xmath , then @xmath and

  -- -------- -- --------
     @xmath      (A.12)
  -- -------- -- --------

where we take @xmath and @xmath .

Eq. A.12 can be simplified by taking advantage of several properties of
the halo density. First, it is spherically symmetric about the origin of
the primed coordinate system. Second, it only has finite support in
@xmath . In particular, it does not make sense to integrate the object
beyond the virial radius, @xmath . This allows us to rewrite the
integral as follows:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

For extragalactic objects, @xmath . As a result, we can take advantage
of the following expansion:

  -- -------- -- --------
     @xmath      (A.14)
  -- -------- -- --------

where @xmath . It follows that the leading-order approximation to Eq.
A.2 is

  -- -------- -- --------
     @xmath      (A.15)
  -- -------- -- --------

which when inserted into Eq. A.4 gives Eq. A.9 , as claimed.

We can calculate the size of the neglected terms in Eq. A.14 to quantify
the accuracy of this approximation. We take the parameters of the halo
with the largest @xmath -factor in the catalog to estimate the largest
error possible amongst the DarkSky halos. For this halo, the fractional
correction to the @xmath -factor of the first neglected term in the
expansion is @xmath for either an NFW or Burkert profile (described
below), whilst for the @xmath -factor it is @xmath . These values are
significantly smaller than the other sources of uncertainty present in
estimating these quantities and so we conclude that the approximations
in Eq. A.9 are sufficient for our purposes.

### a.3 Analytic Relations

Starting from the approximate forms given in Eq. A.9 and specifying a DM
density profile @xmath , the @xmath - and @xmath -factors can often be
determined exactly. We will now demonstrate that the final results only
depend on the distance, mass, and concentration of the halo—for a given
substructure boost model and cosmology.

As a starting point, consider the NFW profile:

  -- -------- -- --------
     @xmath      (A.16)
  -- -------- -- --------

The parameter @xmath is the scale radius and dictates how sharply peaked
the core of the DM distribution is. Starting from this distribution, the
volume integral in the @xmath -factor evaluates to

  -- -------- -------- -------- --------
     @xmath   @xmath            (A.17)
                       @xmath   
  -- -------- -------- -------- --------

where @xmath is the virial concentration. To remove the normalization
factor @xmath from this equation, we can write the virial mass of the
halo as

  -- -------- -------- -------- --------
     @xmath   @xmath            (A.18)
                       @xmath   
  -- -------- -------- -------- --------

which, when combined with Eq. A.17 , gives

  -- -------- -------- -------- --------
     @xmath                     (A.19)
              @xmath   @xmath   
  -- -------- -------- -------- --------

Stopping here, we would conclude that the @xmath -factor scales as
@xmath . However, for a given @xmath and cosmology, @xmath is not an
independent parameter. Using the results of Ref. [ 310 ] , we can write:

  -- -------- -- --------
     @xmath      (A.20)
  -- -------- -- --------

where @xmath is the critical density and

  -- -------- -------- -------- --------
     @xmath   @xmath            (A.21)
              @xmath   @xmath   
  -- -------- -------- -------- --------

This relation can then be used to remove @xmath from the volume integral
and we conclude that

  -- -------- -------- -- --------
     @xmath   @xmath      (A.22)
     @xmath   @xmath      
  -- -------- -------- -- --------

We see the additional mass dimension required from the fact this scales
as @xmath not @xmath is carried by @xmath . The @xmath dependence
highlights that the annihilation flux is critically dependent upon how
sharply peaked the halo is. To summarize, Eq. A.22 demonstrates that the
@xmath -factor is fully specified by three halo parameters for a given
substructure boost model and cosmology: the redshift @xmath , mass
@xmath , and concentration @xmath .

The basic scalings and dependence shown above are not peculiar to the
NFW profile, but are in fact more generic. To demonstrate this, we can
repeat the above exercise for the cored Burkert profile [ 337 ] :

  -- -------- -- --------
     @xmath      (A.23)
  -- -------- -- --------

which is manifestly non-singular as @xmath unlike the NFW profile. Here,
@xmath and @xmath are the Burkert analogues of @xmath and @xmath in the
NFW case, but they are not exactly the same. Indeed, following e.g. ,
Ref. [ 307 ] , by calculating physically measurable properties of halos
such as the radius of maximum rotational velocity for both the NFW and
Burkert cases and setting them equal, we find

  -- -------- -- --------
     @xmath      (A.24)
  -- -------- -- --------

We will replace @xmath with a concentration parameter @xmath . Following
the same steps as for the NFW profile, we arrive at:

  -- -------- -------- -- --------
     @xmath   @xmath      (A.25)
     @xmath   @xmath      
     @xmath   @xmath      
  -- -------- -------- -- --------

from which we see that @xmath .

For the case of decaying DM, the approximate integral given in Eq. A.9
can be evaluated independent of any choice for the halo profile.
Specifically:

  -- -------- -------- -- --------
     @xmath   @xmath      (A.26)
  -- -------- -------- -- --------

where the second equality follows from the fact that the volume integral
gives the virial mass exactly. For DM decays in relatively nearby halos,
the emission can be quite extended, as the flux is not as concentrated
towards the center of the halo as in the annihilation case. As such, it
is often useful to have a version of the extragalactic @xmath -factor
where one only integrates out to some angle @xmath on the sky from the
center of the halo, or equivalently to a distance @xmath . In this case:

  -- -------- -------- -- --------
     @xmath   @xmath      (A.27)
              @xmath      
              @xmath      
  -- -------- -------- -- --------

for the NFW profile, where we have made explicit the fact that @xmath is
a function of @xmath . When @xmath , this reduces to the simple result
in Eq. A.26 .

[]

## Appendix B Supplementary Material on Cluster Searches

### b.1 Extended results \lettrine

[lines=3]In the main analysis, Fig. 5.2 demonstrates how the limit on
the @xmath annihilation cross section depends on the number of halos
included in the stacking, for the case where @xmath GeV. In Fig. B.1 ,
we show the corresponding plot for @xmath GeV (left) and @xmath TeV
(right). As in the 100 GeV case, we see that no single halo dominates
the bound and that stacking a large number of halos considerably
improves the sensitivity.

The left panel of Fig. B.2 shows the maximum test statistic, TS @xmath ,
recovered for the stacked analysis in the @xmath channel. For a given
data set @xmath , we define the maximum test-statistic in preference for
the DM model, relative to the null hypothesis without DM, as

  -- -------- -- -------
     @xmath      (B.1)
  -- -------- -- -------

where @xmath is the cross section that maximizes the likelihood for DM
model @xmath . The observed TS @xmath is negligible at all masses and
well-within the null expectation (green/yellow bands), consistent with
the conclusion that we find no evidence for DM annihilation.

Other Annihilation Channels. In general, DM may annihilate to a variety
of Standard Model final states. Figure B.2 (right) interprets the
results of the analysis in terms of limits on additional final states
that also lead to continuum gamma-ray emission. Final states that
predominantly decay hadronically ( @xmath , @xmath , @xmath , @xmath ,
@xmath , @xmath ) give similar limits because their energy spectra are
mostly set by boosted pion decay. The leptonic channels ( @xmath ,
@xmath ) give weaker limits because gamma-rays predominantly arise from
final-state radiation or, in the case of the muon, radiative decays. The
@xmath limit is intermediate because roughly 35% of the @xmath decays
are leptonic, while the remaining are hadronic. Of course, the DM could
annihilate into even more complicated final states than the two-body
cases considered here and the results can be extended to these cases [
305 , 306 ] . Note that the limits we present for the leptonic final
states are conservative, as they neglect Inverse Compton (IC) emission
and electromagnetic cascades, which are likely important at high DM
masses—see e.g. , Ref. [ 338 , 339 ] . A more careful treatment of these
final states requires modeling the magnetic field strength and energy
loss mechanisms within the galaxy groups.

Injected Signal. An important consistency requirement is to ensure that
the limit-setting procedure does not exclude a putative DM signal. The
likelihood procedure employed here was extensively vetted in Ch. 4 ,
where we demonstrated that the limit never excludes an injected signal.
In Fig. B.3 , we demonstrate a data-driven version of this test. In
detail, we inject a DM signal on top of the actual data set used in the
main analysis, focusing on the case of DM annihilation to @xmath for a
variety of cross sections and masses. We then apply the analysis
pipeline to these maps. The top panel of Fig. B.3 shows the recovered
cross sections, as a function of the injected values. The green line
corresponds to the 95% cross section limit, while the blue line shows
the best-fit cross section. Note that statistical uncertainties arising
from DM annihilation photon counts are not significant here, as the
dominant source of counts arises from the data itself. The columns
correspond to 10, 100, and 10 @xmath GeV DM annihilating to @xmath
(left, center, right, respectively). The bottom row shows the maximum
test statistic in favor of the model with DM as a function of the
injected cross section. The best-fit cross sections are only meaningful
when the maximum test statistic is @xmath , implying evidence for DM
annihilation. We see that across all masses, the cross section limit
(green line) is always weaker than the injected value. Additionally, the
recovered cross section (blue line) closely approaches that of the
injected signal as the significance of the DM excess increases.

Results for Individual Halos. Here, we explore the properties of the
individual galaxy groups that are included in the stacked analysis.
These galaxy groups are taken from the catalogs in Ref. [ 132 ] and [
133 ] , which we refer to as T15 and T17, respectively. Table B.1 lists
the top 25 galaxy groups, ordered by the relative brightness of their
inferred @xmath -factor. If a group in the table is not labeled with a
checkmark, then it is not included in the stacking because one of the
following conditions is met:

  -- -------- -- -------
     @xmath      (B.2)
  -- -------- -- -------

Note that the overlap criteria is applied sequentially in order of
increasing @xmath -factor. These selection criteria have been
extensively studied on mock data in Ch. 4 and have been verified to not
exclude a potential DM signal, even on data as discussed above. Of the
five halos with the largest @xmath -factors that are excluded, Andromeda
is removed because of its large angular extent, and the rest fail the
latitude cut.

The exclusion of Andromeda is not a result of the criteria in Eq. B.2 ,
so some more justification is warranted. As can be seen in Table B.1 ,
the angular extent of Andromeda’s scale radius, @xmath , is
significantly larger than that of any other halo. To justify @xmath as a
proxy for angular extent of the emission, we calculate the 68% (95%)
containment angle of the expected DM annihilation flux, without
accounting for the PSF, and find 1.2 @xmath (4.4 @xmath ). This can be
contrasted with the equivalent numbers for the next most important halo,
Virgo, where the corresponding 68% (95%) containment angles are 0.5
@xmath (2.0 @xmath ). Because Andromeda is noticeably more extended
beyond the Fermi PSF, one must carefully model the spatial distribution
of both the smooth DM component and the substructure. Such a dedicated
analysis of Andromeda was recently performed by the Fermi collaboration
[ 286 ] . Out of an abundance of caution, we remove Andromeda from the
main joint analysis, but we do show how the limits change when Andromeda
is included further below.

Figure LABEL:fig:individual_lims shows the individual limits on the
@xmath annihilation cross section for the top ten halos that pass the
selection cuts and Fig. LABEL:fig:individual_maxts shows the maximum
test statistic (TS @xmath ), as a function of @xmath , for these same
halos. The green and yellow bands in Fig. LABEL:fig:individual_lims and
LABEL:fig:individual_maxts represent the 68% and 95% containment regions
obtained by randomly changing the sky location of each individual halo
200 times (subject to the selection criteria listed above). As is
evident, the individual limits for the halos are consistent with
expectation under the null hypothesis— i.e. , the black line falls
within the green/yellow bands for each of these halos. Some of these
groups have been analyzed in previous cluster studies. For example, the
Fermi Collaboration provided DM bounds for Virgo [ 116 ] ; our limit is
roughly consistent with theirs, and possibly a bit stronger, though an
exact comparison is difficult to make due to differences in the data set
and DM model assumptions. ^(*) ^(*) * Note that the @xmath -factor in
Ref. [ 116 ] is a factor of @xmath too small.

Figure LABEL:fig:individual_flux provides the 95% upper limits on the
gamma-ray flux associated with the DM template for each of the top ten
halos. The upper limits are provided for 26 energy bins and compared to
the expectations under the null hypothesis. The upper limits are
generally consistent with the expectations under the null hypothesis,
though small systematic discrepancies do exist for a few halos, such as
NGC3031, at high energies. This could be due to subtle differences in
the sky locations and angular extents between the objects of interest
and the set of representative halos used to create the null hypothesis
expectations.

To demonstrate the case of a galaxy group with an excess, we show the TS
@xmath distribution and the limit for NGC6822 in Fig. B.4 . This object
fails the selection criteria because it is too close to the Galactic
plane. However, it also exhibits a TS @xmath excess and, as expected,
the limit is weaker than the expectation under the null hypothesis.

Sky maps. In Fig. LABEL:fig:individual_skyrois , we show the counts map
in @xmath square regions around each of the top nine halos that pass the
selection cuts. For each map, we show all photons with energies above
@xmath 500 MeV, indicate all Fermi 3FGL point sources with orange stars,
and show the extent of @xmath with a dashed orange circle. Given a DM
signal, we would expect to see emission extend out to @xmath at the
center of these images.

### b.2 Variations on the Analysis

We have performed a variety of systematic tests to understand the
robustness of the results presented in the main body of the analysis.
Several of these uncertainties are discussed in detail in Ch. 4 ; here,
we focus specifically on how they affect the results of the data
analysis.

Halo Selection Criteria. Here, we demonstrate how variations on the halo
selection conditions listed above affect the baseline results of Fig.
5.1 . In the left panel of Fig. B.5 , the red line shows the limit that
is obtained when starting with 10,000 halos instead of 1000, but
requiring the same selection conditions. Despite the modest improvement
in the limit, we choose to use 1000 halos in the baseline study because
systematically testing the robustness of the analysis procedure, as done
in Ch. 4 , becomes computationally prohibitive otherwise. In order to
calibrate the analysis for higher halo numbers, it would be useful to
use semi-analytic methods to project the sensitivity, such as those
discussed in Ref. [ 340 , 341 ] , although we leave the details to
future work.

Virgo is the object with the highest @xmath -factor in the stacked
sample. As made clear in the dedicated study of this object by the Fermi
Collaboration [ 116 ] , there are challenges associated with modeling
the diffuse emission in Virgo’s vicinity. However, we emphasize that the
baseline limit is not highly sensitive to any one halo, including the
brightest in the sample. For example, the dotted line in the left panel
of Fig. B.5 shows the impact on the limit after removing Virgo from the
stacking. Critically, we see that the limit is almost unchanged,
highlighting that the stacked result is not solely driven by the object
with the largest @xmath -factor.

The effect of including Andromeda (M31) is shown as the gray solid line.
We exclude Andromeda from the baseline analysis because of its large
angular size, as discussed in detail above. Our analysis relies on the
assumption that the DM halos are approximately point-like on the sky,
which fails for Andromeda, and we therefore deem it to fall outside the
scope of the systematic studies performed here.

The dashed line shows the effect of tightening the condition on
overlapping halos from @xmath to @xmath . Predictably, the limit is
slightly weakened due to the smaller pool of available targets. We also
show the effect of decreasing the latitude cut to @xmath (dot-dashed
line). In this case, the number of halos included in the stacked
analysis increases, but the limit is weaker—considerably so below @xmath
GeV. The weakened limits are likely due to enhanced diffuse emission
along the plane as well as contributions from unresolved point sources,
both of which are difficult to accurately model. In cases with such
mismodeling, the addition of a DM template can generically improve the
quality of the fit, which leads to excesses at low energies, in
particular. The baseline latitude cut ameliorates precisely these
concerns.

The right panel of Fig. B.5 illustrates the effects of changing, or
removing completely, the cross section and TS @xmath cuts on the halos.
Specifically, the dashed black line shows what happens when we require
that a halo’s excess be even more inconsistent with the limits set by
other galaxy groups; specifically, requiring that @xmath . The
dot-dashed line shows the limit when we decrease the statistical
significance requirement to @xmath . Note that the two changes have
opposite effects on the limits. This is expected because more halos with
excesses are included in the stacking procedure with the more stringent
cross section requirement, which weakens the limit, whereas fewer are
included if we reduce the TS @xmath cut, strengthening the limit.

The dotted line in the right panel of Fig. B.5 shows what happens when
no requirement at all is placed on the TS @xmath and cross section; in
this case, the limit is dramatically weakened by several orders of
magnitude. We show the same result in Fig. B.6 (dotted line), but with a
comparison to the null hypothesis corresponding to no TS @xmath and
cross section cuts, which is shown as the 68% (95%) red (blue) bands.
^(†) ^(†) † We thank A. Drlica-Wagner for suggesting this test. In the
baseline case, the limit is consistent with the random sky locations—
i.e. , the solid black line falls within the green/yellow bands.
However, with no TS @xmath and cross section cuts, this is no longer
true— i.e. , the dotted black line falls outside the red/blue bands.
Clear excesses are observed above the background expectation in this
case, but they are inconsistent with a DM interpretation as they are
strongly excluded by other halos in the stack. When deciding on the TS
@xmath and cross section requirements that we used for the baseline
analysis in Fig. 5.1 , our goal was to maximize the sensitivity reach
while simultaneously ensuring that an actual DM signal would not be
excluded. We verified the selection criteria thoroughly by performing
injected signal tests on the data (discussed above) as well as on mock
data (discussed in Ch. 4 ). Ideally, galaxy groups would be excluded
from the stacking based on the specific properties of the astrophysical
excesses that they exhibit, as opposed to the TS @xmath and cross
section requirements used here. For example, one can imagine excluding
groups that are known to host AGN or galaxies with high amounts of
star-formation activity. We plan to study such possibilities in future
work.

Data Set and Foreground Models. In the results presented thus far, we
have used all quartiles of the UltracleanVeto event class of the Fermi
data. Alternatively, we can restrict ourselves to the top quartile of
events, as ranked by PSF. Using this subset of data has the advantage of
improved angular resolution, but the disadvantage of a @xmath 75%
reduction in statistics. The left panel of Fig. B.7 shows the limit
(dot-dashed line) obtained by repeating the analysis with the top
quartile of UltracleanVeto data; the bounds are weaker than in the
all-quartile case, as would be expected. However, the amount by which
the limit weakens is not completely consistent with the decrease in
statistics. Rather, it appears that when we lower the photon statistics,
more halos that were previously excluded by the cross section and TS
@xmath criteria in the baseline analysis are allowed into the stacking
and collectively weaken the limit.

Another choice that we made for the baseline analysis was to use the
p8r2 foreground model for gamma-ray emission from cosmic-ray processes
in the Milky Way. In this model, the bremsstrahlung and boosted pion
emission are traced with gas column-density maps and the IC emission is
modeled using Galprop [ 271 ] . After fitting the data with these three
components, any ‘extended emission excesses’ are identified and added
back into the foreground model [ 342 ] . To study the dependence of the
results on the choice of foreground model, we repeat the analysis using
the Pass 7 gal_2yearp7v6_v0.fits ( p7v6 ) model, which includes
large-scale structures like Loop 1 and the Fermi bubbles—in addition to
the bremsstrahlung, pion, and IC emission—but does not account for any
data-driven excesses as is done in p8r2 . The results of the stacked
analysis using the p7v6 model are shown in the left panel of Fig. B.7
(dashed line). The limit is somewhat weaker to that obtained using p8r2
, though it is broadly similar to the latter. This is to be expected for
stacked analyses, where the dependence on mismodeling of the foreground
emission is reduced because the fits are done on small, independent
regions of the sky, so that offsets in the point-to-point normalizations
of the diffuse model can have less impact. For more discussion of this
point, see Ref. [ 95 , 161 , 343 , 344 ] .

Halo Density Profile and Concentration. Our baseline analysis makes two
assumptions about the profiles of gamma-ray emission from the
extragalactic halos. The first assumption is that the DM profile of the
smooth halo is described by an NFW profile:

  -- -------- -- -------
     @xmath      (B.3)
  -- -------- -- -------

where @xmath is the normalization and @xmath the scale radius [ 326 ] .
The NFW profile successfully describes the shape of cluster-size DM
halos in @xmath -body simulations with and without baryons (see, e.g. ,
Ref. [ 138 , 345 ] ). However, some evidence exists pointing to cored
density profiles on smaller scales ( e.g. , dwarf galaxies), and the
density profiles in these systems may be better described by the
phenomenological Burkert profile [ 337 ] :

  -- -------- -- -------
     @xmath      (B.4)
  -- -------- -- -------

where @xmath and @xmath are the Burkert corollaries to the NFW @xmath
and @xmath , but have numerically different values. While it appears
unlikely that the Burkert profile is a good description of the DM
profiles of the cluster-scale halos considered here, using this profile
provides a useful systematic variation because it predicts less
annihilation flux than the NFW profile does. The right panel of Fig. B.7
shows the effect of using the Burkert profile to describe the halos in
the T15 and T17 catalogs (dot-dashed line); the limit is slightly
weaker, as expected.

The second assumption we made is that the shape of the gamma-ray
emission from DM annihilation follows the projected integral of the
DM-distribution squared. This is likely incorrect because the
contribution from the boost factor, which can be substantial, should
have the spatial morphology of the distribution of DM subhalos.
Neglecting tidal effects, we expect the subhalos to follow the DM
distribution (instead of the squared distribution). Including tidal
effects is complicated, as subhalos closer to the halo center are more
likely to be tidally stripped, which both increases their concentration
and decreases their number density. We do not attempt to model the
change in the spatial morphology of the subhalo distribution from tidal
stripping and instead consider the limit where the annihilation flux
from the subhalo boost follows the NFW distribution. This gives a much
wider angular profile for the annihilation flux for large clusters,
compared to the case where the boost is simply a multiplicative factor.
The dashed line in the right panel of Fig. B.7 shows the effect on the
limit of modeling the gamma-ray emission in this way (labeled “ @xmath
-boosted profile”). The extended spatial profile leads to a minimal
change in the limit over most of the mass range, which is to be expected
given that most of the galaxy groups can be well-approximated as point
sources.

A halo’s virial concentration is an indicator of its overall density and
is defined as @xmath , where @xmath is the virial radius and @xmath the
NFW scale radius of the halo. A variety of models exist in the
literature that map from halo mass to concentration. Our fiducial case
is the Correa et al. model from Ref. [ 311 ] . Here we show how the
limit (dotted line) changes when we use the model of Diemer and Kravtsov
[ 312 ] , updated with the Planck 2015 cosmology [ 20 ] . The change to
the limit is minimal, which is perhaps a reflection of the fact that the
change in the mean concentrations between the concentration-mass models
is small compared to the statistical spread predicted in these models,
which is incorporated into the @xmath -factor uncertainties. We have
also verified that increasing the dispersion on the concentration for
the Correa et al. model to 0.24 [ 346 ] , which is above the 0.14–0.19
range used in the baseline study, worsens the limit by a @xmath factor.

Substructure Boost. Hierarchical structure formation implies that larger
structures can host smaller substructures, the presence of which can
significantly enhance signatures of DM annihilation in host halos.
Although several models exist in the literature to characterize this
effect, the precise enhancement sensitively depends on the methods used
as well as the astrophysical and particle physics properties that are
assumed. Phenomenological extrapolation of subhalo properties ( e.g. ,
the concentration-mass relation) over many orders of magnitude down to
very small masses @xmath ) M @xmath lead to large enhancements of @xmath
and @xmath for galaxy- and cluster-sized halos, respectively [ 315 ] .
Recent numerical simulations and analytic studies [ 327 , 311 , 328 ]
suggest that the concentration-mass relation flattens at smaller masses,
yielding boosts that are much more modest, about an order-of-magnitude
below phenomenological extrapolations [ 347 , 139 ] . In addition, the
concentration-mass relation for field halos cannot simply be applied to
subhalos, because the latter undergo tidal stripping as they fall into
and orbit their host. Such effects tend to make the subhalos more
concentrated—and therefore more luminous—than their field-halo
counterparts, though the number-density of such subhalos is also reduced
[ 307 ] .

When taken together, the details of the halo formation process shape the
subhalo mass function @xmath , where @xmath . The mass function does not
follow a power-law to arbitrarily low masses, however, because the
underlying particle physics model for the DM can place a minimum cutoff
on the subhalo mass, @xmath . For example, DM models with longer
free-streaming lengths wash out smaller-scale structures, resulting in
higher cutoffs.

The left panel of Fig. B.8 shows a variety of boost models commonly used
in DM studies. The fiducial boost model used here [ 307 ] is shown as
the thick green solid line and variations on @xmath and @xmath are also
plotted. The right panel of Fig. B.8 shows that the expected limit when
@xmath M @xmath instead of @xmath M @xmath (dot-dashed) is weaker across
all masses. While a minimum subhalo mass of @xmath M @xmath is likely
inconsistent with bounds on the kinetic decoupling temperature of
thermal DM, this example illustrates the importance played by @xmath in
the sensitivity reach. Additionally, Fig. B.8 demonstrates the case
where @xmath (dashed line). Increasing the inner slope of the subhalo
mass function leads to a correspondingly stronger limit, however
observations tend to favor a slope closer to @xmath (which is what the
most massive halos correspond to in our fiducial case).

Ref. [ 139 ] derived a boost factor model that accounts for the
flattening of the concentration-mass relation at low masses, but does
not include the effect of tidal stripping. They assume a minimum
sub-halo mass of @xmath M @xmath and a halo-mass function @xmath . This
was updated by Ref. [ 314 ] to account for the effect of tidal
disruption. This updated boost factor model, which takes @xmath , gives
the constraint shown in Fig. B.8 labeled “Moliné” (dotted). This model
is to be contrasted with the boost factor model of Ref. [ 315 ] ,
labeled “Gao” in Fig. B.8 (grey-dashed), which uses a phenomenological
power-law extrapolation of the concentration-mass relation to low
sub-halo masses. Because the annihilation rate increases with increasing
concentration parameter, the model in Ref. [ 315 ] predicts
substantially larger boosts than other scenarios that take into account
a more realistic flattening of the concentration-mass relation at low
subhalo masses.

Galaxy Group Catalog. We now explore the dependence of the results on
the group catalog that is used to select the halos. In this way, we can
better understand how the DM bounds are affected by uncertainties on
galaxy clustering algorithms and the inference of the virial mass of the
halos. The baseline limits are based on the T15 and T17 catalogs, but
here we repeat the analysis using the Lu et al. catalog [ 295 ] , which
solely relies on 2MRS observations. The group-finding algorithm used by
Ref. [ 295 ] is different to that of T15 and T17 in many ways, relying
on a friends-of-friends algorithm as opposed to one based on matching
group properties at different scales to @xmath -body simulations. Lu et
al. also use a different halo mass determination. For these reasons, it
provides a good counterpoint to T15 and T17 for estimating systematic
uncertainties associated with the identification of galaxy groups. While
T17 includes measured distances for nearby groups, the Lu catalog
corrects for the effect of peculiar velocities following the
prescription in Ref. [ 348 ] and the effect of Virgo infall as in Ref. [
349 ] . Figure B.9 is a repeat of Fig. 5.1 in the main analysis, except
using the Lu et al. catalog. Despite important differences between the
group catalogs used, the Lu et al. results are very similar to the
baseline case.

There are a variety of sources of systematic uncertainty beyond those
described here that deserve further study. For example, a systematic
bias in the @xmath -factor determination due to offsets in either the
mass inference or the concentration-mass relation can be a potential
source of uncertainty. A better understanding of the galaxy-halo
connection and the small-scale structure of halos is required to
mitigate this. Furthermore, we assumed distance uncertainties to be
subdominant in our analysis. While this is certainly a good assumption
over the redshift range of interest—nearby groups have measured
distances, while groups further away come with spectroscopic redshift
measurements with small expected peculiar velocity
contamination—uncertainties on these do exist. We have also assumed that
our targets consist of virialized halos and have not accounted for
possible out-of-equilibrium effects in modeling these [ 350 ] .

[]