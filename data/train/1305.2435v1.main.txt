## Chapter 2 Practical applications

#### 2.1 Quantum cryptography

The idea of quantum cryptography or quantum key distribution , first put
forward in the famous 1984 paper [ BB84 ] by Bennett and Brassard, has
its origins in an early work by S. Wiesner [ Wiesner ] . The main
observation behind it was that two photon polarization bases, say @xmath
and @xmath for rectilinear and diagonal, can be selected in such a way
that photons fully polarized with respect to one of them give totally
random results when measured in the other basis, and vice versa. Equally
important was the fact that quantum measurements affect the measured
systems in general. Bennett and Brassard used these quantum-mechanical
features to construct a protocol, now called BB84, which allows two
parties that do not initially share any secrets, to generate a random
string of bits that is known to both of them, but not to anyone else.
Such bits can subsequently be used as a shared secret key for perfectly
secure classical data transmission. Let us call the two parties @xmath
and @xmath , or Alice and Bob. The protocol designed by Bennett and
Brassard consists in the following steps:

1.  Alice and Bob agree on two polarization bases, say @xmath and @xmath
    , which are rotated by @xmath with respect to each other. Let us
    denote the corresponding pure polarization photon states by @xmath ,
    @xmath for the @xmath basis and @xmath , @xmath for the @xmath
    basis.

2.  Alice generates random sequences of bits, @xmath and @xmath , using
    a classical random number generator.

3.  Bob generates a random sequence of bits @xmath , also using a
    classical generator.

4.  Alice then begins to send photons to Bob. The polarization state of
    the @xmath -th photon is chosen according to the values of the
    random bits @xmath and @xmath . The bit @xmath determines which
    polarization basis is used, with @xmath standing for the @xmath and
    @xmath for the @xmath basis. The bit @xmath determines whether the
    first or the second pure polarization state with respect to the
    given basis is chosen. Table 2.1 summarizes on Alice’s choice of
    photon, depending on @xmath .

5.  Bob measures the received @xmath -th photon in the @xmath or @xmath
    basis, depending on the value of @xmath . When @xmath , Bob uses
    @xmath . Otherwise, he uses @xmath . The first vector in the
    selected basis ( @xmath or @xmath ) is assigned the measurement
    result @xmath , while the remaining vector ( @xmath or @xmath ) is
    assigned @xmath . If Bob happens to choose the same basis as Alice
    did (i.e. @xmath ), his measurement result exactly matches @xmath ,
    assuming the photon transmission was not disrupted nor interfered
    with by an eavesdropper.

6.  After measuring all the @xmath photons, Bob publicly discloses the
    bits @xmath , and Alice does the same with @xmath . Thus done, they
    know which measurement bases they used for individual photons and
    can single out the cases where their basis choices were identical.
    On average, they would have chosen the same basis in @xmath cases.

7.  As their secret key, Alice and Bob choose the bits @xmath for which
    @xmath . They both know these bits, as a result of using identical
    measurement bases.

The power of the above protocol comes from the fact that any
interference by an eavesdropper would very likely have been detected by
Alice and Bob, provided that they perform an additional correctness
check before they agree on the key. The required additional procedure
can be summarized as follows:

1.  After performing Step 6., Alice and Bob select a random subset of
    the indices @xmath for which @xmath . Assume the selected indices
    are @xmath . Alice publicly discloses the bits @xmath , and Bob
    discloses the corresponding measurement results he obtained. If both
    match, the transmission is assumed to be perfect and the remaining
    bits for which @xmath are used as a secret key. Otherwise, it is
    assumed that someone was eavesdropping, and the results of the whole
    secret key generation procedure are discarded.

An exemplary run of the procedure consisting of steps 1.-7., with 7.’
included, is presented in Table 2.2 . Note that in real life
applications, it is impossible to avoid transmission errors, even if
there is no one eavesdropping. Hence, a general strategy has to be
developed to deal with transmission/eavesdropping errors, a strategy
that would allow to produce a secret key, even if the transmission does
not work perfectly. Suitable tools, borrowed from classical coding
theory, were discovered some years after the advent of BB84 [ BBBSS92 ]
. They are very generally described as information reconciliation and
privacy amplification . For more details, cf. [ BBBSS92 ] .

We need to point out that in the above procedures, no use of
entanglement was made. However, in the early nineties, A. Ekert proposed
the first entanglement-based quantum key distribution protocol, known as
E91 [ E91 ] . Although the general idea behind E91 is the same as for
BB84, there are several key differences:

1.  Instead of leaving the photon state preparation to Alice, both
    parties are assigned the identical task of measuring a subsystem in
    a two-partite maximally entangled photon state @xmath . The state is
    assumed to be externally given. Alice measures the first and Bob the
    second subsystem.

2.  Three instead of two photon polarization bases are used at random by
    Alice and Bob. In case of Alice, the polarizer angles @xmath ,
    @xmath and @xmath are used. For Bob, it is @xmath , @xmath and
    @xmath .

3.  Bob and Alice publicly disclose which bases they used in which
    measurement round. Then, they reveal the measurement results for
    which different measurement setups were used. This permits them to
    calculate the CHSH quantity

      -- -- -- -------
               (2.1)
      -- -- -- -------

    where @xmath is the correlation coefficient between the measurement
    results for Alice and Bob when their polarizer angles are @xmath and
    @xmath , respectively. As in the example discussed in Section 1.1 ,
    the value of the function ( 2.1 ) for a truly maximally entangled
    source state is @xmath . By testing whether the equality between
    @xmath and ( 2.1 ) really occurs, Bob and Alice make sure that no
    eavesdropping takes place, nor that the source is corrupted.

4.  If there is (an approximate) equality between ( 2.1 ) and its
    theoretical value, the results which Bob and Alice obtained when
    they measured in the same bases , should be perfectly correlated.
    They were not publicly disclosed so far, so they can be used as a
    secret key.

Shortly after Ekert published his paper, Bennett, Brassard and Mermin [
BBM92 ] suggested another entanglement-based protocol, now called BBM92,
which is basically a version of BB84 that exploits the properties of
entangled quantum states. Thus, the difference from BB84 described by
item @xmath above still exists, but the other ones do not.

It is natural to ask how the above two-qubit key distribution methods
generalize to higher dimensional quantum systems. The question was
addressed by the authors of the paper [ CBKG2001 ] , who used so-called
mutually unbiased bases (MUBs) as a higher dimensional analogue of the
pair of bases @xmath and @xmath . Let us explain that two orthonormal
bases @xmath and @xmath of @xmath are called unbiased if and only if the
following equality

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

holds for all @xmath and @xmath . The unbiasedness condition guarantees
the desirable property that an element of one of the bases gives fully
random results when measured in the other basis.

There can exist at most @xmath mutually unbiased bases in @xmath [ WF89
] . We shall discuss some of their further aspects in Section 8.4 .
Either a pair of them, or more can be used to design quantum key
distribution protocols based on @xmath -dimensional quantum systems [
CBKG2001 ] . These protocols do not differ significantly from the qubit
ones. Let us also remark that in the qubit setting, there are three MUBs
available, so that there exists an alternative to BB84 that uses six
quantum states instead of four. This possibility was first studied in a
paper by Bruss [ Bruss98 ] .

#### 2.2 Quantum teleportation and dense coding

As our next example of how the laws of quantum mechanics can be used for
practical purposes, we shall discuss the two interconnected concepts of
dense coding [ BW92 ] and quantum state teleportation [ BBCJPW93 ] .

In its most basic form, dense coding permits two parties, say Alice and
Bob, to exchange two classical bits of information by just transmitting
one qubit . The fundamental trick behind this feature is the use of
one-sided Pauli transformations, acting on a maximally entangled state.
We have

  -- -------- -------- -- -------
     @xmath   @xmath      (2.3)
     @xmath   @xmath      
  -- -------- -------- -- -------

so that the four states resulting from one-sided Pauli action on @xmath
are perfectly distinguishable. Hence, they can carry two bits of
classical information. In the dense coding scheme proposed in [ BW92 ] ,
Alice and Bob initially share a maximally entangled state @xmath of a
two-partite system, and each of them has access to only one of the
subsystems. Alice then performs one of the four Pauli transformations on
her subsystem, and sends the subsystem to Bob. After this step, Bob is
in possession of one of the two-partite maximally entangled states from
the list ( 2.3 ). Because these states can be perfectly distinguished by
a quantum measurement, Bob can in principle tell which of the four Pauli
operations Alice used. Consequently, two bits of classical information
have been transmitted, even though only one qubit was exchanged between
Alice and Bob.

The aim of quantum state teleportation is, on the other hand, to
transmit an unknown quantum state @xmath between the two parties. In the
basic qubit teleportation model [ BBCJPW93 ] , the required resources
are a maximally entangled state, i.e. @xmath , which is shared between
Alice an Bob, and the state to be teleported, initially held by Alice.
Altogether, they have a tripartite system, initially in the state @xmath
. The first two subsystems are controlled by Alice, and the third one by
Bob. In order to teleport @xmath to Bob, Alice performs a measurement on
the first two qubits, using the measurement basis @xmath . She then
communicates the result to Bob. Provided this information, Bob can
recover @xmath by performing a suitable unitary rotation on his
subsystem. To see that this is actually the case, it suffices to notice
the following identity

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

After the Alice’s measurement on the first two qubits, Bob’s subsystem
is in one of the states @xmath , @xmath , @xmath , @xmath . Moreover,
Alice can perfectly differentiate between these four cases, as she knows
which of the states @xmath , @xmath , @xmath and @xmath she got in her
measurement. If she is so kind to share this knowledge with Bob, he can
then recover the state @xmath by simply undoing the suitable rotation
@xmath , @xmath or @xmath , if his state is not already a multiple of
@xmath .

Naturally, the above dense coding and teleportation schemes for qubits
are expected to have generalizations to higher dimensional systems. Such
generalizations do indeed exist and for the so-called tight type, they
have been completely characterized by Werner [ ref.Werner01 ] .
Moreover, he showed that there is a one-to-one correspondence between
tight dense coding and tight teleportation schemes. In order to fully
understand his result, we first need to explain what a general dense
coding and teleportation scheme is.

###### Definition 2.1.

Let @xmath be a set of @xmath elements. A tight quantum teleportation
scheme consists of

-    A density operator @xmath on @xmath

-    A collection of completely positive and trace preserving maps
    @xmath , @xmath , acting on operators on @xmath

-    A collection of observables @xmath on @xmath , @xmath , such that
    for all density operators @xmath on @xmath and all operators @xmath
    on @xmath , the following equality holds

      -- -------- -- -------
         @xmath      (2.5)
      -- -------- -- -------

###### Definition 2.2.

Let @xmath be a set of @xmath elements. A tight dense coding scheme
consists of the same elements as a tight quantum teleportation scheme,
however the condition ( 2.5 ) is replaced by

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

for all @xmath

Note that in the above mentioned example of a dense coding scheme for
qubits, we had @xmath . We used the maximally entangled state @xmath and
the transformations @xmath , where @xmath , and similarly for @xmath and
@xmath . In the qubit teleportation scheme, on the other hand, we had
@xmath , @xmath , as well as @xmath

Werner proves the following general result [ ref.Werner01 ] .

###### Theorem 2.3.

All tight teleportation or dense coding schemes in @xmath are obtained
by choosing @xmath for a maximally entangled state @xmath , @xmath for
an orthonormal basis of maximally entangled states @xmath and @xmath ,
where @xmath is chosen such that @xmath .

In Particular, Theorem 2.3 applies that there is a one-to-one
correspondence between tight teleportation and dense coding schemes.
Every such scheme needs a basis of maximally entangled states. Let us
remark that Werner proposed a construction of such bases, based on Latin
squares and complex Hadamard matrices, which also appear in the context
of mutually unbiased bases, to be discussed in more detail in Section
8.4 .

#### 2.3 Quantum metrology

In the last section concerning practical applications of quantum
entanglement, we shall give an example of how entanglement can be used
to increase phase sensitivity in a photon interferometry experiment. Our
discussion is based on the paper [ GB02 ] by Gerry and Benmoussa, but we
make a few remarks about related work by other authors. The very simple
experimental setup we would like to discuss is depicted in Figure 2.1 .
It consists of two photodetectors, a beam splitter, and a phase shifter.
Together, they make up a simple interferometer. An important part of the
experiment is also the photonic quantum state which is fed into the arms
of the interferometer, as well as the observable one calculates using
the measurement results from the photodetectors. The aim is to estimate
the phase @xmath , induced by the phase shifter on single photons. Such
phase may result e.g. from propagation through a thin layer of a medium
that has an index of refraction greater than the environment. In the
following, we argue that the estimation of @xmath can be made more
precise if one does exploit entanglement between @xmath photons
impinging on the beam splitter, instead of just repeating single-photon
measurements @xmath times.

We shall use the quantum-mechanical description of the optical
experiment in Figure 2.1 , the basics for which can be found in the
textbook [ GerryKnight , Chapter 6.] . In this formalism, the quantum
state of the photons leaving the beam splitter is described as an
element of a two-particle Fock space, with creation/annihilation
operators @xmath and @xmath corresponding to the upper and the lower
output arm of the interferometer, respectively. It should lead to no
confusion if we call the upper and the lower arm itself @xmath and
@xmath for convenience (cf. Figure 2.1 ). The corresponding
creation/annihilation operators satisfy the commutation relations

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

The vacuum state @xmath corresponds to no photons in arms @xmath and
@xmath , and it satisfies @xmath . We assume that @xmath is normalized.
Photon number states are subsequently defined as

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

They have the clear interpretation of states with @xmath photons in arm
@xmath and @xmath photons in arm @xmath of the interferometer. An
analogous construction works for the upper and lower input arm of the
interferometer, which we call @xmath and @xmath , the same as the
corresponding annihilation operators. Note that the upper arm is denoted
with @xmath and not with @xmath , the same as in Figure 2.1 . The
corresponding photon number states are denoted with @xmath .

In accordance with [ GerryKnight ] , if we have an input state @xmath
for some function @xmath of the creation operators @xmath and @xmath ,
then the output state of the interferometer equals

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

where @xmath and @xmath . Note that we use the same function @xmath ,
but we evaluate it for the creation operators @xmath and @xmath , not
for @xmath , @xmath . By a slight abuse of notation, we can therefore
write ( 2.9 ) as @xmath and consider the interferometer as a unitary
transformation on the input state, which yields an output in the output
Fock space. Let us denote @xmath The estimation of @xmath boils down to
the calculation of the expectation value of an appropriately chosen
observable @xmath on @xmath , from which we recover @xmath , i.e. we
measure @xmath and equate it to the theoretically predicted value of
@xmath for some @xmath . The number @xmath gives us an estimate of
@xmath . A widely applied formula for error propagation then provides us
with an estimate of the error of @xmath ,

  -- -- -- --------
           (2.10)
  -- -- -- --------

where @xmath is the standard deviation of @xmath . As shown in [ GB02 ]
, the choice

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

allows for a significant improvement in the precision of the measurement
of @xmath over a scenario where single-photon states of the type ( 2.11
) are measured @xmath times. The states ( 2.11 ) are called NOON states
[ LKCD02 ] for obvious reasons. It is not easy to create them [ KLD02 ]
, but significant progress has been made in that area in recent years,
cf. e.g. [ AAS10 ] . In the following, we will briefly explain how the
result of [ GB02 ] was obtained.

We already know which state @xmath to use, but we have not yet specified
the operator @xmath to measure. A suitable choice was suggested in [
BIWH96 ] , and it is

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

Note that @xmath is simply the photon number operator for the lower
output arm, so the expectation value of @xmath can be estimated from
experiment by measuring the number @xmath of clicks in the lower
detector and calculating @xmath . Of course, the experiment has to be
repeated many times to get a reliable estimate, equal to the average of
the expressions @xmath over individual runs. Note that we assume that
photodetectors are perfectly efficient, i.e. no photons are lost.

Once we know @xmath and @xmath , it is not very difficult to calculate
@xmath . In order to simplify the calculation, one can introduce

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

The operators @xmath with @xmath were introduced by Schwinger [
Schwinger ] and they satisfy the angular momentum commutation relations,
@xmath . The operator @xmath commutes with all of them and has the
interpretation of the total photon number observable (divided by two).

From the very useful Hadamard lemma (cf. e.g. [ Miller ] )

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

and the commutation relations ( 2.7 ), one quickly obtains the following
equalities

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

which give us

  -- -------- -- --------
     @xmath      (2.16)
  -- -------- -- --------

where we also used the equality @xmath . Another relation which follows
from ( 2.14 ) is

  -- -------- -- --------
     @xmath      (2.17)
  -- -------- -- --------

With ( 2.16 ) and ( 2.17 ) at hand, we can easily calculate @xmath .
Indeed, since @xmath and @xmath commutes with all @xmath , we get

  -- -------- -- --------
     @xmath      (2.18)
  -- -------- -- --------

Thus @xmath for @xmath odd and @xmath for @xmath even. These functions
readily allow us to recover @xmath from @xmath , up to a multiple of
@xmath . Since @xmath , we have @xmath and formula ( 2.10 ) yields the
following estimate for the error of @xmath ,

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

The above equality holds for both @xmath even and @xmath odd. The @xmath
dependence in formula ( 2.19 ) corresponds to so-called Heisenberg limit
, which is widely accepted as the minimum phase estimation error allowed
by quantum mechanics [ YMCSK86 , O96 , O97 ] . On the contrary, by
simply repeating a single photon experiment @xmath times, one gets a
precision @xmath , so-called shot-noise or standard quantum limit ,
which is significantly worse than ( 2.19 ) for large @xmath . In this
way, entanglement between the photons fed into the arms of the
interferometer can increase the phase sensitivity in the experiment by a
factor of @xmath . Compared to one single photon experiment, the
sensitivity is increased @xmath times. A very practical use of this
feature was proposed in [ BKABWD00 ] , where the authors suggest that
NOON states could be used to imprint details of minimum resolution
@xmath times better than usual in photolithography. In particular,
diffraction patterns resulting from the use of NOON states would have
the minimum resolution @xmath times greater than those obtained with
unentangled photons. This was called quantum lithography in [ BKABWD00 ]
. However, the original argument of [ BKABWD00 ] has recently met with
some criticism [ KBIB11 ] , and it is argued that in practice, the
efficiency of quantum lithography would be rather low.

### Chapter 3 Distillability and bound entanglement

#### 3.1 Distillation of quantum entanglement

As we have seen above, a central role in the most popular quantum tasks,
including quantum cryptography and teleportation, is played by maximally
entangled states. However, states encountered in practice never match
perfectly those used in the theory, due to experiment imperfections. In
the early days of quantum information science therefore, it appeared to
be crucial to answer the question whether a noisy entangled state can
somehow be “purified” to yield one that is closer to being maximally
entangled. A partially affirmative answer to this question was first
provided in [ BBPSSW96 ] for the case of two qubits and refined by the
authors of [ HHH97 ] . A method suitable for bipartite systems of
arbitrary dimension, based on the reduction criterion for separability,
was later presented in [ HH99 ] .

Let us briefly discuss a purification, or distillation protocol
developed by the authors of [ BBPSSW96 ] . The procedure starts with an
arbitrary mixed state @xmath of two qubits. The following steps are
designed to yield a state which is closer to @xmath in a sense described
below. However, it should be stressed that the method only works
provided that @xmath , i.e. @xmath is not too far from @xmath at the
outset. We call the parameter @xmath the fidelity of @xmath with respect
to the maximally entangled state @xmath .

1.  First, we apply a local unitary rotation @xmath to the second
    component of @xmath . This yields @xmath , a state which is as close
    to @xmath as @xmath was to @xmath , in the sense that @xmath .

2.  Second, we apply a random bilateral @xmath rotation to @xmath ,
    which effectively yields

      -- -------- -- -------
         @xmath      (3.1)
      -- -------- -- -------

    where @xmath refers to the Haar measure. In practice, the same goal
    can be achieved by randomly choosing the identity and bilateral
    @xmath , @xmath and @xmath rotations. The result of ( 3.1 ) is
    obviously @xmath -invariant, which implies that it must be one of
    the Werner states ( 1.9 ). In the @xmath case considered here, the
    Werner states take the specific form

      -- -------- -- -------
         @xmath      (3.2)
      -- -------- -- -------

    where @xmath and @xmath . Therefore @xmath is of the form given
    above, with @xmath . The last equality follows from the fact that
    @xmath is an @xmath -invariant state.

3.  In the next step, a unilateral @xmath rotation takes @xmath to
    @xmath . In this way, the mostly @xmath state is converted to a
    mostly @xmath one.

4.  Next, we take two copies of @xmath , prepared in the way described
    above, and use one of them as a “source” and the second one as a
    “target” for a BXOR gate, depicted in Figure 3.1 . A BXOR gate
    simply consists of two CNOT gates, applied to distinct pairs of
    source and target qubits.

5.  Next, the target pair of qubits is locally measured in the @xmath
    basis, as depicted in Figure 3.2 , which also includes the BXOR
    operation described above.

    If the results are the same for the qubits @xmath and @xmath , the
    remaining source pair @xmath is kept. Otherwise, it is discarded.

6.  If in the previous step the source pair was kept, it is transformed
    to an almost @xmath state by a unilateral @xmath rotation. Next, it
    is made rotationally symmetric by applying random bilateral @xmath
    rotations, as in equation ( 3.1 ). Let us call the resulting state
    @xmath . The corresponding parameter @xmath in formula ( 3.2 ) is
    then equal to

      -- -------- -- -------
         @xmath      (3.3)
      -- -------- -- -------

    which exceeds @xmath over the range @xmath . Thus @xmath is closer
    to @xmath than @xmath in the sense that @xmath .

7.  In the last step, the almost @xmath state @xmath is converted back
    to an almost @xmath one by a unilateral @xmath rotation. We call the
    resulting state @xmath . The corresponding parameter @xmath is
    bigger than @xmath . Thus, the resulting state is closer to @xmath
    than @xmath was.

As a result, by repeating the above procedure, states which are
arbitrarily close to @xmath can be obtained. Nevertheless, the number of
copies of @xmath needed for the input grows very fast as the expected
fidelity goes to @xmath . Thus, for practical purposes, another
procedure of distillation was designed by the authors of [ BBPSSW96 ] ,
which more efficiently uses the statistical properties of @xmath .
However, it needs a small input of @xmath states, which may be obtained
by the method described above. The mentioned procedure consists of two
rounds of BXOR tests performed on suitably chosen subsets of the whole
supply of @xmath states, using the prepurified @xmath states as targets.
It also uses unilateral and bilateral @xmath rotations, as well as
unilateral @xmath rotations to correct the discrepancies from @xmath
detected by the BXOR operations. More details of the procedure can be
found in [ BBPSSW96 ] . All in all, from a theorist point of view, it is
sufficient to say that all mixed states @xmath of two qubits with @xmath
can be distilled to the maximally entangled state. This result was
further extended, by using the technique of local filters [ Gisin96 ] ,
to arbitrary entangled states of two qubits [ HHH97 ] . In this way, the
authors of [ HHH97 ] showed that any entangled state of two qubits has
some form of nonlocality, which is revealed by the distillation
procedure. Note that a similar result for bipartite states of arbitrary
dimension would have resolved the paradox of Werner’s paper [ Werner89 ]
, which we discussed in Section 1.2 . However, it was quickly realized
that the existence of PPT entangled states, first revealed to the
physicist’ community by the paper [ Pawel97 ] , immediately precludes
the described strategy from working [ HHH98 ] . Let us briefly explain
why this is the case.

In an ideal case, given a source characterized by a bipartite density
matrix @xmath , we have at our disposal the tensor product states @xmath
for arbitrary @xmath . The most general transformation one can perform
on @xmath using only local operations and classical communication is of
the form [ VPRK97 ]

  -- -- -- -------
           (3.4)
  -- -- -- -------

where @xmath and @xmath map into image space in the first and the second
subsystem, respectively. In the case of entanglement distillation, both
the image spaces are @xmath , as we want to obtain the state @xmath ,
living in @xmath . Therefore, @xmath and @xmath , assuming that @xmath
lives on @xmath . Now assume that @xmath has a positive partial
transpose. Thus, @xmath is a PPT state as well. One can also easily
notice that the mapping @xmath preserves the positivity of the partial
transpose of @xmath . Hence the state on the right-hand side of ( 3.4 )
is a PPT state living on @xmath . Consequently, it is separable [ HHH96
] and cannot be distilled (cf. also the discussion in Section 1.2 ). In
this way we have proved the following [ HHH98 ] .

###### Proposition 3.1.

No PPT state living on a bipartite space @xmath can be distilled to
@xmath .

As a result, all PPT entangled states, including those presented in [
Pawel97 ] , cannot be distilled to @xmath , even though they are not
separable. Due to their undistillability, the states are called bound
entangled . For them, the paradox from the Werner’s paper [ Werner89 ]
cannot be resolved by using distillation protocols. Let us mention,
however, that the original Werner states are positive partial transpose
if and only if they are separable. This still does not allow us to
conclude that all entangled Werner states can be distilled to @xmath ,
as there might exist NPT bound entangled states, i.e. undistillable
states which are not PPT. This has become a central, still unresolved
problem in the theory of entanglement, so-called NPT bound entanglement
existence problem . Actually, it was demonstrated in [ DiVicenzo2000 ]
(cf. also [ DCLB00 ] ) that the question whether there exist NPT bound
entangled states only needs to be answered for the Werner family of
states, since all the other ones can be brought to the Werner form by
transformations that do preserve the positivity of the partial
transpose. However, it turns out that the question for Werner states
becomes increasingly difficult to answer as the parameter @xmath in the
definition ( 1.9 ) tends to the boundary value @xmath . Namely, it was
proved in [ DiVicenzo2000 ] that for any @xmath , there exists @xmath
such that the state @xmath from eq. ( 1.9 ) with @xmath cannot be
distilled using operations of the form ( 3.4 ) on @xmath (however, some
of these states may be distillable using @xmath or more copies of @xmath
). Since then, considerable efforts have been made to prove or disprove
the existence of NPT bound entangled states, none of which have lead to
a conclusive answer [ Bandy03 , Watrous04 , Clarisse05 , Clarisse06 ,
VD06 , Chatto06 ] . Moreover, two contradictory statements concerning
the problem can be found in the preprints [ Simon06 , SV09 ] , none of
which is correct. One thing beyond any doubt is that the question of
distillability intimately relates to the structure of @xmath -positive
maps, i.e. positive maps @xmath with the property that the map

  -- -- -- -------
           (3.5)
  -- -- -- -------

denoted with @xmath , is also positive. To see the relation of @xmath
-positivity to distillability, let us first note the following
characterization of distillable states [ HHH98 , DiVicenzo2000 ] .

###### Proposition 3.2.

A state with a density matrix @xmath on @xmath is distillable if and
only if there exists a finite @xmath and two-dimensional projections
@xmath , @xmath in @xmath and @xmath , resp. such that @xmath ,
supported on a @xmath -dimensional space, is entangled. The last
condition is equivalent to the statement that there exists a vector
@xmath of the form @xmath in @xmath such that

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where @xmath denotes the partial transpose with respect to the second
subsystem, @xmath .

###### Proof.

As we mentioned above, the most general distillation operation one can
perform on @xmath is of the form ( 3.4 ). In order for the transformed
state @xmath transformed to be entangled, and thus distillable (remember
that end up with states on @xmath ), at least one of the terms @xmath ,
supported on a @xmath -dimensional subspace, needs to be entangled. The
operators @xmath and @xmath are of the form @xmath and @xmath , where
@xmath belong to @xmath and @xmath belong to @xmath . Let us denote with
@xmath and @xmath the projections onto the subspaces @xmath and @xmath ,
respectively. We have

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

Since a product transformation cannot convert a separable state into an
entangled one, we must have that @xmath is entangled. This proves the
necessity in the first part of the proposition. In order to prove the
sufficiency, it is enough to notice that the projected state @xmath , if
entangled, can be distilled, because it is supported on a @xmath
-dimensional subspace.

To prove the second part of the proposition, we observe the following.
Because @xmath is supported on a @xmath -dimensional subspace @xmath , a
necessary and sufficient condition for @xmath to be entangled is that it
does not have a positive partial transpose. The partial transpose equals

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

where @xmath denotes an operator represented by the complex conjugated
matrix of @xmath . Thus @xmath is also a two-dimensional projection.

The above operator is not positive if and only if there exists a vector
of the form @xmath in @xmath that fulfills the inequality @xmath . This
simply follows because all the vectors in @xmath are of the form @xmath
, i.e. are of Schmidt rank @xmath . But @xmath according to our choice
of @xmath , which finishes the proof of the second part of the
proposition. ∎

Another way of phrasing the above result is that the operator @xmath is
not @xmath -block positive for some @xmath (cf. e.g. [ ref.SSZ09 ] ). By
@xmath -block positivity of an operator @xmath on a bipartite Hilbert
space @xmath we mean the property that @xmath for all @xmath of the form
@xmath (in particular, we can choose @xmath and @xmath ). Thus, by
Proposition 3.2 , a state @xmath on @xmath is distillable if for some
@xmath the state @xmath is not @xmath -block positive. Since @xmath
-block positive operators are in a one-to-one Jamiołkowski-Choi
correspondence to @xmath -positive maps [ RA07 , ref.SSZ09 ] , there is
a direct link between distillability of entanglement and the property of
not being a @xmath -positive map. For additional insights, consult [
Clarisse05 ] .

It was quickly realized [ HHH99 ] that bound entanglement, even though
it is useless for entanglement distillation, can be used to improve
fidelity of a given distillable ( @xmath free entangled) state @xmath in
a process very similar to the one depicted in Figure 3.2 . To this aim,
a copy of the free entangled state @xmath together with a copy of a
bound entangled state @xmath are passed as inputs to the circuit in
Figure 3.3 , where @xmath , an analogue of the CNOT gate used in Fig 3.2
. Later, the target pair (the upper one in Fig. 3.3 ) is measured in the
basis @xmath . If both measurements agree, the source pair (initially in
the state @xmath ) is kept and assumes a new state @xmath of higher
fidelity. Otherwise, it is discarded an the whole procedure fails. If
the run was successful, the described steps are repeated for @xmath and
another copy of @xmath as the source and the target pair, respectively.
It can be shown that a sequence of successful runs of the above scheme
leads, with a nonvanishing probability, to a state of an arbitrary high
fidelity. This phenomenon is called bound entanglement activation [
HHH99 ] . The precise form of the states @xmath and @xmath will be given
in Section 3.2 .

#### 3.2 Examples of bound entangled states

We already know from the previous section that the question about the
existence of undistillable states with negative partial transpose is
still an unsolved problem in the theory of entanglement. Thus no example
of an NPT bound entangled state is known. On the other hand, numerous
successful efforts have been made to give explicit examples of bound
entangled states that do obey the PPT criterion. Here, we give a list of
references where the known examples can be found. For some of them, we
provide the reader with the precise form of the state and briefly
discuss how it was proved to be entangled.

Probably the most famous example in the physics literature is the @xmath
Horodecki state, named after P. Horodecki work [ Pawel97 ] . The name
refers to a one-parameter family of states, given in the canonical
product basis of @xmath by the matrices

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath . As we already mentioned in Section 1.2 , the state @xmath
can be proved to be entangled by using the range criterion, which is our
Proposition 1.6 . Indeed, with some amount of algebra [ Pawel97 ] , one
can show that the vectors in @xmath , the range of @xmath , belong to
one of the following families

  -- -- -------- -------- -- --------
        @xmath   @xmath      (3.10)
        @xmath   @xmath      (3.11)
        @xmath   @xmath      (3.12)
        @xmath   @xmath      (3.13)
        @xmath   @xmath      (3.14)
  -- -- -------- -------- -- --------

where @xmath . The partially conjugated vectors ( 3.10 )-( 3.14 ) do not
span the range of @xmath , as they cannot be linearly combined to yield
@xmath , which is an element of @xmath . In this way, the author of [
Pawel97 ] arrived at a contradiction with the range criterion for the
state @xmath . Hence @xmath was proved to be entangled, so that @xmath
is entangled as well. A similar method was later used in the paper [
Clarisse06b ] , which contains first examples of @xmath PPT entangled
states of types @xmath and @xmath . Here @xmath means that a PPT state
@xmath has rank @xmath , while the rank of @xmath equals @xmath . The
reduction criterion was also employed, in a very straightforward way, to
prove inseparability of a family of PPT chessboard states, introduced in
[ BrussPeres ] . They are states of the form @xmath , where

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (3.15)
     @xmath   @xmath   @xmath      (3.16)
     @xmath   @xmath   @xmath      (3.17)
     @xmath   @xmath   @xmath      (3.18)
  -- -------- -------- -------- -- --------

According to the main result of the thesis, Theorem 9.27 , the
chessboard states are of the type @xmath and they are locally equivalent
to states arising from the Unextendible Product Basis construction to be
discussed below.

Shortly after the first example of a bound entangled state in the
physics literature, C. H. Bennett and coworkers [ Bennett99 ] proposed a
fully algorithmic way to construct more such examples. The method relies
on the notion of an Unextendible Product Basis , which is formally
defined in the following way.

###### Definition 3.3.

An Unextendible Product Basis, UBP for short, is a set of mutually
orthogonal product vectors @xmath in a multipartite Hilbert space @xmath
such that the orthogonal complement @xmath does not contain a product
vector .

Given a UPB in a bipartite space, it is straightforward to give an
example of a PPT entangled state.

###### Proposition 3.4.

Let @xmath be an Unextendible Product Basis in @xmath . The projection

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

defines a PPT bound entangled state @xmath , where @xmath is a suitable
normalization factor.

The proposition follows because the subspace on which @xmath projects,
contains no product vector. Hence, using the range criterion, the state
proportional to ( 3.19 ) is entangled. The fact that it also has a
positive partial transpose can be checked by a simple calculation.
Indeed,

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

where @xmath denotes componentwise conjugation, is another projection,
hence positive definite. Generalizations to a multipartite setting are
immediate.

In the main part of the thesis, we prove Theorem 9.27 , which says that
all PPT bound entangled states of rank @xmath in @xmath systems are
locally equivalent to states of the form ( 3.19 ). This means that any
such state is proportional to @xmath for some UPB and some @xmath
transformations @xmath and @xmath . In this way we obtain a full
characterization of simplest PPT entangled states, as all PPT states of
ranks @xmath are separable [ HLVC2000 ] .

As far as the above examples are considered, the reduction criterion
seems to be the only way to prove that a given PPT state is entangled.
But in reality, it is not the only one known in literature. Another
distinguished approach to the problem is by using so-called
indecomposable positive maps . By the positive maps criterion, the
existence of a positive map @xmath such that @xmath implies
inseparability of a state @xmath . It is precisely in this way that the
earliest examples of PPT entangled states [ Choi82 , Erling82 ] were
obtained by mathematicians ¹ ¹ 1 Note however that the name “bound
entanglement” was not used until [ HHH98 ] . The exemplary PPT entangled
state given in [ Erling82 ] is of the form

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

which can be more concisely written as @xmath , where

  -- -------- -- --------
     @xmath      (3.22)
     @xmath      (3.23)
  -- -------- -- --------

and @xmath stands for the maximally entangled vector, @xmath .

A slightly modified family of states @xmath was later used to
demonstrate the phenomenon of bound entanglement activation [ HHH99 ] ,
which we briefly described in Section 3.1 . The authors of [ HHH99 ]
also used a related family @xmath as their input free entangled states.

Another notable example of a class of PPT entangled states revealed by
indecomposable positive maps was given in [ Piani06 ] . We should also
mention a series of papers by K.-C. Ha and co-workers [ HaKyePark2003 ,
HaKye2004 , HaKye2005 ] , where the authors develop a possible general
approach to constructing PPT entangled states from faces of the cone of
all decomposable positive maps. In particular, they consider a family of
generalized Choi maps, introduced in [ CKL92 ] and use them to construct
the corresponding bound states. For the definition of decomposability
and related notions, check e.g. [ ref.SSZ09 ] .

## Part II A brief introduction to algebraic geometry

### Chapter 4 Varieties, Ideals and Groebner bases

#### 4.1 Preliminaries

Just as we mentioned in previous parts of the thesis, problems
encountered in the theory of quantum channels, measurement and
entanglement are often of purely algebraic nature. More precisely, they
pertain to the existence of solutions of certain algebraic equations or,
for example, to positivity of a number of polynomials. In order to
answer such questions in an effective way, one can uses techniques such
as Groebner bases or resultans, which we briefly discuss in the
following. By their effectiveness we mean the fact that a decisive
answer to a question is obtained in a finite, though sometimes rather
high, number of steps. We also include a proof of Bezout’s theorem,
which we later use to prove the main result of the thesis, concerning
PPT bound entangled states of minimal rank.

Before we introduce the ideas of Groebner bases, let us begin with an
introduction to basic notions of algebraic geometry. A more
comprehensive treatment of the subject can be found in a book like [
IdealsVarieties ] , which we recommend to everyone new to the subject.
By @xmath we shall denote the set of @xmath -variate polynomials in the
variables @xmath and coefficients in @xmath . The two main cases
considered in this thesis are @xmath and @xmath . With this notation,
let us define the basic object of algebraic geometry.

###### Definition 4.1.

By an affine variety we mean a subset of @xmath defined by a set of
equations

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

We shall denote it by @xmath

We can give a simple, although not an entirely trivial example of an
affine variety in @xmath (or @xmath ), which reappears, in somewhat
generalized form of a rational normal curve, in one of the papers
related to the thesis [ S2011 ] .

###### Example 4.2 (Twisted cubic).

The affine variety defined by the set of equations

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

is called the twisted cubic curve.

As we will explain, a concept intimately related to affine varieties is
that of ideals. An ideal can be regarded as a generalization of a linear
subspace, where the arbitrary scalar factors in linear combinations are
replaced by arbitrary polynomials. More formally, we have the following
definition.

###### Definition 4.3 (Ideal).

A subset @xmath

1.  @xmath

2.  @xmath

3.  @xmath

is called an ideal in @xmath .

It turns out (cf. Theorem 4.18 ) that all ideals @xmath are finitely
generated, which means that there always exists a finite set @xmath such
that all elements of @xmath can be written in the form @xmath with
@xmath and no element of @xmath is of that form. Let us make it more
formal.

###### Definition 4.4.

For a subset @xmath , we denote by @xmath the ideal generated by @xmath
, which is by definition the minimal ideal including @xmath . If @xmath
, we write @xmath and say that the ideal @xmath is finitely generated.
Equivalently, @xmath consists of all elements of the form @xmath , where
@xmath for all @xmath .

###### Proof.

Only the last statement needs a proof. First of all, let us denote by
@xmath the set of all elements of the form @xmath . Clearly, by the
definition of an ideal, we have @xmath . Let us also observe that @xmath
is an ideal. Since @xmath is by definition the smallest ideal containing
@xmath , we must have @xmath , which gives us the equality @xmath . ∎

A fixed ideal @xmath may have various sets of generators. One of the
crucial observations of algebraic geometry is that the variety defined
by a set of equations @xmath depends only on the ideal @xmath and not on
the particular set of generators.

###### Proposition 4.5.

Let @xmath . In such case @xmath

###### Proof.

From the last part of Definition 4.4 we know that @xmath for some
polynomials @xmath . Thus @xmath implies @xmath . Consequently, @xmath .
The inverse inclusion can be obtained in a similar way. ∎

Apart from @xmath , there exists another ideal intimately related to
@xmath , namely the ideal of polynomials that vanish on @xmath .

###### Definition 4.6.

Let @xmath be an affine variety in @xmath . The ideal of @xmath is by
definition

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

The above definition is easily generalized to arbitrary subsets in place
of @xmath .

###### Definition 4.7.

Let @xmath be a subset of @xmath . The ideal of @xmath is by definition

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

We leave it as an exercise for the reader to prove that @xmath is an
ideal. Moreover, the maps @xmath and @xmath are inclusion reversing. We
also have the following

###### Proposition 4.8.

For any affine variety @xmath , we have

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

###### Proof.

We know that @xmath and @xmath because all the polynomials @xmath vanish
on @xmath . Consequently, @xmath since @xmath is inclusion-reversing. On
the other hand, the inclusion @xmath follows directly from the fact that
every @xmath vanishes on @xmath . ∎

We can also characterize @xmath for arbitrary subsets @xmath of @xmath .

###### Proposition 4.9.

For @xmath , the affine variety @xmath is the smallest variety that
contains @xmath .

###### Proof.

Let @xmath be an affine variety such that @xmath . Since @xmath is
inclusion-reversing, we have @xmath . Moreover, @xmath because @xmath is
inclusion-reversing. Finally, @xmath , by Proposition 4.8 and the fact
that @xmath is an affine variety. Thus @xmath for any affine variety
@xmath that contains @xmath . ∎

A natural question to ask is whether @xmath . The answer in general is
no , however, under algebraically closed fields like @xmath , there is a
precise criterion, called Nullstellensatz , which allows to check
whether the equality occurs. It can be found in Theorem 4.35 of Section
4.3 .

A number of other questions come very naturally with the notions of an
ideal and an affine variety. Let us give a list of three of them, which
will be answered to in the following.

1.  Does every ideal in @xmath have a finite set of generators? In other
    words, can we always write @xmath for some polynomials @xmath ?

2.  How can we check whether a given polynomial @xmath belongs to an
    ideal @xmath ?

3.  How can we solve a system of polynomial equations @xmath @xmath
    @xmath , i.e. find a parametric description of (a part of) the
    affine variety defined by the equations. Under which conditions
    solutions do exist at all?

In order to better understand the above questions, it is useful to give
a short summary of how they are answered in the univariate case, @xmath
. First of all, let us mention that the leading term of @xmath ( @xmath
) is by definition equal to @xmath , the leading coefficient is @xmath
and the leading monomial is @xmath . Let us denote them by @xmath ,
@xmath and @xmath , respectively. Let us also denote the degree of
@xmath by @xmath . Given two univariate polynomials @xmath , @xmath ,
there is a unique way of writing @xmath as

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where @xmath and either @xmath or @xmath . The classical division
algorithm in @xmath that produces @xmath and @xmath given @xmath and
@xmath consists in the steps given in Figure 4.2

We can now answer question one in the case of univariate polynomials.

###### Proposition 4.10.

Every ideal in @xmath is generated by a single polynomial @xmath , which
is the polynomial of lowest degree in @xmath .

###### Proof.

Clearly, there must exist a polynomial of lowest degree in @xmath . Let
us denote it by @xmath . We shall prove that @xmath . Clearly, @xmath .
If there existed a polynomial @xmath , we could divide @xmath by @xmath
and produce a polynomial @xmath as in formula ( 4.6 ). Since @xmath ,
@xmath . It would satisfy @xmath and @xmath , which is a contradiction,
because we assumed that @xmath is the polynomial of minimal degree in
@xmath . ∎

Question two also has an immediate answer in the univariate case. Since
every ideal in @xmath is of the form @xmath for some @xmath , it is
sufficient to divide an arbitrary polynomial @xmath by @xmath to check
whether @xmath belongs to the ideal or not. If @xmath , it belongs to
the ideal, and if @xmath , it does not. As it is well known from basic
algebra courses, solutions to univariate polynomial equations of the
form @xmath always exist in case of @xmath and other algebraically
closed fields, but may fail to exist when the base field is not
algebraically closed. Explicit general solutions in @xmath are only
known for @xmath of degree up to @xmath as a consequence of the
Abel-Ruffini theorem, cf. e.g. [ Fraleigh ] . Note that the question,
whether solutions exist or not, starts to be non-trivial if we pass to
multiple polynomial equations or a multivariate setting, even if the
base field is algebraically closed (e.g. when it equals @xmath ). In
such case, the techniques of Groebner bases and resultants are of much
help. We shall discuss both in subsequent sections of the thesis.

#### 4.2 Monomial orders and Groebner bases

Let us now pass from one-variable polynomials, discussed at the end of
the previous subsection, to the multivariate setting. We shall avoid
excess notation by using the symbol @xmath with multi-indices @xmath in
place of @xmath . In order to introduce an analogue of the division
algorithm in @xmath , we need to specify what is a leading term of a
multivariate polynomial. Unlike for univariate polynomials, the notions
of the leading term, leading coefficient or monomial are not uniquely
defined. There are many possible choices and one needs to specify an
ordering of monomial terms in order to do multivariate polynomial
division in a sensible way. The orderings also have to respect the
multiplicative and additive structure of @xmath , so they fulfill a
number of constraints. In such case we call them monomial orderings.

###### Definition 4.11 (Monomial ordering).

A monomial ordering in @xmath is any relation @xmath on the set of
monomials in @xmath which fulfills

1.   the ordering @xmath is linear, which means that for any monomials
    @xmath and @xmath , @xmath , either @xmath or @xmath .

2.   If @xmath then @xmath for any multi-index @xmath .

3.   The relation @xmath is w well-ordering, which means that for any
    set of monomials @xmath , there exists a smallest element under the
    ordering @xmath .

In the following, we introduce three most common examples of monomial
orderings.

###### Example 4.12 (Lexicographic order).

Let @xmath and @xmath be monomials in @xmath . We have @xmath if and
only if @xmath has the left-most nonzero entry positive .

###### Example 4.13 (Graded lexicographic order).

Let @xmath and @xmath be monomials in @xmath . We have @xmath if and
only if

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where @xmath denotes the total degree of @xmath . In other words @xmath
if and only if @xmath has a higher total degree than @xmath or has the
same total degree and @xmath .

###### Example 4.14 (Graded Reverse Lexicographic Order).

Let @xmath and @xmath be monomials in @xmath . We have @xmath if and
only if

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

and in @xmath the right-most nonzero entry is negative .

We can now introduce an analogue of the univariate division algorithm in
Figure 4.2 . Let @xmath and @xmath be arbitrary and fix a monomial
ordering in @xmath . There exist @xmath , @xmath and @xmath such that

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

and no monomial of @xmath is divisible by any of the leading monomials
@xmath . Moreover, @xmath . Obviously, @xmath and @xmath in the above
formula are analogues of @xmath and @xmath in equation ( 4.6 ), while
the condition on monomial terms of @xmath corresponds to @xmath in the
univariate setting. An algorithm which gives a decomposition of the form
( 4.9 ) is shown in Figure 4.3 .

In short, the algorithm tries to divide the leading term of @xmath by
the leading terms of @xmath , @xmath . If this is not possible, the
leading term is added to the division remainder and the whole procedure
repeated from the beginning. Note that the ordering of the polynomials
@xmath has an influence on the result of division. In particular, the
remainder @xmath may depend on how the polynomials @xmath are ordered
and thus is not uniquely defined. The last feature can be seen in the
following example

###### Example 4.15.

Let @xmath , @xmath , @xmath and the take the @xmath order in @xmath .
The multivariate division algorithm gives us

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

However, with the choice @xmath , @xmath , @xmath , we get

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

instead.

We see from ( 4.10 ) and ( 4.11 ) that the condition @xmath is not
equivalent to @xmath . We shall see that with a proper choice of the
ideal basis, a Groebner basis , both conditions can be made equivalent
and the remainder @xmath ceases to be ordering dependent, though it
still depends on the particular monomial order we choose in @xmath .

First, we need to introduce the notion of monomial ideals and
investigate their basic properties. A monomial ideal is simply the ideal
generated by a set of monomials in @xmath . More formally, we have the
following definition

###### Definition 4.16 (Monomial ideal).

Let @xmath be a subset of @xmath consisting of componentwise nonnegative
elements. A monomial ideal corresponding to @xmath is the smallest ideal
in @xmath containing @xmath .

We shall denote by @xmath the monomial ideal generated by @xmath . It
turns out that all monomial ideals admit a finite set of generators.
This is the contents of the following Dickson’s lemma .

###### Lemma 4.17 (Dickson’s).

Let @xmath be a monomial ideal. There exists a finite set @xmath such
that @xmath

###### Proof.

Can be found in algebraic geometry textbooks like [ IdealsVarieties ] .
∎

With the Dickson’s lemma at hand, one can prove a key theorem about
ideals in @xmath .

###### Theorem 4.18 (Hilbert basis theorem).

Every ideal @xmath is finitely generated. Thus, there exist @xmath such
that @xmath . In particular, every @xmath with the property @xmath form
an admissible set of generators of @xmath .

###### Proof.

Consider the monomial ideal @xmath . According to Dickson’s lemma, there
exist a finite set of generators of @xmath , which are necessarily of
the form @xmath . We shall prove that @xmath generate @xmath . If that
was not the case, there would exist @xmath . Let us divide @xmath by
@xmath using the algorithm given in Figure 4.3 . It necessarily gives us
@xmath with @xmath by our assumption that @xmath is not in @xmath .
However, @xmath is an element of @xmath and thus @xmath an element of
@xmath . It must therefore be divisible by one of the generators of
@xmath . That is, it must be divisible by one of the @xmath , which is a
contradiction because of the properties of the remainder @xmath on
division by @xmath . ∎

We can also prove the following useful result

###### Corollary 4.19 (Ascending chain condition).

Let @xmath be a sequence of ideals in @xmath . The sequence stabilizes
for some finite @xmath , i.e. @xmath for all @xmath .

###### Proof.

It is easy to check that the set @xmath is an ideal in @xmath .
According to the above theorem, there exists a finite basis @xmath of
@xmath . According to the definition of @xmath , we must have @xmath for
some @xmath . Let us choose @xmath . Since @xmath and @xmath for all
@xmath , we clearly see that @xmath . Thus, @xmath for all @xmath . ∎

In the spirit of Theorem 4.18 , a Groebner basis is defined as a finite
subset @xmath of an ideal @xmath with the property that @xmath .

###### Definition 4.20 (Groebner basis).

Let @xmath be an ideal. A Groebner basis of @xmath is a finite subset
@xmath such that @xmath . In other words, a Groebner basis is a finite
set of polynomials in @xmath with the property that their leading terms
generate the ideal of leading terms of polynomials in @xmath .

Let us list a few properties of Groebner bases.

1.  A Groebner basis of an ideal @xmath generates @xmath . In other
    words, it is a basis of the ideal in the usual sense ,

2.  There exists a Groebner basis an an arbitrary ideal @xmath ,

3.  The remainder of @xmath on division by a Groebner basis @xmath is
    uniquely defined .

Points one and two follow directly from the proof of Theorem 4.18 . We
shall give a more formal version of point three in the following
proposition [ IdealsVarieties ] .

###### Proposition 4.21.

Let @xmath be a Groebner basis for an ideal @xmath and let @xmath . Then
there is a unique @xmath with the following two properties

1.   No term of @xmath is divisible by any of @xmath ,

2.   There is @xmath such that @xmath .

In particular, the polynomial @xmath is the remainder on division of
@xmath by @xmath , no matter how the elements of @xmath are listed when
using the division algorithm.

###### Proof.

An @xmath with the properties @xmath and @xmath can be obtained using
the division algorithm shown in Figure 4.3 . Let us prove the uniqueness
of @xmath . Assume, on the contrary, that for some @xmath , @xmath where
@xmath and both @xmath and @xmath satisfy @xmath and @xmath . Thus
@xmath is an element of @xmath with @xmath . By the definition of a
Groebner basis and @xmath , the leading term must be divisible by some
@xmath , @xmath , which is a contradiction, because by @xmath , the
monomials of @xmath and @xmath are not divisible by any @xmath . ∎

Note that by now, we already have an answer to the ideal membership
question (number two) raised on page 4.1 . Provided a Groebner basis
@xmath , we simply divide @xmath by @xmath using the division algorithm
of Figure 4.3 and check whether @xmath or not. Let us state this as a
proposition.

###### Proposition 4.22.

Let @xmath be a Groebner basis of an ideal @xmath . A polynomial @xmath
belongs to @xmath if and only if the remainder of @xmath on division by
@xmath equals @xmath .

###### Proof.

If the remainder is zero, we clearly have @xmath . On the other hand,
assume that @xmath is an element of @xmath and @xmath . In such case,
@xmath and @xmath ∎

We will see shortly that a Groebner basis of an ideal can be found by
Buchberger’s algorithm [ Buchberger ] in a finite number of steps. Thus
the ideal membership problem can also be solved in a finite number of
steps by calculating the remainder of @xmath on division by a Groebner
basis. For future convenience, let us denote such remainder by @xmath .

In the light of the above developments, it is important to know which
bases of an ideal are Groebner bases, and how to find a Groebner basis
of a given ideal, possibly of a nice form and unique in some sense.
Fortunately, there exist simple answers to all these questions and we
shall explain them in the following. First, we introduce the notion of
so-called @xmath -polynomial [ Buchberger ] .

###### Definition 4.23 (@xmath-polynomial).

Given two polynomials @xmath and some monomial order @xmath , take
@xmath and @xmath . The @xmath -polynomial of @xmath and @xmath is
defined to be

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

where @xmath is a multi-index @xmath defined such that @xmath for @xmath
and @xmath

The @xmath -polynomial is defined such that a cancellation of leading
terms of @xmath and @xmath occurs, and some new leading terms can
possibly be produced.

Let us now state without a proof a key result of Groebner basis theory,
called Buchberger’s @xmath -pair criterion [ Buchberger ] .

###### Theorem 4.24 (@xmath-pair criterion).

A basis @xmath of an ideal @xmath is a Groebner basis of @xmath if and
only if

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

The above criterion suggests an algorithm how to find a Groebner basis
of an ideal @xmath , given a set of generators @xmath . If we calculate
all the possible remainders @xmath and some of them turn out to be
nonzero, we add them to @xmath and repeat the whole procedure for the
extended set of generators. At some point, this extension procedure
should terminate, and the @xmath -pair criterion tells us that we have
obtained a Groebner basis of the ideal @xmath . A more precise
description of the algorithm is shown in Figure 4.4 . We also state its
correctness as a separate theorem.

###### Theorem 4.25 (Buchberger’s algorithm).

The algorithm given in Figure 4.4 returns a Groebner basis @xmath of the
ideal @xmath in a finite number of steps

###### Proof.

The additional elements @xmath , @xmath , produced by the algorithm,
belong to @xmath . This follows inductively because at each step the
@xmath -polynomials @xmath and their remainders @xmath belong to the
same ideal as @xmath do. Moreover, the algorithm terminates if and only
if at some point all the remainders @xmath vanish, which is equivalent
to say, by Theorem 4.24 , that the set @xmath is a Groebner basis of the
ideal @xmath . Thus, we only need to show that the algorithm terminates.
This will be done with help of the ascending chain condition, Corollary
4.19 .

Let us try to assume that a sequence @xmath produced by the algorithm
does not terminate. We have a corresponding sequence of ideals

  -- -------- -- --------
     @xmath      (4.14)
     @xmath      (4.15)
     @xmath      
     @xmath      (4.16)
     @xmath      
  -- -------- -- --------

which must stabilize according to Corollary 4.19 . However, the
algorithm in Figure 4.4 works is such a way that whenever an element
@xmath is added to a sequence @xmath , its leading term @xmath is not
divisible by any of the leading terms @xmath . Thus

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

for all @xmath . This shows that ( 4.14 )-( 4.16 ) forms a strictly
increasing sequence of ideals in @xmath , which is impossible according
to the ascending chain condition. The only possible solution is that the
algorithm always terminates, so that it never produces an infinite
sequence of polynomials @xmath ∎

The Groebner bases obtained by the algorithm in Figure 4.4 are not
optimal in many respects. First of all, different bases can be obtained,
depending on the choice of the order of the inputs @xmath . Moreover, it
may happen that a polynomial @xmath in the output sequence @xmath has a
leading term @xmath which divisible by some of the leading terms of the
polynomials in @xmath . In such case @xmath is another Groebner basis of
the ideal @xmath with a smaller number of elements. A Groebner basis
where no such reduction is possible and all the leading coefficients are
equal to unity, is called a minimal Groebner basis .

###### Definition 4.26 (Minimal Groebner basis).

A Groebner basis @xmath of an ideal in @xmath with @xmath is called
minimal if and only if for any element @xmath , the leading term @xmath
is not divisible by any of the leading terms of the polynomials in
@xmath .

Clearly, a minimal Groebner basis of an ideal @xmath can be obtained
from an arbitrary Groebner basis @xmath of @xmath by first normalizing
the leading terms and then removing all the elements which have their
leading term divisible by the leading term of some other polynomial in
@xmath . It can be proved [ IdealsVarieties ] that the minimal Groebner
bases of an ideal @xmath have identical sets of leading coefficients,
however there usually exist multiple minimal Groebner bases of a given
ideal @xmath . This ambiguity can be entirely removed if we impose one
further condition on the Groebner basis we are looking for.

###### Definition 4.27 (Reduced Groebner basis).

A minimal Groebner basis @xmath of an ideal @xmath is called reduced if
and only if for all @xmath , no monomial of @xmath is divisible by any
of the leading terms of polynomials in @xmath .

With this definition, we have

###### Proposition 4.28.

There exists a unique reduced Groebner basis of any ideal @xmath .
Moreover, given a minimal Groebner basis @xmath of @xmath , the reduced
Groebner basis @xmath can be found by the following procedure

    for all g in G’ do
          g=Remainder(g,G\{g})

###### Proof.

Can be found in [ IdealsVarieties ] . ∎

Let us mention that Proposition 4.28 provides one with an algorithmic
way to solve the ideal equality problem . Given two ideals @xmath and
@xmath , one has the equality @xmath if and only if the corresponding
reduced Groebner bases, which can be computed in a finite number of
steps, are equal.

We see that Groebner bases allow us to answer a number of questions,
including the ideal membership and ideal equality problems. Moreover, it
turns out that they can be used to find solutions to sets of polynomial
equations, which is very interesting from a practical perspective and it
will turn out to be crucial in some parts of the thesis. A simplest way
to see how Groebner bases can be used for this new task is to look into
a concrete example. Consider the following set of polynomial equations:

  -- -------- -- --------
     @xmath      (4.18)
     @xmath      (4.19)
     @xmath      (4.20)
  -- -------- -- --------

A Groebner basis calculation using the lexicographic order with @xmath
for the ideal @xmath provides us with @xmath . Since these polynomials
generate the same ideal as the polynomials in ( 4.18 )-( 4.20 ), we have
an equivalent set of equations:

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

The equations ( 4.18 )-( 4.20 ) do not look much more complicated than
those in ( 4.21 ), but at a first glance, it is not clear how to solve
them. On the other hand, the first equation in ( 4.21 ) involves only
the variable @xmath and it clearly has only two solutions, @xmath . The
solutions hence obtained can later be substituted for @xmath in the
latter two equations in ( 4.21 ). In this way, one can determine the
corresponding values of @xmath and @xmath and find all solutions to the
initial set of polynomial equations. Our aim in the following will be to
explain that a similar phenomenon occurs in general when the
lexicographical ordering of monomials is used for the calculation of
Groebner bases.

#### 4.3 Elimination ideals

For a given ideal @xmath , we define the @xmath -th elimination ideal
@xmath as the intersection @xmath . In other words, we pick up all
polynomials in @xmath that involve only the variables @xmath , or
equivalently, they do not involve @xmath . In the simple example
discussed above, we clearly had @xmath . The following theorem tells us
that Groebner bases calculated with respect to a lexicographical order
provide us with much information about elimination ideals.

###### Theorem 4.29.

Let @xmath be an ideal with a Groebner basis @xmath with respect to the
lexicographical order where @xmath . Then, for every @xmath the set

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

is a Groebner basis of the @xmath -th elimination ideal @xmath .

###### Proof.

By construction of @xmath and @xmath , we have the inclusion @xmath . It
suffices to show that the monomial ideal @xmath of leading terms of
@xmath is generated by @xmath . For every @xmath , the leading term
@xmath is a polynomial in the variables @xmath only. Since @xmath is a
Groebner basis of @xmath , there must exist a @xmath in @xmath such that
@xmath divides @xmath , and the leading term @xmath must necessarily be
a monomial in @xmath . Because we are using lexicographical order with
@xmath , all the other monomials of @xmath do not involve the variables
@xmath . Hence @xmath is a polynomial in @xmath , @xmath . ∎

The importance of elimination ideals was obvious in the simple example
we discussed above, where @xmath , and it generally follows from their
relation to projections of affine varieties in @xmath onto “axes” in the
high dimensional space. In terms of solving polynomial equations, we
obtain partial solutions in a smaller number of variables and try to
extend them to a full solution. More formally, we define the @xmath -th
projection map @xmath by the formula

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

We have the following

###### Proposition 4.30.

Let @xmath be an ideal in @xmath . Let @xmath be the corresponding
affine variety. We have

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

where @xmath is the affine variety corresponding to the @xmath -th
elimination ideal @xmath .

###### Proof.

We want to show that @xmath for all @xmath and @xmath . Since @xmath ,
we have @xmath . But @xmath involves only the variables @xmath , which
gives us @xmath . ∎

The above proposition, although simple, tells us something important
about @xmath . A projection of @xmath onto @xmath is contained in the
affine variety @xmath , which is sometimes possible to determine
explicitly, as in the case of @xmath discussed above. In this way,
@xmath can be regarded as an easily computable approximation of @xmath .
To make the statement more precise, we need some extra knowledge. Let us
start with the following theorem.

###### Theorem 4.31 (The Weak Nullstellensatz).

Let @xmath be an algebraically closed field and let @xmath be an ideal
satisfying @xmath . Then @xmath .

###### Proof.

Can be found in algebraic geometry textbooks like [ IdealsVarieties ] or
[ Harris ] . ∎

Intuitively speaking, the Weak Nullstellensatz asserts that the variety
@xmath corresponding to an ideal @xmath is an empty set if and only if
@xmath contains all polynomials in @xmath . Thus, a set of polynomial
equations @xmath has no solutions in @xmath if and only if the ideal
generated by @xmath is the whole @xmath .

Let us point out that the Weak Nullstellensatz allows us to answer the
important question about the existence of solutions to systems of
polynomial equations. We have the following

###### Proposition 4.32 (Consistency condition).

Let @xmath be a set of polynomials in @xmath over an algebraically
closed field @xmath . The system of equations

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

has no solution in @xmath if and only if the reduced Groebner basis of
@xmath with respect to some monomial order equals @xmath . In such case
we say that the system ( 4.25 ) is inconsistent .

###### Proof.

If a Groebner basis of @xmath equals @xmath , then clearly the set of
equations ( 4.25 ) have no solutions in @xmath . Conversely, if @xmath
is the empty set, by the Weak Nullstellensatz we know that @xmath . By
Proposition 4.28 , there is a unique reduced Groebner basis of @xmath .
Since @xmath is the reduced Groebner basis of @xmath , it must be the
reduced Groebner basis of @xmath . ∎

Note that the above proposition provides us with an algorithmic way to
check consistency of a set of polynomial equations @xmath over an
algebraically closed field @xmath . We simply calculate the reduced
Groebner basis of the ideal @xmath and check whether it equals @xmath or
not. If so, the system of equations is inconsistent. Otherwise, there
exists a solution in @xmath .

By a clever trick, the Weak Nullstellensatz is equivalent to the
following much celebrated result

###### Theorem 4.33 (Hilbert’s Nullstellensatz).

Let @xmath be an algebraically closed field. Consider @xmath . If @xmath
is a polynomial that vanishes on @xmath , then there exists @xmath such
that

  -- -------- -- --------
     @xmath      (4.26)
  -- -------- -- --------

In other words, if @xmath , then the inclusion ( 4.26 ) holds for some
@xmath .

###### Proof.

Consider the ideal

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

where @xmath are as above. It is not difficult to check that @xmath . It
is so because @xmath vanishes whenever @xmath , and hence @xmath in such
case. By the Weak Nullstellensatz, we have @xmath . Therefore

  -- -------- -- --------
     @xmath      (4.28)
  -- -------- -- --------

for some polynomials @xmath . Now set @xmath . The relation ( 4.28 )
implies that

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

If we multiply both sides of ( 4.29 ) by @xmath , where @xmath is chosen
sufficiently large to clear all the denominators, we get

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

for some polynomials @xmath . Thus @xmath . ∎

Another way to formulate the Hilbert’s Nullstellensatz is by means of
radicals .

###### Definition 4.34.

Let @xmath be an ideal. The radical of @xmath , denoted by @xmath , is
the set

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

We leave it as an exercise for the reader to prove that @xmath is an
ideal and @xmath . We call an ideal @xmath with the property @xmath a
radical ideal . Thus, @xmath is a radical ideal. We can now formulate a
version of Theorem 4.33 , often simply called the Nullstellensatz .

###### Theorem 4.35 (The Nullstellensatz).

Let @xmath be an algebraically closed field. If @xmath is an ideal in
@xmath , then

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

###### Proof.

We certainly have @xmath because @xmath implies that @xmath . Therefore
@xmath on @xmath . Conversely, suppose that @xmath . By Hilbert’s
Nullstellensatz, there exists an integer @xmath such that @xmath . This
means that @xmath . ∎

With the help of Proposition 4.9 and the above results, we can now
specify what we meant by saying that @xmath is an approximation of the
projection @xmath .

###### Theorem 4.36.

Let @xmath be an ideal in @xmath and @xmath the corresponding affine
variety. Let @xmath be the @xmath -th elimination ideal of @xmath . Then
@xmath is the smallest affine variety containing @xmath .

###### Proof.

In view of Proposition 4.9 , we must show that @xmath . By Proposition
4.30 , we have @xmath . Since @xmath is the smallest variety containing
@xmath , it follows that @xmath .

On the other hand, let @xmath be an element of @xmath , thus a
polynomial in @xmath that vanishes on @xmath . When considered as an
element of @xmath , @xmath certainly vanishes on all of @xmath . By the
Nullstellensatz, @xmath for some @xmath . Since @xmath does not involve
variables @xmath , @xmath does not either. As a consequence, @xmath is
in the @xmath -th elimination ideal @xmath . This implies that @xmath .
The inclusion is true for any @xmath , so @xmath . Consequently @xmath ,
where we used the fact that @xmath is inclusion-reversing, as well as
the equality @xmath . ∎

The above theorem tells us that the variety @xmath corresponding to the
@xmath -th elimination ideal gives us the best approximation, among all
varieties in @xmath , of a projection of @xmath onto @xmath . Therefore
elimination ideals should be expected to be helpful in solving systems
of polynomial equations.

### Chapter 5 A little intersection theory

#### 5.1 Dimension and degree of a variety

In the present section, we are going to introduce two basic properties
of algebraic varieties, which are their dimension and degree . Before we
do so, we need to introduce a distinction between projective and affine
varieties, which has not yet appeared in our introduction to algebraic
geometry. First, however, it is necessary to define the notion of a
projective space . Note that we choose to work with the set of complex
numbers, @xmath , and polynomials with complex coefficients, @xmath ,
but we could as well have chosen a different field of scalars.

###### Definition 5.1 (Complex projective space).

Let @xmath be a positive integer. The projective space @xmath equals the
set of equivalence classes of @xmath under the equivalence relation

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

The elements of @xmath are often written simply as @xmath , where an
element @xmath of an equivalence class is conveniently identified with
the class itself, however the square brackets and capital letters
indicate that we are dealing with the projective space. The variables
@xmath are called homogeneous coordinates in @xmath . This is easy to
understand if we notice that, given a set of homogeneous polynomials
@xmath , we may naturally identify the corresponding variety @xmath with
a subset of @xmath and write it as @xmath . We call such subsets
projective varieties for obvious reasons, and we do not specify whether
they belong to @xmath or @xmath as long as this is not necessary. More
general varieties in @xmath , not necessarily defined by the vanishing
of a set of homogeneous polynomials, are called affine varieties , in
accordance with Definition 4.1 .

###### Definition 5.2 (Projective variety).

Let @xmath be a set of homogeneous polynomials. The set of elements of
@xmath corresponding to the points @xmath with the property @xmath is
called a projective variety . One can write it as

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

A shorter notation, @xmath , which does not explicitly refer to the
property of being a projective variety, is also used.

One typical example of a projective variety is the Segre variety .

###### Example 5.3 (Segre variety).

Let @xmath , @xmath be positive integers. The Segre variety in @xmath is
the image of @xmath under the mapping

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

Alternatively, it is the projective variety in @xmath , defined by the
vanishing of the homogeneous polynomials

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

where @xmath is the set of homogeneous coordinates in @xmath . We denote
it by @xmath

Note that in quantum entanglement theory, @xmath corresponds to the set
of pure separable states in @xmath .

We can proceed to the definition of the dimension of an algebraic, i.e.
projective or affine, variety. Definitions will be slightly different
for affine and projective varieties, and it is somewhat more convenient
to start from the affine case. Similar to the situation with the
Dickson’s lemma (Lemma 4.17 ), it will also be useful to discuss
varieties corresponding to monomial ideals first. As we know from Lemma
4.17 , monomial ideals are finitely generated by some monomials, hence
for a monomial ideal @xmath in @xmath , we can always assume that

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

where we used the multi-index notation introduced in Section 4.2 , with
@xmath for all @xmath . It follows that @xmath , where each @xmath has a
simple description as @xmath , @xmath . Thus we have

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

By intersecting @xmath for different @xmath ’s, we get

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

which is a linear subspace of dimension @xmath . If some of the @xmath
’s were equal, the dimension of the subspace would have increased
accordingly. From equations ( 5.6 ) and ( 5.7 ), it follows that @xmath
for a monomial ideal @xmath is a union of subspaces of the form @xmath .
We identify the dimension of @xmath as the maximum dimension of a
subspace @xmath included in @xmath . A little thought reveals that this
number can be calculated explicitly, and it equals @xmath , where @xmath
denotes the minimum number of elements in a subset @xmath with the
property @xmath . Thus, for a monomial ideal @xmath , we have

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

and there is a simple way to calculate @xmath from the generators of
@xmath .

A very important insight by Hilbert was that there exists an alternative
way to obtain @xmath , which relates to the number of monomials of total
degree lower or equal @xmath not in @xmath . To explain this in more
detail, we need to introduce some extra notation. First of all, we
define

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

i.e. the set of multi-indices corresponding to the monomials not in
@xmath . We will also be using a basis of multi-indices, @xmath , with
@xmath on the @xmath -th position and zeros elsewhere, and the notation

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

for so-called coordinate subspaces . Their translates by @xmath will be
denoted, in a natural way, by @xmath . When using this notation, it is
assumed that @xmath for all @xmath , so that @xmath is perpendicular to
the coordinate subspace. We have the following.

###### Proposition 5.4.

Let @xmath be a monomial ideal.

1.   The set @xmath , which can also be denoted as @xmath with @xmath ,
    is contained in @xmath if and only if @xmath

2.   The dimension @xmath is the dimension of the largest coordinate
    subspace in @xmath

###### Proof.

We first prove @xmath . Let us assume that @xmath with @xmath is in
@xmath . In particular, the point @xmath with coordinates

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

belongs to @xmath . Assume @xmath . If so, there must exist a monomial
@xmath such that @xmath belongs to @xmath . However, all such monomials
give @xmath when evaluated on @xmath from equation ( 5.11 ), which leads
to a contradiction with @xmath . Thus we have proved the @xmath
implication in @xmath . Conversely, if @xmath , it means that every
monomial in @xmath is of nonzero degree in some of the variables @xmath
, @xmath . Therefore, the monomials in @xmath give @xmath when evaluated
on elements of @xmath . In other words, @xmath , which proves the @xmath
implication in part @xmath of the theorem. Part @xmath follows
immediately from @xmath , since @xmath is defined as the maximum
dimension of a subspace @xmath included in @xmath . If @xmath , the
dimension of @xmath equals @xmath , which is precisely the dimension of
the coordinate subspace @xmath . ∎

An illustrative picture of a monomial ideal @xmath in @xmath is
presented in Figure 5.1 . Empty dots denote the monomials with
multi-indices in @xmath , and black dots correspond to monomials in
@xmath . Generalizing from this example, it is easy to believe in the
following proposition, which we give without a proof [ IdealsVarieties ]
.

###### Proposition 5.5.

For any monomial ideal @xmath , the set @xmath can be written as a
finite (not necessarily disjoint) union of translates @xmath of some
coordinate subspaces @xmath .

We claim that the number of elements @xmath with the property @xmath can
be expressed, for @xmath sufficently large, as a polynomial @xmath of
degree @xmath , with @xmath . Equivalently, the number of monomials of
total degree no larger than @xmath , not in @xmath , is given by such
polynomial for @xmath sufficiently large.

###### Proposition 5.6.

Let @xmath be a monomial ideal. Denote by @xmath the number of
multi-indices @xmath in @xmath with the property @xmath . For @xmath
sufficiently large, @xmath can be written as a polynomial

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

where @xmath and @xmath equals the dimension of @xmath . The function
@xmath and the polynomial ( 5.12 ) are called the (affine) Hilbert
function and the (affine) Hilbert polynomial of @xmath , respectively.
The latter will be denoted by @xmath .

###### Proof.

To prove the statement, we first notice that the number of multi-indices
@xmath with the property @xmath is equal to @xmath . From this, it is
easy to conclude that the number of multi-indices @xmath such that
@xmath and @xmath equals

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

for @xmath sufficiently large. Thus, the above formula gives precisely
an expression for the number of multi-indices @xmath in the translates
@xmath from Proposition 5.5 . Of course, it can be applied to other
translates as well. Note that ( 5.13 ) is a polynomial in @xmath of
degree @xmath , which is precisely the dimension of the coordinate
subspace @xmath .

For convenience, let us denote the set of multi-indices @xmath in @xmath
by @xmath . By the well-known inclusion-exclusion principle from
combinatorics, we get

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

A key point is now that @xmath as well as @xmath and higher-order
intersections are either empty, or equal to @xmath for some translated
coordinate space @xmath of dimension @xmath , simply because @xmath and
higher-order intersections are either empty or equal to some coordinate
space @xmath of the mentioned property. By ( 5.13 ), the second and
further terms in the sum on the right-hand side of ( 5.14 ) are equal to
some polynomials of degrees @xmath for @xmath sufficiently large. Hence,
for @xmath sufficiently large, they cannot cancel the leading term of
@xmath , which sum is also a polynomial, of degree @xmath and a positive
leading term. The last statement is again a consequence of formula (
5.13 ). All in all, for @xmath sufficiently large, the sum in ( 5.14 )
is given by a polynomial of degree @xmath with a nonnegative leading
coefficient. ∎

The degree of the Hilbert polynomial, which we obtained in the above
proof, is equal to the maximum dimension of a coordinate subspace in
@xmath . By Proposition 5.4 , this is equal to @xmath . Thus we have
obtained an alternative characterization of the dimension of a variety
corresponding to a monomial ideal, which can be rather conveniently
generalized to all affine varieties. Before we discuss the general
affine case however, it is important to notice that the varieties
corresponding to monomial ideals in @xmath can be regarded as projective
varieties in @xmath as well. If we look at them in this way, the
definition of their dimension needs to be slightly modified. First of
all, we call @xmath the projective dimension of @xmath . It is therefore
natural to call @xmath the projective dimension of a @xmath -dimensional
linear subspace of @xmath , when we regard it as a subset of @xmath .
Consequently, the projective dimension of @xmath for a monomial ideal
@xmath in @xmath is defined as @xmath , where @xmath is the maximum
dimension of a linear subspace contained in @xmath . Following ( 5.8 ),
the projective dimension can be calculated as @xmath . On the other
hand, using the Hilbert approach, we can calculate the projective
dimension of @xmath as the degree of the polynomial

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

which is called simply the Hilbert polynomial of @xmath . For @xmath
sufficiently large, it equals the number of monomials not in @xmath and
of total degree equal @xmath . The last definition of projective
dimension of a variety corresponding to a monomial ideal is the one
which conveniently generalizes to all projective varieties.

Let us also note that the Hilbert polynomial and affine Hilbert
polynomial are customarily written in the form

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

where @xmath , @xmath , @xmath and @xmath . The possibility to write the
Hilbert polynomials in the above form is a direct consequence of the
fact that a general polynomial @xmath of degree @xmath that takes
integer values for integer @xmath can be written as @xmath in ( 5.16 ) [
IdealsVarieties ] .

After the above lengthy discussion of monomial ideals, we can smoothly
define the dimension of arbitrary projective or affine varieties. Given
an ideal @xmath , we define its affine Hilbert function as

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

where @xmath is the set of polynomials of degree @xmath , @xmath equals
@xmath , and @xmath refers to the dimensionality of these sets when
regarded as @xmath -linear subspaces of @xmath . For monomial ideals
@xmath , it is easy to see that the above definition of @xmath coincides
with the one we gave earlier. A key observation is that for general
@xmath , the Hilbert function of @xmath can be computed from a suitably
chosen monomial ideal. Similar to the situation we encountered in the
proof of the Hilbert basis theorem (Theorem 4.18 ), the monomial of
leading terms @xmath with respect to some monomial ordering @xmath turns
out to be of great importance. However, in the affine case, we
additionally need to assume that @xmath is a graded order , i.e. @xmath
whenever @xmath . We then have the following result.

###### Proposition 5.7.

Let @xmath be an ideal and let @xmath be a graded order on @xmath . The
monomial ideal @xmath has the same affine Hilbert function as @xmath .

###### Proof.

Can be found in [ IdealsVarieties , Chapter 9, §3] . ∎

From the above proposition and the earlier discussion about monomial
ideals, we conclude that for @xmath sufficiently large, @xmath equals
@xmath , the Hilbert polynomial of @xmath . We call the same function
the affine Hilbert polynomial of @xmath and denote it by @xmath . The
same as in equation ( 5.16 ), @xmath can be written as a sum of terms
@xmath with @xmath , @xmath . For closed scalar fields like @xmath , the
dimension of the affine variety @xmath is now simply defined as the
degree of @xmath , cf. Theorem 8 in [ IdealsVarieties , Chapter 9, §3] .

###### Definition 5.8 (Dimension of an affine variety).

Let @xmath be an ideal in @xmath . Let @xmath be the polynomial which
equals @xmath for large @xmath . The dimension of @xmath is defined to
be equal to the degree of @xmath .

Such defined dimension can be calculated from the generators of @xmath .
A suitable procedure consists of two elementary steps:

1.  Choose a graded monomial order in @xmath such as the graded
    lexicographic order of Example 4.13 or graded reverse lexicographic
    order of Example 4.14 . Compute a Groebner basis @xmath of @xmath
    using the selected ordering.

2.  Compute the maximal dimension of a subspace @xmath contained in the
    variety @xmath , using the approach outlined above formula ( 5.8 ).

To define the dimension of a general projective variety, we can proceed
similar as above. First, we denote by @xmath the set of all homogeneous
polynomials of total degree @xmath , together with the zero polynomial.
We also set @xmath for an ideal @xmath , generated by homogeneous
polynomials. The Hilbert function of @xmath is defined as

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

where @xmath refers to the dimension as a @xmath -linear subspace of
@xmath . In full analogy to Proposition 5.7 , we have [ IdealsVarieties
, Chapter 9, §3]

###### Proposition 5.9.

Let @xmath be an ideal generated by homogeneous polynomials. Consider
any monomial order @xmath in @xmath . The monomial ideal @xmath has the
same Hilbert function as @xmath .

Note that this time, unlike in the affine case, it is possible to use
any monomial ordering to obtain the desired monomial ideal.

For monomial ideals like @xmath , the above definition of Hilbert
function coincides with the one we gave previously. It immediately
follows that for large @xmath , @xmath equals @xmath , where @xmath
refers to the Hilbert polynomial, which we have already defined for
monomial ideals. To no surprise, we call the latter function the Hilbert
polynomial of @xmath and denote it with @xmath . By the formula on the
left-hand side of ( 5.16 ), we can write the Hilbert polynomial of an
arbitrary ideal @xmath generated by homogeneous polynomials as

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

for some @xmath , @xmath and @xmath . For algebraically closed scalar
fields like @xmath , we define the projective dimension of @xmath simply
as the degree of @xmath , i.e. @xmath in the above formula.

###### Definition 5.10 (Projective dimension).

Let @xmath be a monomial generated by homogeneous polynomials. Let
@xmath be the polynomial which equals @xmath for large @xmath (i.e. the
Hilbert polynomial of @xmath ). The projective dimension of the
projective variety @xmath is defined to be equal to the degree of @xmath
.

Again, the dimension of a projective variety @xmath can be calculated by
a procedure completely analogous to the one we outlined for affine
varieties. The projective dimension is well behaved under many
operations, cf. [ IdealsVarieties , Chapter 9, §4] and it plays a key
role in the following elegant result (cf. Theorem 7.2 in [ Hartshorne ]
).

###### Theorem 5.11.

Let @xmath and @xmath be two projective varieties in @xmath . Let @xmath
and @xmath be the projective dimensions of @xmath and @xmath ,
respectively. If @xmath , the intersection @xmath is nonempty and of
dimension @xmath .

Another important characteristic of a projective variety, which can be
read off its Hilbert polynomial, is the degree .

###### Definition 5.12 (Degree).

Let @xmath be a monomial generated by homogeneous polynomials. Let
@xmath be the Hilbert polynomial of @xmath . Write @xmath as in ( 5.19
),

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

where @xmath , @xmath . The degree of the projective variety @xmath is
defined to be equal to @xmath – the leading term of @xmath .

As we shall learn from Section 5.3 , the degree of a projective variety
@xmath of dimension @xmath equals, under certain assumptions, the number
of intersection points of @xmath with a projective variety @xmath of
complementary dimension @xmath .

In the thesis, we are particularly interested in Segre varieties. The
following remark tells us about their dimension and degree.

###### Remark 5.13 (Dimension and degree of a Segre variety).

Let @xmath denote the Segre variety in @xmath . The projective dimension
of @xmath is @xmath whereas its degree equals @xmath .

A short discussion of the above facts can be found in the classical
textbook by J. Harris [ Harris , Lectures 12 and 18] .

#### 5.2 Tangent spaces. Smoothness

The notion of the tangent space to a curve or a surface in @xmath is
something intuitively well understood. As we will see, it can be easily
generalized to affine and projective varieties. We choose to work with
@xmath as the field of scalars, but definitions can as well be
formulated for general fields @xmath in place of complex numbers.

Let us start with an affine variety @xmath and consider the ideal @xmath
, i.e. the set of polynomials @xmath that vanish on @xmath . We know
from the Hilbert basis theorem that @xmath is finitely generated, so we
can write it as @xmath for some polynomials @xmath .

###### Definition 5.14.

Let @xmath be a point in an affine variety @xmath . The Zariski tangent
space to @xmath at @xmath is defined as

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

where @xmath denotes the derivative of a polynomial @xmath .
Equivalently,

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

where @xmath is a set of generators of @xmath .

Note that the calculation of @xmath or @xmath can be done in a purely
formal manner, since we are dealing with polynomials.

###### Definition 5.15.

Let @xmath and @xmath be as in Definition 5.14 . We call @xmath the
affine tangent space to @xmath at @xmath . More explicitly, the affine
tangent space is defined as

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

Using the Zariski tangent space to @xmath at @xmath , we can define what
it means for @xmath to be smooth .

###### Definition 5.16.

Let @xmath be an affine variety of (affine) dimension @xmath and such
that @xmath . We call @xmath a smooth point of @xmath if and only if
@xmath .

Given a set of generators of the ideal @xmath , smoothness of a @xmath
can readily be checked by the following Jacobi criterion, cf. e.g. [
Farkas ]

###### Proposition 5.17 (Jacobi criterion for smoothness).

Let @xmath be an affine variety of (affine) dimension @xmath , such that
@xmath . A point @xmath is smooth if and only if the rank of the matrix

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

is equal to @xmath .

For projective varieties, definitions of the tangent space and
smoothness are very similar to the ones presented above. To define the
projective tangent space to a projective variety @xmath , consider first
a dehomogenized version of the polynomials in @xmath . Namely, for a
homogeneous polynomial @xmath taking points @xmath as input and and
giving @xmath as output, let us define @xmath by the formula

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

Consider the affine variety @xmath consisting of the common zeros of the
polynomials @xmath , @xmath . Its affine tangent space at a point @xmath
equals

  -- -------- -- --------
     @xmath      (5.26)
  -- -------- -- --------

To get a projectivized version of @xmath , we can homogenize the
defining polynomial equations in ( 5.26 ), i.e. consider

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

as a projective analogue of @xmath . A key observation is now that
partial derivatives of a homogeneous polynomial @xmath of degree @xmath
satisfy the following Euler relations

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

In particular, the above relation can be applied to @xmath to yield

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

where we used the fact that @xmath vanishes at @xmath . We can use (
5.29 ) and the identity @xmath , where @xmath to rewrite ( 5.27 ) as

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

The tangent space to a projective variety @xmath at a point @xmath is
now simply defined by formula ( 5.30 ) with the requirement @xmath
dropped. Thus we have the following definition

###### Definition 5.18.

Let @xmath be a projective variety and let @xmath be an element of
@xmath . Let us write the elements of @xmath as @xmath . The projective
tangent space to @xmath at a point @xmath is defined as the following
subspace of @xmath ,

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

Alternatively, given a set of generators @xmath of @xmath , we can
restate the definition ( 5.31 ) as

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

Similar as in the affine case, the smoothness of a point @xmath is
defined by a suitable condition for the dimension of @xmath .

###### Definition 5.19.

Let @xmath be a projective variety of projective dimension @xmath . A
point @xmath is called a smooth point of @xmath if and only if the
projective dimension of @xmath equals @xmath .

Clearly, there exists a projective analogue of the Jacobi criterion for
smoothness [ Farkas ] . We state it as the following proposition.

###### Proposition 5.20 (Projective Jacobi criterion).

Let @xmath be a projective variety of projective dimension @xmath , such
that @xmath . A point @xmath is smooth if and only if the rank of the
matrix

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

is equal to @xmath .

Let us discuss the above notions in the example of Segre varieties,
which is crucial for the main result of the thesis.

###### Example 5.21 (Segre varieties).

The tangent space to the Segre variety @xmath at a point @xmath is
spanned by the points @xmath and @xmath with @xmath and @xmath
arbitrary. In particular, it follows that @xmath is smooth at every
point @xmath , for all @xmath .

###### Proof.

A linear transformation @xmath , with @xmath and @xmath nonsingular
linear maps, brings @xmath to @xmath . At the same time, it transforms
all the pairs of the form @xmath and @xmath to @xmath and @xmath with
@xmath and @xmath . These @xmath and @xmath still run over all elements
of @xmath and @xmath if @xmath and @xmath can be taken as arbitrary. As
a result, we see that it is sufficient to prove our assertions about
@xmath for the single point @xmath , and the rest will follow. Recall
that @xmath is defined as the common zero of the polynomials

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

where @xmath , @xmath and @xmath denote the homogeneous coordinates in
@xmath . The calculation of the derivative of @xmath at the point @xmath
is very simple. We have

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

where @xmath , @xmath and @xmath denotes the Kronecker delta. As it is
not difficult to see, points in @xmath with coordinates @xmath do not
satisfy

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

if @xmath for some @xmath and @xmath . All other points in @xmath , with
vanishing @xmath whenever @xmath and @xmath , do satisfy ( 5.36 ). As it
is not difficult to check, all such points can be written as linear
combinations of @xmath and @xmath for some @xmath or @xmath , and all
points of the latter form do satisfy ( 5.36 ). Hence, they are good
candidates for a basis of @xmath . However, to remain in compliance with
the above definition of projective tangent space, we should prove that
points of the form @xmath and @xmath satisfy an analogue of ( 5.36 ),

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

for all elements @xmath of @xmath . However, this easily follows because
the points @xmath and @xmath are again elements of @xmath , for all
@xmath . In conclusion, the tangent space to @xmath at @xmath is spanned
by elements of @xmath of the form @xmath and @xmath . From them, we can
choose a basis, consisting of @xmath elements, so the projective
dimension of @xmath is @xmath . Thus, @xmath is smooth at @xmath . By
our earlier comments, the same applies to any point @xmath of @xmath .
Moreover, the tangent spaces @xmath have the asserted form for all
@xmath . ∎

In Section 9.4 , we are going to use the above characterization of the
tangent space of @xmath to make a key step in the proof of the strongest
result of the thesis, which is Theorem 9.27 .

#### 5.3 Bezout’s theorem

In the last part of our basic introduction to intersection theory, we
will discuss a powerful theorem that allows, among others, to calculate
the number of intersection points between two projective varieties of
complementary dimension. The theorem works under certain assumptions. To
explain them, we need to introduce the notion of transverse intersection
of two projective varieties.

###### Definition 5.22 (Transverse intersection).

Let @xmath and @xmath be two projective varieties in @xmath of
complementary dimension, i.e. @xmath where @xmath refers to the
projective dimension of a variety. We say that @xmath and @xmath
intersect transversely if and only if for any @xmath , the tangent
spaces @xmath and @xmath span @xmath .

Figure 5.2 in the previous page shows, in a schematic way, the
difference between a transverse intersection of two varieties and a one
which is not transverse. There also exists the notion of generic
transverse intersection [ Harris , Chapter 18] . It plays a role in the
formulation of Bezout’s theorem, which is the result mentioned at the
beginning of this section. However, we think for the purpose of this
thesis, it is sufficient to state Bezout’s theorem in its very basic
form, which we do in the following. For more general formulations,
consult the classical book by J. Harris [ Harris , Chapter 18] .

###### Theorem 5.23 (Bezout).

Let @xmath and @xmath be two projective varieties in @xmath of
complementary dimension, i.e. @xmath where @xmath refers to the
projective dimension of a variety. Let the degrees of @xmath and @xmath
be @xmath and @xmath . Assume that @xmath and @xmath intersect
transversely. In such case, @xmath consists of precisely @xmath points .

We also have the immediate

###### Corollary 5.24.

Let @xmath be a projective variety in @xmath of projective dimension
@xmath and let @xmath be a projective plane of complementary dimension,
i.e. @xmath , where @xmath refers to the projective dimension of a
variety. Let the degree of @xmath be @xmath . Assume that @xmath and
@xmath intersect transversely. In such case, @xmath consists of
precisely @xmath points .

The above corollary of Bezout’s theorem proves to be a key ingredient in
the proof of the main result of the thesis, which we present in Chapter
9 . Note, once again, that there exists a very general version of
Bezout’s theorem, which refers to so-called intersection multiplicities
[ Harris ] and does not require the two projective varieties to be of
complementary dimension. However, this topic is beyond the focus of the
thesis.

## Part III Results obtained and examples solved

### Chapter 6 A structure theorem for a class of cones of positive maps

In Sections 1.2 , 3.1 and 3.2 of the introductory Part I of the thesis,
we refered to the notion of positive maps, i.e. maps that preserve the
set of positive-definite matrices. It may seem that positive maps are
perfectly suited for the description of physical processes, as they map
density matrices into density matrices, or positive definite matrices at
least. However, a more careful analysis, which can be found e.g. in [
BZ2006 ] , shows that the first impression is wrong. It turns out that a
physical process that can be described as a map @xmath must necessarily
have @xmath not only positive, but also completely positive . By
complete positive positivity of a @xmath we mean the property that the
map

  -- -- -- -------
           (6.1)
  -- -- -- -------

mapping operators on @xmath into operators on the same space, is
positive for arbitrary @xmath . Here @xmath denotes the space in which
@xmath lives. To see that for @xmath corresponding to a physical process
the map ( 6.1 ) must indeed be positive for all @xmath , one can imagine
two very distant quantum systems, which do not interact at the present
moment. However, they may have interacted in the past. Let one of them
be described by states on @xmath , and let the other one be an @xmath
-dimensional system with states on @xmath . The initial state of the
composite system can in principle be an arbitrary state on @xmath . The
map acting on the composite system when the first subsystem undergoes
the process @xmath and the second subsystem remains untouch, is given by
@xmath . Here @xmath denotes identity on @xmath matrices. This is
precisely the map ( 6.1 ), and it must be positive since, as we
mentioned, the initial state of the composite system can be arbitrary.

Nevertheless, we have already seen that maps which are positive, but not
completely positive are not useless in the theory of quantum
information. In Section 1.2 we explained the role of entanglement
witnesses, which correspond to positive but not completely positive
maps, for entanglement detection. On the other hand, in Section 3.1 we
showed a direct connection of distillability of quantum states to the
property of being @xmath -positive. In the following, we introduce a
unifying framework for completely positive, @xmath -positive and several
other natural classes of positive maps. The idea comes from an early
work by Størmer [ Stormer86 ] and consists in distinguishing the class
of cones with certain symmetry property. They are called mapping cones ,
and in the context discussed here, cones with a mapping cone symmetry or
mcs-cones , due to a minor difference from the original definition by
Størmer.

Let us describe the setup for our discussion. Let @xmath and @xmath be
two Hilbert spaces. We denote with @xmath the inner product in @xmath or
@xmath . In the following, we shall assume that @xmath and @xmath are
finite-dimensional and thus equivalent to @xmath and @xmath for some
@xmath , @xmath , @xmath . We also fix orthonormal bases @xmath and
@xmath of @xmath and @xmath , respectively. Thus we have a very specific
setting for our discussion, but we shall keep the abstract notation of
Hilbert spaces, hoping to bring the attention of the reader to possible
generalizations to the infinite-dimensional case. Let us denote with
@xmath and @xmath the spaces of bounded operators on @xmath and @xmath
respectively, and choose their canonical bases @xmath , @xmath . That
is, @xmath and similarly for the @xmath . Positive elements of @xmath
are operators @xmath such that @xmath . Similarly for elements of @xmath
. The sets of positive elements of @xmath and @xmath will be denoted by
@xmath and @xmath . In the finite-dimensional case, there exists a
natural inner product in @xmath , given by the formula

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

for @xmath . An identical definition works for @xmath and we do not
distinguish notationally between the inner products in @xmath and @xmath
. Note that the bases @xmath and @xmath are orthonormal with respect to
@xmath .

In the following, we will be mostly dealing with linear maps from @xmath
to @xmath . Because of the finite-dimensionality assumption, they are
all elements of @xmath , the space of bounded operators from @xmath to
@xmath . Given a map @xmath , we define its conjugate @xmath as a map
from @xmath into @xmath satisfying @xmath for all @xmath and @xmath . In
our setting, there also exists a natural inner product in @xmath , given
by the formula

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

Note that the spaces @xmath , @xmath and @xmath can be endowed with
analogous inner products and we shall not notationally distinguish
between them. The following proposition summarizes a few elementary
facts about @xmath that will be useful for our later discussion.

###### Proposition 6.1.

For all @xmath and @xmath , @xmath , and @xmath denoting the composition
of maps, one has the following equalities

1.  @xmath ,

2.  @xmath ,

3.  @xmath .

###### Proof.

The first equality in point one follows directly from @xmath and the
definition of @xmath , eq. ( 6.3 ). To prove the other equalities, we
can use a simple lemma.

###### Lemma 6.2.

For any finite-dimensional Hilbert spaces @xmath , @xmath and maps
@xmath , we have

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

###### Proof.

Starting from the definition of @xmath , we get

  -- -- -- -------
           (6.5)
  -- -- -- -------

where the last equality follows because @xmath as a consequence of
@xmath . Similarly, @xmath holds. The final expression in ( 6.5 )
clearly equals @xmath . ∎

Note that the assertion of Lemma 6.2 holds for any choice of @xmath and
@xmath , and thus also when the two finite-dimensional Hilbert spaces
are different from the @xmath and @xmath referred to in the statement of
the proposition. Using the lemma, we get @xmath , which proves the
second equality in point one. Furthermore,

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

where we successively used Lemma 6.2 , the conjugate symmetry of @xmath
, the first equation in point one, the conjugate symmetry again, and
finally Lemma 6.2 for the second time. Obviously, the first, the fifth
and the sixth term in equation ( 6.6 ) are the same as in point two of
the proposition. Hence the only remaining thing to prove is point three.
We have

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

where we used the two properties @xmath with @xmath , @xmath with @xmath
, @xmath and @xmath , and finally Lemma 6.2 . ∎

Consider the tensor product @xmath . This space has a natural inner
product, inherited from @xmath and @xmath , and an orthonormal basis
@xmath . Similarly to @xmath and @xmath , the space @xmath of bounded
operators on @xmath is endowed with a natural Hilbert-Schmidt product,
defined by formula ( 6.2 ) with @xmath . We shall again denote the inner
product with @xmath to avoid excess notation. As we explained in
previous sections, there exists a one-to-one correspondence between
linear maps @xmath of @xmath into @xmath and elements of @xmath , given
by

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

The symbol @xmath denotes the Choi matrix of @xmath [ ref.Choi75 ] and
the mapping @xmath is sometimes called the Jamiołkowski-Choi isomorphism
[ ref.J72 ] . In fact, @xmath is not only an isomorphism, but also an
isometry between @xmath and @xmath in the sense of Hilbert-Schmidt type
inner products. One has the following

###### Lemma 6.3.

The Jamiołkowski-Choi isomorphism is an isometry. One has

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

for all @xmath (with @xmath ).

###### Proof.

By the definition of @xmath and @xmath ,

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

Since @xmath for arbitrary @xmath and @xmath , by formula ( 6.2 ) we
have

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

where we used orthonormality of @xmath . The last expression equals
@xmath by definition ( 6.3 ). ∎

Let us recall that a linear map @xmath from @xmath to @xmath is called
positive if it preserves positivity of operators, which means @xmath .
Moreover, @xmath is called @xmath -positive if @xmath is positive as a
map from @xmath into @xmath , where @xmath denotes the space of @xmath
matrices with complex entries and @xmath refers to the identity map. A
map @xmath is called completely positive if it is @xmath -positive for
all @xmath . From the Choi’s theorem on completely positive maps [
ref.Choi75 ] (cf. also Lemma 6.7 ) it follows that every such map has a
representation @xmath as a sum of conjugation maps , @xmath with @xmath
. Conversely, every map @xmath of the form @xmath is completely
positive. If all the @xmath ’s can be chosen of rank @xmath for some
@xmath , @xmath is said to be @xmath -superpositive [ ref.SSZ09 ] .
One-superpositive maps are simply called superpositive [ ref.Ando04 ] .
The sets of positive, @xmath -positive, completely positive, @xmath
-superpositive and superpositive maps from @xmath to @xmath will be
denoted with @xmath , @xmath , @xmath , @xmath , @xmath or @xmath ,
@xmath , @xmath , @xmath , @xmath for short. It is clear that all of
them are closed convex cones contained in @xmath . They also share a
more special property that the product @xmath of @xmath , @xmath and
@xmath is an element of @xmath again, where @xmath stands for one of the
sets @xmath , @xmath , @xmath , @xmath and @xmath (cf. e.g. [ ref.SSZ09
] ). Thus, following rather closely the original definition by Størmer [
Stormer86 ] , we make

###### Definition 6.4.

A cone with a mapping cone symmetry , or an mcs-cone for short, is
defined as a closed convex cone @xmath in @xmath , different from @xmath
, such that

  -- -------- -- --------
     @xmath      (6.12)
  -- -------- -- --------

for all @xmath , @xmath and @xmath .

In the following, the convexity assumption could sometimes be skept, and
we do include appropriate comments.

Note that the set of positive maps from @xmath into @xmath is contained
in the real-linear subspace @xmath ( @xmath for short) consisting of all
Hermiticity-preserving maps, i.e. @xmath such that @xmath . Moreover,
the image of @xmath by @xmath equals the set of self-adjoint elements of
@xmath [ ref.Pillis ] . Therefore @xmath induces a symmetric inner
product on @xmath (cf. Property 6.3 ). By definition, all mapping cones
are subsets of @xmath and thus of @xmath . Since @xmath is a
finite-dimensional space over @xmath with a symmetric inner product
@xmath , one can easily apply to it tools of convex analysis. In
particular, given any cone @xmath , one defines its dual @xmath as the
cone of elements @xmath such that @xmath for all @xmath ,

  -- -- -- --------
           (6.13)
  -- -- -- --------

Obviously, @xmath is closed and convex. It has a clear geometrical
interpretation as the convex cone spanned by the normals to the
supporting hyperplanes for @xmath . The dual cone has a well-known
counterpart in convex analysis [ ref.Rockafellar ] , @xmath , which is
called the polar of @xmath . We have the following

###### Lemma 6.5.

Let @xmath be a closed convex cone. Then @xmath .

###### Proof.

The formula @xmath is equivalent to @xmath for a closed convex cone
@xmath . The latter equality is a known fact in convex analysis. A proof
can be found e.g. in [ ref.Rockafellar ] (Theorem 14.1). ∎

It can be shown (cf. e.g. [ ref.SSZ09 ] ) that a duality relation @xmath
holds for all @xmath . The converse relation @xmath is also true, as a
consequence of Property 6.5 . In particular, for @xmath we get @xmath
and @xmath . Taking @xmath , one obtains @xmath , which is in accordance
with Choi’s theorem on completely positive maps [ ref.Choi75 ] and with
Property 6.3 .

In the following, we shall be interested in duality relations between
mcs-cones. This is in general a well-posed problem, because the
operation @xmath acts within the “mcs” class. We have

###### Proposition 6.6.

Let @xmath be an arbitrary mcs-cone. Then @xmath , defined as in ( 6.13
), is an mcs-cone as well.

###### Proof.

Let @xmath be an element of @xmath . First we prove that @xmath for all
@xmath and @xmath . We have @xmath and @xmath because the sets of
completely positive maps are @xmath -invariant. Therefore @xmath for an
arbitrary element @xmath of the cone @xmath . By the definition ( 6.13 )
of @xmath , we have @xmath . Using Proposition 6.1 , point three, we can
rewrite this as

  -- -------- -- --------
     @xmath      (6.14)
  -- -------- -- --------

According to definition ( 6.13 ), condition ( 6.14 ) means that @xmath .
This holds for arbitrary @xmath and @xmath . The only thing which is
left to prove is @xmath . The inclusion holds because every mcs-cone
@xmath contains all the conjugation maps @xmath with @xmath .
Consequently, @xmath . To show that indeed @xmath for any mcs-cone
@xmath , take an arbitrary nonzero @xmath . There must exist normalized
vectors @xmath and @xmath such that @xmath , where @xmath and @xmath are
orthogonal projections onto the one-dimensional subspaces spanned by
@xmath and @xmath . Denote @xmath . Consider a pair of maps, @xmath and
@xmath , where @xmath and @xmath are arbitrary normalized vectors in
@xmath and @xmath . A map @xmath , defined as @xmath acts in the
following way, @xmath or @xmath with @xmath . Any rank one operator
@xmath can be written in the latter form for some @xmath and @xmath .
But @xmath is an element of @xmath because of the assumption that @xmath
is an mcs-cone. Thus indeed @xmath for all @xmath such that @xmath . In
the case of @xmath and mapping cones @xmath as in the original
definition by Størmer, the inclusion @xmath follows from Lemma 2.4 in [
Stormer86 ] . Note that we never used convexity of @xmath in the proof.
∎

Using the lemmas introduced above, we can almost immediately prove a
surprising characterization theorem for mcs-cones, which was strongly
suggested by earlier results on the subject [ ref.St09dual ,
ref.St09mappingcones , ref.SSZ09 ] . It holds without any additional
assumptions about the cone, and is noteworthy as it links the condition
that two maps @xmath , @xmath lay in a pair of dual mcs-cones to the
fact that the product @xmath is a @xmath map. Thus it reveals a
connection between convex geometry and a fact which is more likely to be
called algebraic than geometrical. Before we proceed with the proof, let
us show a simple lemma, which is a version of [ ref.SS10 , Lemma 1
@xmath ] for @xmath .

###### Lemma 6.7.

Let @xmath be an arbitrary operator in @xmath and consider the map
@xmath . Then

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

where @xmath is a vector in @xmath and @xmath is proportional to an
orthogonal projection onto the subspace spanned by @xmath .

###### Proof.

Obviously, the map @xmath acts in the following way,

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

Thus

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

where the last expression is easily verified to be equal to @xmath .
Thus we have @xmath and by the definition ( 6.8 ) of the Choi matrix,

  -- -------- -- --------
     @xmath      (6.18)
  -- -------- -- --------

with @xmath . A proof of the last equality in ( 6.18 ) is left as an
elementary exercise for the reader. ∎

We are ready to prove the following result, which is an extension of
Theorem 1 in [ ref.St09dual ] .

###### Theorem 6.8.

Let @xmath be an mcs-cone. The following conditions are equivalent,

1.  @xmath ,

2.  @xmath for all @xmath ,

3.  @xmath for all @xmath .

###### Proof.

We first show @xmath . Let us start with @xmath . Since @xmath , we can
use the facts that @xmath and @xmath to get

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

By using point one of Proposition 6.1 with the identity map @xmath
substituted for @xmath , we get @xmath , which means that @xmath . But
@xmath because @xmath is a closed convex cone and Property 6.5 holds.
Hence @xmath . The proof of @xmath strongly builds on the assumption
that @xmath has the mapping cone symmetry. By Proposition 6.6 , we know
that @xmath is an mcs-cone as well. Therefore @xmath for an arbitrary
@xmath and @xmath . We have @xmath . By Proposition 6.1 , point one, we
get @xmath . Using Property 6.3 and Lemma 6.7 with @xmath , the last
term can be rewritten as

  -- -------- -- --------
     @xmath      (6.20)
  -- -------- -- --------

where @xmath for @xmath . The vector @xmath can be arbitrary, since we
do not assume anything about the operator @xmath . Consequently, the
condition @xmath is equivalent to

  -- -- -- --------
           (6.21)
  -- -- -- --------

which means that @xmath for all @xmath . By the Choi theorem on
completely positive maps [ ref.Choi75 ] , @xmath for all @xmath . Thus
we have finished proving that @xmath . The proof of the equivalence
@xmath only needs a minor modification of the above argument. Instead of
using point one of Proposition 6.1 , point two of the same proposition
has to be used. Other details are practically the same as above and we
shall not give them explicitly. ∎

In case of @xmath and a @xmath -invariant mcs-cone @xmath , Theorem 6.8
can be further simplified.

###### Theorem 6.9.

Let @xmath be a @xmath -invariant mcs-cone. Then the following
conditions are equivalent,

1.  @xmath ,

2.  @xmath for all @xmath ,

3.  @xmath for all @xmath .

###### Proof.

Obvious from Theorem 6.8 . ∎

This result was earlier known for @xmath and @xmath [ ref.SSZ09 ] , and
inexplicitly for all so-called symmetric (and convex) mapping cones [
ref.St09mappingcones ] . As it was pointed to the author by Erling
Størmer, in the case of @xmath -positive maps, not necessarily from
@xmath into itself, an even stronger characterization of the type of
Theorems 6.8 and 6.9 is valid. First, we have the simple

###### Theorem 6.10.

The following conditions are equivalent

1.  @xmath ,

2.  @xmath for all @xmath such that @xmath ,

3.  @xmath for all @xmath such that @xmath .

###### Proof.

Obvious from Theorem 6.8 . The duality relation

  -- -- -- --------
           (6.22)
  -- -- -- --------

holds (cf. [ ref.SSZ09 ] ) and we can substitute @xmath in Theorem 6.8
with @xmath , @xmath . We also use the elementary fact that @xmath . ∎

The next result on @xmath -positive maps seems to be less obvious.

###### Theorem 6.11.

Denote with @xmath and @xmath the sets of @xmath -dimensional
projections in @xmath and @xmath , resp. The following conditions are
equivalent

1.  @xmath ,

2.  @xmath for all @xmath ,

3.  @xmath for all @xmath ,

4.  @xmath for all @xmath , @xmath .

###### Proof.

We shall prove the equivalence @xmath . The other ones follow
analogously. Since @xmath and any @xmath map can be written as @xmath
with @xmath arbitrary, the condition @xmath is equivalent to

  -- -------- -- --------
     @xmath      (6.23)
  -- -------- -- --------

By Proposition 6.1 , point three, equation ( 6.23 ) can be rewritten as

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

where we used the fact that @xmath and the self-adjointness of @xmath
and @xmath . Note that @xmath is an element of @xmath of rank @xmath .
Conversely, every map in @xmath of rank @xmath can be written in the
form @xmath for some @xmath , @xmath and @xmath . It is sufficient to
take @xmath and @xmath , @xmath as the range and rank projections for
@xmath , resp. Therefore the condition ( 6.24 ) is equivalent to @xmath
for all @xmath s.t. @xmath . But this is the same as @xmath for all
@xmath , or @xmath . Thus @xmath . ∎

Let us note that Theorem 6.8 can be perceived as a very broad
generalization of the so-called positive maps entanglement criterion by
the Horodecki family [ HHH96 ] . To see this, we prove the following
general

###### Proposition 6.12 (Generalized positive maps criterion).

Let @xmath be an mcs-cone in @xmath . An operator @xmath belongs to the
image @xmath if and only if the following condition

  -- -------- -- --------
     @xmath      (6.25)
  -- -------- -- --------

holds for all @xmath .

###### Proof.

The proof relies on Theorem 6.8 and the formula ( 6.8 ) for the
isomorphism @xmath . Let us note that

  -- -------- -- --------
     @xmath      (6.26)
  -- -------- -- --------

where @xmath denotes the dimension of the space @xmath . Thus the
condition @xmath is the same as @xmath , which is equivalent, by the
Choi theorem on completely positive maps [ ref.Choi75 ] , to @xmath . If
the last inclusion holds for all @xmath , we know by Theorem 6.8 that
@xmath is in @xmath , or @xmath . Conversely, if @xmath is in @xmath ,
then @xmath belongs to @xmath . By Theorem 6.8 , @xmath belongs to
@xmath for all @xmath , which is equivalent to @xmath according to the
Choi theorem on completely positive maps. By formula ( 6.26 ) the last
expression is equivalent to @xmath for all @xmath . ∎

###### Remark 6.13.

For the choice @xmath , the above theorem reduces to the positive maps
criterion by Horodeccy [ HHH96 ] . We have the following equivalence

  -- -------- -- --------
     @xmath      (6.27)
  -- -------- -- --------

###### Proof.

Follows from Proposition 6.12 if we recall that the set of separable
operators equals @xmath and the dual of @xmath is @xmath [ ref.SSZ09 ] .
∎

### Chapter 7 Algebraic problems solved by hand

#### 7.1 Product numerical range for a three-parameter family of
operators

Product numerical range is a concept derived from the well-known
numerical range (cf. e.g. [ HornJohnson ] ). For an operator @xmath on a
Hilbert space @xmath , the numerical range of @xmath is by definition
the set of numbers which can be obtained as @xmath for some vector
@xmath of unit norm. Accordingly, for an operator @xmath on a bipartite
space @xmath the product numerical range is defined as

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

A generalization to a multipartite setting is possible and very
straightforward. The definition was introduced in [ ref.Product11 ] and
demonstrated to have various links to problems in the quantum
information science [ ref.Restricted11 ] , including the evaluation of
minimum output entropy [ Petz ] , checking whether two unitary
operations are locally distinguishable [ WSHV00 , DuanFengYing08 ] or
the identification of local dark spaces and error correcting codes [
KL97 , MMZ2010 ] . In the present section we analytically calculate the
product numerical range for a three-parameter family of @xmath matrices
introduced in [ SZ09 ] . In order to obtain explicit formulas, some
additional constraints need to put on the parameters of the matrices. We
take

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

which represent operators on @xmath . In order to find @xmath , we first
calculate the quantities @xmath . The result is

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

To calculate the product numerical range of @xmath , we only need to
find the maximum and the minimum of @xmath , where @xmath and @xmath .
Obviously, @xmath for all @xmath that meet the constraint @xmath . The
characteristic polynomial of @xmath is @xmath , which has the roots

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

The last equality follows from a direct calculation of the determinant
of @xmath , @xmath . We see that the product numerical range of @xmath
is

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

where @xmath . Hence to determine the product numerical range of @xmath
, it is enough to calculate the maximum of the expression @xmath over
the elements @xmath with unit norm. First we observe that for @xmath ,
@xmath fixed, the function @xmath attains the maximum value @xmath .
Thus the calculation of @xmath reduces to finding the maximum of @xmath
over @xmath nonnegative and such that @xmath . Equivalently, we may skip
the nonnegativity condition on @xmath and @xmath , substitute @xmath and
maximize @xmath over real @xmath . Using simple algebra, it is easy to
show that @xmath is equal to @xmath for @xmath . The maximum of this
expression over @xmath can be easily found if @xmath and @xmath satisfy
one of the following conditions,

-   @xmath or

-   @xmath for real @xmath .

In the case a), we get @xmath , and thus we are left with the problem of
maximizing

  -- -------- -- -------
     @xmath      (7.6)
  -- -------- -- -------

over real @xmath . The maximum can be calculated explicitly. The result
reads

  -- -------- -- -------
     @xmath      (7.7)
  -- -------- -- -------

Here we only outline how ( 7.7 ) was obtained. The first derivative of
@xmath is

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

and there are either two or four solutions to the equation @xmath in
@xmath , depending on the sign of the expression @xmath . If inequality
@xmath holds, we get a single maximum, equal to @xmath , at @xmath . Let
us define @xmath . When @xmath , the maximum at @xmath turns into a
minimum, but two new maxima of @xmath appear at @xmath and @xmath . The
value of @xmath in both of these maxima is the same and equals @xmath .
Thus we have explained formula ( 7.7 ) but for the case @xmath . With
little additional effort, it can be shown that ( 7.7 ) also works in
that special case. Therefore ( 7.7 ) is true whenever @xmath and we have
found the product numerical range ( 7.5 ) of @xmath in the case a).

When @xmath for real @xmath , it is even simpler to calculate @xmath
than in the situation considered above. Since then we have the equality
@xmath , we can first maximize the expression

  -- -------- -- -------
     @xmath      (7.9)
  -- -------- -- -------

while keeping @xmath and @xmath constant. This yields

  -- -------- -- --------
     @xmath      (7.10)
  -- -------- -- --------

and we are left with the task of maximizing this expression over all
nonnegative @xmath such that @xmath holds. The calculation of the
maximum is elementary, so we only give the final result,

  -- -------- -- --------
     @xmath      (7.11)
  -- -------- -- --------

Hence we have obtained the product numerical range ( 7.5 ) of @xmath in
the case b). In the case of general @xmath , it does not seem easy to
calculate the product numerical range of @xmath .

For the cases where the calculation of the product numerical range of
@xmath turned out to be possible, the results obtained can be used to
find a part of the boundary of the set of entanglement witnesses.
Namely, one can consider the minimal @xmath such that @xmath is positive
on product vectors. From ( 7.5 ), it is not difficult to see that the
appropriate @xmath equals @xmath , which we can explicitly calculate
under certain assumptions on @xmath , @xmath and @xmath . With a little
more effort, the above argument also shows how to explicitly find the
specific product vectors @xmath that satisfy @xmath . The set of product
vectors @xmath that satisfy @xmath for an entanglement witness @xmath
often turns out to be important when considering the optimality of
@xmath [ LKCH00 ] .

#### 7.2 Higher order numerical ranges and code carriers for the qutrit
case

We already know from the introduction to Chapter 6 that physical
processes in quantum systems are best described by completely positive
maps. Every such map, if not simply a unitary transformation, can be
understood as some kind of noisy evolution induced upon the system by an
environment. More precisely, two initially orthogonal pure states of the
system are often no longer orthogonal after the evolution, which is an
analogue of a spontaneous bit flip in classical computing. A way to deal
with the noise in a classical setting is by representing the logical
@xmath and @xmath by multiple physical bits, for example @xmath and
@xmath , resp. Even if one of those is physically flipped, there is
sufficient information in the remaining ones to recover the initial
value @xmath or @xmath . An identical solution encounters severe
difficulties in the quantum setting, since by the no-cloning theorem [
WZ82 ] , there exists no transformation that could transform an
arbitrary quantum state @xmath into @xmath , let alone @xmath .

However, nothing prevents us from encoding, in the qubit case, an
arbitrary pure state @xmath of a qubit as @xmath . In this way, a
similar resistance to single bit flips as in the classical case is
achieved, since the set of bit-flipped states @xmath is orthogonal to
the bit-flipped @xmath . This is the basic idea behind quantum error
correction [ Shor95 , Gottesman2010 ] , but more details need to be
accounted for before it really works. For a fixed completely positive
transformation @xmath , describing the noise affecting a quantum system,
a general criterion for quantum error correction was provided in the
paper [ KL97 ] by E. Knill and R. Laflamme. Note that by the Choi
theorem on completely positive maps [ ref.Choi75 ] , the map @xmath can
be written in the form @xmath for some operators @xmath on the space in
which @xmath lives. The Knill-Laflamme criterion now says that we can
encode a @xmath -dimensional quantum states and send them through the
“quantum channel” described by @xmath if and only if the conditions

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

hold for some @xmath -dimensional projection @xmath and a set of numbers
@xmath . The equations ( 7.12 ) are called Knill-Laflamme equations
accordingly. All of them are of the form @xmath , where @xmath is some
matrix and @xmath a constant. This problem is a generalization of the
eigenvalue problem and, more generally, of the question about the
so-called numerical range of an operator, @xmath , where @xmath runs
over all vectors of unit norm in the respective Hilbert space @xmath .
We already mentioned numerical ranges in Section 7.1 . Because of the
form of Knill-Laflamme conditions, it is natural to introduce so-called
higher order numerical ranges [ ref.CKZ06 ] (HONR),

  -- -------- -- --------
     @xmath      (7.13)
  -- -------- -- --------

where @xmath is a @xmath -dimensional projection. It is also important
to know the description of the set of all projections which give rise to
some @xmath in the above formula. We denote the set of such projections
by @xmath and call it a code carrier , because it relates to the set of
all possible error correcting subspaces.

  -- -------- -- --------
     @xmath      (7.14)
  -- -------- -- --------

In the following, we are going to show how to find @xmath and @xmath for
an arbitrary matrix @xmath of order three. By solving the problem for
@xmath , we shall give a full description of higher order numerical
ranges and code carriers for @xmath matrices. This is so because the
other cases, @xmath , are trivial.

Let us first observe that for a general matrix @xmath , not necessarily
of order three, the equation @xmath is equivalent to

  -- -------- -- --------
     @xmath      (7.15)
  -- -------- -- --------

where @xmath and @xmath are real numbers. In this way, the compression
equation @xmath is transformed into a pair of compression equations for
Hermitian matrices @xmath and @xmath . Thus by solving the compression
equations for a general Hermitian matrix @xmath of respective dimension
and finding @xmath and @xmath , we may hope to be able to find @xmath
and @xmath for a general matrix @xmath just by intersecting @xmath and
@xmath and reading off the @xmath ’s and @xmath ’s corresponding the
elements in the intersection. Note that an almost complete description
of code carriers and numerical ranges for Hermitian matrices of
arbitrary dimension was obtained in [ ref.CKZ06 ] . In the present
section, however, we shall give an alternative proof in the case of
dimension @xmath , which is mainly justified by the fact that we solve
algebraic equations.

Let us first consider a Hermitian @xmath matrix @xmath with three
distinct eigenvalues @xmath and the corresponding eigenvectors @xmath .
By adding a factor proportional to identity to @xmath , we may assume
that all the @xmath ’s are nonzero. We know from [ ref.CKZ06 ] that
@xmath . We shall find @xmath .

Note that the condition @xmath is equivalent to the existence of vectors
@xmath such that

  -- -------- -- --------
     @xmath      (7.16)
  -- -------- -- --------

If we denote with @xmath the @xmath -th coordinate of @xmath with
respect to the basis @xmath , conditions ( 7.16 ) can be rewritten as

  -- -------- -------- -- -- --------
     @xmath   @xmath         (7.17)
     @xmath   @xmath         (7.18)
  -- -------- -------- -- -- --------

with @xmath . By appropriately transforming a solution of ( 7.17 ) and (
7.18 ) according to the following prescription: @xmath @xmath , we can
get another solution, where @xmath has real numbers as coefficients.
Indeed, the transformations of the form given above do not affect
equalities ( 7.17 ) and ( 7.18 ), and the phases @xmath can be chosen as
@xmath to make all the coordinates @xmath real. Therefore in the
following, we assume that all the coordinates of @xmath are real.

From equations ( 7.18 ) it follows that

  -- -------- -- --------
     @xmath      (7.19)
  -- -------- -- --------

where we removed the bars over @xmath using the reality assumption
explained above. Equation ( 7.19 ) implies the existence of a phase
@xmath such that the numbers @xmath and @xmath are real. Moreover, the
first equation in ( 7.18 ) now implies that also @xmath has to be a real
number. We can now transform @xmath according to the following
prescription @xmath and obtain another solution to equations ( 7.17 )
and ( 7.18 ), where both @xmath and @xmath have real coefficients.
Consequently, it is possible first to find all real solutions to the
following set of equations,

  -- -------- -------- -- -- --------
     @xmath   @xmath         (7.20)
     @xmath   @xmath         (7.21)
  -- -------- -------- -- -- --------

and later recover all the solutions to ( 7.17 ) and ( 7.18 ) by
transforming the variables according to the prescription @xmath and
@xmath with arbitrary angles @xmath and @xmath . This follows because
the transformations of the type just described do not affect equations (
7.17 ) and ( 7.18 ) and on the other hand, they allow us to bring any
solution of ( 7.17 ) and ( 7.18 ) to a real solution of equations ( 7.20
) and ( 7.21 ). Thus, let us look for real solutions of equations ( 7.20
) and ( 7.21 ). By multiplying the first equation in ( 7.21 ) by @xmath
and subtracting the result from the second equation in the same line,
one easily gets

  -- -------- -- --------
     @xmath      (7.22)
  -- -------- -- --------

Substitution of this equality back to the first equation in ( 7.21 )
yields

  -- -------- -- --------
     @xmath      (7.23)
  -- -------- -- --------

In a similar fashion, equations ( 7.20 ) give us

  -- -------- -- --------
     @xmath      (7.24)
  -- -------- -- --------

If we multiply the first equation in ( 7.24 ) for @xmath by the same
equation but for @xmath , we obtain

  -- -------- -- --------
     @xmath      (7.25)
  -- -------- -- --------

In a similar way

  -- -------- -- --------
     @xmath      (7.26)
  -- -------- -- --------

On the other hand, we may square the equations in ( 7.23 ) to obtain

  -- -------- -- --------
     @xmath      (7.27)
  -- -------- -- --------

Now we can subtract the first equation in ( 7.27 ) from ( 7.25 ) and the
second equation in ( 7.27 ) from ( 7.26 ) to get

  -- -------- -- --------
     @xmath      (7.28)
     @xmath      (7.29)
  -- -------- -- --------

According to our assumption @xmath , the factors @xmath and @xmath are
non-zero. Therefore the equations ( 7.28 ) and ( 7.29 ) are equivalent
to

  -- -- -- --------
           (7.30)
  -- -- -- --------

The solution of ( 7.30 ) is of the form @xmath , @xmath for an arbitrary
@xmath . We can substitute this in ( 7.24 ) to obtain a general solution
to equations ( 7.20 ) and ( 7.21 ) in the following form,

  -- -------- -- --------
     @xmath      (7.31)
  -- -------- -- --------

where @xmath and @xmath are arbitrary and we have introduced the
notation @xmath and @xmath . Note that we have used equations ( 7.24 )
to establish sign relations between the coordinates of @xmath and @xmath
.

Now, if we recall the discussion preceding equations ( 7.20 ) and ( 7.21
), we can recover a general solution to ( 7.16 ) by introducing complex
phases back into ( 7.31 )

  -- -- -- --------
           (7.32)
  -- -- -- --------

where the phases @xmath and @xmath are arbitrary.

Since we are interested in @xmath rather than the vectors @xmath , it is
sufficient for us to know that @xmath and @xmath given in ( 7.32 ) span
the two-dimensional subspace

  -- -------- -- --------
     @xmath      (7.33)
  -- -------- -- --------

with @xmath arbitrary.

In this way we obtain the following description of @xmath .

###### Proposition 7.1.

Let @xmath be a Hermitian operator on @xmath with eigenvalues @xmath and
the corresponding eigenvectors @xmath . The rank @xmath code carrier of
@xmath is given as

  -- -------- -- --------
     @xmath      (7.34)
  -- -------- -- --------

∎

Obviously, two orthogonal projections are equal iff they project onto
the same subspace. Furthermore, two linear subspaces are identical if
and only if all the vectors spanning one of the subspaces are linearly
dependent of the vectors spanning the second subspace. Using Proposition
7.1 , we can now find the intersection of rank @xmath code carriers of
two distinct Hermitian operators on @xmath . We have

###### Proposition 7.2.

Let @xmath , @xmath be Hermitian operators on @xmath with eigenvalues
@xmath , @xmath , respectively. Let the corresponding eigenvectors be
@xmath ( @xmath ). Let @xmath , @xmath , @xmath , @xmath . The
intersection @xmath is nonempty if and only if there exist @xmath such
that the family of vectors

  -- -------- -- --------
     @xmath      (7.35)
  -- -------- -- --------

as well as the family of vectors

  -- -------- -- --------
     @xmath      (7.36)
  -- -------- -- --------

are linearly dependent. If this is the case,

  -- -------- -- --------
     @xmath      (7.37)
  -- -------- -- --------

where @xmath is the set of all @xmath such that there exists @xmath for
which the families of vectors

  -- -------- -- --------
     @xmath      (7.38)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (7.39)
  -- -------- -- --------

are both linearly dependent.

###### Proof.

Obvious from Proposition 7.1 ∎

Note that the above results have been derived using the assumption
@xmath ( @xmath ) about the eigenvalues of @xmath ( @xmath , resp.).
However, in case that these assumptions do not hold, we can still easily
give a description of @xmath ( @xmath ) and find @xmath . First of all,
if @xmath or @xmath is proportional to identity, the corresponding code
carrier equals the set of all two-dimensional projections in @xmath . It
is also easy to prove the following proposition.

###### Proposition 7.3.

Let @xmath be a Hermitian operator on @xmath with eigenvalues @xmath or
@xmath . Then @xmath consists of an orthogonal projection onto the
eigenspace corresponding to @xmath .

We leave the proof of the proposition as an exercise for the reader (it
is enough to check what conditions ( 7.17 ) imply when exactly two of
the eigenvalues are equal). We should notice that formulas ( 7.32 )
still apply, so we can easily generalize Proposition 7.2 to a situation
where the eigenvalues of @xmath (or @xmath ) are not all distinct.

###### Proposition 7.4.

Let @xmath , @xmath be Hermitian operators on @xmath with eigenvalues
@xmath , @xmath , respectively. Assume that neither @xmath nor @xmath is
proportional to identity. Let the corresponding eigenvectors be @xmath (
@xmath ). Let @xmath , @xmath , @xmath , @xmath . The intersection
@xmath is nonempty if and only if there exist @xmath such that the
family of vectors

  -- -------- -- --------
     @xmath      (7.40)
  -- -------- -- --------

as well as the family of vectors

  -- -------- -- --------
     @xmath      (7.41)
  -- -------- -- --------

are linearly dependent. If this is the case,

  -- -------- -- --------
     @xmath      (7.42)
  -- -------- -- --------

where @xmath is the set of all @xmath such that there exists @xmath for
which the families of vectors

  -- -------- -- --------
     @xmath      (7.43)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (7.44)
  -- -------- -- --------

are both linearly dependent.

The case of @xmath or @xmath proportional to identity, which we excluded
in the above proposition, can be handled in an obvious way. Thus we have
fully characterized the intersection @xmath for a pair of Hermitian
operators on @xmath . Following the discussion after equations ( 7.15 ),
we can now use Proposition 7.4 to obtain the numerical range @xmath for
an arbitrary (not necessarily Hermitian or normal) matrix of dimension
three. Let us discuss this in an example.

###### Example 7.5.

Consider the Jordan matrix

  -- -------- -- --------
     @xmath      (7.45)
  -- -------- -- --------

The second order numerical range @xmath equals @xmath and the
corresponding code carrier consists of a single element, @xmath , where
@xmath , @xmath .

###### Proof.

We have

  -- -------- -- --------
     @xmath      (7.46)
  -- -------- -- --------

Let us denote the eigenvalues of @xmath with @xmath and the
corresponding eigenvectors with @xmath . For @xmath , similarly define
@xmath and the eigenvectors @xmath . One can easily check that

  -- -------- -- --------
     @xmath      (7.47)
  -- -------- -- --------

Thus @xmath . The eigenvectors of @xmath and @xmath are

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (7.48)
     @xmath   @xmath   @xmath      (7.49)
  -- -------- -------- -------- -- --------

We can now easily check that the equations

  -- -------- -------- -- -- --------
     @xmath   @xmath         (7.50)
     @xmath   @xmath         (7.51)
  -- -------- -------- -- -- --------

have a single solution in @xmath , which is @xmath . This corresponds to
the subspace in the assertion of the theorem. ∎

Note that with the methods described in this section, it is possible to
find solutions to Knill-Laflamme equations or prove their non-existence
for all qutrit quantum channels @xmath . As proud as it sounds, the
solutions will however almost never exist. It should also be kept in
mind that solving the problems for all qutrit channels simply means
finding all qutrit channels that allow for encoding of a single qubit.
This is quite a simple setup.

#### 7.3 A separable state of length four and Schmidt rank three

In Section 1.2 , we introduced the concept of separability and discussed
some fundamental subtleties about the distinction between separable and
entangled quantum states. Here, we consider so-called length of a
separable state , which is the minimum number of terms in its separable
decomposition. More precisely, we have the following.

###### Definition 7.6 (Length of a separable state).

Let @xmath be a separable state on a bipartite space @xmath . Thus,
@xmath can be written as

  -- -------- -- --------
     @xmath      (7.52)
  -- -------- -- --------

for some @xmath and some positive operators @xmath , @xmath on @xmath ,
@xmath , resp. The length of @xmath is the minimum number @xmath in a
decomposition of the form ( 7.52 ).

A generalization to a multipartite setting is straightforward, but we
shall not discuss on this point here. Given a density matrix @xmath on a
bipartite space, it is in general very difficult to tell whether and how
it decomposes into a convex sum of products of positive operators.
Successful attempts in this direction can be made in the situation
described in [ HLVC2000 ] , but in general, no exact way to find an
optimal decomposition of the form ( 7.52 ) is known. In particular, it
is difficult to determine the length @xmath of @xmath . On the other
hand, it is fairly simple to find a minimal decomposition of @xmath of
the form

  -- -------- -- --------
     @xmath      (7.53)
  -- -------- -- --------

where @xmath , @xmath are Hermitian, but not required to be positive.
The minimal number @xmath in the decomposition ( 7.53 ) will be called
the Schmidt rank of @xmath . The name originates from the well-known
Schmidt decomposition of vectors, for which ( 7.53 ) is an analogue.

Intuitively, the length and the Schmidt rank of @xmath do not look
entirely independent. Indeed, in [ S2010 ] we showed that separable
states of small lengths @xmath necessarily have their Schmidt rank equal
to their length. In the present section, we give an example of a state
of Schmidt rank @xmath and length @xmath . This shows that the mentioned
result for lengths @xmath cannot be further generalized. Such conclusion
should be expected from the beginning, but the concrete example is
rather illustrative.

###### Example 7.7.

Consider the following @xmath diagonal matrices

  -- -------- -------- -- -- --------
     @xmath   @xmath         (7.54)
     @xmath   @xmath         
  -- -------- -------- -- -- --------

Let @xmath be a density matrix on @xmath of the form

  -- -------- -- --------
     @xmath      (7.55)
  -- -------- -- --------

This bipartite state is separable, has length @xmath and Schmidt rank
@xmath .

###### Proof.

Obviously, @xmath is separable. For further convenience, let us denote
the length of @xmath with @xmath and its Schmidt rank with @xmath . We
first prove that the Schmidt rank of @xmath is 3, which is equivalent to
proving that the Schmidt rank of @xmath is 3. For that purpose, we
observe that the operators @xmath in ( 7.54 ) are linearly dependent.
For example, we can write @xmath as a linear combination of @xmath ,
@xmath and @xmath ,

  -- -------- -- --------
     @xmath      (7.56)
  -- -------- -- --------

We can put ( 7.56 ) in ( 7.55 ) and use distributivity of the tensor
product to get

  -- -------- -- --------
     @xmath      (7.57)
  -- -------- -- --------

From ( 7.57 ), we definitely see that @xmath has Schmidt rank lower than
four. But the matrices @xmath , @xmath and @xmath are linearly
independent ¹ ¹ 1 the matrix @xmath has a nonzero determinant , just as
the matrices @xmath , @xmath and @xmath are. This implies that the
number of product terms in ( 7.57 ) cannot be reduced any further.
Consequently, the Schmidt rank of @xmath and hence of @xmath is 3,
@xmath .

Of course, the length of @xmath is not lower than @xmath , so we have
@xmath . On the other hand, ( 7.55 ) is an expression for @xmath as a
sum of four products of positive operators @xmath . Therefore @xmath
cannot be higher than 4 and the only possibilities left are @xmath and
@xmath . In the following we show that @xmath is excluded. Put it in a
different way, @xmath cannot be written as

  -- -------- -- --------
     @xmath      (7.58)
  -- -------- -- --------

with @xmath and @xmath positive for @xmath . It will be more convenient
to show that @xmath cannot be written in the form ( 7.58 ) with all
@xmath , @xmath positive. To prove this, let us assume that a
decomposition of @xmath of the form ( 7.58 ) exists. We should stress
that ( 7.57 ) is not an example of such a decomposition because @xmath
is not positive. The operators @xmath and @xmath are Hermitian, so we
can write them as @xmath and @xmath , where @xmath , and @xmath is a
basis of the @xmath -linear space of Hermitian operators on @xmath such
that

  -- -------- -------- -- -- --------
     @xmath   @xmath         (7.59)
     @xmath   @xmath         
  -- -------- -------- -- -- --------

and @xmath ’s for @xmath have only off-diagonal elements nonzero.
Because of the form ( 7.54 ) of the operators @xmath , @xmath does not
have any off-diagonal elements and the decomposition of @xmath in the
basis @xmath of all Hermitian operators on @xmath does not include any
terms with @xmath nor with @xmath . If there are any terms including
@xmath with @xmath in @xmath or @xmath , they must eventually cancel out
in the tensor product ( 7.58 ). Therefore we may use @xmath and @xmath
instead of @xmath and @xmath . The relation ( 7.58 ) still holds when
@xmath is replaced with @xmath and @xmath with @xmath . Positivity of
@xmath and @xmath follows from the fact that they are diagonal parts of
positive operators. We see that @xmath equals @xmath , and it is also a
sum of products of positive operators. Consequently, if there exists a
decomposition of @xmath of the form ( 7.58 ) with @xmath and @xmath
positive, another decomposition with diagonal and positive @xmath and
@xmath must also exist. Therefore we can restrict our discussion to
decompositions of the form

  -- -------- -- --------
     @xmath      (7.60)
  -- -------- -- --------

with @xmath and @xmath . Based on the definition ( 7.55 ), it can be
easily checked that

  -- -------- -- --------
     @xmath      (7.61)
  -- -------- -- --------

with @xmath , @xmath and @xmath for the remaining eight coefficient
pairs @xmath . In order for equation ( 7.60 ) to be fulfilled, we must
have

  -- -------- -- --------
     @xmath      (7.62)
  -- -------- -- --------

To see the consequences of ( 7.62 ), let us introduce vectors @xmath and
@xmath with coordinates @xmath and @xmath , respectively. The conditions
( 7.62 ) can be written as

  -- -------- -- --------
     @xmath      (7.63)
     @xmath      (7.64)
     @xmath      (7.65)
     @xmath      (7.66)
  -- -------- -- --------

Keeping in mind nonnegativity of @xmath ’s and @xmath ’s, we can draw
some further conclusions about these numbers. First of all, we should
notice that two real vectors with nonnegative coordinates are orthogonal
if and only if a nonvanishing coordinate of one of the vectors
corresponds to a vanishing coordinate of the other vector and vice
versa. As a consequence of this and ( 7.64 ), each of the vectors @xmath
and @xmath must have a vanishing coordinate. On the other hand, because
of the formula ( 7.63 ) neither of the vectors can be zero. In other
words, each of them must have a nonvanishing coordinate. We are left
with @xmath ’s and @xmath ’s which have either one or two nonzero
coordinates. Let us consider first a situation in which one of the
vectors has two nonzero coordinates. Without any loss of generality we
assume the vector to be @xmath and we put @xmath , @xmath , @xmath .
Because of ( 7.64 ), @xmath , @xmath , @xmath . This in turn implies
@xmath , @xmath and @xmath as a consequence of ( 7.63 ), ( 7.65 ) and (
7.66 ). Therefore @xmath , @xmath and @xmath . If @xmath , the equality
@xmath cannot hold. One of the coordinates @xmath , @xmath must be
nonzero. We may assume @xmath , so that we have @xmath , @xmath , @xmath
. From ( 7.64 ) it follows that @xmath , @xmath , @xmath . Using ( 7.63
) we get @xmath while ( 7.65 ) yields @xmath . We have obtained @xmath
and @xmath , which implies @xmath . But now ( 7.64 ) gives us @xmath ,
@xmath , @xmath and from @xmath we get @xmath .

In the successive steps above we obtained @xmath , @xmath and finally
the inequality @xmath . This is in contradiction with ( 7.64 ), so our
initial assumption about the existence of a vector @xmath (or @xmath )
with two nonzero coordinates, cannot be true for solutions of the
equations ( 7.63 )-( 7.66 ). None of the vectors @xmath , @xmath can
have two nonvanishing coordinates. The only possibility we have not
excluded yet is that of all the vectors @xmath , @xmath having precisely
one nonzero coordinate each. Let us assume that this is the case and
concentrate on @xmath ’s. Because of the fact that @xmath ’s are of
dimension three, there must exist a pair of indices @xmath such that
@xmath is proportional to @xmath . Without loss of generality we may
assume that either @xmath or @xmath holds. The first possibility is
excluded because of the equalities @xmath and @xmath . The second is in
contradiction with @xmath and @xmath . Thus we have excluded the only
remaining possibility for @xmath ’s and we conclude that ( 7.62 ) has no
solutions of the desired properties @xmath . Consequently, @xmath cannot
be written in the form ( 7.58 ) with @xmath ’s and @xmath ’s positive.
The same holds for @xmath . Hence @xmath , which in turn implies @xmath
because @xmath . This proves our assertions about @xmath . ∎

### Chapter 8 Algebraic problems solved by using Groebner bases

#### 8.1 Compression equations – a special case

In Section 7.2 , we defined the notion of a code carrier of an operator
and outlined how it is related to the problem of finding solutions of
generalized eigenvalue problems. Notably, the paper [ ref.CKZ06 ]
contains an almost complete description of code carriers for Hermitian
operators. Therefore, following similar steps to those described in
Section 7.2 , one may attempt to find a general solution to a
compression equation @xmath by first splitting @xmath into its Hermitian
@xmath and anti-Hermitian part @xmath and solving the respective
compression equations

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

separately . Next, the sets of possible projectors @xmath and @xmath may
be intersected to yield @xmath . For a more detailed description of the
method and for the definition of @xmath , cf. Section 7.2 . Finally, if
we start with a system of equations of the form @xmath for @xmath
instead of a single one, we may first determine @xmath following the
steps described above and then find the solutions to the initial set of
equations by intersecting @xmath for @xmath .

The described procedure turns out to be rather difficult to implement in
practice. However, in the present section we present a very simplified
example where the method works. Let @xmath . Let us also take @xmath and
@xmath to be two Hermitian operators on @xmath . We assume that @xmath
and @xmath commute, so that they have a common eigenbasis @xmath .
Moreover, let the respective eigenvalues for @xmath fulfill @xmath ,
while for @xmath we have @xmath . In such case, it is relatively easy to
find all possible solutions of the set of equations

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

This can be done using the technique of Groebner bases discussed in Part
II of the thesis.

In the setting described above, the general characterization of code
carriers included in [ ref.CKZ06 , Section 4] reduces to

  -- -------- -- -------
     @xmath      (8.3)
  -- -------- -- -------

Similarly for @xmath ,

  -- -------- -- -------
     @xmath      (8.4)
  -- -------- -- -------

The question whether there exists a @xmath that satisfies the set of
equations ( 8.2 ) is equivalent to the existence of an identical pair of
subspaces in the sets @xmath and @xmath , given by the equations ( 8.3 )
and ( 8.4 ). Fortunately, the existence can easily be checked. Due to
the specific form of the subspaces in formulas ( 8.3 ) and ( 8.4 ), the
intersection of @xmath and @xmath is nonempty if and only if the
following equations are satisfied for some admissible values of @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath and @xmath .

  -- -------- -- -------- -- -------
     @xmath      @xmath      (8.5)
     @xmath      @xmath      (8.6)
  -- -------- -- -------- -- -------

The formulas above imply a weaker set of equations

  -- -------- -- -------- -- -------
     @xmath      @xmath      (8.7)
     @xmath      @xmath      (8.8)
  -- -------- -- -------- -- -------

which can be rewritten in the form

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

In the above expression, we the following notation was used: @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath . The newly
introduced variables @xmath , @xmath , @xmath , @xmath and @xmath ,
@xmath for @xmath must be nonnegative and fulfill the additional
conditions

  -- -------- -- --------
     @xmath      (8.10)
     @xmath      (8.11)
     @xmath      (8.12)
  -- -------- -- --------

The approach we take in the following is to solve ( 8.9 ) together with
( 8.10 )–( 8.12 ) as if @xmath , @xmath , @xmath , @xmath and @xmath ,
@xmath for @xmath were allowed to take arbitrary values in @xmath .
Next, we look for real, nonnegative solutions. Note that the equalities
( 8.9 ), as well as ( 8.10 )–( 8.12 ), can be rewritten as polynomial
equations in the variables @xmath , @xmath , @xmath , @xmath and @xmath
, @xmath . Therefore, for fixed values of @xmath and @xmath , @xmath ,
we can try to solve the equations ( 8.9 ), ( 8.10 )–( 8.12 ) using the
Groebner basis approach described in Chapter 4 . As an example, let us
consider @xmath , @xmath , @xmath , @xmath and @xmath , @xmath , @xmath
, @xmath . Then, a Groebner basis calculation in @xmath for the
equations ( 8.9 ) and ( 8.10 )–( 8.12 ) gives the following result,

  -- -------- -- --------
     @xmath      (8.13)
  -- -------- -- --------

According to what we learned in Chapter 4 , we get a set of equations
equivalent to ( 8.9 ) and ( 8.10 )–( 8.12 ) by equating the above
polynomials to zero. As expected, the first polynomial in ( 8.13 ) only
involves the variable @xmath . Moreover, its seven roots can be
explicitly found. They are equal to @xmath , @xmath , @xmath , @xmath ,
@xmath , @xmath , @xmath . The structure of the remaining equations
resulting from the Groebner basis ( 8.13 ) is such that after we find
@xmath , the admissible values of the other variables can be determined
by simple substitution. In this way we get the following solutions
@xmath

1.  @xmath ,

2.  @xmath ,

3.  @xmath ,

4.  @xmath ,

5.  @xmath ,

6.  @xmath
    @xmath ,

7.  @xmath
    @xmath .

The numerical values for the solutions were calculated using exact
algebraic expressions. As we can see, only solution number @xmath has
all its coordinates nonnegative. Thus, if there exists a solution to
equations ( 8.5 ) and ( 8.6 ), the respective values of @xmath , @xmath
, @xmath , @xmath , @xmath and @xmath must be such that @xmath , @xmath
, @xmath , @xmath , as well as @xmath , @xmath . Hence @xmath , @xmath .
The formulas ( 8.5 ) and ( 8.6 ) take the form

  -- -------- -- --------
     @xmath      (8.14)
  -- -------- -- --------

where we introduced @xmath and @xmath . There are only two equations
left, since the second one in ( 8.5 ) and the first one in ( 8.6 ) are
trivially fulfilled for @xmath . Clearly, the equalities in ( 8.14 ) are
equivalent to @xmath and @xmath , or @xmath and @xmath , respectively.
From the last two formulas, we get @xmath and @xmath , which means that
@xmath and @xmath for some @xmath . Moreover, @xmath implies that @xmath
. In conclusion, the full set of solutions are parametrized by the two
angles @xmath and @xmath . The solutions are of the form

  -- -------- -- --------
     @xmath      (8.15)
  -- -------- -- --------

The corresponding compression value @xmath for @xmath is @xmath , while
for @xmath , we get @xmath . Further investigation of equations ( 8.5 )
and ( 8.6 ) in Mathematica suggests that for any choice of the
eigenvalues of @xmath and @xmath , such that @xmath and @xmath , there
exists a single family of solutions to equations ( 8.5 ) and ( 8.6 ),
either with @xmath or with @xmath . By choosing the eigenvalues from the
set of rational numbers, we seem always to obtain polynomial equations
that are exactly solvable.

#### 8.2 Completely Entangled Subspaces

Linear subspaces without a product vector are called Completely
Entangled Subspaces or CES for short. In the present section, we shall
discuss the question how to check whether a given subspace is a CES or
not. In particular, we shall give an example of a one-parameter family
of subspaces of @xmath and characterize the values of the parameter for
which the subspace and its orthogonal complement are completely
entangled.

Let us start with the general question about the existence of a product
vector in a linear subspace. Both the set of product states and a linear
subspace are projective varieties and it should be possible to determine
their intersection using the techniques described above. An approach we
successfully used was very straightforward. The general algorithm we
applied is shown in Figure 8.1 . The main idea is to write a set of
polynomial equations, corresponding to @xmath for a subspace @xmath and
then generate the corresponding Groebner basis. The answer can often be
read from the output. According to Proposition 4.32 , a necessary and
sufficient condition for a set of polynomial equations to have a
solution (over @xmath ) is that the corresponding reduced Groebner basis
be different from @xmath .

As a careful reader would notice, more than a single Groebner basis is
actually calculated, and each of them has some additional polynomials.
This is so because they are different dehomogenizations of the set of
equations @xmath , @xmath , which corresponds to @xmath . We
dehomogenize the equations in order to eliminate trivial solutions,
corresponding to a zero “product” vector. Moreover, after
dehomogenization product vectors that are a multiple of each other
appear as a single solution, which is a desirable feature. For example,
if we dehomogenize by adding the polynomials @xmath and @xmath , we
capture all product vectors @xmath with the first coordinate in @xmath
and @xmath nonvanishing. The method can be generalized in an obvious way
to the multipartite case.

In the sequel, we give details of the procedure for the particular case
of product vectors in a family @xmath of six-dimensional subspaces of
@xmath . In this case, we can avoid considering @xmath different
dehomogenizations and we get away with only four, three of which are
different from those we would normally have used with the algorithm in
Figure 8.1 . The elements of the family @xmath we consider are subspaces
spanned by the vectors

  -- -------- -- --------
     @xmath      (8.16)
  -- -------- -- --------

where @xmath . As it can be easily checked, the orthogonal complement
@xmath is spanned by the vectors

  -- -- -- --------
           (8.17)
  -- -- -- --------

Consider product vectors of the form @xmath . The condition @xmath is
equivalent to @xmath @xmath , which is a set of homogeneous polynomial
equations. We would like to find their solutions with @xmath and @xmath
. A possible way to achieve this goal is to: i) add the polynomials
@xmath and @xmath or equivalently, to substitute @xmath , @xmath . This
gives us a dehomogenized set of polynomial equations, which capture all
the nontrivial solutions of @xmath @xmath , apart from those with @xmath
or @xmath . In order to account for the possible deficit, one needs to
consider other dehomogenizations. One way to do it is to proceed as in
Figure 8.1 and dehomogenize in 12 different ways. However, in the case
we consider it is easier to do the following substitutions: ii) @xmath
and @xmath , iii) @xmath and @xmath and iv) @xmath , @xmath .
Equivalently, one adds ii) @xmath and @xmath , iii) @xmath and @xmath ,
iv) @xmath and @xmath to the ideal generated by the equations @xmath for
@xmath . The set of polynomials @xmath reads

  -- -------- -- --------
     @xmath      (8.18)
  -- -------- -- --------

After dehomogenization i) and calculation of the corresponding Groebner
basis in the ring @xmath , we get

  -- -------- -- --------
     @xmath      (8.19)
  -- -------- -- --------

We clearly see that after a substitution of a particular value of @xmath
, the first element of the basis is a nonzero constant in @xmath unless
the substituted value is a solution of the equation @xmath . This
implies that @xmath is in the ideal generated by @xmath unless @xmath or
@xmath . This implies that there is no solution to the corresponding
equations for almost all choices of @xmath . Equivalently, there is no
product vector @xmath with the first coordinate of @xmath and @xmath
nonvanishing in @xmath unless @xmath or @xmath . Obviously, there exist
product vectors in @xmath when @xmath , because the vectors @xmath are
of a product form. Thus we have already excluded @xmath in the
definition of @xmath given above. For @xmath , we get the following
Groebner basis in the ring @xmath

  -- -------- -- --------
     @xmath      (8.20)
  -- -------- -- --------

It is easy to see that the above equations have six solutions,
corresponding to the choices of @xmath and @xmath , @xmath . Thus there
are six product vectors @xmath with nonvanishing first coordinates of
@xmath and @xmath in @xmath . Similarly for @xmath , we get the
following Groebner basis

  -- -------- -- --------
     @xmath      (8.21)
  -- -------- -- --------

Again, there are six product vectors @xmath with nonvanishing first
coordinates of @xmath and @xmath in @xmath .

We still need to consider the dehomogenizations ii)-iv) for a general
@xmath . In the case ii), we get the following Groebner basis

  -- -------- -- --------
     @xmath      (8.22)
  -- -------- -- --------

A solution for @xmath must necessarily have @xmath , which implies that
@xmath . Therefore also @xmath . Thus @xmath admits no product vector
@xmath with nonvanishing first coordinate in @xmath and vanishing first
coordinate in @xmath . In the case iii), we get the following Groebner
basis in @xmath

  -- -------- -- --------
     @xmath      (8.23)
  -- -------- -- --------

One immediately sees that for @xmath , the above polynomials vanish only
if @xmath , which again gives a zero product vector. Therefore, there
are no product vectors @xmath with vanishing first coordinate of @xmath
and nonvanishing first coordinate of @xmath in @xmath for @xmath . We
only need to consider the last case, number iv), when the first
coordinates of both @xmath and @xmath vanish. The corresponding Groebner
basis reads

  -- -------- -- --------
     @xmath      (8.24)
  -- -------- -- --------

If @xmath , we see from the first four polynomials that @xmath .
Therefore we must have @xmath in order to obtain a nonzero vector @xmath
. However, a substitution of @xmath to ( 8.24 ) yields @xmath . We see
that these polynomials vanish simultaneously only if @xmath or @xmath .
In either case, @xmath vanishes. Thus, there are no nonzero product
vectors @xmath with vanishing first coordinates of @xmath and @xmath in
@xmath for @xmath .

We can summarize our results by saying that @xmath is a CES for all
@xmath . We can also easily repeat the above described procedure for the
subspace @xmath and obtain an analogous result. In this case @xmath , so
the rôle of the vectors @xmath is played by the vectors @xmath .
Otherwise, the calculation is almost the same. We obtain the following
four Groebner Bases.

  -- -------- -------- -- -- --------
     @xmath   @xmath         (8.25)
              @xmath         (8.26)
              @xmath         (8.27)
              @xmath         (8.28)
     @xmath   @xmath         (8.29)
     @xmath   @xmath         (8.30)
     @xmath   @xmath         (8.31)
  -- -------- -------- -- -- --------

with the notation i)-iv) referring to dehomogenizations of types i)-iv),
as described above. An argument very similar to the one given above
shows that there are no product vectors in @xmath , as long as @xmath .
The case @xmath is excluded by assumption, whereas for @xmath it can
again be checked that there are six product vectors in the subspace in
question, which this time is @xmath .

The results of the present section can be summarized by saying that,
concerning the @xmath CES problem considered above, the family of
subspaces @xmath , @xmath , spanned by the vectors ( 8.16 ), consists of
CES , with the exception of @xmath . Moreover, the orthogonal complement
@xmath is also completely entangled for @xmath .

#### 8.3 Maximally entangled states in linear subspaces

In the previous section, we discussed the existence of product vectors
in linear subspaces. It is natural to ask somewhat opposite question,
under which conditions a linear subspace admits maximally entangled
vectors, i.e. vectors of the form @xmath , where the summation goes from
@xmath to the dimension of the subsystems and @xmath and @xmath are
orthonormal bases for the first and the second subsystem, respectively.
By solving two examples, we will show that the problem can be tackled
using the techniques of Groebner bases.

Let us start with a subspace orthogonal to an Unextendible Product Basis
in @xmath , i.e. to a set of orthogonal product vectors such that no
other product vector in @xmath is orthogonal to all of them. We shall
discuss Unextendible Product Bases in more detail in Chapter 9 and here
we restrict our attention to the question whether there exist maximally
entangled vectors in the orthogonal complement of a particular UPB,
given by

  -- -------- -- --------
     @xmath      (8.32)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (8.33)
  -- -------- -- --------

It will be clear from the following discussion that the methods we use
can be applied in a much more general setting.

One can easily see that a vector @xmath is maximally entangled if and
only if the matrix @xmath is unitary. This yields a set of polynomial
equations @xmath on the matrix elements @xmath and their complex
conjugates @xmath . Another set of equations comes from the
orthogonality conditions to the UPB given in ( 8.32 ) and ( 8.33 ). The
equations are linear and can be solved explicitly, which we leave as a
simple exercise to the reader. The answer is

  -- -------- -- --------
     @xmath      (8.34)
  -- -------- -- --------

where @xmath are arbitrary complex parameters. The conditions @xmath for
@xmath imply

  -- -------- -- --------
     @xmath      (8.35)
     @xmath      (8.36)
  -- -------- -- --------

For @xmath , we have the following equations @xmath ,

  -- -------- -- --------
     @xmath      (8.37)
     @xmath      (8.38)
     @xmath      (8.39)
  -- -------- -- --------

The crucial observation now is that the complex conjugates of ( 8.37 )-(
8.38 ) consist an independent set of equations if @xmath and @xmath are
perceived as @xmath independent complex variables. This is the approach
we are going to take in the following. The complex conjugates of ( 8.37
)-( 8.38 ) read

  -- -------- -- --------
     @xmath      (8.40)
     @xmath      (8.41)
     @xmath      (8.42)
  -- -------- -- --------

A Groebner basis calculation in @xmath for the nine polynomials in (
8.35 )-( 8.42 ) yields the following basis

  -- -------- -- --------
     @xmath      (8.43)
  -- -------- -- --------

Although the above formulas look very complicated, some of the
polynomials in the ideal generated by ( 8.35 )-( 8.42 ) are of a very
simple form. In particular, we obtain the corresponding equations @xmath
and @xmath which clearly imply @xmath and @xmath if we recall the
interpretation of @xmath and @xmath as complex conjugates of @xmath and
@xmath , resp. A substitution of @xmath in ( 8.43 ) yields

  -- -------- -- --------
     @xmath      (8.44)
  -- -------- -- --------

where we removed all the zero polynomials. Again, because of the
appearance of the polynomial @xmath , a solution must have @xmath and
@xmath . When this is substituted to ( 8.44 ), we obtain a single
nonzero polynomial @xmath , which in turn applies @xmath . In summary,
the only solution to the initial set of equations satisfying the
constraint that @xmath and @xmath are complex conjugate is the zero
matrix. Since it is clearly not unitary, we conclude that there exist no
unitary matrices of the form ( 8.34 ). This is equivalent to say that
there are no maximally entangled states in the orthogonal complement of
the UPB given by the formulas ( 8.32 ) and ( 8.33 ).

The example discussed above, although rather elegant mathematically, may
seem unsatisfactory from the point of view of quantum information
science. A natural question to ask is whether there exist Unextendible
Product Bases in the @xmath case which admit a maximally entangled
vector in their orthogonal complement. It turns out that the method
presented above is powerful enough to give a affirmative answer to the
question.Consider the following one-parameter family of Unextendible
Product Bases in @xmath .

  -- -------- -- --------
     @xmath      (8.45)
  -- -------- -- --------

where @xmath is arbitrary and we used the notation @xmath for a product
vector @xmath , which is practical here. Note that the vectors are not
normalized.

Our aim in the following is to decide whether the orthogonal complement
to the UPB in ( 8.45 ) contains a maximally entangled state for some
@xmath or not. Orthogonality conditions to the subspace spanned by
@xmath are a set of linear equations and can be solved explicitly. The
result is

  -- -------- -- --------
     @xmath      (8.46)
  -- -------- -- --------

where @xmath denote the coordinates of a vector @xmath .

Taking the conjugate transpose of ( 8.46 ) and multiplying by the matrix
( 8.46 ) itself, we get the conditions for @xmath to be unitary, or
@xmath to be maximally entangled, in the form

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

The last expression is not a polynomial in @xmath , but can be easily
transformed to

  -- -------- --
     @xmath   
  -- -------- --

if we remember that by assumption @xmath . Thus we get a set of nine
polynomial equations in the variables @xmath , equivalent to the
condition that @xmath be maximally entangled. Next, we calculate the
corresponding Groebner basis in @xmath , where we take @xmath as a set
of independent variables. In other words, we forget that numbers like
@xmath and @xmath are conjugate, and try to impose this condition only
after a calculation of the Groebner basis. The basis reads

  -- -------- -- --------
     @xmath      (8.47)
  -- -------- -- --------

where we introduced the notation @xmath . From the first polynomial we
see that a solution can exist only if @xmath . But @xmath is impossible
according to our definition of @xmath , and @xmath is excluded by the
assumption @xmath . Therefore maximally entangled vectors in the
orthogonal complement to the UPB in ( 8.45 ) can exist only if @xmath ,
thus if @xmath . If we substitute @xmath in ( 8.47 ) and calculate the
Groebner basis of the resulting polynomials in @xmath , we get

  -- -------- -- --------
     @xmath      (8.48)
  -- -------- -- --------

Clearly, all the polynomials can be made zero by choosing @xmath ,
@xmath , @xmath and @xmath . Therefore there exist a single, up to an
overall phase factor, maximally entangled state in the orthogonal
complement of the UPB in ( 8.45 ). It has the following coordinate
matrix

  -- -------- -- --------
     @xmath      (8.49)
  -- -------- -- --------

In summary, we have shown that the UBP given in equation ( 8.45 ) does
not admit a maximally entangled vector in its complement, with the only
exception of @xmath . When @xmath , there is a maximally entangled
vector in the orthogonal complement of the UPB ( 8.45 ), which has a
coordinate matrix of the form ( 8.49 ).

#### 8.4 Mutually Unbiased Bases

As we already explained in Section 2.1 , a generalization of quantum
cryptography protocols such as BB84 to multidimensional quantum systems
[ CBKG2001 ] relies on the notion of mutually unbiased bases , MUBs for
short. Two orthonormal bases @xmath , @xmath of @xmath are said to be
(mutually) unbiased if and only if

  -- -------- -- --------
     @xmath      (8.50)
  -- -------- -- --------

holds for all @xmath . The importance of the above relation for quantum
state determination has been first pointed out by Ivanović [ Ivanovic81
] , who also proved the existence of @xmath mutually unbiased bases in
@xmath when @xmath is a prime number. Later, Wootters and Fields [ WF89
] showed that there are at most @xmath mutually unbiased bases in @xmath
and gave examples of full sets of MUBs when @xmath is a prime power.
Moreover, they demonstrated that quantum state determination using a
full set of MUBs is optimal in the sense of giving minimum statistical
errors. A broader view of the known constructions of MUBs was then
provided in [ BBRV02 ] , where the authors related MUBs to classes of
pairwise orthogonal and commuting unitary matrices. The main efforts in
the field concentrated on proving or disproving the existence of maximal
sets of MUBs in non-prime power dimensions [ Grassl04 , A05 , BH07 ,
BBELTZ07 , BW08 , BW09 ] , which still remains an open problem. However,
on the basis of the extensive searches presented in [ BW09 ] , the
existence of four, let alone seven, MUBs in @xmath is almost certainly
excluded.

In the present section, we briefly describe how the authors of [ BW09 ]
used the technique of Groebner bases to provide a large number of
examples where a set of two MUBs in @xmath cannot be extended to a set
of four. We first need to introduce the notion of complex Hadamard
matrices (cf. e.g. [ TZ06 ] ). Such matrices are by definition unitaries
@xmath with the property that @xmath for all matrix elements @xmath of
@xmath . It is easy to notice that for any Hadamard matrix @xmath , the
canonical basis @xmath and the columns of @xmath , @xmath , constitute a
pair of MUBs. Any other basis mutually unbiased with respect to these
two, must also consist of columns of some complex Hadamard matrix. Now,
the strategy applied in [ BW09 ] was the following:

1.  Select a known Hadamard matrix @xmath in @xmath , the vast majority
    of which can be found in the online catalogue [ catalogue ] ,

2.  Parametrize a general, up to a phase, vector in @xmath mutually
    unbiased with respect to @xmath , as

      -- -------- -- --------
         @xmath      (8.51)
      -- -------- -- --------

    with @xmath denoting matrix transposition, @xmath and @xmath ,

3.  Multiply @xmath from the left by @xmath and equate the squared
    moduluses of the coordinates of the resulting vector to @xmath .
    This gives a set of polynomial equations in @xmath and @xmath ,
    equivalent to the unbiasedness condition

      -- -------- -- --------
         @xmath      (8.52)
      -- -------- -- --------

    for @xmath ,

4.  Solve the resulting equations, together with @xmath , @xmath , for
    @xmath . In this way, the set of all vectors in @xmath unbiased with
    respect to @xmath and @xmath is obtained,

5.  Check whether it is possible to arrange the resulting vectors in
    @xmath -tuples that consist MUBs, and how many such MUBs can be
    obtained altogether, including @xmath and @xmath .

The authors of [ BW09 ] worked mainly with the case @xmath , but the
above steps can be followed also when the MUB problem in dimension
different from @xmath is considered. For purely expository purposes, in
order not to resort to numerical solutions necessary in @xmath , we
shall now explain how the above method yields a complete set of MUBs in
@xmath , which is well-known to exist [ Ivanovic81 ] . This is in
contrast with the main findings of [ BW09 ] in dimension @xmath , where
the authors conclude that for no single one of the nearly 6000 Hadamard
matrices @xmath they studied, there exists more than three mutually
unbiased bases including @xmath and @xmath .

Up to some simple invariances (for more details, cf. e.g. [ TZ06 ] ),
there only exists one Hadamard matrix when @xmath , which is the Fourier
matrix

  -- -------- -- --------
     @xmath      (8.53)
  -- -------- -- --------

where @xmath . The corresponding unbiasedness conditions @xmath read

  -- -------- -- --------
     @xmath      (8.54)
  -- -------- -- --------

If we take into account the relations @xmath and @xmath , the above
equations take the form

  -- -------- -- --------
     @xmath      (8.55)
  -- -------- -- --------

which are clearly equivalent to the following system of equations

  -- -------- -- --------
     @xmath      (8.56)
  -- -------- -- --------

Taking the above equalities together with @xmath and @xmath , we get the
following set of polynomial equations

  -- -------- -- --------
     @xmath      (8.57)
  -- -------- -- --------

As we know from Chapter 4 , a possible approach to solving equations
like ( 8.57 ) is by the calculation of the corresponding Groebner basis,
preferably with respect to the lexicographic order. The result is

  -- -------- -- --------
     @xmath      (8.58)
  -- -------- -- --------

By equating the above polynomials to zero, we get a system of equations
equivalent to ( 8.57 ), which can be readily solved by backward
substitution. The corresponding solutions @xmath are the elements of the
following set

  -- -------- -- --------
     @xmath      (8.59)
  -- -------- -- --------

Hence, we get six vectors in total that are unbiased with respect to
@xmath and @xmath . Explicitly, we have the following vectors

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (8.60)
     @xmath   @xmath   @xmath      (8.61)
  -- -------- -------- -------- -- --------

By examining the inner products @xmath for @xmath , we get to the
conclusion that @xmath and @xmath are two orthonormal bases, mutually
unbiased with respect to each other. Consequently, all the four bases
@xmath , @xmath , @xmath and @xmath together constitute a full set of
MUBs in @xmath .

The authors of [ BW09 ] followed the same path of reasoning as in the
example described above, however they worked with @xmath and needed to
resort to numerical methods in order to obtain the solutions of the
respective polynomial equations. In their case, it turned out not to be
possible to find four mutually unbiased bases, starting from @xmath and
@xmath for any @xmath Hadamard matrix @xmath they examined.

#### 8.5 Symmetric Informationally Complete vectors

When discussing the applications of polynomial equations in quantum
information science, it seems impossible to neglect the prominent role
they play in the research on so-called Symmetric Informationally
Complete Positive Operator Valued Measures , or SIC-POVMs for short. A
SIC-POVM in @xmath is by definition a set of normalized vectors @xmath
with the property

  -- -------- -- --------
     @xmath      (8.62)
  -- -------- -- --------

for all @xmath , @xmath . The first generally recognized work on SIC
POVMs, although it uses a different name for the same object, is by
Zauner [ Zauner ] , who famously states a (stronger) version of the
following conjecture

###### Conjecture 1 (Zauner).

For every dimension @xmath there exists a SIC-POVM whose elements are
the orbit of a vector @xmath under the Heisenberg group, which consists
of elements @xmath , where @xmath , @xmath and

  -- -------- -- --------
     @xmath      (8.63)
  -- -------- -- --------

∎

The term SIC-POVM was coined by the authors of [ RBSC04 ] , and
SIC-POVMs became popular as a consequence of the usefulness for quantum
state tomography [ Scott06 ] and the rich mathematical structure they
have [ Appleby05 , Zhu2010 , Zhu2010b ] . In the following, we outline
how they relate to polynomial equations and we solve a very simplified
example where it is possible to find explicit algebraic expressions for
vectors constituting a SIC-POVM. Note, however, that the example we
solve is only a subcase of the general solution for @xmath , provided in
[ RBSC04 ] .

An approach to searching SIC-POVM vectors successfully applied in papers
like [ Grassl04 ] and [ ScottGrassl ] starts from writing ( 8.62 ) as a
set of polynomial equations for the real and imaginary parts of the
coefficients of the vectors @xmath . Such equations may contain a
reasonably small number of variables only if the vectors @xmath are not
assumed to be independent. The standard way to follow consists in
assuming that the requested SIC-POVM satisfies the Zauner conjecture,
therefore all the @xmath , @xmath , are determined by a single vector
@xmath , called the fiducial . In this way, the number of real variables
in the polynomial equations is reduced to @xmath , where the factor
@xmath comes from the fact that we can take the first coefficient of
@xmath to be real without affecting the whole SIC-POVM construction as
described by Conjecture 1 . Further simplifications also follow from the
full statement of the Zauner conjecture, which involves elements of the
Clifford group, cf. e.g. [ Appleby05 ] .

In the following, we show how to find an exemplary SIC-POVM in dimension
@xmath by solving a set of polynomial equations, based on the ideas
sketched above. Since a general form of SIC-POVMs in @xmath is known [
RBSC04 ] , our discussion should be perceived as a purely expository
one, aimed at giving a rough picture of what happens in real science
applications.

In our very simplified example, we are looking for @xmath normalized
vectors @xmath that would satisfy @xmath for all @xmath . As explained
above, the related polynomial equations become much easier to tackle if
a form of Conjecture 1 is assumed to hold. Hence, instead of looking for
general sets of nine vectors @xmath , we assume that @xmath is equal to
the set @xmath , where @xmath is a normalized fiducial vector in @xmath
, @xmath , and

  -- -------- -- --------
     @xmath      (8.64)
  -- -------- -- --------

Under the above assumption, the equations @xmath become equivalent to
@xmath for all such that @xmath or @xmath . The latter imply another set
of equalities, @xmath , where @xmath or @xmath and @xmath or @xmath . In
our case, the last set of equations take the explicit form

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

In order to find exemplary SIC-POVMs in @xmath , we add the
normalization condition @xmath for @xmath to the above equations, and
then we try the substitution @xmath . Note that the value @xmath has not
been selected at random, and the specific choice of @xmath makes the
subsequent calculations rather straightforward. However, any other
number of modulus @xmath can be tried as well, and would typically lead
to a few fiducial vectors or to the conclusion that no suitable
fiducials exist. For @xmath not an algebraic number, numerical methods
would be required to find the solutions or to show they are
non-existent.

Once we substituted @xmath for @xmath , we are left with a set of seven
polynomial equations for @xmath , some of which are redundant.
Calculation of the corresponding Groebner basis with respect to the
lexicographic order gives the following result,

  -- -------- -- --------
     @xmath      (8.65)
  -- -------- -- --------

By equating the above polynomials to @xmath , we get a system of
equations that can readily be solved by backward substitution, provided
that one can find solutions to the equation @xmath . Fortunately, this
problem can easily be solved explicitly, as @xmath . Thus, we have
@xmath , @xmath and @xmath as the possible values for the @xmath
coordinate. Substitution of any of these values to ( 8.65 ) gives us a
set of polynomials in @xmath of maximum degree @xmath , whose common
zeros are easy to find. Altogether, there are nine solutions @xmath to
the equations @xmath and @xmath , corresponding to nine fiducials. We
give a list in Table 8.1 . Note that a vector @xmath is a fiducial if
and only if @xmath also has this property. This is a general fact, which
can be confirmed with Table 8.1 . Thus we have completed the task of
finding a set of three-dimensional SIC-POVM vectors with help of the
Groebner basis method.

### Chapter 9 A structure theorem for PPT bound entangled states of
lowest rank

The aim of the present section is to present the main result of the
thesis, concerning positive-partial-transpose non-separable states of
rank @xmath in @xmath systems. As indicated in [ LS2010 ] , they all
seem to be possible to locally transform to projections onto the
orthogonal complement to a subspace spanned by an orthogonal
Unextendible Product Basis [ Bennett99 ] . Thus, there is strong
numerical evidence that they are all locally equivalent to bound
entangled states of the form discussed in [ Bennett99 ] . In the
following, we analytically prove that this is actually the case. Note
that according to the results of [ HLVC2000 ] , four is the minimal rank
for an entangled PPT state. Therefore it is correct to say that our
theorem concerns non-separable PPT states of lowest rank. Very shortly
after our paper [ S2011 ] was available as a preprint on arXiv, Chen and
Đoković [ CD2011 ] presented an alternative proof of the theorem. The
research reported here was conducted independently of [ CD2011 ] , and
the author had no prior knowledge about the manuscript by the other
authors. An important related work by Chen and Đoković is also [
CD2011distill ] .

Before we start with the proof, it will be useful to introduce the
concept of general Unextendible Product Bases, discussed in more detail
elsewhere [ S2011 ] .

#### 9.1 General Unextendible Product Bases

The most common definition of an Unextendible Product Basis (UPB), in
accordance with [ Bennett99 ] , has already been phrased in Section 3.2
. Here, we start with a definition of a general UPB.

###### Definition 9.1.

Take @xmath . By a general Unextendible Product Basis , or a gUPB for
short, we mean a set @xmath of product vectors in @xmath , @xmath , such
that there is no product vector in @xmath , the orthogonal complement to
the linear span of @xmath .

In other words, a gUPB is a set of product vectors @xmath such that
there is no product vector orthogonal to all of them. Note that we do
not require the vectors to be linearly independent, and this choice is
somewhat arbitrary. Yet another way of phrasing the above definition is
that the orthogonal complement to a gUPB is a Completely Entangled
Subspace [ Partha04 , Bhat04 ] , or CES for short, cf. Section 8.2 .

###### Remark 9.2.

The definition of a gUPB can be trivially extended to a multipartite
setting.

We know that gUPBs do exist. Any UPB consisting of orthogonal vectors is
an example (concrete UPBs can be found e.g. in [ DiVicenzo04 ] ). We
also know that for some spaces, no UPB consisting of orthogonal vectors
can exist. For example, it has been noticed as early as in [ Bennett99 ]
that @xmath systems do not admit an orthogonal UPB, and a more general
discussion of existence questions for orthogonal UPBs has been included
in [ AL2001 ] . In the following, we show that gUPBs are much more
common than the usual UPBs, and give a characterization of gUPBs of
minimal number of elements.

First, let us answer a question about the minimum number of elements in
a gUPB in @xmath .

###### Proposition 9.3.

A set of vectors @xmath consisting of @xmath elements is not a
generalized UPB.

###### Proof.

There exists a vector @xmath orthogonal to all the vectors @xmath with
@xmath . Moreover, there exists a @xmath orthogonal to the vectors
@xmath with @xmath (because @xmath ). The product vector @xmath is
orthogonal to all @xmath for @xmath . ∎

###### Proposition 9.4.

A set of vectors @xmath is a gUPB if and only if any @xmath -tuple of
vectors in @xmath consists of linearly independent vectors, the same as
any @xmath -tuple of vectors in @xmath .

###### Proof.

In order to prove necessity, assume that an @xmath -tuple of vectors
@xmath is linearly dependent. Therefore there exists a vector @xmath
orthogonal to all of them. Vectors of the form @xmath with an arbitrary
@xmath are orthogonal to all the vectors @xmath . Obviously, @xmath can
be chosen in such a way that @xmath is orthogonal to the remaining
@xmath elements of @xmath (because @xmath ). For a sufficiency proof,
assume that @xmath is orthogonal to @xmath for @xmath . The vector
@xmath can be orthogonal to at most @xmath of @xmath ’s, whereas @xmath
cannot be orthogonal to more than @xmath @xmath ’s (remember the linear
independence of @xmath -tuples and @xmath -tuples, respectively). This
gives a maximum of @xmath vectors in @xmath orthogonal to @xmath .
Therefore @xmath cannot be orthogonal to all the @xmath ’s, the set
@xmath is a gUPB. ∎

It is natural to ask for a generalization of Proposition 9.4 for sets of
product vectors consisting of more than @xmath elements. We have the
following

###### Proposition 9.5.

A set of vectors @xmath with @xmath is a gUBP if and only if for any
@xmath such that @xmath and @xmath , at least one of the sets of vectors
@xmath and @xmath spans the entire corresponding vector space ( @xmath
or @xmath , resp.).

###### Proof.

Let us first prove necessity. Assume that the vectors @xmath constitute
a gUPB and choose some @xmath as in the statement of the proposition. If
neither of the sets @xmath and @xmath spans the respective vector space,
there exist @xmath and @xmath such that @xmath and @xmath for all @xmath
and @xmath . Because of the condition @xmath , we clearly have @xmath
for @xmath . This contradicts the fact that the vectors @xmath
constitute a gUPB. In order to show sufficiency, assume that @xmath is
such that @xmath for all @xmath . Define the set of indices @xmath and
@xmath . Clearly, we must have @xmath . Thus it is possible to choose
@xmath and @xmath such that @xmath and @xmath . By the very definition
of @xmath and @xmath , we have @xmath for all @xmath and @xmath for
@xmath . But according to the assumptions of the theorem, this is only
possible if @xmath or @xmath is equal to zero. Thus @xmath is a gUPB. ∎

Certain characterizations of gUPBs were earlier obtained in [
Pittenger03 ] , but the above results, rather surprisingly, seem to
appear for the first time in our work [ S2011 ] . They can also easily
be generalized to a multipartite setting.

###### Proposition 9.6.

A set of vectors @xmath with @xmath is a gUBP if and only if for any
@xmath such that @xmath for all @xmath and @xmath , at least one of the
sets of vectors @xmath , @xmath , spans the entire corresponding vector
space @xmath .

###### Proof.

Follows the same lines as the proof of Proposition 9.5 and will be
omitted here. ∎

#### 9.2 The concept of local equivalence

Before we present the proof of the main result of the thesis (Theorem
9.27 ), we also need to introduce the concept of local equivalence.
Numerous questions of physical or mathematical origin need the proper
identification of a symmetry group relevant to the problem in order to
simplify the solution, or even to find it at all. The same is the case
for the result we are going to obtain below. For PPT states, a natural
group of symmetries should be of a product form, @xmath , because all
such transformations preserve the property of being PPT. In physical
terms, they preserve the splitting of a composite system into
subsystems, which is a highly desirable property. The remaining question
is, what group should @xmath and @xmath belong to. When the amount of
entanglement between the two subsystems is in question, a natural choice
is @xmath and @xmath in the Unitary or Special Unitary group. Such
transformations cannot change any measure of entanglement. However, if
the aim is to classify PPT states with respect to the property of being
extreme, being an edge state [ LKHC2001 ] , or the number and
dimensionalities spanned by the product vectors in their kernels or
ranges, @xmath and @xmath should most naturally belong to the General
Linear or Special Linear group. There is no essential difference between
the two latter choices. Since we are not interested in positive scaling
factors in front of the states, we choose to work with the Special
Linear group. This was also the approach so successfully used by the
authors of [ LMO2006 , LS2010 , HHMS2011 ] . We should remark that,
while a PPT state is transformed according to @xmath , the product
vectors in its kernel and its range undergo the following
transformation, @xmath . Conversely, a transformation @xmath forces a
change of @xmath into @xmath . It is these kind of transformations we
will have in mind when we talk about “local equivalence”, “local
equivalence” or “ @xmath equivalence” in the following sections.

Any similar terms, even not listed here, will also refer to precisely
the same situation. Nevertheless, when product vectors in the kernel of
a PPT state @xmath are in question, it is more convenient to look at
them as rays, points in the projective space. In such case, it is also
more accurate to refer to the projectivisation of the group @xmath ,
namely to @xmath , with referring to the Projective Special Linear
group. In simple words, we may multiply vectors @xmath by arbitrary
individual factors, and they will remain elements of the kernel of
@xmath . We may also transform them by a @xmath transformation. All in
all, we have a group of transformations that is most properly described
as @xmath . Note that the use of this term is motivated mainly by the
possibility to avoid excessive comments about constant factors in front
of the product vectors in @xmath . We are legitimate to use the
previously introduced name “local equivalence” also for the @xmath
transformations we just described because constant multiplicative
factors in front of vectors in @xmath are completely irrelevant to
@xmath itself.

The ultimate reason for using equivalences of the form described above
will be the simplicity of our main result, a characterization theorem
that we are going to obtain in Section 9.8 . The equivalence classes
under @xmath of non-separable PPT states of rank @xmath in @xmath
systems turn out to be parametrized by just four real, positive numbers.
Moreover, each class has a representative which is a projection onto a
Completely Entangled Subspace complementary to a @xmath orthogonal UBP.
This is quite a striking result, for which strong numerical evidence was
provided by Leinaas et al. in [ LS2010 ] and later supported by certain
analytical results of [ HHMS2011 ] .

#### 9.3 Outline of the proof

The proof of our main result is not excessively complicated, but it
needs a considerable amount of work. It also consists of a number of
steps which do not seem easy to merge. In order to simplify the reading,
we start with a list of building blocks. We will elaborate on each of
them in the following sections.

1.  The kernel of a rank four PPT state @xmath must intersect the Segre
    variety @xmath in a transverse way. In particular, according to the
    Bezout’s Theorem, the intersection must consist of exactly six
    points.

2.  The product vectors in the kernel of a rank @xmath PPT state in the
    @xmath case span the kernel. As a result, they must be a generalized
    UPB. There cannot exist a product vector orthogonal to all of them.

3.  A generalized UPB in the @xmath case is locally equivalent to an
    orthogonal one if and only if certain invariants @xmath , introduced
    by Leinaas et al. in [ LS2010 ] , are all positive, possibly after
    the vectors are permuted.

4.  A generalized UPB in a @xmath system is contained in a kernel of
    some rank four PPT state if and only if the corresponding values of
    @xmath are positive, possibly after the vectors are permuted.
    Moreover, in such case the PPT state in question is uniquely
    determined.

The final conclusion from the facts mentioned in items @xmath is that
the only non-separable PPT states of rank @xmath in @xmath systems are
local transforms of projections onto orthogonal complements of
orthogonal pentagram-type Unextendible Product Bases [ Bennett99 ,
DiVicenzo04 ] .

#### 9.4 Product vectors in the kernel of a PPT state

The present section elaborates on item @xmath in the list given above
and on related topics. Let us start with an elementary fact.

###### Lemma 9.7.

A product vector @xmath is in the kernel of a PPT state @xmath if and
only if the partially conjugated states @xmath and @xmath are in the
kernels of @xmath and @xmath , respectively.

###### Proof.

It follows from the equality between the expressions @xmath , @xmath and
@xmath , by the positivity of @xmath , @xmath and @xmath . ∎

In the above lemma, we did not assume anything about the dimensionality
of the system. Neither we do it in the following.

###### Lemma 9.8.

Assume that a product vector @xmath is in the kernel of a PPT state
@xmath . In such case

  -- -- -- -------
           (9.1)
  -- -- -- -------

###### Proof.

Since @xmath , we know from Lemma 9.7 that @xmath , which obviously
implies @xmath . This is the first equality in ( 9.1 ). The second one
can be obtained in a similar way. ∎

Let us denote by @xmath , @xmath and @xmath the rank, the range and the
kernel of @xmath . Our next lemma applies specifically to the @xmath
case and concerns so-called edge states . For more information about
this topic, consult [ LKHC2001 ] . In short, edge PPT states are PPT
states @xmath that do not admit a product vector @xmath such that @xmath
.

###### Lemma 9.9.

Assume that both @xmath and @xmath , with @xmath , @xmath in @xmath and
@xmath in @xmath , @xmath , belong to the kernel of a PPT state @xmath ,
acting on @xmath . The state @xmath is either supported on a @xmath or
smaller subspace, or it can be written as @xmath for some @xmath ,
@xmath , @xmath linearly independent of @xmath and @xmath , and a PPT
state @xmath , supported on a @xmath or smaller subspace. Moreover, the
rank @xmath and @xmath . In a situation when the reduction is possible,
the state @xmath is not an edge PPT state. In particular, @xmath is not
an extreme and non-separable PPT state.

###### Proof.

Let us assume that the product states @xmath and @xmath belong to the
kernel of @xmath . Let @xmath be an @xmath transformation that brings
@xmath to @xmath and @xmath . A little inspection shows that Lemmas 1
and 2 of [ HLVC2000 ] can be applied to @xmath . Consequently, we see
that either @xmath is supported on a @xmath or smaller space, or the
assertion of Lemma 2 of [ HLVC2000 ] tells us that @xmath for some
@xmath , and moreover, @xmath is a PPT state supported on a @xmath or
smaller subspace, with @xmath and @xmath . We have @xmath , where @xmath
and @xmath . The states @xmath and @xmath still have their ranks reduced
by one with respect to the ranks of @xmath and @xmath , respectively.
The subspaces on which they are supported are of the same type as for
@xmath , hence @xmath or smaller. The statement that @xmath is not an
edge state simply follows because @xmath is in @xmath while its partial
conjugation is in @xmath . ∎

The following result reduces a more general case to the situation
considered above. However, this time we assume @xmath .

###### Lemma 9.10.

Let @xmath be an element of a PPT state @xmath , acting on @xmath .
There cannot exist a nonzero vector @xmath , with @xmath or @xmath , in
the kernel of @xmath , unless one of the following is true: i) @xmath
for @xmath , @xmath and @xmath a PPT state supported on a @xmath or
smaller subspace with @xmath and @xmath or ii) @xmath is supported on a
@xmath or smaller subspace itself.

###### Proof.

Assume that there is a state of the form @xmath in the kernel of @xmath
. This is equivalent to saying that @xmath . The inner product
factorizes as

  -- -- --
        
  -- -- --

The two factors in the middle vanish according to Lemma 9.8 , while the
two remaining factors are nonnegative as a consequence of positivity of
@xmath . Therefore, the only possibility for the above expression to
vanish is when @xmath and @xmath . This in turn means that @xmath and
@xmath . According to our assumptions, at least one of these equalities
is nontrivial (i.e. @xmath or @xmath ). Lemma 9.9 can be applied, and
Lemma 9.10 follows directly. ∎

The importance of Lemma 9.10 is evident if we realize that the tangent
space to the Segre variety, or to the set of product states at a point
@xmath , consists precisely of the vectors of the form considered above.
We have

###### Lemma 9.11.

Elements of the tangent space to the Segre variety, or to the set of
product vectors at a point @xmath , are of the form

  -- -------- -- -------
     @xmath      (9.2)
  -- -------- -- -------

with @xmath and @xmath arbitrary.

###### Proof.

A heuristic proof may consist in writing @xmath , where the approximate
equality holds to the first order. A more rigorous proof can be found in
Example 5.21 of Section 5.2 , as well as in Example 14.16 of the
textbook by Harris [ Harris ] . ∎

Next, we specify the rank of @xmath to be @xmath and keep the assumption
that @xmath acts on @xmath . Thus the kernel of @xmath is of dimension
@xmath , which is the smallest number @xmath such that a @xmath
-dimensional linear subspace must intersect the set of product vectors
in @xmath , cf. e.g. [ Partha04 ] . Following Lemmas 9.10 and 9.11 , we
can show that the nonempty intersection is generic in the sense of
Bezout’s theorem [ Harris , Theorem 18.3] and thus it consists of
exactly six points.

###### Lemma 9.12.

Let @xmath be a non-separable PPT state of rank @xmath acting on @xmath
. The intersection between the respective Segre variety and the
five-dimensional kernel of @xmath is transverse at every point. There
are exactly six product vectors in the kernel of @xmath .

###### Proof.

Let us take @xmath . As we mentioned above, such a vector exists [
Partha04 , Cubitt07 ] by a dimensionality argument for projective
varieties. We easily see from Lemma 9.11 that the dimension of the
tangent space @xmath to the Segre variety at @xmath is @xmath , and thus
the projective dimension is @xmath . Being more explicit, any vector of
the form @xmath can be written in the form @xmath , where @xmath and
@xmath are two sets of three linearly independent vectors in @xmath and
@xmath , @xmath are arbitrary complex coefficients. From Lemmas 9.10 and
9.11 we know that the only vector in the intersection of @xmath and
@xmath is @xmath itself. It must be so, because otherwise we could
reduce the rank of @xmath by subtracting a projection onto a product
state. After the reduction, we would be left with a PPT state of rank
@xmath . However, all such PPT states are separable according to [
HLVC2000 ] , and @xmath would have to be separable as well. The other
option is that @xmath could be supported on @xmath or even a less
dimensional space itself. But then it is well-known that @xmath is
separable as as consequence of being PPT [ HHH96 ] . In either case, we
get a contradiction with the assumption that @xmath is non-separable.
Therefore, @xmath must be, up to a scalar factor, the only element of
the intersection between @xmath and @xmath . Consequently, the dimension
of @xmath equals @xmath , while its projective dimension is @xmath .
This equals the projective dimension of @xmath , or simpler, the
dimension of the complex projective space @xmath . In other words,
@xmath and @xmath span @xmath , which is equivalent to saying that the
intersection between @xmath and the Segre variety is transverse at
@xmath . Since we did not make any additional assumptions about @xmath
apart from that it belongs to the intersection, we see that the
intersection is transverse at every point. Therefore Bezout’s theorem
applies. The fact that there are exactly six points in the intersection
follows because the degree of the Segre variety @xmath is six [ Harris ,
Example 18.15] . ∎

In summary, in the present section we have shown that a non-separable
rank @xmath PPT state in a @xmath system must have exactly six vectors
in its kernel. This is in full agreement with an assertion of [ LS2010n
] . It should be noticed that, as a part of the proof of the above
lemma, we have shown that non-separable PPT states of rank @xmath in
@xmath systems are edge states. Thus, Lemmas 9.9 and 9.10 can be
directly applied. We will frequently use them in the following section.

#### 9.5 Product vectors in the kernel must be a gUPB

We already know that the number of product vectors in the kernel of a
rank @xmath non-separable PPT state of a @xmath system is six. In the
following, we discuss more specific properties of the set of six product
vectors. Let us denote them with @xmath , @xmath . It turns out that, up
to local equivalence, five of them can always be brought to a special
form, which has only four real parameters, the numbers @xmath introduced
in [ LS2010 ] . It then follows that the vectors @xmath , if they belong
to the kernel of a rank @xmath PPT state, must span a five-dimensional
subspace. Thus they span the kernel.

In order to prove our assertion, first observe that @xmath for @xmath
(cf. Lemmas 9.11 & 9.12 ), and thus they must span at least a
two-dimensional subspace of @xmath . Similarly for the @xmath ’s. Let us
try to assume first that one of the sets @xmath and @xmath spans a
two-dimensional subspace. We may, for example, try to assume this about
@xmath . Up to @xmath transformations, we have

  -- -------- -- -------
     @xmath      (9.3)
  -- -------- -- -------

where @xmath , @xmath , @xmath are all different and different from
@xmath and @xmath . When writing ( 9.3 ), we used the fact that there is
no pair of identical vectors in @xmath . Up to local transformations, we
have @xmath and @xmath . As for the other vectors @xmath , we use the
following notation, @xmath , @xmath . We also introduce coordinates
@xmath for general vectors @xmath in @xmath . Our aim is to show that
there exists a linear combination of the vectors @xmath of the form
@xmath from Lemma 9.11 . This will lead us to a contradiction and show
that @xmath ’s cannot be as in ( 9.3 ), and must span @xmath . An
analogous conclusion for @xmath ’s will be immediate.

Let us first observe that @xmath for all @xmath . Otherwise, we would
have three product vectors supported on @xmath . Up to local
equivalence, they would be of the form @xmath , @xmath and @xmath . In
such case, @xmath would be in the kernel of @xmath , which contradicts
Lemma 9.10 . Therefore we must have @xmath for all @xmath . Let us
choose @xmath and @xmath so that @xmath . The vector @xmath has a
vanishing coordinate @xmath and a non-vanishing coordinate @xmath
(remember that @xmath ). By subtracting @xmath times @xmath , we can
cancel the @xmath coordinate, and similarly cancel @xmath by subtracting
@xmath times @xmath . In the end, we see that a vector of the form
@xmath with @xmath is in the kernel of @xmath . But this contradicts
Lemma 9.10 . In summary, the vectors @xmath cannot be brought to the
form ( 9.3 ), or in other words, they span @xmath . Obviously, the same
is true for the set @xmath . A more careful analysis of the above
argument leads to even stronger conclusions. Firstly, an assumption that
there exist three vectors @xmath supported on a @xmath dimensional
subspace lead us to a contradiction. Therefore we have the following

###### Lemma 9.13.

Let @xmath be the six product vectors in the kernel of a non-separable
PPT state of rank @xmath in the @xmath case. For any triple @xmath , at
least one of the sets of vectors @xmath or @xmath spans @xmath .

Moreover, we only needed four product vectors with @xmath ’s as in ( 9.3
) to arrive at a contradiction with Lemma 9.10 . As a consequence, we
have

###### Lemma 9.14.

For any quadruple @xmath , both the sets of vectors @xmath and @xmath
span @xmath .

As an immediate consequence of Lemma 9.13 , there exists a set of three
linearly independent vectors in @xmath . With no loss of generality, we
may assume that @xmath is a linearly independent set. After a @xmath
transformation, @xmath , @xmath and @xmath . There are in principle two
possibilities concerning the remaining vectors @xmath , @xmath and
@xmath . Either one of them is of the form @xmath with @xmath , or all
of them have exactly one coordinate equal to zero. Two vanishing
coordinates in a single vector cannot occur because there is no pair of
identical vectors among @xmath . Moreover, according to Lemma 9.14 , the
zeros must occur in different places in @xmath , @xmath and @xmath . Up
to @xmath transformations and permuting the vectors, we may assume that
@xmath , @xmath , @xmath with @xmath , @xmath , @xmath all different
from @xmath . But then, write the coordinate matrix for @xmath ,

  -- -------- -- -------
     @xmath      (9.4)
  -- -------- -- -------

It is easy to check that all the @xmath minors in ( 9.4 ) are
non-vanishing. In other words, any triple of vectors in @xmath spans
@xmath . The corresponding vectors @xmath , @xmath , @xmath and @xmath
may or may not have all triples linearly independent. It is not
difficult to show that if all the triples span @xmath , we can
simultaneously, by using a @xmath transformation, bring @xmath and
@xmath to the form

  -- -- -- -------
           (9.5)
  -- -- -- -------

By adding a fifth product vector, say @xmath , we get, up to local
transformation and relabelling the vectors @xmath ,

  -- -------- -- -------
     @xmath      (9.6)
  -- -------- -- -------

where @xmath are some complex numbers. We should remark that the
possibility to have @xmath in the first coordinate of @xmath and @xmath
follows because there must exist @xmath such that @xmath , where @xmath
and @xmath denote the @xmath -th coordinate of @xmath and @xmath ,
respectively. Otherwise, @xmath or @xmath would have to be proportional
to @xmath for some @xmath .

If not all triples in @xmath are linearly independent, it is still
possible, according to Lemma 9.14 , to find a linearly independent
triple among them. Without loss of generality, we may assume that the
triple is @xmath . By an identical argument as for the @xmath ’s, we
know that there is a vector @xmath , @xmath such that @xmath have all
triples linearly independent. Without loss of generality, we may assume
that @xmath . This time, a local transformation and possible relabelling
brings the product vectors @xmath with @xmath to the form

  -- -------- -- -------
     @xmath      (9.7)
  -- -------- -- -------

To make a final touch to this section, we need to show that product
vectors of the form ( 9.6 ) or ( 9.7 ) are linearly independent if no
two of them coincide, and thus they span the five-dimensional kernel of
@xmath . We will also show that they constitute a minimal gUPB, and that
the parameters @xmath have to be real when the vectors are in the kernel
of a PPT state.

Let us use @xmath to denote vectors @xmath in @xmath . In the case ( 9.6
), we have

  -- -------- -- -------
     @xmath      (9.8)
  -- -------- -- -------

In the case ( 9.7 ), the coordinates of the product vectors are the
following,

  -- -------- -- -------
     @xmath      (9.9)
  -- -------- -- -------

It is an elementary exercise to check that the matrices on the
right-hand side of ( 9.8 ) and ( 9.9 ) are of rank @xmath for all
choices of @xmath , with the only exception of @xmath . But the last
possibility is excluded because it implies @xmath .

Next, we can show that the vectors @xmath with @xmath , chosen as above,
constitute a general Unextendible Product Basis. In order to prove it,
let us first show that the rank of @xmath has to be @xmath .

###### Proposition 9.15.

Let @xmath be a non-separable PPT state of rank @xmath acting on @xmath
. The rank of the partially transposed state @xmath is also @xmath .

###### Proof.

If @xmath is non-separable, we know by the above argument that the
product vectors @xmath in the kernel of @xmath span a five-dimensional
subspace, which is the kernel itself. Moreover, five of them are, up to
local transformations, of the form ( 9.6 ) or ( 9.7 ). But this implies
that the corresponding product vectors in the kernel of @xmath , which
are @xmath according to Lemma 9.7 , can also be brought to the form (
9.6 ) or ( 9.7 ). To be more explicit, if a local transformation @xmath
brings the vectors @xmath with @xmath to the form ( 9.6 ) or ( 9.7 ),
@xmath does the same to the partial conjugations @xmath . The only
difference is that @xmath and @xmath change into @xmath and @xmath in (
9.6 ) or ( 9.7 ). But this does not change the conclusion about the
dimensionality of the subspace spanned by vectors of the form ( 9.6 ) or
( 9.7 ). As a consequence, the product vectors in the kernel of @xmath
span at least a five-dimensional subspace. Thus the kernel of @xmath is
at least five-dimensional. If it had higher dimension, the rank of
@xmath would be lower or equal @xmath , which is, according to [
HLVC2000 ] , impossible for non-separable @xmath . Therefore, the
dimension of the kernel equals @xmath , and the rank of @xmath is @xmath
. ∎

There exist separable states @xmath of rank @xmath in @xmath systems
that have the rank of @xmath different from @xmath . However, our next
proposition shows that if @xmath is supported on @xmath and it cannot be
written as @xmath with @xmath and @xmath supported on a @xmath subspace,
the rank of @xmath is also @xmath (cf. Figure 4 in [ LS2010n ] , which
we reproduce here as Table 9.1 ).

###### Proposition 9.16.

Let @xmath be a separable state of rank @xmath supported on @xmath ,
which cannot be written as @xmath with @xmath and @xmath supported on a
@xmath subspace of @xmath . The rank of @xmath is also @xmath .

###### Proof.

First, we should remark that @xmath . This fact will be important for
some parts of the proof, although never explicitly referred to. The main
idea that we are going to use is that the argument preceding formulas (
9.6 ) and ( 9.7 ) works for separable states as well, provided that they
cannot be reduced according to Lemma 9.10 . In other words, the argument
works when the kernel of a PPT state in question does intersect the
Segre variety in a transverse way, irrespectively of the state being
entangled or not. Thus, if a reduction according to Lemma 9.10 is not
possible for a separable state @xmath , we have vectors of the form (
9.6 ) or ( 9.7 ) in @xmath , and they span a five-dimensional space.
This is also the dimensionality of the subspace spanned by their partial
conjugates, which are in @xmath . Therefore, the rank of @xmath is not
bigger than @xmath . If it was less than four, the intersection between
@xmath and the Segre variety @xmath would be more than zero-dimensional,
according to the Projective Dimension Theorem [ Hartshorne , Theorem
7.2] . But this contradicts the fact that there are only a finite number
of product vectors in @xmath (equal to @xmath for all @xmath ). In
summary, the rank of @xmath has to be @xmath when @xmath intersects the
Segre variety transversely. If not, we know from Lemmas and 9.10 and
9.11 that there are two options:

1.  it is possible to write @xmath as @xmath , where @xmath and @xmath
    is a rank @xmath PPT state supported on a @xmath or smaller subspace
    of @xmath , with @xmath and @xmath ,

2.  @xmath is supported on a @xmath or smaller subspace itself.

Option ii) is excluded because of the assumption of @xmath supported on
@xmath . Our aim in the following will be to show that @xmath unless
@xmath is supported on a @xmath subspace, which is precisely the second
possibility we allow in the proposition. First, observe that if @xmath
is supported on a @xmath subspace, we can use an analogue of Lemma 9.10
. Either we have @xmath where @xmath and @xmath is supported on a @xmath
, @xmath or @xmath subspace, @xmath and @xmath , or @xmath intersects
the respective Segre variety @xmath transversely. In the latter case, by
Bezout’s Theorem the @xmath -dimensional kernel of @xmath has precisely
three product vectors in it. Actually, we can repeat the argument
preceding Lemmas 9.13 and 9.14 to conclude that the product vectors in
@xmath have to be locally equivalent to

  -- -------- -- --------
     @xmath      (9.10)
  -- -------- -- --------

Obviously, these vectors span the kernel. We see that there are, within
the @xmath subspace, only three product vectors in @xmath . They are
locally equivalent to

  -- -------- -- --------
     @xmath      (9.11)
  -- -------- -- --------

Since @xmath is separable and of rank @xmath , it must be locally
equivalent to a convex sum of projections onto the vectors @xmath in (
9.11 ), which implies that @xmath is an analogous sum of projections
onto @xmath . But @xmath if the product vectors are as in ( 9.11 ).
Therefore @xmath , which implies @xmath , as expected. This proves our
assertion for @xmath supported on a @xmath subspace with @xmath that
intersects the corresponding Segre variety @xmath transversely. For the
other nontrivial cases, we can have @xmath separable and of rank @xmath
, supported on a @xmath subspace. There is also the trivial case of
@xmath supported on a @xmath or @xmath subspace, in which the equality
@xmath clearly holds, and it implies equality of ranks of @xmath and
@xmath .

In the case of @xmath supported on a @xmath subspace, we can repeat the
argument with transverse intersections. Either @xmath can be reduced
once again, in which case it turns out to be equal to @xmath with @xmath
, @xmath and @xmath not proportional to @xmath , or @xmath must
intersect the respective Segre variety @xmath in a transverse way. The
first possibility clearly gives us @xmath . The latter implies, by
Bezout’s Theorem, that there are exactly two product vectors in @xmath .
Similarly as for ( 9.10 ), we can prove that the two product vectors
must be locally equivalent to @xmath and @xmath . Clearly, they span the
kernel of @xmath and there are only two product vectors, locally
equivalent to @xmath and @xmath , in @xmath . But @xmath is separable
and of rank @xmath . Therefore it must be locally equivalent to a convex
sum of projections onto these two vectors. Accordingly, @xmath is
locally equivalent to a sum of two projections onto product vectors,
which are @xmath and @xmath , actually equal to @xmath and @xmath . This
implies @xmath and the equality between the ranks of @xmath and @xmath
follows. ∎

###### Remark 9.17.

The two propositions above explain why PPT states of ranks @xmath ,
@xmath should not be expected to appear in the upper part of Table II in
[ LS2010n ] , which we reproduced above as Table 9.1 . They do exist,
but they are always separable and of a rather special form.

It is useful to formulate the following

###### Corollary 9.18.

All rank @xmath non-separable PPT states @xmath in @xmath systems are
edge states.

###### Proof.

If some non-separable @xmath of rank @xmath had a product vector @xmath
in its range, and the partial conjugated vector @xmath was in the range
of @xmath , we could diminish the rank of @xmath or @xmath by
subtracting @xmath , where

  -- -------- -- --------
     @xmath      (9.12)
  -- -------- -- --------

cf. [ LKHC2001 ] . In such case, @xmath could be written as @xmath with
@xmath PPT and of rank @xmath or with @xmath of rank @xmath . But this
implies, by [ HLVC2000 ] , that @xmath would have to be separable. This
further implies separability of @xmath , which is a contradiction. ∎

At this point, we can easily prove that the vectors @xmath in the kernel
of a non-separable @xmath of rank @xmath , chosen as in ( 9.6 ) or ( 9.7
), constitute a generalized Unextendible Product Basis. If there was a
product vector @xmath orthogonal to all of them, it would be an element
of the range of @xmath . From the proof of Proposition 9.15 we know that
the partially conjugated vectors @xmath span the kernel of @xmath .
Since @xmath for all @xmath , we see that @xmath is in the range of
@xmath , @xmath . Therefore we have a product vector @xmath in the range
of @xmath such that its partial conjugation is in the range @xmath . In
other words, @xmath is not an edge state. But this contradicts Corollary
9.18 and therefore cannot happen. In this way, we have proved the
following.

###### Proposition 9.19.

Let @xmath be a rank @xmath non-separable PPT state in a @xmath system.
The six vectors in the kernel of @xmath constitute a generalized UPB.
There is a subset of five of them that constitutes a minimal gUPB in the
sense of Proposition 9.4 .

###### Proof.

Most of the proof has already been provided above. We only need to
comment on the fact that five of the product vectors constitute a
minimal gUPB. It must be so because the five vectors we brought to the
form ( 9.6 ) or ( 9.7 ) span the kernel of @xmath , and the orthogonal
complement to the kernel has no product vector in it. Thus, the five
vectors are a gUPB of @xmath , which is minimal according to Proposition
9.4 , because @xmath for @xmath . ∎

By Proposition 9.4 we know that a minimal gUPB @xmath has the property
that all triples in @xmath and in @xmath are linearly independent. In
such case, the forms ( 9.6 ) and ( 9.7 ) are locally equivalent, and we
may choose to work with only one of them. In the sequel, we prefer to
assume the form ( 9.6 ) of the product vectors, which is in agreement
with the convention used in [ HHMS2011 ] . Our next step is to prove
that the parameters @xmath , @xmath , @xmath and @xmath in ( 9.6 ) must
be real if the corresponding product vectors belong to the kernel of a
rank @xmath PPT state in the @xmath case. This is not of much use here,
but will prove to be important in Section 9.7 .

We know from Lemma 9.12 that there are exactly six product vectors in
the kernel of @xmath , while we have only five of them in ( 9.6 ), and
we know that they span the kernel. Consequently, the sixth vector is a
linear combination of the other five ones,

  -- -------- -- --------
     @xmath      (9.13)
  -- -------- -- --------

Note that explicit formulas for the sixth vector can be found in [
HHMS2011 , Section 5.2] . Interestingly, since @xmath , we know from
Lemma 9.8 that @xmath is in the kernel of @xmath . However, the vectors
@xmath with @xmath are also there and moreover, since they are, up to
local equivalence, of the form ( 9.6 ) with @xmath and @xmath complex
conjugated, we already know that they span @xmath . Thus the sixth
partially conjugated vector must be a linear combination of the former
five,

  -- -------- -- --------
     @xmath      (9.14)
  -- -------- -- --------

where the coefficients @xmath are in principle not related to the @xmath
’s in ( 9.13 ). However, we can already see at this point that it may be
very difficult to simultaneously satisfy equations ( 9.13 ) and ( 9.14
), if we do not assume that @xmath for all @xmath . In the latter case,
one can obviously choose @xmath . Our aim in the following will be to
show that @xmath is the only possible choice. By projecting ( 9.13 )
onto the first, the second and the third coordinate in the first
subsystem, we get

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (9.15)
     @xmath   @xmath   @xmath      (9.16)
     @xmath   @xmath   @xmath      (9.17)
  -- -------- -------- -------- -- --------

where @xmath are coordinates of @xmath . Similarly, from ( 9.14 ) we get

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (9.18)
     @xmath   @xmath   @xmath      (9.19)
     @xmath   @xmath   @xmath      (9.20)
  -- -------- -------- -------- -- --------

Let us note that the triples @xmath , @xmath , @xmath all consist of
linearly independent vectors, according to Proposition 9.19 . This
implies that each of the formulas ( 9.15 )–( 9.20 ) gives exactly one
solution for the coefficients @xmath or @xmath which it contains. For
one of the consequences, all the coefficients @xmath must be
non-vanishing. Two of them cannot vanish, because @xmath proportional to
any of @xmath with @xmath would contradict @xmath or Lemma 9.9 . To see
this, let us assume that one of them vanishes, e.g. @xmath . In such
case, equation ( 9.17 ) implies @xmath , where we used the fact that
@xmath . Hence ( 9.15 ) and ( 9.16 ) reduce to @xmath and @xmath . But
neither of these equalities can hold, since @xmath and @xmath , while
@xmath proportional to @xmath or @xmath contradicts Lemma 9.9 . Thus our
assumption @xmath must have been false. By repeating the same argument
for @xmath and @xmath , we arrive at @xmath . Let us also notice that
necessarily @xmath and @xmath . We cannot have, for example @xmath and
@xmath since the only vector in the intersection of @xmath and @xmath is
@xmath , and we know that @xmath by Lemma 9.9 . In a similar way, one
obtains @xmath and @xmath . With such amount of knowledge, we can prove
the expected result.

###### Proposition 9.20.

Let @xmath for @xmath be product vectors of the form ( 9.6 ) in the
kernel of a non-separable PPT state of rank four, acting on @xmath . The
parameters @xmath , @xmath , @xmath and @xmath must necessarily be real.

###### Proof.

By dividing ( 9.15 ) by @xmath and ( 9.18 ) by @xmath , which is
possible according to @xmath , we get

  -- -------- -- --------
     @xmath      (9.21)
  -- -------- -- --------

Since @xmath is a linearly independent triple, the above equality
implies @xmath , @xmath and @xmath . In a similar way, from ( 9.16 ) and
( 9.19 ) we can get @xmath , @xmath and @xmath , whereas ( 9.17 ) and (
9.20 ) give us @xmath , @xmath and @xmath . From the equalities
involving @xmath and @xmath , we get

  -- -------- -- --------
     @xmath      (9.22)
  -- -------- -- --------

Together with @xmath , the above equations give us @xmath and @xmath .
But

  -- -------- -- --------
     @xmath      (9.23)
  -- -------- -- --------

In a similar way, from @xmath and @xmath we can get @xmath . ∎

#### 9.6 An equivalence between generalized and orthonormal Unextendible
Product Bases

In the following, we discuss item @xmath of the list given in Section
9.3 . Let us start with a set of five vectors in @xmath ,

  -- -- -- --------
           (9.24)
  -- -- -- --------

and assume that any three of them are linearly independent, as in
Proposition 9.19 . For the moment, we do not require the vectors in (
9.24 ) to be equal to @xmath in ( 9.6 ), but our ultimate goal is to
apply the results we are going to obtain to ( 9.6 ). @xmath
transformations of the above set correspond to the multiplication of the
@xmath matrix in ( 9.24 ) from the left by an element of @xmath and to
the multiplication of the columns of ( 9.24 ) by arbitrary non-zero
scalar factors. It is clear that we can transform ( 9.24 ) by a @xmath
transformation to the following form,

  -- -------- -- --------
     @xmath      (9.25)
  -- -------- -- --------

By another @xmath transformation, we get

  -- -- -- --------
           (9.26)
  -- -- -- --------

We should remark that the matrix we multiply with from the left is
well-defined, since @xmath according to the assumption about linear
independence of triples. Let us transform once again, in the following
way,

  -- -- -- --------
           (9.27)
  -- -- -- --------

This is again possible because @xmath according to our assumptions.

In a similar way as before, we see that @xmath and @xmath . If we
multiply the fourth column by @xmath and the fifth by @xmath , the above
transforms to

  -- -------- -- --------
     @xmath      (9.28)
  -- -------- -- --------

where we introduced the notation @xmath , @xmath , @xmath , @xmath ,
@xmath . It is quite straightforward to see that all the coefficients
@xmath have to be different from zero according to the independent
triples assumption.

Now, introduce the following invariants [ LS2010 ] ,

  -- -- -- --------
           (9.29)
  -- -- -- --------

  -- -- -- --------
           (9.30)
  -- -- -- --------

The numbers @xmath , @xmath are indeed invariant. They do not change
under the family of @xmath transformations we were using in the
consecutive steps ( 9.24 )–( 9.28 ). Thus we can substitute

  -- -------- -- --------
     @xmath      (9.31)
  -- -------- -- --------

in the above formulas for @xmath and @xmath . In this way, we can
quickly calculate the values of the invariants,

  -- -------- -- --------
     @xmath      (9.32)
  -- -------- -- --------

Now, impose the conditions @xmath and @xmath . From the first one, we
clearly get @xmath , where @xmath is a positive real number. Thus, we
have the vectors

  -- -------- -- --------
     @xmath      (9.33)
  -- -------- -- --------

Next, let us multiply from the left by a diagonal matrix @xmath , as
well as multiply the second column by @xmath , the fourth by @xmath and
the fifth by @xmath , where @xmath and @xmath stands for the square root
of @xmath with the argument in @xmath . Under such @xmath transformation
the vectors ( 9.33 ) change into

  -- -------- -- --------
     @xmath      (9.34)
  -- -------- -- --------

where @xmath is real and positive, and all the other parameters @xmath
are non-zero. Moreover, the conditon @xmath transforms to

  -- -------- -- --------
     @xmath      (9.35)
  -- -------- -- --------

simply by formula ( 9.32 ) and the invariance of @xmath . The last
equivalence holds by strict positivity of @xmath . In our next step, we
we are going to multiply ( 9.34 ) from the left by a diagonal matrix
@xmath , with @xmath and @xmath , and also multiply the consecutive
columns, beginning with the first, by @xmath , @xmath , @xmath , @xmath
and @xmath , where @xmath . Our aim is to choose the numbers @xmath in
such a way that @xmath transforms to a set of vectors with orthogonality
relations given by a pentagon graph (that is, any two consecutive ones
are orthogonal, and these are the only orthogonality relations). We
would like to have

  -- -------- -- --------
     @xmath      (9.36)
  -- -------- -- --------

where @xmath and @xmath is a positive real number in place of ( 9.34 ).
Let us write the numbers @xmath as @xmath , where @xmath is a positive
real number and @xmath . In order to obtain ( 9.36 ) with @xmath and
@xmath real and positive, certain phase matching conditions have to be
fulfilled. Let us consider them first. If @xmath , @xmath , @xmath are
such that @xmath , @xmath and @xmath with @xmath , @xmath and @xmath
real and positive, complex phases match correctly if and only if the
following set of equations hold

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (9.37)
     @xmath   @xmath   @xmath      (9.38)
     @xmath   @xmath   @xmath      (9.39)
     @xmath   @xmath   @xmath      (9.40)
     @xmath   @xmath   @xmath      (9.41)
     @xmath   @xmath   @xmath      (9.42)
     @xmath   @xmath   @xmath      (9.43)
  -- -------- -------- -------- -- --------

The requirement that @xmath adds a condition @xmath to equations ( 9.37
)–( 9.38 ). However, a substitution of the form

  -- -------- --
     @xmath   
  -- -------- --

with an appropriately chosen @xmath can always bring @xmath to zero and
it has no effect on ( 9.37 )–( 9.43 ). Therefore, as long as existence
of solutions is in question, we may neglect the additional condition. It
is easy to see that the relations ( 9.37 )–( 9.40 ) are fulfilled if and
only if @xmath for some @xmath . Thus the set of equations ( 9.37 )–(
9.43 ) are reduced to

  -- -------- -- --------
     @xmath      (9.44)
  -- -------- -- --------

Interestingly, the @xmath matrix in equation ( 9.44 ) has rank @xmath .
A solution @xmath exists if and only if

  -- -------- -- --------
     @xmath      (9.45)
  -- -------- -- --------

But this is exactly the positivity condition ( 9.35 ) for the invariant
@xmath . Thus, if @xmath in addition to @xmath , we can cancel the
complex phases, as in ( 9.36 ). The only remaining thing to do is to
match the modules, which gives us the following set of equations,

  -- -------- -- --------
     @xmath      (9.46)
  -- -------- -- --------

There is also an equation @xmath , following from the requirement that
@xmath . As we see, there are five equations in ( 9.46 ), and the
variables @xmath are six in number. Therefore, one can expect a solution
to exist. It can easily be checked that the following, with @xmath , is
a one-parameter family of solutions,

  -- -------- -- --------
     @xmath      (9.47)
  -- -------- -- --------

By choosing @xmath we can satisfy the additional condition @xmath . Thus
we have proved that the positivity of the invariants @xmath , @xmath
guarantees that the family of five vectors ( 9.24 ) can be transformed
by a @xmath transformation, without permuting them, to the form ( 9.36
). Obviously, a converse statement is also true, since the values of
@xmath and @xmath calculated from ( 9.36 ) are @xmath and @xmath ,
respectively. In this way we arrive at the following

###### Proposition 9.21.

A set of five vectors @xmath with the property that any triple of them
is linearly independent, can be transformed by a @xmath transformation,
without permuting them, to the form ( 9.36 ) with @xmath and @xmath real
and positive, if and only if the invariants @xmath and @xmath , defined
in ( 9.29 ), are positive.

Let us note that any set of five vectors @xmath with orthogonality
relations @xmath can be transformed by @xmath transformations to the
form ( 9.36 ). A simple argument shows that they can be transformed to

  -- -------- -- --------
     @xmath      (9.48)
  -- -------- -- --------

with @xmath and @xmath complex. But since @xmath and @xmath in the above
case, the argument following equation ( 9.34 ) tells us that a @xmath
transformation brings ( 9.48 ) to the form ( 9.36 ). As a consequence,
Proposition 9.21 is a necessary and sufficient criterion for a set of
five vectors @xmath to be @xmath equivalent, without permuting them, to
a set of vectors @xmath with orthogonality relations @xmath .

From [ DiVicenzo04 ] we know that orthogonal UPBs in the @xmath case
always have five elements, and they are, up to permutations, precisely
the sets of product vectors @xmath with orthogonality relations @xmath
and @xmath . Consider the question, whether an arbitrary set of five
vectors @xmath with linearly independent triples can be brought by
@xmath transformations to such @xmath , without permuting the vectors.
In other words, what are the necessary and sufficient conditions for
@xmath ’s to be convertible into @xmath ’s with the orthogonality
conditions given above. By using Proposition 9.21 , we can already deal
with the question about @xmath ’s being convertible into @xmath ’s.
Namely, an @xmath transformation on the first subsystem can bring the
vectors @xmath , without permuting them, to @xmath with @xmath if and
only if the corresponding values of the invariants @xmath and @xmath are
positive. We are only missing a similar criterion for @xmath ’s and
@xmath ’s. However, it is not difficult to check that a permutation
@xmath brings any @xmath with @xmath to @xmath with @xmath . Therefore,
it is sufficient to calculate the invariants ( 9.29 ) and ( 9.30 )
corresponding to the permuted vectors @xmath and check their positivity
in order to tell whether the vectors @xmath are convertible into some
@xmath with the desired orthogonality relations. Following the
definitions ( 9.29 ) and ( 9.30 ), let us introduce additional
invariants

  -- -- -- --------
           (9.49)
  -- -- -- --------

and

  -- -- -- --------
           (9.50)
  -- -- -- --------

in accordance with [ LS2010 ] . From the discussion above it follows
that arbitrary five vectors @xmath in @xmath can be transformed, without
permuting them, to @xmath with orthogonality relations @xmath if and
only if the above invariants @xmath and @xmath are positive. Together
with the previously obtained convertibility result between @xmath and
@xmath , the last result gives us the following.

###### Proposition 9.22.

A set of product vectors @xmath can be transformed by a @xmath
transformation to an orthogonal UPB @xmath with orthogonality relations
@xmath and @xmath , without permuting the @xmath ’s, if and only if the
invariants @xmath , @xmath , @xmath and @xmath , defined in ( 9.29 ), (
9.30 ), ( 9.49 ) and ( 9.50 ), are positive.

###### Proof.

Most of the proof has already been included above. Let @xmath denote an
orthogonal UPB with the orthogonality relations @xmath and @xmath for
all @xmath . The possibility to convert

  -- -------- -- --------
     @xmath      (9.51)
  -- -------- -- --------

by @xmath transformations, or by local equivalence in our usual terms,
is the same as the possibility to separately convert @xmath into @xmath
and @xmath into @xmath by some @xmath transformations. However, we know
that the first conversion is possible if and only if @xmath and @xmath
are positive, while the second needs positivity of @xmath and @xmath .
Altogether, positivity of all the invariants @xmath , @xmath is a
necessary and sufficient criterion for the transformation ( 9.51 ) to be
possible. ∎

In the context of product vectors in the kernel of a PPT state, as well
as elements of an orthogonal UPB, permutations are obviously possible.
Therefore we would like to have a version of Proposition 9.22 with no
restriction on the ordering of the vectors @xmath .

###### Proposition 9.23.

A set of product vectors @xmath can be transformed by a @xmath
transformation to an orthogonal UPB, if and only if for some permutation
@xmath the invariants @xmath , @xmath , @xmath and @xmath , calculated
with the permuted vectors @xmath and @xmath substituted for @xmath and
@xmath , respectively, are all positive.

###### Proof.

Immediate given the fact [ DiVicenzo04 ] that an orthogonal UPB in a
@xmath system can always be brought by a permutation to a @xmath with
the orthogonality relations as in Proposition 9.22 . ∎

Let us also note that, in accordance with [ HHMS2011 ] , not every
single permutation of the five product vectors needs to be considered if
we want to check whether they can be transformed into an orthogonal UPB
or not.

###### Remark 9.24.

Only @xmath permutations, given in Table 9.2 , have to be checked in
order to obtain a decisive answer to the question raised in Proposition
9.23 .

###### Proof.

An explanation is included in [ LS2010 ] and [ HHMS2011 ] , but we
repeat it quickly here for completeness. Let us denote by @xmath the
symmetric group of @xmath . The permutations given in Table 9.2 are
representatives of equivalence classes in @xmath of the regular pentagon
subgroup @xmath , generated by the cycle @xmath and the inversion @xmath
. The regular pentagon symmetry subgroup has the expected property that
it does not change signs of @xmath , @xmath , @xmath and @xmath , just
as it does not change orthogonality relations between the vectors @xmath
and @xmath . Therefore, we may divide @xmath by @xmath when we check
positivity of the invariants in Proposition 9.23 . The number of
invariance classes is @xmath because @xmath and @xmath . ∎

#### 9.7 Determination of a PPT state by product vectors in its kernel

In the last part of the proof of our main result, concerning PPT states
of rank four in two qutrit systems, we recall a number of surprising
facts that were earlier reported in [ HHMS2011 , Section 5] without a
complete explanation. Here we fill in that little gap, and we collect a
sufficient amount of information to quickly explain the findings of
Leinaas et al. , concerning the relation of extreme PPT states to
Unextendible Product Bases [ LS2010 ] .

Note that, given a set of product vectors in @xmath , the conditions in
Lemma 9.8 are a set of linear equations for @xmath . An idea, earlier
presented in [ HHMS2011 ] , is to try to solve these equations assuming
a specific form of the product vectors, namely ( 9.6 ). Let us repeat
formula ( 9.6 ) here for the convenience of the reader.

  -- -------- -- --------
     @xmath      (9.52)
  -- -------- -- --------

We actually know from Proposition 9.19 that there always exists a local
@xmath transformation @xmath that brings five vectors in the kernel of a
non-separable PPT state of rank @xmath , possibly multiplied by some
scalar factors, into the form ( 9.52 ) with all triples linearly
independent . Moreover, Proposition 9.20 tells us that the parameters
@xmath , @xmath , @xmath and @xmath are necessarily real numbers. By
solving the linear conditions on a PPT state following from Lemma 9.8
with @xmath , @xmath as in ( 9.52 ) substituted for @xmath , we will
actually be solving a set of constraints on @xmath . However, according
to the discussion in Section 9.2 , such local transformations are
irrelevant to all the questions considered in this paper. Therefore we
may simply assume that a PPT state @xmath in question has the product
vectors ( 9.52 ) in its kernel and check the consequences. As previously
reported by the authors of [ HHMS2011 ] , the conditions @xmath for
@xmath together with @xmath and @xmath imply the following form of
@xmath ,

  -- -------- -- --------
     @xmath      (9.53)
  -- -------- -- --------

with @xmath and @xmath real for all @xmath and such that

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (9.54)
     @xmath   @xmath   @xmath      (9.55)
              @xmath               (9.56)
  -- -------- -------- -------- -- --------

Derivation of the equations ( 9.53 ) and ( 9.54 )–( 9.56 ) is left as a
simple exercise for the reader. It may be useful to consult Section 5.4
of [ HHMS2011 ] in order to solve it.

We still have not used the condition @xmath , which gives us additional
six linear equations on @xmath and @xmath ,

  -- -------- -------- -- -- --------
     @xmath   @xmath         (9.57)
     @xmath   @xmath         (9.58)
     @xmath   @xmath         (9.59)
  -- -------- -------- -- -- --------

Under the assumption of @xmath of the form ( 9.52 ) being a gUPB, there
exists, up to scaling by arbitrary real factors, exactly one solution to
the equations ( 9.54 )–( 9.59 ). We know from Proposition 9.19 that the
assumption is true for vectors @xmath in the kernel of a non-separable
rank @xmath PPT state in @xmath systems. It is most important for us
that there exist, up to scaling by arbitrary positive factors, exactly
two solutions

  -- -- -- --------
           (9.60)
  -- -- -- --------

The above matrix is well-defined since all the numbers @xmath , @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath , @xmath , @xmath and @xmath
are nonzero as a consequence of all triples of vectors in ( 9.52 ) being
linearly independent.

Note that, for both choices of sign, ( 9.60 ) is a symmetric matrix.
Moreover, it is symmetric with respect to the partial transpose .
Therefore @xmath is PPT iff it is positive definite. A necessary
condition for ( 9.60 ) to be positive definite is that all the nonzero
elements on its diagonal, as well as all nontrivial @xmath minors of the
form @xmath are positive. Altogether, we have six nonzero elements on
the diagonal

  -- -------- -- --------
     @xmath      (9.61)
  -- -------- -- --------

and six nontrivial minors

  -- -------- -- --------
     @xmath      (9.62)
  -- -------- -- --------

The @xmath sign in ( 9.61 ) corresponds to the choice we make in ( 9.60
). We see that all the expressions in ( 9.62 ) and ( 9.61 ) are
quotients and products of the following nineteen numbers

  -- -------- -- --------
     @xmath      (9.63)
     @xmath      (9.64)
  -- -------- -- --------

Concerning the list ( 9.63 ), we already know that all its elements have
to be nonzero. This follows from the condition of @xmath being a gUPB.
It turns out that the same holds for the elements of ( 9.64 ). The
number @xmath must be nonzero, because otherwise the vector

  -- -------- -- --------
     @xmath      (9.65)
  -- -------- -- --------

would be of the form @xmath , thus contradicting Lemma 9.10 and
Corollary 9.18 . In a similar way, one can show that @xmath and @xmath .
Let us now assume that @xmath . In such case, we have the following
submatrix in ( 9.60 )

  -- -------- -- --------
     @xmath      (9.66)
  -- -------- -- --------

In order for ( 9.66 ) to be positive definite for some choice of the
sign @xmath , we need to have @xmath , which we know is impossible. Thus
we have proved that @xmath for @xmath positive definite. Finally, the
fact that @xmath , @xmath and @xmath must also be nonvanishing for
@xmath positive definite follows by a suitable modification of the above
argument. Different submatrices need to be chosen, but otherwise the
proof is identical.

Our task in the following will be to relate positivity of all the
numbers in ( 9.61 ) and ( 9.62 ) to the fact that all the invariants
@xmath , given in Section 9.6 , are positive, possibly after we suitably
permute the vectors @xmath . Note that we already know that only the
@xmath permutations listed in Table 9.2 need to be considered. An
explanation is included in the proof related to Remark 9.24 . Not to
much surprise, the formulas for the invariants @xmath for permuted
vectors of the form ( 9.52 ) are always expressed as products and
quotients including only the numbers listed in ( 9.63 ). Explicit
formulas can be found in Table 9.3 . To explain the notation we used in
the table, it is sufficient to say, for example, that by using @xmath
from Table 9.2 to permute the product vectors ( 9.52 ), we obtain @xmath
, @xmath , @xmath and @xmath as the expressions for the invariants.

It turns out that the values of @xmath corresponding to one of the
permutations @xmath have to be all positive to assure that @xmath ,
given in ( 9.60 ), is a positive matrix for some choice of the sign
@xmath . Our computer-aided proof of this fact consisted in simply
checking all admissible sign choices for the numbers listed in ( 9.63 )
and ( 9.64 ). We already know that neither of those numbers can be zero,
and thus it seems that we have @xmath cases to check. However, some
further constraints apply, which reduce this number considerably. First
of all, the requirement that @xmath of the list ( 9.61 ) and a very
similar element @xmath of ( 9.62 ) have the same sign implies that
@xmath , with the @xmath sign depending on the choice we made in ( 9.60
). Along the same lines, by comparing the last element of ( 9.61 ) with
the second element of ( 9.62 ), one can prove that @xmath . More
importantly, the signs of the numbers listed in ( 9.63 ) and ( 9.64 )
are not all independent. Various relations have to hold between them.
For example, @xmath clearly implies @xmath , and we cannot have a plus
sign for @xmath and a minus sign for @xmath . More sophisticated
relations like

  -- -------- -- --------
     @xmath      (9.67)
  -- -------- -- --------

have to hold as well. Alternatively, the above formula can be written as

  -- -------- -- --------
     @xmath      (9.68)
  -- -------- -- --------

We provide a more or less exhaustive list, consisting of 76 elements, in
Tables 9.4 and 9.5 on pages 9.4 and 9.5 . For example, the relation 9.68
corresponds to the following row in Table 9.4 ,

  -- -- --
        
  -- -- --

which should explain the notation we used ¹ ¹ 1 To better explain the
symbols in the header of Tables 9.4 , 9.5 and 9.6 , let us add that
@xmath , @xmath , @xmath and @xmath denote @xmath , @xmath , @xmath and
@xmath , respectively, while @xmath , @xmath , @xmath , @xmath , @xmath
, @xmath , @xmath , @xmath , @xmath and @xmath stand for @xmath , @xmath
, @xmath , @xmath , @xmath , @xmath , @xmath , @xmath and @xmath ,
respectively. . While some further relations could still possibly exist,
the use of those listed in the appendix allowed us to confirm the
necessity result mentioned above. When all the constraints are imposed,
a comparably small number of @xmath or @xmath out of the @xmath sign
choices remain possible when “ @xmath ” or “ @xmath ” is fixed in ( 9.60
), respectively. It then turns out that, by choosing an admissible sign
configuration, all the numbers in the lists ( 9.61 ) and ( 9.62 ) can be
made positive only if one of the quadruples listed in Table 9.3 consists
solely of positive numbers. This is in full agreement with, and provides
a rigorous, although not very insightful proof of the results reported
in Section 5 of [ HHMS2011 ] . Actually, it turns out that there are
precisely @xmath admissible sign configurations that correspond to a
positive @xmath for some choice of the sign @xmath in ( 9.60 ) and each
of the quadruples in Table 9.3 is positive precisely for one of them. A
complete list of the selected sign choices and the corresponding
permutations is given in Table 9.6 . Interestingly, @xmath of them
correspond to choosing the plus sign in ( 9.60 ), while only @xmath to
the minus sign. This is rather an uneven partitioning of the total of
@xmath configurations, which is somewhat puzzling.

To summarize, the computer-aided proof we carried out allows us to state
the following.

###### Proposition 9.25.

A necessary and sufficient criterion for a generalized Unextendible
Product Basis @xmath to belong to the kernel of a rank @xmath PPT state
@xmath is that there exists a permutation of the vectors @xmath that it
yields all the values of the invariants @xmath , @xmath , @xmath and
@xmath , defined as in equations ( 9.29 ), ( 9.30 ), ( 9.49 ) and ( 9.50
), positive. When checking positivity of @xmath , it is possible to
consider only the @xmath permutations, listed in Table 9.2 , and the
corresponding expressions for the invariants, given in Table 9.3 .

###### Proof.

First of all, let us note that a separable state @xmath cannot have a
gUPB in its kernel, since it must have a product state in its range.
Thus in the following we may always assume that @xmath is entangled. Let
us prove sufficiency first. If the invariants are positive for the
permuted vectors @xmath , we know from Proposition 9.23 that there
exists a @xmath transformation @xmath such that the transformed vectors
@xmath are elements of an orthogonal UPB @xmath . With no loss of
generality, we may assume that the vectors @xmath are normalized to
unity. In such case the projection

  -- -------- -- --------
     @xmath      (9.69)
  -- -------- -- --------

has all the vectors @xmath in its kernel and it is a PPT entangled state
[ Bennett99 ] . The locally transformed PPT state @xmath has all the
vectors @xmath in its kernel.

In order to prove necessity, note that from the discussion above we know
that positivity of @xmath , possibly after a permutation, is a necessary
condition for a PPT entangled state @xmath with vectors @xmath in its
kernel to exist, provided that the vectors are as in equation ( 9.52 ).
But any gUPB @xmath can be brought to the form ( 9.52 ) by a local
transform, say @xmath . If we assume that a PPT state @xmath has @xmath
in its kernel, then the locally transformed @xmath has @xmath in its
kernel. But @xmath are of the form ( 9.52 ). From the above discussion,
@xmath is PPT if and only if the invariants @xmath are positive,
possibly after we permute the vectors @xmath . But @xmath does not
change the value of the invariants, and thus @xmath , permuted in the
same way as the @xmath , must also have all of them positive. ∎

Let us also state the following result, which should be expected from
the discussion above.

###### Proposition 9.26.

Let @xmath be a gUPB that yields, after a suitable permutation of the
product vectors, positive values of all the invariants @xmath . The PPT
state @xmath with @xmath in its kernel is uniquely determined, up to
scaling by a constant positive factor.

###### Proof.

We already know that the assertion of the proposition holds for gUPBs of
the form ( 9.52 ). We also know that any gUPB @xmath can be locally
transformed so that it looks like in ( 9.52 ). Let us denote the
transformation which does it by @xmath . There cannot exist two PPT
states @xmath and @xmath with @xmath in their kernels, because in such
case the PPT states @xmath and @xmath would both have the same gUPB of
the form ( 9.52 ) in their kernel, which we know is not possible. ∎

#### 9.8 The main result

Using the knowledge from the previous sections, we can now easily prove
our main result.

###### Theorem 9.27.

Positive-partial-transpose states of rank @xmath in @xmath systems are
either separable or they are of the form

  -- -------- -- --------
     @xmath      (9.70)
  -- -------- -- --------

with @xmath and @xmath an orthonormal Unextendible Product Basis. In the
latter case, they are entangled, and extreme in the set of PPT states.
The rank of the partial transpose of the state is @xmath in case of
nonseparable states.

###### Proof.

In case of separable states, there is nothing to prove. Let @xmath be a
non-separable PPT state of rank @xmath in a @xmath system. We know from
Proposition 9.19 that there is a generalized UPB, say @xmath , in the
kernel of @xmath . From Proposition 9.25 we know that the corresponding
values of the invariants @xmath must be all positive after we suitably
permute the vectors @xmath . Next, Proposition 9.22 tells us that there
exists a @xmath transformation @xmath that brings @xmath to an
orthogonal UPB @xmath . With no loss of generality, we may assume that
the vectors @xmath are normalized. From Proposition 9.26 we know that
there exists, up to scaling, exactly one PPT state which has @xmath in
its kernel. It must be @xmath . The state given by the formula ( 9.70 )
clearly is PPT, and it has all the vectors @xmath in its kernel. By
using Proposition 9.26 again, we see that it must be equal to the @xmath
we started with. The fact that the rank of the partial transpose is
@xmath for non-separable states, is simply the assertion of Proposition
9.15 . ∎

In this way, we have obtained a full characterization of bound entangled
states of minimal rank. Let us also mention a special property they
have, which can be loosely described as saying that it is not enough for
an entanglement witness to be indecomposable in order to detect them.

###### Remark 9.28.

According to [ SBL2001 , Lemma 3] , all PPT states of rank @xmath in
@xmath systems can be written as a sum of four projections onto vectors
of Schmidt rank @xmath . By Theorem 9.27 , or Proposition 9.15 , their
partial transposes are also of rank @xmath and thus can be decomposed in
an analogous way. Using the notation of [ ref.SSZ09 ] , we can write
that all such PPT states are elements of the cone @xmath . The dual cone
@xmath consists of Jamiołkowski-Choi transforms of convex sums of @xmath
-positive and @xmath -co-positive maps. Consequently, any entanglement
witness that detects a PPT state of rank @xmath in a @xmath system is
atomic [ Ha98 ] . This applies in particular to the witness discussed in
Example 1 of [ Terhal2001 ] and the Choi map, in relation to the PPT
state discussed in Section 4 of [ HaKyePark2003 ] .

### Conclusion

Computational advances in the field of algebraic geometry have not
become well-known among the quantum information community, despite a
number of problems that are, at the very bottom, systems of polynomial
equations. In the present thesis, I tried to outline a few possible
applications of Groebner basis methods in quantum information and
quantum entanglement science, including:

-   Compression equations for Quantum Error Correction (QEC), Section
    8.1

-   Completely Entangled Subspaces (CES), Section 8.2

-   Maximally entangled states, Section 8.3

-   Mutually Unbiased Bases (MUBs) and Symmetric Informationally
    Complete vectors (SICs), Sections 8.4 and 8.5

The main result, which is a characterization of rank four entangled
states of two qutrits with positive-partial-transpose (PPT), was
presented in Chapter 9 . Its proof uses a tool from algebraic geometry,
but this time it is the theorem of Bezout, a basic result in
intersection theory. In the thesis, I also included a few problems that
I solved during my PhD studies using simple algebra tricks. They can be
found in Chapter 7 . Moreover, I felt it was appropriate to present a
characterization result for certain cones of positive maps, included in
Chapter 6 .

The central idea of the thesis was that the problems solved should be
algebraic in nature. Obviously, I also required them to be of interest
for the quantum information community. I did not presume the readers to
be experts neither in mathematics, nor in foundational or practical
questions relating to quantum mechanics. Hence, I included introduction
to both the mathematical apparatus I used and to certain aspects of
quantum theory. I hope the thesis may contribute to a better
understanding of some tools of algebraic geometry among the quantum
information community and hence lead to their new applications in areas
such as the classification of PPT states or Completely Entangled
Subspaces, solving QEC equations or the investigation of MUBs and SICs,
and hopefully a few more. One of big questions that remains open is how
to understand all the numerical findings on PPT states included in the
work by Leinaas, Myrheim and Sollid [ LS2010n ] . I believe algebraic
geometry, which turned out to be so useful in the three-by-three, rank
four case, could still be used to explain properties observed for higher
rank and/or higher dimensional cases. However, there does not seem to
exist a direct generalization of the results of Chapter 9 to these
cases.

### A list of papers published

The following papers were published by the author as a part of the PhD
project reported in this thesis (inverse chronological order):

1.  Ł. Skowronek, E. Størmer, Choi matrices, norms and entanglement
    associated with positive maps on matrix algebras , J. Func. Analysis
    262 (2012), 639–647

2.  Ł. Skowronek, Three-by-three bound entanglement with general
    unextendible product bases , J. Math. Phys. 52 (2011), 122202

3.  Ł. Skowronek, Cones with a mapping cone symmetry in the
    finite-dimensional case , Lin. Alg. Appl. 435 (2011), 361–370

4.  Z. Puchała, P. Gawron, J. A. Miszczak, Ł. Skowronek, M.-D. Choi, K.
    Życzkowski, Product numerical range in a space with tensor product
    structure , Lin. Alg. Appl. 434 (2011), 327–342

5.  P. Gawron, Z. Puchała, J. A. Miszczak, Ł. Skowronek, K. Życzkowski,
    Restricted numerical range: a versatile tool in the theory of
    quantum information , J. Math. Phys. 51 (2010), 102204

6.  Ł. Skowronek, Dualities and positivity in the study of quantum
    entanglement , Int. J. Quantum Inf. Vol. 8, No. 5 (2010), 721–754

7.  Ł. Skowronek, K. Życzkowski, Positive maps, positive polynomials and
    entanglement witnesses , J. Phys. A: Math. Theor. 42 (2009), 325302

8.  Ł. Skowronek, E. Størmer, K. Życzkowski, Cones of positive maps and
    their duality relations , J. Math. Phys. 50 (2009), 062106

###### Contents

-    Foreword
-    I Basics of quantum entanglement theory
    -    1 Fundamental questions
        -    1.1 Local hidden variables
        -    1.2 Separable states and separability criteria
        -    1.3 Beyond quantum entanglement
    -    2 Practical applications
        -    2.1 Quantum cryptography
        -    2.2 Quantum teleportation and dense coding
        -    2.3 Quantum metrology
    -    3 Distillability and bound entanglement
        -    3.1 Distillation of quantum entanglement
        -    3.2 Examples of bound entangled states
-    II A brief introduction to algebraic geometry
    -    4 Varieties, Ideals and Groebner bases
        -    4.1 Preliminaries
        -    4.2 Monomial orders and Groebner bases
        -    4.3 Elimination ideals
    -    5 A little intersection theory
        -    5.1 Dimension and degree of a variety
        -    5.2 Tangent spaces. Smoothness
        -    5.3 Bezout’s theorem
-    III Results obtained and examples solved
    -    6 A structure theorem for a class of cones of positive maps
    -    7 Algebraic problems solved by hand
        -    7.1 Product numerical range for a three-parameter family of
            operators
        -    7.2 Higher order numerical ranges and code carriers for the
            qutrit case
        -    7.3 A separable state of length four and Schmidt rank three
    -    8 Algebraic problems solved by using Groebner bases
        -    8.1 Compression equations – a special case
        -    8.2 Completely Entangled Subspaces
        -    8.3 Maximally entangled states in linear subspaces
        -    8.4 Mutually Unbiased Bases
        -    8.5 Symmetric Informationally Complete vectors
    -    9 A structure theorem for PPT bound entangled states of lowest
        rank
        -    9.1 General Unextendible Product Bases
        -    9.2 The concept of local equivalence
        -    9.3 Outline of the proof
        -    9.4 Product vectors in the kernel of a PPT state
        -    9.5 Product vectors in the kernel must be a gUPB
        -    9.6 An equivalence between generalized and orthonormal
            Unextendible Product Bases
        -    9.7 Determination of a PPT state by product vectors in its
            kernel
        -    9.8 The main result
    -    Conclusion
    -    A list of papers published