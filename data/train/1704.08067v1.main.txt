## Résumé

Parmi les techniques d’apprentissage automatique, l’apprentissage
supervisé vise à modéliser les relations entrée-sortie d’un système, à
partir d’observations de son fonctionnement. Les arbres de décision
caractérisent cette relation entrée-sortie à partir d’un ensemble
hiérarchique de questions appelées les noeuds tests amenant à une
prédiction, les noeuds feuilles. Plusieurs de ces arbres sont souvent
combinés ensemble afin d’atteindre les performances de l’état de l’art:
les ensembles de forêts aléatoires calculent la moyenne des prédictions
d’arbres de décision randomisés, entraînés indépendamment et en
parallèle alors que les ensembles d’arbres de boosting entraînent des
arbres de décision séquentiellement, améliorant ainsi les prédictions
faites par les précédents modèles de l’ensemble.

L’apparition de nouvelles applications requiert des algorithmes
d’apprentissage supervisé efficaces en terme de puissance de calcul et
d’espace mémoire par rapport au nombre d’entrées, de sorties, et
d’observations sans sacrifier la précision du modèle. Dans cette thèse,
nous avons identifié trois domaines principaux où les méthodes d’arbres
de décision peuvent être améliorées pour lequel nous fournissons et
évaluons des solutions algorithmiques originales: (i) apprentissage sur
des espaces de sortie de haute dimension, (ii) apprentissage avec de
grands ensembles d’échantillons et des contraintes mémoires strictes au
moment de la prédiction et (iii) apprentissage sur des espaces d’entrée
creux de haute dimension.

Une première approche pour résoudre des tâches d’apprentissage avec un
espace de sortie de haute dimension , appelée «binary relevance» ou
«single target», est l’apprentissage d’un ensemble d’arbres de décision
par sortie. Toutefois, cette approche néglige complètement les
corrélations potentiellement existantes entre les sorties. Une approche
alternative, appelée «arbre de décision multi-sorties», est
l’apprentissage d’un seul ensemble d’arbres de décision pour toutes les
sorties, faisant l’hypothèse que toutes les sorties sont corrélées.
Cependant, les deux approches ont (i) exactement la même complexité en
temps de calcul et (ii) visent des structures de corrélation de sorties
extrêmes. Dans notre première contribution, nous montrons comment
combiner des projections aléatoires (une méthode de réduction de
dimensionnalité) de l’espace de sortie avec l’algorithme des forêts
aléatoires diminuant la complexité en temps de calcul de la phase
d’apprentissage. La précision est préservée, et peut même être améliorée
en atteignant un compromis biais-variance différent. Dans notre seconde
contribution, nous adaptons d’abord formellement la méthode d’ensemble
«gradient boosting» à la régression multi-sorties et à la classification
multi-labels. Nous proposons ensuite de combiner une seule projection
aléatoire de l’espace de sortie avec l’algorithme de «gradient boosting»
sur de telles tâches afin de s’adapter automatiquement à la structure
des corrélations existant entre les sorties.

Les algorithmes de forêts aléatoires génèrent souvent de grands
ensembles de modèles complexes grâce à la disponibilité d’un grand
nombre d’observations. Toutefois, la complexité mémoire, proportionnelle
au nombre total de noeuds, de tels modèles est souvent prohibitive, et
donc ces modèles ne sont pas adaptés à des contraintes mémoires fortes
lors de la phase de prédiction . Dans notre troisième contribution, nous
proposons de compresser ces ensembles en résolvant un problème de
régularisation basé sur la norme @xmath sur l’ensemble des fonctions
indicatrices défini par tous leurs noeuds.

Certaines tâches d’apprentissage supervisé ont un espace d’entrée de
haute dimension mais creux , où chaque observation possède seulement
quelques variables d’entrée avec une valeur non-nulle. Les
implémentations standards des arbres de décision ne sont pas adaptées
pour traiter des espaces d’entrée creux, contrairement à d’autres
techniques d’apprentissage supervisé telles que les machines à vecteurs
de support ou les modèles linéaires. Dans notre quatrième contribution,
nous montrons comment exploiter algorithmiquement le creux de l’espace
d’entrée avec les méthodes d’arbres de décision. Notre implémentation
diminue significativement le temps de calcul sur des ensembles de
données synthétiques et réelles, tout en fournissant exactement le même
modèle. Cela permet aussi de réduire la mémoire nécessaire pour
apprendre de tels modèles en exploitant des méthodes de stockage
appropriées pour la matrice des entrées.

\pdfbookmark

[1]Acknowledgmentsacknowledgments

### Acknowledgments

This PhD thesis started with the trust granted by Prof. Louis Wehenkel,
joined soon after by Prof. Pierre Geurts. I would like to express my
sincere gratitude for their continuous encouragements, guidance and
support. I have without doubt benefitted from their motivations,
patience and knowledge. Our insightful discussions and interactions
definitely moved the thesis forward.

I would like to thank the University of Liège, the FRS-FNRS, Belgium,
the EU Network of Excellence PASCAL2, and the IUAP DYSCO, initiated by
the Belgian State, Science Policy Office to have funded this research.
Computational resources have been provided by the Consortium des
Équipements de Calcul Intensif (CÉCI), funded by the Fonds de la
Recherche Scientifique de Belgique (F.R.S.-FNRS) under Grant No.
2.5020.11.

The presented research would not have been the same without my
co-authors (here in alphabetic order): Jean-Michel Begon, Mathieu
Blondel, Lars Buitinck, Pierre Damas, Céline Delierneux, Damien Ernst,
Hedayati Fares, Alexandre Gramfort, Pierre Geurts, André Gothot, Olivier
Grisel, Jaques Grobler, Alexandre Hego, Bryan Holt, Justine Huart,
Vincent François-Lavet, Nathalie Layios, Robert Layton, Christelle
Lecut, Gilles Louppe, Andreas Mueller, Vlad Niculae, Cécile Oury,
Panagiotis Papadimitriou, Fabian Pedregosa, Peter Prettenhofer, Zixiao
Aaron Qiu, François Schnitzler, Antonio Sutera, Jake Vanderplas, Gael
Varoquaux, and Louis Wehenkel.

I would like to thank the members of the jury, who take interests in my
work, and took the time to read this dissertation.

Diane Zander and Sophie Cimino have been of an invaluable help with all
the administrative procedures. I would like to thank them for their
patience and availability. I would also like to thank David Colignon and
Alain Empain for their helpfulness about anything related to
super-computers.

I would like to thank my colleagues from the Montefiore Institute,
Department of Electrical Engineering and Computer Science from the
University of Liège, whom have created a pleasant, rich and stimulating
environment (in alphabetic order): Samir Azrour, Tom Barbette, Julien
Beckers, Jean-Michel Begon, Kyrylo Bessonov, Hamid Soleimani Bidgoli,
Vincent Botta, Kridsadakorn Chaichoompu, Célia Châtel, Julien Confetti,
Mathilde De Becker, Renaud Detry, Damien Ernst, Ramouna Fouladi,
Florence Fonteneau, Raphaël Fonteneau, Vincent François-Lavet, Damien
Gérard, Quentin Gemine, Pierre Geurts, Samuel Hiard, Renaud Hoyoux,
Fabien Heuze, Van Anh Huynh-Thu, Efthymios Karangelos, Philippe Latour,
Gilles Louppe, Francis Maes, Alejandro Marcos Alvarez, Benjamin
Laugraud, Antoine Lejeune, Raphael Liégeois, Quentin Louveaux, Isabelle
Mainz, Raphael Marée, Sébastien Mathieu, Axel Mathei, Romain Mormont,
Frédéric Olivier, Julien Osmalsky, Sébastien Pierard, Zixiao Aaron Qiu,
Loïc Rollus, Marie Schrynemackers, Oliver Stern, Benjamin Stévens,
Antonio Sutera, David Taralla, François Van Lishout, Rémy Vandaele,
Philippe Vanderbemden, and Marie Wehenkel.

I would like to thank the scikit-learn community who has shared with me
their passion about computer science, machine learning and Python. By
contributing to this open source project, I have learnt much since my
first contribution.

I also offer my regards and blessing to all the people near and dear to
my heart for their continuous support, and to all of those who supported
in any respect during the completion of this project.

\pdfbookmark

[1]Contentstableofcontents \manualmark

###### Contents

-    \thechapter Introduction
    -    1 Publications
    -    2 Outline
-    I Background
    -    \thechapter Supervised learning
        -    3 Introduction
        -    4 Classes of supervised learning algorithms
            -    4.1 Linear models
            -    4.2 (Deep) Artificial neural networks
            -    4.3 Neighbors based methods
            -    4.4 Decision tree models
            -    4.5 From single to multiple output models
        -    5 Evaluation of model prediction performance
        -    6 Criteria to assess model performance
            -    6.1 Metrics for binary classification
            -    6.2 Metrics for multi-class classification
            -    6.3 Metrics for multi-label classification and ranking
            -    6.4 Regression metrics
        -    7 Hyper-parameter optimization
        -    8 Unsupervised projection methods
            -    8.1 Principal components analysis
            -    8.2 Random projection
            -    8.3 Kernel functions
    -    \thechapter Decision trees
        -    9 Decision tree model
        -    10 Growing decision trees
            -    10.1 Search among node splitting rules
            -    10.2 Leaf labelling rules
            -    10.3 Stop splitting criterion
        -    11 Right decision tree size
        -    12 Decision tree interpretation
        -    13 Multi-output decision trees
    -    \thechapter Bias-variance and ensemble methods
        -    14 Bias-variance error decomposition
        -    15 Averaging ensembles
            -    15.1 Variance reduction
            -    15.2 Generic randomization induction methods
            -    15.3 Randomized forest model
        -    16 Boosting ensembles
            -    16.1 Adaboost and variants
            -    16.2 Functional gradient boosting
-    II Learning in compressed space through random projections
    -    \thechapter Random projections of the output space
        -    17 Methods
            -    17.1 Multi-output regression trees in randomly
                projected output spaces
            -    17.2 Exploitation in the context of tree ensembles
        -    18 Bias/variance analysis
            -    18.1 Single random trees.
            -    18.2 Ensembles of @xmath random trees.
        -    19 Experiments
            -    19.1 Effect of the size @xmath of the Gaussian output
                space
            -    19.2 Systematic analysis over 24 datasets
            -    19.3 Input vs output space randomization
            -    19.4 Alternative output dimension reduction techniques
            -    19.5 Learning stage computing times
        -    20 Conclusions
    -    \thechapter Random output space projections for gradient
        boosting
        -    21 Introduction
        -    22 Gradient boosting with multiple outputs
            -    22.1 Standard extension of gradient boosting to
                multi-output tasks
            -    22.2 Adapting to the correlation structure in the
                output-space
            -    22.3 Effect of random projections
            -    22.4 Convergence when @xmath
        -    23 Experiments
            -    23.1 Experimental protocol
            -    23.2 Experiments on synthetic datasets with known
                output correlation structures
            -    23.3 Effect of random projection
            -    23.4 Systematic analysis over real world datasets
        -    24 Conclusions
-    III Exploiting sparsity for growing and compressing decision trees
    -    \thechapter @xmath -based compression of random forest models
        -    25 Compressing tree ensembles by @xmath -norm
            regularization
        -    26 Empirical analysis
            -    26.1 Overall performances
            -    26.2 Effect of the regularization parameter @xmath .
            -    26.3 Influence of the Extra-Tree meta parameters @xmath
                and @xmath .
        -    27 Conclusion
    -    \thechapter Exploiting input sparsity with decision tree
        -    28 Tree growing
            -    28.1 Standard node splitting algorithm
            -    28.2 Splitting rules search on sparse data
            -    28.3 Partitioning sparse data
        -    29 Tree prediction
        -    30 Experiments
            -    30.1 Effect of the input space density on synthetic
                datasets
            -    30.2 Effect of the input space density on real datasets
            -    30.3 Algorithm comparison on 20 newsgroup
        -    31 Conclusion
    -    \thechapter Conclusions
        -    32 Conclusions
        -    33 Perspectives and future works
            -    33.1 Learning in compressed space through random
                projections
            -    33.2 Growing and compressing decision trees
            -    33.3 Learning in high dimensional and sparse
                input-output spaces
-    IV Appendix
    -    \thechapter Description of the datasets
        -    A Synthetic datasets
        -    B Regression datasset
        -    C Multi-label dataset
        -    D Multi-output regression datasets

\automark

[section]chapter

### Chapter \thechapter Introduction

Progress in information technology enables the acquisition and storage
of growing amounts of rich data in many domains including science
(biology, high-energy physics, astronomy, etc.), engineering (energy,
transportation, production processes, etc.), and society (environment,
commerce, etc.). Connected objects, such as smartphones, connected
sensors or intelligent houses, are now able to record videos, images,
audio signals, object localizations, temperatures, social interactions
of the user through a social network, phone calls or user to computer
program interactions such as voice help assistant or web search queries.
The accumulating datasets come in various forms such as images, videos,
time-series of measurements, recorded transactions, text etc. WEB
technology often allows one to share locally acquired datasets, and
numerical simulation often allows one to generate low cost datasets on
demand. Opportunities exist thus for combining datasets from different
sources to search for generic knowledge and enable robust decision.

All these rich datasets are of little use without the availability of
automatic procedures able to extract relevant information from them in a
principled way. In this context, the field of machine learning aims at
developing theory and algorithmic solutions for the extraction of
synthetic patterns of information from all kinds of datasets, so as to
help us to better understand the underlying systems generating these
data and hence to take better decisions for their control or
exploitation.

Among the machine learning tasks, supervised learning aims at modeling a
system by observing its behavior through samples of pairs of inputs and
outputs. The objective of the generated model is to predict with high
accuracy the outputs of the system given previously unseen inputs. A
genomic application of supervised learning would be to model how a DNA
sequence, a biological code, is linked to some genetic diseases. The
samples used to fit the model are the input-output pairs obtained by
sequencing the genome, the inputs, of patients with known medical
records for the studied genetic diseases, the outputs. The objective is
here twofold: (i) to understand how the DNA sequence influences the
appearing of the studied genetic diseases and (ii) to use the predictive
models to infer the probability of contracting the genetic disease.

The emergence of new applications, such as image annotation,
personalized advertising or 3D image segmentation, leads to high
dimensional data with a large number of inputs and outputs. It requires
scalable supervised learning algorithms in terms of computational power
and memory space without sacrificing accuracy.

Decision trees (Breiman et al., 1984 ) are supervised learning models
organized in the form of a hierarchical set of questions each one
typically based on one input variable leading to a prediction. Used in
isolation, trees are generally not competitive in terms of accuracy, but
when combined into ensembles (Breiman, 2001 ; Friedman, 2001 ) , they
yield state-of-the-art performances on standard benchmarks (Caruana
et al., 2008 ; Fernández-Delgado et al., 2014 ; Madjarov et al., 2012 )
. They however suffer from several limitations that make them not always
suited to address modern applications of machine learning techniques in
particular involving high dimensional input and output spaces.

In this thesis, we identify three main areas where random forest methods
could be improved and for which we provide and evaluate original
algorithmic solutions: (i) learning over high dimensional output spaces,
(ii) learning with large sample datasets and stringent memory
constraints at prediction time and (iii) learning over high dimensional
sparse input spaces. We discuss each one of these solutions in the
following paragraphs.

###### High dimensional output spaces

New applications of machine learning have multiple output variables,
potentially in very high number (Agrawal et al., 2013 ; Dekel and
Shamir, 2010 ) , associated to the same set of input variables. A first
approach to address such multi-output tasks is the so-called binary
relevance / single target method (Tsoumakas et al., 2009 ;
Spyromitros-Xioufis et al., 2016 ) , which separately fits one decision
tree ensemble for each output variable, assuming that the different
output variables are independent. A second approach called multi-output
decision trees (Blockeel et al., 2000 ; Geurts et al., 2006b ; Kocev
et al., 2013 ) fits a single decision tree ensemble targeting
simultaneously all the outputs, assuming that all outputs are
correlated. However in practice, (i) the computational complexity is the
same for both approaches and (ii) we have often neither of these two
extreme output correlation structures. As our first contribution, we
show how to make random forest faster by exploiting random projections
(a dimensionality reduction technique) of the output space. As a second
contribution, we show how to combine gradient boosting of tree ensembles
with single random projections of the output space to automatically
adapt to a wide variety of correlation structures.

###### Memory constraints on model size

Even with a large number of training samples @xmath , random forest
ensembles have good computational complexity ( @xmath ) and are easily
parallelizable leading to the generation of very large ensembles.
However, the resulting models are big as the model complexity is
proportional to the number of samples @xmath and the ensemble size. As
our third contribution, we propose to compress these tree ensembles by
solving an appropriate optimization problem.

###### High dimensional sparse input spaces

Some supervised learning tasks have very high dimensional input spaces,
but only a few variables have non zero values for each sample. The input
space is said to be “sparse”. Instances of such tasks can be found in
text-based supervised learning, where each sample is often mapped to a
vector of variables corresponding to the (frequency of) occurrence of
all words (or multigrams) present in the dataset. The problem is sparse
as the size of the text is small compared to the number of possible
words (or multigrams). Standard decision tree implementations are not
well adapted to treat sparse input spaces, unlike models such as support
vector machines (Cortes and Vapnik, 1995 ; Scholkopf and Smola, 2001 )
or linear models (Bottou, 2012 ) . Decision tree implementations are
indeed treating these sparse variables as dense ones raising the memory
needed. The computational complexity also does not depend upon the
fraction of non zero values. As a fourth contribution, we propose an
efficient decision tree implementation to treat supervised learning
tasks with sparse input spaces.

#### 1 Publications

This dissertation features several publications about random forest
algorithms:

-   (Joly et al., 2014 ) A. Joly, P. Geurts, and L. Wehenkel. Random
    forests with random projections of the output space for high
    dimensional multi-label classification. In Machine Learning and
    Knowledge Discovery in Databases, pages 607–622. Springer Berlin
    Heidelberg, 2014.

-   (Joly et al., 2012 ) A. Joly, F. Schnitzler, P. Geurts, and L.
    Wehenkel. L1-based compression of random forest models. In European
    Symposium on Artificial Neural Networks, Computational Intelligence
    and Machine Learning, 2012.

-   (Buitinck et al., 2013 ) L. Buitinck, G. Louppe, M. Blondel, F.
    Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Prettenhofer, A.
    Gramfort, J. Grobler, R. Layton, J. Vanderplas, A. Joly, B. Holt,
    and G. Varoquaux. Api design for machine learning software:
    experiences from the scikit-learn project. arXiv preprint
    arXiv:1309.0238, 2013.

and also the following submitted article:

-   H. Fares, A. Joly, and P. Papadimitriou. Scalable Learning of
    Tree-Based Models on Sparsely Representable Data.

Some collaborations were made during the thesis, but are not discussed
within this manuscript:

-   (Sutera et al., 2014 ) A. Sutera, A. Joly, V. François-Lavet, Z. A.
    Qiu, G. Louppe, D. Ernst, and P. Geurts. Simple connectome inference
    from partial correlation statistics in calcium imaging. In JMLR:
    Workshop and Conference Proceedings, pages 1–12, 2014.

-   (Delierneux et al., 2015a ) C. Delierneux, N. Layios, A. Hego, J.
    Huart, A. Joly, P. Geurts, P. Damas, C. Lecut, A. Gothot, and C.
    Oury. Elevated basal levels of circulating activated platelets
    predict icu-acquired sepsis and mortality: a prospective study.
    Critical Care, 19(Suppl 1):P29, 2015a.

-   (Delierneux et al., 2015b ) C. Delierneux, N. Layios, A. Hego, J.
    Huart, A. Joly, P. Geurts, P. Damas, C. Lecut, A. Gothot, and C.
    Oury. Prospective analysis of platelet activation markers to predict
    severe infection and mortality in intensive care units. In journal
    of thrombosis and haemostasis, volume 13, pages 651–651.

-   (Begon et al., 2016 ) J.-M. Begon, A. Joly, and P. Geurts. Joint
    learning and pruning of decision forests. In Belgian-Dutch
    Conference On Machine Learning, 2016.

The following article has been submitted:

-   C. Delierneux, N. Layios, A. Hego, J. Huart, C. Gosset, C. Lecut, N.
    Maes, P. Geurts, A. Joly, P. Lancellotti, P. Damas, A. Gothot,
    and C. Oury. Incremental value of platelet markers to clinical
    variables for sepsis prediction in intensive care unit patients: a
    prospective pilot study.

#### 2 Outline

In Part I of this thesis, we start by introducing in Chapter \thechapter
the key concepts about supervised learning: (i) what are the most
popular supervised learning models, (ii) how to assess the prediction
performance of a supervised learning model and (iii) how to optimize the
hyper-parameters of theses models. We also present some unsupervised
projection methods, such as random projections, which transform the
original space to another one. We describe more in detail the decision
tree model classes in Chapter \thechapter . More specifically, we
describe the methodology to grow and to prune such trees. We also show
how to adapt decision tree growing and prediction algorithms to
multi-output tasks. In Chapter \thechapter , we show why and how to
combine models into ensembles either by learning models independently
with averaging methods or sequentially with boosting methods.

In Part II , we first show how to grow an ensemble of decision trees on
very high dimensional output spaces by projecting the original output
space onto a random sub-space of lower dimension. In Chapter \thechapter
, it turns out that for random forest models, an averaging ensemble of
decision trees, the learning time complexity can be reduced without
affecting the prediction performance. Furthermore, it may lead to
accuracy improvement (Joly et al., 2014 ) . In Chapter \thechapter , we
propose to combine random projections of the output space and the
gradient tree boosting algorithm, while reducing learning time and
automatically adapting to any output correlation structure.

In Part III , we leverage sparsity in the context of decision tree
ensembles. In Chapter \thechapter , we exploit sparsifying optimization
algorithms to compress random forest models while retaining their
prediction performances (Joly et al., 2012 ) . In Chapter \thechapter ,
we show how to leverage input sparsity to speed up decision tree
induction.

During the thesis, I made significant contributions to the open source
scikit-learn project (Pedregosa et al., 2011 ; Buitinck et al., 2013 )
and developed my own open source libraries random-output-trees ¹ ¹ 1
https://github.com/arjoly/random-output-trees , containing the work
presented in Chapter \thechapter and Chapter \thechapter , and
clusterlib ² ² 2 https://github.com/arjoly/clusterlib , containing the
tools to manage jobs on supercomputers.

## Part I Background

### Chapter \thechapter Supervised learning

Outline In the field of machine learning, supervised learning aims at
finding the best function which describes the input-output relation of a
system only from observations of this relationship. Supervised learning
problems can be broadly divided into classification tasks with discrete
outputs and into regression tasks with continuous outputs. We first
present major supervised learning methods for both classification and
regression. Then, we show how to estimate their performance and how to
optimize the hyper-parameters of these models. We also introduce
unsupervised projection techniques used in conjunction with supervised
learning methods.

Supervised learning aims at modeling an input-output system from
observations of its behavior. The applications of such learning methods
encompass a wide variety of tasks and domains ranging from image
recognition to medical diagnosis tools. Supervised learning algorithms
analyze the input-output pairs and learn how to predict the behavior of
a system (see Figure \thechapter .1 ) by observing its responses,
described by output variables @xmath , also called targets, to its
environment described by input variables @xmath , also called features.
The outcome of the supervised learning is a function @xmath modeling the
behavior of the system.

Supervised learning has numerous applications in the multimedia, in
biology, in engineering or in the societal domain:

-   Identification of digits from photos, such as house number from
    street photos or digit post code from letters.

-   Automatic image annotation such as detecting tumorous cells or
    identifying people in photos.

-   Detection of genetic diseases from DNA screening.

-   Disease diagnostic based on clinical and biological data of a
    patient.

-   Automatic text translation from a source language to a target
    language such as from French to English.

-   Automatic voice to text transcription from audio records.

-   Market price prediction on the basis of economical and performance
    indicators.

We introduce the supervised learning framework in Section 3 . We
describe in Section 4 the most common classes of supervised learning
models used to map the outputs of the system to its inputs. We introduce
how to assess their performances in Section 5 , how to compare the model
predictions to a ground truth in Section 6 and how to select the best
hyper-parameters of such models in Section 7 . We also show some input
space projection methods in Section 8 , often used in combination with
supervised learning models improving the computational time and / or the
accuracy of the model.

#### 3 Introduction

The goal of supervised learning is to learn the function @xmath mapping
an input vector @xmath of a system to a vector of system outputs @xmath
, only from observations of input-output pairs. The set of possible
input (resp. output) vectors form the input space @xmath (resp. output
space @xmath ).

Once we have identified the input and output variables, we start to
collect input-output pairs, also called samples. Table \thechapter .1
displays 5 samples collected from a system with 4 inputs and 3 outputs.
We distinguish three types of variables: binary variables taking only
two different values, like the variables @xmath and @xmath ; categorical
variables taking two or more possible values, like variables @xmath and
@xmath , and numerical variables having numerical values, like @xmath ,
@xmath and @xmath . A binary variable is also a categorical variable.
For simplicity, we will assume in the following without loss of
generality that binary and categorical variables have been mapped from
the set of their @xmath original values to a set of integers of the same
cardinality @xmath .

When we collect data, some input and/or output values might be missing
or unavailable. Tasks with missing input values are said to have missing
data. Missing values are marked by a “?” in Table \thechapter .1 .

We classify supervised learning tasks into two main families based on
their output domains. Classification tasks have either binary outputs as
in disease prediction ( @xmath ) or categorical outputs as in digits
recognition ( @xmath ). Regression tasks have numerical outputs ( @xmath
) such as in house price predictions. A classification task with only
one binary output (resp. categorical output) is called a binary
classification task (resp. multi-class classification task). A
multi-class classification task is assumed to have more than two
classes, otherwise it is a binary classification task. In the presence
of multiple outputs, we further distinguish multi-label classification
tasks which associate multiple binary output values to each input
vector. In the multi-label context, the output variables are also called
“labels” and the output vectors are called “label set”. From a modeling
perspective, multi-class classification tasks are multi-label
classification problems whose labels are mutually exclusive. Table
\thechapter .2 summarizes the different supervised learning tasks.

We will denote by @xmath an input space, and by @xmath an output space.
We denote by @xmath the joint (unknown) sampling density over @xmath .
Superscript indices ( @xmath ) denote (input, output) vectors of an
observation @xmath . Subscript indices (e.g. @xmath ) denote components
of vectors. With these notations supervised learning can be defined as
follows:

Supervised learning

Given a learning sample @xmath of @xmath observations in the form of
input-output pairs, a supervised learning task is defined as searching
for a function @xmath in a hypothesis space @xmath that minimizes the
expectation of some loss function @xmath over the joint distribution of
input / output pairs:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

The choice of the loss function @xmath depends on the property of the
supervised learning task (see Table \thechapter .3 for their
definitions):

-   In regression ( @xmath ), we often use the squared loss, except when
    we want to be robust to the presence of outliers, samples with
    abnormal output values, where we prefer other losses such as the
    absolute loss.

-   In classification tasks ( @xmath ), the reported performance is
    commonly the average @xmath loss, called the error rate. However,
    the model does not often directly minimize the @xmath loss as it
    leads to non convex and non continuous optimization problems with
    often high computational cost. Instead, we can relax the multi-class
    or the binary constraint by optimizing a smoother loss such as the
    hinge loss or the logistic loss. To get a binary or multi-class
    prediction, we can threshold the predicted value @xmath .

Figure \thechapter .2 plots several loss discrepancies @xmath whenever
the ground truth is @xmath d as a function of the value @xmath predicted
by the model. The @xmath loss is a step function with a discontinuity at
@xmath . The hinge loss has a linear behavior whenever @xmath and is a
constant with @xmath . The logistic loss strongly penalizes any mistake
and is zero only if the model is correct with an infinite score. The
plot also highlights that we can use regression losses for
classification tasks. It shows that regression losses penalize any
predicted value @xmath different from the ground truth @xmath . However,
this is not always the desired behavior. For instance whenever @xmath
(resp. @xmath ), regression losses penalize any score greater than
@xmath (resp. smaller than @xmath ), while the model truly believes that
the output is positive (resp. negative). This is often the reason why
regression losses are avoided for classification tasks.

#### 4 Classes of supervised learning algorithms

Supervised learning aims at finding the best function @xmath in a
hypothesis space @xmath to model the input-output function of a system.
If there is no restriction on the hypothesis space @xmath , the model
@xmath can be any function @xmath .

Consider a binary function @xmath which has @xmath binary inputs. The
binary function is uniquely defined by knowing the output values of the
@xmath possible input vectors. The hypothesis space of all binary
functions contains @xmath binary functions. If we observe @xmath
different input-output pair assignments, there remain @xmath possible
binary functions. For a binary function of @xmath inputs, we have @xmath
possible binary functions. If we observe @xmath input-output pair
assignments among the @xmath possible ones, we still have @xmath
possible binary functions. The number of possible functions highly
increases with the cardinality of each variable. The hypothesis space
will be even larger with a stochastic function, where different output
values are possible for each possible input assignment.

By making assumptions on the model class @xmath , we can largely reduce
the size of the hypothesis space. For instance in the previous example,
if we assume that 2 out of the 5 binary input variables are independent
of the output, there remain @xmath possible functions. The correct
function would be uniquely identified by observing the @xmath possible
assignments.

Given the data stochasticity, those model classes can directly model the
input-output mapping @xmath , but also the conditional probability
@xmath and predictions are made through

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

We will present some of the most popular model classes: linear models in
Section 4.1 ; artificial neural networks in Section 4.2 which are
inspired from the neurons in the brain; neighbors-based models in
Section 4.3 which find the nearest samples in the training set; decision
tree based-models in Section 4.4 (and in more details in Chapter
\thechapter ). Note that we introduce ensemble methods in Section 4.4
and discuss them more deeply in Chapter \thechapter .

##### 4.1 Linear models

Let us denote by @xmath a vector of input variables. A linear model
@xmath is a model having the following form

  -- -- --
        
  -- -- --

where the vector @xmath is a concatenation of the intercept @xmath and
the coefficients @xmath .

Given a set of @xmath input-output pairs @xmath , we retrieve the
coefficient vector @xmath of the linear model by minimizing a loss
@xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

With the square loss @xmath , there exists an analytical solution to
Equation \thechapter .3 called ordinary least squares. Let us denote by
@xmath the concatenation of the input vectors with a first column of
@xmath full of ones to model the intercept @xmath and by @xmath the
concatenation of the output values. We can now express the sum of
squares in matrix notation:

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .4)
              @xmath      ( \thechapter .5)
  -- -------- -------- -- -------------------

The first order differentiation of the sum of squares with respect to
@xmath yields to

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

The vector minimizing the square loss is thus

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

The solution exists only if @xmath is invertible.

Whenever the number of inputs plus one @xmath is greater than the number
of samples @xmath , the analytical solution is ill posed as the matrix
@xmath is rank deficient ( @xmath ). To ensure a unique solution, we can
add a regularization penalty @xmath with a multiplying constant @xmath
on the coefficients @xmath of the linear model:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

With a @xmath -norm constraint on the coefficients, we transform the
ordinary least square model into a ridge regression model (Hoerl and
Kennard, 1970 ) :

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

One can show (see Section 3.4.1 of (Hastie et al., 2009 ) ) that the
constant @xmath controls the maximal value of all coefficients @xmath in
the ridge regression solution.

With a @xmath -norm constraint ( @xmath ) on the coefficient @xmath , we
have the Lasso model (Tibshirani, 1996b ) :

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

Contrarily to the ridge regression, the Lasso has no closed formed
analytical solution even though the resulting optimization problem
remains convex. However, we gain that the @xmath -norm penalty
sparsifies the coefficients @xmath of the linear model. If the constant
@xmath tends towards infinity, all the coefficients will be zero @xmath
. While with @xmath , we have the ordinary least square formulation.
With @xmath moving from @xmath to @xmath , we progressively add
variables to the linear model with a magnitude @xmath depending on
@xmath . The monotone Lasso (Hastie et al., 2007 ) further restricts the
coefficient to monotonous variation with respect to @xmath and has been
shown to perform better whenever the input variables are correlated.

A combination of the @xmath -norm and the @xmath -norm constraints on
the coefficients is called an elastic net penalty (Zou and Hastie, 2005
) . It shares both the property of the Lasso and the ridge regression:
sparsely selecting coefficients as in Lasso and considering groups of
correlated variables together as in the ridge regression. With a careful
design of the penalty term @xmath , we can enforce further properties
such as selecting variables in groups of pre-defined variables with the
group Lasso (Yuan and Lin, 2006 ; Meier et al., 2008 ) or taking into
account the variable locality in the coefficient vector @xmath while
adding a new variable to the linear model with the fused Lasso
(Tibshirani et al., 2005 ) .

By selecting an appropriate loss and penalty term, we have a wide
variety of linear models at our disposal with different properties. In
regression, an absolute loss leads to the least absolute deviation
algorithm (Bloomfield and Steiger, 2012 ) which is robust to outliers.
In classification, we can use a logistic loss to model the class
probability distribution leading to the logistic regression model. With
a hinge loss, we aim at finding a hyperplane which maximizes the
separations between the classes leading to the support vector machine
algorithm (Cortes and Vapnik, 1995 ) .

A linear model can handle non linear problems by applying first a non
linear transformation to the input space @xmath . For instance, consider
the classification task of Figure (a)a where each class is located on a
concentric circle. Given the non linearity of the problem, we can not
find a straight line separating both classes in the cartesian plane as
shown in Figure (b)b . If instead we fit a linear model on the distance
from the origin @xmath as illustrated in Figure (c)c , we find a model
separating perfectly both classes. We often use linear models in
conjunction with kernel functions (presented in Section 8.3 ), which
provide a range of ways to achieve non-linear transformations of the
input space.

##### 4.2 (Deep) Artificial neural networks

An artificial neural network is a statistical model mimicking the
structure of the brain and composed of artificial neurons. A neuron, as
shown in Figure (a)a , is composed of three parts: the soma , the cell
body, processes the information from its dendrites and transmits its
results to other neurons through the axon , a nerve fiber. An artificial
neuron follows the same structure (see Figure (b)b ) replacing
biological processing by numerical computations. The basic neuron
(Rosenblatt, 1958 ) used for supervised learning consists in a linear
model of parameters @xmath followed by an activation function @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The activation function replicates artificially the non linear
activation of real neurons. It is a scalar function such as a hyperbolic
tangent @xmath , a sigmoid @xmath or a rectified linear function @xmath
.

More complex artificial neural networks are often structured into layers
of artificial neurons. The inputs of a layer are the input variables or
the outputs of the previous layer. Each neuron of the layer has one
output. The neural network is divided into three parts as in Figure
\thechapter .5 : the first and last layers are respectively the input
layer and the output layer , while the layers in between are the hidden
layers . The hidden layer of Figure \thechapter .5 is called a fully
connected layer as all the neurons (here the input variables) from the
previous layer are connected to each neuron of the layer. Other layer
structures exist such as convolutional layers (Krizhevsky et al., 2012 ;
LeCun et al., 2004 ) which mimic the visual cortex (Hubel and Wiesel,
1968 ) . A network is not necessarily feed forward, but can have a more
complex topology for example recurrent neural networks
(Boulanger-Lewandowski et al., 2012 ; Graves et al., 2013 ) mimic the
brain memory by forming internal cycles of neurons. Neural networks with
many layers are also known (LeCun et al., 2015 ) as deep neural
networks.

Artificial neurons form a graph of variables. Through this
representation, we can learn such models by applying gradient based
optimization techniques (Bengio, 2012 ; Glorot and Bengio, 2010 ; LeCun
et al., 2012 ) to find the coefficient vector associated to each neuron
minimizing a given loss function.

##### 4.3 Neighbors based methods

The @xmath -nearest neighbors model is defined by a distance metric
@xmath and a set of samples. At learning time, those samples are stored
in a database. We predict the output of an unseen sample by aggregating
the outputs of the @xmath -nearest samples in the input space according
to the distance metric @xmath , with @xmath being a user-defined
parameter.

More precisely, given a training set @xmath and a distance measure
@xmath , an unseen sample with value in the input space @xmath is
assigned a prediction through the following procedure:

1.  Compute the distances @xmath in the input space, @xmath , between
    the training samples @xmath and the input vector @xmath .

2.  Search for the @xmath samples in the training set which have the
    smallest distance to the vector @xmath .

3.  In classification, compute the proportion of samples of each class
    among these @xmath -nearest neighbors: the final prediction is the
    class with the highest proportion. This corresponds to a majority
    vote over the @xmath nearest neighbors. In regression, the
    prediction is the average output of the @xmath -nearest neighbors.

The @xmath -nearest neighbor method adapts to a wide variety of
scenarios by selecting or by designing a proper distance metric such as
the euclidean distance or the Hamming distance.

##### 4.4 Decision tree models

A decision tree model is a hierarchical set of questions leading to a
prediction. The internal nodes, also called test nodes, test the value
of a feature. In Figure \thechapter .6 , the starting node, also called
root node, tests whether the feature “Petal width” is bigger or smaller
than @xmath . According to the answer, you follow either the right
branch ( @xmath ) leading to another test node or the left branch (
@xmath ) leading to an external node, also called a leaf. To predict an
unseen sample, you start at the root node and follow the tree structure
until reaching a leaf labelled with a prediction. With the decision tree
of Figure \thechapter .6 , an iris with petal width smaller than @xmath
is an iris Setosa.

A classification or a regression tree (Breiman et al., 1984 ) is built
using all the input-output pairs @xmath as follows: for each test node,
the best split @xmath of the local subsample @xmath reaching the node is
chosen among the @xmath input features combined with the selection of an
optimal cut point. The best sample split @xmath of @xmath minimizes the
average reduction of impurity

  -- -------- -- --------------------
     @xmath      
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

where @xmath is the impurity of the output such as the entropy in
classification or the variance in regression. The decision tree growth
continues until we reach a stopping criterion such as no impurity @xmath
.

To avoid over-fitting, we can stop earlier the tree growth by adding
further stopping criteria such as a maximal depth or a minimal number of
samples to split a node.

Instead of a single decision tree, we often train an ensemble of such
models:

-   Averaging-based ensemble methods grow an ensemble by randomizing the
    tree growth. The random forest method (Breiman, 2001 ) trains
    decision trees on bootstrap copies of the training set, i.e. by
    sampling with replacement from the training dataset, and it
    randomizes the best split selection by searching this split among
    @xmath out of the @xmath features at each nodes ( @xmath ).

-   Boosting-based methods (Freund and Schapire, 1997 ; Friedman, 2001 )
    build iteratively a sequence of weak models such as shallow trees
    which perform only slightly better than random guessing. Each new
    model refines the prediction of the ensemble by focusing on the
    wrongly predicted training input-output pairs.

We further discuss decision tree models in Chapter \thechapter and
ensemble methods in Chapter \thechapter .

##### 4.5 From single to multiple output models

With multiple outputs supervised learning tasks, we have to infer the
values of a set of @xmath output variables @xmath (instead of a single
one) from a set of @xmath input variables @xmath . We hope to improve
the accuracy and / or computational performance by exploiting the
correlation structure between the outputs. There exist two main
approaches to solve multiple output tasks: problem transformation
presented in Section 4.5.1 and algorithm adaptation in Section 4.5.2 .
We present here a non exhaustive selection of both approaches. The
interested reader will find a broader review of the multi-label
literature in (Zhang and Zhou, 2014 ; Tsoumakas et al., 2009 ; Madjarov
et al., 2012 ; Gibaja and Ventura, 2014 ) and of the multi-output
regression literature in (Spyromitros-Xioufis et al., 2016 ; Borchani
et al., 2015 ) .

###### 4.5.1 Problem transformation

The problem transformation approach transforms the original multi-output
task into a set of single output tasks. Each of these single output
tasks is then solved by classical classifiers or regressors. The
possible output correlations are exploited through a careful
reformulation of the original task.

###### Independent estimators

The simplest way to handle multi-output learning is to treat all outputs
in an independent way. We break the prediction of the @xmath outputs
into @xmath independent single output prediction tasks. A model is
fitted on each output. At prediction time, we concatenate the
predictions of these @xmath models. This is called the binary relevance
method (Tsoumakas et al., 2009 ) in multi-label classification and the
single target method (Spyromitros-Xioufis et al., 2016 ) in multi-output
regression. Since we consider the outputs independently, we neglect the
output correlation structure. Some methods may however benefit from
sharing identical computations needed for the different outputs. For
instance, the @xmath -nearest neighbor method can share the search for
the @xmath -nearest neighbors in the input space, and the ordinary
linear least squares method can share the computation of @xmath in
Equation \thechapter .7 .

###### Estimator chain

If the outputs are dependent, the model of a single output might benefit
from the values of the correlated outputs. In the estimator chain
method, we sequentially learn a model for each output by providing the
predictions of the previously learnt models as auxiliary inputs. This is
called a classifier chain (Read et al., 2011 ) in classification and a
regressor chain (Spyromitros-Xioufis et al., 2016 ) in regression.

More precisely, the estimator chain method first generates an order
@xmath on the outputs for instance based on prior knowledge, the output
density, the output variance or at random. Then with the training
samples and the output order @xmath , it sequentially learns @xmath
estimators: the @xmath -th estimator @xmath aims at predicting the
@xmath -th output using as inputs the concatenation of the input vectors
with the predictions of the models learnt for the @xmath previous
outputs. To reduce the model variance, we can generate an ensemble of
estimator chains by randomizing the chain order (and / or the underlying
base estimator), and then we average their predictions.

In multi-label classification, Cheng et al. ( 2010 ) formulates a Bayes
optimal classifier chain by modeling the conditional probability of
@xmath . Under the chain rule, we have

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

Each estimator of the chain approximates a probability factor of the
chain rule decomposition. Using the estimation of @xmath made by the
chain and a given loss function @xmath , we can perform Bayes optimal
prediction:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

###### Error correcting codes

Error correcting codes are techniques from information and coding theory
used to properly deliver a message through a noisy channel. It first
codes the original message, and then corrects the errors made during the
transmission at decoding time. This idea have been applied to
multi-class classification ( Dietterich and Bakiri, ; Guruswami and
Sahai, 1999 ) , multi-label classification (Ferng and Lin, 2011 ; Zhang
and Schneider, 2011 ; Kajdanowicz and Kazienko, 2012 ; Kouzani and
Nasireding, 2009 ; Guo et al., 2008 ; Hsu et al., 2009 ; Kapoor et al.,
2012 ; Cisse et al., 2013 ) and multi-output regression (Tsoumakas
et al., 2014 ; Yu et al., 2006 ) tasks by viewing the predictions made
by the supervised learning model(s) as a message transmitted through a
noisy channel. It transforms the original task by encoding the output
values with a binary error correcting code or output projections. One
classifier is then fitted for each bit of the code or output projection.
At prediction time, we concatenate the predictions made by each
estimator and decode them by solving the inverse problem. Note that the
output coding might also have for objective to reduce the dimensionality
of the output space (Hsu et al., 2009 ; Kapoor et al., 2012 ) .

###### Pairwise comparisons

In multi-label tasks, the ranking by pairwise comparison approach
(Hüllermeier et al., 2008 ) aims to generate a ranking of the labels by
making all the pairwise label comparisons. The original tasks is
transformed into @xmath binary classification tasks where we compare if
a given label is more likely to appear than another label. The datasets
comparing each label pair is obtained by collecting all the samples
where only one of the outputs is true, but not both. This approach is
similar to the one-versus-one approach (Park and Fürnkranz, 2007 ) in
multi-class classification task, however we can not directly transform
the ranking into a prediction, i.e. label set. To decrease the
prediction time, alternative ranking construction schemes have been
proposed (Mencia and Fürnkranz, 2008 ; Mencía and Fürnkranz, 2010 )
requiring less than @xmath classifier predictions.

The Calibrated label ranking method (Brinker et al., 2006 ; Fürnkranz
et al., 2008 ) extends the previous approach by adding a virtual label
which will serve as a split point between the true and the false labels.
For each label, we add a new tasks using all the samples comparing the
label @xmath to the virtual label whose value is the opposite of the
label @xmath . To the @xmath tasks, we effectively add @xmath tasks.

###### Label power set

For multi-label classification tasks, the label power set method
(Tsoumakas et al., 2009 ) encodes each label set in the training set as
a class. It transforms the original task into a multi-class
classification task. At prediction time, the class predicted by the
multi-class classifier is decoded thanks to the one-to-one mapping of
the label power set encoding. The drawback of this approach is to
generate a large number of classes due to the large number of possible
label sets. For @xmath samples and @xmath labels, the maximal number of
classes is @xmath . This leads to accuracy issues if some label sets are
not well represented in the training set. To alleviate the explosion of
classes, rakel (Tsoumakas and Vlahavas, 2007 ) generates an ensemble of
multi-class classifiers by subsampling the output space and then
applying the label power set transformation.

###### 4.5.2 Algorithm adaptation

The algorithm adaptation approach modifies existing supervised learning
algorithms to handle multiple output tasks. We show here how to extend
the previously presented models classes to multi-output regression and
to multi-label classification tasks.

###### Linear-based models

Linear-based models have been adapted to multi-output tasks by
reformulated their mathematical formulation using multi-output losses
and (possibly) regularization constraints enforcing assumptions on the
input-output and the output-output correlation structures. The proposed
methods are based for instance on extending least-square regression
(Dayal and MacGregor, 1997 ; Breiman and Friedman, 1997 ; Similä and
Tikka, 2007 ; Baldassarre et al., 2012 ; Evgeniou et al., 2005 ; Zhou
and Tao, 2012 ) (with possibly regularization), canonical correlation
analysis (Izenman, 1975 ; Van Der Merwe and Zidek, 1980 ) , support
vector machine (Elisseeff and Weston, 2001 ; Jiang et al., 2008 ; Xu,
2012 ; Evgeniou and Pontil, 2004 ; Evgeniou et al., 2005 ) , support
vector regression (Vazquez and Walter, 2003 ; Sánchez-Fernández et al.,
2004 ; Liu et al., 2009 ; Xu et al., 2013 ) , and conditional random
fields (Ghamrawi and McCallum, 2005 ) .

###### (Deep) Artificial neural networks

Neural networks handles multi-output tasks by having one node on the
output layer per output variable. The network minimizes a global error
function defined over all the outputs (Specht, 1991 ; Zhang and Zhou,
2006 ; Ciarelli et al., 2009 ; Zhang, 2009 ; Nam et al., 2014 ) . The
output correlation are taken into account by sharing the input and the
hidden layers between all the outputs.

###### Nearest neighbors

The @xmath -nearest neighbors algorithm predicts an unseen sample @xmath
by aggregating the output value of the @xmath nearest neighbors of
@xmath . This algorithm is adapted to multi-output tasks by sharing the
nearest neighbors search among all outputs. If we just share the search,
this is called binary relevance of @xmath -nearest neighbors in
classification and single target of @xmath -nearest neighbors in
regression. Multi-output extensions of the @xmath -nearest neighbors
modifies how the output values of the nearest neighbors are aggregated
for the predictions for instance it can utilize the maximum a posteriori
principle (Zhang and Zhou, 2007 ; Younes et al., 2011 ; Cheng and
Hüllermeier, 2009 ) or it can re-interpret the output aggregation as a
ranking problem (Chiang et al., 2012 ; Brinker and Hüllermeier, 2007 ) ,

###### Decision trees

The decision tree model is a hierarchical structure partitioning the
input space and associating a prediction to each partition. The growth
of the tree structure is done by maximizing the reduction of an impurity
measure computed in the output space. When the tree growth is stopped at
a leaf, we associate a prediction to this final partition by aggregating
the output values of the training samples. We adapt the decision tree
algorithm to multi-output tasks in two steps (Segal, 1992 ; De’Ath, 2002
; Blockeel et al., 2000 ; Clare and King, 2001 ; Zhang, 1998 ; Vens
et al., 2008 ; Noh et al., 2004 ) : (i) multi-output impurity measures
are used to grow the structure as the sum over the output space of the
entropy or the variance; (ii) the leaf predictions are obtained by
computing a constant minimizing a multi-output loss function such as the
@xmath -norm loss in regression or the Hamming loss in classification.
We discuss in more details how to adapt the decision tree algorithm to
multi-output tasks in Section 13 .

Instead of growing a single decision tree, they are often combined
together to improve their generalization performance. Random forest
models (Breiman, 2001 ; Geurts et al., 2006a ) averages the predictions
of several randomized decision trees and has been studied in the context
of multi-output learning (Kocev et al., 2007 ; Segal and Xiao, 2011 ;
Kocev et al., 2013 ; Madjarov et al., 2012 ; Joly et al., 2014 ) .

###### Ensembles

Ensemble methods aggregate the predictions of multiple models into a
single one so to improve its generalization performance. We discuss how
the averaging and boosting approaches have been adapted to multi-output
supervised learning tasks.

Averaging ensemble methods have been straightforwardly adapted by
averaging the prediction of multi-output models. Instead of averaging
scalar predictions, it averages (Kocev et al., 2007 ; Segal and Xiao,
2011 ; Kocev et al., 2013 ; Madjarov et al., 2012 ; Joly et al., 2014 )
the vector predictions of each model of the ensemble. If the learning
algorithm is not inherently multi-output, we could use one the problem
transformation techniques as in rakel (Tsoumakas and Vlahavas, 2007 ) ,
which uses the label power set transformation, or ensemble of estimator
chain (Read et al., 2011 ) .

Boosting ensembles are grown by sequentially adding weak models
minimizing a selected loss, such as the Hamming loss (Schapire and
Singer, 2000 ) , the ranking loss (Schapire and Singer, 2000 ) , the
@xmath -norm loss (Geurts et al., 2006b ) or any differentiable loss
function (see Chapter \thechapter ).

#### 5 Evaluation of model prediction performance

For a given supervised learning model @xmath trained on a set of samples
@xmath , we want a model having good generalization able to predict
unseen samples. Otherwise said, the model @xmath should have minimal
generalization error over the input-output pair distribution, where the
generalization error is defined as:

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

for a given loss function @xmath .

Evaluating Equation \thechapter .14 is generally unfeasible, except in
the rare cases where (i) the input-output distribution @xmath is fully
known and (ii) for restricted classes of models. In practice, neither of
these conditions are met. However, we still need a principle way to
approximate the generalization error.

A first approach to approximate Equation \thechapter .14 is to evaluate
the error of the model @xmath on the training samples @xmath leading to
the resubstitution error:

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

A model with a high resubstitution error often has a high generalization
error and indeed underfits the data. The linear model shown in Figure
(a)a underfits the data as it is not complex enough to fit the non
linear data (here a second degree polynomial). Instead, we can fit a
high order polynomial model to have a zero resubstitution error as
illustrated in Figure (b)b . This complex model has poor generalization
error as it perfectly fits the noisy samples unable to retrieve the
second order parabola. Such overly complex models with zero
resubstitution error and non zero generalization error are said to
overfit the data. Since a zero resubstitution error does not imply a low
generalization error, it is a poor proxy of the generalization error.

Since we assess the quality of the model with the training samples, the
resubstitution error is optimistic and biased. Furthermore, it favors
overly complex models (as depicted in Figure \thechapter .7 ). To
improve the approximation of the generalization error, we need to use
techniques which avoid to use the training samples for performance
evaluation. They are either based on sample partitioning methods, such
as hold out methods and cross validation techniques, or sample
resampling methods, such as bootstrap estimation methods. Since the
amount of available data and time are fixed for both the model training
and the model assessment, there is a trade-off between (i) the quality
of the error estimate, (ii) the number of samples available to learn a
model and (iii) the amount of computing time available for the whole
process.

The hold out evaluation method splits the samples into a training set
@xmath , also called learning set, and a testing set @xmath commonly
with a ratio of @xmath - @xmath . The hold out error is given by

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
  -- -------- -- --------------------

This methods requires a high number of samples as a large part of the
data is devoted to the model assessment impeding the model training. If
too few samples are allocated to the testing set, the hold out estimate
becomes unreliable as its confidence intervals widen (Kohavi et al.,
1995 ) . Since the hold out error is a random number depending on the
sample partition, we can improve the error estimation by (i) generating
@xmath random partitions @xmath of the available samples, (ii) fitting a
model @xmath on each learning set @xmath and (iii) averaging the
performance of the @xmath models over their respective testing sets
@xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

To improve the data usage efficiency, we can resort to cross-validation
methods, also called rotation estimation, which split the samples into
@xmath folds @xmath approximately of the same size. Cross validation
methods average the performance of @xmath models @xmath each tested on
one of the @xmath folds and trained using the @xmath remaining folds:

  -- -------- -- --------------------
     @xmath      ( \thechapter .18)
  -- -------- -- --------------------

The number of folds @xmath is usually @xmath or @xmath . If @xmath is
equal to the number of samples ( @xmath ), it is called leave-one-out
cross validation.

Given that the folds do not overlap for cross validation methods, we are
tempted to assess the performance over the pooled cross validation
estimates with a given @xmath obtained by concatenating the predictions
made by each model @xmath over each of the @xmath -folds

  -- -- -- --------------------
           ( \thechapter .19)
  -- -- -- --------------------

where @xmath is the concatenation operator. There is no difference for
sample-wise losses such as the square loss. However, this is not the
case for metrics comparing a whole set of predictions to their ground
truth. Depending on the metrics, it has been showed that pooling may or
may not biase the error estimation (Parker et al., 2007 ; Forman and
Scholz, 2010 ; Airola et al., 2011 ) .

We can improve the quality of the estimate by repeating the cross
validation procedures over @xmath different @xmath -fold partitions,
averaging the performance of the models @xmath over each associated
testing set @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .20)
  -- -------- -- --------------------

If all combinations are tested exhaustively as in the leave-one-out
case, it is called complete cross validation. Since it is often too
expensive (Kohavi et al., 1995 ) , we can instead draw several sets of
folds at random.

The bootstrap method (Efron, 1983 ) draws @xmath bootstrap datasets
@xmath by sampling with replacement @xmath samples from the original
dataset of size @xmath . Each samples has a probability of @xmath to be
selected in a bootstrap which is approximately @xmath for large @xmath .
A first approach to estimate the error is to train a model @xmath on
each bootstrap dataset and use the original dataset as a testing set:

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

This leads to over optimistic results, given the overlap between the
training and the test data.

A better approach (discussed in Chapter 7.11 of (Hastie et al., 2009 ) )
is to imitate cross validation methods by fitting on each bootstrap
dataset a model @xmath and using the unused samples as a testing set.
This approach is called bootstrap leave-one-out:

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

where @xmath gives the bootstrap indices where the sample @xmath was not
drawn. It is similar to a 2-fold repeated cross validation or random
subsampling error with a ratio of 2/3 - 1/3 for the training and testing
set. The estimation is thus biased as it uses approximately @xmath
training samples instead of @xmath . We can alleviate this bias due to
the sampling procedure through the “0.632” estimator which averages the
training error and LOO Bootstrap error:

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      ( \thechapter .23)
  -- -------- -------- -- --------------------

Note that with very low sample size, it has been shown (Braga-Neto and
Dougherty, 2004 ) that the bootstrap approach yields better error
estimate than the cross validation approach.

Until now, we have assumed that the samples are independent and
identically distributed. Whenever this is no longer true, such as with
time series of measurements, we have to modify the assessment procedure
to avoid biasing the error estimation. For instance, the hold out
estimate would train the model on the oldest samples and test the model
on the more recent samples. Similarly in the medical context if we have
several samples for one patient, we should keep these samples together
in the training set or in the testing set.

Partition-based methods (hold out, cross validation) break the
assumption in classification that the samples from the training set are
independent from the samples in the testing set as they are drawn
without replacement from a pool of samples. The representation of each
class in the testing set is thus not guaranteed to be the same as in the
training set. It is advised (Kohavi et al., 1995 ) to perform stratified
splits by keeping the same proportion of classes in each set.

#### 6 Criteria to assess model performance

Assessing the performance of a model requires evaluation metrics which
will compare the ground truth to a prediction, a score or a probability
estimate. The selection of an appropriate scoring or error measure is
essential and is dependent of the supervised learning task and the goal
behind the modeling.

A first approach to assess a model is to define a goal for the model and
to quantify its realization. For instance, a company wants to maximize
its benefits and consider that the revenue must exceed the data analysis
cost of gathering samples, fitting a model and exploiting its
predictions. Unfortunately, this model optimization criterion is hardly
expressible into economical terms. We could instead consider the
effectiveness of the model such as the click-through-rate, used by
online advertising companies, which counts the number of clicks on a
link to the number of opportunities that users have to click on this
link. However, it is hard to formulate a model optimizing directly this
score and it requires to put the model into a production setting (or at
least simulate its behavior). Other optimization criteria exit that are
more amenable to mathematical analysis and numerical computation such as
the square loss or the logistic loss. Knowing the properties of such
criteria is necessary to make a proper choice.

We present binary classification metrics in Section 6.1 . Then, we show
how to extend these metrics to multi-class classification tasks in
Section 6.2 and to multi-label classification tasks in Section 6.3 . We
introduce metrics for regression tasks and multi-output regression tasks
in Section 6.4 .

More details or alternative descriptions of these metrics can be found
in the following references (Sokolova and Lapalme, 2009 ; Hossin and
Sulaiman, 2015 ; Ferri et al., 2009 ) . Note that I made significant
contributions to the implementations and the documentations of these
metrics in the scikit-learn library (Pedregosa et al., 2011 ; Buitinck
et al., 2013 ) .

##### 6.1 Metrics for binary classification

Given a set of @xmath ground truth values @xmath and their associated
model predictions @xmath , we can distinguish in binary classification
four categories of predictions (as shown in Table \thechapter .4 ). We
denote by true positives (TP) and true negatives (TN) the predictions
where the model accurately predicts the target respectively as true or
false:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .24)
     @xmath   @xmath      ( \thechapter .25)
  -- -------- -------- -- --------------------

Whenever the model wrongly predicts the samples, we call false positives
(FP) samples predicted as true while their labels are false and false
negatives (FN) samples predicted as false while their labels are true:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .26)
     @xmath   @xmath      ( \thechapter .27)
  -- -------- -------- -- --------------------

Together, the true positive, true negatives, false negatives and false
positives form the so called confusion or contingency matrix shown in
Table \thechapter .4 .

Two common metrics to assess classification performance are the error
rate, the average of the @xmath loss, and its complement the accuracy:

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .28)
        @xmath      ( \thechapter .29)
  -- -- -------- -- --------------------

Both metrics can be expressed in term of the confusion matrix:

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .30)
        @xmath      ( \thechapter .31)
  -- -- -------- -- --------------------

The error rate does not distinguish the false negatives from the false
positives. Similarly, the accuracy does not differentiate true positives
from true negatives. Thus, two classifiers may have exactly the same
accuracy or error rate, while leading to a totally different outcome by
increasing either the number of misses (false negatives) or the number
of false alarms (false positives). Furthermore, the error rate and the
accuracy can be overly optimistic whenever there is a high class
imbalance. A classification task with @xmath of samples in one of the
classes would easily lead to an accuracy of @xmath (and an error rate of
@xmath ) by alway predicting the most common class. The choice of an
appropriate metric thus depends on the properties of the classification
task, such as the class imbalance.

To differentiate false positives from false negative, we can assess
separately the proportion of correctly classified positive and negative
samples. This leads to the true positive rate (resp. true negative rate
) which computes the proportion of correctly classified positive (resp.
negative) samples:

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .32)
        @xmath      ( \thechapter .33)
  -- -- -------- -- --------------------

The complement of the true positive rate (resp. true negative rate) is
the false negative rate (resp. false positive rate):

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .34)
        @xmath      ( \thechapter .35)
  -- -- -------- -- --------------------

The true positive rate is also called sensitivy and tests the ability of
the classifier to correctly classify all positive samples as true. A
test with @xmath sensitivity implies that all positive samples are
correctly classified. However, this does not imply that all samples are
correctly classified. A classifier predicting all samples as true leads
to @xmath sensitivity and totally neglects false positives. We have to
look to the true negative rate, also called specifity , which tests the
ability of the classifier to correctly classify all negative samples as
negative. A perfect classifier should thus have a high sensitivity and a
high specifity. In the medical domain, the sensitivity and the
specificity are often used to characterize and to choose the behavior of
diagnosis tests such as pregnancy tests.

The average of the specifity and sensitivity is called the balanced
accuracy :

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .36)
        @xmath      ( \thechapter .37)
        @xmath      ( \thechapter .38)
  -- -- -------- -- --------------------

In the information retrieval context, a user sets a query to an
information system, e.g. a web search engine, to detect which documents
are relevant among a collection of such documents. In such systems, the
collection of documents is often extremely large with only a few
relevant documents to a given query. Due to the small proportion of
relevant documents, we want to maximize the precision , the fraction of
correctly predicted documents among the predicted documents. Binary
classification tasks with a high class imbalance can be viewed as an
information retrieval problems. In the context of binary classification
tasks, the precision is expressed as

  -- -------- -- --------------------
     @xmath      ( \thechapter .39)
  -- -------- -- --------------------

To have a perfect precision, one could predict all documents or samples
as negative (as irrelevant documents). In parallel, we want also to
maximize the recall, the proportion of correctly predicted true samples
among the true samples. The recall is a synonym for true positive rate
and sensitivity.

The precision and recall are often combined into a single number by
computing the @xmath score, the harmonic mean of the precision and
recall,

  -- -------- -- --------------------
     @xmath      ( \thechapter .40)
  -- -------- -- --------------------

Some classifiers associate a score or a probability @xmath to a sample
instead of a class label. We can threshold these continuous predictions
by a constant @xmath to compute the number of true positives, false
positives, false negatives and true negatives:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .41)
     @xmath   @xmath      ( \thechapter .42)
     @xmath   @xmath      ( \thechapter .43)
     @xmath   @xmath      ( \thechapter .44)
  -- -------- -------- -- --------------------

By varying @xmath , we can first derive performance curves to analyze
the prediction performance of those more models and then select a
classifier performance point with pre-determined classification
performance.

The receiver operating characteristic (ROC) curve (Fawcett, 2006 ) plots
the true positive rate as a function of the false positive rate by
varying the threshold @xmath as shown in Figure (a)a . The receiver, the
model user, can indeed choose any point on the curve to operate at a
given model specifity / sensitivity tradeoff. A random estimator has its
performance on the line @xmath , while a perfect classifier has the
points @xmath with @xmath of false positive rate and @xmath of true
positive rate on its curve. Any curve below the random line can be
reversed symmetrically to the line @xmath by flipping the classifier
prediction. The ROC curve is often used in the clinical domain (Metz,
1978 ) and coupled to a cost analysis to determine the proper threshold
@xmath . The area under the ROC curve can be interpreted as (Hanley and
McNeil, 1982 ) the probability to rank with a higher score one true
sample than one false sample chosen at random.

The precision-recall (PR) curve is the precision as a function of the
recall as shown in Figure (b)b . The ROC curve and the PR curves are
linked as there is a one to one mapping between points in the ROC space
and in the precision-recall space (Davis and Goadrich, 2006 ) . However
conversely to the ROC curve, the precision recall curve is sensitive to
the class imbalance between the positive and negative classes. Since
both the precision and recall do not take into account the amount of
true negatives, the precision-recall curve (compared to the ROC curve)
focuses on how well the estimator is able to classify correctly the
positive class.

##### 6.2 Metrics for multi-class classification

From binary classification to multi-class classification, the output
value is no more restricted to two classes and can go up to @xmath
-classes. Given the ground truths @xmath and the associated model
predictions @xmath , we can now divide the model predictions into @xmath
categories leading to a @xmath confusion matrix:

  -- -------- -- --------------------
     @xmath      ( \thechapter .45)
  -- -------- -- --------------------

Metrics such as the accuracy, the error rate or the log loss (see Table
\thechapter .3 ) naturally extend to multi-class classification tasks.
To extend other binary classification metrics (such as those developed
in Section 6.1 ), we need to break the @xmath confusion matrix into a
set of @xmath confusion matrices.

A first approach is to consider that each class @xmath is in turns the
positive class while the remaining labels form together the negative
class. We thus have @xmath confusion matrices whose true positives
@xmath , true negatives @xmath , false negatives @xmath and false
positives @xmath for the class @xmath are

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .46)
     @xmath   @xmath      ( \thechapter .47)
     @xmath   @xmath      ( \thechapter .48)
     @xmath   @xmath      ( \thechapter .49)
  -- -------- -------- -- --------------------

By averaging a metric @xmath computed on each derived confusion matrix,
we have the so called macro-averaged (Sokolova and Lapalme, 2009 ) of
the corresponding binary classification metric

  -- -------- --
     @xmath   
  -- -------- --

Note that the balanced accuracy in binary classification is thus equal
to the macro-specificity or macro-sensitivity in multi-class
classification.

Another useful averaging is the micro-averaging (Sokolova and Lapalme,
2009 ) . It uses as true positives @xmath and true negatives @xmath the
sum of the diagonal elements of the confusion matrix and as false
negatives @xmath (resp. false positives @xmath ) the sum of the lower
(resp. upper) triangular part of the confusion matrix:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .50)
     @xmath   @xmath      ( \thechapter .51)
     @xmath   @xmath      ( \thechapter .52)
     @xmath   @xmath      ( \thechapter .53)
  -- -------- -------- -- --------------------

Each averaging has its own properties: the macro-averaging considers
that each class has the same importance and the micro-averaging reduces
the importance given to the minority classes.

##### 6.3 Metrics for multi-label classification and ranking

From binary to multi-label classification, the ground truths @xmath and
the model predictions @xmath are no longer scalars, but vectors of size
@xmath or label sets. Both representations are interchangeable. Usually,
the number of labels associated to a sample is small compared to the
total number of labels.

The accuracy (Ghamrawi and McCallum, 2005 ) , also called subset
accuracy, has a direct extension in multi-label classification

  -- -------- -- --------------------
     @xmath      ( \thechapter .54)
  -- -------- -- --------------------

and requires for each prediction that the predicted label set matches
exactly the ground truth. This is an overly pessimistic metric,
especially for high dimensional label space, as it penalizes any single
mistake made for one sample. The complement of the subset accuracy is
called the subset 0-1 loss .

In information theory, the Hamming distance compares the number of
differences between two coded messages. The Hamming error metric
(Schapire and Singer, 1999 ) averages the Hamming distance between the
ground truth and the model prediction over the samples

  -- -------- -- --------------------
     @xmath      ( \thechapter .55)
  -- -------- -- --------------------

By contrast to the subset accuracy, the Hamming error is an optimistic
metric when the label space is sparse. For a sufficiently large number
of samples and a label density ³ ³ 3 The label density is the average
number of labels per samples on the ground truth divided by the size of
the label space. @xmath , a (useless) model predicting always the
presence of a label if its frequency of apparition is higher than @xmath
in the training set will roughly have a Hamming error of @xmath . In
some situations, the label density @xmath is so small that (more useful)
models have hardly an Hamming error lower than @xmath .

Both the Hamming error and the subset accuracy ignore the sparsity of
the label space leading to either overly optimistic or pessimistic
error. Multi-label metrics should be aware of the label space sparsity.

In statistics, the Jaccard index @xmath or Jaccard similarity
coefficient computes the similarity between two sets. Given two sets
@xmath and @xmath , the Jaccard index is defined as

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .56)
  -- -------- -------- -- --------------------

With label sets encoded as boolean vectors @xmath , the Jaccard index
becomes

  -- -------- -- --------------------
     @xmath      ( \thechapter .57)
  -- -------- -- --------------------

where @xmath is a vector of ones of size @xmath . The Jaccard similarity
score (Godbole and Sarawagi, 2004 ) , also sometimes called accuracy,
averages over the samples the Jaccard index between the ground truths
and the model predictions:

  -- -------- -- --------------------
     @xmath      ( \thechapter .58)
  -- -------- -- --------------------

By contrast to the Hamming loss, the Jaccard similarity score puts more
emphasis on the labels in the ground truth and the ones predicted by the
models. Moreover, it totally ignores all the negative labels. The
Jaccard similarity score can be viewed as “local” measure of similarity
and the Hamming loss a “global” measure of distance.

A fitted model @xmath applied to an input vector @xmath can go beyond
label prediction and associate to each label @xmath a score or a
probability estimate @xmath . When the density of the label space @xmath
is small and the size of the label space @xmath is very high, it is
often hard to correctly predict all labels. Instead, the classifier can
rank or score all the labels. We developed here metrics for such
classifiers with different possible goals, e.g. to predict correctly the
label with the highest score @xmath .

Note that in the following, we use indifferently the notation @xmath to
express the cardinality of a set or the @xmath -norm of a vector.

If only the top scored label has to be correctly predicted, we are
minimizing the one error (Schapire and Singer, 1999 ) which computes the
fraction of labels with the highest score or probability that are
incorrectly predicted:

  -- -------- -- --------------------
     @xmath      ( \thechapter .59)
  -- -------- -- --------------------

If we want to discover all the true labels at the expense of some false
labels, the coverage error (Schapire and Singer, 2000 ) is the metrics
to minimize. It counts the average number of labels with the highest
scores or probabilities to consider to cover all true labels:

  -- -------- -- --------------------
     @xmath      ( \thechapter .60)
  -- -------- -- --------------------

For a label density of @xmath , the best coverage error is thus @xmath
and the worst is @xmath .

If we want to ensure that pairwise label comparisons are correctly made
by the classifier, we will minimize the (pairwise) ranking loss metrics
(Schapire and Singer, 1999 ) . It counts for each sample the number of
wrongly made pairwise comparisons divided by the number of true labels
and false labels

  -- -- -- --------------------
           ( \thechapter .61)
  -- -- -- --------------------

The ranking loss is between @xmath and @xmath . A ranking loss of @xmath
(resp. @xmath ) indicates that all pairwise comparisons are correct
(resp. wrong).

If we want that the classifier gives on average a higher score to true
labels, we will use the label ranking average precision metric (Schapire
and Singer, 2000 ) to assess the accuracy of the models. For each
samples @xmath , it averages over each true labels @xmath the ratio
between (i) the number of true label (i.e. @xmath ) with higher scores
or probabilities than the label @xmath to (ii) the number of labels (
@xmath ) with higher score @xmath than the label @xmath .
Mathematically, we average the LRAP of all pairs of ground truth @xmath
and its associated prediction @xmath :

  -- -- -- --------------------
           ( \thechapter .62)
  -- -- -- --------------------

where

  -- -------- --
     @xmath   
  -- -------- --

The best possible average precision is thus 1. Note that the LRAP score
is equal to fraction of positive labels if all labels are predicted with
the same score or all negative labels have a score higher than the
positive one.

Let us illustrate the computation of the previous metrics with a
numerical example. We compare the ground truth @xmath of @xmath samples
in a label of size @xmath to the probability score @xmath given by the
classifier:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Thresholding @xmath at 0.5 yields the prediction @xmath of the
classifier:

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Here, you will find the detailed computation of all previous metrics:

  -- -- -------- --
        @xmath   
        @xmath   
        @xmath   
        @xmath   
        @xmath   
        @xmath   
        @xmath   
  -- -- -------- --

While the previous metrics are suited to assess multi-label
classification models, we can complement these metrics with those
developed for binary classification tasks, e.g. specifity, precision,
ROC AUC,…(see Section 6.1 ). They are well understood in their
respective domains and have attractive properties such as a good
handling of class imbalance. We extend those metrics in three steps: (i)
we break the ground truth and the model prediction vectors into its
elements, (ii) we concatenate the elements into groups such as all
predictions associated to a given sample or all samples associated to a
given label and (iii) we average the binary classification metrics over
each group. We will focus here on three averaging methods:
macro-averaging, micro-averaging and sample-averaging. Each averaging
method stems from a vision and different sets of assumptions.

If we view the multi-label classification task as a set of independent
binary classification tasks, we compute the metrics @xmath over each
output separately and average the performance over all @xmath labels
leading the macro-averaging version (Yang, 1999 ) of the metrics @xmath
:

  -- -------- -- --------------------
     @xmath      ( \thechapter .63)
  -- -------- -- --------------------

If instead we view each sample as the result of a query (like in a
search engine), we want to evaluate the quality of each query (or
sample) separately. Under this perspective, the sample-averaging
approach (Godbole and Sarawagi, 2004 ) computes and averages the metric
@xmath over each sample separately:

  -- -------- -- --------------------
     @xmath      ( \thechapter .64)
  -- -------- -- --------------------

The micro-averaging approach (Yang, 1999 ) views all label-sample pairs
as forming an unique binary classification task. It compute the metric
@xmath as if all label predictions were independent:

  -- -------- -- --------------------
     @xmath      ( \thechapter .65)
  -- -------- -- --------------------

##### 6.4 Regression metrics

Given a set of @xmath ground truths @xmath and their associated model
predictions @xmath , regression tasks are often assessed using the mean
square error (MSE), the average of the square loss, expressed by

  -- -------- -- --------------------
     @xmath      ( \thechapter .66)
  -- -------- -- --------------------

From the mean square error, we can derive the @xmath score, also called
the coefficient of determination. It is the fraction of variance
explained by the model:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .67)
              @xmath      ( \thechapter .68)
  -- -------- -------- -- --------------------

The @xmath score is normally between 0 and 1. A @xmath score of zero
indicates that the models is no better than a constant, while a @xmath
of one indicates that the model perfectly explains the output given the
inputs. A negative @xmath score might occur and it indicates that the
model is worse than a constant model.

Square-based metrics are highly sensitive to the presence of outliers
with abnormally high prediction errors. The mean absolute error (MAE),
the average of the absolute loss, is often suggested as a robust
replacement of the MSE:

  -- -------- -- --------------------
     @xmath      ( \thechapter .69)
  -- -------- -- --------------------

These single output metrics naturally extend to multi-output regression
tasks. The multi-output mean squared error and mean absolute error for
an output space size @xmath is given by

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .70)
        @xmath      ( \thechapter .71)
  -- -- -------- -- --------------------

These measures average the metrics over all outputs assuming they are
independent.

Similarly, averaging the @xmath score over each output leads to the
macro- @xmath score:

  -- -------- -- --------------------
     @xmath      ( \thechapter .72)
  -- -------- -- --------------------

An alternative extension of the @xmath score is to consider the total
fraction of the output variance, or more strictly the sum of the
variance over each output, explained by the model

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .73)
              @xmath      ( \thechapter .74)
  -- -------- -------- -- --------------------

which is equal to 1 minus the fraction of explained variance (Bakker and
Heskes, 2003 ) .

The variance- @xmath is a variance weighted average of the @xmath score.
We can reformulate the variance- @xmath as:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .75)
              @xmath      ( \thechapter .76)
  -- -------- -------- -- --------------------

with @xmath . By contrast, the macro- @xmath score would have uniform
weights @xmath in Equation \thechapter .76 .

#### 7 Hyper-parameter optimization

Supervised learning algorithms can be viewed as a function @xmath taking
as input a learning set @xmath and a set of hyper-parameters @xmath and
outputting a function @xmath in a hypothesis space @xmath . The
hypothesis space @xmath is defined through one or several
hyper-parameter variables that can be either discrete, like the number
of neighbors for a nearest neighbors model, or continuous, like the
multiplying constant of a penalty loss in penalized linear models.

We need hyper-parameter tuning methods to find the best hyper-parameter
set @xmath that minimizes the expectation of some loss function @xmath
over the joint distribution of input / output pairs @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .77)
  -- -------- -- --------------------

Directly optimizing Equation \thechapter .77 is in general not possible
as it consists in minimizing the generalization error over unseen
samples. Thus, we resort to validation techniques to split the samples
into one (or more) validation set(s) @xmath to estimate the
generalization error (see Section 5 ) and to select the best set of
hyper-parameter @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .78)
  -- -------- -- --------------------

Note that we can optimize a metric defined over a set of samples instead
a loss as the area under the ROC curve.

In its simplest form, the hyper-parameter search assesses all possible
hyper-parameter sets @xmath . While it is optimal on the validation
set(s), this is impractical as the size of the hyper-parameter space
@xmath is often unbounded. The hyper-parameter space often consists of
continuous hyper-parameter variables leading to an infinite number of
possible hyper-parameter sets. Whenever the number of hyper-parameter
sets is finite ( @xmath ), we are limited by computational budget
constraints. Instead, we resort to evaluate a subset of the
hyper-parameter space @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .79)
  -- -------- -- --------------------

The classical approach to design a finite and reduced subspace @xmath is
to sample the hyper-parameter space @xmath through a manually defined
grid. A too coarse grid will miss the optimum hyper-parameter set @xmath
, while a too fine grid will be very costly. In (Hsu et al., 2003 ) ,
Hsu et al. suggests a two-stage approach: (i) a coarse parameter grid
first identifies regions of interest in the hyper-parameter space, and
then a finer grid locates the optimum. Nevertheless, we might still miss
the optimal hyper-parameter set @xmath since the objective function of
Equation \thechapter .79 is not necessarily convex nor concave

How to wrongly optimize and / or to wrongly validate a model? Given a
set of samples @xmath and a supervised learning algorithm @xmath , one
wants simultaneously to find the hyper-parameter set @xmath and estimate
the generalization error of the associated model @xmath . A wrong
approach would be to use directly one of the validation techniques
presented in Section 5 dividing the sample set @xmath into (multiple)
pair(s) of a training set and a test set @xmath . If we select the best
hyper-parameter set @xmath based on the test set(s) @xmath , then the
approximation of the generalization error on @xmath is biased: the
hyper-parameter set @xmath has been selected on the same test set(s).
Another approach would be to repeat independently the described process
using different partitions of the sample set @xmath to first select the
best model and then to estimate the generalization error. However, the
generalization error is still biased: we might use the same samples to
train, to select or to validate the model. The correct approach is to
use nested validation techniques . We first divide the sample set into
(multiple) pair(s) of a test set @xmath and training-validation set
@xmath . Then we again apply a validation technique to split the
training-validation set into (multiple) pair(s) of a training set @xmath
and a validation set @xmath . The models @xmath with hyper-parameter
@xmath are first trained on @xmath , then we select the best
hyper-parameter set @xmath on @xmath . We finally estimate the
generalization error of the overall model training and selection
procedure by re-training a model on @xmath using the best
hyper-parameter set @xmath on the testing set @xmath . Proper validation
is necessary and comes at the expense of the sample efficiency and
computing time. Note that nested validation methods are not needed if we
want solely either to select the best model or to estimate the
generalization error of a given model.

In the grid search approach, we first sample each hyper-parameter
variable and then build all possible combinations of hyper-parameter
sets. However, some of these hyper-parameter variables have no or small
influence on the performances of the models. In these conditions, large
hyper-parameter grids are doomed to fail due to the explosion of
hyper-parameter sets. Random search techniques (Solis and Wets, 1981 )
tackles such optimization problems by (i) defining a distribution over
the optimization variables, (ii) drawing random values from this
distribution and (iii) selecting the best one out of these values.
(Bergstra and Bengio, 2012 ) have shown that random hyper-parameter
search scales better than grid search as the search is not affected by
the hyper-parameter variables having few or no influence on the model
performance. As an illustration, let us consider a model with one
parameter and one without impact on its generalization error. Sampling 9
random hyper-parameter sets would yield more information than making a
@xmath grid as we evaluate 9 different values of the dependent variable
in the random search instead of 3 in the grid.

For a continuous loss and a continuous hyper-parameter space, Bayesian
hyper-parameter optimization (Snoek et al., 2012 ; Bergstra et al., 2011
; Hutter et al., 2011 ) goes beyond random search and models the
performance of a supervised learning algorithm @xmath with
hyper-parameters @xmath . Starting from an initial Gaussian prior over
the hyper-parameter space, it refines a posterior distribution of the
model error with each new tested sets of hyper-parameters. New
hyper-parameter sets are drawn to minimize the overall uncertainty and
the model error.

#### 8 Unsupervised projection methods

Supervised learning aims at finding the best function @xmath which maps
the input space @xmath to the output space @xmath given a set of @xmath
samples @xmath . However with very high dimensional input space, we need
a very high number of samples @xmath to find an accurate function @xmath
. This is the so-called curse of dimensionality. Another problem arises
if the model @xmath is unable to model the input-output relationship
because the model classes @xmath is too restricted, for instance a
linear model will fail to model quadratic data.

Unsupervised projection methods lift the original space @xmath of size
@xmath to another space @xmath of size @xmath . If the projection lowers
the size of the original space ( @xmath ), this is a dimensionality
reduction technique. In the context of supervised learning, we hope to
break the curse of dimensionality with such projection methods while
speeding up the model training. If the projections perform non linear
transformations of the input space, it might also improve the model
performance. For instance, a linear estimator will be able to fit
quadratic data if we enrich the input variables with their quadratic and
bilinear forms. Note that projecting the input space to two or three
dimensions ( @xmath ) is an opportunity to get insights on the data
through visualization.

We present three popular unsupervised projection methods and discuss
their properties: (i) the principal component analysis approach in
Section 8.1 , which aims to find a subspace maximizing the total
variance of the data; (ii) random projection methods in Section 8.2 ,
which project the original space onto a lower dimensional space while
approximately preserving pairwise euclidean distances, and (iii) kernel
functions in Section 8.3 , which compute pairwise sample similarities
lifting the original space to a non linear one.

##### 8.1 Principal components analysis

The principal component analysis (PCA) method (Jolliffe, 2002 ) is a
technique to find from a set of samples @xmath an orthogonal linear
transformation @xmath which maximizes the variance along each axis of
the transformed space as shown in Figure \thechapter .9 .

Principal component analysis reduces the dimensionality of the dataset
by keeping a fraction of the principal components vectors which totalize
a large amount of the total variance. If we keep only two components,
PCA allows to visualize high dimensional datasets as illustrated in
Figure \thechapter .10 with digits.

Mathematically, we want to find the first principal component vector
@xmath which maximizes the variance along its direction:

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      ( \thechapter .80)
  -- -------- -------- -- --------------------

Given that the covariance matrix @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

we have

  -- -------- -- --------------------
     @xmath      ( \thechapter .81)
  -- -------- -- --------------------

where @xmath is the Lagrange multiplier of the normalization constraint.

By derivating with respect to @xmath and setting the first derivative to
zero, we have that the maximum is indeed an eigen vector of the
covariance matrix:

  -- -------- -- --------------------
     @xmath      ( \thechapter .82)
  -- -------- -- --------------------

We also note that the variance along @xmath is given by @xmath . Thus
@xmath is the eigenvector with the highest eigen value.

The following vector @xmath maximizing the variance are obtained by
imposing that the @xmath -th vector is orthogonal to the @xmath previous
one:

  -- -- -------- -- --------------------
        @xmath      
        @xmath      
        @xmath      ( \thechapter .83)
  -- -- -------- -- --------------------

or alternatively in Lagrangian form

  -- -------- -- --------------------
     @xmath      ( \thechapter .84)
  -- -------- -- --------------------

By differentiating with respect to @xmath and multiplying by @xmath , we
have that the Lagrangian constants of the orthogonality constraints are
equal to zero @xmath . And it follows that the @xmath -th principal
component is the @xmath -th eigen vector with the @xmath -th largest
eigen value @xmath since

  -- -------- -- --------------------
     @xmath      ( \thechapter .85)
  -- -------- -- --------------------

##### 8.2 Random projection

Random projection is a dimensionality reduction method which projects
the space onto a smaller random space. For randomly projection, the
Jonhson-Lindenstrauss lemma gives the conditions of existence such that
the distance between pairs of points is approximately preserved.

Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984 ) Given
@xmath and an integer @xmath , let @xmath be a positive integer such
that @xmath . For any sample @xmath of @xmath points in @xmath there
exists a matrix @xmath such that for all @xmath

( \thechapter .86)

Moreover, when @xmath is sufficiently large, several random matrices
satisfy Equation \thechapter .86 with high probability. In particular,
we can consider Gaussian matrices whose elements are drawn i.i.d. in
@xmath , as well as (sparse) Rademacher matrices whose elements are
drawn in the finite set @xmath with probability @xmath , where @xmath
controls the sparsity of @xmath . If @xmath , we will say that those
projections are Achlioptas random projections (Achlioptas, 2003 ) . When
the size of the original space is @xmath and @xmath , then we will say
that we have sparse random projection as in (Li et al., 2006 ) . Note a
random sub-space (Ho, 1998 ) is also a random projection scheme (Candes
and Plan, 2011 ) : the projection matrix @xmath is obtained by
sub-sampling with or without replacement the identity matrix.
Theoretical results proving \thechapter .86 with high probability for
each random projection matrix can be found in the relevant paper.

The choice of the number of random projections @xmath is a trade-off
between the quality of the approximation and the size of the resulting
embedding as illustrated in Figure \thechapter .11 .

##### 8.3 Kernel functions

A kernel function @xmath computes the similarity between pairs of
samples (usually in the input space). Machine learning algorithms
relying solely on dot products, such as support vector machine (Cortes
and Vapnik, 1995 ) or principal components analysis (Jolliffe, 2002 ) ,
are indeed using the linear kernel @xmath . We can kernelize these
algorithms by replacing their dot product with a kernel presented in
Table \thechapter .5 . This is the so called kernel trick (Scholkopf and
Smola, 2001 ) . Kernel functions define non linear projection schemes
lifting the original space to the one defined by the chosen kernel. It
has been used in classification (Hsu et al., 2003 ) , in regression
(Jaakkola and Haussler, 1999 ) and in clustering (Schölkopf et al., 1997
) . Random kernels (Rahimi and Recht, 2007 , 2009 ) can be used to
compress the input space.

The task shown in Figure (b)b requires to classify points belonging to
one of two interleaved moons. Given the non linearities, we can not
linearly separate both classes as illustrated in Figure (b)b with a
(linear) support vector machine. By lifting the linear kernel to the
radial basis function kernel, the support vector machine algorithm now
separates both classes as shown in Figure (c)c . Effectively, kernel
functions enable machine learning algorithms to handle a wide varieties
of structured and unstructured data such as sequences, graphs, texts or
vectors through an appropriate choice and design of kernel functions.

### Chapter \thechapter Decision trees

Outline Decision trees are non parametric supervised learning models
mapping the input space to the output space. The model is a hierarchical
structure made of test and leaf nodes. Starting at the root node, the
top of the tree, the test nodes lead the reasoning through the tree
structure until reaching a leaf node outputting a prediction. In this
chapter, we first describe the decision tree model and show how to
predict unseen samples. Then, we present the methods and the techniques
to grow and to prune these tree structures. We also introduce how to
interpret a decision tree model to gain insights on the studied systems
and phenomena.

A decision tree is comparable to a human reasoning organized through a
hierarchical set of yes/no questions. As in medical diagnosis, an expert
(here the doctor) diagnoses the patient state (“Is the patient healthy
or sick?”) by screening the patient body and by retrieving important
past patient history through a directed set of questions. We can view
each step of the reasoning as a branch of a decision tree structure.
Each answer leads either to another question refining a set of
hypotheses or finally to a conclusion, a prediction.

The binary questions at test nodes can target binary variables, like “Do
you smoke?”, categorical variables, like “Do you prefer pear or apple to
orange or lemon?”, or numerical variables, like “Is the outside
temperature below 25 degree Celsius (77 degree Fahrenheit)?”. Note that
we can formulate multi-way questions as a set of binary questions. For
instance, the multi-way question “Do you want to eat a pear, a peach or
an apple?” is equivalent to ask sequentially “Do you want to eat a pear
or one fruit among peach and apple?”, then you would also ask “Do you
want to eat a peach or an apple?” if you answered “a peach or an apple”.

With only numerical input variables, questions that are typically asked
are in the form “Is the value of variable @xmath lower than a given
value?”. The decision tree is then a geometric structure partionning the
input space into a recursive set of @xmath -dimensional
(hyper)rectangles (also called @xmath -orthotopes). The root node first
divides the whole input space into two half-spaces. Each of those may
further be divided into smaller (hyper)rectangles. The partition
structure highlights the hierarchical nature of a decision tree. An
artistic example of such partitioning in a two dimensional space is the
‘‘Composition II in Red, Blue, and Yellow’’ by Piet Mondrian ⁴ ⁴ 4 Piet
Mondrian (1872-1944) is a painter famous for his grid-based paintings
partitioning the tableau through black lines into colored rectangles
usually blue, red, yellow and white shown in Figure \thechapter .1 .
Here, Piet Mondrian divides hierarchically the whole painting into
colored rectangles through heavy thick black lines. Each black line is
conceptually a testing node of a decision tree, while the colored
rectangles would be the predictions of leaves node.

Decision trees are popular machine learning model, because of several
nice properties:

-   The hierarchical nature of the model takes into account non linear
    effects between the inputs and outputs, as well as conditional
    dependencies among inputs and outputs.

-   Growing a decision tree is computationally fast.

-   Decision tree techniques work with heterogeneous data combining
    binary, categorical or numerical input variables.

-   Decision trees are interpretable models giving insights on the
    relationship between the inputs and the outputs.

-   The tree training algorithm can be adapted to handle missing input
    values (Friedman, 1977 ; Breiman et al., 1984 ; Quinlan, 1989 ) .

In Section 9 , we present the structure of such models and how to
exploit these to predict unseen samples. We show in Section 10 how to
train a decision tree model. In Section 11 , we describe the techniques
used to prune a fully grown decision tree to the right size: too shallow
trees tend to under-fit the data as they might lack predicting power,
while large trees might overfit the data as they are too complex. In
Section 12 , we show how to interpret a decision tree to gain insights
over the input-output relationships: (i) through input variable
importance measures and (ii) through the conversion of the tree
structure to a set of rules. In Section 13 , we show how to extend
decision trees to handle multi-output tasks.

#### 9 Decision tree model

The binary decision tree model is a tree structure built by recursively
partitioning the input space. The root node is the node at the top of
the tree. We distinguish two types of nodes: (i) the test nodes, also
called internal nodes or branching nodes, and (ii) the leaves outputting
predictions, also called external nodes or terminal nodes. A test node
@xmath has two children called the left child and the right child; it
furthermore has a splitting rule @xmath testing whether or not a sample
belongs to its left or right child. For a continuous or categorical
ordered input, the splitting rules are typically of the form @xmath
testing whether or not the input value @xmath is smaller or equal to a
constant @xmath . For a binary or categorical input, the splitting rule
is of the form @xmath testing whether or not the input value @xmath
belongs to the subset of values @xmath .

The decision tree of Figure \thechapter .2 first splits the input space
into two disjoint partitions @xmath and @xmath at the root node @xmath .
The node @xmath has two children: @xmath the left child and @xmath the
right child. The node @xmath is a leaf and thus a terminal partition.
The test node @xmath further splits the input space based on a
categorical set @xmath into partitions @xmath and @xmath with @xmath .
The input space is further split with 3 more testing nodes with two
continuous splitting rules ( @xmath , @xmath ) or one categorical
splitting rule @xmath . The remaining nodes @xmath , @xmath , @xmath ,
@xmath and @xmath are leaf nodes. In total, the decision tree defines a
partition of the input space into 11 (hyper)rectangles ( @xmath ). A one
to one relationship exists between the leaf nodes and the subsets of
this input space partition. Note that all partitions of the input space
are not expressible as a decision tree structure.

A decision tree predicts the output of an unseen sample by following the
tree structure as described by Algorithm 1 . The recursive process
starts at the root node, then the splitting rules of the testing nodes
send the sample further down the tree structure. We traverse the tree
structure until reaching a leaf, a terminal node, outputting its
associated prediction value. A decision tree model @xmath is then
expressible as a sum of indicator functions, denoted by @xmath , over
the @xmath tree nodes:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where @xmath is the prediction associated to a node @xmath . The
computational complexity of predicting an unseen sample is thus
proportional to the depth of the followed branch.

1: function tree_predict ( @xmath , @xmath )

2: @xmath Index of the root node of the tree.

3: while the node @xmath is not a leaf. do

4: if The splitting rule @xmath of node @xmath is true then

5: @xmath Index of the left child of node @xmath .

6: else

7: @xmath Index of the right child node @xmath .

8: end if

9: end while

10: return @xmath .

11: end function

Algorithm 1 Predict a sample @xmath with a decision tree.

Binary versus multi-way partitions Decision trees do not have to respect
a binary tree structure. Each one of their internal nodes could have
more than two children with multi-way splitting rules. However such
multi-way partitions are equivalent to a set of consecutive binary
partitions. Furthermore, multi-way splits tends to quickly fragment the
training data during the decision tree growth impeding its
generalization performance. In practice, decision trees are therefore
most of the time restricted to be binary. (Hastie et al., 2009 )

#### 10 Growing decision trees

We grow a decision tree using a set of samples of input-output pairs.
Tree growth starts at the root node and divides recursively the input
space through splitting rules until we reach a stopping criterion such
as a maximal tree depth or minimum sample size in a node. For each new
testing node, we search for the best splitting rule to divide the sample
set at that node into two subsets. We hope to make partitions “purer” at
each new division. The decision tree growing procedure has three main
elements:

-   a splitting rule search algorithm (see Section 10.1 );

-   stop splitting criteria (see Section 10.3 ) which dictate whenever
    we stop the development of a branch;

-   a leaf labelling rule (see Section 10.2 ) determining the output
    value of a terminal partition.

Putting all those key elements together leads to the decision tree
growing algorithm shown in Algorithm 2 .

1: function grow_tree ( @xmath )

2: @xmath EmptyQueue ()

3: Initialize the tree structure with the root node ( @xmath )

4: @xmath . enqueue ( @xmath ).

5: while @xmath is not empty do

6: @xmath . dequeue ( )

7: if Node @xmath satisfies one stopping criterion then

8: Label node @xmath as a leaf using samples @xmath

9: else

10: Search for the best splitting rule @xmath using samples @xmath .

11: Split @xmath into @xmath and @xmath using the splitting rule @xmath
.

12: Label node @xmath as a test node with the splitting rule @xmath .

13: @xmath . enqueue (( @xmath )).

14: @xmath . enqueue (( @xmath )).

15: end if

16: end while

17: return The grown decision @xmath .

18: end function

Algorithm 2 Grow a decision tree using the sample set @xmath .

##### 10.1 Search among node splitting rules

During tree growing, we recursively partition the input space @xmath and
the sample set @xmath . At each testing node @xmath , we split the
sample set @xmath reaching node @xmath into two smaller subsets @xmath
and @xmath using a binary splitting rule @xmath as shown in Figure
\thechapter .3 . This raises two questions (i) what is the set of
available binary and axis-wise splitting rules @xmath given a sample set
@xmath and (ii) how to select the best one among all of them so as to
make the descendants “purer” than the parent node.

For a variable @xmath of cardinality @xmath , the associated family
@xmath of splitting rules consists of all possible subsets of @xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

The size of the splitting rule family is increasing exponentially with
the total number of possible values ( @xmath ).

If the possible values of the variable @xmath are also ordered, we can
reduce the size of the splitting rule family @xmath from an exponential
number of candidates to a linear number of splitting rules ( @xmath ):

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .3)
  -- -------- -------- -- -------------------

With a numerical variable @xmath , the number of possible splitting
rules is infinite. However, the training set is of finite size. We
consider a family of splitting rules similar to Equation \thechapter .3
with the possible values @xmath available in the training set.

The selected splitting rule @xmath should split the sample set @xmath
such that the following conditions hold: the sample sets @xmath and
@xmath are non empty ( @xmath , @xmath ) forming a disjoint ( @xmath )
and non overlapping partition ( @xmath ) of the original sample set
@xmath . During the expansion of a test node @xmath into a left child
and a right child, we thus select a splitting rule @xmath among all
possible splitting rules @xmath :

  -- -------- -------- -- -------------------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      ( \thechapter .4)
  -- -------- -------- -- -------------------

We strive to select the “best” possible local splitting rule @xmath for
the split at node @xmath leading ideally to good generalization
performance. However, it is impossible to minimize directly the
generalization error. Thus instead, we are going to minimize the
resubstitution error, the error over the training set. However,
obtaining such a tree is trivial and it has poor generalization
performance. A more meaningful criterion is to search for the smallest
tree minimizing the resubstitution error. However, this optimization
problem is a NP-complete (Hyafil and Rivest, 1976 ) . Instead, we
greedily grow the tree by maximizing the reduction of an impurity
measure function @xmath . Mathematically, we define the impurity
reduction @xmath obtained by dividing the sample set @xmath into two
partitions @xmath as

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

The splitting rule selection problem (line 10 of the tree growing
Algorithm 2 ) is thus written as

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

Intuitively, the impurity @xmath should be minimal whenever all samples
have the same output value. The node is then said to be “pure”

Given the additivity of the impurity reduction, the best split at node
@xmath according to the impurity reduction computed locally is also the
best split at this node in terms of global impurity. The remaining tree
impurity @xmath of a tree @xmath is the sum of the remaining impurities
of all leaf nodes:

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

with @xmath the proportion of samples reaching node @xmath . If we
develop the leaf node @xmath into a test node, it leads to a new tree
@xmath with a new splitting rule @xmath having a left @xmath and a right
@xmath children node. The overall impurity decreases from the original
tree @xmath to the bigger tree @xmath and the impurity decrease is given
by

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .8)
              @xmath      ( \thechapter .9)
  -- -------- -------- -- -------------------

Thus, the decision tree growing procedure is a repeated process aiming
at decreasing the total impurity as quickly as possible by suitably
choosing the local splitting rules.

In classification, a node is pure if all samples have the same class.
Given a sample set @xmath reaching node @xmath , we will denote by
@xmath the proportion of samples reaching node @xmath having the class
@xmath . A node will be pure if @xmath is equal to 1 for a class @xmath
and zero for the others. The node impurity should increase whenever
samples with different classes are mixed together. We require that the
impurity measure @xmath in classification satisfies the following
properties:

1.  @xmath is minimal only whenever the node is pure @xmath and @xmath ;

2.  @xmath is maximal only whenever @xmath ;

3.  @xmath is a symmetric function with respect to the class proportions
    @xmath so as not to favor any class.

A first function satisfying those three properties is the
misclassification error rate:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

However, this is not an appropriate criterion. In practice, many
candidate splitting rules have often the same error rate reduction,
especially in the multi-class classification where only the number of
samples of the majority class matters.

Consider the following split selection problem, we have a binary
classification task with 500 negative and 500 positive samples. The
first splitting rule leads to a left child with 125 positive and 375
negative samples, while the right child has 375 positive and 125
negative samples. The misclassification error reduction is thus given
by:

  -- -------- --
     @xmath   
  -- -------- --

Now, let’s consider a second splitting rule leading to a pure node with
250 positive samples and another node with 250 positive and 500 negative
samples. This second split has the same impurity reduction score leading
to a tie:

  -- -------- --
     @xmath   
  -- -------- --

The misclassification does not discriminate enough node purity as it
varies linearly with the fraction of the majority class. To solve this
issue, we add a fourth required properties to classification impurity
functions (Breiman et al., 1984 ) :

1.  @xmath must be a strictly concave function with respect to the class
    proportion @xmath .

This fourth property will increase the granularity of impurity reduction
scores leading fewer ties in splitting rule scores. It will reduce the
tree instability with respect to the training set.

Two more suitable classification impurity criteria satisfying all four
properties are the Gini index, a statistical dispersion measure, and the
entropy measure, an information theory measure.

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .11)
     @xmath   @xmath      ( \thechapter .12)
  -- -------- -------- -- --------------------

By minimizing the Gini index, we minimize the class dispersion. While
selecting the splitting rule based on the entropy measure minimizes the
unpredictability of the target, the remaining unexplained information of
the output variable.

Given the strict concavity of the Gini index and entropy, we can now
discriminate the two splits of the previous example. The first split
would have an impurity reduction with the Gini index of @xmath and the
entropy of @xmath . The second split with a pure node would have an
impurity reduction of @xmath with the Gini index and of @xmath with the
entropy. Based on these criteria, both measures would choose the second
split.

In regression tasks, we consider a node as pure if the dispersion of the
output values is zero. We require the impurity regression criterion to
be zero only if all output values have the same value. A common
dispersion measure used to grow regression trees is the empirical
variance

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

By maximizing the variance reduction, we are searching for a splitting
rule minimizing the square loss @xmath . Note that the Gini index and
the empirical variance lead to the same impurity measure for binary
classification tasks and multi-label classification tasks with output
classes encoded with @xmath numerical variables.

##### 10.2 Leaf labelling rules

When tree growing is stopped by the activation of a stop splitting
criterion, the newly created leaf needs to be labeled by an output value
(see line 8 of Algorithm 2 ). It is a constant @xmath chosen to minimize
a given loss function @xmath over the samples @xmath reaching the node
@xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .14)
  -- -------- -- --------------------

In regression tasks, we want to find the constant @xmath minimizing the
square loss in single output regression :

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

By setting the first derivative to zero, we have

  -- -------- -- --------------------
     @xmath      ( \thechapter .16)
     @xmath      ( \thechapter .17)
  -- -------- -- --------------------

The constant leaf model minimizing the square loss is the average output
value of the samples reaching node @xmath .

In classification tasks, the constant @xmath minimizing the @xmath loss
( @xmath ) is the most frequent class. The decision tree can also be a
class probability estimator by outputting the proportion @xmath of
samples of class @xmath reaching node @xmath from the sample set @xmath
.

Beyond constant leaf modeling The leaf labelling rule can go beyond a
constant model with supervised learning models such as linear models
(Quinlan et al., 1992 ; Wang and Witten, 1996 ; Frank et al., 1998 ;
Landwehr et al., 2005 ) , kernel-based methods (Torgo, 1997 ) ,
probabilistic models (Kohavi, 1996 ) or even tree-ensemble models
(Matthew et al., 2015 ) . It increases the modeling power of the
decision tree at the expense of computing time and new hyper-parameters.

##### 10.3 Stop splitting criterion

The tree growth at a node @xmath naturally stops if all the samples
@xmath reaching the node @xmath (i) share the same output value (zero
impurity) or (ii) share the same input values (but not necessarily the
same output) as in this case we can not find a valid split of the data.
In both cases, we can not find a splitting rule to grow the tree further
due to a lack of data.

A tree developed in such ways is then said to be fully developed. The
question is “Should we stop sooner the tree growth?”. A testing node
@xmath splits the data @xmath into two partitions @xmath leading to a
left child node @xmath and a right child node @xmath . If we denote by
@xmath , @xmath and @xmath the leaf models that would be assigned to the
nodes @xmath , @xmath or @xmath , we have that the resubstition error
reduction @xmath associated to a loss function @xmath is given by:

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      ( \thechapter .18)
              @xmath      
              @xmath      ( \thechapter .19)
  -- -------- -------- -- --------------------

Since we choose @xmath and @xmath so as to minimize the resubstitution
error on their respective training data ( @xmath and @xmath ), the
resubstitution error never increases through node splitting. With only
“natural” splitting rule, decision trees are optimally fitting the
training data.

Stop splitting criteria avoid over-fitting by stopping earlier the tree
growth. They are either based on (i) structural properties or on (ii)
data statistics. Criteria computed on the left and the right children
can also discard splitting rules, for example requiring a minimal number
of samples in the left and right children to split a node.

Structural-based stop splitting criteria regularize the tree growth by
explicitly limiting the tree complexity, by restricting for example:

-   branch depths or

-   the total number of nodes.

In the second case, the order in which the tree nodes are split starts
to matter and can be chosen so as to maximize the total impurity
reduction.

Data-based stop splitting criteria stop the tree growth if some statical
properties computed on the data used to split the node are below a
threshold such as

-   the number of samples reaching the node,

-   the number of samples reaching the left and right children obtained
    after splitting,

-   the impurity reduction or

-   the p-value of a significance test, such as a Chi-square test,
    testing the independence of the split and the output variable.

#### 11 Right decision tree size

To find the right decision tree size, there are two main families of
complexity reduction techniques, also called pruning techniques: (i)
pre-pruning techniques stop the tree growth before the tree is fully
developed (line 5 of Algorithm 2 and presented in Section 10.3 ) and
(ii) post-pruning techniques remove tree nodes a posteriori setting a
trade-off between the tree size and the resusbstitution error. Both
approaches lead to smaller decision trees aiming to improve
generalization performance and to simplify decision tree interpretation.

Pre-pruning criteria are straightforward tools to control the decision
tree size. However, it is unclear which pruning level (or
hyper-parameter values) leads to the best generalization performance.
Too “strict” stop splitting criteria will grow shallow trees
under-fitting the data. While too “loose” stop splitting criteria have
the opposite effect, i.e., growing overly complex trees over-fitting the
data.

While pre-pruning techniques select the tree complexity a priori,
post-pruning techniques select the optimal complexity a posteriori. A
naive approach to post-pruning would be to build independently a
sequence of decision trees with different complexity by varying the stop
splitting criteria, and then to select the one minimizing an
approximation of the generalization error such as the error on a hold
out sample set. However, this is not computationally efficient as it
re-grows each time a (new) decision tree.

Post-pruning techniques first grow a single decision tree @xmath with
very loose or no stop splitting criterion. This decision tree clearly
overfits the training data. Then, they select a posteriori a subtree
@xmath among all possible subtrees of @xmath . The original decision
tree is thus pruned by collapsing nodes from the original tree into new
leaf nodes. The post-pruning method minimizes a tradeoff between the
decision tree error over a sample set @xmath and a function measuring
the decision tree complexity such as the number of nodes:

  -- -------- -- --------------------
     @xmath      ( \thechapter .20)
  -- -------- -- --------------------

The cost complexity pruning method (Breiman et al., 1984 ) , also known
as the weakest link pruning, implements Equation \thechapter .20 through
a complexity coefficient @xmath measuring a tradeoff between the
resubstitution error of a tree @xmath and its complexity @xmath defined
by the number of leaves:

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

For each @xmath , there exists a unique tree @xmath minimizing the cost
complexity coefficient @xmath . Large values of @xmath lead to small
trees, while conversely small values of @xmath allow bigger sub-trees.
For the extreme case @xmath (resp. @xmath ), we have the original
decision tree @xmath (resp. the subtree containing only the root node).
One can show (Breiman et al., 1984 ) that we can sequentially obtain the
@xmath from the original tree @xmath by removing the node with the
smallest increase in resubtitution error. We select the optimal subtree
@xmath among all subtrees @xmath by minimizing an approximation of the
generalization error using for instance cross-validation methods.

The reduced error pruning method (Quinlan, 1987 ) , another post pruning
technique, splits the learning set into a training set and a pruning
set. It grows on the training set a large decision tree. During the
pruning phase, it first computes the error reduction of pruning each
node and its descendants on the pruning set, then removes greedily the
node reducing the most the error. It repeats this two steps procedure
until the error on the pruning set starts increasing.

Other post pruning methods have been developed with pruning criteria
based on statistical procedures (Quinlan, 1987 , 1993 ) or on cost
complexity criteria based on information theory (Quinlan and Rivest,
1989 ; Mehta et al., 1995 ; Wallace and Patrick, 1993 ) . Instead of
relying on greedy processes, authors (Bohanec and Bratko, 1994 ;
Almuallim, 1996 ) have proposed dynamic programming algorithms to find
an optimal sub-tree sequence minimizing the resubtistution error with
increasing tree complexity at the expense of computational complexity.

#### 12 Decision tree interpretation

A strength of the decision tree model is its interpretability. A closer
inspection reveals that we can convert a decision tree model to a set of
mutually exclusive classification or regression rules. We get these
rules by following the path from each leaf to the root node. We have
converted the decision tree shown in Figure \thechapter .4 to three sets
of predicting rules, one for each class of iris flower (Versicolor,
Virginica and Setosa):

1.  “If Petal width @xmath 0.7cm, then Setosa”

2.  “If Petal width ¿ 0.7cm and Petal width @xmath 1.65cm and Petal
    length @xmath 5.25cm, then Versicolor.”

3.  “If Petal width ¿ 0.7cm and Petal width @xmath 1.65cm and Sepal
    length @xmath 5.95cm and Sepal length @xmath 5.85cm, then
    Versicolor.”

4.  “If Petal width ¿ 0.7cm and Petal width @xmath 1.65cm and Petal
    length @xmath 5.25cm, then Virginica.”

5.  “If Petal width ¿ 0.7cm and Petal width @xmath 1.65cm and Sepal
    length @xmath 5.95cm and Sepal length @xmath 5.85, then Virginica.”

6.  “If Petal width ¿ 0.7cm and Petal width @xmath 1.65cm and Sepal
    length @xmath 5.95cm, then Virginica.”

Remark that given the binary hierarchical structure, some rules are
redundant and can be further simplified. For instance, we can collapse
the constraints “Petal width ¿ 0.7cm and Petal width @xmath 1.65cm” into
“Petal width @xmath 1.65cm” for the 6 -th rule.

The decision tree model also shows which input variables @xmath are
important to predict the output variable(s) @xmath . During the decision
tree growth, we select at each node @xmath an axis-wise splitting rules
@xmath minimizing the reduction of an impurity measure @xmath dividing
the samples @xmath reaching the node into two subsets @xmath . The mean
decrease of impurity (MDI) of a variable @xmath (Breiman et al., 1984 )
sums, over the nodes of a decision tree @xmath where @xmath is used to
split, the total reduction of impurity associated to the split weighted
by the probability of reaching that node over the training set @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

The mean decrease of impurity scores and ranks the input variables
according to their importances during the decision tree growth process
as illustrated in Figure \thechapter .5 . It takes into account variable
correlations, multivariate and non linear effects. As such, decision
trees are often used as pre-processing tools to select a fraction of the
top most important variables.

#### 13 Multi-output decision trees

Decision trees naturally extend from single output tasks to multiple
output tasks (Segal, 1992 ; De’Ath, 2002 ; Blockeel et al., 2000 ; Clare
and King, 2001 ; Zhang, 1998 ; Vens et al., 2008 ; Noh et al., 2004 ;
Siciliano and Mola, 2000 ) such as multi-output regression, multi-label
classification or multi-class classification. No core modification are
needed. Instead, we need appropriate impurity measures and leaf
labelling rules for the tree prediction Algorithm 1 and the tree growth
Algorithm 2 . Note that a multi-output decision tree can still be pruned
(Struyf and Džeroski, 2005 ) .

###### Multi-output impurity measures

During the decision tree growth, we aim to select a splitting rule
dividing the sample set @xmath reaching the node @xmath into a left and
a right sample sets @xmath . The best multi-output splitting rule is the
one maximizing the reduction of a multi-output impurity measure @xmath .
We can use native multi-output impurity measures such as the variance in
regression (Segal, 1992 ) :

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

or any impurity criterion derived from an appropriate distance measure
(Blockeel et al., 2000 ) .

We can also extend known impurity measures, such as the Gini index or
the entropy (see Section 10.1 ), by summing the impurity measures over
each output (De’Ath, 2002 ) :

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

where @xmath is a sample set.

Since we can define an impurity measure on any set of outputs, we can
derive the mean decrease of impurity MDI (see Section 12 ) either on all
or a subset of the outputs.

###### Leaf labelling and prediction rule

In the multi-output context, the leaf prediction @xmath of a node @xmath
is a constant vector of output values chosen so as to minimize a
multi-output loss function @xmath over the samples @xmath reaching the
node:

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

In multi-output regression, the loss @xmath is commonly the @xmath -norm
loss @xmath the multi-output extension of the square loss. The constant
@xmath minimizing the @xmath -norm loss is the average output vector

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

Whenever we extend the label rule assignment to multi-label and to
multi-output classification tasks, there are two common possibilities
either minimizing the subset @xmath loss which is equal to zero if only
if all outputs are correctly predicted @xmath and the Hamming loss which
counts among the @xmath outputs the number of wrongly predicted outputs
@xmath .

Minimizing the subset @xmath loss takes into account output
correlations. The constant vector @xmath minimizing this loss is the
most frequent output value combination. Note that the constant @xmath
might not be unique as we might have several output combinations with
the same frequency of appearance in the sample set reaching the leaf.

The Hamming loss makes the assumption that all outputs are independent.
The constant @xmath minimizing this loss is the vector containing the
most frequent class of each output.

When the trees are fully developed with only pure leaves, minimizing the
Hamming loss or the subset @xmath loss leads to identical leaf
prediction rules.

### Chapter \thechapter Bias-variance and ensemble methods

Outline Ensemble methods combine supervised learning models together so
as to improve generalization performance. We present two families of
ensemble methods: averaging methods and boosting methods. Averaging
ensembles grow independent unstable estimators and average their
predictions. Boosting methods increase sequentially their total
complexity by adding biased and stable estimators. In this chapter, we
first show how to decompose the generalization error of supervised
learning estimators into their bias, variance and irreducible error
components. Then we show how to exploit averaging techniques to reduce
variance and boosting techniques to sequentially decrease bias.

Ensemble methods fit several supervised learning models instead of a
single one and combine their predictions. The goal is to reduce the
generalization error by solving the same supervised learning task
multiple times. We hope that the errors made by the different models
will compensate and thereby improve the overall accuracy whenever we
consider them together.

Real life examples of “ensemble methods” in the human society are
democratic elections. Each eligible person is asked to cast its vote for
instance to choose between political candidates. This approach considers
each person of the committee as an independent expert and averages
simultaneously their opinions. In supervised learning, these kinds of
voting mechanism are called “averaging methods”.

Instead of querying all experts independently, we can instead collect
their opinions sequentially. We ask to each new expert to refine the
predictions made by the previous ones. The expert sequence is chosen so
that each element of the sequence improves the accuracy focusing on the
unexplained phenomena. For instance in medical diagnosis, a person
itself is the first one to assess its health status. The next expert in
the line is the general practitioner followed by a series of
specialists. We call these ensemble methods “boosting methods”.

The “averaging” approach aims to reduce the variability in the expert
pool by averaging their predictions. At the other end, the “boosting”
approach carefully refines its predictions by cumulating the predictions
of each expert.

In Section 14 , we show how to decompose the error made by supervised
learning models into three terms: a variance term due to the variability
of the model with respect the learning sample set, a bias term due to a
lack of modeling power of the estimator and an irreducible error term
due to the nature of the supervised learning task. Averaging methods
presented in Section 15 are variance reducing techniques growing
independently supervised learning estimators. In Section 16 , we show
how to learn sequentially a series of estimators through boosting
methods increasing the overall ensemble complexity and reducing the
ensemble model bias.

#### 14 Bias-variance error decomposition

The expected error or generalization error @xmath associated to a loss
@xmath of a supervised learning algorithm is a random variable depending
on the learning samples @xmath drawn independently and identically from
a distribution @xmath and used to fit a model @xmath in a hypothesis
space @xmath . We want here to analyze the expectation of the
generalization error @xmath over the distribution of learning samples
defined as follows:

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .1)
              @xmath      ( \thechapter .2)
  -- -------- -------- -- -------------------

Let us denote the Bayes model by @xmath , the best model possible
minimizing the generalization error:

  -- -------- -- -------------------
     @xmath      ( \thechapter .3)
  -- -------- -- -------------------

For the squared loss @xmath , we can decompose the expected error over
the learning set @xmath into three terms (see (Geman et al., 1992 ) for
the proof):

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .4)
              @xmath      ( \thechapter .5)
  -- -------- -------- -- -------------------

where

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .6)
     @xmath   @xmath      ( \thechapter .7)
  -- -------- -------- -- -------------------

We interpret each term of Equation \thechapter .5 as follows:

-   The variance of a supervised learning algorithm @xmath describes the
    variability of the model with a randomly drawn learning sample set
    @xmath . Supervised learning algorithms with a high variance have
    often a high complexity which makes them overfit the data.

-   The square bias @xmath is the distance between the average model and
    the Bayes model. Biased models are not complex enough to model the
    input-output function. They are indeed underfitting the data.

-   The irreducible error @xmath is the variance of the target around
    its true mean. It is the minimal attainable error on a supervised
    learning problem.

The bias-variance decomposition allows to analyze and to interpret the
effect of hyper-parameters. It highlights and gives insights on their
effects on the bias and the variance. In Figure \thechapter .1 , we have
fitted decision tree models with an increasing number of leaves on the
Friedman1 dataset (Friedman, 1991 ) , a simulated regression dataset. We
first assess the resubstitution error over 300 samples and an
approximation of the generalization error, the hold out error, computed
on an independent testing set of 20000 samples. We compute these errors
(see Figure (a)a ) by averaging the performance of decision tree models
over 100 learning sets @xmath drawn from the same distribution @xmath .
By increasing the number of leaves, the resubstitution error decreases
up to zero with fully developed trees. On the other hand, the hold out
error starts increasing beyond 20 leaves indicating that the model is
under-fitting with less than 20 leaves and over-fitting with more than
20 leaves. By increasing the number of leaves, we decrease the bias as
we grow more complex models as shown in Figure (b)b . It also increases
the variance as the tree structures become more unstable with the
learning set @xmath .

In general, we have the following trends for a single decision tree
model. Large decision trees overfit and are unstable with respect to the
learning set @xmath which corresponds to a high variance and a small
bias. Shallow decision trees, on the other hand, underfit the learning
set @xmath and have stable structures, which corresponds to a small
variance and a high bias. The pruning technique presented in Section 11
allows to select a tradeoff between the variance and the bias of the
algorithm by adjusting the tree complexity.

The bias-variance decomposition of the square loss is by far the most
studied decomposition, but there nevertheless exist similar
decompositions for other losses, e.g. see (Domingos, 2000 ) for a
decomposition of the polynomial loss @xmath , see (Friedman, 1997 ;
Kohavi et al., 1996 ; Tibshirani, 1996a ; Domingos, 2000 ) for the
@xmath loss, or see (James, 2003 ) for losses in general.

#### 15 Averaging ensembles

An averaging ensemble model @xmath builds a set of @xmath supervised
learning models @xmath , instead of a single one. Each model @xmath of
the ensemble is different as we randomize and perturb the original
supervised learning algorithm at fitting time. We describe entirely the
induced randomization of one model @xmath by drawing i.i.d. a random
vector of parameters @xmath from a distribution of model parameters
@xmath .

In regression, the averaging ensemble predicts an unseen sample by
averaging the predictions of each model of the ensemble:

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

It minimizes the square loss (or its extension the @xmath -norm loss)
between the ensemble model and its members:

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

In classification, the averaging ensemble combines the predictions of
its members to minimize the 0-1 loss by a majority vote of all its
members:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

An alternative approach, called soft voting, is to classify according to
the average of the probability estimates @xmath provided by the ensemble
members:

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

Both approaches have been studied and yield almost exactly the same
result, but soft voting provides smoother probability class estimates
than majority vote (Breiman, 1996a ; Zhou, 2012 ) . The multi-output
extension to ensemble predictions often minimizes the Hamming loss
applying either soft-voting or majority voting to each output
independently. Minimizing the subset @xmath loss for multi-label tasks
would lead to predict the most frequent label set.

Ambiguity decomposition The ambiguity decomposition (Krogh et al., 1995
) of the square loss shows that the generalization error of an ensemble
@xmath of @xmath models @xmath is always lower or equal than the average
generalization error @xmath of its members:

@xmath ( \thechapter .12)

with

@xmath @xmath ( \thechapter .13) @xmath ( \thechapter .14)

The ambiguity term @xmath is the variance of the ensemble around its
average model @xmath . The equality occurs only if all average models
are identical @xmath .

Averaging ensemble models are obtained by first perturbing supervised
learning models and then combining them. They aim to reduce the
generalization error of the ensemble compared to the original single
model by reducing the variance of the learning algorithm. Let us
illustrate the effects of an averaging method called bagging on the bias
and variance of fully grown decision trees. The bagging method fits each
estimator of the ensemble on a bootstrap copy of the learning set. In
Figure \thechapter .2 , we show the variance and the bias as function of
the number of fully grown decision trees in the bagging ensemble. With a
single decision tree, the variance is the dominating error component. By
increasing the size of the Bagging ensemble, the variance and the hold
out error are reduced, while leaving the bias mostly unchanged.

In Section 15.1 , we show how the bias-variance decomposition of a
randomized supervised learning algorithm is affected by the ensemble
averaging method. In Section 15.2 , we present how to induce
randomization without modifying the original supervised learning
algorithm. In Section 15.3 , we describe specific randomization schemes
for decision tree methods leading to random forest models.

##### 15.1 Variance reduction

Let us first study the bias-variance decomposition for a model @xmath
trained on a learning set @xmath whose randomness is entirely captured
by a random vector of parameters @xmath . The model @xmath is thus a
function of two random variables @xmath and the learning set @xmath . It
admits the following bias-variance decomposition of the generalization
error for the average square loss (Geurts, 2002 ) :

  -- -------- -- --------------------
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

where

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .16)
     @xmath   @xmath      ( \thechapter .17)
  -- -------- -------- -- --------------------

By comparison to the bias-variance decomposition of an unperturbed model
(see Equation \thechapter .15 ), we have two main differences:

1.  The squared bias is now the distance between the Bayes model @xmath
    and the average model @xmath over both the learning set @xmath and
    the randomization parameter @xmath . Note that the average model of
    the randomized algorithm is different from the non-randomized model
    @xmath . The randomization of the original algorithm might increase
    the squared bias.

2.  The variance @xmath of the algorithm now depends on the two random
    variables @xmath and @xmath . With the law of total variance, we can
    further decompose the variance term into two terms:

      -- -------- -- --------------------
         @xmath      ( \thechapter .18)
      -- -------- -- --------------------

    The first term is the variance brought by the learning sets of the
    average model over all parameter vectors @xmath . The second term
    describes the variance brought by the parameter vector @xmath
    averaged over all learning sets @xmath .

Now, we can study the bias-variance decomposition of the generalization
error for an ensemble model @xmath whose constituents @xmath depend each
on the learning set @xmath and a random parameter vector @xmath
capturing the randomness of the models. The bias-variance decomposition
of the ensemble model @xmath is given by

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      ( \thechapter .19)
  -- -------- -------- -- --------------------

Let us compare the decomposition for a single random model (Equations
\thechapter .15 - \thechapter .18 ) to the decomposition for an ensemble
of random models (Equation \thechapter .19 ). We are going to expand the
bias-variance decomposition using the ensemble prediction formula @xmath
(the demonstration follows (Geurts, 2002 ) ). As previously, the
variance @xmath is irreducible as this term does not depend on the
supervised learning model.

The average ensemble model of an ensemble of randomized models is equal
to the average model of a single model @xmath of random parameter vector
@xmath :

  -- -- -------- -- --------------------
        @xmath      ( \thechapter .20)
        @xmath      ( \thechapter .21)
  -- -- -------- -- --------------------

The squared bias of the ensemble is thus unchanged compared to a single
randomized model.

Now, let us consider the two variance terms. The first one depends on
the variability of the learning set @xmath . With an ensemble of
randomized models, it becomes:

  -- -------- -- --------------------
     @xmath      
     @xmath      ( \thechapter .22)
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

The variance of the ensemble of randomized model with respect to the
learning set @xmath drawn from the input-output pair distribution @xmath
is not affected by the averaging and is equal to the variance of a
single randomized model.

Let us developed the second variance term of the decomposition
describing the variance with respect to the set of random parameter
vectors @xmath :

  -- -------- -- --------------------
     @xmath      
     @xmath      ( \thechapter .24)
     @xmath      ( \thechapter .25)
     @xmath      ( \thechapter .26)
     @xmath      ( \thechapter .27)
  -- -------- -- --------------------

where we use the following properties: (i) @xmath where @xmath is a
constant, (ii) at a fixed learning set @xmath , the models @xmath are
independent, (iii) the variance of a sum of independent random variables
is equal to the sum of the variance of each independent random variables
( @xmath ).

Putting all together the bias-variance decomposition of Equation
\thechapter .19 becomes (Geurts, 2002 ) :

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      ( \thechapter .28)
  -- -------- -------- -- --------------------

The bias variance decomposition of an ensemble of randomized models
@xmath (see Equation \thechapter .28 ) shows that averaging @xmath
models reduces the variance related to the randomization @xmath by a
factor @xmath over a single randomized model @xmath (see Equation
\thechapter .15 ) without modifying the other terms. Note that we can
not compare the bias variance decomposition of an ensemble of randomized
models @xmath to its non randomized counterparts @xmath (see Equation
\thechapter .5 ). The bias and variance terms are indeed not comparable.

In practice, we first perturb the learning algorithm which increases the
variance of the models and then we combine them through averaging. The
variance reduction effect is expected to be higher than the added
variance at training time. The bias is either unaffected or increased
through the randomization induction. Perturbing the algorithm is thus a
tradeoff between the reduction in variance and the increase in bias. An
ensemble of randomized models @xmath will have better performance than
its non perturbed counterparts @xmath if the bias increase is
compensated by the variance reduction. In (Louppe, 2014 ) , the authors
have shown that the generalization error is reduced if the randomization
induction decorrelates the models of the ensemble.

The previous decomposition does not apply to the @xmath loss in
classification. However, the main conclusions remains valid (Breiman,
1996a ; Domingos, 2000 ; Geurts, 2002 ) .

##### 15.2 Generic randomization induction methods

In this section, we first discuss generic randomization methods to
perturb a supervised learning algorithm without modifying the original
algorithm through (i) the learning sample set @xmath available at
fitting time, (ii) the input space @xmath or (iii) the output space
@xmath . Those three perturbation principles can be either applied
separately or together.

We present in succession these three model agnostic randomization
principles (perturbing @xmath , @xmath or @xmath ).

###### 15.2.1 Sampling-based randomization

One of the earliest randomization method, called bagging (Breiman, 1996a
) , fits independent models on bootstrap copies of the learning set. A
bootstrap copy (Efron, 1979 ) is obtained by sampling with replacement
@xmath samples from the learning set @xmath . The original motivation
was a first theoretical development and empirical experiments showing
that bagging reduces the error of an unstable estimator such as a
decision tree. Bootstrap sampling totally ignores the class distribution
in the original sample set @xmath and might lead to highly unbalanced
bootstraps. A partial solution is to use stratified bootstraps or to
bootstrap (Chen et al., 2004 ) separately the minority and majority
classes. In the bagging approach only a fraction of the dataset is
provided as training set to each estimator, the wagging approach (Bauer
and Kohavi, 1999 ) fits instead each estimator on the entire training
set with random weights. Instead of using bootstrap copies of training
set, Büchlmann and Yu ( 2002 ) proposes to subsample the training set,
i.e. to sample without replacement the training set.

To take into account the input space structure, Kuncheva et al. ( 2007 )
proposes to generate random input space partition with a random
hyperplane. An estimator is then build for each partition. To get an
ensemble, Kuncheva et al. ( 2007 ) repeats this process multiple times.

###### 15.2.2 Input-based randomization

Input-based randomization techniques are often based on dimensionality
reduction techniques. The random subspace method (Ho, 1998 ) builds each
model on a random subset of the input space @xmath obtained by
sub-sampling inputs without replacement. It was later combined with the
bagging method in (Panov and Džeroski, 2007 ) , by bootstrapping the
learning set @xmath before learning an estimator, and with sub-sampling
techniques with/without replacement in (Louppe and Geurts, 2012 )
generating random (sample-input) patches of the data. Note that while we
reduce the input space size, we can also over-sample the learning set.
For instance, Maree et al. ( 2005 ) apply a supervised learning
algorithm on random sub-windows extracted from an image, which
effectively (i) increases the sample size available to train each model;
(ii) reduces the input space size, (iii) and takes into account spatial
(pixel) correlation in the images.

Since decision tree made their split orthogonally to the input space,
authors have proposed to randomize such ensembles by randomly projecting
the input space. Rotation forest (Rodriguez et al., 2006 ) is an
ensemble method combining bagging with principal component analysis. For
each bootstrap copy, it first slices the @xmath input variables into
@xmath subsets, then projects each subset of inputs of size @xmath on
its principal components and finally grows @xmath models (one on each
subset). Kuncheva and Rodríguez ( 2007 ) further compares three input
dimensionality reduction techniques (described in Section 8 ): (i) the
PCA approach of Rodriguez et al. ( 2006 ) , (ii) Gaussian random
projections and (iii) sparse Gaussian random projections. On their
benchmark, they find that the PCA-based rotation matrices yield the best
results and also that sparse random projections are strictly better than
dense random projections. The idea of using dense Rademacher or Gaussian
random projections was again re-discovered by Schclar and Rokach ( 2009
) . Similarly, Blaser and Fryzlewicz ( 2015 ) proposed to make ensembles
through random rotation of the input space.

###### 15.2.3 Output-based randomization

Output-based randomization methods directly perturb the output space
@xmath of each member of the ensemble.

In regression, we can induce randomization to an output variable through
the addition of an independent Gaussian noise (Breiman, 2000 ) . We fit
each model of the ensemble on the perturbed output @xmath with @xmath .

In classification, we perturb the output of each model of the ensemble
by having a non zero probability to randomly switch the class associated
to each sample (Breiman, 2000 ; Martínez-Muñoz and Suárez, 2005 ;
Martínez-Muñoz et al., 2008 ) .

For multi-label tasks and multi-output tasks, supervised learning
algorithms, such as random k-label subset (see also Section 4.5 )
(Tsoumakas and Vlahavas, 2007 ) , randomizes the ensemble by building
each model of the ensemble on a subset of the output space or the label
sets present in the learning set.

##### 15.3 Randomized forest model

The decision tree algorithm has a high variance, due to the instability
of its structure. Large decision trees, such as fully developed trees,
are often very unstable, especially at the bottom of the tree. The
selected splitting rules depend on the samples reaching those nodes.
Small changes in the learning set might lead to very different tree
structures. Authors have proposed randomization schemes to perturb the
search and selection of the best splitting rule improving the
generalization error through averaging methods.

One of the first propositions to perturb the splitting rule search
(Dietterich and Kong, 1995 ) was to select randomly at each node one
splitting rule among the top @xmath splitting rules with the highest
impurity reduction. The variance of the algorithm increases with the
number @xmath of splitting rule candidates, leaving the bias unchanged.
Later in the context of digit recognition, Amit et al. ( 1997 )
randomized the tree growth by restricting the splitting rule search at
each node to a random subset of @xmath input variables out of the @xmath
available. The original motivation was to drastically reduce the
splitting rule search space as the number of input variables is very
high in digit recognition tasks. This randomization scheme increases
more the variance of the algorithm than the one of Dietterich and Kong ,
at the expense of increasing the bias. Note the similarity with the
random subspace approach (Ho, 1998 ) which subsamples the input space
prior fitting a new estimator in an averaging ensemble.

Breiman got inspired by the work of Amit et al. and combined its bagging
method (Breiman, 1996a ) with the input variable sub-sampling leading to
the well known ⁵ ⁵ 5 The random forest method usually refers to the the
algorithm of Breiman , however any averaging ensemble of randomized
trees is also a random forest. “random forest” algorithm (Breiman, 2001
) . The combination of both randomization schemes has led to one of the
best of the shelf estimator for many supervised learning tasks (Caruana
et al., 2008 ; Fernández-Delgado et al., 2014 ) .

Later on, Geurts et al. ( 2006a ) randomized the cut point and input
variable selection of the splitting rules. At each test node, it draws
one random splitting rule for @xmath randomly selected input variables
(without replacement) and then selects the best one. This randomized
tree ensemble is called extremely randomized trees or extra trees. For a
splitting rule @xmath associated to an ordered variable, the algorithm
draws uniformly at random the threshold @xmath between the minimum and
maximum of the possible cut point values. Similarly for an unordered
variable, the algorithm draws a non empty subset @xmath among the
possible values to generate a splitting rule of the form @xmath .
Empirically, it has been shown (Geurts, 2002 ) that the variance of the
decision tree algorithm is due to the variability of the cut point
selection with respect to the learning set. We can view the perturbation
of the cut point selection as a way to transfer the variance due to the
learning set to the variance due to the randomization of the cut point
selection. The hyper-parameter @xmath controls the trade-off between the
bias and variance of the algorithm.

Besides perturbing the binary and axis wise splitting rules, they have
been some research to make splitting rule through random hyper-planes.
Breiman ( 2001 ) proposed to select the best splitting rule obtained
from random sparse linear input combinations with non zero values drawn
uniformly in @xmath . Tomita et al. ( 2015 ) proposed to use sparse
random projection where non zero elements are drawn uniformly in @xmath
. Those approaches increase the variance, while also trying to reduce
the bias by allowing random oblique splits. However, Menze et al. ( 2011
) have shown that those random sparse hyper-planes are inferior to
deterministic linear models such as ridge regressors or a linear
discriminant analysis (LDA) models.

For supervised learning tasks with many outputs, we can also perturb the
output space of each decision tree by randomly projecting the output
space (Joly et al., 2014 ) onto a lower dimensional subspace or through
random output sub-sampling. The leaves are later re-labelled on the
original output space. This approach is developed in Chapter \thechapter
of this thesis.

#### 16 Boosting ensembles

Boosting methods originate from the following question: “How can we
combine a set of weak models together, each one doing slightly better
than random guessing, so as to get one stronger model having good
generalization performance?”. A boosting model @xmath answers this
question through a weighted combination of @xmath weak models @xmath
leading to

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

where the coefficients @xmath highlight the contribution of each model
@xmath to the ensemble.

For a boosting ensemble, we usually want to minimize a loss function
@xmath over a learning set @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

where we select each model @xmath over a hypothesis space @xmath .
Solving this equation for many loss functions and models is either
intractable or numerically too intensive for practical purpose. However,
we can solve easily Equation \thechapter .30 for a single model ( @xmath
).

So boosting methods develop iterative and tractable schemes to solve
Equation \thechapter .30 by adding sequentially models to the ensemble.
A new model @xmath builds over the work done by the previous @xmath
models to yield better predictions. It further minimizes the loss @xmath
averaged over the training data:

  -- -------- -- --------------------
     @xmath      ( \thechapter .31)
  -- -------- -- --------------------

To improve the predictions made by the @xmath models, the new model
@xmath with coefficient @xmath concentrates its efforts on the wrongly
predicted samples.

From a bias-variance perspective, each newly added model aims to reduce
the bias while leaving the variance term unmodified if possible. We
choose the base model so that it has a high bias and a small variance
such as a stump, i.e. a decision tree with only one testing node, or
such as a linear model with only one non-zero coefficient. In Figure
\thechapter .3 , we sequentially fit stumps to decrease the least square
loss @xmath . With a few stumps, the squared bias component of the
generalization error dominates with a low variance. By adding more
stumps to the ensemble, we drastically decrease the generalization error
by diminishing the squared bias. The best performance is a trade-off
between the bias reduction and the increase in variance.

We present the adaptive boosting and its variants in Section 16.1 ,
which directly solve Equation \thechapter .31 , and the functional
gradient boosting approach in Section 16.2 , which approximately solves
Equation \thechapter .31 through the loss gradient.

##### 16.1 Adaboost and variants

One of the most popular and influential boosting algorithms is the
“AdaBoost” algorithm (Freund and Schapire, 1997 ) . This supervised
learning algorithm aims to solve binary classification tasks with @xmath
. The algorithm generates iteratively an ensemble of estimators @xmath
by minimizing the exponential loss function:

  -- -------- -- --------------------
     @xmath      ( \thechapter .32)
  -- -------- -- --------------------

assuming a binary response of the weak models @xmath .

The prediction of an unseen sample @xmath by an AdaBoost ensemble is a
majority vote from its members:

  -- -------- -- --------------------
     @xmath      ( \thechapter .33)
  -- -------- -- --------------------

where the @xmath are constant weights indicating the contribution of a
model @xmath to solve the binary classification task. The @xmath
operator transforms the sum into an appropriate output value ( @xmath ).

Given a learning set @xmath , we iteratively fit a weak model @xmath
over the learning set @xmath by making the weak learner focuses on each
sample with a weight @xmath . The higher the value of @xmath , the more
the algorithm will concentrate to predict correctly the @xmath -th
sample. To design this algorithm, we need to answer to the following
questions: (i) how to assess the contribution @xmath of the @xmath -th
model @xmath to the ensemble and (ii) how to update the weight @xmath to
reduce iteratively the exponential loss.

We can write the resubtitution error of the exponential loss as

  -- -------- -- --------------------
     @xmath      
     @xmath      ( \thechapter .34)
     @xmath      ( \thechapter .35)
     @xmath      ( \thechapter .36)
     @xmath      ( \thechapter .37)
  -- -------- -- --------------------

with

  -- -------- -- --------------------
     @xmath      ( \thechapter .38)
  -- -------- -- --------------------

Note that the weight computation is expressible as a recursive equation
starting with @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .39)
  -- -------- -- --------------------

The sample weight @xmath highlights how well the @xmath -th sample is
predicted by the @xmath first estimators of the boosting ensemble. A
zero weight @xmath means that the @xmath -th sample is perfectly
predicted. The @xmath -th estimators should thus focus on the sample
with high weight @xmath to reduce the resubtitution error. Otherwise, it
should minimize the weighted resubstitution error.

Let us now separate the resubtitution error of the correctly classified
points from the misclassified ones:

  -- -------- -------- -- --------------------
              @xmath      
     @xmath   @xmath      ( \thechapter .40)
  -- -------- -------- -- --------------------

By derivating the last equation with respect to @xmath and setting the
derivative to zero, the @xmath minimizing the resubtitution error of the
exponential loss is

  -- -------- -- --------------------
     @xmath      ( \thechapter .41)
  -- -------- -- --------------------

with

  -- -------- -- --------------------
     @xmath      ( \thechapter .42)
  -- -------- -- --------------------

The optimization of the constant @xmath means that the resubstitution
error is upper bounded and can not increase with the size of the
ensemble on the learning set.

Putting everything together, we obtain the AdaBoost algorithm (see
Algorithm 1 ). Many extensions and enhancements of this fundamental idea
have been proposed. If the weak model is able to predict a probability
estimate, Friedman et al. ( 2000 ) have proposed an appropriate
extension called “Real Adaboost” by contrast to Algorithm 1 which they
call “Discrete Adaboost”.

1: function Adaboost ( @xmath )

2: Initialize the sample weights @xmath .

3: for @xmath = 1 to @xmath do

4: Fit a model @xmath to the learning set @xmath and @xmath .

5: Compute the weighted error rate

  -- -------- --
     @xmath   
  -- -------- --

6: Compute @xmath .

7: Update the weights

  -- -------- --
     @xmath   
  -- -------- --

8: end for

9: return @xmath

10: end function

Algorithm 1 AdaBoost.M1 for binary classification @xmath .

A direct multi-class extension, called AdaBoost.M1, of the AdaBoost
algorithm is to use a multi-class weak learner instead of a binary one.
The AdaBoost.M1 ensemble predicts a new sample through:

  -- -------- -- --------------------
     @xmath      ( \thechapter .43)
  -- -------- -- --------------------

An improvement over this approach is to directly minimize the
multi-class exponential loss as in the SAMME algorithm (Zhu et al., 2009
) . It replaces the line 6 of Algorithm 1 by

  -- -------- -- --------------------
     @xmath      ( \thechapter .44)
  -- -------- -- --------------------

We can minimize other losses than the exponential loss during the
ensemble growth, such as the logistic loss @xmath with the LogitBoost
algorithm (Collins et al., 2002 ) for binary classification tasks; the
Hamming loss @xmath leading to the AdaBoost.MH algorithm (Schapire and
Singer, 2000 ) and the pairwise ranking loss @xmath

  -- -------- --
     @xmath   
  -- -------- --

leading to the AdaBoost.MR algorithm (Schapire and Singer, 2000 ) for
multi-label classification tasks and also a wide range of regression
losses as proposed in (Drucker, 1997 ) for regression tasks.

How to take into account sample weights in supervised learning
algorithms? For a set of learning samples @xmath and a set of weights
@xmath , the weighted resubtitution error is given by

@xmath ( \thechapter .45)

Extending supervised learning algorithms to support sample weights means
that we have to modify the learning algorithm so as to minimize the
weighted resubtitution error: For linear models, we will minimize the
weighted average of a given loss @xmath over the learning set @xmath

@xmath ( \thechapter .46)

where the @xmath are the weight associated to each sample. There is an
analytical solution in the case of a @xmath norm regularization penalty
and algorithms for a @xmath regularization penalty can be easily
extended to accommodate for the weights. For a decision tree, we will
use a weighted impurity criterion and a weight-aware leaf labelling rule
assignment procedure. It also allows new stopping rule based on
sample-weight, such as a minimal total weight to split a node. For a
@xmath -nearest neighbors, we will store the sample weight during fit
and we will predict an unseen sample through a weighted aggregation of
the nearest neighbors. Conversely, to support unweighted supervised
learning task with a weight-aware implementation, we can set the sample
weights to a constant such as @xmath prior the model training.

##### 16.2 Functional gradient boosting

The AdaBoost algorithm has an analytical and closed-form solution to
Equation \thechapter .31 with the exponential loss. However, we would
like to build boosting ensembles when such closed-form solutions are not
available. Functional gradient boosting, a forward stagewise additive
approach, approximately solves Equation \thechapter .31 for a given loss
@xmath by sequentially adding new basis function @xmath , a regression
model, with a weight @xmath without modifying the previous models.

If we want to add a new model @xmath to a boosting ensemble with @xmath
models while minimizing the square loss, the loss for a sample @xmath is
given by

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .47)
              @xmath      ( \thechapter .48)
              @xmath      ( \thechapter .49)
  -- -------- -------- -- --------------------

where @xmath is the remaining residual of the @xmath models to predict a
sample @xmath . Thus for the square loss, we can add a new models @xmath
by fitting the new model on the residuals left by the @xmath previous
models. This approach is called least square regression boosting.

Solving Equation \thechapter .31 is difficult for general loss
functions. It requires to be able to expand a new basis function @xmath
while minimizing the chosen loss function. For instance in the context
of decision trees, it would require a specific splitting criterion and a
leaf labelling rule minimizing the chosen loss.

Instead of solving Equation \thechapter .31 , Friedman ( 2001 ) proposed
a fast approximate solution for arbitrary differentiable losses inspired
from numerical optimization. We can re-write the loss function
minimization as

  -- -------- -- --------------------
     @xmath      ( \thechapter .50)
  -- -------- -- --------------------

with the constraint that @xmath is a sum of supervised learning models.
Ignoring this constraint, the Equation \thechapter .50 is an
unconstrained minimization problem with @xmath being a @xmath
-dimensional vector. Iterative solvers solve such minimization problems
by correcting an initial estimate through a recursive equation. The
final solution is a sum of vectors

  -- -------- -- --------------------
     @xmath      ( \thechapter .51)
  -- -------- -- --------------------

where @xmath is the initial estimate. The construction of the sequence
of @xmath depends on the chosen optimization algorithm.

The gradient boosting algorithm (Friedman, 2001 ) uses the same approach
as the gradient descent method. The update rule of the gradient descent
algorithm @xmath is of the form

  -- -------- -- --------------------
     @xmath      ( \thechapter .52)
  -- -------- -- --------------------

where @xmath is a scalar and @xmath is the gradient of @xmath with
respect to @xmath evaluated at the current approximate solution @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .53)
  -- -------- -- --------------------

The scalar @xmath is the step length in the negative loss gradient
direction @xmath chosen so as to minimize the objective function @xmath
:

  -- -------- -- --------------------
     @xmath      ( \thechapter .54)
  -- -------- -- --------------------

Back to supervised learning, we can only compute the loss gradient for
the training samples. To generalize to unseen data, the idea is to
approximate the direction of the negative gradient using a regression
model @xmath selected within a hypothesis space @xmath of weak
base-learners minimizing the square loss on the training data:

  -- -------- -- --------------------
     @xmath      ( \thechapter .55)
  -- -------- -- --------------------

The gradient boosting approach can be summarized as follows: start at an
initial constant estimate @xmath , then iteratively follows the negative
gradient of the loss @xmath as estimated by a regression model @xmath
fitted over the training samples and make an optimal step length @xmath
minimizing the loss @xmath . The gradient boosting ensemble predicts an
unseen sample through

  -- -------- -- --------------------
     @xmath      ( \thechapter .56)
  -- -------- -- --------------------

The whole procedure is given in Algorithm 2 . The algorithm is
completely defined once we have (i) a starting model, usually the
constant minimizing the chosen loss (line 4 ) and (ii) the gradient of
the loss (line 2 ).

1: function GradientBoosting ( @xmath )

2: @xmath .

3: for @xmath = 1 to @xmath do

4: Compute the loss gradient for the training set points

  -- -------- --
     @xmath   
  -- -------- --

5: Find a correlated direction to the loss gradient

  -- -------- --
     @xmath   
  -- -------- --

6: Find an optimal step length in the direction @xmath

  -- -------- --
     @xmath   
  -- -------- --

7: @xmath .

8: end for

9: return @xmath

10: end function

Algorithm 2 Gradient boosting algorithm

We compute the optimal step length (line 6 of Algorithm 2 ) either
analytically as for the square loss or numerically using, e.g., the
Brent’s method (Brent, 2013 ) , a robust root-finding method allowing to
minimize single unconstrained optimization problem, as for the logistic
loss. Friedman ( 2001 ) advises to use one step of the Newton–Raphson
method. However, the Newton–Raphson algorithm might not converge if the
first and second derivative of the loss are small. These conditions
occurs frequently in highly imbalanced supervised learning tasks.

A learning rate @xmath is often added to shrink the size of the gradient
step @xmath in the residual space in order to avoid overfitting the the
training samples. Another possible modification is to induce
randomization, e.g. by subsampling without replacement the samples
available (from all learning samples) at each iteration (Friedman, 2002
) .

Table \thechapter .1 gives an overview of regression and classification
losses with their gradients, while Table \thechapter .2 gives the
starting constant models minimizing losses. The square loss in
regression and the exponential loss in classification leads to nice
gradient boosting algorithm (respectively the least square regression
boosting algorithm and the exponential classification boosting algorithm
(Zhu et al., 2009 ) ). However, these losses are not robust to outlier.
More robust losses can be used such as the absolute loss in regression
and the logistic loss or the hinge loss in classification.

## Part II Learning in compressed space through random projections

### Chapter \thechapter Random forests with random projections of the
output space for high dimensional multi-label classification

Outline We adapt the idea of random projections applied to the output
space, so as to enhance tree-based ensemble methods in the context of
multi-label classification. We show how learning time complexity can be
reduced without affecting computational complexity and accuracy of
predictions. We also show that random output space projections may be
used in order to reach different bias-variance tradeoffs, over a broad
panel of benchmark problems, and that this may lead to improved accuracy
while reducing significantly the computational burden of the learning
stage. This chapter is based on previous work published in Arnaud Joly,
Pierre Geurts, and Louis Wehenkel. Random forests with random
projections of the output space for high dimensional multi-label
classification. In Machine Learning and Knowledge Discovery in
Databases, pages 607–622. Springer Berlin Heidelberg, 2014.

Within supervised learning, the goal of multi-label classification is to
train models to annotate objects with a subset of labels taken from a
set of candidate labels. Typical applications include the determination
of topics addressed in a text document, the identification of object
categories present within an image, or the prediction of biological
properties of a gene. In many applications, the number of candidate
labels may be very large, ranging from hundreds to hundreds of thousands
(Agrawal et al., 2013 ) and often even exceeding the sample size (Dekel
and Shamir, 2010 ) . The very large scale nature of the output space in
such problems poses both statistical and computational challenges that
need to be specifically addressed.

A simple approach to multi-label classification problems, called binary
relevance, is to train independently a binary classifier for each label.
Several more complex schemes have however been proposed to take into
account the dependencies between the labels (see Section 4.5 ). In the
context of tree-based methods, one way is to train multi-output trees
(see Section 13 ), i.e. trees that can predict multiple outputs at once.
With respect to binary relevance, the multi-output tree approach has the
advantage of building a single model for all labels. It can thus
potentially take into account label dependencies and reduce memory
requirements for the storage of the models. An extensive experimental
comparison (Madjarov et al., 2012 ) shows that this approach compares
favorably with other approaches, including non tree-based methods, both
in terms of accuracy and computing times. In addition, multi-output
trees inherit all intrinsic advantages of tree-based methods, such as
robustness to irrelevant features, interpretability through feature
importance scores, or fast computations of predictions, that make them
very attractive to address multi-label problems. The computational
complexity of learning multi-output trees is however similar to that of
the binary relevance method. Both approaches are indeed @xmath , where
@xmath is the number of input features, @xmath the number of candidate
output labels, and @xmath the sample size; this is a limiting factor
when dealing with large sets of candidate labels.

One generic approach to reduce computational complexity is to apply some
compression technique prior to the training stage to reduce the number
of outputs to a number @xmath much smaller than the total number @xmath
of labels. A model can then be trained to make predictions in the
compressed output space and a prediction in the original label space can
be obtained by decoding the compressed prediction. As multi-label
vectors are typically very sparse, one can expect a drastic
dimensionality reduction by using appropriate compression techniques.
This idea has been explored for example in (Hsu et al., 2009 ) using
compressed sensing, and in (Cisse et al., 2013 ) using bloom filters, in
both cases using regularized linear models as base learners. The
approach obviously reduces computing times for training the model.
Random projections are also exploited in (Tsoumakas et al., 2014 ) for
multi-target regression. In this latter work however, they are not used
to improve computing times by compression but instead to improve
predictive performance. Indeed, more (sparse) random projections are
computed than there are outputs and they are used each as an output to
train some single target regressor. As in (Hsu et al., 2009 ; Cisse
et al., 2013 ) , the predictions of the regressors need to be decoded at
prediction time to obtain a prediction in the original output space.
This is achieved in (Tsoumakas et al., 2014 ) by solving an
overdetermined linear system.

In this chapter, we explore the use of random output space projections
for large-scale multi-label classification in the context of tree-based
ensemble methods. We first explore the idea proposed for linear models
in (Hsu et al., 2009 ) with random forests: a (single) random projection
of the multi-label vector to a @xmath -dimensional random subspace is
computed and then a multi-output random forest is grown based on score
computations using the projected outputs. We exploit however the fact
that the approximation provided by a tree ensemble is a weighted average
of output vectors from the training sample to avoid the decoding stage:
at training time all leaf labels are directly computed in the original
multi-label space. We show theoretically and empirically that when
@xmath is large enough, ensembles grown on such random output spaces are
equivalent to ensembles grown on the original output space. When @xmath
is large enough compared to @xmath , this idea hence may reduce
computing times at the learning stage without affecting accuracy and
computational complexity of predictions.

Next, we propose to exploit the randomization inherent to the projection
of the output space as a way to obtain randomized trees in the context
of ensemble methods: each tree in the ensemble is thus grown from a
different randomly projected subspace of dimension @xmath . As
previously, labels at leaf nodes are directly computed in the original
output space to avoid the decoding step. We show, theoretically, that
this idea can lead to better accuracy than the first idea and,
empirically, that best results are obtained on many problems with very
low values of @xmath , which leads to significant computing time
reductions at the learning stage. In addition, we study the interaction
between input randomization (à la Random Forests) and output
randomization (through random projections), showing that there is an
interest, both in terms of predictive performance and in terms of
computing times, to optimally combine these two ways of randomization.
All in all, the proposed approach constitutes a very attractive way to
address large-scale multi-label problems with tree-based ensemble
methods.

The rest of the chapter is structured as follows: Section 17 presents
the proposed algorithms and their theoretical properties; Section 18
analyses the proposed algorithm from a bias-variance perspective;
Section 19 provides the empirical validations, whereas Section 20
discusses our work and provides further research directions.

#### 17 Methods

We first present how we propose to exploit random projections to reduce
the computational burden of learning single multi-output trees in very
high-dimensional output spaces. Then we present and compare two ways to
exploit this idea with ensembles of trees.

##### 17.1 Multi-output regression trees in randomly projected output
spaces

The multi-output single tree algorithm described in Chapter \thechapter
requires the computation of the sum of impurity criterion, such as the
variance (or Gini), at each tree node and for each candidate split. When
@xmath is very high-dimensional, this computation constitutes the main
computational bottleneck of the algorithm. We thus propose to
approximate variance computations by using random projections of the
output space. The multi-output regression tree algorithm is modified as
follows (denoting by @xmath the learning sample @xmath ):

-   First, a projection matrix @xmath of dimension @xmath is randomly
    generated.

-   A new dataset @xmath is constructed by projecting each learning
    sample output using the projection matrix @xmath .

-   A tree (structure) @xmath is grown using the projected learning
    sample @xmath .

-   Predictions @xmath at each leaf of @xmath are computed using the
    corresponding outputs in the original output space.

The resulting tree is exploited in the standard way to make predictions:
an input vector @xmath is propagated through the tree until it reaches a
leaf from which a prediction @xmath in the original output space is
directly retrieved.

If @xmath satisfies the Jonhson-Lindenstrauss lemma (Equation
\thechapter .86 ), the following theorem shows that variance computed in
the projected subspace is an @xmath -approximation of the variance
computed over the original space.

###### Theorem 1.

Given @xmath , a sample @xmath of @xmath points @xmath , and a
projection matrix @xmath such that for all @xmath the condition given by
Equation \thechapter .86 holds, we have also:

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

###### Proof.

The sum of the variances of @xmath observations drawn from a random
vector @xmath can be interpreted as a sum of squared euclidean distances
between the pairs of observations

  -- -------- -- -------------------
     @xmath      ( \thechapter .2)
  -- -------- -- -------------------

Starting from the defition of the variance, we have

  -- -------- -- --------------------
     @xmath      
     @xmath      ( \thechapter .3)
     @xmath      ( \thechapter .4)
     @xmath      ( \thechapter .5)
     @xmath      ( \thechapter .6)
     @xmath      ( \thechapter .7)
     @xmath      ( \thechapter .8)
                 ( \thechapter .9)
     @xmath      ( \thechapter .10)
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

From the Johnson-Lindenstrauss Lemma we have for any @xmath

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

By summing the three terms of Equation \thechapter .12 over all pairs
@xmath and dividing by @xmath and by then using Equation \thechapter .2
, we get Equation \thechapter .1 .

∎

As a consequence, any split score approximated from the randomly
projected output space will be @xmath -close to the unprojected scores
in any subsample of the complete learning sample. Thus, if the condition
given by Equation \thechapter .86 ) is satisfied for a sufficiently
small @xmath then the tree grown from the projected data will be
identical to the tree grown from the original data ⁶ ⁶ 6 Strictly
speaking, this is only the case when the optimum scores of test splits
as computed over the original output space are isolated, i.e. when there
is only one single best split, no tie. .

For a given size @xmath of the projection subspace, the complexity is
reduced from @xmath to @xmath for the computation of one split score and
thus from @xmath to @xmath for the construction of one full (balanced)
tree, where one can expect @xmath to be much smaller than @xmath and at
worst of @xmath . The whole procedure requires to generate the
projection matrix and to project the training data. These two steps are
respectively @xmath and @xmath but they can often be significantly
accelerated by exploiting the sparsity of the projection matrix and/or
of the original output data, and they are called only once before
growing the tree.

All in all, this means that when @xmath is sufficiently large, the
random projection approach may allow us to significantly reduce tree
building complexity from @xmath to @xmath , without impact on predictive
accuracy (see Section 19 , for empirical results).

##### 17.2 Exploitation in the context of tree ensembles

The idea developed in the previous section can be directly exploited in
the context of ensembles of randomized multi-output regression trees.
Instead of building a single tree from the projected learning sample,
one can grow a randomized ensemble of them. This “shared subspace”
algorithm is described in pseudo-code in Algorithm 1 .

1: function GrowForestSharedOutputSubspace ( @xmath , @xmath )

2: Generate a sub-space @xmath ;

3: for @xmath to @xmath do

4: Build a tree structure @xmath using @xmath ;

5: Label the leaves of @xmath using @xmath ;

6: Add the labelled tree @xmath to the ensemble;

7: end for

8: end function

Algorithm 1 Grow @xmath decision trees on a single shared subspace
@xmath using learning samples @xmath

Another idea is to exploit the random projections used so as to
introduce a novel kind of diversity among the different trees of an
ensemble. Instead of building all the trees of the ensemble from a same
shared output-space projection, one could instead grow each tree in the
ensemble from a different output-space projection. Algorithm 2
implements this idea in pseudo-code. The randomization introduced by the
output space projection can of course be combined with any existing
randomization scheme to grow ensembles of trees. In this chapter, we
will consider the combination of random projections with the
randomizations already introduced in Random Forests and Extra Trees. The
interplay between these different randomizations will be discussed
theoretically in the next subsection by a bias/variance analysis and
empirically in Section 19 . Note that while when looking at single trees
or shared ensembles, the size @xmath of the projected subspace should
not be too small so that condition (Equation \thechapter .86 ) is
satisfied, the optimal value of @xmath when projections are randomized
at each tree is likely to be smaller, as suggested by the bias/variance
analysis in the next section.

1: function GrowForestOutputSubspace ( @xmath , @xmath )

2: for @xmath to @xmath do

3: Generate a sub-space @xmath ;

4: Build a tree structure @xmath using @xmath ;

5: Label the leaves of @xmath using @xmath ;

6: Add the labelled tree @xmath to the ensemble;

7: end for

8: end function

Algorithm 2 Grow @xmath decision trees on individual random subspaces
@xmath using learning samples @xmath

From the computational point of view, the main difference between these
two ways of transposing random-output projections to ensembles of trees
is that in the case of Algorithm 2 , the generation of the projection
matrix @xmath and the computation of projected outputs is carried out
@xmath times, while it is done only once for the case of Algorithm 1 .
These aspects will be empirically evaluated in Section 19 .

#### 18 Bias/variance analysis

In this section, we adapt the bias/variance analysis carried out in
Section 15.1 to take into account random output projections. The details
of the derivations are reported in Section 18.1 for a single tree and in
Section 18.2 for an ensemble of @xmath randomized trees.

Let us denote by @xmath a single multi-output tree obtained from a
projection matrix @xmath (below we use @xmath to denote the
corresponding random variable), where @xmath is the value of a random
variable @xmath capturing the random perturbation scheme used to build
this tree (e.g., bootstrapping and/or random input space selection). The
square error of this model at some point @xmath is defined by:

  -- -------- --
     @xmath   
  -- -------- --

and its average can decomposed in its residual error, (squared) bias,
and variance terms denoted:

  -- -------- --
     @xmath   
  -- -------- --

where the variance term @xmath can be further decomposed as the sum of
the following three terms:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath            
  -- -------- -------- -------- --

that measure errors due to the randomness of, respectively, the learning
sample, the tree algorithm, and the output space projection (see Section
18.1 ).

Approximations computed respectively by Algorithm 1 and Algorithm 2 take
the following forms:

-   @xmath

-   @xmath

where @xmath and @xmath are vectors of i.i.d. values of the random
variables @xmath and @xmath respectively.

We are interested in comparing the average errors of these two
algorithms, where the average is taken over all random parameters
(including the learning sample). We show that these can be decomposed as
follows (see Section 18.2 ):

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

From this result, it is hence clear that Algorithm 2 can not be worse,
on the average, than Algorithm 1 . If the additional computational
burden needed to generate a different random projection for each tree is
not problematic, then Algorithm 2 should always be preferred to
Algorithm 1 .

For a fixed level of tree randomization ( @xmath ), whether the
additional randomization brought by random projections could be
beneficial in terms of predictive performance remains an open question
that will be addressed empirically in the next section. Nevertheless,
with respect to an ensemble grown from the original output space, one
can expect that the output-projections will always increase the bias
term, since they disturb the algorithm in its objective of reducing the
errors on the learning sample. For small values of @xmath , the average
error will therefore decrease (with a sufficiently large number @xmath
of trees) only if the increase in bias is compensated by a decrease of
variance.

The value of @xmath , the dimension of the projected subspace, that will
lead to the best tradeoff between bias and variance will hence depend
both on the level of tree randomization and on the learning problem. The
more (resp. less) tree randomization, the higher (resp. the lower) could
be the optimal value of @xmath , since both randomizations affect bias
and variance in the same direction.

##### 18.1 Single random trees.

Let us denote by @xmath a single multi-output (random) tree obtained
from a projection matrix @xmath (below we use @xmath to denote the
corresponding random variable), where @xmath is the value of a random
variable @xmath capturing the random perturbation scheme used to build
this tree (e.g., bootstrapping and/or random input space selection).
Denoting by @xmath the square error of this model at some point @xmath
defined by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .13)
  -- -------- -- --------------------

The average of this square error can decomposed as follows:

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The three terms of this decomposition are respectively the residual
error, the bias, and the variance of this estimator (at @xmath ).

The variance term can be further decomposed as follows using the law of
total variance:

  -- -------- -- -------- -------- --------------------
     @xmath                        ( \thechapter .14)
                 @xmath   @xmath   
                          @xmath   
  -- -------- -- -------- -------- --------------------

The first term is the variance due to the learning sample randomization
and the second term is the average variance (over @xmath ) due to both
the random forest randomization and the random output projection. By
using the law of total variance a second time, the second term of
Equation \thechapter .14 ) can be further decomposed as follows:

  -- -------- -- --------------------
     @xmath      
     @xmath      
     @xmath      ( \thechapter .15)
  -- -------- -- --------------------

The first term of this decomposition is the variance due to the random
choice of a projection and the second term is the average variance due
to the random forest randomization. Note that all these terms are non
negative. In what follows, we will denote these three terms respectively
@xmath , @xmath , and @xmath . We thus have:

  -- -------- --
     @xmath   
  -- -------- --

with

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath            
  -- -------- -------- -------- --

##### 18.2 Ensembles of @xmath random trees.

When the random projection is fixed for all @xmath trees in the ensemble
(Algorithm 1 ), the algorithm computes an approximation, denoted @xmath
, that takes the following form:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a vector of i.i.d. values of the random variable @xmath
. When a different random projection is chosen for each tree (Algorithm
2 ), the algorithm computes an approximation, denoted by @xmath , of the
following form:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is also a vector of i.i.d. random projection matrices.

We would like to compare the average errors of these two algorithms with
the average errors of the original single tree method, where the average
is taken for all algorithms over their random parameters (that include
the learning sample).

Given that all trees are grown independently of each other, one can show
that the average models corresponding to each algorithm are equal:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

They thus all have the exact same bias (and residual error) and differ
only in their variance.

Using the same argument, the first term of the variance decomposition in
( \thechapter .14 ), ie. @xmath , is the same for all three algorithms
since:

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

Their variance thus only differ in the second term of Equation
\thechapter .14 .

Again, because of the conditional independence of the ensemble terms
given the learning set @xmath and the projection matrix @xmath ,
Algorithm 1 , which keeps the output projection fixed for all trees, is
such that

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

It thus divides the second term of Equation \thechapter .15 by the
number @xmath of ensemble terms. Algorithm 2 , on the other hand, is
such that:

  -- -------- --
     @xmath   
  -- -------- --

and thus divides the second term of Equation \thechapter .14 by @xmath .

Putting all these results together one gets that:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Given that all terms are positive, this result clearly shows that
Algorithm 2 can not be worse than Algorithm 1 .

#### 19 Experiments

##### 19.1 Effect of the size @xmath of the Gaussian output space

To illustrate the behaviour of our algorithms, we first focus on the
“Delicious” dataset (Tsoumakas et al., 2008a ) , which has a large
number of labels ( @xmath ), of input features ( @xmath ), and of
training ( @xmath ) and testing ( @xmath ) samples.

The top part of figure \thechapter .1 shows, when Gaussian output-space
projections are combined with the standard CART algorithm building a
single tree, how the precision converges (cf Theorem 1 ) when @xmath
increases towards @xmath . We observe that in this case, convergence is
reached around @xmath at the expense of a slight decrease of accuracy,
so that a compression factor of about 5 is possible with respect to the
original output dimension @xmath .

The bottom part of figure \thechapter .1 shows, on the same dataset, how
the method behaves when combined with Random Forests. Let us first
notice that the Random Forests grown on the original output space (green
line) are significantly more accurate than the single trees, their
accuracy being almost twice as high. We also observe that Algorithm 2
(orange curve) converges much more rapidly than Algorithm 1 (blue curve)
and slightly outperforms the Random Forest grown on the original output
space. It needs only about @xmath components to converge, while
Algorithm 1 needs about @xmath of them. These results are in accordance
with the analysis of Section 18 , showing that Algorithm 2 can’t be
inferior to Algorithm 1 . In the rest of this chapter we will therefore
focus on Algorithm 2 .

##### 19.2 Systematic analysis over 24 datasets

To assess our methods, we have collected 24 different multi-label
classification datasets from the literature (see Section D of the
supplementary material, for more information and bibliographic
references to these datasets) covering a broad spectrum of application
domains and ranges of the output dimension ( @xmath , see Table
\thechapter .1 ). For 21 of the datasets, we made experiments where the
dataset is split randomly into a learning set of size @xmath , and a
test set of size @xmath , and are repeated 10 times (to get average
precisions and standard deviations), and for 3 of them we used a
ten-fold cross-validation scheme (see Table \thechapter .1 ).

Table \thechapter .1 shows our results on the 24 multi-label datasets,
by comparing Random Forests learnt on the original output space with
those learnt by Algorithm 2 combined with Gaussian subspaces of size
@xmath ⁷ ⁷ 7 @xmath is rounded to the nearest integer value; in Table
\thechapter .1 the values of @xmath vary between 2 for @xmath and 8 for
@xmath . . In these experiments, the three parameters of Random Forests
are set respectively to @xmath , @xmath (default values, see (Geurts
et al., 2006a ) ) and @xmath (reasonable computing budget). Each model
is learnt ten times on a different shuffled train/testing split, except
for the 3 EUR-lex datasets where we kept the original 10 folds of
cross-validation.

We observe that for all datasets (except maybe SCOP-GO), taking @xmath
leads to a similar average precision to the standard Random Forests,
i.e. no difference superior to one standard deviation of the error. On
11 datasets, we see that @xmath already yields a similar average
precision (values not underlined in column @xmath ). For the 13
remaining datasets, increasing @xmath to @xmath significantly decreases
the gap with the Random Forest baseline and 3 more datasets reach this
baseline. We also observe that on several datasets such as
“Drug-interaction” and “SCOP-GO”, better performance on the Gaussian
subspace is attained with high output randomization ( @xmath ) than with
@xmath . We thus conclude that the optimal level of output randomization
(i.e. the optimal value of the ratio @xmath ) which maximizes accuracy
performances, is dataset dependent.

While our method is intended for tasks with very high dimensional output
spaces, we however notice that even with relatively small numbers of
labels, its accuracy remains comparable to the baseline, with suitable
@xmath .

To complete the analysis, let’s carry out the same experiments with a
different base-learner combining Gaussian random projections (with
@xmath ) with the Extra Trees method of (Geurts et al., 2006a ) .
Results on 23 datasets are compiled in Table \thechapter .2 .

Like for Random Forests, we observe that for all 23 datasets taking
@xmath leads to a similar average precision to the standard Random
Forests, ie. no difference superior to one standard deviation of the
error. This is already the case with @xmath for 12 datasets and with
@xmath for 4 more datasets. Interestingly, on 3 datasets with @xmath and
3 datasets with @xmath , the increased randomization brought by the
projections actually improves average precision with respect to standard
Random Forests (bold values in Table \thechapter .2 ).

##### 19.3 Input vs output space randomization

We study in this section the interaction of the additional randomization
of the output space with that concerning the input space already built
in the Random Forest method.

To this end, we consider the “Drug-interaction” dataset ( @xmath input
features and @xmath output labels (Yamanishi et al., 2011 ) ), and we
study the effect of parameter @xmath controlling the input space
randomization of the Random Forest method with the randomization of the
output space by Gaussian projections controlled by the parameter @xmath
. To this end, Figure \thechapter .2 shows the evolution of the accuracy
for growing values of @xmath (i.e. decreasing strength of the input
space randomization), for three different quite low values of @xmath (in
this case @xmath ). We observe that Random Forests learned on a very
low-dimensional Gaussian subspace (red, blue and pink curves) yield
essentially better performances than Random Forests on the original
output space, and also that their behaviour with respect to the
parameter @xmath is quite different. On this dataset, the output-space
randomisation makes the method completely immune to the ‘over-fitting’
phenomenon observed for high values of @xmath with the baseline method
(green curve).

We carry out the same experiment, but on the “Delicious” dataset. Figure
\thechapter .3 shows the evolution of the accuracy for growing values of
@xmath (i.e. decreasing strength of the input space randomization), for
three different values of @xmath (in this case @xmath ) on a Gaussian
output space.

Like on “Drug-interaction” (see Figure \thechapter .2 ), using
low-dimensional output spaces makes the method more robust with respect
to over-fitting as @xmath increases. However, unlike on
“Drug-interaction”, it is not really possible to improve over baseline
Random Forests by tuning jointly input and output randomization. This
shows that the interaction between @xmath and @xmath may be different
from one dataset to another.

It is thus advisable to jointly optimize the value of @xmath and @xmath
, so as to maximise the tradeoff between accuracy and computing times in
a problem and algorithm specific way.

##### 19.4 Alternative output dimension reduction techniques

In this section, we study Algorithm 2 when it is combined with
alternative output-space dimensionality reduction techniques. We focus
again on the “Delicious” dataset, but similar trends could be observed
on other datasets.

Figure (a)a first compares Gaussian random projections with two other
dense projections: Rademacher matrices with @xmath (cf. Section 2.2) and
compression matrices obtained by sub-sampling (without replacement)
Hadamard matrices (Candes and Plan, 2011 ) . We observe that Rademacher
and subsample-Hadamard sub-spaces behave very similarly to Gaussian
random projections.

In a second step, we compare Gaussian random projections with two (very)
sparse projections: first, sparse Rademacher sub-spaces obtained by
setting the sparsity parameter @xmath to @xmath and @xmath , selecting
respectively about 33% and 2% of the original outputs to compute each
component, and second, sub-sampled identity subspaces, similar to
(Tsoumakas and Vlahavas, 2007 ) , where each of the @xmath selected
components corresponds to a randomly chosen original label and also
preserve sparsity. Sparse projections are very interesting from a
computational point of view as they require much less operations to
compute the projections but the number of components required for
condition ( \thechapter .86 ) to be satisfied is typically higher than
for dense projections (Li et al., 2006 ; Candes and Plan, 2011 ) .
Figure (b)b compares these three projection methods with standard Random
Forests on the “delicious” dataset. All three projection methods
converge to plain Random Forests as the number of components @xmath
increases but their behaviour at low @xmath values are very different.
Rademacher projections converge faster with @xmath than with @xmath and
interestingly, the sparsest variant ( @xmath ) has its optimum at @xmath
and improves in this case over the Random Forests baseline. Random
output subspaces converge slower but they lead to a notable improvement
of the score over baseline Random Forests. This suggests that although
their theoretical guarantees are less good, sparse projections actually
provide on this problem a better bias/variance tradeoff than dense ones
when used in the context of Algorithm 2 .

Another popular dimension reduction technique is the principal component
analysis (PCA). In Figure (c)c , we repeat the same experiment to
compare PCA with Gaussian random projections. Concerning PCA, the curve
is generated in decreasing order of eigenvalues, according to their
contribution to the explanation of the output-space variance. We observe
that this way of doing is far less effective than the random projection
techniques studied previously.

##### 19.5 Learning stage computing times

Our implementation of the learning algorithms is based on the
scikit-learn Python package version 0.14-dev (Pedregosa et al., 2011 ;
Buitinck et al., 2013 ) . To fix ideas about computing times, we report
these obtained on a Mac Pro 4.1 with a dual Quad-Core Intel Xeon
processor at 2.26 GHz, on the “Delicious” dataset. Matrix operation,
such as random projections, are performed with the BLAS and the LAPACK
from the Mac OS X Accelerate framework. Reported times are obtained by
summing the user and sys time of the UNIX time utility.

The reported timings correspond to the following operation: (i) load the
dataset in memory, (ii) execute the algorithm. All methods use the same
code to build trees. In these conditions, learning a random forest on
the original output space ( @xmath , @xmath , @xmath ) takes 3348  s;
learning the same model on a Gaussian output space of size @xmath
requires 311 s, while @xmath and @xmath take respectively 236 s and
1088 s. Generating a Gaussian sub-space of size @xmath and projecting
the output data of the training samples is done in less than 0.25 s,
while @xmath and @xmath takes around 0.07 s and 1 s respectively. The
time needed to compute the projections is thus negligible with respect
to the time needed for the tree construction.

We see that a speed-up of an order of magnitude could be obtained, while
at the same time preserving accuracy with respect to the baseline Random
Forests method. Equivalently, for a fixed computing time budget,
randomly projecting the output space allows to build more trees and thus
to improve predictive performances with respect to standard Random
Forests.

#### 20 Conclusions

This chapter explores the use of random output space projections
combined with tree-based ensemble methods to address large-scale
multi-label classification problems. We study two algorithmic variants
that either build a tree-based ensemble model on a single shared random
subspace or build each tree in the ensemble on a newly drawn random
subspace. The second approach is shown theoretically and empirically to
always outperform the first in terms of accuracy. Experiments on 24
datasets show that on most problems, using gaussian projections allows
to reduce very drastically the size of the output space, and therefore
computing times, without affecting accuracy. Remarkably, we also show
that by adjusting jointly the level of input and output randomizations
and choosing appropriately the projection method, one could also improve
predictive performance over the standard Random Forests, while still
improving very significantly computing times. As future work, it would
be very interesting to propose efficient techniques to automatically
adjust these parameters, so as to reach the best tradeoff between
accuracy and computing times on a given problem.

To the best of our knowledge, our work is the first to study random
output projections in the context of multi-output tree-based ensemble
methods. The possibility with these methods to relabel tree leaves with
predictions in the original output space makes this combination very
attractive. Indeed, unlike similar works with linear models (Hsu et al.,
2009 ; Cisse et al., 2013 ) , our approach only relies on
Johnson-Lindenstrauss lemma, and not on any output sparsity assumption,
and also does not require to use any output reconstruction method.
Besides multi-label classification, we would like to test our method on
other, not necessarily sparse, multi-output prediction problems.

### Chapter \thechapter Gradient boosting with random output projections
for multi-label and multi-outputs regression tasks

Outline We first formally adapt the gradient boosting ensemble method
for multi-output supervised learning tasks such as multi-output
regression and multi-label classification. We then propose to combine
single random projections of the output space with gradient boosting on
such tasks to adapt automatically to the output correlation structure.
The idea of this method is to train each weak model on a single random
projection of the output space and then to exploit the predictions of
the resulting model to approximate the gradients of all other outputs.
Through weak model sharing and random projection of the output space, we
implicitly take into account the output correlations. We perform
extensive experiments with these methods both on artificial and real
problems using tree-based weak learners. Randomly projecting the output
space shows to provide a better adaptation to different output
correlation patterns and is therefore competitive with the best of the
other methods in most settings. Thanks to the model sharing, the
convergence speed is also faster, reducing the computing times to reach
a specific accuracy. This contribution is a joint work with Pierre
Geurts and Louis Wehenkel from the University of Liège.

#### 21 Introduction

Multi-output supervised learning aims to model the input-output
relationship of a system from observations of input-output pairs
whenever the output space is a vector of random variables. Multi-output
classification and regression tasks have numerous applications in
domains ranging from biology to multimedia.

The most straightforward way to address multi-output tasks is to apply
standard single output methods separately and independently on each
output. Although simple, this method, called binary relevance (Tsoumakas
et al., 2009 ) in multi-label classification or single target
(Spyromitros-Xioufis et al., 2016 ) in multi-output regression, is often
suboptimal as it does not exploit potential correlations that might
exist between the outputs. For this reason, several approaches have been
proposed in the literature that improve over binary relevance by
exploiting output correlations. These approaches include for example the
explicit construction of the output dependency graph (Dembczynski
et al., 2010 ; Gasse et al., 2015 ; Zhang and Zhang, 2010 ) or the
sharing of models learnt for one output to the other outputs (Huang
et al., 2012 ; Yan et al., 2007 ; Read et al., 2011 ) . Our contribution
falls into the latter category.

Classification and regression trees (Breiman et al., 1984 ) are popular
supervised learning methods that provide state-of-the-art accuracy when
exploited in the context of ensemble methods, namely Random forests
(Breiman, 2001 ) and gradient boosting (Friedman, 2001 ) .
Classification and regression trees have been extended by several
authors to the joint prediction of multiple outputs (see, e.g., Segal,
1992 ; Blockeel et al., 2000 ) ). These extensions build a single tree
to predict all outputs at once. They adapt the score measure used to
assess splits during the tree growth to take into account all outputs
and label each tree leaf with a vector of values, one for each output
(see Section 13 for more information). Like standard classification or
regression trees, multiple output trees can be exploited in the context
of random forests (Barutcuoglu et al., 2006 ; Joly et al., 2014 ; Kocev
et al., 2007 , 2013 ; Segal and Xiao, 2011 ) or boosting (Geurts et al.,
2007 ) ensembles, which often offer very significant accuracy
improvements with respect to single trees. Multiple output trees have
been shown to be competitive with other multiple output methods
(Madjarov et al., 2012 ) , but, to the best of our knowledge, it has not
been studied as extensively in the context of gradient boosting.

Binary relevance / single target of single output tree models and
multiple output tree models represent two extremes in terms of tree
structure learning: the former builds a separate tree ensemble structure
for each output, while the latter builds a single tree ensemble
structure for all outputs. Building separate ensembles for each output
may be rather inefficient when the outputs are strongly correlated.
Correlations between the outputs could indeed be exploited either to
reduce model complexity (by sharing the tree structures between several
outputs) or to improve accuracy by regularization. Trying to fit a
single tree structure for all outputs seems however counterproductive
when the outputs are independent. Indeed, in the case of independent
outputs, simultaneously fitting all outputs with a single tree structure
may require a much more complex tree structure than the sum of the
individual tree complexities required to fit the individual outputs.
Since training a more complex tree requires a larger learning sample,
multiple output trees are expected to be outperformed by binary
relevance / single target in this situation.

In this chapter, we first formally adapt gradient boosting to multiple
output tasks. We then propose a new method that aims at circumventing
the limitations of both binary relevance / single target and multiple
output methods, in the specific context of tree-based base-learners. Our
method is an extension of gradient tree boosting that can adapt itself
to the presence or absence of correlations between the outputs. At each
boosting iteration, a single regression tree structure is grown to fit a
single random projection of the outputs, or more precisely, of their
residuals with respect to the previously built models. Then, the
predictions of this tree are fitted linearly to the current residuals of
all the outputs (independently). New residuals are then computed taking
into account the resulting predictions and the process repeats itself to
fit these new residuals. Because of the linear fit, only the outputs
that are correlated with the random projection at each iteration will
benefit from a reduction of their residuals, while outputs that are
independent of the random projection will remain mostly unaffected. As a
consequence, tree structures will only be shared between correlated
outputs as one would expect. Another variant that we explore consists in
replacing the linear global fit by a relabelling of all tree leaves for
each output in turn.

The chapter is structured as follows. We show how to extend the gradient
boosting algorithms to multi-output tasks in Section 22 . We provide for
these algorithms a convergence proof on the training data and discuss
the effect of the random projection of the output space. We study
empirically the proposed approach in Section 23 . Our first experiments
compare the proposed approaches to binary relevance / single target on
artificial datasets where the output correlation is known. We also
highlight the effect of the choice and size of the random projection
space. We finally carry out an empirical evaluation of these methods on
21 real-world multi-label and 8 multi-output regression tasks. We draw
our conclusions in Section 24 .

#### 22 Gradient boosting with multiple outputs

Starting from a multi-output loss, we show in Section 22.1 how to extend
the standard gradient boosting algorithm to solve multi-output tasks,
such as multi-output regression and multi-label classification, by
exploiting existing weak model learners suited for multi-output
prediction. In Section 22.2 , we then propose to combine single random
projections of the output space with gradient boosting to automatically
adapt to the output correlation structure on these tasks. We discuss and
compare the effect of the random projection of the output space in
Section 22.3 . We give a convergence proof on the training data for the
proposed algorithms in Section 22.4 .

##### 22.1 Standard extension of gradient boosting to multi-output tasks

A loss function @xmath computes the difference between a ground truth
@xmath and a model prediction @xmath . It compares scalars with single
output tasks and vectors with multi-output tasks. The two most common
regression losses are the square loss @xmath and the absolute loss
@xmath . Their multi-output extensions are the @xmath -norm and @xmath
-norm losses:

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .1)
     @xmath   @xmath      ( \thechapter .2)
  -- -------- -------- -- -------------------

In classification, the most commonly used loss to compare a ground truth
@xmath to the model prediction @xmath is the @xmath loss @xmath , where
@xmath is the indicator function. It has two standard multiple output
extensions (i) the Hamming loss @xmath and (ii) the subset @xmath loss
@xmath :

  -- -------- -------- -- -------------------
     @xmath   @xmath      ( \thechapter .3)
     @xmath   @xmath      ( \thechapter .4)
  -- -------- -------- -- -------------------

Since these losses are discrete, they are non-differentiable and
difficult to optimize. Instead, we propose to extend the logistic loss
@xmath used for binary classification tasks to the multi-label case, as
follows:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

where we suppose that the @xmath components @xmath of the target output
vector belong to @xmath , while the @xmath components @xmath of the
predictions may belong to @xmath .

Given a training set @xmath and one of these multi-output losses @xmath
, we want to learn a model @xmath expressed in the following form

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

where the terms @xmath are selected within a hypothesis space @xmath of
weak multi-output base-learners, the coefficients @xmath are @xmath
-dimensional vectors highlighting the contributions of each term @xmath
to the ensemble, and where the symbol @xmath denotes the Hadamard
product. Note that the prediction @xmath targets the minimization of the
chosen loss @xmath , but a transformation might be needed to have a
prediction in @xmath , e.g. we would apply the @xmath function to each
output for the multi-output logistic loss to get a probability estimate
of the positive classes.

The gradient boosting method builds such a model in an iterative
fashion, as described in Algorithm 1 , and discussed below.

1: function GB-mo ( @xmath )

2: @xmath

3: for @xmath = 1 to @xmath do

4: Compute the loss gradient for the learning set samples

  -- -------- --
     @xmath   
  -- -------- --

5: Fit the negative loss gradient

  -- -------- --
     @xmath   
  -- -------- --

6: Find an optimal step length in the direction of @xmath

  -- -------- --
     @xmath   
  -- -------- --

7: @xmath

8: end for

9: return @xmath

10: end function

Algorithm 1 Gradient boosting with multi-output regressor weak models.

To build the ensemble model, we first initialize it with the constant
model defined by the vector @xmath minimizing the multi-output loss
@xmath (line 2):

  -- -------- -- -------------------
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

At each subsequent iteration @xmath , the multi-output gradient boosting
approach adds a new multi-output weak model @xmath with a weight @xmath
to the current ensemble model by approximating the minimization of the
multi-output loss @xmath :

  -- -------- -- -------------------
     @xmath      ( \thechapter .8)
  -- -------- -- -------------------

To approximate Equation \thechapter .8 , it first fits a multi-output
weak model @xmath to model the negative gradient @xmath of the
multi-output loss @xmath

  -- -------- -- -------------------
     @xmath      ( \thechapter .9)
  -- -------- -- -------------------

associated to each sample @xmath of the training set, by minimizing the
@xmath -loss:

  -- -------- -- --------------------
     @xmath      ( \thechapter .10)
  -- -------- -- --------------------

It then computes an optimal step length vector @xmath in the direction
of the weak model @xmath to minimize the multi-output loss @xmath :

  -- -------- -- --------------------
     @xmath      ( \thechapter .11)
  -- -------- -- --------------------

##### 22.2 Adapting to the correlation structure in the output-space

Binary relevance / single target of gradient boosting models and
gradient boosting of multi-output models (Algorithm 1 ) implicitly
target two extreme correlation structures. On the one hand, binary
relevance / single target predicts all outputs independently, thus
assuming that outputs are not correlated. On the other hand, gradient
boosting of multi-output models handles them all together, thus assuming
that they are all correlated. Both approaches thus exploit the available
dataset in a rather biased way. To remove this bias, we propose a more
flexible approach that can adapt itself automatically to the correlation
structure among output variables.

Our idea is that a weak learner used at some step of the gradient
boosting algorithm could be fitted on a single random projection of the
output space, rather than always targeting simultaneously all outputs or
always targeting a single a priori fixed output.

We thus propose to first generate at each iteration of the boosting
algorithm one random projection vector of size @xmath . The weak learner
is then fitted on the projection of the current residuals according to
@xmath reducing dimensionality from @xmath outputs to a single output. A
weight vector @xmath is then selected to minimize the multi-output loss
@xmath . The whole approach is described in Algorithm 2 . If the loss is
decomposable, non zero components of the weight vector @xmath highlight
the contribution of the current @xmath -th model to the overall loss
decrease. Note that sign flips due to the projection are taken into
account by the additive weights @xmath . A single output regressor can
now handle multi-output tasks through a sequence of single random
projections.

The prediction of an unseen sample @xmath by the model produced by
Algorithm 2 is now given by

  -- -------- -- --------------------
     @xmath      ( \thechapter .12)
  -- -------- -- --------------------

where @xmath is a constant prediction, and the coefficients @xmath
highlight the contribution of each model @xmath to the ensemble. Note
that it is different from Equation \thechapter .6 (no Hadamard product),
since here the weak models @xmath produce single output predictions.

1: function GB-rpo ( @xmath )

2: @xmath

3: for @xmath = 1 to @xmath do

4: Compute the loss gradient for the learning set samples

  -- -------- --
     @xmath   
  -- -------- --

5: Generate a random projection @xmath .

6: Fit the projected loss gradient

  -- -------- --
     @xmath   
  -- -------- --

7: Find an optimal step length in the direction of @xmath .

  -- -------- --
     @xmath   
  -- -------- --

8: @xmath

9: end for

10: return @xmath

11: end function

Algorithm 2 Gradient boosting on randomly projected residual spaces.

Whenever we use decision trees as models, we can grow the tree structure
on any output space and then (re)label it in another one as in Chapter
\thechapter Section 17.1 by (re)propagating the training samples in the
tree structure. This idea of leaf relabelling could be readily applied
to Algorithm 2 leading to Algorithm 3 . After fitting the decision tree
on the random projection(s) and before optimizing the additive weights
@xmath , we relabel the tree structure in the original residual space
(line 7). More precisely, each leaf is labelled by the average
unprojected residual vector of all training examples falling into that
leaf. The predition of an unseen sample is then obtained with Equation
\thechapter .6 as for Algorithm 1 . We will investigate whether it is
better or not to relabel the decision tree structure in the experimental
section. Note that Algorithm 3 can be straightforwardly used in a
multiple random projection context ( @xmath ) using a random projection
matrix @xmath . The resulting algorithm with arbitrary @xmath
corresponds to the application to gradient boosting of the idea explored
in Chapter 5 in the context of random forests. We will study in Section
22.3.3 and Section 23.3.2 the effect of the size of the projected space
@xmath .

1: function GB-relabel-rpo ( @xmath )

2: @xmath

3: for @xmath = 1 to @xmath do

4: Compute the loss gradient for the learning set samples

@xmath

5: Generate a random projection @xmath .

6: Fit a single-output tree @xmath on the projected negative loss
gradients

@xmath

7: Relabel each leaf of the tree @xmath in the original (unprojected)
residual space, by averaging at each leaf the @xmath vectors of all
examples falling into that leaf.

8: Find an optimal step length in the direction of @xmath .

  -- -------- --
     @xmath   
  -- -------- --

9: @xmath

10: end for

11: return @xmath

12: end function

Algorithm 3 Gradient boosting on randomly projected residual spaces with
relabelled decision trees as weak models.

To the three presented algorithms, we also add a constant learning rate
@xmath to shrink the size of the gradient step @xmath in the residual
space. Indeed, for a given weak model space @xmath and a loss @xmath ,
optimizing both the learning rate @xmath and the number of steps @xmath
typically improves generalization performance.

##### 22.3 Effect of random projections

Randomly projecting the output space in the context of the gradient
boosting approach has two direct consequences: (i) it strongly reduces
the size of the output space, and (ii) it randomly combines several
outputs. We will consider here the following random projection matrices
@xmath ordered from the sparsest to the densest ones:

-    Random output subsampling matrices is obtained by sampling random
    lines from the identity matrix.

-    (Sparse) Rademacher matrices is obtained by drawing its elements in
    @xmath with probability @xmath , where @xmath controls the sparsity
    of @xmath . With @xmath , we have (dense) Rademacher random
    projections . If @xmath , we will call them Achlioptas random
    projections (Achlioptas, 2003 ) . When @xmath , we will say that we
    have sparse random projections as in (Li et al., 2006 ) .

-    Gaussian matrices are obtained by drawing their elements i.i.d. in
    @xmath .

We discuss in more details the random sub-sampling projection in Section
22.3.1 and the impact of the density of random projection matrices in
Section 22.3.2 . We study the benefit to use more than a single random
projection of the output space ( @xmath ) in Section 22.3.3 . We
highlight the difference in model representations between tree ensemble
techniques, i.e. the gradient tree boosting approaches and the random
forest approaches, in Section 22.3.4 .

###### 22.3.1 @xmath-norm loss and random output sub-sampling

The gradient boosting method has an analytical solution when the loss is
the square loss or its extension the @xmath -norm loss @xmath :

-   The constant model @xmath minimizing this loss is the average output
    value of the training set given by

      -- -------- -- --------------------
         @xmath      ( \thechapter .13)
      -- -------- -- --------------------

-   The gradient of the @xmath -norm loss for the @xmath -th sample is
    the difference between the ground truth @xmath and the prediction of
    the ensemble @xmath at the current step @xmath ( @xmath ):

      -- -------- -------- -- --------------------
         @xmath   @xmath      ( \thechapter .14)
      -- -------- -------- -- --------------------

-   Once a new weak estimator @xmath has been fitted on the loss
    gradient @xmath or the projected gradient @xmath with or without
    relabelling, we have to optimize the multiplicative weight vector
    @xmath of the new weak model in the ensemble. For Algorithm 1 and
    Algorithm 3 that exploit multi-output weak learners, this amounts to

      -- -------- -------- -- --------------------
         @xmath   @xmath      ( \thechapter .15)
                  @xmath      ( \thechapter .16)
      -- -------- -------- -- --------------------

    which has the following solution:

      -- -------- -- --------------------
         @xmath      ( \thechapter .17)
      -- -------- -- --------------------

    For Algorithm 2 , we have to solve

      -- -------- -------- -- --------------------
         @xmath   @xmath      ( \thechapter .18)
                  @xmath      ( \thechapter .19)
      -- -------- -------- -- --------------------

    which has the following solution

      -- -------- -- --------------------
         @xmath      ( \thechapter .20)
      -- -------- -- --------------------

From Equation \thechapter .17 and Equation \thechapter .20 , we have
that the weight @xmath is proportional to the correlation between the
loss gradient of the output @xmath and the weak estimator @xmath . If
the model @xmath is independent of the output @xmath , the weight @xmath
will be close to zero and @xmath will thus not contribute to the
prediction of this output. On the opposite, a high magnitude of @xmath
means that the model @xmath is useful to predict the output @xmath .

If we subsample the output space at each boosting iteration (Algorithm 2
with random output sub-sampling), the weight @xmath is then proportional
to the correlation between the model fitted on the sub-sampled output
and the output @xmath . If correlations exist between the outputs, the
optimization of the constant @xmath allows to share the trained model at
the @xmath -th iteration on the sub-sampled output to all the other
outputs. In the extreme case where all outputs are independent given the
inputs, the weight @xmath is expected to be nearly zero for all outputs
except for the sub-sampled output, and Algorithm 2 would be equivalent
to the binary relevance / single target approach. If all outputs are
strictly identical, the elements of the constant vector @xmath would
have the same value, and Algorithm 2 would be equivalent to the
multi-output gradient boosting approach (Algorithm 1 ). Algorithm 2
would also produce in this case the exact same model as binary relevance
/ single target approach asymptotically but it would require @xmath
times less trees to reach similar performance, as each tree would be
shared by all @xmath outputs.

Algorithm 3 with random output sub-sampling is a gradient boosting
approach fitting one decision tree at each iteration on a random output
space and relabelling the tree in the original output space. The leaf
relabelling procedure minimizes the @xmath -norm loss over the training
samples by averaging the output values of the samples reaching the
corresponding leaves. In this case, the optimization of the weight
@xmath is unnecessary, as it would lead to an all ones vector. For
similar reasons if the multi-output gradient boosting method (Algorithm
1 ) uses decision trees as weak estimators, the weight @xmath is also an
all ones vector as the leaf predictions already minimize the @xmath
-norm loss. The difference between these two algorithms is that
Algorithm 3 grows trees using a random output at each iteration instead
of all of them with Algorithm 1 .

###### 22.3.2 Density of the random projections

In Chapter \thechapter , we have combined the random forest method with
a wide variety of random projection schemes. While the algorithms
presented in this chapter were originally devised with random output
sub-sampling in mind (see Section 22.3.1 ), it seems natural to also
combine the proposed approaches with random projection schemes such as
Gaussian random projections or (sparse) Rademacher random projections.

With random output sub-sampling, the projection matrix @xmath is
extremely sparse as only one element is non zero. With denser random
projections, the weak estimators of Algorithm 2 and Algorithm 3 are
fitted on the projected gradient loss @xmath . It means that a weak
estimator @xmath is trying to model the direction of a weighted
combination of the gradient loss.

Otherwise said, the weak model fitted at the @xmath -th step
approximates a projection @xmath of the gradient losses given the input
vector. We can interpret the weight @xmath when minimizing the @xmath
-norm loss as the correlation between the output @xmath and a weighted
approximation of the output variables @xmath . With an extremely sparse
projection having only one non zero element, we have the situation
described in the previous section. If we have two non zero elements, we
have the following extreme cases: (i) both combined outputs are
identical and (ii) both combined outputs are independent given the
inputs. In the first situation, the effect is identical to the case
where we sub-sample only one output. In the second situation, the weak
model makes a compromise between the independent outputs given by @xmath
. Between those two extremes, the loss gradient direction @xmath
approximated by the weak model is useful to predict both outputs. The
random projection of the output space will indeed prevent over-fitting
by inducing some variance in the learning process. The previous
reasoning can be extended to more than two output variables.

Dense random projection schemes, such as Gaussian random projection,
consider a higher number of outputs together and is hoped to speed up
convergence by increasing the correlation between the fitted tree in the
projected space and the residual space. Conversely, sparse random
projections, such as random output sub-sampling, make the weak model
focus on few outputs.

###### 22.3.3 Gradient tree boosting and multiple random projections

The gradient boosting multi-output strategy combining random projections
and tree relabelling (Algorithm 3 ) can use random projection matrices
@xmath with more than one line ( @xmath ).

The weak estimators are multi-output regression trees using the variance
as impurity criterion to grow their tree structures. With an increasing
number of projections @xmath , we have the theoretical guarantee (see
Chapter \thechapter ) that the variance computed in the projected space
is an approximation of the variance in the original output space.

When the projected space is of infinite size @xmath , the decision trees
grown on the original space or on the projected space are identical as
the approximation of the variance is exact. We thus have that Algorithm
3 is equivalent to the gradient boosting with multi-output regression
tree method (Algorithm 1 ).

Whenever the projected space is of finite size ( @xmath ), Algorithm 3
is thus an approximation of Algorithm 1 . We study empirically the
effect of the number of projections @xmath in Algorithm 3 in Section
23.3.2 .

###### 22.3.4 Representation bias of decision tree ensembles

Random forests and gradient tree boosting build an ensemble of trees
either independently or sequentially, and thus offer different
bias/variance tradeoffs. The predictions of all these ensembles can be
expressed as a weighted combination of the ground truth outputs of the
training set samples. In the present section, we discuss the differences
between single tree models, random forest models and gradient tree
boosting models in terms of the representation biases of the obtained
models. We also highlight the differences between single target models
and multi-output tree models.

###### Single tree models.

The prediction of a regression tree learner can be written as a weighted
linear combination of the training samples @xmath . We associate to each
training sample @xmath a weight function @xmath which gives the
contribution of a ground truth @xmath to predict an unseen sample @xmath
. The prediction of a single output tree @xmath is given by

  -- -------- -- --------------------
     @xmath      ( \thechapter .21)
  -- -------- -- --------------------

The weight function @xmath is non zero if both the samples @xmath and
the unseen sample @xmath reach the same leaf of the tree. If both @xmath
and @xmath end up in the same leaf of the tree, @xmath is equal to the
inverse of the number of training samples reaching that leaf. The weight
@xmath can thus be rewritten as @xmath and the function @xmath is
actually a positive semi-definite kernel (Geurts et al., 2006a ) .

We can also express multi-output models as a weighted sum of the
training samples. With a single target regression tree , we have an
independent weight function @xmath for each sample of the training set
and each output as we fit one model per output. The prediction of this
model for output @xmath is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .22)
  -- -------- -- --------------------

With a multi-output regression tree , the decision tree structure is
shared between all outputs so we have a single weight function @xmath
for each training sample:

  -- -------- -- --------------------
     @xmath      ( \thechapter .23)
  -- -------- -- --------------------

###### Random forest models.

If we have a single target random forest model , the prediction of the
@xmath -th output combines the predictions of the @xmath models of the
ensemble in the following way:

  -- -------- -- --------------------
     @xmath      ( \thechapter .24)
  -- -------- -- --------------------

with one weight function @xmath per tree, sample and output. We note
that we can combine the weights of the individual trees into a single
one per sample and per output

  -- -------- -- --------------------
     @xmath      ( \thechapter .25)
  -- -------- -- --------------------

The prediction of the @xmath -th output for an ensemble of independent
models has the same form as a single target regression tree model:

  -- -------- -- --------------------
     @xmath      ( \thechapter .26)
  -- -------- -- --------------------

We can repeat the previous development with a multi-output random forest
model . The prediction for the @xmath -th output of an unseen sample
@xmath combines the predictions of the @xmath trees:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .27)
  -- -------- -------- -- --------------------

with

  -- -------- -- --------------------
     @xmath      ( \thechapter .28)
  -- -------- -- --------------------

With this framework, the prediction of an ensemble model has the same
form as the prediction of a single constituting tree.

###### Gradient tree boosting models.

The prediction of a single output gradient boosting tree ensemble is
given by

  -- -------- -- --------------------
     @xmath      ( \thechapter .29)
  -- -------- -- --------------------

but also as

  -- -------- -- --------------------
     @xmath      ( \thechapter .30)
  -- -------- -- --------------------

where the weight @xmath takes into account the learning rate @xmath ,
the prediction of all tree models @xmath and the associated @xmath .
Given the similarity between gradient boosting prediction and random
forest model, we deduce that the single target gradient boosting tree
ensemble has the form of Equation \thechapter .22 and that multi-output
gradient tree boosting (Algorithm 1 ) and gradient boosting tree with
projection of the output space and relabelling (Algorithm 3 ) has the
form of Equation \thechapter .23 .

However, we note that the prediction model of the gradient tree boosting
with random projection of the output space (Algorithm 2 ) is not given
by Equation \thechapter .22 and Equation \thechapter .23 as the
prediction of a single output @xmath can combine the prediction of all
@xmath outputs. More formally, the prediction of the @xmath -th output
is given by:

  -- -------- -- --------------------
     @xmath      ( \thechapter .31)
  -- -------- -- --------------------

where the weight function @xmath takes into account the contribution of
the @xmath -th model fitted on a random projection @xmath of the output
space to predict the @xmath -th output using the @xmath -th outputs and
the @xmath -th sample. The triple summation can be simplified by using a
single weight to summarize the contribution of all @xmath models:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .32)
  -- -------- -------- -- --------------------

Between the studied methods, we can distinguish three groups of
multi-output tree models. The first one considers that all outputs are
independent as with binary relevance / single target trees, random
forests or gradient tree boosting models. The second group with
multi-output random forests, gradient boosting of multi-output tree and
gradient boosting with random projection of the output space and
relabelling share the tree structures between all outputs, but the leaf
predictions are different for each output. The last and most flexible
group is the gradient tree boosting with random projection of the output
space sharing both the tree structures and the leaf predictions. We will
highlight in the experiments the impact of these differences in
representation biases.

##### 22.4 Convergence when @xmath

Similarly to (Geurts et al., 2007 ) , we can prove the convergence of
the training-set loss of the gradient boosting with multi-output models
(Algorithm 1 ), and gradient boosting on randomly projected spaces with
(Algorithm 2 ) or without relabelling (Algorithm 3 ).

Since the loss function is lower-bounded by @xmath , we merely need to
show that the loss @xmath is non-increasing on the training set at each
step @xmath of the gradient boosting algorithm.

For Algorithm 1 and Algorithm 3 , we note that

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      ( \thechapter .33)
  -- -------- -------- -- --------------------

and the learning-set loss is hence non increasing with @xmath if we use
a learning rate @xmath . If the loss @xmath is convex in its second
argument @xmath (which is the case for those loss-functions that we use
in practice), then this convergence property actually holds for any
value @xmath of the learning rate. Indeed, we have

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

given Equation \thechapter .33 and the convexity property.

For Algorithm 2 , we have a weak estimator @xmath fitted on a single
random projection of the output space @xmath with a multiplying constant
vector @xmath , and we have:

  -- -------- -------- -- --------------------
     @xmath   @xmath      
              @xmath      ( \thechapter .34)
  -- -------- -------- -- --------------------

and the error is also non increasing for Algorithm 2 , under the same
conditions as above.

The previous development shows that Algorithm 1 , Algorithm 2 and
Algorithm 3 are converging on the training set for a given loss @xmath .
The binary relevance / single target of gradient boosting regression
trees admits a similar convergence proof. We expect however the
convergence speed of the binary relevance / single target to be lower
assuming that it fits one weak estimator for each output in a round
robin fashion.

#### 23 Experiments

We describe the experimental protocol in Section 23.1 . Our first
experiments in Section 23.2 illustrate the multi-output gradient
boosting methods on synthetic datasets where the output correlation
structure is known. The effect of the choice and / or the number of
random projections of the output space is later studied for Algorithm 2
and Algorithm 3 in Section 23.3 . We compare multi-output gradient
boosting approaches and multi-output random forest approaches in Section
23.4 over 29 real multi-label and multi-output datasets.

##### 23.1 Experimental protocol

We describe the metrics used to assess the performance of the supervised
learning algorithms in Section 23.1.1 . The protocol used to optimize
hyper-parameters is given in Section 23.1.2 .

Note that the datasets used in the following experiments are described
in Appendix \thechapter . Whenever the number of testing samples is not
given, we use half of the data as training set and half of the data as
testing set.

###### 23.1.1 Accuracy assessment protocol

We assess the accuracy of the predictors on a test set using the “Label
Ranking Average Precision (LRAP)” (defined in Section 6.3 ) for
multi-label classification tasks and the “macro- @xmath score” (defined
in Section 6.4 ) for multi-output regression tasks.

###### 23.1.2 Hyper-parameter optimization protocol

The hyper-parameters of the supervised learning algorithms are optimized
as follows: we define an hyper-parameter grid and the best
hyper-parameter set is selected using @xmath of the training samples as
a validation set. The results shown are averaged over five random split
of the dataset while preserving the training-testing set size ratio.

For the boosting ensembles, we optimize the learning rate @xmath among
@xmath and use decision trees as weak models whose hyper-parameters are
also optimized: the number of features drawn at each node @xmath during
the tree growth is selected among @xmath , the maximum number of tree
leaves @xmath grown in best-first fashion is chosen among @xmath . Note
that a decision tree with @xmath and @xmath is called a stump. We add
new weak models to the ensemble by minimizing either the square loss or
the absolute loss (or their multi-output extensions) in regression and
either the square loss or the logistic loss (or their multi-output
extensions) in classification, the choice of the loss being an
additional hyper-parameter tuned on the validation set.

We also optimize the number of boosting steps @xmath of each gradient
boosting algorithm over the validation set. However note that the number
of steps has a different meaning depending on the algorithm. For binary
relevance / single target gradient boosting, the number of boosting
steps @xmath gives the number of weak models fitted per output. The
implemented algorithm here fits weak models in a round robin fashion
over all outputs. For all other (multi-output) methods, the number of
boosting steps @xmath is the total number of weak models for all outputs
as only one model is needed to fit all outputs. The computing time of
one boosting iteration is thus different between the approaches. We will
set the budget, the maximal number of boosting steps @xmath , for each
algorithm to @xmath on synthetic experiments (see Section 23.2 ) so that
the performance of the estimator is not limited by the computational
power. On the real datasets however, this setting would have been too
costly. We decided instead to limit the computing time allocated to each
gradient boosting algorithm on each classification (resp. regression)
problem to @xmath (resp. @xmath ), where @xmath is the time needed on
this specific problem for one iteration of multi-output gradient
boosting (Algorithm 1 ) with stumps and the @xmath -norm loss. The
maximum number of iterations, @xmath , is thus set independently for
each problem and each hyper-parameter setting such that this time
constraint is satisfied. As a consequence, all approaches thus receive
approximately the same global time budget for model training and
hyper-parameter optimization.

For the random forest algorithms, we use the default hyper-parameter
setting suggested in (Hastie et al., 2009 ) , which corresponds in
classification to 100 totally developed trees with @xmath and in
regression to @xmath trees with @xmath and a minimum of 5 samples to
split a node ( @xmath ).

The base learner implementations are based on the random-output-trees ⁸
⁸ 8 https://github.com/arjoly/random-output-trees (Joly et al., 2014 )
version 0.1 and on the scikit-learn (Buitinck et al., 2013 ; Pedregosa
et al., 2011 ) of version 0.16 Python package. The algorithms presented
in this chapter will be provided in random-output-trees version 0.2.

##### 23.2 Experiments on synthetic datasets with known output
correlation structures

We study here the proposed boosting approaches on synthetic datasets
whose output correlation structures are known. The datasets are first
presented in Section 23.2.1 . We then compare on these datasets
multi-output gradient boosting approaches in terms of their convergence
speed in Section 23.2.2 and in terms of their best performance whenever
hyper-parameters are optimized in Section 23.2.3 .

###### 23.2.1 Synthetic datasets

To illustrate multi-output boosting strategies, we use three synthetic
datasets with a specific output structure: (i) chained outputs, (ii)
totally correlated outputs and (iii) fully independent outputs. Those
tasks are derived from the friedman1 regression dataset which consists
in solving the following single target regression task (Friedman, 1991 )

  -- -------- -------- -------- -- --------------------
     @xmath   @xmath   @xmath      ( \thechapter .35)
  -- -------- -------- -------- -- --------------------

with @xmath and @xmath where @xmath is an identity matrix of size @xmath
.

The friedman1-chain problem consists in @xmath regression tasks forming
a chain obtained by cumulatively adding independent standard Normal
noise. We draw samples from the following distribution

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .36)
     @xmath   @xmath      ( \thechapter .37)
  -- -------- -------- -- --------------------

with @xmath and @xmath . Given the chain structure, the output with the
least amount of noise is the first one of the chain and averaging a
subset of the outputs would not lead to any reduction of the output
noise with respect to the first output, since total noise variance
accumulates more than linearly with the number of outputs. The optimal
multi-output strategy is thus to build a model using only the first
output and then to replicate the prediction of this model for all other
outputs.

The friedman1-group problem consists in solving @xmath regression tasks
simultaneously obtained from one friedman1 problem without noise where
an independent normal noise is added. Given @xmath and @xmath , we have
to solve the following task:

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .38)
  -- -------- -------- -- --------------------

If the output-output structure is known, the additive noises @xmath can
be filtered out by averaging all outputs. The optimal strategy to
address this problem is thus to train a single output regression model
to fit the average output. Predictions on unseen data would be later
done by replicating the output of this model for all outputs.

The friedman1-ind problem consists in @xmath independent friedman1
tasks. Drawing samples from @xmath and @xmath , we have

  -- -------- -------- -- --------------------
     @xmath   @xmath      ( \thechapter .39)
  -- -------- -------- -- --------------------

where @xmath is a slice of feature vector from feature @xmath to @xmath
. Since all outputs are independent, the best multi-output strategy is
single target: one independent model fits each output

For each multi-output friedman problem, we consider 300 training
samples, 4000 testing samples and @xmath outputs.

###### 23.2.2 Convergence with known output correlation structure

We first study the macro- @xmath score convergence as a function of time
(see Figure \thechapter .1 ) for three multi-output gradient boosting
strategies: (i) single target of gradient tree boosting (st-gbrt), (ii)
gradient boosting with multi-output regression tree (gbmort, Algorithm 1
) and (iii) gradient boosting with output subsampling of the output
space (gbrt-rpo-subsample, Algorithm 2 ). We train each boosting
algorithm on the three friedman1 artificial datasets with the same set
of hyper-parameters: a learning rate of @xmath and stumps as weak
estimators (a decision tree with @xmath , @xmath ) while minimizing the
square loss.

On the friedman1-chain (see Figure (a)a ) and friedman1-group (see
Figure (b)b ), gbmort and gbrt-rpo-subsampled converge more than 100
times faster (note the logarithmic scale of the abscissa) than single
target. Furthermore, the optimal macro- @xmath is slightly better for
gbmort and gbrt-rpo-subsampled than st-gbrt. Since all outputs are
correlated on both datasets, gbmort and gbrt-rpo-susbsampled are
exploiting the output structure to have faster convergence. The gbmort
method exploits the output structure by filtering the output noise as
the stump fitted at each iteration is the one that maximizes the
reduction of the average output variance. By contrast,
gbrt-rpo-subsample detects output correlations by optimizing the @xmath
constant and then shares the information obtained by the current weak
model with all other outputs.

On the friedman1-ind dataset (see Figure (c)c ), all three methods
converge at the same rate. However, the single target strategy converges
to a better optimum than gbmort and gbrt-rpo-subsample. Since all
outputs are independent, single target enforces the proper correlation
structure (see Figure (c)c ). The gbmort method has the worst
performance as it assumes the wrong set of hypotheses. The
gbrt-rpo-subsampled method pays the price of its flexibility by
over-fitting the additive weight associated to each output, but less
than gbmort.

This experiment confirms that enforcing the right correlation structure
yields faster convergence and the best accuracy. Nevertheless, the
output structure is unknown in practice. We need flexible approaches
such as gbrt-rpo-subsampled that automatically detects and exploits the
correlation structure.

###### 23.2.3 Performance and output modeling assumption

The presence or absence of structures among the outputs have shown to
affect the convergence speed of multi-output gradient boosting methods.
As discussed in (Cheng et al., 2010 ) , we talk about conditionally
independent outputs when:

  -- -------- --
     @xmath   
  -- -------- --

and about unconditionally independent outputs when:

  -- -------- --
     @xmath   
  -- -------- --

When the outputs are not conditionally independent and the loss function
can not be decomposed over the outputs (eg., the subset @xmath loss),
one might need to model the joint output distribution @xmath to obtain a
Bayes optimal prediction. If the outputs are conditionally independent
however or if the loss function can be decomposed over the outputs, then
a Bayes optimal prediction can be obtained by modeling separately the
marginal conditional output distributions @xmath for all @xmath . This
suggests that in this case, binary relevance / single target is not
really penalized asymptotically with respect to multiple output methods
for not considering the outputs jointly. In the case of an infinite
sample size, it is thus expected to provide as good models as all the
multiple output methods. Since in practice we have to deal with finite
sample sizes, multiple output methods may provide better results by
better controlling the bias/variance trade-off.

Let us study this question on the three synthetic datasets:
friedman1-chain, friedman1-group or friedman1-ind. We optimize the
hyper-parameters with a computational budget of 10000 weak models per
hyper-parameter set. Five strategies are compared (i) the
artificial-gbrt method, which assumes that the output structure is known
and implements the optimal strategy on each problem as explained in
Section 23.2.1 , (ii) single target of gradient boosting regression
trees (st-gbrt), (iii) gradient boosting with multi-output regression
tree (gbmort, Algorithm 1 ) and gradient boosting with randomly
sub-sampled outputs (iv) without relabelling (gbrt-rpo-subsampled,
Algorithm 2 ) and (v) with relabelling (gbrt-relabel-rpo-subsampled,
Algorithm 3 ). All boosting algorithms minimize the square loss, the
absolute loss or their multi-outputs extension the @xmath -norm loss.

We give the performance on the three tasks for each estimator in Table
\thechapter .1 and the p-value of Student’s paired @xmath -test
comparing the performance of two estimators on the same dataset in Table
\thechapter .2 .

As expected, we obtain the best performance if the output correlation
structure is known with the custom strategies implemented with
artifical-gbrt. Excluding this artificial method, the best boosting
methods on the two problems with output correlations, friedman1-chain
and friedman1-group, are the two gradient boosting approaches with
output subsampling (gbrt-relabel-rpo-subsampled and
gbrt-rpo-subsampled).

In friedman1-chain, the output correlation structure forms a chain as
each new output is the previous one in the chain with a noisy output.
Predicting outputs at the end of the chain, without using the previous
ones, is a difficult task. The single target approach is thus expected
to be sub-optimal. And indeed, on this problem, artificial-gbrt,
gbrt-relabel-rpo-subsampled and gbrt-rpo-subsampled are significantly
better than st-gbrt (with @xmath ). All the multi-output methods,
including gbmort, are indistinguishable from a statistical point of
view, but we note that gbmort is however not significantly better than
st-gbrt.

In friedman1-group, among the ten pairs of algorithms, four are not
significantly different, showing a p-value greater than 0.05 (see Table
\thechapter .2 ). We first note that gbmort is not better than st-gbrt
while exploiting the correlation. Secondly, the boosting methods with
random output sub-sampling are the best methods. They are however not
significantly better than gbmort and significantly worse than
artificial-gbrt, which assumes the output structure is known. Note that
gbrt-relabel-rpo-subsampled is significantly better than
gbrt-rpo-subsampled.

In friedman1-ind, where there is no correlation between the outputs, the
best strategy is single target which makes independent models for each
output. From a conceptual and statistical point of view, there is no
difference between artificial-gbrt and st-gbrt. The gbmort algorithm,
which is optimal when all outputs are correlated, is here significantly
worse than all other methods. The two boosting methods with output
subsampling (gbrt-rpo-subsampled and gbrt-relabel-rpo-subsampled
method), which can adapt themselves to the absence of correlation
between the outputs, perform better than gbmort, but they are
significantly worse than st-gbrt. For these two algorithms, we note that
not relabelling the leaves (gbrt-rpo-subsampled) leads to superior
performance than relabelling them (gbrt-relabel-rpo-subsampled). Since
in friedman1-ind the outputs have disjoint feature support, the test
nodes of a decision tree fitted on one output will partition the samples
using these features. Thus, it is not suprising that relabeling the
trees leaves actually deteriorates performance.

In the previous experiment, all the outputs were dependent of the
inputs. However in multi-output tasks with very high number of outputs,
it is likely that some of them have few or no links with the inputs,
i.e., are pure noise. Let us repeat the previous experiments with the
main difference that we add to the original 16 outputs 16 purely noisy
outputs obtained through random permutations of the original outputs. We
show the results of optimizing each algorithm in Table \thechapter .3
and the associated p-values in Table \thechapter .4 . We report the
macro- @xmath score computed either on all outputs (macro- @xmath )
including the noisy outputs or only on the 16 original outputs
(half-macro- @xmath ). P-value were computed between each pair of
algorithms using Student’s @xmath -test on the macro @xmath score
computed on all outputs.

We observe that the gbrt-rpo-subsampled algorithm has the best
performance on friedman1-chain and friedman1-group and is the second
best on the friedman1-ind, below st-gbrt. Interestingly on
friedman1-chain and friedman1-group, this algorithm is significantly
better than all the others, including gbmort. Since this latter method
tries to fit all outputs simultaneously, it is the most disadvantaged by
the introduction of the noisy outputs.

##### 23.3 Effect of random projection

With the gradient boosting and random projection of the output space
approaches (Algorithms 2 and 3 ), we have considered until now only
sub-sampling a single output at each iteration as random projection
scheme. In Section 23.3.1 , we show empirically the effect of other
random projection schemes such as Gaussian random projection. In Section
23.3.2 , we study the effect of increasing the number of projections in
the gradient boosting algorithm with random projection of the output
space and relabelling (parameter @xmath of Algorithm 3 ). We also show
empirically the link between Algorithm 3 and gradient boosting with
multi-output regression tree (Algorithm 1 ).

###### 23.3.1 Choice of the random projection scheme

Beside random output sub-sampling, we can combine the multi-output
gradient boosting strategies (Algorithms 2 and 3 ) with other random
projection schemes. A key difference between random output sub-sampling
and random projections such as Gaussian and (sparse) Rademacher
projections is that the latter combines together several outputs.

We show in Figures \thechapter .2 , \thechapter .3 and \thechapter .4
the LRAP or macro- @xmath score convergence of gradient boosting with
randomly projected outputs (gbrt-rpo, Algorithm 2 ) respectively on the
mediamill, delicious, and Friedman1-ind datasets with different random
projection schemes.

The impact of the random projection scheme on convergence speed of
gbrt-rpo (Algorithm 2 ) is very problem dependent. On the mediamill
dataset, Gaussian, Achlioptas, or sparse random projections all improve
convergence speed by a factor of 10 (see Figure \thechapter .2 )
compared to subsampling randomly only one output. On the delicious
(Figure \thechapter .3 ) and friedman1-ind (Figure \thechapter .4 ),
this is the opposite: subsampling leads to faster convergence than all
other projections schemes. Note that we have the same behavior if one
relabels the tree structure grown at each iteration as in Algorithm 3
(results not shown).

Dense random projections, such as Gaussian random projections, force the
weak model to consider several outputs jointly and it should thus only
improve when outputs are somewhat correlated (which seems to be the case
on mediamill). When all of the outputs are independent or the
correlation is less strong, as in friedman1-ind or delicious, this has a
detrimental effect. In this situation, sub-sampling only one output at
each iteration leads to the best performance.

###### 23.3.2 Effect of the size of the projected space

The multi-output gradient boosting strategy combining random projections
and tree relabelling (Algorithm 3 ) can use more than one random
projection ( @xmath ) by using multi-output trees as base learners. In
this section, we study the effect of the size of the projected space
@xmath in Algorithm 3 . This approach corresponds to the one developed
in Chapter \thechapter for random forest.

Figure \thechapter .5 shows the LRAP score as a function of the fitting
time for gbmort (Algorithm 1 ) and gbrt-relabel-rpo (Algorithm 3 ) with
either Gaussian random projection (see Figure (a)a ) or output
subsampling (see Figure (b)b ) for a number of projections @xmath on the
delicious dataset. In Figure (a)a and Figure (b)b , one Gaussian random
projection or one sub-sampled output has faster convergence than their
counterparts with a higher number of projections @xmath and gbmort at
fixed computational budget. Note that when the number of projections
@xmath increases, gradient boosting with random projection of the output
space and relabeling becomes similar to gbmort.

Instead of fixing the computational budget as a function of the training
time, we now set the computational budget to 100 boosting steps. On the
delicious dataset, gbrt-relabel-rpo (Algorithm 3 ) with Gaussian random
projection yields approximately the same performance as gbmort with
@xmath random projections as shown in Figure (a)a and reduces computing
times by a factor 7 at @xmath projections (see Figure (b)b ).

These experiments show that gradient boosting with random projection and
relabelling (gbrt-relabel-rpo, Algorithm 3 ) is indeed an approximation
of gradient boosting with multi-output trees (gbmort, Algorithm 1 ). The
number of random projections @xmath influences simultaneously the
bias-variance tradeoff and the convergence speed of Algorithm 3 .

##### 23.4 Systematic analysis over real world datasets

We perform a systematic analysis over real world multi-label
classification and multi-output regression datasets. For this study, we
evaluate the proposed algorithms: gradient boosting of multi-output
regression trees (gbmort, Algorithm 1 ), gradient boosting with random
projection of the output space (gbrt-rpo, Algorithm 2 ), and gradient
boosting with random projection of the output space and relabelling
(gbrt-relabel-rpo, Algorithm 3 ). For the two latter algorithms, we
consider two random projection schemes: (i) Gaussian random projection,
a dense random projection, and (ii) random output sub-sampling, a sparse
random projection. They will be compared to three common and well
established tree-based multi-output algorithms: (i) binary relevance /
single target of gradient boosting regression tree (br-gbrt / st-gbrt),
(ii) multi-output random forest (mo-rf) and (iii) binary relevance /
single target of random forest models (br-rf / st-rf).

We will compare all methods on multi-label tasks in Section 23.4.1 and
on multi-output regression tasks in Section 23.4.2 . Following the
recommendations of (Demšar, 2006 ) , we use the Friedman test and its
associated Nemenyi post-hoc test. Pairwise comparisons are also carried
out using the Wilcoxon signed ranked test.

###### 23.4.1 Multi-label datasets

Table \thechapter .5 and Table \thechapter .6 show the performance of
the random forest models and the boosting algorithms over the 21
multi-label datasets. The critical distance diagram of Figure
\thechapter .7 gives the ranks of the algorithms and has an associated
Friedman test p-value of @xmath with a critical distance of @xmath given
by the Nemenyi post-hoc test ( @xmath ). Thus, we can reject the null
hypothesis that all methods are equivalent. Table \thechapter .7 gives
the outcome of the pairwise Wilcoxon signed ranked tests.

The best average performer is gbrt-relabel-rpo-gaussian which is
significantly better according to the Nemenyi post-hoc test than all
methods except gbrt-rpo-gaussian and gbmort.

Gradient boosting with the Gaussian random projection has a
significantly better average rank than the random output sub-sampling
projection. Relabelling tree leaves allows to have better performance on
the 21 multi-label dataset. Indeed, both gbrt-relabel-rpo-gaussian and
gbrt-relabel-rpo-subsampled are better ranked and significantly better
than their counterparts without relabelling (gbrt-rpo-gaussian and
gbrt-rpo-subsampled). These results somewhat contrast with the results
obtained on the artificial datasets, where relabelling was always
counterproductive.

Among all compared methods, br-gbrt has the worst rank and it is
significantly less good than all gbrt variants according to the Wilcoxon
signed rank test. This might be actually a consequence of the constant
budget in time that was allocated to all methods (see Section 23.1 ).
All methods were given the same budget in time but, given the very slow
convergence rate of br-gbrt, this budget may not allow to grow enough
trees per output with this method to reach competitive performance.

We notice also that both random forests based methods (mo-rf and br-rf)
are less good than all gbrt variants, most of the time significantly,
except for br-gbrt. It has to be noted however that no hyper-parameter
was tuned for the random forests. Such tuning could slightly change our
conclusions, although random forests often work well with default
setting.

###### 23.4.2 Multi-output regression datasets

Table \thechapter .8 shows the performance of the random forest models
and the boosting algorithms over the 8 multi-output regression datasets.
The critical distance diagram of Figure \thechapter .8 gives the rank of
each estimator. The associated Friedman test has a p-value of @xmath .
Given the outcome of the test, we can therefore not reject the null
hypothesis that the estimator performances can not be distinguished.
Table \thechapter .9 gives the outcomes of the pairwise Wilcoxon signed
ranked tests. They confirm the fact that all methods are very close to
each other as only two comparisons show a p-value lower than 0.05 (st-rf
is better than st-gbrt and gbrt-rpo-subsampled). This lack of
statistical power is probably partly due here to the smaller number of
datasets included in the comparison (8 problems versus 21 problems in
classification).

If we ignore statistical tests, as with multi-label tasks,
gbrt-relabel-rpo-gaussian has the best average rank and st-gbrt the
worst average rank. This time however, gbrt-relabel-rpo-gaussian is
followed by the random forest based algorithms (st-rf and mo-rf) and
gbmort. Given the lack of statistical significance, this ranking should
however be intrepreted cautiously.

#### 24 Conclusions

In this chapter, we have first formally extended the gradient boosting
algorithm to multi-output tasks leading to the “multi-output gradient
boosting algorithm” (gbmort). It sequentially minimizes a multi-output
loss using multi-output weak models considering that all outputs are
correlated. By contrast, binary relevance / single target of gradient
boosting models fit one gradient boosting model per output considering
that all outputs are independent. However in practice, we do not expect
to have either all outputs independent or all outputs dependent. So, we
propose a more flexible approach which adapts automatically to the
output correlation structure called “gradient boosting with random
projection of the output space” (gbrt-rpo). At each boosting step, it
fits a single weak model on a random projection of the output space and
optimize a multiplicative weight separately for each output. We have
also proposed a variant of this algorithm (gbrt-relabel-rpo) only valid
with decision trees as weak models: it fits a decision tree on the
randomly projected space and then it relabels tree leaves with
predictions in the original (residual) output space. The combination of
the gradient boosting algorithm and the random projection of the output
space yields faster convergence by exploiting existing correlations
between the outputs and by reducing the dimensionality of the output
space. It also provides new bias-variance-convergence trade-off
potentially allowing to improve performance.

We have evaluated in depth these new algorithms on several artificial
and real datasets. Experiments on artificial problems highlighted that
gb-rpo with output subsampling offers an interesting tradeoff between
single target and multi-output gradient boosting. Because of its
capacity to automatically adapt to the output space structure, it
outperforms both methods in terms of convergence speed and accuracy when
outputs are dependent and it is superior to gbmort (but not st-rt) when
outputs are fully independent. On the 29 real datasets, gbrt-relabel-rpo
with the denser Gaussian projections turns out to be the best overall
approach on both multi-label classification and multi-output regression
problems, although all methods are statistically undistinguisable on the
regression tasks. Our experiments also show that gradient boosting based
methods are competitive with random forests based methods. Given that
multi-output random forests were shown to be competitive with several
other multi-label approaches in (Madjarov et al., 2012 ) , we are
confident that our solutions will be globally competitive as well,
although a broader empirical comparison should be conducted as future
work. One drawback of gradient boosting with respect to random forests
however is that its performance is more sensitive to its
hyper-parameters that thus require careful tuning. Although not
discussed in this chapter, besides predictive performance, gbrt-rpo
(without relabeling) has also the advantage of reducing model size with
respect to mo-rf (multi-output random forests) and gbmort, in particular
in the presence of many outputs. Indeed, in mo-rf and gbmort, one needs
to store a vector of the size of the number of outputs per leaf node. In
gbrt-rpo, one needs to store only one real number (a prediction for the
projection) per leaf node and a vector of the size of the number of
outputs per tree ( @xmath ). At fixed number of trees and fixed tree
complexity, this could lead to a strong reduction of the model memory
requirement when the number of labels is large. Note that the approach
proposed in Chapter \thechapter does not solve this issue because of
leaf node relabeling. This could be addressed by desactivating leaf
relabeling and inverting the projection at prediction time to obtain a
prediction in the original output space, as done for example in (Hsu
et al., 2009 ; Kapoor et al., 2012 ; Tsoumakas et al., 2014 ) . However,
this would be at the expense of computing times at prediction time and
of accuracy because of the potential introduction of errors at the
decoding stage. Finally, while we restricted our experiments here to
tree-based weak learners, Algorithms 1 and 2 are generic and could
exploit respectively any multiple output and any single output
regression method. As future work, we believe that it would interesting
to evaluate them with other weak learners.

## Part III Exploiting sparsity for growing and compressing decision
trees

### Chapter \thechapter @xmath-based compression of random forest models

Outline Random forests are effective supervised learning methods
applicable to large-scale datasets. However, the space complexity of
tree ensembles, in terms of their total number of nodes, is often
prohibitive, specially in the context of problems with large sample
sizes and very high-dimensional input spaces. We propose to study their
compressibility by applying a @xmath -based regularization to the set of
indicator functions defined by all their nodes. We show experimentally
that preserving or even improving the model accuracy while significantly
reducing its space complexity is indeed possible. This chapter extends
on previous work published in Arnaud Joly, François Schnitzler, Pierre
Geurts, and Louis Wehenkel. L1-based compression of random forest
models. In European Symposium on Artificial Neural Networks,
Computational Intelligence and Machine Learning, 2012.

High-dimensional supervised learning problems, e.g. in image
exploitation and bioinformatics, are more frequent than ever. Tree-based
ensemble methods, such as random forests (Breiman, 2001 ) and extremely
randomized trees (Geurts et al., 2006a ) , are effective variance
reduction techniques offering in this context a good trade-off between
accuracy, computational complexity, and interpretability. The number of
nodes of a tree ensemble grows as @xmath ( @xmath being the size of the
learning sample and @xmath the number of trees in the ensemble).
Empirical observations show that the variance of individual trees
increases with the dimension @xmath of the original feature space used
to represent the inputs of the learning problem. Hence, the number
@xmath of ensemble terms yielding near-optimal accuracy, which is
proportional to this variance, also increases with @xmath . The net
result is that the space complexity of these tree-based ensemble methods
will grow as @xmath , which may jeopardize their practicality in large
scale problems, or when memory is limited.

While pruning of single tree models is a standard approach, less work
has been devoted to pruning ensembles of trees. On the one hand, Geurts
( 2000 ) proposes to transpose the classical cost-complexity pruning of
individual trees to ensembles. On the other hand, Meinshausen ( 2010 );
Friedman and Popescu ( 2008 ); Meinshausen et al. ( 2009 ) propose to
improve model interpretability by selecting optimal rule subsets from
tree-ensembles. Another approach to reduce complexity and/or improve
accuracy of ensembles of trees is to merely select an optimal subset of
trees from a very large ensemble generated in a random fashion at the
first hand (see, e.g. (Bernard et al., 2009 ; Martinez-Muoz et al., 2009
) ).

To further investigate the feasibility of reducing the space complexity
of tree-based ensemble models, we consider in this chapter the following
method (Joly et al., 2012 ) : (i) build an ensemble of trees; (ii) apply
to this ensemble a ‘compression step’ by reformulating the tree-ensemble
based model as a linear model in terms of node indicator functions and
by using an @xmath -norm regularization approach - à la Lasso
(Tibshirani, 1996b ) - to select a minimal subset of these indicator
functions while maintaining predictive accuracy. We propose an
algorithmic framework and an empirical investigation of this idea, based
on three complementary datasets, and we show that indeed it is possible
to so compress significantly tree-based ensemble models, both in
regression and in classification problems. We also observe that the
compression rate and the accuracy of the compressed models further
increase with the ensemble size @xmath , even beyond the number @xmath
of terms required to ensure convergence of the variance reduction
effect.

The rest of this chapter is organized as follows: Section 25 introduces
the @xmath -norm based compression algorithm of random forests; Section
26 provides our empirical study and Section 27 concludes and describes
further perspectives.

#### 25 Compressing tree ensembles by @xmath-norm regularization

From an ensemble of @xmath decision trees, one can extract a set of node
indicator functions as follows: each indicator function @xmath is a
binary variable equal to 1 if the input vector @xmath reaches the @xmath
th node in the @xmath th tree, @xmath otherwise. Using these indicator
functions, the output predicted by the model may be rewritten as (Geurts
et al., 2006a ; Vens and Costa, 2011 ) :

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

where @xmath is the number of nodes in the @xmath th tree and @xmath is
equal to the leaf-label if node @xmath is a leaf and to zero if it is an
internal node.

We can therefore interpret a tree building algorithm as the (random)
inference of a new representation which lifts the original input space
@xmath towards the space @xmath of dimension @xmath by

  -- -------- --
     @xmath   
  -- -------- --

As an illustration, Figure \thechapter .1 shows a set of three decision
trees with respective sizes 7, 7 and 5 nodes. The propagation of a
sample @xmath through the forest makes it pass trough nodes 1.1, 1.2,
1.5 in the left tree, nodes 2.1, 2.2, 2.5 in the middle tree and nodes
3.1, 3.3 and 3.4 in the left tree (highlighted in orange).

We propose to compress the tree ensemble by applying a variable
selection method to its induced feature space @xmath . Namely, by @xmath
-regularization we can search for a linear model by solving the
following optimization problem:

  -- -- -------- -- -------------------
        @xmath      
        @xmath      ( \thechapter .2)
  -- -- -------- -- -------------------

This optimization problem, also called Lasso (Tibshirani, 1996b ) (see
Section 4.1 ), has received much attention in the past decade and is
particularly successful in high dimension. The @xmath -norm constraint
leads to a sparse solution: only a few weights @xmath will be non zero,
and their number tends to zero with @xmath ; the optimal value @xmath of
@xmath is problem specific and is typically adjusted by
cross-validation.

In order to solve Equation \thechapter .2 for growing values of @xmath ,
we use the ‘incremental forward stagewise regression’ algorithm (Hastie
et al., 2007 ) solving the monotone Lasso which imposes that each @xmath
increases monotonically with @xmath . This version deals indeed better
with many correlated variables, which is relevant in our setting, since
each node indicator function is highly correlated with those of its
neighbor nodes in the tree from which it originates. The final weights
@xmath may be exploited to prune the randomized tree ensemble: a test
node can be deleted if all its descendants correspond to @xmath .

Starting from a forest model @xmath , a value of parameter @xmath , and
a sample @xmath , the tree ensemble compression procedure is described
in Algorithm 1 .

1: function ForestCompression ( @xmath , @xmath , t)

2: Lift the sample @xmath to the random forest space @xmath

  -- -------- --
     @xmath   
  -- -------- --

with the induced feature space by the forest model @xmath

  -- -------- --
     @xmath   
  -- -------- --

3: Select weight vector @xmath over @xmath through @xmath minimization

  -- -- -------- --
        @xmath   
        @xmath   
  -- -- -------- --

4: Compress the random forest model @xmath using vector @xmath

5: return The compressed model.

6: end function

Algorithm 1 @xmath -based compression of tree ensemble model @xmath
using a sample @xmath

Note that in practice both the forest construction and the generation of
its sequence of compressed versions for growing values of @xmath may use
the same sample (the learning set). A separate validation set is however
required to select the optimal value of parameter @xmath . This is
similar to what is done with the pruning of a single decision tree (see
Section 11 ).

#### 26 Empirical analysis

In the following experiments, datasets are pre-whitened: input/output
data are translated to zero mean and rescaled to unit variance. All
results shown are averaged over @xmath runs in order to avoid
randomization artifacts.

Each one of these runs consisted of first generating a training set, and
a testing set, and then working as follows. When using the monotone
Lasso, we apply the incremental forward stagewise algorithm with a step
size @xmath . The optimal number of steps @xmath or the optimal point
@xmath was chosen by ten-fold cross-validation @xmath over the training
set (to this end, we used a quadratic loss in regression and a @xmath
loss in classification). More precisely, the training set is first
divided ten times through cross-validation into a learning set, used
both to fit a forest model and to run the incremental forward stagewise
algorithm on it, and into a validation set, to estimate the losses of
the resulting sequence of compressed forests. For each fold, we assess
the model fitted over the training set using the validation set with
increasing values of @xmath by steps of @xmath . For each value of
@xmath , the ten model losses are averaged. The optimal value of @xmath
and the corresponding model compression level are those leading to the
best average loss over the ten folds. The model is then refitted using
the entire training set with @xmath .

Below, we will apply our approach while using the extremely randomized
trees method (Geurts et al., 2006a ) to grow the forests (abbreviated by
“ET”) and we denote their @xmath -regularization-based compressed
version “rET”.

We present an overall performance analysis in Section 26.1 . Later on,
we enhance our comprehension of the pruning algorithm by studying the
effect of the regularization parameter @xmath in Section 26.2 and of the
complexity of the initial forest model by varying the pre-pruning rule
values @xmath , the minimum number of samples to split, and @xmath , the
number of trees, in Section 26.3 . While in these last two sections, we
focus our analysis on models obtained on the Friedman1 problem, we
notice that similar conclusions can also be drawn for Two-norm and SEFTi
datasets.

##### 26.1 Overall performances

We have evaluated our approach on two regression datasets Friedman1 and
SEFTi and one classification dataset Two-norm (see Appendix \thechapter
for their description).

We have used a set of representative meta-parameter values ( @xmath ,
@xmath and @xmath ) of the Extra-Trees algorithm (see Table \thechapter
.1 ). Accuracies are measured on the test sample and complexity is
measured by the number of test nodes of the ET and rET models (the
compression factor being the ratio of the former to the latter). We
observe a compression factor between 9 and 34, a slightly lower error
for the rET model than for the ET model on the two regression problems
(Friedman1 and SEFTi) and the opposite on Two-norm. To compare, we show
the results obtained with the linear Lasso based on the original
features (its complexity is measured by the number of kept features): it
is much less accurate than both ET and rET on the (non-linear)
regression problems (Friedman1 and SEFTi), but superior on the (linear)
classification problem (Two-norm).

Side experiments (results not provided) show that changing the value of
parameter @xmath does not influence significantly the final accuracy and
complexity on the Two-norm and Friedman1 datasets, while for SEFTi,
accuracy increases strongly with @xmath (presumably due to a large
number of noisy and/or irrelevant features) with however little impact
on the final complexity.

##### 26.2 Effect of the regularization parameter @xmath.

The complexity of the regularized ET model is shrunk with the @xmath
-norm constraint of Equation \thechapter .2 in a way depending on the
value of @xmath . As shown in Figure \thechapter .2 (a), an increase of
@xmath decreases the error of rET until @xmath , leading to a complexity
(Figure \thechapter .2 (b)) of about 900 test nodes. Notice that in
general the rET model eventually overfits when @xmath becomes large,
although this is not visible on the range of values displayed in Figure
\thechapter .2 (a) as the algorithm stops before.

##### 26.3 Influence of the Extra-Tree meta parameters @xmath and
@xmath.

The complexity of an ET model grows (linearly) with the size of the
ensemble @xmath and is inversely proportional to its pre-pruning
parameter @xmath .

Figure \thechapter .3 shows the effect of @xmath on both ET and rET.
Interestingly, the accuracy and the complexity of the rET model are both
more robust with respect to the choice of the precise value of @xmath
than those of the ET model, specially for the smaller values of @xmath (
@xmath , in Figures \thechapter .3 ).

Figure \thechapter .4 shows the effect of @xmath on both ET and rET
models. We observe that increasing the value of @xmath beyond the value
@xmath where variance reduction has stabilized ( @xmath in Figure
\thechapter .4 ) allows to further improve the accuracy of the rET model
without increasing its complexity.

#### 27 Conclusion

Compression of randomized tree ensembles with @xmath -norm
regularization leads to a drastic reduction of space complexity while
preserving accuracy. The complexity of the pruned model does not seem to
be directly related to the complexity of the original forest, i.e. the
number and complexity of each randomized tree, as long as this forest
has explored a large enough space of variable interactions.

The strong compressibility of large randomized tree ensemble models
suggests that it could be possible to design novel algorithms based on
tree-based randomization which would scale in a better way to very
high-dimensional input spaces than the existing methods. To achieve
this, one open question is how to get the compressed tree ensemble
directly, i.e. without generating a huge randomized tree ensemble and
then pruning it.

Tree-based ensemble models may be interpreted as lifting the original
input space towards a (randomly generated) high-dimensional discrete and
sparse representation, where each induced feature corresponds to the
indicator function of a particular tree node, and takes the value @xmath
for a given observation if this observation reaches this node, and
@xmath otherwise. The dimension of this representation is on the order
of @xmath , but the number @xmath of non-zero components for a given
observation is only on the order of @xmath . Compressed sensing theory
(Candès and Wakin, 2008 ) tells us that high-dimensional sparsely
representable observations may be compressed by projecting them on a
random subspace of dimension proportional to @xmath , where @xmath is
the original dimension of the observations and @xmath is the number of
non-zero terms in their sparse representation basis. This suggests that
one could reduce the space complexity of tree-based method by applying
compressed sensing to their original input feature space if its
dimension is high, and/or to their induced feature space if @xmath is
too large.

Since the publication of our work on this subject, several authors have
proposed similar ideas to post-prune a fully grown random forest model:
Ren et al. ( 2015 ) propose to iteratively remove or re-weight the
leaves of the random forest model, while Duroux and Scornet ( 2016 )
study the impact of pre-pruning on random forest models.

### Chapter \thechapter Exploiting input sparsity with decision tree

Outline Many supervised learning tasks, such as text annotation, are
characterized by high dimensional and sparse input spaces where the
input vectors of each sample has only a few non zero values. We show how
to exploit algorithmically the input space sparsity within decision tree
methods. It leads to significant speed up both on synthetics and real
datasets, while leading to exactly the same model. We also reduce the
required memory to grow such models by exploiting sparse memory storage
instead of dense memory storage for the input matrix. This contribution
is a joint work with Fares Hedayati and Panagiotis Papadimitriou,
working at www.upwork.com . The outcome of this research has been
proposed and merged in the scikit-learn (Buitinck et al., 2013 ;
Pedregosa et al., 2011 ) open source package.

Many machine learning tasks such as text annotation usually require
training over very big datasets with millions of web documents. Such
tasks require defining a mapping between the raw input space and the
output space. For example in text classification, a text document (raw
input space) is usually mapped to a vector whose dimensions correspond
to all of the possible words in a dictionary and the values of the
vector elements are determined by the frequency of the words in the
document. Although such vectors have many dimensions, they are often
sparsely representable. For instance, the number of unique words
associated to a text document is actually small compared to the number
of words of a given language. We describe those samples with sparse
input vectors as having a few non zero values.

Exploiting the low density, i.e. the fraction of non zero elements, and
the high sparsity, i.e. the fraction of zero elements, is key to address
such high dimensional supervised learning tasks. Many models directly
formulate their entire algorithm to exploit the input sparsity. Linear
models such as logistic regression or support vector machine harness the
sparsity by expressing most of their operations as a set of linear
algebra operations such as dot products who directly exploit the
sparsity to speed up computations.

Unfortunately, decision tree methods are not expressible only through
linear algebra operations. Decision tree methods are recursively
partitioning the input space by searching for the best possible
splitting rules. As a consequence, most machine learning packages either
do not support sparse input vectors for tree-based methods, only support
stumps (decision tree with only one internal node) or have a sub-optimal
implementation through the simulation of a random access memory as in
the dense case. The only solution is often to densify the input space
which leads first to severe memory constraints and then to slow training
time.

In Section 28 , we present efficient algorithms to grow vanilla decision
trees, boosting and random forest methods on sparse input data. In
Section 29 , we describe how to adapt the prediction algorithm of these
models to sparse input data. In Section 30 , we show empirically the
speed up obtained by fitting decision trees with this input sparsity
aware implementation.

#### 28 Tree growing

During the decision tree fitting, the tree growing algorithm (see
Algorithm 2 ) interacts with the input space at two key points:

1.  during the search of a splitting rule @xmath using a sample set
    @xmath at the expansion of node @xmath (see line 10 of Algorithm 2
    );

2.  during the data partitioning of the sample @xmath into a left and a
    right partition following the splitting rule @xmath at node @xmath
    (see line 11 of Algorithm 2 ).

In this section, we show how to adapt decision tree at these three key
points to handle sparsely expressed data. While at the same time, we
will show how to harness the sparsity to speed up the original
algorithm. We first explain how node splitting is implemented in
standard decision trees in Section 28.1 and then explain our efficient
implementation for sparse input data in Section 28.2 . In Section 28.3 ,
we further describe how to propagate samples with sparse input during
the decision tree growing.

##### 28.1 Standard node splitting algorithm

During the decision tree growth, the crux of the tree growing algorithm
in high dimensional input space is the search of the best possible local
splitting rule @xmath (as described in Section 10.1 ). Given a learning
set @xmath reaching a node @xmath , we search for the splitting rule
@xmath among all the possible binary and axis-wise splitting rules
@xmath . We strive to maximize the impurity reduction @xmath obtained by
dividing the sample set @xmath into two partitions @xmath . The
splitting rule selection problem (line 10 of the tree growing Algorithm
2 ) is written as

  -- -------- -- -------------------
     @xmath      ( \thechapter .1)
  -- -------- -- -------------------

with

  -- -------- -------- -- -------------------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      ( \thechapter .2)
  -- -------- -------- -- -------------------

where @xmath is the set of all splitting rules associated to an input
variable @xmath .

Decision tree libraries carefully optimize this part of the algorithm to
have the proper computational complexity with a low constant. A careful
design of the algorithm for instance does not move around samples
according to the partitions, but instead move an identification number
linked to each sample. The learning set @xmath is implemented as an
array of row indices @xmath linked to the rows of the input matrix
@xmath and the output matrix @xmath . The sample set @xmath reaching a
node @xmath is implemented as a slice of the array @xmath where the
elements from the @xmath to the @xmath indices gives the indices of the
samples reaching node @xmath .

Let us take a small example with a set of @xmath training samples @xmath
illustrating the management of the array @xmath . During the tree growth
(see Algorithm 2 ) when we partition the sample set @xmath into two
sample sets @xmath and @xmath . In practice, we modify the array @xmath
such that from 0 to @xmath (resp. from @xmath to @xmath ) are located
the samples of the left child (resp. right child). It leads to

  -- -------- --
     @xmath   
  -- -------- --

We represent each sample set @xmath as a slice @xmath , a chunk, of the
array @xmath . The sample set @xmath is the slice @xmath of @xmath ,
while the sample set @xmath is the slice @xmath of @xmath . Now if the
right node @xmath is further split into a left node with samples @xmath
and a right node with samples @xmath (in orange), then @xmath is further
modified to reflect the split:

  -- -------- --
     @xmath   
  -- -------- --

To further speed up the best splitting rule search with ordered
variables, we sort the possible thresholds associated to an ordered
input variable @xmath (programmatically @xmath ). By sorting the
possible thresholds sets, we can evaluate the impurity measure @xmath
and the impurity reduction @xmath in an online fashion. For instance,
the Gini index and entropy criteria can be computed by updating the
class frequency in the left and right split when moving from one
splitting threshold to the next.

##### 28.2 Splitting rules search on sparse data

To handle sparse input data with decision trees, we need efficient
procedures to select the best splitting rules @xmath among all the
possible splitting rules knowing that we have a high proportion of zeros
in the input matrix @xmath . In this section, we propose an efficient
method to exploit the sparsity of the input space with decision tree
models. Our method takes advantage of the input sparsity by avoiding
sorting sample sets of a node along a feature unless there are non zero
elements at this feature. This approach speeds up training substantially
as extracting the possible threshold values and sorting them is a costly
but essential and ubiquitous component of tree-based models.

The splitting rule search algorithm for an ordered variable @xmath at a
node @xmath is divided in two parts (see Algorithm 1 ): (i) to extract
efficiently the non zero values associated to @xmath in the sample
partition @xmath (line 3 ) and (ii) to search separately among the
splitting rules with the positive, negative or zero threshold (line 4 ).

1: function FindBestSparseSplit ( @xmath , @xmath )

2: for @xmath to @xmath do

3: Extract strictly positive @xmath and strictly negatives @xmath values
from @xmath given @xmath .

4: Search for the best splitting rule of the form @xmath with @xmath
maximizing the impurity reduction @xmath over the sample set @xmath .

5: Update @xmath if the splitting rule @xmath leads to higher impurity
reduction.

6: end for

7: return @xmath

8: end function

Algorithm 1 Search for the best splitting rule @xmath given a sparse
input matrix @xmath and a set of samples @xmath

To extract the non zero values of a sparse input matrix @xmath with
sparsity @xmath in the context of the decision tree growth, we need to
perform efficiently two operations on matrices: (i) the column indexing
for a given input variable @xmath and (ii) the extraction of the non
zero row values associated to the set of samples @xmath reaching the
node @xmath in this column @xmath . The overall cost of extracting
@xmath samples at a column @xmath from the input matrix @xmath should be
proportional to the number of non zero elements and not to @xmath . ⁹ ⁹
9 We assume here that the matrix @xmath is uniformly sparse.

Among the different sparse matrix representations (Pissanetzky, 1984 ;
Barrett et al., 1994 ; Hwu and Kirk, 2009 ) , the sparse csc matrix
format is the most appropriate for tree growing as it allows efficient
column indexing, as required during node splitting. Let us show how to
perform an efficient extraction of the non zero values given the sample
set @xmath using this matrix format. Note that using a compressed row
storage sparse format ¹⁰ ¹⁰ 10 The compressed row storage (csr) sparse
array format is made of three arrays @xmath , @xmath and @xmath . The
non zero elements of the @xmath -th row of the sparse csc matrix are
stored from @xmath to @xmath in the @xmath arrays, giving the column
indices, and @xmath arrays, giving the stored values. It is the
transposed version of the csc sparse format. would not be appropriate
during the tree growth as we need to be able to efficiently subsample
input variables at each expansion of a new testing node.

Compressed Sparse Column (CSC) matrix format The sparse csc matrix with
@xmath non zero elements is a data structure composed of three arrays:
@xmath containing the row indices of the non zero elements. @xmath
containing the values of the non zero elements. @xmath containing the
slice of the non zero elements. For a column @xmath , the row index and
the values of the non zero elements of columns @xmath are stored from
@xmath to @xmath in the @xmath and @xmath arrays. The non zero values
associated to an input variable @xmath are located from @xmath to @xmath
in the @xmath and the @xmath arrays (when @xmath , the column thus
contains only zeros). Extracting them requires to perform a set
intersection between the sample set @xmath reaching node @xmath and the
non zero values @xmath of the column @xmath . For instance, the
following matrix @xmath has only 3 non zero elements, but we would use
an array of 20 elements:

@xmath ( \thechapter .3)

The csc representation of this matrix @xmath is given by

@xmath @xmath @xmath @xmath @xmath @xmath

Let @xmath be the number of samples with non zero values for input
variable @xmath and let us assume that the @xmath of the input csc
matrix array are sorted column-wise, i.e. for the @xmath -th row, the
elements of @xmath from @xmath to @xmath are sorted. Standard
intersection algorithms have the following time complexity:

1.  in @xmath by performing @xmath binary search on the sorted @xmath
    nonzero elements;

2.  in @xmath by sorting the sample set @xmath and retrieving the
    intersection by iterating over both arrays;

3.  in @xmath by maintaining a data structure such as a hash table of
    @xmath allowing to efficiently check if the elements of @xmath :
    @xmath are contained in the sample partition @xmath .

The optimal intersection algorithm depends on the number of non zero
elements for input variable @xmath and the number of samples @xmath
reaching node @xmath . During the decision tree growth, we have two
opposite situations: either the size of the sample partition @xmath is
high with respect to the number of non zero elements ( @xmath ) or,
typically at the bottom of the tree, the partition size is small with
respect to the number of non zero elements ( @xmath ). In the first case
(i.e., at the top of the tree), the most efficient approach is thus
approach ( 3 ), while in the second case (i.e., at the bottom of the
tree), approach ( 1 ) should be faster. We first describe how to
implement approach ( 3 ), then approach ( 1 ), and finally how to
combine both approaches.

A straightforward implementation of approach ( 3 ) is, at each node, to
allocate a hash table containing all training examples in that node (in
@xmath ) and then to compute the intersection by checking if the non
zero elements of the csc matrix belong to the hash table (in @xmath ).
We can however avoid the overhead required for the allocation, creation,
and deallocation of the hash table by maintaining and exploiting a
mapping between the csc matrix and the sample set @xmath . Since the
array @xmath is constantly modified during the tree growth, we propose
to use an array, denoted @xmath , to keep track of the position of a
sample @xmath in the array @xmath as illustrated in Figure \thechapter
.1 . During the tree growing, we keep the following invariant:

  -- -------- -- -------------------
     @xmath      ( \thechapter .4)
  -- -------- -- -------------------

In the above example, the array @xmath was @xmath with @xmath . After a
few splits, the array @xmath has become

  -- -------- --
     @xmath   
  -- -------- --

and the associated @xmath array is

  -- -------- --
     @xmath   
  -- -------- --

Thanks to the @xmath array, we can now check in constant time @xmath
whether a sample @xmath belongs to the sample set @xmath . Indeed, given
that @xmath is represented by a slice from an index @xmath to an index
@xmath in @xmath , sample @xmath belongs to @xmath if the following
condition holds:

  -- -------- -- -------------------
     @xmath      ( \thechapter .5)
  -- -------- -- -------------------

To extract the non zero values of the csc matrix associated to the
@xmath -th input variable in the sample set @xmath , we check the
previous condition for all samples from @xmath to @xmath in the @xmath
array. Thus, we perform the intersection between @xmath and the @xmath
non zero values in @xmath . The whole method is described in Algorithm 2
. Note that to swap samples in the array @xmath , we use a modified swap
function (see Algorithm 3 ) which preserves the mapping invariant .

1: function extract_nnz_mapping ( @xmath , @xmath , @xmath , @xmath ,
@xmath , @xmath )

2: @xmath

3: @xmath

4: @xmath = @xmath

5: @xmath = @xmath

6: for @xmath do

7: @xmath

8: @xmath

9: if @xmath then

10: @xmath = @xmath

11: if @xmath then

12: @xmath . append ( @xmath )

13: @xmath

14: Swap ( @xmath , @xmath , @xmath , @xmath )

15: else

16: @xmath . append ( @xmath )

17: Swap ( @xmath , @xmath , @xmath , @xmath )

18: @xmath

19: end if

20: end if

21: end for

22: return @xmath

23: end function

Algorithm 2 Return the @xmath strictly negative and @xmath positive
values @xmath associated to the @xmath -th variable from the sample set
@xmath through a given @xmath satisfying @xmath . The array @xmath is
modified so that @xmath contains the samples with negatives values,
@xmath contains the zero values and @xmath the samples with positives
values.

1: function Swap ( @xmath , @xmath , @xmath , @xmath )

2: @xmath , @xmath = @xmath , @xmath

3: @xmath

4: @xmath

5: end function

Algorithm 3 Swap two elements at positions @xmath and @xmath in the
array @xmath in place while maintaining the invariant of the @xmath
array.

In practice, the number of non zero elements @xmath of feature @xmath
could be much greater than the size of a sample set @xmath . This is
likely to happen near the leaf nodes. Whenever the tree is fully
developed, there are only a few samples reaching these nodes. The
approach ( 1 ) shown in Algorithm 4 exploits the relatively small size
of the sample set and performs repeated binary search on the @xmath non
zero elements associated to the feature @xmath .

1: function extract_nnz_bsearch ( @xmath , @xmath , @xmath , @xmath ,
@xmath , @xmath )

2: @xmath

3: @xmath

4: @xmath = @xmath

5: @xmath = @xmath

6: @xmath

7: @xmath

8: @xmath = sort ( @xmath , start, end)

9: for @xmath do

10: // Get the position of @xmath in @xmath , and -1 if it is not found:
@xmath = BinarySearch ( @xmath , @xmath )

11: if @xmath then

12: if @xmath then

13: @xmath

14: @xmath . append ( @xmath )

15: Swap ( @xmath , @xmath , @xmath , @xmath )

16: else

17: @xmath . append ( @xmath )

18: Swap ( @xmath , @xmath , @xmath , @xmath )

19: @xmath

20: end if

21: end if

22: end for

23: return @xmath

24: end function

Algorithm 4 Return the @xmath strictly negative and @xmath positive
values @xmath associated to the @xmath -th variable from the sample set
@xmath through repeated binary search . The array @xmath is modified so
that @xmath contains the samples with negatives values, @xmath contains
the zero values and @xmath the samples with positives values.

The optimal extraction of non zero values is a hybrid approach combining
the mapping-based algorithm (Algorithm 2 ) and the binary search
algorithm (Algorithm 4 ). Empirical experiments have shown that it is
advantageous to use the mapping-based algorithm whenever

  -- -------- -- -------------------
     @xmath      ( \thechapter .6)
  -- -------- -- -------------------

and the binary search otherwise (see Algorithm 5 ). The formula is based
on the computational complexity of both algorithms. We have determined
the constant of @xmath empirically.

function extract_nnz ( @xmath , @xmath , @xmath , @xmath , @xmath ,
@xmath )

Let @xmath be the number of non zero values in column @xmath of @xmath .

if @xmath then

return extract_nnz_mapping ( @xmath , @xmath , @xmath , @xmath , @xmath
, @xmath )

else

return extract_nnz_bsearch ( @xmath , @xmath , @xmath , @xmath , @xmath
, @xmath )

end if

end function

Algorithm 5 Return the @xmath strictly negative and @xmath positive
values @xmath associated to the @xmath -th variable from the sample set
@xmath . The array @xmath is modified so that @xmath contains the
samples with negatives values, @xmath contains the zero values and
@xmath the samples with positives values.

Note that after extracting the non zero values, we need to sort the
thresholds of the splitting rules to search efficiently for the best
one. Thanks to Algorithm 5 , Algorithm 2 and Algorithm 4 , we have
already made a three way partition pivot as in the quicksort on the
value 0. This speeds up the overall splitting algorithm (the line 4 of
Algorithm 1 ). Instead of sorting the thresholds in the sample set
@xmath in @xmath , we can perform the sort in @xmath given an input
space density @xmath .

As a further refinement, let us note that we can sometimes significantly
speed up the decision tree growth by avoiding to search for splitting
rules on constant input variables. To do so, we can cache the input
variables that were found constant during the expansion of the parents
of the node of @xmath . If an input variable is found constant, caching
this information avoids the overhead of searching for a splitting rule
when no valid one exists.

##### 28.3 Partitioning sparse data

During the tree growth, we need to partition a sample set @xmath at a
testing node @xmath according to a splitting rule @xmath . The splitting
rule @xmath associated to an ordered input variable is of the form
@xmath , where @xmath is a threshold constant on the @xmath -th input
variable.

During the tree growth, we have the constraint that the input data
matrix is in the csc sparse format. We can not convert the current
sparse format to another one as it would require to store both the new
and old representations into memory. An efficient way to split the
sample set @xmath into its left @xmath and right @xmath subset is to use
the Algorithm 5 . It will extract the non zero values of a given input
variable, but also partition the array @xmath representing the sample
set @xmath into three parts: (i) @xmath contains the @xmath samples with
negatives values, (ii) @xmath contains the elements with zero values and
(iii) @xmath the @xmath samples with positives values. Once the non zero
values have been extracted, we have to partition the samples either with
negative values ( @xmath ) or with positive values ( @xmath ) according
to the sign of the threshold @xmath .

The complexity to partition once the data is @xmath for a batch of
@xmath samples instead of the usual @xmath with dense input data.

#### 29 Tree prediction

The prediction of an unseen sample @xmath by a decision tree (see
Algorithm 1 ) is done by traversing the tree from the top to the bottom.
At each test node, a splitting rule of the form tests whether or not the
sample @xmath should go in the left or the right branch. The splitting
rule @xmath associated to an ordered input variable is of the form
@xmath , where @xmath is a threshold constant on the @xmath -th input
variable.

We need to have an efficient row and column indexing of the input data
matrix. We discuss here two options: (i) using the dictionary of key
(dok) sparse matrix format and (ii) using a csr sparse matrix format ¹¹
¹¹ 11 The compressed row storage (csr) sparse array format (Pissanetzky,
1984 ; Barrett et al., 1994 ; Hwu and Kirk, 2009 ) is made of three
arrays @xmath , @xmath and @xmath . The non zero elements of the @xmath
-th row of the sparse csc matrix are stored from @xmath to @xmath in the
@xmath arrays, giving the column indices, and @xmath arrays, giving the
stored values. It is the transposed version of the csc sparse format. .

The dictionary of key (dok) sparse matrix format store the non zero
values in a hash table whose keys are the pairs formed from the row and
the column index. It is straightforward to apply Algorithm 1 with the
dok format. The computational complexity is thus unchanged.

To predict one or several unseen samples using a csr array, we need a
procedure to efficiently access to both the row and the column index
without densifying the csr matrix. We allocate two arrays @xmath with
all elements having the value “ @xmath ” and @xmath of size @xmath . To
predict the @xmath -th sample from a test set, we set in the array
@xmath the value @xmath to the non zero values @xmath associated to this
sample. The array @xmath is modified to contain the non zero values of
the @xmath -th sample, i.e. @xmath . During the tree traversal (see
Algorithm 1 ), we get the @xmath -th input value by first checking if it
is zero with @xmath , otherwise the value is stored at @xmath . Assuming
a proportion of zero elements @xmath for a batch of @xmath test samples
with @xmath input variables, the extra cost of using this approach is
@xmath . Note that using the @xmath and the @xmath arrays is more
efficient than densifying each sample as it would add an extra cost of
@xmath .

The csr sparse format leads to a worse computational complexity than the
dok format, which has no extra computing cost. However, the csr approach
was chosen in the scikit-learn machine learning python library. The
standard implementation of the dok format in scipy is indeed currently
implemented using the python dict object. While with the csr format, we
can work only with low level c arrays and we can also easily release the
global interpreter lock (GIL). Note that here the csr format is also
better suited than the csc format as the complexity is independent of
the number of samples to predict at once.

#### 30 Experiments

In this section, we compare the training time and prediction time of
decision tree growing and prediction algorithms using either dense data
representation or sparse data representation. More specifically, we
compare three input matrix layouts using scikit-learn version 0.17: (i)
the dense c array layout, a row major order layout whose consecutive and
contiguous elements are row values, (ii) the dense fortran array layout,
a column major order layout whose consecutive and contiguous elements
are column values, and (iii) the sparse array layout, the csc sparse
format during tree growing and the csr sparse format during tree
prediction (as proposed respectively in Sections 28.2 and 28.3 ). The
comparison will be made with stumps, a decision tree with a single test
node, and fully grown decision trees. These results will be indicative
of what could be gained in the context of boosting methods using
decision tree of low complexity such as stumps and random forest methods
using deep decision trees. Note that all splitting algorithms lead
exactly to the same decision tree structure and have the same
generalization performance.

We assess the effect of the input space density on synthetic datasets in
Section 30.1 . Then, we compare training times using each of the three
input matrix layouts on real datasets in Section 30.2 .

##### 30.1 Effect of the input space density on synthetic datasets

As a synthetic experiment, we compare the decision tree growing
algorithm and tree prediction algorithm on synthetic regression tasks
with @xmath samples and @xmath features. The input matrices are sparse
random matrices whose non zero elements are drawn uniformly in @xmath .
The output vector is drawn uniformly at random in @xmath . The input
space density ranges from @xmath to @xmath . Each point is an average
over 20 experiments.

Figure (a)a shows in logarithmic scale the computing times to grow a
single stump. We first note that column-based layouts (the fortran and
csc format) are more appropriate to grow decision tree on sparse data.
While the density is ranging from @xmath to @xmath , the most expensive
part of the tree growing for a single stump is to retrieve the input
values from a sample set and to sort them. For a set of @xmath samples
and @xmath features with a sparsity @xmath , the computational cost to
grow a stump on dense data is @xmath . By contrast, growing a stump
using a csc sparse input matrix has a computational complexity of

  -- -------- -- -------------------
     @xmath      
     @xmath      ( \thechapter .7)
  -- -------- -- -------------------

The first term of Equation \thechapter .7 corresponds to the sorting of
non zero elements. The second term highlights the contribution of the
retrieval of the non zero values, which is always less costly than the
sorting operation. If the density is @xmath ( @xmath ), both the dense
and sparse formats have the same complexity as shown in the right point
of Figure (a)a . Overall with a sparse dataset, the csc format is
significantly faster as it leverages the sparsity. The bad performance
of the dense c layout compared to the fortran layout or csc layout can
be explained by the higher number of cache misses.

The time required to predict the training set using the fitted stump is
shown in Figure (b)b . The only difference between the three input
matrix layouts (c, fortran and csr) is the access to the non zero
elements. The differences between the dense and the sparse input matrix
format can be explained by a better exploitation of the cache for the
sparse format, especially when the density is below @xmath . When the
density if over @xmath , the cost of copying the non zero values to the
arrays @xmath and @xmath becomes dominant.

Figure \thechapter .3 shows the time required to learn a fully grown
decision tree as a function of the dataset density. Note that the
maximal depth decreases as a function of the dataset density (see Figure
(c)c ) as the decision tree becomes more balanced. As with the stump,
the fortran layout is more appropriate than the c layout to grow a fully
grown decision tree on sparse data. The sparse csc algorithm is
significantly faster than the dense splitting algorithm if the density
is sufficiently low (here below @xmath ). The sparse splitting algorithm
becomes slower as the density increases (here beyond @xmath ) since the
extraction of the non zero values becomes more costly than finding the
right split.

With fully developed decision trees, the prediction time (see Figure
(b)b ) is similar between its dense and sparse version. The prediction
time is lower when the input space density is high as the trees are more
balanced. We note that the prediction time is correlated with the
maximal depth of the tree.

Whenever the complexity of the decision tree lies between a stump and a
fully developed tree, the behavior moves from one extreme to the other.

##### 30.2 Effect of the input space density on real datasets

To further study the impact of the input space density, we have selected
9 datasets whose input space density ranges from @xmath to @xmath .
These datasets are presented in Table \thechapter .1 ordered by input
space density.

Table \thechapter .2 shows the time to train a single stump. The fastest
algorithm here is the tree growing algorithm with the input sparse csc
matrix. The speed up factor between the sparse and fortran memory layout
ranges from 1.1 to 23 times. Note that the fortran layout is always
faster than the c layout. The column major order layout is here better
suited for sparse dataset. The difference could be explained by fewer
cache misses with the fortran memory layout than with the c memory
layout.

Table \thechapter .3 shows the time required to grow a fully developed
decision tree with c, fortran or sparse csc memory layout. The dense
fortran layout is here always faster than the dense c layout. The sparse
memory layout is faster by a factor between 1.3 and 7.5 than the fortran
layout when the input space density is below @xmath .

Note that we were unable to grow a decision tree with a dense memory
layout on the news20.binary dataset as it would require 108.4 Gigabyte
to only store the input matrix instead of 78 Megabytes.

##### 30.3 Algorithm comparison on 20 newsgroup

Decision trees are rarely used in the context of sparse input datasets.
One reason is the lack of implementations exploiting sparsity during the
decision tree growth. With the previous experiments, we have shown that
it increases significantly the computing time, but also the amount of
memory needed. With the proposed tree growing and prediction algorithms,
it is interesting to compare the training time, prediction time and
accuracy of some tree based models, such as random forest or adaboost,
with methods more commonly used in the presence of sparse data.

We compare tree based methods to methods more commonly used on sparse
datasets on the 20 newsgroup dataset, which have @xmath input variables,
11314 training sample ands 7532 testing samples.

The compared algorithms are optimized on their respective
hyper-parameters (see Table \thechapter .4 for the details) using 5-fold
cross validation strategy on the training samples.

The results obtained on the 20 newsgroup dataset are shown in Table
\thechapter .5 using scikit-learn version 0.17.1 and input
sparsity-aware implementations. The algorithm with the highest accuracy
is the linear estimator trained with ridge regression. It is closely
followed by the random forest ( @xmath ) model, the multinomial naives
Bayes and extra-trees ( @xmath ). More generally, tree-based ensemble
methods (random forest, extra trees, and adaboost) show similar
performance as linear methods (ridge, naive bayes, linear SVC and SGD),
with all methods from these two families reaching at least @xmath of
accuracy. On the other hand, the @xmath -nearest neighbors and the
single decision tree perform very poorly (with an accuracy below @xmath
).

We also note that increasing the number of trees from 100 to 1000
significantly improves the performance of both random forests and extra
trees. Their accuracy increases respectively by @xmath and @xmath in
absolute value. Building tree ensembles also very significantly improves
the accuracy with respect to single trees (by at least @xmath ). This
further suggests that the variance of single trees is very high on this
problem.

From a modeling perspective, growing decision tree ensemble on datasets
with sparse inputs is possible. From a training time perspective, the
time needed to grow and to optimize an ensemble of 100 trees is
comparable to the time needed to train linear models, e.g., with SGD or
ridge regression. Note that naive Bayes models are particularly fast to
train compared to the other estimators. However note that the comparison
is dependent upon the chosen hyper-parameters, the implementation and
the grid size. From the point of view of prediction time, tree ensemble
methods are particularly slow compared to the other estimators.

#### 31 Conclusion

We propose an algorithm to grow decision tree models whenever the input
space is sparse. Our approach takes advantage of input sparsity to speed
up the search and selection of the best splitting rule during the tree
growing. It first speeds up the expansion of a tree node by extracting
efficiently the non zero threshold of the splitting rules used to
partition data. Secondly, the selection of best splitting rule is also
faster as we avoid to sort data with zero values. We reduce the memory
needed as we do not need to densify the input space. We also show how to
predict samples with sparse inputs.

### Chapter \thechapter Conclusions

#### 32 Conclusions

As we now gather or generate data at every moment, machine learning
techniques are emerging as ubiquitous tools in sciences, engineering, or
society. Within machine learning, this thesis focuses on supervised
learning, which aims at modelling input-output relationships only from
observations of input-output pairs, using tree-based ensemble methods, a
popular method family exploiting tree structured input-output models.
Modern applications of supervised learning raise new computational,
memory, and modeling challenges to existing supervised learning
algorithms. In this work, we identified and addressed the following
questions in the context of tree-based supervised learning methods: (i)
how to efficiently learn in high dimensional, and possibly sparse,
output spaces? (ii) how to reduce the memory requirement of tree-based
models at prediction time? (iii) how to efficiently learn in high
dimensional and sparse input spaces? We summarize below our main
contributions and conclusions around these three questions.

###### Learning in high dimensional and possibly sparse output spaces.

Decision trees are grown by recursively partitioning the input space
while selecting at each node the split maximizing the reduction of some
impurity measure. Impurity measures have been extended to address with
such models multi-dimensional output spaces, so as to solve multi-label
classification or multi-target regression problems. However, when the
output space is of high dimension, the computation of the impurity
becomes a computational bottleneck of the tree growing algorithm. To
speed up this algorithm, we propose to approximate impurity computations
during tree growing through random projections of the output space. More
precisely, before growing a tree, a few random projections of the output
space are computed and the tree is grown to fit these projections
instead of the original outputs. Tree leaves are then relabelled in the
original output space to provide proper predictions at test time. We
show theoretically that when the number of projections is large enough,
impurity scores, and thereby the learned tree structures and their
predictions, are not affected by this trick. We then exploit the
randomization introduced by the projections in the context of random
forests, by building each tree of the forest from a different randomly
projected subspace. Through experiments on several multi-label
classification problems, we show that randomly projecting the outputs
can significantly reduce computing times at training without affecting
predictive performance. On some problems, the randomization induced by
the projections even allows to reach a better bias-variance tradeoff
within random forests, which leads to improved overall performance. In
contrast with existing works on random projections of the output, our
proposed leaf relabelling strategy also allows to avoid any decoding
step and thus preserves computational efficiency at prediction time with
respect to standard unprojected multi-output random forests.

Multi-output random forests build a single tree ensemble to predict all
outputs simultaneously. While often effective, this approach is
justified only when the individual outputs are strongly dependent
(conditionally to the inputs). On the other hand, building a separate
ensemble for each output, as done in the binary relevance / single
target approach, is justified only when the outputs are (conditionally)
independent. In our second contribution, we build on gradient boosting
and random projections to propose a new approach that tries to bridge
the gap between these two extreme assumptions and can hopefully adapt
automatically to any intermediate output dependency structure. The idea
of this approach is to grow each tree of a gradient boosting ensemble to
fit a random projection of the original (residual) output space and then
to weight this model in the prediction of each output according to its
“correlation” with this output. Through extensive experiments on several
artificial and real problems, we show that the resulting method has a
faster convergence than binary relevance and that it can adapt better
than both binary relevance and multi-output gradient boosting to any
output dependency structure. The resulting method is also competitive
with multi-output random forests. Although we only carried out
experiments with tree-based weak models, the resulting gradient boosting
methods are generic and can thus be used with any base regressor.

###### Reducing memory requirements of tree-based models at prediction
time.

One drawback of random forest methods is that they need to grow very
large ensembles of unpruned trees to achieve optimal performance. The
resulting models are thus potentially very complex, especially with
large datasets, as the complexity of unpruned trees typically depends
linearly on the dataset size. On the other hand, only very few nodes are
required to make a prediction for a given test example. Our
investigation of the question of ensemble compression started with the
observation that the random forest model can be viewed as linear models
in the node indicator space. Each of these binary variables defining
this space indicates whether or not a sample reaches a given node of the
forest. In the original linear representation of a forest in the
indicator space, non zero “coefficients” are given only to the leaf
nodes. We propose to post-prune the random forest model by selecting and
re-weighting the nodes of the linear model through the exploitation of
sparse linear estimators. More precisely, from the tree ensemble, we
first extract node indicator variables. Then, we project a sample set on
this new representation and select a subset of these variables through a
Lasso model. The non zero coefficients of the obtained Lasso model are
later used to prune and to re-weight the decision tree structure. The
resulting post-pruning approach is shown experimentally to reduce very
significantly the size of random forests, while preserving, and even
sometimes improving, their predictive performance.

###### Learning in high dimensional and sparse input spaces.

Some supervised learning tasks (e.g., text classification) need to deal
with high dimensional and sparse input spaces, where input variables
have each only a few non zero values. Dealing with input sparsity in the
context of decision tree ensembles is challenging computationally for
two reasons: (i) it is more difficult algorithmic-wise to exploit
sparsity during the tree growth than for example with linear models,
leading to slow tree training algorithms requiring a high amount of
memory, (ii) the decision tree structures are very unbalanced, which
further affects computational complexity. For these two reasons, linear
methods are often preferred to decision tree algorithms to learn with
sparse datasets. In our last contribution, we specifically developed an
efficient implementation of the tree growing algorithm to handle sparse
input variables. While previous implementations required to densify the
input data matrix, our implementation allows to directly fit decision
trees on appropriate sparse input matrices. It speeds up decision tree
training on sparse input data and saves memory by avoiding input data
“densification”. We also show how to predict unseen sparse input samples
with a decision tree model. Note that in this contribution we only focus
on improving computing times without modifying the original algorithm.

#### 33 Perspectives and future works

We collect in this section some future research directions based on the
presented ideas of this thesis.

##### 33.1 Learning in compressed space through random projections

-   The combination of random forest models with random projections adds
    two new hyper-parameters to tree based methods: the choice and the
    size of the random output projection subspace. It is not clear yet
    what would be good default hyper-parameter choices. Extensive
    empirical studies and the Johnson-Lindenstrauss lemma might help us
    to define good default values. These two hyper-parameters also
    introduce randomization in the output space. It would be interesting
    to further investigate how the input and output space randomizations
    jointly modify the bias-variance tradeoff of the ensemble.

-   Single random projection of the output space with the gradient
    boosting algorithm is a generic multi-output method usable with any
    single output regressor. In this thesis, we specifically focused on
    tree-structured weak models. We suggest to investigate other kinds
    of weak models.

-   We have combined a dimensionality reduction method with an ensemble
    method, random forest methods in Chapter \thechapter and with
    gradient boosting methods in Chapter \thechapter , while keeping the
    generation of the random projection matrix independent from the
    supervised learning task. It would be interesting to investigate
    more adaptive projection schemes. A simple instance of this approach
    would be to draw a new random projection matrix according to the
    residuals, e.g. by sub-sampling an output variable with a
    probability proportional to the fraction of unexplained variance. An
    optimal instance of this approach, but computationally more
    expensive, would compute a projection maximizing the variance along
    each axis with the principal component analysis algorithm.

-   Kernelizing the output of tree-based methods (Geurts et al., 2006b ,
    2007 ) allows one to treat complex outputs such as images, texts and
    graphs. It would be interesting to investigate how to combine output
    kernel tree-based methods with random projection of the output space
    to improve computational complexity and accuracy. This idea has been
    studied (Lopez-Paz et al., 2014 ) in the context of the kernel
    principal component algorithm and the kernel canonical correlation
    analysis algorithm.

##### 33.2 Growing and compressing decision trees

-   In the context of the @xmath -based compression of random forests,
    we first grow a whole forest, and then prune it. The post-pruning
    step is costly in terms of computational time and memory as we start
    from a complex random forest model. A first study in collaboration
    with Jean-Michel Begon (Begon et al., 2016 ) shows that we actually
    do not need to start from the whole forest, and can grow a
    compressed random forest model greedily. Starting from a set of root
    nodes, the idea is to sequentially develop the nodes that reduce the
    most the error of the chosen loss function. The process continues
    until reaching a complexity constraint, saving time and memory.

-   The space complexity of a decision tree model is linear in the
    number of (test and leaf) nodes, which is typically proportional to
    the training set size @xmath . In the context of @xmath outputs
    multi-output classification or regression tasks, or @xmath classes
    multi-class classification tasks, a leaf node is a constant model
    stored as a vector of size @xmath . The space complexity of a
    decision tree model is thus @xmath . With high dimensional output
    spaces or many classes, it thus may become prohibitive to store
    decision tree or random forest models. We would like to investigate
    two further approaches to compress such models: (i) by adapting the
    (post)-pruning method developed in (Joly et al., 2012 ) and in
    Chapter \thechapter to multi-output tasks and multi-class
    classification tasks and (ii) by compressing exactly or
    approximately each constant leaf model. Both approaches can be used
    together. For approach (ii), an exact solution would compute the
    constant leaf models on-the-fly at prediction time by storing once
    the output matrix and the indices of the samples reaching the leaf
    at learning time. With totally developed trees, it should not modify
    the computational complexity of the prediction algorithm. If we
    agree to depart from the vanilla decision tree model, it is also
    possible to approximate the leaf model, for instance by keeping at
    the leaf nodes only the subset of the @xmath output-values reducing
    the most the error either at the leaf level as in (Prabhu and Varma,
    2014 ) or at the tree level. Also, if the output space is sparse,
    appropriate sparse data structures could help to further reduce the
    memory footprint of the models.

##### 33.3 Learning in high dimensional and sparse input-output spaces

-   In Chapter \thechapter , we have shown how to improve tree growing
    efficiency in the case of sparse high-dimensional input spaces.
    However, the small fraction of non zero input values exploited for
    each split typically leads to highly unbalanced tree structure. On
    such tasks, it would be interesting to grow decision trees with
    multivariate splitting rules to make the tree balanced. For instance
    in text-based supervised learning, the input text is often converted
    to a vector through a bag-of-words, where each variable indicates
    the presence of a single word. In this context, each test node
    assesses the presence or the absence of a single word. The tree
    growing algorithm lead to unbalanced trees as each training sample
    has only a small fraction of all possible words. We propose to
    investigate node splitting methods that would combine several sparse
    input variables into a dense one. In the text example, we would
    generate new dense variables by collapsing several words together.
    In a more general context, we could use random “or” or random
    “addition” functions of several sparse input variables.

    Furthermore, while we have shown empirically that the implementation
    proposed in Chapter \thechapter indeed translates into a speed up
    and reduction of memory consumption, it would be interesting to
    study formally its computational complexity as a function of the
    input-space sparsity.

-   In the multi-label classification task, the output space is often
    very large and sparse (as in Chapter \thechapter and Chapter
    \thechapter ), having few ‘‘non zero values’’ ¹² ¹² 12 We assume
    that the majority class of each output is coded as “0” and the
    minority classes is coded with the value “1”. . It would be
    interesting to exploit the output space sparsity to speed up the
    decision tree algorithm. The algorithm interacts with the output
    space during the search of the best split and the training of the
    leaf models. The search for the best split for an ordered variable
    is done by first sorting the possible splitting rules according to
    their threshold values, and then the best split is selected by
    computing incrementally the reduction of impurity by moving samples
    from the right partition to the left partition. The leaf models are
    constant estimators obtained by aggregating output values. Both
    procedures require efficient sample-wise indexing or row-wise
    indexing as provided by the compressed row storage (csr) sparse
    matrix format. We propose to implement impurity functions and leaf
    model training procedures to work with csr sparse matrices.

## Part IV Appendix

### Chapter \thechapter Description of the datasets

### Appendix A Synthetic datasets

-   Friedman1 (Friedman, 1991 ) is a regression problem with @xmath
    independent input variables of uniform distribution @xmath . We try
    to estimate the output @xmath , where @xmath is a Gaussian noise
    @xmath . There are @xmath learning samples and @xmath testing
    samples.

-   Two-norm (Breiman, 1996b ) is a binary classification problem with
    @xmath normally distributed (and class-conditionally independent)
    input variables: either from @xmath if the class is @xmath or from
    @xmath if the class is 1 (with @xmath ). There are @xmath learning
    and @xmath testing samples.

### Appendix B Regression datasset

-   SEFTi (AA&YA, 2008 ) is a (simulated) regression problem which
    concerns the tool level fault isolation in a semiconductor
    manufacturing. One quarter of the values are missing at random and
    were replaced by the median. There are @xmath input variables,
    @xmath learning samples and @xmath testing samples.

### Appendix C Multi-label dataset

Experiments are performed on several multi-label datasets: the yeast
(Elisseeff and Weston, 2001 ) and the bird (Briggs et al., 2013 )
datasets in the biology domain; the corel5k (Duygulu et al., 2002 ) and
the scene (Boutell et al., 2004 ) datasets in the image domain; the
emotions (Tsoumakas et al., 2008b ) and the CAL500 (Turnbull et al.,
2008 ) datasets in the music domain; the bibtex (Katakis et al., 2008 )
, the bookmarks (Katakis et al., 2008 ) , the delicious (Tsoumakas
et al., 2008a ) , the enron (Klimt and Yang, 2004 ) , the EUR-Lex
(subject matters, directory codes and eurovoc descriptors) (Mencía and
Fürnkranz, 2010 ) the genbase (Diplaris et al., 2005 ) , the medical ¹³
¹³ 13 The medical dataset comes from the computational medicine center’s
2007 medical natural language processing challenge
http://computationalmedicine.org/challenge/previous . , the tmc2007
(Srivastava and Zane-Ulman, 2005 ) datasets in the text domain and the
mediamill (Snoek et al., 2006 ) dataset in the video domain.

Several hierarchical classification tasks are also studied to increase
the diversity in the number of label and treated as multi-label
classification task. Each node of the hierarchy is treated as one label.
Nodes of the hierarchy which never occured in the training or testing
set were removed. The reuters (Rousu et al., 2005 ) , WIPO (Rousu
et al., 2005 ) datasets are from the text domain. The Diatoms
(Dimitrovski et al., 2012 ) dataset is from the image domain. SCOP-GO
(Clare, 2003 ) , Yeast-GO (Barutcuoglu et al., 2006 ) and Expression-GO
(Vens et al., 2008 ) are from the biological domain. Missing values in
the Expression-GO dataset were inferred using the median for continuous
features and the most frequent value for categorical features using the
entire dataset. The inference of a drug-protein interaction network
(Yamanishi et al., 2011 ) is also considered either using the drugs to
infer the interactions with the protein (drug-interaction), either using
the proteins to infer the interactions with the drugs
(protein-interaction).

Those datasets were selected to have a wide range of number of outputs
@xmath . Their basic characteristics are summarized at Table \thechapter
.1 . For more information on a particular dataset, please see the
relevant paper.

### Appendix D Multi-output regression datasets

Multi-output regression is evaluated on several real world datasets: the
edm (Karalič and Bratko, 1997 ) dataset in the industrial domain; the
water-quality (Džeroski et al., 2000 ) dataset in the environmental
domain; the atp1d (Spyromitros-Xioufis et al., 2016 ) , the atp7d
(Spyromitros-Xioufis et al., 2016 ) , the scm1d (Spyromitros-Xioufis
et al., 2016 ) and the scm20d (Spyromitros-Xioufis et al., 2016 )
datasets in the price prediction domain; the oes97 (Spyromitros-Xioufis
et al., 2016 ) and the oes10 (Spyromitros-Xioufis et al., 2016 )
datasets in the human resource domain. The output of those datasets were
normalized to have zero mean and unit variance.

If the number of testing samples is unspecified, we use a @xmath of the
samples as training and validation set and @xmath of the samples as
testing set.

\manualmark