##### Contents

-    1 Preface
-    2 Mean curvature flow and parabolic equations
    -    2.1 Parabolic equations
    -    2.2 Mean curvature flow
-    3 Gradient estimates for parabolic equations of curve shortening
    flow type in one space dimension
    -    3.1 Outline of the ‘double coordinate’ method
    -    3.2 An estimate for periodic solutions
    -    3.3 Description of a barrier
    -    3.4 An explicit estimate for periodic equations
    -    3.5 Interior estimates for non-periodic equations with @xmath
    -    3.6 A generalisation to fully nonlinear equations
-    4 An existence result for a parabolic equation in one space
    dimension
    -    4.1 Existence of solutions with @xmath initial and boundary
        data
    -    4.2 Displacement estimates
    -    4.3 Existence of solutions with continuous initial data
    -    4.4 Existence of entire solutions with stepped initial
        conditions
-    5 Gradient estimates for parabolic equations in higher dimensions
    -    5.1 Reduction to a one-dimensional problem
    -    5.2 Estimates for periodic solutions
    -    5.3 Estimates for boundary value problems
-    6 Application of gradient estimates to the Neumann problem
    -    6.1 Some remarks about changes of coordinates that straighten
        boundaries
    -    6.2 Existence of solutions with continuous initial data and
        Neumann boundary conditions
-    7 Existence of solutions to the Dirichlet problem for mean
    curvature flow
-    8 Gradient estimates found by counting intersections
    -    8.1 Counting zeroes
    -    8.2 Gradient estimates for equations in one space dimension
-    9 Estimates for isotropic and anisotropic mean curvature flow
    -    9.1 A gradient estimate for mean curvature flow
    -    9.2 Gradient estimates for anisotropic mean curvature flows
-    A Function spaces and regularity estimates for parabolic equations
    -    A.1 Function spaces
    -    A.2 Regularity estimates

## Chapter 1 Preface

#### Mean curvature flow

Let @xmath be a family of hypersurfaces, each smoothly embedded in
@xmath and indexed by @xmath . We say @xmath is moving by mean curvature
flow when

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the mean curvature vector at @xmath .

In the last twenty-five years, this flow has been the subject of
concerted study, as have other geometric flows such as the Ricci flow
and Gauss curvature flow. Notable results have included Grayson’s proof
that the curve shortening flow (which is mean curvature flow, reduced to
one space dimension) shrinks embedded curves to a spherical point [ 17 ]
and Huisken’s proof that convex surfaces become spherical under mean
curvature flow [ 18 ] .

If we observe that @xmath , where @xmath is the Laplace-Beltrami
operator on the manifold @xmath , then it seems natural to consider mean
curvature flow as the heat flow for manifolds.

Unlike classical heat flow, this is a nonlinear operator, but it still
has some of the same attributes; in particular, this flow exhibits a
smoothing property . In [ 14 ] , Ecker and Huisken showed that if the
initial surface is given by a locally Lipschitz graph, then there exists
a smooth solution for positive times. In this thesis, this result is
extended to non-Lipschitz initial conditions.

#### Mean curvature flow and parabolic differential equations

Chapter 2 is a short introduction to parabolic differential operators
and some key results in mean curvature flow.

If @xmath is locally represented as @xmath for some @xmath , then @xmath
will satisfy the parabolic equation

  -- -------- --
     @xmath   
  -- -------- --

This places mean curvature flow in the setting of classical parabolic
partial differential equations, the framework for most of this thesis.

Motivated by this setting, we examine other parabolic operators that
have similar diffusion properties to mean curvature flow. This includes
anisotropic mean curvature flow, a generalization of mean curvature flow
arising in many physical applications.

#### Gradient estimates using a ‘double coordinates’ approach

In this thesis three distinct methods are used for deriving gradient
estimates. The first of these is introduced in Chapter 3 . It originated
with Kružkov in [ 22 ] . Given a parabolic equation in one space
dimension

  -- -------- --
     @xmath   
  -- -------- --

one can form an evolution equation for the difference @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Under favourable conditions on @xmath , one can use the maximum
principle and an appropriate barrier to find estimates for @xmath that
depend on @xmath . Letting @xmath will then give a gradient estimate for
@xmath .

In this thesis, Kružkov’s method is extended by making use of the full
Hessian

  -- -------- --
     @xmath   
  -- -------- --

rather that just the diagonal elements @xmath and @xmath . In the
one-dimensional case this is of little importance, but in the
higher-dimensional case it gives us greater scope to choose barriers.

In Chapter 3 , we also describe a barrier which begins with an unbounded
gradient, but instantly becomes smooth.

Such barriers allow estimates that are independent of initial gradient
bounds, and that therefore may be used to prove the existence of
solutions to parabolic equations with continuous initial data. This is
one of the main features of the gradient estimates in this thesis.

The ‘double coordinate’ method is extended to higher dimensions in
Chapter 5 for a class of operators that have similar diffusion to the
mean curvature flow.

As this class includes anisotropic curvature flow , this is a
significant improvement to the existing regularity theory.

Gradient estimates are found for both entire periodic solutions and
boundary value problems.

#### Existence results

The gradient estimates of Chapters 3 and 5 may be used to extend
standard existence results.

We do this for a class of parabolic equations in the one dimensional
case in Chapter 4 ; for mean curvature flow with Neumann boundary
conditions in Chapter 6 ; and for mean curvature flow with Dirichlet
boundary conditions in Chapter 7 .

#### Gradient estimates by counting intersections

The second technique for finding gradient estimates is found in Chapter
8 , and it involves examination of the intersections between a given
solution @xmath and a barrier @xmath .

In [ 5 ] , Angenent proved that the number of points in the zero set —
the set where @xmath — of a solution to a parabolic equation in one
space dimension is non-increasing.

This is applied to the difference @xmath . The intersections of @xmath
and @xmath are the zeroes of @xmath , and so Angenent’s results allow us
to show that @xmath and @xmath do not develop new intersections as they
evolve.

When two functions intersect only once, the gradient of one of them
dominates the gradient of the other at that point. Tailoring the
barriers gives us estimates for @xmath in terms of the height of @xmath
, the time @xmath , and (in the case of bounded domains) the distance of
@xmath from the boundary. Again, there is no dependence on an initial
gradient estimate.

As Angenent’s results are limited to equations in one space dimension —
it is difficult to imagine what a generalization of these results to
higher dimensions would look like — this technique applies only to
parabolic operators in one space dimension.

These methods will apply to a wide range of parabolic operators,
provided that suitable barriers exist. In particular, we can apply this
method to the class of operators studied in Chapter 4 .

#### Gradient estimates using a geometric approach

The third method for finding gradient estimates, in Chapter 9 , is a
rather geometrical approach found in the classic Ecker–Huisken curvature
flow papers [ 13 ] , [ 14 ] , [ 18 ] , [ 19 ] and [ 20 ] .

A maximum principle is applied to the difference @xmath , where @xmath
is a “gradient function” (for example, @xmath , and @xmath is a barrier.
In contrast to the earlier two approaches, this creates a direct
estimate for the gradient @xmath itself, rather than for the difference
@xmath .

We apply this to the mean curvature flow (re-creating some of the
results obtained in earlier chapters) and also to the anisotropic mean
curvature flow, under some restrictions on the degree of anisotropy
allowed. Results for entire periodic solutions and strictly interior
results are found in both cases. The estimates found are again
independent of initial gradient bounds, but dependent on the height.

#### Appendices

An inventory of standard results for parabolic partial differential
equations, relevant to the existence results in Chapters 4 , 6 and 7 ,
is included for the convenience of the reader. There is also a
nomenclature listing.

## Chapter 2 Mean curvature flow and parabolic equations

### 2.1 Parabolic equations

An operator @xmath is considered parabolic on a domain when

  -- -------- --
     @xmath   
  -- -------- --

for any positive definite @xmath , positive number @xmath , and any
@xmath in the domain. Here, @xmath is the set of @xmath symmetric
matrices.

In this thesis we look only at operators of the form

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is differentiable with respect to the first variable, then
@xmath will be parabolic if the matrix of derivatives @xmath is positive
definite.

If we can write @xmath for some symmetric @xmath , then we call the
operator quasilinear . It is to quasilinear operators that we will pay
most attention in the following pages.

In this case, @xmath and so the operator is parabolic on @xmath if
@xmath is positive definite for all @xmath , where @xmath is a subset of
@xmath .

We denote the maximum and minimum eigenvalues of @xmath (or @xmath ) by
@xmath and @xmath .

If the ratio @xmath is bounded on @xmath , then @xmath is called
uniformly parabolic on @xmath .

An operator is parabolic with respect to a function @xmath when @xmath
is parabolic.

When @xmath — for example, when @xmath , as in the parabolic @xmath
-Laplacian equation — such an operator is called degenerate .

The key to much of the theory used here is the Comparison Principle . As
presented in [ 25 ] :

###### Theorem 2.1 (Quasilinear comparison principle 1).

Suppose that @xmath is a quasilinear parabolic operator

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and @xmath be in @xmath and let @xmath be parabolic with
respect to either @xmath or @xmath . Then if @xmath in @xmath and if
@xmath on @xmath , then @xmath in @xmath .

Here @xmath denotes the parabolic boundary

  -- -------- --
     @xmath   
  -- -------- --

The proof of this theorem is simple, and is an excellent illustration of
later arguments.

Proof: Suppose that there is an interior point @xmath at time @xmath
where @xmath for the first time. Since this is an internal maximum of
@xmath , @xmath and @xmath must be negative semi-definite. Now,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

However, as this is the first such maximum we must have @xmath , which
give us a contradiction. It follows that @xmath . ∎

We also use the following form, again as in [ 25 ] :

###### Theorem 2.2 (Quasilinear comparison principle 2).

Suppose that @xmath is a quasilinear parabolic operator

  -- -------- --
     @xmath   
  -- -------- --

and that there is an increasing function @xmath such that @xmath is a
decreasing function of @xmath on @xmath for any @xmath . If @xmath and
@xmath are functions in @xmath with @xmath in @xmath and @xmath on
@xmath , and if @xmath is parabolic with respect to either @xmath or
@xmath , then @xmath in @xmath .

### 2.2 Mean curvature flow

If our family of hypersurfaces @xmath is also a family of embeddings
@xmath , then we can write mean curvature flow as

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath is the mean curvature vector at @xmath . In the case that
@xmath can be written locally as a graph over a set @xmath , we write
@xmath , and can calculate geometric quantities such as the upwards unit
normal

  -- -------- --
     @xmath   
  -- -------- --

the metric on the surface

  -- -------- --
     @xmath   
  -- -------- --

\nomenclature

[gij] @xmath metric on a Riemannian manifold \refpage the second
fundamental form

  -- -------- --
     @xmath   
  -- -------- --

\nomenclature

[h1ij] @xmath second fundamental form \refpage and the mean curvature

  -- -------- --
     @xmath   
  -- -------- --

\nomenclature

[H2] @xmath mean curvature \refpage

The mean curvature vector is @xmath , and so (if we remove movement
tangential to the surface) mean curvature flow for graphs is given by

  -- -------- --
     @xmath   
  -- -------- --

In the case when @xmath , this reduces to curve-shortening flow

  -- -------- --
     @xmath   
  -- -------- --

With reference to the previous section, note that the largest and
smallest eigenvalues for mean curvature flow for graphs are @xmath and
@xmath , so it will be uniformly parabolic only when the gradient is
bounded.

Whether studied in a geometric setting as ( 2.1 ), or as a special case
of a quasilinear parabolic differential equation as ( 2.2 ), the
comparison principle has been crucial. Applied to mean curvature flow,
it gives:

###### Theorem 2.3.

Let @xmath and @xmath be two smooth compact surfaces moving under mean
curvature flow. If they are disjoint at the initial time, they are
disjoint at later times.

We can also make similar comparisons between surfaces with boundaries,
and between other quantities (such as the gradient function @xmath ).
The following theorem from [ 14 ] is one such result.

###### Theorem 2.4 (Interior gradient estimate).

Suppose that @xmath satisfies the mean curvature flow equation ( 2.2 )
on a cylinder @xmath . Then we have an estimate for the gradient at the
center of the ball at later times:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where @xmath and @xmath depend only on @xmath .

We will also use the following a priori estimates for higher
derivatives, from the same paper:

###### Theorem 2.5 (@xmath interior estimate for mean curvature flow).

Suppose that @xmath satisfies ( 2.2 ) on @xmath . Then for arbitrary
@xmath the estimate

  -- -------- --
     @xmath   
  -- -------- --

holds for all @xmath . Here @xmath .

###### Theorem 2.6 (@xmath interior estimate for mean curvature flow).

Suppose that @xmath satisfies ( 2.2 ) on @xmath . Then for @xmath and
arbitrary @xmath we have the estimate

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a constant depending on @xmath and @xmath .

A bound on @xmath gives a bound on @xmath , since (using coordinates in
which @xmath is diagonal)

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we have used that the smallest eigenvalue of @xmath is @xmath .
So,

  -- -------- --
     @xmath   
  -- -------- --

and in a similar manner, bounds on derivatives of @xmath give bounds on
higher derivatives of @xmath .

These estimates may be used to show long-time existence results, such as
the following from [ 13 ] :

###### Theorem 2.7.

If @xmath is a locally Lipschitz, entire graph over @xmath , then there
is smooth solution to ( 2.2 ) for all @xmath .

In the following pages, we will derive new existence results of this
sort.

## Chapter 3 Gradient estimates for parabolic equations of curve
shortening flow type in one space dimension

In this chapter we outline gradient estimates for a class of parabolic
equations in one space dimension.

This chapter takes inspiration from the work of Huisken in [ 21 ] ,
where he investigated embedded plane curves evolving by curve shortening
flow by looking at the evolution equation for the quotient of @xmath ,
the distance between two points @xmath and @xmath in the metric of the
plane, and @xmath , the length of curve between @xmath and @xmath . This
introduced a double set of space coordinates (those around the point
@xmath and those around the point @xmath ). At a maximum point, the
first and second derivative conditions give strong conditions at both
@xmath and @xmath , allowing close examination of all possible
situations. An application of the maximum principle resulted in a new
proof of Grayson’s theorem regarding the evolution of embedded curves.

In this chapter, we follow the approach of Kružkov in [ 22 ] (and well
described in Lieberman’s book [ 25 ] , chapter XI, section 6).

If @xmath solves a parabolic partial differential equation in one space
variable, then @xmath solves a parabolic equation in two space
variables, for which we can seek a barrier.

In the paper cited, Kružkov was interested in fully nonlinear equations

  -- -------- --
     @xmath   
  -- -------- --

with uniform parabolicity condition

  -- -------- --
     @xmath   
  -- -------- --

In this section, we do not require uniform parabolicity, but in order to
show existence of the barriers, we will require a scaling similar to
that of the curve shortening flow equation

  -- -------- --
     @xmath   
  -- -------- --

in that @xmath for large @xmath .

We begin with a description of the ideas motivating the method. The
notation @xmath will indicate derivatives with respect to the space
variable, which I hope I will use only where this is unambiguous.

### 3.1 Outline of the ‘double coordinate’ method

Consider a smooth @xmath satisfying

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Let @xmath be given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is some smooth function.

Suppose now that @xmath attains a maximum at some point @xmath , with
@xmath . At the maximum point, the first derivatives are zero, and so

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

Similarly, the matrix of second order partial derivatives is
non-positive, by which we mean that for all @xmath , @xmath , where
@xmath is the Hessian matrix

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

If we now consider the evolution equation satisfied by @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and if we take this at the local maximum we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for some @xmath and @xmath . If the first matrix above is positive
semi-definite, then as @xmath is negative semi-definite, the trace above
is non-positive and

  -- -------- --
     @xmath   
  -- -------- --

A useful choice for @xmath and @xmath that makes the first matrix
positive semi-definite is @xmath , @xmath ; then

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

The idea now is to choose @xmath in a way so that at the local maximum,
@xmath . We begin by observing that for simple equations, a solution to
a simplified version of the equation itself is acceptable for use as the
barrier @xmath .

Remark: We could simplify the method by choosing @xmath , in which case
the factor of @xmath is absent from ( 3.3 ). The use of
cross-derivatives will be important when we extend this method to higher
dimensions.

### 3.2 An estimate for periodic solutions

###### Theorem 3.1.

Suppose @xmath is a @xmath , periodic and bounded solution of

  ------------------------ -- -------- -- -------
                              @xmath      (3.4)
  with initial condition                  
                              @xmath      
  ------------------------ -- -------- -- -------

with @xmath and @xmath .

If @xmath is a solution of

  -- -------- --
     @xmath   
  -- -------- --

with the initial and boundary conditions

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

then

  -- -------- --
     @xmath   
  -- -------- --

Proof: Following on from the previous remarks, we set

  -- -------- --
     @xmath   
  -- -------- --

and choose @xmath .

As @xmath , @xmath for all @xmath , and so @xmath .

As @xmath is periodic, @xmath and so @xmath is periodic over strips

  -- -------- --
     @xmath   
  -- -------- --

Within each strip, @xmath and

  -- -------- --
     @xmath   
  -- -------- --

as @xmath , so @xmath attains its spatial maximum in each strip, and
hence in the entire plane, for each @xmath . We can calculate

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and in particular at a maximum point @xmath with @xmath and @xmath
non-negative, equation ( 3.3 ) becomes

  -- -------- --
     @xmath   
  -- -------- --

Therefore, at such a maximum point @xmath is non-increasing in time and
so @xmath .

The reason for the restriction @xmath is that @xmath is not
differentiable here. When the maximum is attained at such a point, then
@xmath , so in either case @xmath for all @xmath . The result follows. ∎

We can find explicit estimates for more general equations by choosing an
explicit barrier.

### 3.3 Description of a barrier

This barrier, @xmath , will be used often.

Let @xmath be the fundamental solution to the heat equation,

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath , where @xmath is a positive constant and @xmath .
Implicitly define @xmath by

  -- -------- --
     @xmath   
  -- -------- --

This function has the property that as @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and that @xmath for all @xmath .

We can calculate

  ------------------------------------------------------------------------------------------ -------- -- --
                                                                                             @xmath      
                                                                                             @xmath      
  and (third derivatives are included for completeness but not used until a later chapter)               
                                                                                             @xmath      
  while                                                                                                  
                                                                                             @xmath      
  ------------------------------------------------------------------------------------------ -------- -- --

Routine calculations yield

  -- -------- -- -------
     @xmath      
     @xmath      
     @xmath      (3.5)
     @xmath      
  -- -------- -- -------

The partial differential equation satisfied by @xmath is

  -- -------- --
     @xmath   
  -- -------- --

### 3.4 An explicit estimate for periodic equations

###### Theorem 3.2.

Let @xmath be a @xmath solution of

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where @xmath is continuous; both @xmath and @xmath are periodic, @xmath
, @xmath (and therefore @xmath is also periodic); @xmath ; and where we
can find positive constants @xmath and @xmath such that

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

Then there is a @xmath such that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are dependent on @xmath , @xmath and
@xmath .

Proof: Let @xmath be as before, with @xmath for the @xmath defined in
Section 3.3 , with the constant @xmath given by @xmath , where @xmath
will be chosen later.

Consider the region

  -- -------- --
     @xmath   
  -- -------- --

where @xmath satisfies @xmath . Explicitly,

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

As before, @xmath is periodic over strips parallel to @xmath and so it
attains its maximum on @xmath . We first show that @xmath on the
boundary of @xmath .

For @xmath , as @xmath , @xmath and so @xmath .

At @xmath , @xmath for all @xmath , and so @xmath . At @xmath , @xmath
and so @xmath .

Now suppose that @xmath attains a maximum on the interior of @xmath .

It follows from ( 3.3 ) that at the maximum,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The second derivative

  -- -------- --
     @xmath   
  -- -------- --

is negative, as @xmath is positive, and the part inside the square
brackets @xmath is positive in @xmath if @xmath .

We can estimate

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the last line follows by choosing @xmath and recalling that @xmath
.

Now we can use the condition ( 3.6 ), controlling the degeneracy of
@xmath , to estimate

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

as @xmath . So, @xmath at an internal maximum, @xmath on the boundary,
and so @xmath on @xmath .

Explicitly, for @xmath , @xmath means

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (3.8)
  -- -------- -------- -- -------

which is an estimate for the difference quotient @xmath .

We can obtain an estimate for @xmath , @xmath by observing that for
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

so that as @xmath ,

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

So far, we have estimates for when @xmath . We can find identical
estimates for the region where @xmath by reflecting in the line @xmath .

Letting @xmath in the estimates for the difference quotients ( 3.8 ) and
( 3.9 ) gives the result. ∎

Comparing @xmath with the special barrier @xmath gives us a gradient
estimate for solutions of quasilinear equations with this scaling. We
will use this estimate in Chapter 5 .

###### Corollary 3.3 (Gradient estimates for the barrier @xmath).

Let @xmath be a smooth solution of

  -- -------- --
     @xmath   
  -- -------- --

on @xmath , with the initial and boundary conditions

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

If

  -- -------- --
     @xmath   
  -- -------- --

then there is a @xmath such that for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath depend on @xmath and @xmath .

Proof: We apply the comparison principle to @xmath and @xmath , where
@xmath is as in Section 3.3 with the constant @xmath .

The barrier @xmath dominates @xmath on @xmath and @xmath .

Choose @xmath and @xmath so that on @xmath , @xmath , @xmath and at
@xmath , @xmath .

Then @xmath and

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

so Theorem 2.1 implies that @xmath on @xmath for all @xmath , and as
@xmath , @xmath . Since @xmath this gives us the boundary gradient
estimate

  -- -------- --
     @xmath   
  -- -------- --

∎

### 3.5 Interior estimates for non-periodic equations with @xmath

###### Theorem 3.4.

Let @xmath be a @xmath solution to

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is an open interval and where there are positive constants
@xmath and @xmath so that

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

If @xmath , then we can find an estimate for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are dependent on @xmath , @xmath ,
@xmath and @xmath .

We modify the previous proof, introducing a new boundary in the @xmath
coordinates, since we no longer have compactness of the domain through
periodicity. We will seek to avoid a maximum of @xmath occurring on the
new boundary.

Proof: Firstly, suppose that @xmath .

We consider the sub-region

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is as before in ( 3.7 ) and @xmath is chosen so that @xmath
for @xmath .

Define @xmath on @xmath by

  -- -------- --
     @xmath   
  -- -------- --

In order to avoid a positive maximum of @xmath on the boundary, we will
ensure that @xmath satisfies

-   @xmath for @xmath

-   @xmath for @xmath

-   @xmath for @xmath

-   @xmath for @xmath .

We choose

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the explicit barrier defined in Section 3.3 , for
positive constants @xmath and @xmath . We will also choose @xmath with
@xmath later.

This @xmath satisfies the first three conditions. In order to fulfill
the final condition, choose @xmath . Then at @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Now suppose that @xmath first reaches a positive maximum at an internal
point @xmath . As usual, we calculate that at this point first
derivatives are zero

  -- -------- --
     @xmath   
  -- -------- --

and the matrix of second derivatives is negative semi-definite

  -- -------- --
     @xmath   
  -- -------- --

We use these in the evolution equation for @xmath

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

the last line applying at the internal maximum . As before, @xmath for
@xmath . The first derivative condition for @xmath implies that @xmath
and @xmath and so

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

where the final inequality comes from choosing @xmath as in the previous
section.

We can then exploit ( 3.10 ), the condition on @xmath ;

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

This is greater than @xmath whenever @xmath , so we use ( 3.11 ), the
lower bound on @xmath , and choose

  -- -------- --
     @xmath   
  -- -------- --

Now @xmath at interior maxima, and so the parabolic maximum principle
ensures that @xmath on @xmath .

For points outside @xmath , but in the rectangle @xmath , @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

We can repeat both these estimates for a reflected region where @xmath ;
putting them all together gives

  -- -------- --
     @xmath   
  -- -------- --

Now, for @xmath , set @xmath and let @xmath to give a gradient estimate
at @xmath

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

For the case of a general interval @xmath , we can rescale around a
point @xmath by using scaled coordinates @xmath . We obtain the estimate

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends on @xmath , @xmath , @xmath , and @xmath . ∎

### 3.6 A generalisation to fully nonlinear equations

In this section we consider equations of the form

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

where @xmath is @xmath .

Let @xmath be a smooth solution to ( 3.12 ). As before, define

  -- -------- --
     @xmath   
  -- -------- --

and suppose it first becomes non-negative at some point @xmath , with
@xmath . First and second derivatives of @xmath will satisfy ( 3.1 ) and
( 3.2 ), but the evolution equation for @xmath will be given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we have used that @xmath at a spatial maximum, and have
abbreviated

  -- -------- --
     @xmath   
  -- -------- --

and where we have added and subtracted @xmath for some @xmath , @xmath .
If we choose

  -- -------- --
     @xmath   
  -- -------- --

the first matrix above is positive semi-definite. Since the matrix of
second derivatives is negative semi-definite, we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

If we make quite harsh restrictions on @xmath , then we can use our
explicit barrier to find an analogue of Theorem 3.2 for periodic
nonlinear equations.

###### Theorem 3.5 (Nonlinear version of Theorem 3.2).

Let @xmath be a @xmath solution of

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where @xmath is continuous and periodic, @xmath , (and therefore @xmath
is also periodic); @xmath ; where we can find positive constants @xmath
and @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

and where @xmath .

Then there is a @xmath such that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are dependent on @xmath , @xmath and
@xmath .

Proof: The proof is the same as that of Theorem 3.2 , including the
choice @xmath , but instead of using inequality ( 3.3 ) for interior
maximum points, we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the omitted argument of the derivatives of @xmath , denoted by
@xmath , is @xmath . ∎

## Chapter 4 An existence result for a parabolic equation in one space
dimension

Although this is a standard result (see Theorem 12.25 of [ 25 ] ), for
completeness we sketch a short time existence result in the
one-dimensional case, where the spatial domain is @xmath , and the
initial and boundary data is continuous.

The parabolic equation is

  -- -------- -------- -- -------
     @xmath   @xmath      (4.1)
  -- -------- -------- -- -------

with initial and boundary data prescribed by

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

We require that @xmath is in @xmath for all bounded @xmath and some
@xmath .

This implies that for every such @xmath we can find positive @xmath and
@xmath such that

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

When we can find bounds of this type that depend only on the gradient,
we will write

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

Suppose also that there are positive constants @xmath and @xmath such
that

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

The first part of this chapter is a survey of the main steps needed to
find the existence result for the Cauchy-Dirichlet problem with @xmath
initial and boundary data. We follow the treatment in Lieberman [ 25 ] .

These results mean that when we approximate continuous initial data by
smooth initial data, a solution will exist for the approximate initial
data. In the later parts of the chapter, we use the gradient estimate
established in Chapter 3 to find uniform gradient estimates for @xmath .
This will gives us a solution for @xmath ; in order to show that this
approaches the initial data as @xmath , we will need some displacement
estimates which limit the distance a function can travel in a given
time.

### 4.1 Existence of solutions with @xmath initial and boundary data

###### Theorem 4.1.

Consider the Cauchy-Dirichlet problem given by ( 4.1 ) and ( 4.2 ),
where @xmath .

Suppose that @xmath is defined on the parabolic boundary @xmath and
@xmath . Also, suppose that either @xmath is time-independent, or else
there are constants @xmath and @xmath such that ( 4.5 ) is satisfied.

Then there is a smooth solution @xmath .

This solution has a gradient bound @xmath where @xmath depends on @xmath
, @xmath , @xmath , @xmath and @xmath .

The proof of this result follows a standard pattern for showing
existence — a bound on @xmath ; a bound on @xmath ; a Hölder gradient
bound @xmath ; and then the application of a fixed point theorem. These
steps are sketched by the following results.

We begin by using the comparison principle to bound @xmath .

###### Lemma 4.2 (A bound on @xmath).

If @xmath is a smooth solution of ( 4.1 ), ( 4.2 ) in @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

Proof idea: Set @xmath and apply the comparison principle (Theorem 2.1 )
to @xmath and @xmath on @xmath . Since @xmath on @xmath , it follows
that @xmath on all of @xmath and so on all of @xmath .

Similar steps can be followed to find that @xmath , completing the
result. ∎

We begin our gradient estimates with a boundary gradient estimate.

###### Lemma 4.3 (Boundary gradient estimate).

Let @xmath If @xmath is a smooth solution of ( 4.1 ), ( 4.2 ) in @xmath
, and either @xmath is time-independent or else @xmath satisfies the
condition ( 4.5 ), then

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends only on @xmath , @xmath and @xmath .

In fact, we can relax the regularity requirements on the initial and
boundary data and still find a continuity estimate on the boundary.

A modulus of continuity is a concave, continuous function @xmath , with
@xmath . This @xmath is a modulus of continuity for a function @xmath at
@xmath if \nomenclature [omega] @xmath a modulus of continuity \refpage

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath in the domain of @xmath . It is a modulus of continuity
for @xmath if the above relationship also holds for all @xmath in the
domain of @xmath .

A modulus of continuity can be defined for every continuous function on
a closed bounded set.

###### Lemma 4.4 (Boundary continuity estimate).

Let @xmath satisfy ( 4.1 ), ( 4.2 ), where @xmath has modulus of
continuity @xmath , and suppose there are positive constants @xmath and
@xmath so that

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

whenever @xmath .

Then @xmath has a modulus of continuity on the boundary

  -- -------- --
     @xmath   
  -- -------- --

for @xmath and @xmath , where @xmath can be determined by @xmath ,
@xmath , @xmath , and @xmath .

Equipped with the boundary gradient estimate, we can now find a global
gradient estimate. In this one-dimensional case, the global gradient
estimate is the result of Kružkov mentioned in Chapter 3 .

###### Lemma 4.5 (Global gradient estimate).

If @xmath is a smooth solution of ( 4.1 ), ( 4.2 ) in @xmath with an
oscillation bound @xmath , and a Lipschitz estimate on the parabolic
boundary @xmath for all @xmath and @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 4.6 (Global Hölder gradient estimate).

Suppose that @xmath satisfies ( 4.1 ), ( 4.2 ) in @xmath , where there
are positive constants @xmath and @xmath such that whenever @xmath in
the set @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , set @xmath and @xmath . Then there are positive constants
@xmath and @xmath determined by @xmath , @xmath , @xmath and @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

Now that we have bounds for @xmath , we can apply the following
existence theorem, which is derived from a fixed point theorem.

###### Lemma 4.7 (Existence theorem).

Let @xmath be in @xmath for some @xmath

If there is a constant @xmath independent of @xmath such that any
solution of ( 4.1 ), ( 4.2 ) on @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

then there is a solution of the Cauchy-Dirichlet problem ( 4.1 ), ( 4.2
) in @xmath .

### 4.2 Displacement estimates

The following estimates for the displacement suffered in a given
interval of time by a function moving under a parabolic flow apply to
any strictly parabolic operator satisfying bounds of the form ( 4.3 ) or
( 4.4 ).

###### Lemma 4.8 (Displacement estimate for Lipschitz initial data).

Let @xmath satisfy ( 4.1 ), where @xmath , and @xmath has bounds of the
form ( 4.4 ).

Suppose that @xmath has initial data whose graph lies below a cone
centred at some point @xmath

  -- -------- --
     @xmath   
  -- -------- --

and, in the case that @xmath , whose boundary data lies below the same
cone

  -- -------- --
     @xmath   
  -- -------- --

Then, at later times,

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where @xmath is given by ( 4.4 ).

Proof: For some small @xmath , set

  -- -------- --
     @xmath   
  -- -------- --

which satisfies the heat equation

  -- -------- --
     @xmath   
  -- -------- --

and approaches the cone of gradient @xmath centred at @xmath as @xmath .

Note that @xmath , that @xmath and that @xmath , so

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we choose @xmath .

The estimate follows by applying the comparison principle (Theorem 2.2 )
to show that @xmath , and then letting @xmath .

∎

Now, we apply this to three different cases, firstly when @xmath
initially satisfies a Hölder condition and when we have polynomial
growth in @xmath , secondly when @xmath has a modulus of continuity, and
thirdly when @xmath is initially bounded by a step function.

###### Corollary 4.9 (Displacement estimate for Hölder initial data).

Let @xmath satisfy ( 4.1 ). Suppose that @xmath not only satisfies ( 4.4
), but more specifically has at most polynomial growth, so that

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

where @xmath and @xmath are positive constants. Also, suppose that
@xmath satisfies a Hölder condition around some point @xmath

  -- -------- --
     @xmath   
  -- -------- --

Then, at later times,

  -- -------- --
     @xmath   
  -- -------- --

Proof:

For simplicity, assume @xmath and @xmath . The initial data is bounded
above by cones centred at @xmath and indexed by @xmath , the (positive)
@xmath -coordinate of the point of contact with the bounding cusp @xmath
, so

  -- -------- --
     @xmath   
  -- -------- --

The estimate ( 4.7 ) taken at @xmath is then

  ---------------------------------- -------- -------- -- --
                                     @xmath   @xmath      
  and optimizing over @xmath gives                        
                                     @xmath   @xmath      
  ---------------------------------- -------- -------- -- --

∎

###### Corollary 4.10 (Displacement estimate for continuous initial
data).

Suppose that @xmath satisfies ( 4.1 ) and ( 4.4 ), where @xmath has
initial data with a modulus of continuity @xmath at a point @xmath

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is dependent on @xmath and @xmath , and where @xmath as
@xmath .

Proof: For simplicity, assume @xmath and @xmath . Consider the cones

  -- -------- --
     @xmath   
  -- -------- --

indexed by @xmath , the (positive) @xmath -coordinate of a point of
contact with @xmath . As @xmath is concave it has both left and right
derivatives, and we can choose the slope of the cone @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

Now we have a cone as an upper boundary, we can use estimate ( 4.7 ) at
@xmath

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

where @xmath is given by ( 4.4 ). Minimize this over @xmath to get the
displacement bound

  -- -------- --
     @xmath   
  -- -------- --

In order to show that @xmath as @xmath , let @xmath . As @xmath is
concave and positive, it has positive left derivative and for @xmath we
have

  -- -------- --
     @xmath   
  -- -------- --

And as @xmath is continuous,

  -- -------- --
     @xmath   
  -- -------- --

Choose @xmath so that @xmath Choose @xmath so that

  -- -------- --
     @xmath   
  -- -------- --

then for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and so @xmath . ∎

Set @xmath to be the maximal monotone graph

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

which we will refer to as the step “function” .

###### Corollary 4.11 (Displacement estimate for step functions).

If @xmath satisfies ( 4.1 ) and ( 4.4 ), and is initially bounded by a
step function

  -- -------- --
     @xmath   
  -- -------- --

then for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath as in ( 4.4 ).

Proof: Near some point @xmath , @xmath satisfies a Lipschitz condition

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Lemma 4.8 then gives that

  -- -------- --
     @xmath   
  -- -------- --

and if we let @xmath then we find that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The final result is found by comparison to the constant function @xmath
. ∎

Remark: If @xmath satisfies the condition ( 4.8 ), then

  -- -------- --
     @xmath   
  -- -------- --

∎

### 4.3 Existence of solutions with continuous initial data

###### Theorem 4.12.

Consider the Cauchy-Dirichlet problem given by ( 4.1 ) and ( 4.2 ). If
@xmath and if there are constants @xmath and @xmath such that ( 4.5 )
holds, then ( 4.1 ), ( 4.2 ) has a solution @xmath .

The first step in the proof of the above is to approximate @xmath by
@xmath in @xmath , so that @xmath .

###### Lemma 4.13 (Existence of solutions with approximate boundary
data).

For all @xmath , there exist solutions @xmath to ( 4.1 ) with boundary
data @xmath These solutions are in @xmath .

Proof: As @xmath is in the Hölder space @xmath , this is a consequence
of Theorem 4.1 . ∎

###### Lemma 4.14 (Existence of uniform oscillation bound).

For all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: For any fixed @xmath , set @xmath and apply the comparison
principle (Theorem 2.1 ) to @xmath and @xmath on @xmath . Since @xmath
on @xmath , it follows that @xmath on all of @xmath and hence on all of
@xmath , and so

  -- -------- --
     @xmath   
  -- -------- --

where the last inequality will hold for small enough @xmath .

This leads to a uniform oscillation bound for @xmath , which we denote
by @xmath —

  -- -------- --
     @xmath   
  -- -------- --

∎

Theorem 3.4 gives a uniform gradient bound on interior sets, up to some
time @xmath . For @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are dependent on @xmath , @xmath ,
@xmath and @xmath .

###### Lemma 4.15 (Higher regularity on interior sets).

On interior sets @xmath we can estimate higher derivatives

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends on @xmath , @xmath , @xmath , @xmath , @xmath ,
@xmath and @xmath .

Proof: Once we have an oscillation bound @xmath and a gradient bound
@xmath , ( 4.3 ) implies uniform parabolicity. A uniform Hölder gradient
bound on interior sets results from Theorem 12.2 of [ 25 ] . In
particular, on interior sets and when @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where both @xmath and @xmath depend on @xmath and @xmath , given by (
4.3 ), with

  -- -------- --
     @xmath   
  -- -------- --

and @xmath also depends on @xmath , and @xmath .

Equipped with a Hölder gradient bound, we can treat the equation as a
uniformly parabolic equation with Hölder continuous coefficients, and
use standard results, such as Theorem A.4 , to find that @xmath is
uniformly bounded in @xmath .

From here, it is possible to use the bootstrapping method to obtain
interior estimates for all higher derivatives.

∎

###### Corollary 4.16.

On any interior set @xmath , there exists a subsequence converging to
some @xmath that also solves the partial differential equation ( 4.1 ).

In order for this @xmath to be a solution of the Cauchy-Dirichlet
problem, we need to show that @xmath attains the initial and boundary
data.

###### Lemma 4.17 (Convergence to initial data).

On any spatially interior set @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: Let @xmath be any point in @xmath . Let @xmath be a modulus of
continuity for @xmath .

We can off-set @xmath by defining

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath . Let @xmath be the limit of a subsequence @xmath , as in
Corollary 4.16 .

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The second of these terms is zero. To estimate the first term, note that
the approximations @xmath satisfy the same the same continuity condition
as @xmath , and therefore so does @xmath , with @xmath , for all @xmath
. Corollary 4.10 then gives the estimate

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends only on the exact forms of @xmath and @xmath (given
in ( 4.3 )). In particular, @xmath is independent of @xmath and @xmath ,
and as @xmath as @xmath , the result follows. ∎

More specific continuity-in-time estimates are given by the continuity
of the initial data and the upper growth bound of @xmath . If, for
example, the initial data is Hölder continuous

  -- -------- --
     @xmath   
  -- -------- --

and @xmath has polynomial growth in the gradient term, satisfying ( 4.8
) for constants @xmath and @xmath , then Corollary 4.9 indicates that
@xmath .

###### Lemma 4.18 (Convergence to boundary data).

We can continuously extend @xmath , defined on the interior of @xmath at
time @xmath , to @xmath . Moreover, @xmath on the boundary.

Proof: We need to show that for @xmath , @xmath .

We note that our parabolic equation satisfies condition ( 4.6 ), since

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , using ( 4.5 ).

Let @xmath be a modulus of continuity for @xmath . As each @xmath has at
least the same modulus of continuity as @xmath , Lemma 4.4 gives us an
estimate uniform in @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Then for a point @xmath and fixed @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

so as @xmath , @xmath and @xmath — that is, we can continuously extend
@xmath to @xmath on @xmath for @xmath . ∎

### 4.4 Existence of entire solutions with stepped initial conditions

Consider equation ( 4.1 ), under the conditions on @xmath given by ( 4.3
) and ( 4.5 ).

###### Lemma 4.19.

There exist entire solutions to this equation with the periodic,
crenellated initial data

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is given by ( 4.9 ).

Proof: If we let @xmath be the smooth mollification of @xmath , then for
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Theorem 4.1 ensures that there is a smooth solution @xmath to 4.1 with
initial condition @xmath , with a Hölder gradient bound dependent on
@xmath . The gradient bound in Theorem 3.2 is independent of @xmath ;
for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are dependent on @xmath , @xmath and
@xmath , but not @xmath . Higher gradient bounds for @xmath follow from
the interior estimate ( 4.15 ) and we can find a subsequence converging
to @xmath which also solves the equation on @xmath .

To show convergence of @xmath to the initial data, suppose that @xmath .
As in Section 4.2 , we can bound the initial data @xmath by cones
centred at @xmath —

  -- -------- --
     @xmath   
  -- -------- --

Applying Lemma 4.8 to this, and setting @xmath , we find that for @xmath
,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath as in ( 4.3 ) and so we have the estimate

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

A similar estimate holds for all @xmath , and so for all @xmath , we can
find @xmath (dependent on @xmath ) such that @xmath . ∎

###### Corollary 4.20.

There exists an entire solution to this problem with the initial data

  -- -------- --
     @xmath   
  -- -------- --

This solution has a gradient estimate for @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath are dependent on @xmath , @xmath and
@xmath .

Proof: Take the limit of the solutions @xmath given by the previous
lemma as @xmath . ∎

## Chapter 5 Gradient estimates for parabolic equations in higher
dimensions

In this chapter we extend the methods of Chapter 3 to higher dimensions.

Consider a smooth solution @xmath to

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath is a symmetric, positive semi-definite @xmath matrix that
is smoothly dependent on @xmath .

Define

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

Compare this definition to that of the Bernstein @xmath function , (see
Chapter 10 of [ 16 ] )

  -- -------- --
     @xmath   
  -- -------- --

Clearly, @xmath and if @xmath , @xmath are the smallest and largest
eigenvalues of @xmath , then for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The middle inequality here becomes an equality when @xmath is an
eigenvector of @xmath .

Our aim is to reduce the @xmath -dimensional problem to a parabolic
equation in one space dimension; we can do this if @xmath is bounded
below by a positive function of @xmath . We will call this

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

For the existence of specific barriers we will require a control on the
degeneracy of @xmath — the existence of positive constants @xmath and
@xmath such that

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

Example 1: If @xmath is an eigenvector of @xmath , then @xmath is the
associated eigenvalue.

Example 2: As a specific example of the above, if @xmath is of the form

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

for functions @xmath , with @xmath and @xmath , then @xmath .

In the mean curvature flow case, @xmath and

  -- -------- --
     @xmath   
  -- -------- --

Example 3: In the most general situation, if @xmath are the non-null
eigenvectors of @xmath with eigenvalues @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

Example 4: In the case that @xmath is positive definite, all eigenvalues
are positive and

  -- -------- --
     @xmath   
  -- -------- --

As Example 2 shows, @xmath need not be positive definite.

Example 5: An elliptic operator @xmath is called of mean curvature type
if there are positive constants @xmath , @xmath so that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the coefficients of mean curvature flow [ 15 , 27 , 16
] . For such equations, one can (under some conditions, particularly on
the shape of the boundary) find apriori estimates on @xmath in terms of
@xmath . It is therefore interesting to note that if @xmath satisfies
only the lower inequality above, then it also satisfies ( 5.4 ).

Example 6: If one may be forgiven for referring to a future section,
note that if the flow is the anisotropic mean curvature flow ( 9.5 ) of
Section 9.2 , then Lemma 9.5 implies that @xmath when @xmath . The
function @xmath will be positive and homogeneous of order one, so that
@xmath .

Thus, anisotropic mean curvature flow satisfies conditions ( 5.3 ) and (
5.4 ).

### 5.1 Reduction to a one-dimensional problem

Let @xmath satisfy ( 5.1 ), where @xmath is a symmetric, positive
semi-definite matrix with @xmath .

In the following, we generalise the calculations of Section 3.1 to
higher dimensions. As in the one-dimensional case, we begin our
discussion by defining

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a @xmath function that will be chosen later.

At an internal maximum point of @xmath , the first derivative conditions
are

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

The second derivatives of @xmath are

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and at a maximum point, the matrix @xmath must be negative
semi-definite.

The evolution equation for @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we add and subtract cross derivative terms with yet-to-be-chosen
coefficients @xmath . If we write @xmath , and assume that we are at an
internal maximum, then ( 5.6 ) implies that @xmath , and we can continue
the calculation:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The idea now is to choose the off-diagonal block @xmath in such a way
that the @xmath matrix

  -- -------- --
     @xmath   
  -- -------- --

is positive semi-definite, leaves the coefficient of @xmath positive and
sets the coefficient of @xmath to zero.

The first and third of these requirements imply that @xmath is given by
@xmath for some @xmath . We can check that

  -- -------- --
     @xmath   
  -- -------- --

and that

  -- -------- --
     @xmath   
  -- -------- --

If we set @xmath , with @xmath defined by ( 5.2 ), this maximizes the
coefficient of @xmath , while keeping @xmath positive semi-definite. For
any @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

At the maximum point of @xmath , we find that

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

In this way, we have reduced our problem to finding @xmath that
satisfies the above equation, or, if we can find a lower bound on @xmath
dependent only on @xmath , as in ( 5.3 ), then

  -- -------- --
     @xmath   
  -- -------- --

We will use the results of Chapter 3 to do this.

### 5.2 Estimates for periodic solutions

The following theorem is an analogue of Theorem 3.2 for higher
dimensions. In the special case of mean curvature flow, this is joint
work with Ben Andrews.

Let @xmath be defined by ( 5.2 ).

###### Theorem 5.1.

Let @xmath be a smooth solution to

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where @xmath is smooth with oscillation bound @xmath , and is also
spatially periodic, @xmath , for some lattice @xmath .

Suppose that @xmath for all @xmath .

If @xmath is a smooth solution to the auxilliary one-dimensional
equation

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

and satisfies the boundary conditions

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

then

  -- -------- --
     @xmath   
  -- -------- --

###### Corollary 5.2.

If there are positive constants @xmath and @xmath so that

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

then there is a @xmath such that for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath and @xmath depend on @xmath , @xmath , @xmath ,
and @xmath .

Proof of Theorem 5.1 . The proof of this is substantially the same as
the proof of Theorem 3.2 , the gradient estimate for periodic,
one-dimensional equations.

As in the previous pages, let

  -- -------- --
     @xmath   
  -- -------- --

and choose @xmath , so that @xmath .

As @xmath is periodic over the lattice @xmath , @xmath is periodic over
regions

  -- -------- --
     @xmath   
  -- -------- --

On any one of these regions, note that @xmath and that @xmath as @xmath
, so @xmath attains a spatial maximum on the region (and hence on the
entire domain @xmath ).

If there is a maximum point @xmath at some @xmath with @xmath , then at
this point @xmath is smooth and

  -- -------- --
     @xmath   
  -- -------- --

If @xmath at the maximum point, then here @xmath and in either case,
@xmath . The estimate for @xmath follows. ∎

Proof of Corollary 5.2 : If @xmath satisfies the degeneracy condition (
5.9 ), then for small times the gradient of @xmath may be estimated by
Corollary 3.3 . Letting @xmath gives that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

∎

### 5.3 Estimates for boundary value problems

In the special case of mean curvature flow the following theorem is
joint work with Ben Andrews.

###### Theorem 5.3 (Neumann problem).

Let @xmath be a convex domain with @xmath boundary, and let @xmath be a
smooth solution of

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

If @xmath and @xmath satisfy the same conditions as in Theorem 5.1 ,
then for any @xmath and @xmath in @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Furthermore if @xmath satisfies the degeneracy condition ( 5.4 ), then
for @xmath a short-time gradient bound holds:

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

where @xmath , @xmath and @xmath depend on @xmath , @xmath , @xmath ,
and @xmath .

Proof: As in the previous proof, set

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath when @xmath .

For any @xmath , suppose that @xmath is a spatial maximum of @xmath . We
will consider the possibility that @xmath and @xmath are both interior
points, that @xmath is a boundary point and so is @xmath , or that
@xmath is a boundary point while @xmath is not (the converse follows
without loss of generality).

If both @xmath and @xmath are interior points, then the arguments of
Theorem 5.1 apply and @xmath at this point.

Consider the case that @xmath is on the boundary @xmath . If we take
derivatives at @xmath that are in directions @xmath that have no
component normal to the boundary, then as before @xmath . On the other
hand, let @xmath be the outward unit normal at @xmath . The
outwards-pointing derivative of @xmath here is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we have used the boundary condition @xmath and that as @xmath is
convex, @xmath .

This inequality cannot be strict, for if it is, then there is a small
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

which would contradict that @xmath is a maximum of @xmath . Therefore

  -- -------- --
     @xmath   
  -- -------- --

and indeed @xmath .

Now consider the position of @xmath . If it is on the boundary, let
@xmath be the outward unit normal at @xmath , and so

  -- -------- --
     @xmath   
  -- -------- --

Again, this inequality cannot be strict if @xmath is to be a maximum of
@xmath , so the outward derivative @xmath . As before, other non-normal
derivatives are also zero, so @xmath .

So, when both @xmath and @xmath are boundary points, @xmath and @xmath
is negative semi-definite. We can argue as before that @xmath .

In the case that @xmath is an interior point, @xmath and so @xmath ,
@xmath is negative semi-definite here, and @xmath .

It follows that @xmath for all @xmath . ∎

The highly geometric nature of mean curvature flow allows us to relax
the conditions on the convexity of the boundary. In the following
theorem we consider domains that are merely mean-convex. This means that
at every point on the boundary, the sum of the principal curvatures of
@xmath is positive:

  -- -------- --
     @xmath   
  -- -------- --

Under the assumption of convexity (rather than mean-convexity), the
following theorem is joint work with Ben Andrews.

###### Theorem 5.4 (Dirichlet problem for mean curvature flow).

Let @xmath be a mean-convex domain with a @xmath boundary, and let
@xmath be a smooth solution of the mean curvature flow for graphs

  --------------------------------- -------- -- --
                                    @xmath      
  with prescribed boundary values               
                                    @xmath      
                                    @xmath      
  --------------------------------- -------- -- --

If @xmath is a smooth solution to curve shortening flow

  -- -------- --
     @xmath   
  -- -------- --

with boundary conditions given by ( 5.8 ), then there is an estimate

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Proof: We find a boundary gradient estimate by defining a new @xmath on
@xmath which incorporates the distance to the boundary

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a @xmath function in the neighbourhood of the boundary

  -- -------- --
     @xmath   
  -- -------- --

Here, @xmath and @xmath are the principal curvatures of @xmath . This
close to the boundary, each point @xmath has a unique closest point
@xmath .

Choose @xmath so that for @xmath , if @xmath then @xmath , and if @xmath
then @xmath .

At @xmath , @xmath for all @xmath . For @xmath on the boundary, @xmath .
For @xmath and points at least distance @xmath from the boundary, @xmath
, @xmath

We can find spatial derivatives for @xmath :

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Now suppose that @xmath is an interior maximum of @xmath at some time
@xmath . At this point, @xmath and @xmath is negative semi-definite, so

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we have used that @xmath and @xmath .

As in Lemma 14.17 of [ 16 ] ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the principal curvatures of @xmath at @xmath , the
closest point on @xmath to @xmath . If @xmath , then @xmath and

  -- -------- --
     @xmath   
  -- -------- --

the last inequality resulting from the mean-convexity of @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

It follows that @xmath for @xmath , and so for all @xmath and @xmath ,

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

This gives us an estimate on the boundary. We complete our proof by
using the same @xmath as before:

  -- -------- --
     @xmath   
  -- -------- --

Once again, @xmath , and when both @xmath and @xmath are on the
boundary, @xmath . If @xmath is a boundary point and @xmath is an
interior point (or vice-versa), then

  -- -------- --
     @xmath   
  -- -------- --

by ( 5.11 ).

Finally, if @xmath is a maximum of @xmath at some time @xmath , where
both @xmath and @xmath are interior, then as in Section 5.2 @xmath at
this point, and so @xmath for all @xmath . The estimate follows. ∎

Remark: We can use these methods to find gradient estimates for
equations of more general form.

For the Dirichlet problem with conditions on @xmath given in Theorem 5.1
, and @xmath on @xmath , we can find estimates of the type in Theorem
5.4 for convex @xmath .

If @xmath has the form ( 5.5 ), then we can find estimates of this type
on domains that are merely mean-convex.

## Chapter 6 Application of gradient estimates to the Neumann problem

In this chapter we use the gradient estimate derived previously to
establish the existence of solutions to the mean curvature flow equation
with Neumann boundary conditions

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where @xmath is a compact, open convex domain with @xmath boundary
@xmath , and @xmath . The outward unit normal on the boundary is @xmath
.

This extends Huisken’s result in [ 19 ] showing the existence of smooth
solutions to ( 6.1 ) for initial data with greater regularity.

###### Theorem 6.1 (Huisken).

Let @xmath be a bounded domain in @xmath with @xmath . If @xmath
satisfies @xmath on @xmath , then ( 6.1 ), ( 6.2 ) has a smooth solution
on @xmath .

Note that while this theorem makes no restriction on the convexity of
@xmath , the main result of this chapter does.

###### Theorem 6.2.

Let @xmath be a smoothly bounded, open, convex domain, and let @xmath .
Then the Neumann problem ( 6.1 ) has a smooth solution for @xmath ,
which converges uniformly to @xmath as @xmath , at a rate dependent on
the modulus of continuity of @xmath .

### 6.1 Some remarks about changes of coordinates that straighten
boundaries

A similar discussion of boundary curvatures and the distance function
may be found in Appendix 14.6 of [ 16 ] . Consider a bounded domain
@xmath with boundary @xmath . The boundary is said to be @xmath if for
each boundary point @xmath we can find a @xmath mapping @xmath which has
the boundary in a neighbourhood of @xmath as its graph.

Set

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

If @xmath is convex, then we can take

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the principal curvatures of @xmath .

On balls @xmath centred on the boundary, we introduce a change of
coordinates @xmath such that if the new coordinates are denoted @xmath ,
@xmath , and @xmath is the immersion of the boundary into @xmath , then
@xmath . In other words, @xmath is the closest point to @xmath on @xmath
and @xmath is the signed distance between @xmath and @xmath , being
positive if @xmath , zero if @xmath , and negative otherwise.

The inverse transformation is easier to work with, being given by @xmath
, where @xmath is the outward-pointing unit normal to @xmath .

As the boundary in the new coordinates is simply @xmath , this is
referred to as a boundary-straightening transformation.

[]

If the graph @xmath is a local immersion of the boundary, then the
outward unit normal is given by

  -- -------- --
     @xmath   
  -- -------- --

and on @xmath we have

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (6.4)
  -- -------- -------- -- -------

so eigenvalues for @xmath are @xmath , and lie between @xmath and @xmath
on @xmath . The curvatures of the boundary, @xmath , are given by the
eigenvalues of @xmath .

Also, second derivatives are

  -- -------- --
     @xmath   
  -- -------- --

The smoothness of this change of coordinates is dependent on the
smoothness of the boundary: if @xmath is @xmath then @xmath is @xmath .

When @xmath is defined in the old coordinates on @xmath , in the new
coordinates we can define a new function

  -- -------- --
     @xmath   
  -- -------- --

First derivatives are related by @xmath , and second derivatives by
@xmath .

Putting this all together, we notice that if @xmath satisfies ( 6.1 )
then @xmath satisfies

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (6.5)
  -- -------- -------- -- -------

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

where the mean curvature operator is abbreviated as @xmath , and we
write

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

Once we have straightened out the boundary, we will find it useful later
on to define a reflection @xmath in the boundary that extends @xmath
outside @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Let @xmath be the intersection of a parabolic cylinder with the domain
of interest:

  -- -------- --
     @xmath   
  -- -------- --

When @xmath satisfies ( 6.5 ) on @xmath , @xmath will satisfy

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (6.8)
  -- -------- -------- -- -------

on @xmath where @xmath .

The regularity of the coefficients of the reflected equation is
estimated:

###### Lemma 6.3.

If @xmath is a @xmath function on @xmath , with @xmath , then the
coefficients for the reflected equation

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

satisfy Hölder estimates

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

for some @xmath .

Proof: In general, if a function is defined piecewise on a convex domain
@xmath divided into @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and is continuous across any shared boundary @xmath then if @xmath is
@xmath on @xmath and @xmath is @xmath on @xmath , it follows that @xmath
is @xmath on @xmath .

We can see this by letting @xmath and @xmath be in @xmath and @xmath
respectively. We can find a point @xmath in the shared boundary @xmath
directly between the two, with @xmath .

Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

for @xmath .

This observation applies to both @xmath and @xmath — as @xmath on the
boundary, @xmath is continuous across the boundary, even though @xmath
itself is not — and so @xmath is @xmath .

It is clear that @xmath is continuous, and @xmath shares the same
regularity as @xmath .

As

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

we only need to check whether the terms in the off-diagonal block @xmath
are continuous. These are given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

(here there is no summation over @xmath ).

Between the third and the fourth line, we have used that @xmath is
tangent to the boundary while @xmath is normal to the boundary (see
equation ( 6.4 )), so for @xmath , we have @xmath . It follows that
@xmath .

In the last step, the second, third and fourth terms are zero (and so
continuous) on the boundary, as @xmath . The first term is zero due to
the presence of

  -- -------- --
     @xmath   
  -- -------- --

since @xmath .

So, both @xmath and @xmath are continuous across the boundary.

It follows from the first observation that the Hölder constant of @xmath
on @xmath is the same as that of @xmath on @xmath ; and if we consider
@xmath as a function of @xmath , then we find that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Similarly, if we consider @xmath as a function of @xmath , we find that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

∎

### 6.2 Existence of solutions with continuous initial data and Neumann
boundary conditions

We begin our proof of Theorem 6.2 by approximating the continuous
initial data by mollified functions that will satisfy the requirements
of Theorem 6.1 , being smooth and satisfying the Neumann boundary
condition.

###### Lemma 6.4.

There exists an approximating sequence @xmath with @xmath in @xmath ,
@xmath on @xmath , and @xmath whenever @xmath .

Proof: Let @xmath be a ball centred on the boundary. We work in the new
coordinates on @xmath , and write @xmath .

Remembering that @xmath denotes the extension by reflection of @xmath ,
define the mollified function

  -- -------- --
     @xmath   
  -- -------- --

where we use the usual mollifier

  -- -------- --
     @xmath   
  -- -------- --

and @xmath .

This approximation has all the usual qualities of mollifications: @xmath
, where @xmath ; and since @xmath , @xmath uniformly on compact subsets
of @xmath .

In addition, each @xmath satisfies the Neumann condition @xmath for
@xmath , since

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Recalling the relationship between the reflected and original functions,
@xmath , we observe that when @xmath , @xmath and @xmath . Consequently,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

as the mollifier has the symmetry @xmath .

This is only a local approximation, but in the next step we extend it to
the entire domain, taking care to preserve the Neumann boundary
condition.

Let the set of boundary-centred balls @xmath be a finite cover of the
boundary @xmath with the property that the set of balls of half the
radius @xmath is also a cover. On each ball @xmath we can define the
approximation @xmath as described above.

Now, define a new cover of @xmath by the sets

  -- -------- --
     @xmath   
  -- -------- --

The cover is completed by @xmath .

Note that @xmath and so this is indeed a cover; also, @xmath is defined
on @xmath . On @xmath we define the usual mollification with no
reflection, which we call @xmath .

Let @xmath be a partition of unity with respect to the sets @xmath which
cover the boundary; that is, @xmath for @xmath , @xmath (that is,
compactly supported with respect to @xmath ), and @xmath for all @xmath
.

In the new coordinates on @xmath , we could write @xmath , since @xmath
is defined only on the boundary. We can extend @xmath to all of @xmath
by setting @xmath .

Let @xmath be a smooth cut-off function satisfying

  -- -------- --
     @xmath   
  -- -------- --

We will set @xmath where @xmath is the signed distance function.

Now, we claim that the functions @xmath and @xmath are a partition of
unity with respect to the sets @xmath and @xmath . Firstly, all
functions are smooth and compactly supported on their respective domains
(but they are not zero on the external boundary @xmath ). Secondly, if
@xmath , then

  -- -------- --
     @xmath   
  -- -------- --

This is because if @xmath then @xmath , while if @xmath then @xmath has
a unique closest boundary point @xmath . In the latter case @xmath and
@xmath as @xmath is a partition of unity on the boundary.

This construction ensures that @xmath when @xmath and so if we define
our global approximation as

  -- -------- --
     @xmath   
  -- -------- --

we find that @xmath uniformly in @xmath , and each @xmath satisfies
@xmath on @xmath .

We can ensure that this sequence is monotone in @xmath , in the sense
that @xmath whenever @xmath by restricting to a subsequence and
off-setting if necessary. ∎

The result of Huisken mentioned at the start of this chapter now implies
that there is a smooth solution @xmath to ( 6.1 ) with @xmath .

###### Lemma 6.5.

The approximate solutions @xmath have a uniform height bound

  -- -------- --
     @xmath   
  -- -------- --

Proof: As the mollification @xmath is created by a local averaging of
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Suppose at some time @xmath and point @xmath , @xmath equals @xmath .
From the Comparison Principle (Theorem 2.2 ), @xmath can be assumed to
be a boundary point. The Neumann condition @xmath implies that @xmath
and so @xmath is negative semi-definite at this point; it follows that
@xmath and so @xmath is not increasing at this point. ∎

This height estimate is of course also an oscillation bound

  -- -------- --
     @xmath   
  -- -------- --

We are now in a position to use the gradient estimate of Theorem 5.3 .
For some @xmath , there is a gradient bound

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

where @xmath and @xmath are dependent on @xmath and @xmath .

###### Lemma 6.6.

Higher derivatives of @xmath are uniformly bounded on the interior, with

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

for @xmath and all @xmath , where @xmath is the interior set @xmath .

Proof: This is an application of the Ecker-Huisken interior curvature
estimate described in Theorem 2.6 , originally in the paper [ 14 ] .

We apply it to the interior of @xmath (with @xmath and @xmath ) to find
bounds on all higher derivatives.∎

This estimate provides no information as we approach the boundary.
However, our uniform gradient bound @xmath ensures that the evolution
equation is uniformly parabolic, since for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

As we have uniform parabolicity for strictly positive times, extending
regularity up to the boundary is a routine application of known results.
This is the subject of Lemma 6.7 – Lemma 6.9 .

We begin by showing that a function with a Hölder estimate on the
boundary of a region, and a strictly interior gradient estimate, has a
global Hölder estimate. We plan to apply this to finding a Hölder
estimate for the gradient @xmath .

###### Lemma 6.7.

Let @xmath be a convex domain. If @xmath has a Hölder oscillation bound
on the boundary

  -------------------------------------------------------------------------------- -------- -- --
                                                                                   @xmath      
  where @xmath is non-increasing in @xmath ; and gradient bounds on the interior               
                                                                                   @xmath      
  and                                                                                          
                                                                                   @xmath      
  -------------------------------------------------------------------------------- -------- -- --

then we can find an @xmath such that for all @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends on @xmath , @xmath , @xmath and @xmath , and @xmath
depends on @xmath .

Proof: We split the difference in the obvious way

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

and look at the first term.

Without loss of generality, set @xmath , and @xmath to be the closest
point to @xmath in @xmath , so that @xmath .

If we are close to the boundary, so that @xmath , then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we have used that @xmath , and have set @xmath .

If we are further from the boundary, so that @xmath , then for some
@xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

setting @xmath so that @xmath , and @xmath .

Now consider the second term of ( 6.11 ), and suppose without loss of
generality that @xmath . As before, set @xmath and @xmath .

If we are close to the boundary, so that @xmath , then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Otherwise, if @xmath , then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath so that @xmath , and @xmath .

We find the final estimate by choosing @xmath and @xmath ∎

###### Lemma 6.8.

The gradient @xmath is Hölder continuous, with bound

  -- -------- --
     @xmath   
  -- -------- --

on @xmath , for @xmath and some @xmath , where @xmath .

Proof: We can use the Hölder gradient estimate near a flat boundary from
Theorem A.1 , but we will need to work locally with @xmath in the
flat-boundary coordinates.

The gradient bound ( 6.9 ) for @xmath implies that

  -- -------- --
     @xmath   
  -- -------- --

on the cylinder @xmath for some @xmath to be chosen later, and where
@xmath and @xmath .

On @xmath , @xmath satisfies the evolution equation ( 6.5 ). To check
that the coefficients @xmath and @xmath (given by ( 6.7 )) satisfy the
conditions of Theorem A.1 , we note that: @xmath is uniformly parabolic,
since for @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath and @xmath are the smallest and largest eigenvalues of
@xmath , which are bounded between @xmath and @xmath on @xmath ; @xmath
is bounded above, as

  -- -------- --
     @xmath   
  -- -------- --

and @xmath has bounded derivative with respect to the gradient, for if
we write @xmath , then @xmath and

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

It is straightforward to bound the lower-order term in the equation —

  -- -------- --
     @xmath   
  -- -------- --

Finally, we need an oscillation bound smaller than @xmath for @xmath on
@xmath , but since

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

we find that @xmath . By choosing @xmath small enough, we can ensure
that this is less than @xmath .

Now Theorem A.1 implies that for all @xmath there is some @xmath so that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where @xmath .

This boundary oscillation estimate for @xmath on @xmath for all @xmath
and @xmath , together with the interior gradient bounds given by Lemma
6.6 , means we can use Lemma 6.7 to give a global Hölder bound for
@xmath and hence for @xmath . ∎

###### Lemma 6.9.

We can find bounds for @xmath in @xmath

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath .

Proof: To establish this, it is possible to use boundary estimates for
the Neumann problem, but instead our approach is to use the reflection
@xmath on @xmath — a domain that extends beyond @xmath — which satisfies
the reflected evolution equation ( 6.1 ), and apply the interior
estimate from Theorem A.4 .

We need to check that equation ( 6.1 ) satisfies the conditions of
Theorem A.4 .

In Lemma 6.3 , we showed that the coefficients in equation ( 6.1 ) have
regularity estimates

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

and in Lemma 6.8 we found a uniform global bound for @xmath for @xmath .

Our gradient estimate ensures that @xmath is uniformly parabolic for
@xmath .

Applying Theorem A.4 results in the bound

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is dependent on the dimension @xmath , @xmath (which is
determined by @xmath ), the ellipticity constant of @xmath (dependent on
@xmath ), the Hölder exponent @xmath , and the bound on the @xmath norm
of the coefficients (which is bounded by @xmath ). That is, @xmath .

We can repeat this over the entire boundary, and together with the
interior estimate ( 6.10 ), this gives us the claim. ∎

###### Lemma 6.10.

The sequence of approximate solutions @xmath converges

  -- -------- --
     @xmath   
  -- -------- --

to some @xmath on @xmath , for all @xmath .

Proof: The uniform @xmath bounds on the @xmath ensure that there is a
convergent subsequence (for a slightly smaller @xmath ); the
disjointness of initial data @xmath (and hence the disjointness of
@xmath ) implies that the entire sequence must converge, and so this
limit is unique. The limit @xmath is in @xmath (and is @xmath on the
interior, by virtue of the interior estimate in Lemma 6.6 ).

It also satisfies @xmath on the boundary, and so is a solution to the
Neumann problem given by ( 6.1 ).∎

###### Lemma 6.11.

This solution @xmath converges to @xmath in @xmath as @xmath . The
convergence is uniform in time, and if @xmath then @xmath .

Proof: Let @xmath be fixed. Our aim is to show that we can find @xmath
so that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The modulus of continuity for @xmath is

  -- -------- --
     @xmath   
  -- -------- --

Recall the approximate solutions @xmath , converging uniformly

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . The approximations at the initial time have (at least)
the same modulus of continuity as @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Fix @xmath to be any point in the interior of @xmath . We can define a
new solution to ( 6.1 ) by off-setting @xmath around @xmath :

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath .

Then, for @xmath , we have

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (6.12)
  -- -------- -------- -- --------

To estimate the first term of this, we observe that every @xmath is
inside the ‘envelope’ given by the continuity condition,

  -- -------- --
     @xmath   
  -- -------- --

Above and below this envelope, we can place two spheres of radius @xmath
centred at @xmath . At @xmath , the spheres and the graph of @xmath are
completely disjoint. The spheres are also completely disjoint from the
graph of @xmath .

The evolution of spheres under mean curvature flow is well-known — the
centre remains fixed and the radius shrinks from the initial radius
@xmath , with

  -- -------- --
     @xmath   
  -- -------- --

until the sphere disappears at time @xmath .

The parts of these spheres closest to the graph of @xmath — the lower
part of the upper sphere and the upper part of the lower sphere — are

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Suppose that one of these spheres and @xmath first touch at some time
@xmath . From the Comparison Theorem 2.2 , we know that at this time
there must be an intersection occurring on the boundary of @xmath , say
at @xmath (this doesn’t rule out other intersections occurring
simultaneously on the interior).

This intersection on the boundary is an extreme point of @xmath (either
a minimum of @xmath or a maximum of @xmath ).

Therefore the sign on the outward derivative of the intersecting sphere
at this point is known — either

  -- -------- --
     @xmath   
  -- -------- --

or else

  -- -------- --
     @xmath   
  -- -------- --

where we have used that @xmath satisfies the Neumann condition @xmath on
the boundary.

On the other hand, we can explicitly calculate the gradients of the
spheres —

  -- -------- --
     @xmath   
  -- -------- --

The convexity of @xmath means that for any @xmath and @xmath on the
boundary, @xmath , and the inequality is strict if @xmath is in the
interior of @xmath . Hence the sign of the normal derivative is known —

  -- -------- --
     @xmath   
  -- -------- --

Between these two observations, it must be the case that the
intersecting sphere has a flat normal gradient —

  -- -------- --
     @xmath   
  -- -------- --

and so @xmath , which in turn implies that @xmath is on the boundary of
@xmath , contradicting our original assumption that @xmath was an
interior point. It follows that such spheres, centred on interior
points, never touch the graph of @xmath for the duration of the spheres’
existence, until @xmath .

In particular, above the point @xmath the surfaces move by no more than
@xmath in the time @xmath . We can choose @xmath so that @xmath , and a
corresponding @xmath , ensuring that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is dependent on @xmath and @xmath alone.

This estimate is independent of @xmath and @xmath , so

  -- -------- --
     @xmath   
  -- -------- --

We still need to estimate the second part of equation ( 6.12 ), @xmath .
However the convergence here is uniform (for @xmath ), so that

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , and so @xmath is in @xmath , with @xmath .

This is also an estimate for the smoothness in time of the convergence;
we can consider @xmath , with

  -- -------- --
     @xmath   
  -- -------- --

by setting @xmath , so that @xmath and thus @xmath . The convergence in
time that this gives is at best like @xmath — which is in concordance
with the result for initial data with a Lipschitz bound. In that case,
@xmath and the convergence is @xmath in time (Theorem 3.5 of [ 20 ] ).

In the case that the initial data has a Hölder gradient bound, @xmath ,
then the convergence to the initial data is as @xmath . ∎

Remark: While we have mined the rich theory arising from mean curvature
flow to find this result, there are similar results for other equations
of the type studied in Chapter 5 , and we expect to be able to find
similar existence results.

In particular, one can find short-time existence results for anisotropic
mean curvature flow with a zero Neumann boundary condition and
continuous initial data on @xmath , for convex @xmath .

## Chapter 7 Existence of solutions to the Dirichlet problem for mean
curvature flow

In this chapter we use the gradient estimate to establish existence of
solutions to the Cauchy-Dirichlet problem with zero boundary data.

###### Theorem 7.1 (Existence of solutions to the Dirichlet problem).

Let @xmath be a domain in @xmath , with @xmath boundary @xmath that has
non-negative mean curvature. If @xmath and @xmath on @xmath , then the
problem

  -- -------- -------- -- -------
     @xmath   @xmath      (7.1)
     @xmath   @xmath      
     @xmath   @xmath      
  -- -------- -------- -- -------

has a smooth solution for @xmath which converges uniformly to @xmath as
@xmath .

The existence of solutions to the mean curvature flow problem with
prescribed boundary values was considered by Lieberman in [ 24 ] (and by
Huisken in [ 19 ] , where the long-time behaviour of solutions was also
studied).

Lieberman considered time-dependent boundary data @xmath , with a
Lipschitz bound (in time) on @xmath . The following theorem may be found
as Theorems 12.10 and 12.18 of [ 25 ] .

###### Theorem 7.2 (Lieberman).

Let @xmath be a domain with @xmath boundary, and let @xmath be a
function defined on the boundary, with @xmath for some @xmath . Then if
the mean curvature @xmath of @xmath is non-negative, there exists a
solution to ( 7.1 ) with initial and boundary data

  -- -------- --
     @xmath   
  -- -------- --

Moreover, such a solution satisfies

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath depend on @xmath .

The proof of Theorem 7.1 is very similar to that for the Neumann
problem, Theorem 6.2 . We will use the boundary-straightening change of
coordinates described in Section 6.1 , and the corresponding @xmath

###### Lemma 7.3.

There exists an approximating sequence @xmath with @xmath on @xmath and
@xmath in @xmath .

Proof: Let @xmath be a boundary centred ball, where @xmath is given by (
6.3 ). We will define a local approximation on @xmath and then put
similar local approximations together to give a global one.

In Section 6.1 we defined @xmath to be a reflection across the boundary;
this time, we let @xmath be the odd reflection over the boundary

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

Mollifying this in the standard way

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

we note that @xmath uniformly on subsets of @xmath , and we can check
that if @xmath , then @xmath . Returning to the original coordinates,
set @xmath on @xmath .

Now, cover @xmath by @xmath such boundary-centred balls @xmath on which
are defined approximations @xmath . Complete the cover of @xmath by the
set @xmath On this interior set let @xmath be the usual mollification of
@xmath .

If @xmath is a partition of unity with respect to these sets, then the
sum @xmath converges uniformly to @xmath on @xmath and also has @xmath
on @xmath .

We can restrict this to a subsequence @xmath , where @xmath (but retain
the notation @xmath for the subsequence). If we off-set each member of
the subsequence, by replacing @xmath by @xmath , then this is a
completely disjoint subsequence that still converges to @xmath as @xmath
. ∎

As the boundary values

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

are in @xmath on @xmath , Theorem 7.2 ensures that a solution with these
boundary values for ( 7.1 ) exists. Denote these approximate solutions
by @xmath .

The gradient estimate derived for the Dirichlet problem in Theorem 5.4
implies that

  -- -------- --
     @xmath   
  -- -------- --

for constants @xmath and @xmath dependent only on @xmath .

###### Lemma 7.4.

The approximate solutions @xmath have a Hölder gradient bound

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath is dependent on @xmath , @xmath , @xmath ,
@xmath , and @xmath .

Proof: If we revert to @xmath in the straightened-boundary coordinates
satisfying ( 6.5 ) on @xmath , Theorem A.2 gives an oscillation bound
for the gradient on the boundary-centred cylinder @xmath for @xmath and
@xmath —

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath depend on @xmath , @xmath , @xmath , @xmath and
@xmath , where @xmath and @xmath are the coefficients of ( 6.5 ), as in
( 6.7 ). These last four are in turn dependent on @xmath and @xmath .

On the interior we have the bounds on higher derivatives given by Lemma
6.6

  -- -------- -- -------
     @xmath      (7.4)
  -- -------- -- -------

With @xmath , this is a gradient bound for @xmath .

These can be linked together using Lemma 6.7 to find a global Hölder
bound

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath additionally depends on @xmath and @xmath . ∎

Now, if we consider the approximate solutions to begin at some time
@xmath with the initial data @xmath , the uniform Hölder gradient bound
on @xmath means that Theorem 7.2 gives a Hölder gradient bound

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , where @xmath and @xmath depend on @xmath , @xmath ,
@xmath , @xmath , and @xmath .

With these uniform estimates for positive times, there must be a
convergence subsequence in @xmath for some @xmath . If we off-set the
initial data as mentioned in Lemma 7.3 , the convergence of the
subsequence implies the convergence of the entire sequence to a limit
@xmath , defined on all @xmath for @xmath . Finally, the interior bounds
( 7.4 ) mean that on the interior, @xmath is smooth.

We now need to show that @xmath as @xmath .

###### Lemma 7.5.

As @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, @xmath has a modulus of continuity in time dependent only
on that of @xmath .

Proof: As in the proof of Lemma 6.11 , fix @xmath to be any point in the
interior of @xmath and set

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath is a solution to ( 7.1 ) with @xmath and @xmath on the
boundary.

Let @xmath be a modulus of continuity for @xmath and hence for @xmath ,
so that

  -- -------- --
     @xmath   
  -- -------- --

Above and below these two bounds, we can place two spheres of radius
@xmath centred at @xmath . At @xmath , the spheres and the graph of
@xmath are completely disjoint. The spheres are also completely disjoint
from the graph of @xmath .

As they evolve under mean curvature, the parts of these spheres closest
to the graph of @xmath — the lower part of the upper sphere and the
upper part of the lower sphere — are

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Initially @xmath is positive: suppose that @xmath is the first time that
@xmath decreases to zero. The comparison principle (Theorem 2.2 ) means
that this occurs at some point @xmath on the boundary. On the boundary,

  -- -------- --
     @xmath   
  -- -------- --

(since @xmath is constant on the boundary) and so this cannot be the
first zero point; it follows that @xmath for the duration of the
sphere’s existence.

The same argument shows that @xmath .

We can conclude that the @xmath move at most by @xmath in the time
@xmath .

This estimate is independent of @xmath and @xmath , so this implies that
@xmath and that @xmath . Furthermore, if @xmath , then @xmath . ∎

## Chapter 8 Gradient estimates found by counting intersections

In the paper [ 5 ] , Angenent proved a series of results regarding the
finiteness and non-proliferation of the zeroes of a parabolic equation
in one space dimension.

A zero of @xmath is simply a point @xmath where @xmath . A multiple zero
is a point where both @xmath and @xmath vanish. In contrast to earlier
results, Angenent did not exclude multiple zeroes from the zero set,
defining the zero set as

  -- -------- --
     @xmath   
  -- -------- --

In the following, @xmath is often used as shorthand for the counting
measure @xmath .

These zero-counting results have been influential in many different
areas, and have been used for geometric flows by Angenent himself, in [
6 ] , [ 8 ] , [ 7 ] , and [ 2 ] , the last with Altshuler and Giga. Many
others working in the area have also used these results.

Unlike approaches that depend more explicitly on the maximum principle,
this technique seems limited to equations in one dimension. The gradient
estimates found do not depend on the initial gradient, but do depend
explicitly on the height: the smallest gradient estimates are found for
when the height is largest.

This work originates in an idea of Ben Andrews; also, this approach to
finding gradient estimates has been independently used by Nagase and
Tonegawa in the forthcoming paper [ 26 ] .

### 8.1 Counting zeroes

The estimates in this chapter rely on Theorem D of Angenent’s paper:

###### Theorem 8.1 (Angenent).

Let @xmath be a solution of

  -- -------- --
     @xmath   
  -- -------- --

such that there are no zeroes on the boundaries

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath , @xmath , @xmath satisfy

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Then if @xmath , @xmath and @xmath are continuous on @xmath ,

-    for @xmath , @xmath is finite

-    if @xmath is a multiple zero of @xmath at @xmath then for all
    @xmath we have @xmath .

Consider a fully nonlinear equation on a domain @xmath , where @xmath is
a connected subset of @xmath ,

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

where @xmath is parabolic, by which we mean that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath .

Suppose that @xmath and @xmath are smooth solutions of ( 8.1 ), with

  ----- -------- -- --
        @xmath      
  and               
        @xmath      
  ----- -------- -- --

Then we can form the difference @xmath satisfying the evolution equation

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (8.2)
  -- -------- -------- -- -------

where the omitted argument of the derivatives of @xmath , denoted by
@xmath , is always @xmath . In the last line,

  ----- -------- -- -------
        @xmath      (8.3)
        @xmath      (8.4)
  and               
        @xmath      (8.5)
  ----- -------- -- -------

In order to use Angenent’s theorem, we need to establish that:

-   @xmath , @xmath , @xmath , @xmath , and @xmath are bounded,

-   @xmath , @xmath and @xmath are bounded

-   and @xmath is bounded on @xmath .

Let @xmath .

If @xmath is continuous, then there are positive constants @xmath and
@xmath for which

  -- -------- -- -------
     @xmath      (8.6)
  -- -------- -- -------

Bounds on @xmath and @xmath follow from this:

  -- -------- -- -------
     @xmath      (8.7)
  -- -------- -- -------

@xmath is given by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and so

  -- -------- -- -------
     @xmath      (8.8)
  -- -------- -- -------

Similarly,

  -- -------- --
     @xmath   
  -- -------- --

so

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

@xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are combinations of terms involving second and
first derivatives of @xmath respectively. Consequently,

  -- -------- -- --------
     @xmath      (8.10)
  -- -------- -- --------

The bounds for @xmath , its derivatives, and @xmath follow in a similar
manner:

  -- -------- --
     @xmath   
  -- -------- --

so that

  -- -------- -- --------
     @xmath      (8.11)
  -- -------- -- --------

while

  -- -------- --
     @xmath   
  -- -------- --

so that

  -- -------- -- --------
     @xmath      (8.12)
  -- -------- -- --------

and finally

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (8.13)
  -- -------- -------- -- --------

It is clear that we will be able to apply Angenent’s result to a smooth
solution of a nonlinear parabolic equation @xmath , when @xmath
satisfies the parabolicity condition ( 8.6 ) and both @xmath and @xmath
are @xmath on the bounded domain @xmath .

These conditions are not optimal — for example, in estimate ( 8.13 )
above, it is sufficient if @xmath is @xmath along line segments in
@xmath — however, they are enough to allow a theorem for intersections
of two solutions rather than zeroes of one solution.

###### Theorem 8.2 (Intersection-counting theorem).

Let @xmath and @xmath be solutions of

  -- -------- --
     @xmath   
  -- -------- --

which do not intersect on the boundaries

  -- -------- --
     @xmath   
  -- -------- --

If @xmath and @xmath are @xmath on @xmath

  -- -------- --
     @xmath   
  -- -------- --

and if @xmath is parabolic

  -- -------- --
     @xmath   
  -- -------- --

and if both @xmath and @xmath are @xmath on

  -- -------- --
     @xmath   
  -- -------- --

then for @xmath the number of intersections of @xmath and @xmath are
finite; and if @xmath is an intersection of @xmath and @xmath at @xmath
then for all @xmath , the number of intersections at @xmath is strictly
less than the number of intersections at @xmath .

Proof: We apply Theorem ( 8.1 ) to the difference @xmath which satisfies
equation ( 8.2 ). ∎

### 8.2 Gradient estimates for equations in one space dimension

In this chapter we seek interior estimates for bounded solutions of
parabolic equations on connected domains @xmath . When two functions
intersect at a single point, then the gradient of the one that is
smaller on the left of the intersection will dominate the gradient of
the one that is smaller on the right.

The main idea is that optimal regularity of @xmath is found by
comparison to the solution of the same parabolic equation with initial
data @xmath , where @xmath is the maximal monotone graph

  -- -------- -- --------
     @xmath      (8.14)
  -- -------- -- --------

\nomenclature

[sig] @xmath the step “function” \refpage which we will refer to as the
step “function” .

The method can be broken into the following steps:

-   Creation of family of barriers @xmath with @xmath approaching @xmath
    as @xmath

-   Show that for all @xmath in a subdomain of @xmath , and for all
    @xmath , we can find an @xmath such that @xmath

-   Show that @xmath at the boundaries of @xmath

-   Then use the Angenent result to count the intersections of @xmath
    and @xmath . For small enough @xmath , there will be only one
    intersection, and so a gradient bound will follow.

We begin by looking at a simple estimate for entire solutions on @xmath
, then find more specific estimates, firstly for the heat equation

  -- -------- -- --------
     @xmath      (8.15)
  -- -------- -- --------

and then for a nonlinear problem.

The following theorem says that if a solution with the step function as
initial condition exists, then it will serve as a barrier for other
solutions.

###### Theorem 8.3.

Consider the parabolic equation

  -- -------- -- --------
     @xmath      (8.16)
  -- -------- -- --------

where @xmath satisfies ( 8.6 ). Let @xmath be a solution of ( 8.16 ) on
@xmath that has a bound

  -- -------- --
     @xmath   
  -- -------- --

and has a uniform gradient bound at @xmath .

Suppose there exists a solution to ( 8.16 ) on @xmath which is smooth
for @xmath , and has initial condition

  -- -------- --
     @xmath   
  -- -------- --

and boundary condition

  -- -------- --
     @xmath   
  -- -------- --

Then there is a gradient estimate

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is chosen so that @xmath .

Proof: Let a family of barriers indexed by @xmath be given by @xmath for
all @xmath and @xmath . Each of these satisfies ( 8.16 ), and is smooth
on @xmath .

As @xmath has a uniform gradient bound, there exists a @xmath such that
not only do @xmath and @xmath intersect only once, but also, @xmath and
@xmath intersect only once for all @xmath .

Let @xmath be fixed. For each @xmath , there exists @xmath such that
@xmath .

Now, apply Angenent’s theorem to @xmath on some region @xmath containing
@xmath and which is sufficiently large enough that for all @xmath ,
@xmath and @xmath . The last conditions ensure that @xmath has no zeroes
on the boundary.

As @xmath has only one zero at @xmath , it has no more than one zero for
all @xmath ; as @xmath is positive at @xmath and negative at @xmath , it
has exactly one zero for all @xmath . In particular the zero at @xmath
is the only zero, and

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

from which we find a gradient estimate:

  -- -------- --
     @xmath   
  -- -------- --

This holds for all @xmath and so letting @xmath gives the result. ∎

The following theorem describes an explicit barrier in the case of the
heat equation.

###### Theorem 8.4 (Gradient estimate for the heat equation).

Let @xmath and @xmath be a smooth solution to the heat equation ( 8.15 )
with a height bound @xmath and Lipschitz bound @xmath .

Then for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

This leads to an estimate for an entire solution.

###### Corollary 8.5.

Let @xmath be a smooth solution of the heat equation ( 8.15 ) with
@xmath .

Then for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: Theorem 8.4 applies on any interval @xmath ; let @xmath to find
the result. ∎

Proof of Theorem 8.4 : Without loss of generality, let @xmath .

For some @xmath , @xmath and @xmath to be chosen later, we can define
the barrier

  -- -------- --
     @xmath   
  -- -------- --

which satisfies the heat equation ( 8.15 ) on @xmath . As @xmath ,
@xmath , where @xmath is the step function ( 8.14 ).

Choose @xmath small enough, such that for any @xmath and @xmath there is
only one intersection of @xmath and @xmath . This is possible as @xmath
has a uniform gradient bound.

Now, let @xmath be fixed, and consider @xmath The bound on @xmath
implies that @xmath .

If we choose @xmath , then @xmath . Also choose

  -- -------- -- --------
     @xmath      (8.17)
  -- -------- -- --------

With these choices, we can check that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and that on the boundaries @xmath whenever @xmath , since

  -------------------------------------------------- -------- -------- -- --
                                                     @xmath   @xmath      
                                                              @xmath      
                                                              @xmath      
                                                              @xmath      
  and if we use ( 8.17 ) for @xmath , then this is                        
                                                              @xmath      
                                                              @xmath      
  -------------------------------------------------- -------- -------- -- --

We can now apply the intersection counting Theorem 8.2 with @xmath ,
with the previous calculation ensuring that there are no intersections
on the boundary, and with the coefficients in equation ( 8.2 ) given by
@xmath , @xmath and @xmath . Since there is only one intersection at the
initial time, there is never more than one intersection at later times
(in particular, for our given @xmath , there is no other intersection at
time @xmath than the one at @xmath ).

It follows that

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

from which we find a gradient estimate:

  -- -------- --
     @xmath   
  -- -------- --

This holds for any smaller @xmath , so letting @xmath gives the final
result. ∎

This method applies to all parabolic operators for which we can find
solutions that have the step function as the initial condition.

When a quasilinear parabolic equation satisfies the conditions of
Section 4.4 , we can use the solutions with stepped initial data, whose
existence was shown in that chapter.

Let @xmath be in @xmath for all bounded @xmath , and some @xmath . This
implies that for every such @xmath we can find positive @xmath and
@xmath such that @xmath

Since we will be looking at bounded solutions in @xmath , the bound on
the gradient is the pertinent bound on @xmath ; we highlight this by
writing:

  -- -------- -- --------
     @xmath      (8.18)
  -- -------- -- --------

where we assume that @xmath , @xmath and @xmath .

Also, suppose that there are positive constants @xmath and @xmath such
that

  -- -------- -- --------
     @xmath      (8.19)
  -- -------- -- --------

###### Theorem 8.6.

Let @xmath be a smooth solution to

  -- -------- -- --------
     @xmath      (8.20)
  -- -------- -- --------

where @xmath satisfies ( 8.18 ) and ( 8.19 ).

Let @xmath be bounded, @xmath .

Let @xmath solve ( 8.20 ) on @xmath , with @xmath as @xmath , where
@xmath is the step function ( 8.14 ), and where @xmath is chosen so that
@xmath .

If @xmath , where @xmath is given by ( 8.18 ), then

  -- -------- --
     @xmath   
  -- -------- --

That is, the gradient of @xmath is bounded by the gradient of the
barriers, at the same height.

Remark 1: We can replace @xmath by @xmath here, in which case we find
that

  -- -------- --
     @xmath   
  -- -------- --

Remark 2: If @xmath has polynomial growth, so that @xmath for some
@xmath , then the interior region on which we can find bounds of this
form is given by @xmath , for some constant @xmath .

Remark 3: If @xmath in Theorem 8.6 , then the gradient bound applies for
all @xmath .

Proof of Theorem 8.6 :

We initially assume that @xmath . We will derive a gradient bound at a
single point @xmath , and then generalize it to interior points on a
general domain.

As @xmath is smooth there are bounds on the first derivative and on
higher derivatives

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , let @xmath be the standard mollification of @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the step function ( 8.14 ).

Choose @xmath small enough so that for all @xmath , @xmath satisfies a
gradient estimate from below:

  -- -------- -- --------
     @xmath      (8.21)
  -- -------- -- --------

Now, for fixed @xmath , define a family @xmath of barriers, each of
which solves ( 8.20 ) on @xmath (for some @xmath to be decided later)
with initial condition

  -- -------- --
     @xmath   
  -- -------- --

The existence of such solutions follows from Corollary 4.20 , which
applies as @xmath satisfies ( 8.19 ).

Standard results give

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

To avoid intersections of @xmath and the barriers occurring on the
boundary, we need to show that @xmath when @xmath .

Each barrier in the family is initially bounded above by a step function

  -- -------- --
     @xmath   
  -- -------- --

and so Corollary 4.11 provides an estimate for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

In particular, at @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

As @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

If we choose @xmath where @xmath , then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

whenever @xmath .

A similar calculation for the other boundary point @xmath gives that

  -- -------- --
     @xmath   
  -- -------- --

when @xmath .

Let @xmath be fixed.

For each @xmath , we can define @xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

on @xmath . Here, @xmath is given by setting @xmath in ( 8.3 ), @xmath
by setting @xmath in ( 8.4 ), and @xmath by setting @xmath in ( 8.5 ).

Let @xmath

@xmath , and @xmath are bounded by constants dependent on @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath and @xmath , as in ( 8.7 )–(
8.13 ).

Since @xmath , and at the boundary @xmath , @xmath is never zero on the
boundary.

In particular, @xmath and @xmath , so there is always at least one zero
of @xmath . The lower gradient bound ( 8.21 ) implies that there is at
most one zero of @xmath at the initial time.

Then the intersection counting theorem (Theorem 8.2 ) implies there is
exactly one zero of @xmath for all @xmath .

In the following lemma we show that given @xmath — or more specifically,
@xmath — we can find @xmath such that @xmath is a zero of @xmath . We
will then return to the proof of Theorem 8.6 .

###### Lemma 8.7.

Let @xmath and @xmath be fixed. For each @xmath , there exists an @xmath
such that

  -- -------- --
     @xmath   
  -- -------- --

Proof: Firstly, we check that @xmath and @xmath . Using Corollary 4.11 ,

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath . Since @xmath , at @xmath we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

It can similarly be shown that @xmath .

As @xmath is continuous in @xmath , and @xmath is a continuous operator,
@xmath is also continuous in @xmath . In particular, @xmath is onto
@xmath . ∎

Continuation of the proof of Theorem 8.6 : Let @xmath be given. From the
previous lemma, there exists @xmath such that @xmath . This is the only
intersection point of @xmath and @xmath , and so

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

from which we find the gradient estimate:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

This estimate holds for all @xmath .

If we let @xmath , we firstly have that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the solution to ( 8.20 ) with discontinuous initial data
@xmath ; and secondly that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is chosen so that @xmath .

Now we turn to the general domain with @xmath . Given @xmath , set
@xmath . If @xmath then we can repeat the same calculation on the small
domain @xmath to find the given result. ∎

## Chapter 9 Estimates for isotropic and anisotropic mean curvature flow

### 9.1 A gradient estimate for mean curvature flow

This section follows the style established in papers such as [ 13 ] and
[ 14 ] , in particular the local gradient estimates of Section 2 of the
latter paper. Ecker and Huisken consider the evolution of a hypersurface
by mean curvature

  -- -------- -- -------
     @xmath      (9.1)
  -- -------- -- -------

where @xmath is the immersion of the manifold @xmath at each time @xmath
and @xmath is the mean curvature vector.

@xmath can be written as a graph when a fixed vector @xmath can be found
so that for a choice of unit normal @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

everywhere. Equivalently, @xmath is bounded above. The existence of an
upper bound for this quantity and its analogue for anisotropic mean
curvature flow is the subject of this chapter.

Given the image @xmath of a point @xmath , its coordinate vector is
@xmath . The height of @xmath above the hyperplane defined by @xmath is
denoted by

  -- -------- --
     @xmath   
  -- -------- --

The gradient function is given by

  -- -------- --
     @xmath   
  -- -------- --

We recall the evolution equations from [ 13 ] :

###### Lemma 9.1.

If @xmath satisfies ( 9.1 ) then

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where @xmath is the Laplace-Beltrami operator on @xmath and @xmath is
the second fundamental form.

We derive a gradient estimate for periodic entire graphs, followed by an
interior estimate. Estimates of this type have also been recently found
by Colding and Minicozzi [ 12 ] in the isotropic case using similar
techniques, although without the explicit dependence on the height of
the graph that the following estimates display.

###### Theorem 9.2 (Estimate for periodic mean curvature flow).

Let @xmath be a smooth, entire solution to mean curvature flow ( 9.1 )
which is a periodic graph over a hyperplane, in that @xmath for a fixed
point @xmath in the hyperplane, and has a height bound @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath and @xmath depend on @xmath .

Proof: Define a new quantity

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a smooth positive function on @xmath chosen so that
@xmath as @xmath . This means that @xmath is strictly negative
initially, regardless of the initial gradient.

The evolution equation for @xmath is given by

  -- -------- -- -------
     @xmath      (9.2)
  -- -------- -- -------

and we can use this with the identities from Lemma 9.1 to find that

  -- -------- --
     @xmath   
  -- -------- --

Now, suppose @xmath is the first point at which @xmath becomes
non-negative. Since @xmath is periodic, this is an internal spatial
maximum, and (spatial) first deriatives are zero:

  -- -------- --
     @xmath   
  -- -------- --

so

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

and the evolution equation at this point is

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

A good choice for @xmath that will allow us to make the final terms
negative is @xmath , where @xmath solves the heat equation @xmath for
some @xmath . In this case

  ----------------------------------------- -------- -- --
                                            @xmath      
                                            @xmath      
                                            @xmath      
  and the equation satisfied by @xmath is               
                                            @xmath      
  ----------------------------------------- -------- -- --

The final three terms of the evolution equation have become

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

In the last line we have used that, with respect to a local orthonormal
frame on @xmath , @xmath , while @xmath has unit length with @xmath : it
follows that

  -- -------- --
     @xmath   
  -- -------- --

the last equality holding only at a maximum point.

If we let @xmath be a fundamental solution of the heat equation

  -- -------- -- -------
     @xmath      (9.3)
  -- -------- -- -------

we can choose @xmath and @xmath depending only on @xmath so that @xmath
and @xmath for @xmath .

So, at the first interior point where @xmath , @xmath and so @xmath for
@xmath . ∎

###### Theorem 9.3 (Interior estimate for mean curvature flow).

Let @xmath be a smooth solution to mean curvature flow ( 9.1 ) which is
a graph over a ball in the hyperplane @xmath . Then we have the interior
estimate

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath , @xmath , and @xmath depend on @xmath and
@xmath .

Proof: We replace @xmath in our previous definition of @xmath by @xmath
:

  -- -------- --
     @xmath   
  -- -------- --

where a smooth positive function @xmath is chosen so that @xmath on the
boundary of a ball of shrinking radius @xmath , and, as before, @xmath
is chosen so that @xmath at the initial time.

In particular, choose @xmath . The evolution equation for @xmath is
given by

  -- -------- --
     @xmath   
  -- -------- --

and we can use this, the identities from Lemma 9.1 , and the evolution
equation for @xmath ( 9.2 ) to find that

  -- -------- --
     @xmath   
  -- -------- --

Now, suppose @xmath is an internal point of this domain at which @xmath
first becomes non-negative.

At an internal spatial maximum of @xmath , @xmath so

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Use this to replace the @xmath term in the evolution equation, so that
at this point

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We can bound the @xmath term by a @xmath term, since (using a local
orthonormal frame @xmath )

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

and

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

so that (writing @xmath for @xmath , the position in the hyperplane)

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

With this we estimate the term in the evolution equation—

  -- -------- -- -------
     @xmath      (9.4)
  -- -------- -- -------

The evolution equation itself becomes

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

This time, we choose

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath , where @xmath still satisfies the heat equation @xmath
. Then

  --------------------------------------------- -------- -- --
                                                @xmath      
                                                @xmath      
                                                @xmath      
  so that the equation satisfied by @xmath is               
                                                @xmath      
  --------------------------------------------- -------- -- --

The final term in the evolution equation is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we have used @xmath . The first term above is positive if, as in
the previous case, @xmath and @xmath . The second term is positive if
@xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

Choose @xmath to be a fundamental solution of the heat equation, as in
the previous proof —

  -- -------- --
     @xmath   
  -- -------- --

Now we can choose some @xmath and @xmath small so that @xmath and @xmath
for @xmath . Here, @xmath is dependent on @xmath and @xmath only. We can
also find @xmath dependent on @xmath , @xmath , @xmath and @xmath ,
satisfying

  -- -------- --
     @xmath   
  -- -------- --

At an internal maximum of @xmath , @xmath and the result follows. ∎

### 9.2 Gradient estimates for anisotropic mean curvature flows

A more general case of curvature flows is that of anisotropic mean
curvature flow . This has been specifically studied by Almgren, Taylor
and Wang [ 1 ] , Gurtin and Angenent [ 9 ] , and Andrews [ 3 , 4 ] ,
among others. The anisotropic surface energy arises in applications from
materials science, such as crystalline growth and phase changes; it also
arises in Finsler geometry [ 11 ] (on a Finsler manifold, at each point
only a normed space is defined, rather than an inner product space as on
a Riemannian manifold).

In this section, we use the framework and notation of [ 4 ] .

As before, we consider surfaces that can be written (either entirely or
locally) as graphs, so that @xmath .

The equation for motion of the graph by anisotropic mean curvature is
derived in [ 4 ] ; in the present work, we set the homogeneous degree
zero “mobility factor” @xmath to be identically @xmath , and so

  -- -------- -- -------
     @xmath      (9.5)
  -- -------- -- -------

where @xmath , with respect to some basis for the tangent space @xmath .

The function @xmath is defined by @xmath , where @xmath is a basis for
the cotangent space @xmath , with dual basis for @xmath given by @xmath
, and where @xmath is a positive convex function that is homogeneous of
degree one, @xmath for @xmath . The unit ball of @xmath

  -- -------- --
     @xmath   
  -- -------- --

must be strictly convex. We also require that @xmath is at least @xmath
. \nomenclature [F] @xmath generating function for anisotropic mean
curvature flow \refpage

#### Differences between the isotropic and anisotropic cases

The introduction of the unspecified anisotropic @xmath into the flow has
the effect of highlighting the special nature of the isotropic case,
when @xmath .

One immediately notices that in the isotropic case, the term with third
derivatives arising in the evolution equation is zero. In ( 9.21 ), this
is the term

  -- -------- --
     @xmath   
  -- -------- --

This absence of third derivatives is apparent in the third identity of
Lemma 9.1 —

  -- -------- --
     @xmath   
  -- -------- --

the left-hand side involves second derivatives of the gradient so we
might expect to see some derivatives of curvature in the right hand side
— instead we see only curvature terms and first derivatives of the
gradient function.

The second difference is that there is no estimate of the form

  -- -------- --
     @xmath   
  -- -------- --

as in ( 9.4 ) for the isotropic case. An equivalent estimate in the
anisotropic context would be: given @xmath , find @xmath so that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath . This is certainly true if we restrict @xmath to the
unit ball, @xmath . If we replace @xmath by @xmath ( @xmath is a
scalar), the putative estimate would be

  -- -------- --
     @xmath   
  -- -------- --

Rewriting the left-hand side using homogeneity gives us

  -- -------- --
     @xmath   
  -- -------- --

As @xmath increases, the left-hand side is converging to a constant
defined on the unit ball, @xmath , multiplied by @xmath . Unless @xmath
is zero , the left-hand side will not remain bounded by the right-hand
side as @xmath . The estimate will not hold without further restrictions
on @xmath .

#### Calculating with the homogeneous function @xmath

We make some observations about properties arising directly from the
homogeneity and convexity of @xmath , and introduce some notation.

Let @xmath be a basis for the cotangent space @xmath dual to @xmath ,
the basis for the tangent space @xmath . Both @xmath and @xmath are
copies of @xmath .

For @xmath (all repeated indices are summed from @xmath to @xmath unless
indicated otherwise) we will write

  -- -------- --
     @xmath   
  -- -------- --

In general we will prefer to write all derivatives of @xmath in a form
that is homogeneous of degree zero, that is, as @xmath , @xmath , or
@xmath . This means that we can evaluate them on the unit ball, or scale
as we wish — for example, we can use @xmath instead of @xmath .

Homogeneity also means that some derivatives in the radial direction
disappear — for all @xmath ,

  -- -------- -- -------
     @xmath      (9.6)
     @xmath      (9.7)
     @xmath      (9.8)
  -- -------- -- -------

The strict convexity of the unit ball of @xmath means that for all
@xmath on the unit ball, with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

As @xmath is homogeneous, all the level sets of @xmath are also strictly
convex, so this holds for all non-zero @xmath .

We denote by @xmath the removal of a component in the direction of
@xmath from @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is such that @xmath is tangent to the unit ball of @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We have used ( 9.6 ) in the last line. It follows that

  -- -------- --
     @xmath   
  -- -------- --

In the next two lemmas, we show that the coefficients of the evolution
operator satisfy a condition similar to the control on degeneracy that
we required with condition ( 5.9 ) of Chapter 5 .

###### Lemma 9.4.

Let @xmath be a @xmath , positive, homogeneous degree one function with
a strictly convex unit ball @xmath .

Let @xmath be a basis for @xmath .

Then there exist positive constants @xmath and @xmath so that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath with @xmath

Proof: Write

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

is non-zero whenever @xmath . As @xmath is a non-zero tangent covector,
the strict convexity of the unit ball means that @xmath if @xmath .

Fix @xmath on the unit ball of @xmath , @xmath .

Consider @xmath . We want to show that we can find some @xmath and some
strictly positive @xmath so that for all @xmath , @xmath . If this is
not possible, then we can find a sequence @xmath with

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is @xmath , @xmath is continuous in @xmath , and we have
that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Clearly,

  -- -------- --
     @xmath   
  -- -------- --

so

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

by the strict convexity of the unit ball, as at @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

is a non-zero tangent covector. The contradiction implies that we can
indeed find such @xmath and @xmath .

We can find such @xmath and @xmath for every @xmath . Let

  -- -------- --
     @xmath   
  -- -------- --

As we are optimizing over a compact space, @xmath and @xmath .

The result follows directly, for given any @xmath with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is on the unit ball. ∎

We use this to show that the anisotropic mean curvature flow satisfies (
5.9 ), the condition controlling the degeneracy of the parabolic
operator in Chapter 5 .

###### Lemma 9.5.

For all non-zero @xmath and @xmath , we can find positive constants
@xmath and @xmath such that

  -- -------- -- -------
     @xmath      (9.9)
  -- -------- -- -------

whenever @xmath . Here, @xmath and @xmath .

Proof: Set @xmath .

Since @xmath is invariant under @xmath , we need only to consider @xmath
in the unit ball.

Suppose that @xmath is in the unit ball. Since @xmath is a non-zero
tangent covector at @xmath , @xmath by the strict convexity of the unit
ball. By compactness,

  -- -------- --
     @xmath   
  -- -------- --

Also, as neither @xmath nor @xmath are zero,

  -- -------- --
     @xmath   
  -- -------- --

and so @xmath .

Suppose, in order to obtain a contradiction, that there is a pair @xmath
in the unit ball for which there are no such constants @xmath and @xmath
. That is,

  -- -------- --
     @xmath   
  -- -------- --

There are two possibilities here: @xmath or @xmath . In the first case,
we must have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

However, since @xmath , @xmath which is a contradiction.

On the other hand, if @xmath , then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

by Lemma 9.4 , which is again a contradiction.

Therefore for every pair of covectors @xmath , there is a pair of
positive constants @xmath and @xmath such that @xmath whenever @xmath .
To get bounds for all @xmath we take the infimum of the @xmath and the
supremum of the @xmath . ∎

We will consider two different restrictions on @xmath . The first is
that third derivatives are small; the second is a symmetry in the
distinguished direction @xmath .

In order to define the first condition, consider the tensor

  -- -------- -- --------
     @xmath      (9.10)
  -- -------- -- --------

for @xmath covectors tangent to the unit ball of @xmath at @xmath , so
that @xmath (and similarly for @xmath and @xmath ).

(This is the Cartan tensor of Bao, Chern and Shen [ 10 ] , or the tensor
@xmath of [ 4 ] restricted to the tangent space of the unit ball.)

The smallness-of-third-derivatives condition is that @xmath satisfies

  -- -------- -- --------
     @xmath      (9.11)
  -- -------- -- --------

for all @xmath tangent to the unit ball of @xmath , where @xmath is a
positive constant dependent on @xmath .

The symmetry condition is that

  -- -------- -- --------
     @xmath      (9.12)
  -- -------- -- --------

###### Lemma 9.6.

If @xmath satisfies ( 9.12 ), then

  -- -------- -- --------
     @xmath      (9.13)
     @xmath      
     @xmath      (9.14)
     @xmath      (9.15)
  -- -------- -- --------

for all @xmath and all @xmath .

Proof: This is a direct consequence of homogeneity. ∎

We can show that the symmetry condition ( 9.12 ) can be used in a
similar way to the smallness-of-third-derivatives condition ( 9.11 ).

###### Lemma 9.7.

Suppose the symmetry condition ( 9.12 ) holds. Then a constant dependent
only on @xmath bounds

  -- -------- -- --------
     @xmath      (9.16)
  -- -------- -- --------

for all @xmath and @xmath , where @xmath . Furthermore, for all @xmath
we can find @xmath so that when @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: Consider @xmath and @xmath on the unit ball and set

  -- -------- --
     @xmath   
  -- -------- --

When we project @xmath and @xmath onto the tangent plane at @xmath they
give non-zero tangent covectors @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

so the terms in the denominator of ( 9.16 ), @xmath and @xmath , are
strictly positive, and hence bounded below when we take the supremum
over @xmath and @xmath in the unit ball.

Also, @xmath is a homogeneous degree zero tensor, and so bounded above
on the unit ball.

It follows that @xmath is finite.

The constant @xmath is unchanged if we scale @xmath so we only need to
consider the behaviour of ( 9.16 ) as @xmath becomes large: that is,

  -- -------- -- --------
     @xmath      (9.17)
  -- -------- -- --------

where @xmath .

Firstly, consider the case that @xmath is parallel to @xmath . Let
@xmath be on the unit ball, and without loss of generality, let @xmath .

We note that the @xmath term in the denominator converges to zero,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

so to deal with this we will multiply both top and bottom by @xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (9.18)
  -- -------- -------- -- --------

Now, the denominator is strictly positive, and by Lemma 9.4 bounded
below —

  -- -------- --
     @xmath   
  -- -------- --

The limiting value of the coefficient of @xmath in ( 9.18 ) is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

using ( 9.6 ).

The limiting value of the coefficient of @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where we use that @xmath . The above term is a derivative, so we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the first term of the second last line is zero due to ( 9.7 )
while the second term is zero as consequence ( 9.13 ) of the symmetry
condition.

The equation ( 9.18 ) is then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where symmetry has been used again in the form of ( 9.15 ).

Now consider the case that @xmath is not parallel to @xmath . In this
case, the denominator of ( 9.17 ) is non-zero:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

since @xmath is a non-zero tangent vector at @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

This cannot be zero as @xmath is not parallel to @xmath .

Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the second limit is zero by ( 9.14 ), since @xmath has no
component in the direction of @xmath .

We have shown that for a fixed @xmath on the unit ball, the quantity (
9.16 ) is bounded above (by @xmath ), and that as @xmath is scaled
outwards this decreases to zero, so for fixed @xmath the quantity is
bounded above.

By compactness of the unit ball, it is bounded for all @xmath . ∎

#### Estimates for periodic, anisotropic mean curvature flows

Let @xmath be a @xmath , bounded

  -- -------- --
     @xmath   
  -- -------- --

periodic

  -- -------- --
     @xmath   
  -- -------- --

solution to the anisotropic curvature flow equation ( 9.5 ), where
@xmath is a positive convex function, homogeneous degree one, with a
strictly convex unit ball.

Remark: We have one estimate in the case that @xmath satisfies the
smallness-of-third-derivatives condition (Theorem 9.8 ) and another in
the case that @xmath satisfies the symmetry condition (Theorem 9.9 ).
However, in Theorem 5.1 and Corollary 5.2 we found an estimate for
periodic anisotropic flows without the need to impose either of these
conditions. In that case, the fact that we were estimating on the
difference quotient, rather than the first derivative itself, avoided
the need to take derivatives of the flow coefficients. Strictly
speaking, Theorems 9.8 and 9.9 are redundant, but they are a good
introduction for the interior estimate of Theorem 9.12 .

###### Theorem 9.8.

If the tensor @xmath given by ( 9.10 ) satisfies ( 9.11 ) with

  -- -------- -- --------
     @xmath      (9.19)
  -- -------- -- --------

then

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath depends on @xmath , @xmath and @xmath (both
given by Lemma 9.4 ), and @xmath .

###### Theorem 9.9.

If @xmath satisfies the symmetry condition ( 9.12 ), then

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath depends on @xmath , @xmath and @xmath (both
given by Lemma 9.4 ), and @xmath is given by Lemma 9.7 .

Proof of Theorem 9.8 : As in the previous sections, define the quantity

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is given by Lemma 9.4 , and where @xmath is a positive
function chosen so that @xmath as @xmath . Suppose that we are at the
first point where @xmath is no longer negative, and let us assume that
at this point, @xmath . This point will be a spatial maximum of @xmath ,
due to the periodicity of @xmath .

Then at this point, @xmath and the first derivative condition is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . That is, for all vectors @xmath ,

  -- -------- -- --------
     @xmath      (9.20)
  -- -------- -- --------

Using ( 9.7 ), we can rewrite the evolution equation for @xmath in terms
of the purely tangential directions @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We make use of this in finding an evolution equation for @xmath

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where in the third step we have added and subtracted second derivatives
of @xmath .

Derivatives of @xmath are @xmath . We this to simplify those terms with
derivatives of @xmath —

  --------------------------------------- -------- -------- -- --
                                          @xmath   @xmath      
                                                   @xmath      
  and remembering that @xmath , this is                        
                                                   @xmath      
                                                   @xmath      
  --------------------------------------- -------- -------- -- --

The evolution equation is now

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (9.21)
  -- -------- -------- -- --------

When we are at a critical point of @xmath , we can use the first
derivative condition ( 9.20 ) to simplify further. The first term of (
9.21 ) becomes

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

while the second becomes

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

as does the third.

The evolution equation for @xmath , at the local maximum of @xmath , is
now

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we have multiplied some terms through by @xmath (since we assume
that @xmath here) in order that derivatives of @xmath appear as
homogeneous degree zero terms.

Derivatives of @xmath are given by

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

for @xmath , so an evolution equation for @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

and the entire evolution equation for @xmath , at a local maximum, is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Notice that all the covectors @xmath , @xmath appear in places where
they may be replaced by @xmath , @xmath respectively (using ( 9.7 ) and
( 9.8 )). That is, we are working exclusively on the tangent space to
the unit ball.

Restricted to the tangent space, @xmath is positive definite so we can
define a Riemannian metric on the tangent space @xmath , @xmath . We can
choose the basis @xmath so that @xmath is the identity at our maximum
point, @xmath . The evolution equation for @xmath is now

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (9.22)
  -- -------- -------- -- --------

Recall the Cauchy-Schwarz inequality for positive matrices: If @xmath is
a positive semi-definite @xmath matrix, then for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and so

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is positive definite, then we can replace @xmath by @xmath to
find that

  -- -------- --
     @xmath   
  -- -------- --

As we assume that @xmath is smooth, @xmath is positive definite and we
can use the above inequality to estimate the second term of ( 9.22 ) :

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (9.23)
  -- -------- -------- -- --------

where the first term of the second line is zero because @xmath is
tangent to the unit ball, so @xmath . In the last line, we have used the
notation for the inverse @xmath . We will choose @xmath later.

We can use ( 9.11 ), the smallness-of-third-derivatives condition, to
estimate the second term in this inequality:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now we can estimate ( 9.22 ) from above —

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The second term is zero if we choose

  -- -------- --
     @xmath   
  -- -------- --

the inequality here is a consequence of ( 9.19 ).

As in the proof of Theorem 9.3 , choose @xmath for some @xmath , with
@xmath given by ( 9.3 ). This satisfies the heat equation @xmath . We
will choose @xmath , where @xmath is given by Lemma ( 9.4 ).
Substituting @xmath and its derivatives for @xmath and its derivatives
(see page 9.1 ) we find that

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now, the first term is zero if we choose

  -- -------- --
     @xmath   
  -- -------- --

As we assumed at the beginning that @xmath , Lemma 9.4 ensures that

  -- -------- --
     @xmath   
  -- -------- --

Consequently, if we choose @xmath small, @xmath is positive and then the
final term will be negative.

On the other hand, if we consider the possibility that @xmath at this
local maximum, we could replace @xmath by @xmath in the definition of
@xmath . In that case, the first maximum of @xmath occurs at a point
where the barrier is flat, and so the first variation is

  -- -------- --
     @xmath   
  -- -------- --

and the evolution equation for @xmath at the local maximum is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Since @xmath at the first point where @xmath , @xmath for all @xmath and
the conclusion follows. ∎

Proof of Theorem 9.9 : We begin by assuming that at a local maximum
point @xmath , and follow the proof of Theorem 9.8 up to equation ( 9.22
), the evolution equation for @xmath at a local maximum:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

This time we do not choose coordinates to make @xmath the identity.

We use Cauchy-Schwarz (with @xmath ) to estimate the second term —

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Choose the basis @xmath at this point so that @xmath is diagonal. Also,
let @xmath be the @xmath diagonal matrix with @xmath or @xmath as its
diagonal entries, chosen so that @xmath . As @xmath is the identity
matrix, @xmath .

In these coordinates,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the term @xmath comes from the estimation of @xmath in Lemma 9.7 ,
under the assumption that @xmath .

This term is dominated by the fourth term of the evolution equation,
since (in the same coordinates)

  ------------------------------------------ -------- -------- -- --
                                             @xmath   @xmath      
                                                      @xmath      
                                                      @xmath      
                                                      @xmath      
  where we use the trace inequality @xmath                        
                                                      @xmath      
  ------------------------------------------ -------- -------- -- --

What is left of the evolution equation is

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

This is negative at a local maximum if we make the same choice of
barrier as before — @xmath for @xmath with @xmath given by ( 9.3 ).

If our assumption that @xmath does not hold, then we can replace @xmath
by @xmath . At the local maximum, @xmath and so the conclusion follows.
∎

Remark: In the last theorem, we have chosen @xmath somewhat arbitrarily;
in fact @xmath needs only to be strictly greater than @xmath , since we
can set @xmath , for @xmath given by Lemma 9.7 . However, a smaller
@xmath may entail a larger @xmath , so the optimal choice would depend
on the exact form of @xmath .

#### Interior estimate for anisotropic mean curvature flow

We begin by showing that when we have the symmetry condition, an
estimate analogous to ( 9.4 ) in the isotropic case is possible.

###### Lemma 9.10.

Suppose that @xmath satisfies the symmetry condition ( 9.12 ). Then
there exists a constant @xmath depending only on @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath .

Proof: This estimate is unchanged under @xmath , so we need only to show
that this holds for @xmath on the unit ball. Let @xmath be a fixed point
on the unit ball.

By compactness, the estimate holds for all @xmath on the unit ball.

Let @xmath be a fixed point on the unit ball and consider

  ------------------------------------------------ -------- -------- -- --
                                                   @xmath   @xmath      
                                                            @xmath      
                                                            @xmath      
  where we have added zero in the form of @xmath                        
                                                            @xmath      
  ------------------------------------------------ -------- -------- -- --

So, either the supremum of @xmath over @xmath is the limit above, or
else it is attained at some finite @xmath . In either case,

  -- -------- --
     @xmath   
  -- -------- --

is finite, and we can set

  -- -------- --
     @xmath   
  -- -------- --

to complete the lemma. ∎

The next lemma shows that the trace of @xmath is bounded below.

###### Lemma 9.11.

Let @xmath be a basis for @xmath , where @xmath . Then there is a
constant @xmath so that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

Proof: By compactness and strict convexity of the unit ball,

  -- -------- --
     @xmath   
  -- -------- --

since @xmath is a non-zero tangent covector.

Now consider

  -- -------- --
     @xmath   
  -- -------- --

where, in the limit, @xmath .

At most one of the @xmath may be parallel to @xmath — suppose it is
@xmath , in which case @xmath . For the remaining @xmath basis
covectors, @xmath are non-zero tangent covectors (to the unit ball at
@xmath ) and so @xmath is again bounded below for @xmath .

It follows that

  -- -------- --
     @xmath   
  -- -------- --

If we take the infimum of all such lower bounds, over all @xmath in the
unit ball, then the conclusion follows. ∎

Let @xmath be a @xmath , bounded

  -- -------- --
     @xmath   
  -- -------- --

solution on the ball of radius @xmath to the anisotropic curvature flow
equation ( 9.5 ), where @xmath is a positive, convex homogeneous degree
one function, with a strictly convex unit ball.

###### Theorem 9.12 (Interior estimate for anisotropic mean curvature
flow).

If @xmath satisfies both the smallness of third derivatives condition (
9.11 ) with some constant

  -- -------- --
     @xmath   
  -- -------- --

and the symmetry condition ( 9.12 ), then

  -- -------- --
     @xmath   
  -- -------- --

for . @xmath .

Here, @xmath is given by Lemma 9.4 and depends on @xmath ; @xmath is
given by Lemma 9.11 and depends on @xmath ; and @xmath , @xmath , and
@xmath depend on @xmath , @xmath , @xmath (which is also given by Lemma
9.4 ) and @xmath .

Proof: We introduce the localising term @xmath into our definition of
@xmath , now restricted to the shrinking ball:

  -- -------- --
     @xmath   
  -- -------- --

for @xmath , where @xmath is the constant given by Lemma 9.11 , @xmath
is a smooth strictly positive function chosen so that @xmath at the
initial time, and @xmath is a smooth positive function chosen so that
@xmath on the boundary of the shrinking ball.

Assume that at the first interior point where @xmath , @xmath .

Then @xmath and as this is a spatial maximum (since the choice of @xmath
ensures that there are no boundary maxima) we have a first derivative
condition

  -- -------- -- --------
     @xmath      (9.24)
  -- -------- -- --------

An evolution equation for @xmath is:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where in the last line we have used the notation @xmath .

We can incorporate the first derivative condition ( 9.24 ) into ( 9.21
), the evolution equation for @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Putting the last two steps together gives an evolution equation for
@xmath at a local maximum:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The second term here may be split up into a part with @xmath and a part
with @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

These may be individually estimated using the Cauchy-Schwarz inequality
and the smallness-of-third-derivatives condition, as described in the
proof of Theorem 9.8 on page 9.9 —

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

for some @xmath .

We choose the localising term to be @xmath for some @xmath , @xmath ,
and @xmath given in Lemma 9.11 . Then

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

and the second-last term of the evolution equation is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

As @xmath satisfies the symmetry condition ( 9.12 ), we may use Lemma
9.10 to estimate the final term of the evolution equation:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The evolution equation can now be estimated from above —

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      (9.25)
  -- -------- -------- -- --------

Since @xmath , we can choose @xmath and @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

With such choices, the second term of the evolution inequality ( 9.25 )
will be negative. We can also set @xmath , so the coefficient of @xmath
is zero.

As in the previous cases we can set @xmath where @xmath is given by (
9.3 ) with @xmath , where @xmath is given by Lemma 9.4 .

The bracketted part of the second line of the evolution equation is then

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      (9.26)
  -- -------- -- --------

If we choose @xmath small enough that @xmath , then the term @xmath is
negative.

As @xmath , @xmath , where @xmath depends only on @xmath .

With this choice of @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

if we choose @xmath small enough that @xmath .

In order to ensure that the last part of ( 9.26 ) is positive, we choose
@xmath so that

  -- -------- --
     @xmath   
  -- -------- --

So, at such maxima, @xmath .

At local maxima where @xmath , then in the definition of @xmath we
replace @xmath by @xmath , in which case the barrier is flat at the
local maxima, and we again find that @xmath .

In either case, the maximum principle ensures that @xmath is never
greater than zero and the conclusion follows. ∎

## Appendix A Function spaces and regularity estimates for parabolic
equations

Here, we define relevant function spaces, and survey some regularity
results used in the existence theorems of Chapters 4 , 6 and 7 . This
treatment follows the books of Krylov [ 23 ] and Lieberman [ 25 ] .

### A.1 Function spaces

On the space of continuous functions @xmath , we have the supremum norm

  -- -------- --
     @xmath   
  -- -------- --

Define the Hölder semi-norm with exponent @xmath by

  -- -------- --
     @xmath   
  -- -------- --

For an integer @xmath , we define the Hölder @xmath -norm by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a multi-index — an @xmath -tuple of non-negative
integers with @xmath , and where @xmath .

The Banach space associated with this norm is @xmath .

#### The parabolic Hölder spaces @xmath

With parabolic equations, it is useful to weight the space and time
variables differently- that is, two space derivatives to one time
derivative. Following Lieberman, we will denote parabolic Hölder spaces
by @xmath rather than @xmath . \nomenclature [H4alpha] @xmath a
parabolic Hölder space \refpage For points @xmath in a domain @xmath ,
define the parabolic Hölder semi-norm by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . The parabolic norm is

  -- -------- --
     @xmath   
  -- -------- --

Higher spatial derivatives and derivatives in time are bounded by @xmath
norms, where @xmath is an integer:

  -- -------- --
     @xmath   
  -- -------- --

The Banach space associated with the @xmath norm is

  -- -------- --
     @xmath   
  -- -------- --

When the region @xmath is a cylinder, in the sense that @xmath , the
parabolic boundary is given by

  -- -------- --
     @xmath   
  -- -------- --

On the boundary, we can define parabolic norms @xmath exactly as above.

### A.2 Regularity estimates

In the following, @xmath is a quasilinear parabolic operator

  -- -------- --
     @xmath   
  -- -------- --

with positive constants @xmath and @xmath such that

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

whenever @xmath . We work on a domain @xmath for some smoothly bounded
@xmath .

Here @xmath is the intersection of the region and a cylinder:

  -- -------- --
     @xmath   
  -- -------- --

We begin with an oscillation estimate for the gradient of a solution for
a Neumann problem near a flat boundary:

###### Theorem A.1.

Suppose that @xmath , @xmath , and that inside @xmath , the boundary of
@xmath is @xmath . Let @xmath be a solution of @xmath when @xmath and
@xmath , when @xmath .

Suppose there are positive constants @xmath and @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath with @xmath .

If @xmath , then there are positive constants @xmath and @xmath
determined only by @xmath , @xmath , @xmath , @xmath and @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

implies

  -- -------- --
     @xmath   
  -- -------- --

Similarly, we can find an estimate near the boundary for problems with
Dirichlet boundary conditions:

###### Theorem A.2.

Suppose that @xmath and @xmath are uniformly continuous, that @xmath is
differentiable with respect to @xmath , and where, if @xmath , we can
find a positive constant @xmath such that

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

If @xmath satisfies @xmath on @xmath and @xmath on @xmath , then there
are positive constants @xmath and @xmath depending on @xmath and @xmath
such that for any @xmath

  -- -------- --
     @xmath   
  -- -------- --

for @xmath .

On the interior of the domain, one can also find a Hölder bound for the
gradient:

###### Theorem A.3.

Suppose that @xmath and @xmath are continuous; @xmath is differentiable
with respect to @xmath ; and where if @xmath , we can find a positive
constant @xmath satisfying ( A.1 ).

If @xmath satisfies @xmath and @xmath in @xmath , then there is a
positive constant @xmath determined by @xmath and @xmath such that for
interior sets @xmath we have

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

If @xmath is a cylinder in the interior of the domain, we also have

  -- -------- --
     @xmath   
  -- -------- --

as long as @xmath , where @xmath is the distance from @xmath to the
parabolic boundary @xmath .

The following interior estimate is a @xmath bound for @xmath when the
coefficients of @xmath are smooth:

###### Theorem A.4.

Suppose that @xmath , and @xmath , and that there is a constant @xmath
such that

  -- -------- --
     @xmath   
  -- -------- --

for @xmath .

Then for any @xmath there is a constant @xmath dependent on @xmath ,
@xmath , @xmath , @xmath and @xmath such that if @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

In a similar vein, there are higher regularity estimates on the interior
of a domain:

###### Theorem A.5.

If we have @xmath , @xmath , @xmath for any @xmath , and if @xmath and
@xmath , then @xmath for @xmath , and there is a constant @xmath such
that

  -- -------- --
     @xmath   
  -- -------- --

\printglossary

[1.8cm]

\nomenclature

[Q] @xmath the Cartan tensor restricted to the tangent space of the unit
ball of @xmath \refpage \nomenclature [a] @xmath coefficient of second
derivative in a parabolic operator; also appears as @xmath

\nomenclature

[b] @xmath lower order terms in a parabolic operator

\nomenclature

[Qr] @xmath the parabolic cylinder of radius @xmath centred at @xmath ,
@xmath

\nomenclature

[E] @xmath the Bernstein @xmath -function, @xmath \nomenclature [Erf]
@xmath the error function @xmath

\nomenclature

[F] @xmath embedding of a manifold

\nomenclature

[H3nu] @xmath mean curvature vector @xmath

\nomenclature

[B] @xmath the open ball of radius @xmath centred at @xmath , @xmath

\nomenclature

[P] @xmath the parabolic boundary of a set: @xmath .

\nomenclature

[P1] @xmath a parabolic operator, page 2.1 \nomenclature [sn] @xmath the
set of symmetric @xmath matrices

\nomenclature

[la] @xmath lower parabolicity constant on a bounded set @xmath

\nomenclature

[Lb] @xmath upper parabolicity constant on a bounded set @xmath

\nomenclature

[lc] @xmath lower parabolicity constant on a set with @xmath , page 4.4
\nomenclature [Ld] @xmath upper parabolicity constant on a set with
@xmath , page 4.4

\nomenclature

[mij] @xmath the second-order coefficients of the equation for mean
curvature flow of graphs, @xmath

\nomenclature

[nu] @xmath outwards unit normal — to the surface, page 2.2 ; to the
boundary of the region, page 6.1

\nomenclature

[phi] @xmath the fundamental solution to the heat equation, @xmath

\nomenclature

[psi] @xmath a special barrier defined in Section 3.3 , page 3.3

\nomenclature

[Psi] @xmath a boundary-straightening change of coordinates

\nomenclature

[Omega] @xmath an interior set: given @xmath , @xmath