# Abstract

## Chapter 1 Introduction to Renewal Systems

### 1.1 Optimization over a single renewal system: A review

Renewal systems are generalizations of renewal processes studied in
probability and random processes courses. Parallel to Markov decision
processes versus Markov chains, renewal systems are controlled renewal
processes. Since this is not a widely used term, to set the tone of the
thesis, we start with a review of optimization over a single renewal
system.

Consider a dynamical system operating over a discrete slotted timeline
@xmath . The timeline is segmented into back-to-back intervals of time
slots called renewal frames . The start of each renewal frame for a
system is called a renewal time or simply a renewal for that system. The
duration of each renewal frame is a random positive integer whose
distribution depends on a control action chosen at the start of the
frame. We use @xmath to index the renewals. Let @xmath be the time slot
corresponding to the @xmath -th renewal with the convention that @xmath
. Let @xmath be the set of all slots from @xmath to @xmath . See Fig.
1.1 for a graphical illustration.

At time @xmath , the decision maker chooses a possibly random decision
@xmath in a set @xmath . This action determines the distributions of the
following random variables:

-   The duration of the @xmath -th renewal frame @xmath , which is a
    positive integer.

-   A vector of performance metrics at each slot of that frame @xmath ,

    @xmath , where @xmath is a fixed positive integer.

-   A penalty incurred at each slot of the frame @xmath , @xmath .

In the special case where @xmath , this reduces to the classical slotted
stochastic system, which has been relatively well-understood. Let @xmath
be the system history up to @xmath , which includes @xmath , @xmath and
@xmath . The key property we rely on is as follows.

###### Definition 1.1.1 (Renewal property).

A system is said to satisfy the renewal property if the random @xmath ,
@xmath and @xmath , @xmath are conditionally independent of the history
@xmath given @xmath .

The goal is to minimize the time average penalty subject to @xmath time
average constraints on the performance metrics, i.e. we aim to solve the
following optimization problem:

  -- -------- -- -- -------
     @xmath         (1.1)
                    (1.2)
  -- -------- -- -- -------

where @xmath are known constants. Let

  -- -------- --
     @xmath   
  -- -------- --

be realizations during the @xmath -th frame using an action @xmath .
Under mild technical conditions (e.g. existence of second moments, see
Section 2.1 for details), the problem ( 1.1 )-( 1.2 ) can also be
written as a fractional program form:

  -- -------- -------- -- -------
     @xmath               (1.3)
                          (1.4)
              @xmath      
  -- -------- -------- -- -------

#### 1.1.1 Optimization over i.i.d. actions

Suppose the system adopts an i.i.d. sequence of random actions @xmath ,
where the decision @xmath made on frame @xmath independent of the past.
Then, by the renewal property, it is easy to see that @xmath are i.i.d.
random variables. We have

  -- -- --
        
        
  -- -- --

As a consequence, if we consider solving ( 1.3 )-( 1.4 ) over the set of
i.i.d. random actions, then?

  -- -------- -- -- -------
     @xmath         (1.5)
                    (1.6)
  -- -------- -- -- -------

###### Assumption 1.1.1.

The problem ( 1.5 )-( 1.6 ) is feasible, i.e. there exists @xmath such
that ( 1.6 ) are satisfied. Furthermore, we assume the set of all
feasible performance vectors @xmath over all i.i.d. actions @xmath is
compact.

The compactness assumption is adopted so that there exists at least one
i.i.d. action which solves ( 1.5 )-( 1.6 ). In fact, one can show that
under proper technical conditions the minimum achieved by ( 1.3 )-( 1.4
) is the same as that of ( 1.5 )-( 1.6 ) (see, for example, Lemma 2.3.2
in the next section).

#### 1.1.2 Ergodic Markov decision process (MDP): An example

As one of the main motivations for this line of research, in this
section, we show that the well-known MDP is a special case of the
renewal system. Consider a discrete time MDP over an infinite horizon.
It consists of a finite state space @xmath , and an action space @xmath
at each state @xmath ¹ ¹ 1 To simplify the notation, we assume each
state has the same action space @xmath . All our analysis generalizes
trivially to states with different action spaces. For each state @xmath
, we use @xmath to denote the transition probability from @xmath to
@xmath when taking action @xmath , i.e.

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are state and action at time slot @xmath .

At time slot @xmath , after observing the state @xmath and choosing the
action @xmath , the MDP receives a penalty @xmath and @xmath types of
resource costs @xmath , where these functions are all bounded mappings
from @xmath to @xmath . For simplicity we write @xmath and @xmath . The
goal is to minimize the time average penalty with constraints on time
average overall costs. This problem can be written in the form ( 1.1 )-(
1.2 ).

In order to define the renewal frame, we need one more assumption on the
MDP. We assume the MDP is ergodic , i.e. there exists a state which is
recurrent and the corresponding Markov chain is aperiodic under any
randomized stationary policy ² ² 2 A randomized stationary policy @xmath
is an algorithm which chooses actions at state @xmath according to a
fixed conditional distribution @xmath and is independent of all other
past information, i.e. @xmath , @xmath , @xmath and @xmath is the past
information up to time @xmath . , with bounded expected recurrence time.
Under this assumption, the renewals for the MDP can be defined as
successive revisitations to the recurrent state, and the action set
@xmath in such scenario is defined as the set of all randomized
stationary policies that can be implemented in one renewal frame. Thus,
our renewal system formulation includes ergodic MDPs. We refer to [ Al99
] , [ Be01 ] , and [ Ro02 ] for more details on MDP theory and related
topics. We also refer readers to Chapter 5 for more MDP specific
algorithms and analysis.

#### 1.1.3 The Drift-plus-penalty(DPP) ratio algorithm

In this section, we introduce the classical DPP ratio algorithm solving
( 1.3 )-( 1.4 ) ( [ neely2010stochastic ] , [ neely2013dynamic ] ). It
is a frame-based algorithm which updates parameters at the beginning of
each frame. We start by defining the “virtual queues” @xmath for each
constraint with @xmath and

  -- -------- --
     @xmath   
  -- -------- --

which is updated per frame. Let @xmath be the vector of virtual queues.
Define the drift as follows:

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the system history up to @xmath , which includes @xmath ,
@xmath Then, it is easy to show that

  -- -- --
        
  -- -- --

Assuming that the second moment of @xmath exists, then, there exists a
constant @xmath such that

  -- -- --
        
  -- -- --

We define the DPP expression as @xmath , where @xmath is a trade-off
parameter, which has the following bound:

  -- -------- -- -- -------
                    
     @xmath         (1.7)
     @xmath         (1.8)
  -- -------- -- -- -------

Then, the algorithm (Algorithm 1 ) aims at minimizing the ratio on the
right hand side.

###### Algorithm 1.

DPP ratio algorithm: Fix a trade-off parameter @xmath .

-    At the beginning of each frame, the proposed algorithm takes action
    @xmath in order to minimize the ratio

      -- -- -- -------
               (1.9)
      -- -- -- -------

-    Update the virtual queue @xmath via

      -- -------- -- --------
         @xmath      (1.10)
      -- -------- -- --------

Note that due to the renewal property of the system, maximizing the
above ratio is the same as maximizing the ratio:

  -- -- --
        
  -- -- --

#### 1.1.4 A (somewhat) simple illustrative performance analysis

The performance of this algorithm has been shown in a number of works (
[ neely2010stochastic , neely2013dynamic ] ). We reproduce the proof
here but from a somewhat different perspective compared to previous
works since it is more illustrative for our purpose and serves as the
foundation of our new analysis later.

The key step, which is repeatedly used throughout the thesis is as
follows: Since our proposed algorithm minimizes ( 1.9 ), it must
satisfy:

  -- -- -- --------
           (1.11)
  -- -- -- --------

for any i.i.d. decisions @xmath , where we use the fact that the @xmath
is independent of history @xmath and thus the conditioning on the right
hand side can be omitted. In particular, we can choose @xmath to be the
solution to ( 1.5 )-( 1.6 ) and let @xmath be the optimal performance
vector. Rearranging terms in above inequality gives

  -- -- --
        
  -- -- --

This implies that the expression inside the expectation is a
supermartingale difference sequence , a fact not necessarily needed here
but is the key to our new analysis later. Now, taking expectation from
both sides and sum up from @xmath to @xmath give

  -- -- --
        
  -- -- --

Substituting @xmath gives

  -- -- -- --------
           (1.12)
  -- -- -- --------

On the other hand, taking expectation from both sides of the inequality
( 1.7 ) and sum up from @xmath to @xmath gives

  -- -- --
        
  -- -- --

Sum the above inequality and ( 1.12 ) gives

  -- -- -- --------
           (1.13)
  -- -- -- --------

This bound “kills two birds in the same cage”, allowing us to get
objective bound and constraint violations at the same time immediately.
On one hand, since @xmath , we have

  -- -- -- --------
           (1.14)
  -- -- -- --------

On the other hand, Let @xmath be a constant such that @xmath ,

  -- -- -- --------
           (1.15)
  -- -- -- --------

which follows from the virtual queue updating rule ( 1.10 ) that @xmath
and @xmath .

###### Remark 1.1.1.

The bounds ( 1.14 ), ( 1.15 ) are not the tightest possible bounds, but
(I believe) simple enough to highlight the key steps.

### 1.2 The coupled renewal systems

So far readers have gain some understanding on the renewal systems we
will talk about throughout the thesis. In this section, we introduce our
coupled renewal system model. Many of the notations are the same as
those of the last section except we add a superscript @xmath to index
the renewal systems. Consider @xmath renewal systems that operate over a
slotted timeline ( @xmath ). The timeline for each system @xmath is
segmented into back-to-back intervals, which are renewal frames. The
duration of each renewal frame is a random positive integer with
distribution that depends on a control action chosen by the system at
the start of the frame. The decision at each renewal frame also
determines the penalty and a vector of performance metrics during this
frame. The systems are coupled by time average constraints placed on
these metrics over all systems. The goal is to design a decision
strategy for each system so that overall time average penalty is
minimized subject to time average constraints.

We use @xmath to index the renewals. Let @xmath be the time slot
corresponding to the @xmath -th renewal of the @xmath -th system with
the convention that @xmath . Let @xmath be the set of all slots from
@xmath to @xmath . At time @xmath , the @xmath -th system chooses a
possibly random decision @xmath in a set @xmath . This action determines
the distributions of the following random variables:

-   The duration of the @xmath -th renewal frame @xmath , which is a
    positive integer.

-   A vector of performance metrics at each slot of that frame @xmath ,

    @xmath .

-   A penalty incurred at each slot of the frame @xmath , @xmath .

We assume each system has the renewal property as defined in Definition
1.1.1 that given @xmath , the random variables @xmath , @xmath and
@xmath , @xmath are independent of the information of all systems from
the slots before @xmath with the following known conditional
expectations @xmath , @xmath and @xmath . Fig. 1.2 plots a sample
timeline of three parallel renewal systems.

To make the framework a little bit more general, we introduce an
uncontrollable external i.i.d. random process @xmath which can be
observed during each time slot. Let @xmath . The expectation of @xmath
often serves as the constraints of corresponding performance metrics. As
we shall see in the example application on an energy-aware scheduling
problem, @xmath and @xmath could represent vectors of job services and
arrivals for difference classes, respectively, and the constraints are
that the time average service is no less than the time average of
arrivals for all classes of jobs. The goal is to minimize the total time
average penalty of these @xmath renewal systems subject to @xmath total
time average constraints on the performance metrics related to the
external i.i.d. process, i.e. we aim to solve the following optimization
problem:

  -- -------- -- -- --------
     @xmath         (1.16)
                    (1.17)
  -- -------- -- -- --------

### 1.3 Example Applications and previous works

#### 1.3.1 Multi-server energy-aware scheduling

Consider a slotted time system with @xmath classes of jobs and @xmath
servers. Job arrivals are Poisson distributed with rates @xmath ,
respectively. These jobs are stored in separate queues denoted as @xmath
in a router waiting to be served. Assume the system is empty at time
@xmath so that @xmath . Let @xmath be the precise number of class @xmath
job arrivals at slot @xmath , then, we have @xmath . Let @xmath and
@xmath be the number of class @xmath jobs served and the energy
consumption for server @xmath at time slot @xmath , respectively. Fig.
1.3 sketches an example architecture of the system with 3 classes of
jobs and 10 servers.

Each server makes decisions over renewal frames and the first frame
starts at time slot @xmath . Successive renewals can happen at different
slots for different servers. For the @xmath -th server, at the beginning
of the @xmath -th frame ( @xmath ), it chooses a processing mode @xmath
within the set of all modes @xmath . The processing mode @xmath
determines distributions on the number of jobs served, the service time,
and the energy expenditure, with conditional expectations:

-   @xmath . The expected frame size.

-   @xmath . The expected number of class @xmath jobs served.

-   @xmath . The expected energy consumption.

The goal is to minimize the time average energy consumption, subject to
the queue stability constraints, i.e.

  -- -------- -- -- --------
     @xmath         (1.18)
     @xmath         (1.19)
  -- -------- -- -- --------

Thus, we have formulated the problem into the form ( 5.1 )-( 1.17 ).
Note that the external process in this example is the arrival process of
@xmath classes of jobs with potentially unknown arrival rates @xmath .

#### 1.3.2 Coupled ergodic MDPs

Consider @xmath discrete time Markov decision processes (MDPs) over an
infinite horizon. Each MDP consists of a finite state space @xmath , and
an action space @xmath at each state @xmath ³ ³ 3 To simplify the
notation, we assume each state has the same action space @xmath . All
our analysis generalizes trivially to states with different action
spaces. For each state @xmath , we use @xmath to denote the transition
probability from @xmath to @xmath when taking action @xmath , i.e.

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are state and action at time slot @xmath .

At time slot @xmath , after observing the state @xmath and choosing the
action @xmath , the n-th MDP receives a penalty @xmath and @xmath types
of resource costs @xmath , where these functions are all bounded
mappings from @xmath to @xmath . For simplicity we write @xmath and
@xmath . The goal is to minimize the time average overall penalty with
constraints on time average overall costs, where these MDPs are weakly
coupled through the time average constraints. This problem can be
written in the form ( 5.1 )-( 1.17 ).

In order to define the renewal frame, we need one more assumption on the
MDPs. We assume each of the MDPs is ergodic , i.e. there exists a state
which is recurrent and the corresponding Markov chain is aperiodic under
any randomized stationary policy, with bounded expected recurrence time.
Under this assumption, the renewals for each MDP can be defined as
successive revisitations to the recurrent state, and the action set
@xmath in such scenario is defined as the set of all randomized
stationary policies that can be implemented in one renewal frame. Thus,
our formulation includes coupled ergodic MDPs. We refer to [ Al99 ] , [
Be01 ] , and [ Ro02 ] for more details on MDP theory and related topics.

As a side remark, this multi-MDP problem can be viewed as a single MDP
on an enlarged state space. Constrained MDPs are discussed previously in
[ Al99 ] . One can show that under the previous ergodic assumption, the
minimum of ( 5.1 )-( 1.17 ) is achieved by a randomized stationary
policy, and furthermore, such a policy can be obtained via solving a
linear program reformulated from ( 5.1 )-( 1.17 ) offline. However,
formulating such LP requires the knowledge of all the parameters in the
problem, including the statistics of the external process @xmath , and
the resulting LP is often computationally intractable when the number of
MDPs is very large.

#### 1.3.3 Why this problem is difficult

Compared to ( 1.1 )-( 1.2 ), this problem is much more challenging
because these @xmath systems are weakly coupled by the time average
constraints ( 1.17 ), yet each of them operates over its own renewal
frames. The renewals of different systems do not have to be synchronized
and they do not have to occur at the same rate (e.g. see Fig. 1.2 ). Our
goal is to develop an algorithm that does not need the knowledge of
@xmath with a provable performance guarantee.

Note that due to the asynchronicity, the DPP ratio algorithm (Algorithm
1 ) does not apply. More specifically, in order to cope with the time
average constraints, Algorithm 1 introduces virtual queues to penalize
constraint violations. These virtual queues are then updated frame-wise
and the analysis is also on the per frame scale of this particular
system. For parallel renewal systems, it is however not clear what is a
proper scale to update the virtual queues.

Naturally, one would think of introducing a virtual queue for each
constraint and update the queue whenever at least one of the systems
starts its new renewal frame. However, this means for those systems who
have yet to reach the renewal, we are updating algorithm parameters in
the middle of the renewal of these systems. This creates grave
difficulties on how to piece together the analysis from each individual
systems. On the other hand, since time is slotted, one could also think
of “giving up” the notion of renewals, synchronizing all systems on the
slot scale and designing a slot-based algorithm. However, this does not
make the problem any simpler since by doing so, the algorithm can still
update algorithm parameters in the middle of renewals.

Prior approaches treat this challenge only in special cases. The works [
Ne12 ] and [ Neely12 ] consider a special case where all quantities
introduced above are deterministic functions of the actions. The work in
[ neely2011online ] develops a two stage algorithm for stochastic
multi-renewal systems, but the first stage must be solved offline.

On the other hand, for the special case where the system is a coupled
Markov decision processes (MDPs). Classical methods for MDPs, such as
dynamic programming and linear programming [ bertsekas1995dynamic ] [
puterman2014markov ] [ Ro02 ] , can be used to solve this problem.
However, it can be impractical for two reasons: First, the state space
has dimension that depends on the number of renewal systems, making
solutions difficult when the number of renewal systems is large. Second,
some statistics of the system, such as the average @xmath process
governing the resource constraints, can be unknown.

#### 1.3.4 Other works related to renewal and asynchronous optimization

The problem considered in the current paper is a generalization of
optimization over a single renewal system. It is shown in [ Ne09 ] that
for the single renewal system with finite action set, the problem can be
solved (offline) via a linear fractional program. Methods for solving
linear fractional programs can be found in [ BV04 ] and [ Sc83 ] . The
drift-plus-penalty ratio approach is also developed in [ Neely2010 ] and
[ neely2013dynamic ] for the single renewal system.

Note that there are also many other algorithms which consider
“asynchronous optimization” in a different sense compare to ours. More
specifically, the works [ BT97 ] [ BGPS06 ] [ SN11 ] [ PXYY15 ] consider
the scenario where the asynchronicity shown in Fig. 1.2 results from
uncontrollable delays due to environmental uncertainties. These delays
are of fixed distributions independent of the actions or even
deterministic. Thus, the delays do not appear in the optimization
objectives.

On the other hand, our problem is also related to the multi-server
scheduling as is shown in one of the example applications. When assuming
proper statistics of the arrivals and/or services, energy optimization
problems in multi-server systems can also be treated via queueing
theory. Specifically, by assuming both arrivals and services are Poisson
distributed, [ GDHS13 ] treats the multi-server system as an M/M/k/setup
queue and explicitly computes several performance metrics via the
renewal reward theorem. By assuming arrivals are Poisson and only one
server, [ LN14 ] and [ Yao02 ] treat the system as a multi-class M/G/1
queue and optimize the average energy consumption via polymatroid
optimization.

### 1.4 Outline and our contributions

The rest of the thesis is organized as follows:

-    Chapter 2: (published in [ wei2018asynchronous ] ) We develop a new
    algorithm for the general asynchronous renewal optimization, where
    each system operates on its own renewal frame. It is fully analyzed
    with convergence as well as convergence time results. As a first
    technical contribution, we fully characterize the fundamental
    performance region of the problem ( 5.1 )-( 1.17 ). We then
    construct a supermartingale along with a stopping-time to
    “synchronize” all systems on a slot basis, by which we could piece
    together analysis of each individual system to prove the convergence
    of the proposed algorithm. Furthermore, encapsulating this new idea
    into convex analysis tools, we prove the @xmath convergence time of
    the proposed algorithm to reach @xmath near optimality under a mild
    assumption on the existence of a Lagrange multiplier. Specifically,
    we show that for any accuracy @xmath and any time @xmath , the
    sequence @xmath and @xmath produced by our algorithm satisfies,

      -- -- --
            
            
      -- -- --

    where @xmath denotes the optimal objective value of ( 5.1 )-( 1.17
    ). Simulation experiments on the aforementioned multi-server
    energy-aware scheduling problem also demonstrate the effectiveness
    of the proposed algorithm.

-    Chapter 3 Data center server provision: (published in [ wei2017data
    ] ) We consider a cost minimization problem for data centers with N
    servers and randomly arriving service requests. A central router
    decides which server to use for each new request. We formulate this
    problem as an asynchronous renewal optimization, and develop a
    distributed control algorithm so that each server makes its own
    decisions, the request queues are bounded and the overall time
    average cost is near optimal with probability 1. The algorithm does
    not need probability information for the arrival rate or job sizes.
    Next, an improved algorithm that uses a single queue is developed
    via a “virtualization” technique which is shown to provide the same
    (near optimal) costs. Simulation experiments on a real data center
    traffic trace demonstrate the efficiency of our algorithm compared
    to other existing algorithms.

-    Chapter 4 Multi-user file downloading: (published in [ wei2015power
    ] ) We treat power-aware throughput maximization in a multi-user
    file downloading system. Each user can receive a new file only after
    its previous file is finished. The file state processes for each
    user act as coupled Markov chains that form a generalized restless
    bandit system. First, an optimal algorithm is derived for the case
    of one user. The algorithm maximizes throughput subject to an
    average power constraint. Next, the one-user algorithm is extended
    to a low complexity heuristic for the multi-user problem. The
    heuristic uses a simple online index policy. In a special case with
    no power-constraint, the multi-user heuristic is shown to be
    throughput optimal. Simulations are used to demonstrate
    effectiveness of the heuristic in the general case. For simple cases
    where the optimal solution can be computed offline, the heuristic is
    shown to be near-optimal for a wide range of parameters.

-    Chapter 5 Opportunistic Scheduling over Renewal Systems: (published
    in [ wei2016online ] ) In this chapter, we consider an opportunistic
    scheduling problem over a single renewal system. Different from
    previous chapters, we consider teh scenario where at the beginning
    of each renewal frame, the controller observes a random event and
    then chooses an action in response to the event, which affects the
    duration of the frame, the amount of resources used, and a penalty
    metric. The goal is to make frame-wise decisions so as to minimize
    the time average penalty subject to time average resource
    constraints. This problem has applications to task processing and
    communication in data networks, as well as to certain classes of
    Markov decision problems. We formulate the problem as a dynamic
    fractional program and propose an adaptive algorithm which uses an
    empirical accumulation as a feedback parameter. A key feature of the
    proposed algorithm is that it does not require knowledge of the
    random event statistics and potentially allows (uncountably)
    infinite event sets. We prove the algorithm satisfies all desired
    constraints and achieves @xmath near optimality with probability 1.

-    Chapter 6 Online Learning in Weakly Coupled Markov Decision
    Processes: (published in [ wei2018online ] ) In this chapter, we
    consider a special case of the multiple parallel renewal systems,
    namely, the parallel Markov decision processes coupled by global
    constraints, where the time varying objective and constraint
    functions can only be observed after the decision is made. Special
    attention is given to how well the decision maker can perform in
    @xmath slots, starting from any state, compared to the best feasible
    randomized stationary policy in hindsight. We develop a new
    distributed online algorithm where each MDP makes its own decision
    each slot after observing a multiplier computed from past
    information. While the scenario is significantly more challenging
    than the classical online learning context, the algorithm is shown
    to have a tight @xmath regret and constraint violations
    simultaneously. To obtain such a bound, we combine several new
    ingredients including ergodicity and mixing time bound in weakly
    coupled MDPs, a new regret analysis for online constrained
    optimization, a drift analysis for queue processes, and a
    perturbation analysis based on Farkas’ Lemma.

## Chapter 2 Asynchronous Optimization over Weakly Coupled Renewal
Systems

In this chapter, we present our asynchronous algorithm along with the
new analysis. Along the way, we try to provide some intuitions and high
level ideas of the analysis.

Consider @xmath renewal systems that operate over a slotted timeline (
@xmath ). The timeline for each system @xmath is segmented into
back-to-back intervals, which are renewal frames. The duration of each
renewal frame is a random positive integer with distribution that
depends on a control action chosen by the system at the start of the
frame. The decision at each renewal frame also determines the penalty
and a vector of performance metrics during this frame. The systems are
coupled by time average constraints placed on these metrics over all
systems. The goal is to design a decision strategy for each system so
that overall time average penalty is minimized subject to time average
constraints.

Recall that we use @xmath to index the renewals. Let @xmath be the time
slot corresponding to the @xmath -th renewal of the @xmath -th system
with the convention that @xmath . Let @xmath be the set of all slots
from @xmath to @xmath . At time @xmath , the @xmath -th system chooses a
possibly random decision @xmath in a set @xmath . This action determines
the distributions of the following random variables:

-   The duration of the @xmath -th renewal frame @xmath , which is a
    positive integer.

-   A vector of performance metrics at each slot of that frame @xmath ,

    @xmath .

-   A penalty incurred at each slot of the frame @xmath , @xmath .

We assume each system has the renewal property as defined in Definition
1.1.1 that given @xmath , the random variables @xmath , @xmath and
@xmath , @xmath are independent of the information of all systems from
the slots before @xmath with the following known conditional
expectations @xmath , @xmath and @xmath .

### 2.1 Technical preliminaries

Throughout the chapter, we make the following basic assumptions.

###### Assumption 2.1.1.

The problem ( 5.1 )-( 1.17 ) is feasible, i.e. there are action
sequences @xmath for all @xmath so that the corresponding process @xmath
satisfies the constraints ( 1.17 ).

Following this assumption, we define @xmath as the infimum objective
value for ( 5.1 )-( 1.17 ) over all decision sequences that satisfy the
constraints.

###### Assumption 2.1.2 (Boundedness).

For any @xmath and any @xmath , there exist absolute constants @xmath ,
@xmath and @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, there exists an absolute constant @xmath such that for
every fixed @xmath and every @xmath for which @xmath ,

  -- -- -- -------
           (2.1)
  -- -- -- -------

###### Remark 2.1.1.

The quantity @xmath is usually referred to as the residual lifetime. In
the special case where @xmath , ( 2.1 ) gives the uniform second moment
bound of the renewal frames as

  -- -- --
        
  -- -- --

Note that ( 2.1 ) is satisfied for a large class of problems. In
particular, it can be shown to hold in the following three cases:

1.   If the inter-renewal @xmath is deterministically bounded.

2.   If the inter-renewal @xmath is geometrically distributed.

3.   If each system is a finite state ergodic MDP with a finite action
    set.

###### Definition 2.1.1.

For any @xmath , let

  -- -- --
        
  -- -- --

and @xmath . Define

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

and let @xmath be a performance vector under the action @xmath .

Note that by Assumption 5.2.1 , @xmath and @xmath in Definition 2.1.1
are both bounded, and @xmath , thus, the set @xmath is also bounded. The
following mild assumption states that this set is also closed.

###### Assumption 2.1.3.

The set @xmath is compact.

The motivation of this assumption is to guarantee that there always
exists at least one solution to each subproblem in our algorithm.
Finally, we define the performance region of each individual system as
follows.

###### Definition 2.1.2.

Let @xmath be the convex hull of @xmath . Define

  -- -- --
        
  -- -- --

as the performance region of system @xmath .

### 2.2 Algorithm

#### 2.2.1 Proposed algorithm

In this section, we propose an algorithm where each system can make its
own decision after observing a global vector of multipliers which is
updated using the global information from all systems. We start by
defining a vector of virtual queues @xmath , which are 0 at @xmath and
updated as follows,

  -- -- -- -------
           (2.2)
  -- -- -- -------

These virtual queues will serve as global multipliers to control the
growth of corresponding resource consumptions. Then, the proposed
algorithm is presented in Algorithm 2 .

###### Algorithm 2.

Fix a trade-off parameter @xmath :

-    At the beginning of @xmath -th frame of system @xmath , the system
    observes the vector of virtual queues @xmath and makes a decision
    @xmath so as to solve the following subproblem:

      -- -- -- -------
               (2.3)
      -- -- -- -------

-    Update the virtual queue after each slot:

      -- -- -- -------
               (2.4)
      -- -- -- -------

Note that using the notation specified in Definition 2.1.1 , we can
rewrite ( 2.3 ) in a more concise way as follows:

  -- -- -- -------
           (2.5)
  -- -- -- -------

which is a deterministic optimization problem. Then, by the compactness
assumption (Assumption 2.1.3 ), there always exists a solution to this
subproblem.

###### Remark 2.2.1.

We would like to compare this algorithm to the DPP ratio algorithm
(Algorithm 1 ). For each renewal system, both algorithms update the
decision variable frame-wise based on the virtual queue value at the
beginning of each frame. The major difference is that the proposed
algorithm updates virtual queue slot-wise while Algorithm 1 updates
virtual queues per frame. Such a seemingly small change, somewhat
surprisingly, requires significant generalizations of the analysis on
Algorithm 1 .

This algorithm requires knowledge of the conditional expectations
associated with the performance vectors @xmath , but only requires
individual systems @xmath to know their own @xmath , and therefore
decouples these systems. Furthermore, the virtual queue update uses
observed @xmath and does not require knowledge of distribution or mean
of @xmath .

In addition, we introduce @xmath as “virtual queues” for the following
two reasons: First, it can be mapped to real queues in applications
(such as the server scheduling problem mentioned in Section 1.3.1 ),
where @xmath stands for the arrival process and @xmath is the service
process. Second, stabilizing these virtual queues implies the
constraints ( 1.17 ) are satisfied, as is illustrated in the following
lemma.

###### Lemma 2.2.1.

If @xmath and @xmath , then, @xmath .

###### Proof of Lemma 2.2.1.

Fix @xmath . For any fixed @xmath , @xmath . For each summand, by queue
updating rule ( 5.5 ),

  -- -------- -------- --
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

Thus, by the assumption @xmath ,

  -- -- --
        
  -- -- --

Taking expectations of both sides with @xmath , gives

  -- -- --
        
  -- -- --

Dividing both sides by @xmath and passing to the limit gives

  -- -- --
        
  -- -- --

finishing the proof. ∎

#### 2.2.2 Computing subproblems

Since a key step in the algorithm is to solve the optimization problem (
2.5 ), we make several comments on the computation of the ratio
minimization ( 2.5 ). In general, one can solve the ratio optimization
problem ( 2.3 ) (therefore ( 2.5 )) via a bisection search algorithm.
For more details, see section 7 of [ Neely2010 ] . However, more often
than not, bisection search is not the most efficient one. We will
discuss two special cases arising from applications where we can find a
simpler way of solving the subproblem.

First of all, when there are only a finite number of actions in the set
@xmath , one can solve ( 2.5 ) simply via enumerating. This is a typical
scenario in energy-aware scheduling where a finite action set consists
of different processing modes that can be chosen by servers.

Second, when the set @xmath specified in Definition 2.1.2 is itself a
convex hull of a finite sequence @xmath , then, ( 2.5 ) can be rewritten
as a simple enumeration:

  -- -- --
        
  -- -- --

To see this, note that by definition of convex hull, for any @xmath ,
@xmath for some @xmath , @xmath and @xmath . Thus,

  -- -------- -- --
                 
     @xmath      
     @xmath      
  -- -------- -- --

where we let @xmath . Note that @xmath and @xmath because @xmath .
Hence, solving ( 2.5 ) is equivalent to choosing @xmath to minimize the
above expression, which boils down to choosing a single @xmath among
@xmath which achieves the minimum.

Note that such a convex hull case stands out not only because it yields
a simple solution, but also because of the fact that ergodic coupled
MDPs discussed in Section 1.3.2 have the region @xmath being the convex
hull of a finite sequence of points @xmath , where each point @xmath
results from a pure stationary policy ( [ Al99 ] ). ¹ ¹ 1 A pure
stationary policy is an algorithm where the decision to be taken at any
time @xmath is a deterministic function of the state at time @xmath ,
and independent of all other past information. Thus, solving ( 2.5 ) for
the ergodic coupled MDPs reduces to choosing a pure policy among a
finite number of pure policies.

### 2.3 Limiting Performance

In this section, we provide the performance analysis of Algorithm 2 .
Let @xmath be the optimal objective value for problem ( 5.1 )-( 1.17 ).
The goal is to show the following bound similar to that of Algorithm 1 :

  -- -- --
        
        
  -- -- --

for some constant @xmath . Then, by Lemma 2.2.1 , one readily obtains
the constraint satisfaction result.

For the rest of the chapter, the underlying probability space is denoted
as the tuple @xmath . Let @xmath be the system history up until time
slot @xmath . Formally, @xmath is a filtration with @xmath and each
@xmath is the @xmath -algebra generated by all random variables from
slot 0 to @xmath .

For the rest of the chapter, we always assume Assumptions 2.1.1 - 2.1.3
hold without explicitly mentioning them.

#### 2.3.1 Convexity, performance region and other properties

In this section, we present several lemmas on the fundamental properties
of the optimization problem ( 5.1 )-( 1.17 ).

The following lemma demonstrates the convexity of @xmath in Definition
2.1.2 .

###### Lemma 2.3.1.

The performance region @xmath specified in Definition 2.1.2 is convex
for any @xmath . Furthermore, it is the convex hull of the set @xmath
and thus compact, where @xmath is specified Definition 2.1.1 .

First of all, we have the following fundamental performance lemma which
states that the optimality of ( 5.1 )-( 1.17 ) is achievable within
@xmath specified in Definition 2.1.2 .

###### Lemma 2.3.2.

For each @xmath , there exists a pair @xmath such that the following
hold:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

where @xmath is the optimal objective value for problem ( 5.1 )-( 1.17
), i.e. the optimality is achievable within @xmath , the Cartesian
product of @xmath .

Furthermore, for any @xmath , satisfying @xmath , we have @xmath , i.e.
one cannot achieve better performance than ( 5.1 )-( 1.17 ) in @xmath .

The proof of this Lemma is delayed to Section 2.6 . In particular, the
proof uses the following lemma, which also plays an important role in
several lemmas later.

###### Lemma 2.3.3.

Suppose @xmath , @xmath and @xmath are processes resulting from any
algorithm, ² ² 2 Note that this algorithm might make decisions using the
past information. then, @xmath ,

  -- -- -- -------
           (2.6)
           (2.7)
  -- -- -- -------

where @xmath , @xmath and @xmath , @xmath are constant over each renewal
frame for system @xmath defined by

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

and @xmath are defined in Definition 2.1.1 .

The proof of this lemma is delayed to Section 2.6 .

###### Remark 2.3.1.

Note that directly computing @xmath and @xmath indicated by Lemma 2.3.2
would be difficult because of the fractional nature of @xmath , the
coupling between different systems through time average constraints and
the fact that @xmath might be unknown. However, Lemma 2.3.2 can be used
to prove important performance theorems regarding our proposed algorithm
as is indicated by the following lemma.

#### 2.3.2 Main result and near optimality analysis

The following theorem gives the performance bound of our proposed
algorithm.

###### Theorem 2.3.1.

The sequences @xmath and @xmath produced by the proposed algorithm
satisfy all the constraints in ( 1.17 ) and achieves @xmath near
optimality, i.e.

  -- -- --
        
  -- -- --

where @xmath is the optimal objective of ( 5.1 )-( 1.17 ), @xmath i and
@xmath .

###### Proof of Theorem 2.3.1.

Define the drift-plus-penalty (DPP) expression at time slot @xmath as

  -- -- -- -------
           (2.8)
  -- -- -- -------

By the queue updating rule ( 5.5 ), we have

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second inequality follows from the boundedness assumption
(Assumption 5.2.1 ) that @xmath , and the equality follows from the fact
that @xmath is i.i.d. and independent of @xmath , thus,

  -- -- --
        
  -- -- --

For simplicity, define @xmath . Now, by the achievability of optimality
in @xmath (Lemma 2.3.2 ), we have @xmath , thus, substituting this
inequality into the above bound for @xmath gives

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where we use the definition of @xmath in ( 2.15 ) by substituting @xmath
with @xmath , i.e. @xmath , in the second from last equality and use the
optimality condition (Lemma 2.3.2 ) in the final equality. Thus, it
follows

  -- -------- -- --
     @xmath      
  -- -------- -- --

By the virtual queue updating rule ( 2.4 ) and the trivial bound @xmath
, we readily get

  -- -- --
        
  -- -- --

for some constant @xmath . However, this bound is too weak to allow us
proving the convergence result. The key to this proof is to improve such
a bound so that

  -- -- --
        
  -- -- --

where @xmath and @xmath are two constants independent of @xmath or
@xmath . This is Lemma 2.3.8 . As a consequence for any @xmath ,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

On the other hand, by the definition of @xmath in ( 2.8 ) and then
telescoping sums with @xmath , we have

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

Combining this with inequality ( 2.9 ) gives

  -- -- -- --------
           (2.10)
  -- -- -- --------

Since @xmath , we can throw away the term and the inequality still
holds, i.e.

  -- -- -- --------
           (2.11)
  -- -- -- --------

Taking @xmath from both sides gives the near optimality in the theorem.

To get the constraint violation bound, we use Assumption 5.2.1 that
@xmath , then, by ( 2.10 ) again, we have

  -- -- --
        
  -- -- --

By Jensen’s inequality @xmath . This implies that

  -- -- --
        
  -- -- --

which implies

  -- -- -- --------
           (2.12)
  -- -- -- --------

Sending @xmath gives

  -- -- --
        
  -- -- --

Finally, by Lemma 2.2.1 , all constraints are satisfied. ∎

Note that the above proof implies a more refined result that illustrates
the convergence time. Fix an @xmath , let @xmath , then, for all @xmath
, ( 2.11 ) implies that

  -- -- --
        
  -- -- --

However, ( 2.12 ) suggests a larger convergence time is required for
constraint satisfaction! For @xmath , it can be shown that ( 2.12 )
implies that

  -- -- --
        
  -- -- --

whenever @xmath . The next section shows a tighter @xmath convergence
time with a mild Lagrange multiplier assumption. The rest of this
section is devoted to proving Lemma 2.3.8 .

#### 2.3.3 Key-feature inequality and supermartingale construction

In this section and the next section, our goal is to show that the term

  -- -- -- --------
           (2.13)
  -- -- -- --------

Learning from the single renewal analysis (equation ( 1.11 )), we have
the following key-feature inequality connecting our proposed algorithm
with the performance vectors inside @xmath .

###### Lemma 2.3.4.

Consider the stochastic processes @xmath , @xmath , and @xmath resulting
from the proposed algorithm. For any system @xmath , the following holds
for any @xmath and any @xmath ,

  -- -- -- --------
           (2.14)
  -- -- -- --------

###### Proof of Lemma 5.5.4.

First of all, since the proposed algorithm solves ( 2.3 ) over all
possible decisions in @xmath , it must achieve value less than or equal
to that of any action @xmath at the same frame. This gives,

  -- -- --
        
  -- -- --

where @xmath is defined in ( 2.3 ) and the equality follows from the
renewal property of the system that @xmath , @xmath and @xmath are
conditionally independent of @xmath given @xmath .

Since @xmath , this implies

  -- -- --
        
  -- -- --

thus, for any @xmath ,

  -- -- --
        
  -- -- --

Since @xmath specified in Definition 2.1.2 is the convex hull of @xmath
, it follows for any vector @xmath , we have

  -- -- --
        
  -- -- --

Dividing both sides by @xmath and using the definition of @xmath in
Definition 2.1.2 give

  -- -- --
        
  -- -- --

Finally, since @xmath , @xmath , and @xmath result from the proposed
algorithm and the action chosen is determined by @xmath as in ( 2.3 ),

  -- -- --
        
  -- -- --

This finishes the proof. ∎

Our next step is to give a frame-based analysis for each system by
constructing a supermartingale on the per-frame timescale. We start with
a definition of supermartingale:

###### Definition 2.3.1 (Supermartingale).

Consider a probability space @xmath and a filtration @xmath on this
space with @xmath , @xmath and @xmath . Consider a process @xmath
adapted to this filtration, i.e. @xmath . Then, we have @xmath is a
supermartigale if @xmath and @xmath . Furthermore, @xmath is called a
supermartingale difference sequence.

Note that by definition of supermartigale, we always have @xmath . Along
the way, we also have a standard definition of stopping time which will
be used later:

###### Definition 2.3.2 (Stopping time).

Given a probability space @xmath and a filtration @xmath in @xmath . A
stopping time @xmath with respect to the filtration @xmath is a random
variable such that for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

i.e. the stopping time occurring at time @xmath is contained in the
information during slots @xmath .

Recall that @xmath is a filtration (with @xmath representing system
history during slots @xmath ). Fix a system @xmath and recall that
@xmath is the time slot where the @xmath -th renewal occurs for system
@xmath . We would like to define a filtration corresponding to the
random times @xmath . To this end, define the collection of sets @xmath
such that for each @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For example, the following set @xmath is an element of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are specific values. Then @xmath because for @xmath we have
@xmath , and for @xmath we have @xmath . The following technical lemma
is proved in Section 2.6 .

###### Lemma 2.3.5.

The sequence @xmath is a valid filtration, i.e. @xmath . Furthermore,
for any real-valued adapted process @xmath with respect to @xmath , ³ ³
3 Meaning that for each @xmath in @xmath , the random variable @xmath is
determined by events in @xmath .

  -- -- --
        
  -- -- --

is also adapted to @xmath , where for any @xmath , @xmath is a fixed
real-valued measurable mappings. That is, for any @xmath , it holds that
the value of any measurable function of @xmath is determined by events
in @xmath .

With Lemma 5.5.4 and Lemma 2.3.5 , we can construct a supermartingale as
follows,

###### Lemma 2.3.6.

Consider the stochastic processes @xmath , @xmath , and @xmath resulting
from the proposed algorithm. For any @xmath , let

  -- -- -- --------
           (2.15)
  -- -- -- --------

then,

  -- -- --
        
  -- -- --

where @xmath , @xmath and @xmath are as defined in Assumption 5.2.1 .
Furthermore, define a real-valued process @xmath on the frame such that
@xmath and

  -- -- --
        
  -- -- --

Then, @xmath is a supermartingale adapted to the aforementioned
filtration @xmath .

###### Remark 2.3.2.

Note that in the above lemma the quantity @xmath is the term we aim to
bound in ( 2.13 ). Having @xmath being a supermartingale implies @xmath
. This implies

  -- -- --
        
  -- -- --

Thus, this lemma proves ( 2.13 ) is true when @xmath is taken to be the
end of any renewal frame of system @xmath . Our goal in the next section
is to get rid of this restriction and finish the proof via a stopping
time argument.

###### Proof of Lemma 2.3.6.

Consider any @xmath , then, we can decompose @xmath as follows

  -- -------- -- -- --------
     @xmath         (2.16)
  -- -------- -- -- --------

By the queue updating rule ( 5.5 ), we have for any @xmath and any
@xmath ,

  -- -- -- --------
           (2.17)
  -- -- -- --------

Thus, for the last term in ( 2.16 ), by Hölder’s inequality,

  -- -------- -------- --
              @xmath   
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

where the second inequality follows from ( 2.17 ) and the last
inequality follows from the boundedness assumption (Assumption 5.2.1 )
of corresponding quantities. Substituting the above bound into ( 2.16 )
gives a bound on @xmath as

  -- -------- -- -- --------
                    
                    
     @xmath         
                    (2.18)
  -- -------- -- -- --------

where we use the fact that @xmath in the last inequality.

Next, by the queue updating rule ( 5.5 ), @xmath is determined by @xmath
( @xmath ) and @xmath for any @xmath . Thus, by Lemma 2.3.5 , @xmath is
determined by @xmath . For the proposed algorithm, each system makes
decisions purely based on the virtual queue state @xmath , and by the
renewal property of each system, given the decision at the @xmath -th
renewal, the random quantities @xmath , @xmath and @xmath , @xmath are
independent of the outcomes from the slots before @xmath . This implies
the following display,

  -- -- -- --------
           
           
           
           (2.19)
  -- -- -- --------

By Lemma 5.5.4 , we have the following:

  -- -- --
        
  -- -- --

Thus, rearranging terms in above inequality gives the expectation on the
right hand side of ( 2.19 ) is no greater than 0 and hence the first
expectation on the right hand side of ( 2.18 ) is also no greater than
0. For the second expectation in ( 2.18 ), using ( 2.1 ) in Assumption
5.2.1 gives @xmath and the first part of the lemma is proved.

For the second part of the lemma, by Lemma 2.3.5 and the definition of
@xmath , the process @xmath is adapted to @xmath . Moreover, by
Assumption 5.2.1 ,

  -- -- --
        
  -- -- --

Thus, @xmath , i.e. it is absolutely integrable. Furthermore, by the
first part of the lemma,

  -- -- --
        
  -- -- --

finishing the proof. ∎

#### 2.3.4 Synchronization lemma

So far, we have analyzed the processes related to each individual system
over its renewal frames. However, due the asynchronous behavior of
different systems, the supermartingales of each system cannot be
immediately summed.

In order to prove the result ( 2.13 ) and get a global performance
bound, we have to get rid of any index related to individual renewal
frames only. In other words, we need to look at the system property at
any time slot @xmath as opposed to any renewal @xmath .

For any fixed slot @xmath , let @xmath be the number of renewals up to
(and including) time slot @xmath , with the convention that the first
renewal occurs at time @xmath , so @xmath and @xmath , i.e. @xmath . The
next lemma shows @xmath is a valid stopping time, whose proof is in the
appendix.

###### Lemma 2.3.7.

For each @xmath , the random variable @xmath is a stopping time with
respect to the filtration @xmath , i.e. @xmath .

The following theorem tells us a stopping-time truncated supermartingale
is still a supermartingale.

###### Theorem 2.3.2 (Theorem 5.2.6 in [Durrett]).

If @xmath is a stopping time and @xmath is a supermartingale with
respect to @xmath , then @xmath is also a supermartingale, where @xmath
.

With this theorem and the above stopping time construction, we have the
following lemma which finishes the argument proving ( 2.13 ):

###### Lemma 2.3.8.

For each @xmath and any fixed @xmath , we have

  -- -- --
        
  -- -- --

where @xmath is defined in ( 2.16 ) and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

First, note that the renewal index @xmath starts from 0. Thus, for any
fixed @xmath , @xmath , and

  -- -------- -- -- --------
                    
     @xmath         
     @xmath         
     @xmath         (2.20)
  -- -------- -- -- --------

where the third equality follows from the definition of @xmath in Lemma
2.3.6 and the last inequality follows from the fact that the number of
renewals up to time slot @xmath is no more than the total number of
slots, i.e. @xmath . For the term @xmath , we apply Theorem 2.3.2 with
@xmath and index @xmath to obtain @xmath is a supermartingale. This
implies

  -- -- --
        
  -- -- --

Since @xmath , it follows by substituting @xmath ,

  -- -- --
        
  -- -- --

For the last term in ( 2.20 ), by queue updating rule ( 5.5 ), for any
@xmath ,

  -- -- --
        
  -- -- --

it then follows from Hölder’s inequality again that

  -- -------- -------- --
                       
     @xmath            
     @xmath            
     @xmath            
                       
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where in the second from last inequality we use ( 2.1 ) of Assumption
5.2.1 that the residual life @xmath satisfies

  -- -- --
        
  -- -- --

and @xmath , and in the last inequality we use the fact that @xmath ,
thus, @xmath . Substitute the above bound into ( 2.20 ) gives

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where we use the definition @xmath from Lemma 2.3.6 in the equality and
use @xmath in the final equality. Dividing both sides by @xmath finishes
the proof. ∎

### 2.4 Convergence Time Analysis

#### 2.4.1 Lagrange Multipliers

Consider the following optimization problem:

  -- -------- -------- -- --------
     @xmath   @xmath      (2.21)
     @xmath   @xmath      (2.22)
              @xmath      (2.23)
  -- -------- -------- -- --------

Since @xmath is convex, it follows @xmath is convex and @xmath is also
convex. Thus, ( 2.21 )-( 2.23 ) is a convex program. Furthermore, by
Lemma 2.3.2 , we have ( 2.21 )-( 2.23 ) is feasible if and only if ( 5.1
)-( 1.17 ) is feasible, and when assuming feasibility, they have the
same optimality @xmath as is specified in Lemma 2.3.2 .

Since @xmath is convex, one can show (see Proposition 5.1.1 of [ Be09 ]
) that there always exists a sequence @xmath so that @xmath and

  -- -------- --
     @xmath   
  -- -------- --

i.e. there always exists a hyperplane parametrized by @xmath , supported
at @xmath and containing the set @xmath on one side. This hyperplane is
called “separating hyperplane”. The following assumption stems from this
property and simply assumes this separating hyperplane to be
non-vertical (i.e. @xmath ):

###### Assumption 2.4.1.

There exists non-negative finite constants @xmath such that the
following holds,

  -- -------- --
     @xmath   
  -- -------- --

i.e. there exists a separating hyperplane parametrized by @xmath .

###### Remark 2.4.1.

The parameters @xmath are called Lagrange multipliers and this
assumption is equivalent to the existence of Lagrange multipliers for
constrained convex program ( 2.21 )-( 2.23 ). It is known that Lagrange
multipliers exist if the Slater’s condition holds ( [ Be09 ] ), which
states that there exists a nonempty interior of the feasible region for
the convex program. Slater’s condition is very common in convex
optimization theory and plays an important role in convergence rate
analysis, such as the analysis of the interior point algorithm ( [ BV04
] ). In the current context, this condition is satisfied, for example,
in energy aware server scheduling problems, if the highest possible sum
of service rates from all servers is strictly higher than the arrival
rate.

###### Lemma 2.4.1.

Suppose @xmath , @xmath and @xmath are processes resulting from the
proposed algorithm. Under the Assumption 2.4.1 ,

  -- -- --
        
  -- -- --

where @xmath , and @xmath , @xmath are defined in Lemma 2.3.3 .

###### Proof.

First of all, from the statement of Lemma 2.3.3 , for the proposed
algorithm, we can define the corresponding processes @xmath for all
@xmath as

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the last equality follows from the definition of @xmath and @xmath
in Definition 2.1.1 . Since @xmath , by definition of @xmath in
Definition 2.1.2 , @xmath . Since @xmath is a convex set by Lemma 2.3.1
, it follows

  -- -- --
        
  -- -- --

By Assumption 2.4.1 , we have

  -- -- --
        
  -- -- --

Rearranging terms gives

  -- -- --
        
  -- -- --

Taking the time average from 0 to @xmath gives

  -- -- -- --------
           (2.24)
  -- -- -- --------

For the left hand side of ( 2.24 ), we have

  -- -------- -- -- --------
     @xmath         
                    (2.25)
  -- -------- -- -- --------

where the inequality follows from ( 2.6 ) in Lemma 2.3.3 . For the right
hand side of ( 2.24 ), we have

  -- -------- -- -- --------
     @xmath         
                    (2.26)
  -- -------- -- -- --------

where the inequality follows from the fact that @xmath and ( 2.7 ) in
Lemma 2.3.3 . Substituting ( 2.25 ) and ( 2.26 ) into ( 2.24 ) finishes
the proof. ∎

#### 2.4.2 Convergence time theorem

###### Theorem 2.4.1.

Fix @xmath and define @xmath . If the problem ( 5.1 )-( 1.17 ) is
feasible and the Assumption 2.4.1 holds, then, for all @xmath ,

  -- -- -- --------
           (2.27)
           (2.28)
  -- -- -- --------

Thus, the algorithm provides @xmath approximation with the convergence
time @xmath .

###### Proof.

First of all, by queue updating rule ( 5.5 ),

  -- -- -- --------
           (2.29)
  -- -- -- --------

By Lemma 2.4.1 , we have

  -- -------- -- -- --------
                    
     @xmath         (2.30)
  -- -------- -- -- --------

Combining this with ( 2.10 ) gives

  -- -- -- -- --------
              
              
              (2.31)
  -- -- -- -- --------

where @xmath , the second inequality follows from ( 2.30 ) and the final
inequality follows from Cauchy-Schwarz. Then, by Jensen’s inequality, we
have

  -- -- --
        
  -- -- --

Thus, it follows by ( 2.31 ) that

  -- -- --
        
  -- -- --

The left hand side is a quadratic form on @xmath , and the inequality
implies that @xmath is deterministically upper bounded by the largest
root of the equation @xmath with @xmath and @xmath . Thus,

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Thus, for any @xmath ,

  -- -- --
        
  -- -- --

By ( 2.29 ) again,

  -- -- --
        
  -- -- --

Substituting @xmath and @xmath into the above inequality gives @xmath ,

  -- -------- -------- --
                       
     @xmath   @xmath   
  -- -------- -------- --

Finally, substituting @xmath and @xmath into ( 2.11 ) gives

  -- -- --
        
  -- -- --

finishing the proof. ∎

### 2.5 Simulation Study in Energy-aware Scheduling

Here, we apply the algorithm introduced in Section 2.2 to deal with the
energy-aware scheduling problem described in Section 1.3 . To be
specific, we consider a scenario with 5 homogeneous servers and 3
different classes of jobs, i.e. @xmath and @xmath . We assume that each
server can only choose one class of jobs to serve during each frame. So
the mode set @xmath contains three actions @xmath and the action @xmath
stands for serving the @xmath -th class of jobs and we count the number
of serviced jobs at the end of each service duration. The action @xmath
determines the following quantities:

-   The uniformly distributed total number of class @xmath jobs that can
    be served with expectation @xmath .

-   The geometrically distributed service duration @xmath slots with
    expectation @xmath .

-   The energy consumption @xmath for serving all these jobs.

-   The geometrically distributed idle/setup time @xmath slots with
    constant energy consumption @xmath per slot and zero job service.
    The expectation @xmath .

The idle/setup cost is @xmath units per slot and the rest of the
parameters are listed in Table 1.

Following the algorithm description in Section 2.2 , the proposed
algorithm has the queue updating rule

  -- -- --
        
  -- -- --

and each system minimizes ( 2.3 ) each frame, which can be written as

  -- -- --
        
  -- -- --

Each plot for the proposed algorithm is the result of running 1 million
slots and taking the time average as the performance of the proposed
algorithm. The benchmark is the optimal stationary performance obtained
by performing a change of variable and solving a linear program, knowing
the arrival rates (see also [ Neely12 ] for details).

Fig. 5.3 shows as the trade-off parameter @xmath gets larger, the time
average energy consumptions under the proposed algorithm approaches the
optimal energy consumption. Fig. 5.4 shows as @xmath gets large, the
time average number of services also approaches the optimal service rate
for each class of jobs. In Fig. 5.5 , we plot the time average queue
backlog for each class of jobs verses @xmath parameter. We see that the
queue backlog for the first class is always low whereas the rest queue
backlogs scale up linearly with @xmath . This is because the service
rate for the first class is always strictly larger than the arrival rate
whereas for the rest classes, as @xmath gets larger, the service rates
approach the arrival rates. This plot, together with Fig. 5.3 , also
demonstrate that @xmath is indeed a trade-off parameter which trades
queue backlog for near optimality.

### 2.6 Additional lemmas and proofs.

#### 2.6.1 Proof of Lemma 2.3.1

###### Proof.

We first prove the convexity of @xmath . Consider any two points @xmath
. We aim to show that for any @xmath , @xmath . Notice that by
definition of @xmath , there exists @xmath such that @xmath , @xmath ,
@xmath , and @xmath . Thus, it is enough to show

  -- -- -- --------
           (2.32)
  -- -- -- --------

To show this, we make a change of variable by letting @xmath . It is
obvious that @xmath . Furthermore, @xmath and

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Since @xmath is convex,

  -- -------- --
     @xmath   
  -- -------- --

Thus, by definition of @xmath again, ( 2.32 ) holds and the first part
of the proof is finished.

To show the second part of the claim, let

  -- -- --
        
  -- -- --

and let @xmath be the convex hull of @xmath . First of all, By
Definition 2.1.2 ,

  -- -- --
        
  -- -- --

for @xmath being the convex hull of @xmath , thus, in view of the
definition of @xmath , we have @xmath . Since both @xmath and @xmath are
convex, by definition of convex hull ( [ rockafellar2015convex ] ) that
@xmath is the smallest convex set containing @xmath , we have @xmath .

To show the reverse inclusion @xmath , note that any point in @xmath can
be written in the form @xmath , where @xmath . Since @xmath by
definition is the convex hull of

  -- -- --
        
  -- -- --

by the definition of convex hull, @xmath can be written as a convex
combination of @xmath points in the above set. Let @xmath be these
points, so that

  -- -------- --
              
     @xmath   
  -- -------- --

As a result, we have

  -- -- --
        
  -- -- --

We make a change of variable by letting @xmath , then,

  -- -------- --
     @xmath   
  -- -------- --

it follows,

  -- -- --
        
  -- -- --

Since @xmath and @xmath , it follows any point in @xmath can be written
as a convex combination of finite number of points in @xmath , which
implies @xmath . Overall, we have @xmath .

Finally, by Assumption 2.1.3 , we have @xmath is compact. Thus, @xmath ,
being a convex hull of a compact set, is also compact. ∎

#### 2.6.2 Proof of Lemma 2.3.3

###### Proof.

We prove bound ( 2.6 ) (( 2.7 ) is proved similarly). By definition of
@xmath in Definition 2.1.1 , we have for any @xmath ,

  -- -- --
        
  -- -- --

thus,

  -- -- --
        
  -- -- --

By the renewal property of the system, given @xmath , @xmath and @xmath
are independent of the past information before @xmath . Thus, the same
equality holds if conditioning also on @xmath , i.e.

  -- -- --
        
  -- -- --

Hence,

  -- -- --
        
  -- -- --

By the definition of @xmath , this further implies that

  -- -- --
        
  -- -- --

Since @xmath and @xmath , it follows @xmath and the process @xmath
defined as

  -- -- --
        
  -- -- --

@xmath is a martingale .

Consider any fixed @xmath and define @xmath as the number of renewals up
to @xmath . Lemma 2.3.7 shows @xmath is a valid stopping time with
respect to the filtration @xmath . Furthermore, @xmath is a
supermartingale by Theorem 2.3.2 , where @xmath .

For this fixed @xmath , we have

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

Since the number of renewals is always bounded by the number of slots at
any time, i.e. @xmath , it follows

  -- -- --
        
  -- -- --

On the other hand,

  -- -- --
        
  -- -- --

where the last inequality follows from Assumption 5.2.1 for the residual
life time. Thus,

  -- -- --
        
  -- -- --

Dividing both sides by @xmath finishes the proof. ∎

#### 2.6.3 Proof of Lemma 2.3.5

###### Proof.

Recall that @xmath is the time slot where the @xmath -th renewal occurs
( @xmath ), then, it follows from the definition of stopping time ( [
Durrett ] ) that @xmath is a sequence of stopping times with respect to
@xmath satisfying @xmath . Thus, by definition of @xmath , for any set
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Thus, @xmath , which implies @xmath , and @xmath is indeed a filtration.
This finishes the first part of the proof.

Next, we would like to show that @xmath is measurable with respect to
@xmath , i.e. @xmath , for any Borel set @xmath . By definition of
@xmath , this is equivalent to showing @xmath for any slot @xmath . For
@xmath , this is obvious because @xmath . Consider any @xmath ,

  -- -- --
        
        
        
  -- -- --

where the last step follows from the assumption that the random variable
@xmath is measurable with respect to @xmath for any @xmath and @xmath is
a stopping time with respect to @xmath for all @xmath . This gives the
second part of the claim. ∎

#### 2.6.4 Proof of Lemma 2.3.7

###### Proof.

We aim to prove @xmath . First of all, recall that the index of the
renewal starts from @xmath and @xmath , thus, for any @xmath , @xmath ,
and any @xmath ,

  -- -------- -------- -- --------
     @xmath   @xmath      (2.33)
  -- -------- -------- -- --------

Consider two cases as follows:

1.  @xmath . In this case, the set ( 2.33 ) is empty and obviously
    belongs to @xmath .

2.  @xmath . In this case, we have @xmath as well as @xmath . Thus, the
    set ( 2.33 ) belongs to @xmath .

Overall, we have @xmath . Thus, @xmath and @xmath is indeed a valid
stopping time with respect to the filtration @xmath . ∎

#### 2.6.5 Proof of Lemma 2.3.2

###### Proof.

To prove the first part of the claim, we define the following notation:

  -- -- --
        
  -- -- --

is the Minkowski sum of sets @xmath , and for any sequence @xmath taking
values in @xmath , define

  -- -- --
        
  -- -- --

is a vector of @xmath s. By definition, any vector in @xmath can be
constructed from @xmath , thus, it is enough to show that there exists a
vector @xmath such that @xmath and the rest of the entries @xmath .

By the feasibility assumption for ( 5.1 )-( 1.17 ), we can consider any
algorithm that achieves the optimality of ( 5.1 )-( 1.17 ) and the
corresponding process @xmath defined in Lemma 2.3.3 for any system
@xmath . Notice that @xmath . This follows from the definition of @xmath
and @xmath in Definition 2.1.1 that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and @xmath . By definition of @xmath in Definition 2.1.2 , @xmath .

Since @xmath is convex by Lemma 2.3.1 , it follows that @xmath . Hence,

  -- -- --
        
  -- -- --

This further implies that

  -- -- --
        
  -- -- --

By Lemma 2.3.1 , @xmath is compact in @xmath . Thus, @xmath is also
compact. This implies that the sequence @xmath has at least one limit
point, and any such limit point is contained in @xmath .

We consider a specific limit point of @xmath denoted as @xmath , with
the first entry denoted as @xmath satisfying

  -- -- --
        
  -- -- --

Then, we have the rest of the entries of @xmath must satisfy

  -- -- --
        
  -- -- --

Now, by Lemma 2.3.3 , we can connect the @xmath with respect to @xmath
and @xmath to that of @xmath and @xmath as follows:

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

Similarly, we can show that

  -- -- --
        
  -- -- --

Thus, by our preceeding assumption that the algorithm under
consideration achieves the optimality of ( 5.1 )-( 1.17 ), we have

  -- -- --
        
        
  -- -- --

Overall, we have shown that @xmath achieves the optimality of ( 5.1 )-(
1.17 ), and the first part of the lemma is proved.

To prove the second part of the lemma, we show that any point in @xmath
is achievable by the corresponding time averages of some algorithm.
Specifically, consider the following class of randomized stationary
algorithms : For each system @xmath , at the beginning of @xmath -th
frame, the controller independently chooses an action @xmath from the
set @xmath with a fixed probability distribution.

Thus, the actions @xmath result from any randomized stationary algorithm
is i.i.d.. By the renewal property of each system, we have

  -- -- --
        
  -- -- --

is also an i.i.d. process for each system @xmath .

Next, we would like to show that any point in @xmath can be achieved by
the corresponding expectations of some randomized stationary algorithm.
Recall that @xmath defined in Definition 2.1.2 is the convex hull of

  -- -- --
        
  -- -- --

By definition of convex hull, any point @xmath , can be written as a
convex combination of a finite number of points from the set @xmath .
Let @xmath be these points, then, we have there exists a finite sequence
@xmath , such that

  -- -------- --
              
     @xmath   
  -- -------- --

We can then use @xmath to construct the following randomized stationary
algorithm: At the start of each frame @xmath , the controller
independently chooses action @xmath with probability @xmath defined
above for @xmath . Then, the one-shot expectation of this particular
randomized stationary algorithm on system @xmath satisfies

  -- -- --
        
  -- -- --

which implies any point in @xmath can be achieved by the corresponding
expectations of a randomized stationary algorithm.

Next, by definition of @xmath in Definition 2.1.2 , any @xmath can be
written as @xmath , where @xmath . Thus, it is achievable by the ratio
of one-shot expectations from a randomized stationary algorithm, i.e.

  -- -- --
        
  -- -- --

Now we claim that for @xmath , @xmath and @xmath result from the
randomized stationary algorithm,

  -- -- -- --------
           (2.34)
           (2.35)
  -- -- -- --------

We prove ( 2.34 ) and ( 2.35 ) is shown in a similar way. Consider any
fixed @xmath , and let @xmath be the number of renewals up to (and
including) time @xmath . Then, from Lemma 2.3.7 in Section 2.3 , @xmath
is a valid stopping time with respect to the filtration @xmath . We
write

  -- -- -- --------
           (2.36)
  -- -- -- --------

For the first part on the right hand side of ( 2.36 ), since @xmath is
an i.i.d. process, by Wald’s equality (Theorem 4.1.5 of [ Durrett ] ),

  -- -- --
        
  -- -- --

By renewal reward theorem (Theorem 4.4.2 of [ Durrett ] ),

  -- -- --
        
  -- -- --

Thus,

  -- -- --
        
  -- -- --

For the second part on the right hand side of ( 2.36 ), by Assumption
5.2.1 ,

  -- -- --
        
  -- -- --

which implies @xmath . Overall, we have ( 2.34 ) holds.

To this point, we have shown that for any @xmath , @xmath , there exists
a randomized stationary algorithm so that

  -- -- --
        
  -- -- --

for any @xmath . Since @xmath is the optimal solution to ( 5.1 )-( 1.17
) over all algorithms, it follows for any @xmath , @xmath satisfying
@xmath , we have @xmath , and the second part of the lemma is proved. ∎

## Chapter 3 Data Center Server Provision via Theory of Coupled Renewal
Systems

The previous chapter introduces a new algorithm and analysis framework
for coupled parallel renewal systems. In this chapter, we show that the
previous algorithm can be applied (extended) to solve a data center
power minimization problem consisting of a central controller who makes
load balancing decisions per slot and parallel servers having multiple
states making decisions per renewal frame. In particular, the analysis
in this chapter, which is customized to the data center application, is
stronger than that of previous general algorithm in the sense that we
obtain a probability 1 convergence of the algorithm rather than an
expected convergence.

### 3.1 System model and problem formulation

Consider a data center that consists of a central controller and @xmath
servers that serve randomly arriving requests. The system operates in
slotted time with time slots @xmath . Each server @xmath has three basic
states:

-   Active: The server is available to serve requests. Server @xmath
    incurs a cost of @xmath on every active slot, regardless of whether
    or not requests are available to serve. In data center applications,
    such cost often represents the power consumption of each individual
    server.

-   Idle: A low cost sleep state where no requests can be served. The
    idle state is actually comprised of a choice of multiple sleep modes
    with different per-slot costs. The specific sleep mode also affects
    the setup time required to transition from the idle state to the
    active state. For the rest of the paper, we use “idle” and “sleep”
    exchangeably.

-   Setup: A transition period from idle to active during which no
    requests can be served. The setup cost and duration depend on the
    preceding sleep mode. The setup duration is typically more than one
    slot, and can be a random variable that depends on the server @xmath
    and on the preceding sleep mode.

An active server can choose to transition to the idle state at any time.
When it does so, it chooses the specific sleep mode to use and the
amount of time to sleep. For example, deeper sleep modes can shut down
more electronics and thereby save on per-slot idling costs. However, a
deeper sleep incurs a longer setup time when transitioning back to the
active state. Each server makes separate decisions about when to
transition and what sleep mode to use. The resulting transition times
for each server are asynchronous. On top of this, a central controller
makes slot-wise decisions for routing requests to servers. It can also
reject requests (with a certain amount of cost) if it decides they
cannot be supported. The goal is to minimize the overall time average
cost.

This problem is challenging mainly for two reasons: First, since each
setup state generates cost but serves no request, it is not clear
whether or not transitioning to idle from the active state indeed saves
power. It is also not clear which sleep mode the server should switch
to. Second, if one server is currently in a setup state, it cannot make
another decision until it reaches the active state (which typically
takes more than one slot), whereas other active servers can make
decisions during this time. Thus, this problem can be viewed as a system
with coupled Markov decision processes (MDPs) making decisions
asynchronously.

#### 3.1.1 Related works

Experimental work on power and delay minimization in data centers is
treated in [ gandhi2013dynamic ] , which proposes to turn each server ON
and OFF according to the rule of an @xmath queue. The work in [
urgaonkar2010dynamic ] applies Lyapunov optimization to optimize power
in virtualized data centers. However, it assumes each server has
negligible setup time and that ON/OFF decisions are made synchronously
at each server. The works [ yao2012data ] , [ lin2013dynamic ] focus on
power-aware provisioning over a time scale large enough so that the
whole data center can adjust its service capacity. Specifically, [
yao2012data ] considers load balancing across geographically distributed
data centers, and [ lin2013dynamic ] considers provisioning over a
finite time interval and introduces an online 3-approximation algorithm.

Prior works [ horvath2008multi , meisner2009powernap , meisner2011power
] consider servers with multiple hypothetical sleep states with
different levels of power consumption and setup times. Although
empirical evaluations in these works show significant power saving by
introducing sleep states, they are restricted to the scenario where the
setup time from sleep to active is on the order of milliseconds, which
is not realistic for today’s data center. Realistic sleep states with
setup time on the order of seconds are considered in [ gandhi2012sleep ]
, where effective heuristic algorithms are proposed and evaluated via
extensive testbed simulations. However, little is known about the
theoretical performance bound regarding these algorithms.

#### 3.1.2 Front-end load balancing

At each time slot @xmath , @xmath new requests arrive at the system (see
Fig. 3.1 ). We assume @xmath takes values in a finite set @xmath . Let
@xmath denote the number of requests routed into server @xmath at time
@xmath . In addition, the system is allowed to reject requests. Let
@xmath be the number of requests that are rejected on slot @xmath , and
let @xmath be the corresponding per-request cost for such rejection.
Assume @xmath takes values in a finite state space @xmath . The @xmath
and @xmath decision variables on slot @xmath must be nonnegative
integers that satisfy:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

for a given integer @xmath . The vector process @xmath takes values in
@xmath and is assumed to be an independent and identically distributed
(i.i.d.) vector over slots @xmath with an unknown probability mass
function.

Each server @xmath maintains a request queue @xmath that stores the
requests that are routed to it. Requests are served in a FIFO manner
with queueing dynamics as follows:

  -- -- -- -------
           (3.1)
  -- -- -- -------

where @xmath is an indicator variable that is 1 if server @xmath is
active on slot @xmath , and @xmath else, and @xmath is a random variable
that represents the number of requests can be served on slot @xmath .
Each queue is initialized to @xmath . Assume that, every slot in which
server @xmath is active, @xmath is independent and identically
distributed with a known mean @xmath . This randomness can model
variation in job sizes.

###### Assumption 3.1.1.

The process @xmath is observable, i.e. the router can observe the @xmath
realization each time slot @xmath before making decisions. In contrast,
the process @xmath is not observable, i.e. given that @xmath , the
server @xmath cannot observe the realization of @xmath until the end of
slot @xmath . Moreover, @xmath and @xmath are all bounded by @xmath ,
@xmath and @xmath respectively.

#### 3.1.3 Server model

Each server @xmath has three types of states: active, idle, and setup
(see Fig. 3.2 ). The idle state of each server @xmath is further
decomposed into a collection of distinct sleep modes. Each server @xmath
makes decisions over its own renewal frames . Define the renewal frame
for server @xmath as the time period between successive visits to active
state (with each renewal period ending in an active state). Let @xmath
denote the frame size of the @xmath -th renewal frame for server @xmath
, for @xmath . Let @xmath denote the start of frame @xmath , so that
@xmath . Assume that @xmath for all @xmath , so that time slot @xmath is
the start of the first renewal frame (labeled frame @xmath ) for all
servers. For simplicity, assume all servers are “active” on slot @xmath
. Thus, the slot just before each renewal frame is an active slot.

Fix a server @xmath and a frame index @xmath . Time @xmath marks the
start of renewal frame @xmath . At this time, server @xmath must decide
whether to remain active or to go idle. If it remains active then the
renewal frame lasts for one slot, so that @xmath . If it goes idle, it
chooses an idle mode from a finite set @xmath , representing the set of
idle mode options. Let @xmath represent this initial decision for server
@xmath at the start of frame @xmath , so that:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath means the server chooses to remain active. If the server
chooses to go idle, so that @xmath , it then chooses a variable @xmath
that represents how much time it remains idle . The decision variable
@xmath is chosen as an integer in the set @xmath for some given integer
@xmath . The consequences of these decisions are described below.

-   Case @xmath . The frame starts at time @xmath and has size @xmath .
    The active variable becomes @xmath and an activation cost of @xmath
    is incurred on this slot @xmath . A random service variable @xmath
    is generated and requests are served according to the queue update (
    3.1 ). Recall that, under Assumption 3.1.1 , the value of @xmath is
    not known until the end of the slot.

-   Case @xmath . In this case, the server chooses to go idle and @xmath
    represents the specific sleep mode chosen. The idle duration @xmath
    is also chosen as an integer in the set @xmath . After the idle
    duration completes, the setup duration starts and has an independent
    and random duration @xmath , where @xmath is an integer random
    variable with a known mean and variance that depends on the sleep
    mode @xmath . At the end of the setup time the system goes active
    and serves with a random @xmath as before. The active variable is
    @xmath for all slots @xmath in the idle and setup times, and is
    @xmath at the very last slot of the frame. Further:

    -   Idle cost: Every slot @xmath of the idle time of frame @xmath ,
        an idle cost of @xmath is incurred (so that the idle cost
        depends on the sleep mode). We have @xmath if server @xmath is
        not idle on slot @xmath . The idle cost can be zero, but can
        also be a small but positive value if some electronics are still
        running in the sleep mode chosen.

    -   Setup cost: Every slot @xmath of the setup time of frame @xmath
        , a cost of @xmath is incurred. We have @xmath if server @xmath
        is not in a setup duration on slot @xmath .

Thus, the length of frame @xmath for server @xmath is:

  -- -- -- -------
           (3.2)
  -- -- -- -------

In summary, the costs @xmath , @xmath and the setup time @xmath are
functions of @xmath . We further make the following assumption regarding
@xmath :

###### Assumption 3.1.2.

For any @xmath , the function @xmath is an integer random variable with
known mean and variance, as well as bounded first four moments. Denote
@xmath and @xmath .

Note that this is a very mild assumption in view of the fact that the
setup time of a real server is always bounded. The motivation behind
emphasizing the fourth moment here instead of simply proceeding with
boundedness assumption is more of theoretical interest than practical
importance.

Table I summarizes the parameters introduced in this section. The data
center architecture is shown is Fig. 3.1 . Since different servers might
make different decisions, the renewal frames are not necessarily
aligned.

#### 3.1.4 Performance Objective

For each @xmath , let @xmath , @xmath , @xmath , @xmath be the time
average costs resulting from rejection, setup, service and idle,
respectively. They are defined as follows: @xmath , @xmath , @xmath ,
@xmath .

The goal is to design a joint routing and service policy so that the
time average overall cost is minimized and all queues are stable, i.e.

  -- -- -- -------
           (3.3)
  -- -- -- -------

Notice that the constraint in ( 3.3 ) is not easy to work with. In order
to get an optimization problem one can deal with, we further define the
time average request rate, rejection rate, routing rate and service rate
as @xmath , @xmath , @xmath , and @xmath respectively: @xmath , @xmath ,
@xmath , @xmath .

Then, rewrite the problem ( 3.3 ) as follows

  -- -------- -------- -- -------
     @xmath               (3.4)
              @xmath      (3.5)
              @xmath      (3.6)
  -- -------- -------- -- -------

Constraint ( 3.5 ) requires the time average arrival rate to server
@xmath to be less than the time average service rate. We aim to develop
an algorithm so that each server can make its own decision (without
looking at the workload or service decision of any other server) and
prove its near optimality.

### 3.2 Coupled renewal optimization

In this section, we show one can apply the algorithm introduced in the
previous section to solve ( 3.4 )-( 3.6 ). But before jumping into
details, we would like to discuss some intuitions behind solving this
problem. As a side remark, this data center work is written and
published before the general algorithm introduced in the last section,
so this intuition is the origin of thesis.

#### 3.2.1 Prelude: The original intuition

First of all, from the queueing model described in the last section and
Fig. 3.1 , it is intuitive that an efficient algorithm would have each
server make decisions regarding its own queue state @xmath , whereas the
front-end load-balancer make routing and rejection decisions slot-wise
based on the global information @xmath .

Next, to get an idea on what exactly the decision should be, by virtue
of Lyapunov optimization, one would introduce a trade-off parameter
@xmath and penalize the time average constraint ( 3.5 ) via @xmath to
solve the following slotwise optimization problem

  -- -------- -------- -- -------
     @xmath               (3.7)
              @xmath      
  -- -------- -------- -- -------

which is naturally separable regarding the load-balancing decision (
@xmath , @xmath ), and the service decision ( @xmath ). However, because
of the existence of a setup state (on which no decision could be made),
the server does not have an identical decision set every slot and
furthermore, the decision set itself depends on previous decisions. This
poses a significant difficulty analyzing the above optimization ( 3.7 ).

In order to resolve this difficulty, we try to find the smallest
“identical time unit” for each individual server in lieu of slots. This
motivates the notion of renewal frame in the previous section (see Fig.
3.2 ). Specifically, from Fig. 3.2 and the related renewal frame
construction, at the starting slot of each renewal, the server faces the
identical decision set (remain active or go to idle with certain slots)
regardless of previous decisions. Following this idea, we modify ( 3.7 )
as follows:

-   For the front-end load balancer, we observe @xmath and solve @xmath
    , which is detailed in Section 3.2.3 .

-   For each server, instead of per slot optimization @xmath , we
    propose to minimize the time average of this quantity per renewal
    frame @xmath .

#### 3.2.2 Coupled renewal optimization

In order to apply Algorithm 2 to this scenario, we can view the
admission control (which chooses @xmath and @xmath ) as one another
system besides @xmath servers. Thus, this problem is equivalent to an
asynchronous optimization over @xmath parallel renewal systems where one
of them is just a slotted system. This falls into the form of ( 5.1 )-(
1.17 ) when setting @xmath ,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and the control variable @xmath , @xmath are non-negative, and must
satisfy the following instant constraints:

  -- -------- --
     @xmath   
  -- -------- --

The only difference compared to ( 3.4 )-( 3.6 ) is that here the
decision variables @xmath and @xmath must take values from time-varying
ranges per slot and they must be chosen after observing the random
variable @xmath . However, since @xmath and @xmath are updated
slot-wise, this minor difference is easy to handle via our renewal
optimization framework and we have the following Algorithm 3 .

###### Algorithm 3.

Fix a trade-off parameter @xmath , and at each time slot @xmath :

-    The admission controller chooses @xmath and @xmath according to

      -- -------- -- -------
         @xmath      (3.8)
      -- -------- -- -------

-    Each server chooses service options @xmath and @xmath via the
    following:

      -- -- -- -------
               (3.9)
      -- -- -- -------

-    Update @xmath :

      -- -- --
            
      -- -- --

#### 3.2.3 Solving (3.8) and (3.9)

Note first that in Algorithm 3 , the solution to problem ( 3.8 ) admits
a simple thresholding rule (with shortest queue ties broken
arbitrarily):

  -- -- -- --------
           (3.10)
  -- -- -- --------

  -- -- -- --------
           (3.11)
  -- -- -- --------

Next, for the problem ( 3.9 ), recall the definition of @xmath and
@xmath . If the server chooses to remain active, then the frame length
is exactly 1, otherwise, the server is allowed to choose how long it
stays in idle with @xmath , where @xmath . It can be easily shown that
over all randomized decisions between staying active and going to
different idle states, it is optimal to make a pure decision which
either stays active or goes to one of the idle states with probability
1.

More specifically, let

  -- -- -- --------
           (3.12)
  -- -- -- --------

We have when the server @xmath chooses to be active, then

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

Otherwise, choosing a specific idle option @xmath gives

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

which follows from the fact that if the server goes idle, then, @xmath
are all zero during the frame except for the last slot. Then, solving (
3.9 ) is equivalent to choosing one option which achieves a smaller
value of @xmath between ( 3.13 ) and ( 3.14 ).

A closer look at the optimization problem ( 3.14 ) indicates that the
best idle period @xmath solving ( 3.14 ) is either 1 or @xmath . This is
unfortunately problematic for the application of data center since it
means the server is either not idle at all or going to idle for a very
long time. When the arrival task stream is of high volatility, this
could cause significant delay. In the next section, we will introduce
our proposed algorithm for the servers which makes relatively “smooth”
decisions.

#### 3.2.4 The proposed online control algorithm

Our main idea pushing the server away from the binary decision is to add
a term in the ratio ( 3.12 ) which is quadratic on the renewal frame
length. Specifically, for server @xmath , at the beginning of its @xmath
-th renewal frame @xmath , it observes its current queue state @xmath
and makes decisions on @xmath and @xmath so as to solve the minimization
of ratio of expectations in ( 3.15 ) as follows:

  -- -- -- --------
           (3.15)
  -- -- -- --------

where @xmath . Compared to the objective ( 3.12 ), the quantity @xmath
has an extra term @xmath on the numerate that is quadratic in @xmath .

Similar to the last section, we are then able to simplify the problem by
computing @xmath for active and idle options separately.

-   If the server chooses to go active, i.e. @xmath , then,

      -- -------- -- --------
         @xmath      (3.16)
      -- -------- -- --------

-   If the server chooses to go idle, i.e. @xmath , then,

      -- -- -- --------
               (3.17)
      -- -- -- --------

    which follows from the fact that if the server goes idle, then,
    @xmath are all zero during the frame except for the last slot. Now
    we try to compute the optimal idle option @xmath and idle time
    length @xmath given the server chooses to go idle. The following
    lemma illustrates that the decision on @xmath can also be reduced to
    pure decision.

    ###### Lemma 3.2.1.
    The best decision minimizing ( 3.17 ) is a pure decision which takes
    one @xmath and one integer value @xmath minimizing the deterministic
    function:

      -- -------- -- --------
         @xmath      (3.18)
      -- -------- -- --------

    The proof of above lemma is given in appendix A.

Then, the server computes the minimum of ( 3.18 ), which is nothing but
a deterministic optimization problem. It goes in the following two
steps:

1.  For each @xmath , first differentiating ( 3.18 ) with respect to
    @xmath to get a real minimizer. Then, choosing @xmath as one of the
    two integer values bracketing the real minimizer which achieves a
    smaller value on ( 3.18 ).

2.  Compare ( 3.18 ) for different @xmath and choose the one achieving
    the minimum.

Thus, the server compares ( 3.16 ) with the minimum of ( 3.18 ). If (
3.16 ) is less than the minimum of ( 3.18 ), then, the server chooses to
go active. Otherwise, the server chooses to go idle and stay idle for
@xmath time slots.

Overall, our final algorithm is summarized in Algorithm 4 .

###### Algorithm 4.

-    At each time slot @xmath , the data center observes @xmath , @xmath
    , and @xmath chooses rejection decision @xmath according to ( 3.10 )
    and chooses routing decision @xmath according to ( 3.11 ).

-    For each server @xmath , at the beginning of its @xmath -th frame
    @xmath , observe its queue state @xmath and compute ( 3.16 ) and the
    minimum of ( 3.18 ). If ( 3.16 ) is less than the minimum of ( 3.18
    ), then the server still stays active. Otherwise, the server
    switches to the idle state minimizing ( 3.18 ) and stays idle for
    @xmath achieving the minimum of ( 3.18 ).

-    Update @xmath according to

      -- -- --
            
      -- -- --

### 3.3 Probability 1 Performance Analysis of Algorithm 4

In this section, we prove a probability 1 convergence result for the
proposed algorithm (Algorithm 4 ). More specifically, we prove the
online algorithm introduced in the last section makes all request queues
@xmath bounded (on the order of @xmath ) and achieves the near
optimality with sub-optimality gap on the order of @xmath with
probability 1.

#### 3.3.1 Bounded request queues

In this section, we show that the request queues are deterministically
bounded due to the special thresholding nature of the admission control.
Such a result is stronger (yet simpler) than the expected virtual queue
analysis presented in the last section.

###### Lemma 3.3.1.

If @xmath , then, each request queue @xmath is deterministically bounded
with bound: @xmath , where @xmath .

###### Proof.

We use induction to prove the claim. Base case is trivial since @xmath .
Suppose the claim holds at the beginning of @xmath for @xmath , so that
@xmath Then,

1.  If @xmath , then, it is possible for the queue to increase during
    slot @xmath . However, the increase of the queue within one slot is
    bounded by @xmath . which implies at the beginning of slot @xmath ,
    @xmath

2.  If @xmath , then, according to ( 3.11 ), it is impossible to route
    any request to server @xmath during slot @xmath , and @xmath which
    results in @xmath

Above all, we finished the proof of lemma. ∎

###### Lemma 3.3.2.

The proposed algorithm meets the constraint ( 3.5 ) with probability 1.

###### Proof.

From the queue update rule ( 3.1 ), it follows, @xmath . Taking
telescoping sums from 0 to @xmath gives @xmath . Since @xmath , dividing
both sides by @xmath gives @xmath . Substitute the bound @xmath from
lemma 3.3.1 into above inequality and take limit as @xmath give the
desired result. ∎

#### 3.3.2 Optimal randomized stationary policy

In this section, we introduce a class of algorithms which are
theoretically helpful for doing analysis, but practically impossible to
implement.

Since servers are coupled only through time average constraint ( 3.5 ),
each server @xmath can be viewed as a separate renewal system, thus, it
can be shown that any possible time average service rate @xmath can be
achieved through a frame based stationary randomized service decision,
meaning that the decisions are i.i.d. over frames. Furthermore, it can
be shown that the optimality of ( 3.4 )-( 3.6 ) can be achieved over the
following randomized stationary algorithms: At the beginning of each
time slot @xmath , the data center observes the incoming requests @xmath
and rejecting cost @xmath , then routes @xmath incoming requests to
server @xmath and rejects @xmath requests, both of which are random
functions of @xmath . They satisfy the same instantaneous relation as (
3.6 ). Meanwhile, server @xmath chooses a frame based stationary
randomized service decision @xmath , so that the optimal service rate is
achieved.

If one knows the stationary distribution for @xmath , then, this optimal
control algorithm can be computed using dynamic programming or linear
programming. Moreover, the optimal setup cost @xmath , idle cost @xmath
, and the active state indicator @xmath can also be deduced. Since the
algorithm is stationary, these three cost processes are all ergodic
Markov processes. Let @xmath be the frame length process under this
algorithm. Thus, it follows from the renewal reward theorem that @xmath
, @xmath , @xmath , @xmath and @xmath are all i.i.d. random variables
over frames . Let @xmath , @xmath , @xmath and @xmath be the optimal
time average costs. Let @xmath , @xmath and @xmath be the optimal time
average routing rate, service rate and rejection rate respectively.
Then, by the strong law of large numbers,

  -- -- -- --------
           (3.19)
  -- -- -- --------

  -- -- -- --------
           (3.20)
  -- -- -- --------

  -- -- -- --------
           (3.21)
  -- -- -- --------

  -- -- -- --------
           (3.22)
  -- -- -- --------

Also, notice that @xmath and @xmath depend only on the random variables
@xmath and @xmath , which is i.i.d. over slots. Thus, @xmath and @xmath
are also i.i.d. random variables over slots . By the law of large
numbers,

  -- -------- -- -- --------
     @xmath         (3.23)
     @xmath         (3.24)
  -- -------- -- -- --------

###### Remark 3.3.1.

Since the idle time @xmath and the first two moments of the setup time
are bounded, it follows the first two moments of @xmath are bounded.

#### 3.3.3 Key features of thresholding algorithm

In this part, we compare the algorithm deduced from the two optimization
problems ( 3.8 ) and ( 3.15 ) to that of the best stationary algorithm
in section 3.3.2 , illustrating the key features of the proposed online
algorithm. Define @xmath as the system history up till slot @xmath ,
which includes all the decisions taken and all the random events before
slot @xmath . We first consider ( 3.8 ). For simplicity of notations,
define two random processes @xmath and @xmath as follows

  -- -------- -- --
     @xmath      
                 
     @xmath      
  -- -------- -- --

where @xmath and @xmath .

Given the system information @xmath , the random events @xmath and
@xmath , the solutions ( 3.10 ) and ( 3.11 ) take rejecting and routing
decisions so as to minimize ( 3.8 ) over all possible routing and
rejecting decisions at time slot @xmath . Thus, the proposed algorithm
achieves smaller value on ( 3.8 ) compared to that of the best
stationary algorithm in section 3.3.2 . Formally, this idea can be
stated as the following inequality: @xmath @xmath . Taking expectation
regarding @xmath and @xmath using the fact that the best stationary
algorithm on @xmath and @xmath are i.i.d. over slots (independent of
@xmath ), together with ( 3.23 ) and ( 3.24 ), we get

  -- -- -- --------
           (3.25)
  -- -- -- --------

Similarly, for ( 3.15 ), the proposed service decisions within frame
@xmath minimize @xmath in ( 3.15 ), thus, compared to the best
stationary policy, the inequality ( 3.3.3 ) holds.

Again, using the fact that the optimal stationary algorithm gives i.i.d.
@xmath , @xmath , @xmath and @xmath over frames (independent of @xmath
), as well as ( 3.19 ), ( 3.20 ) and ( 3.22 ), we get

  -- -- -- --------
           (3.27)
  -- -- -- --------

#### 3.3.4 Bounded average of supermartingale difference sequeces

The key feature inequalities ( 3.25 ) and ( 3.27 ) provide us with
bounds on the expectations. The following lemma serves as a stepping
stone passing from expectation bounds to probability 1 bounds. Recall
the basic definition of supermartingale in Definition 2.3.1 . We have
the following strong law of large numbers for supermartingale difference
sequences:

###### Lemma 3.3.3 (Corollary 4.2 of [neely2012stability]).

Let @xmath be a supermartingale difference sequence. If

  -- -- --
        
  -- -- --

then,

  -- -------- --
     @xmath   
  -- -------- --

with probability 1.

With this lemma, we are ready to prove the following result:

###### Lemma 3.3.4.

Under the proposed algorithm, the following hold with probability 1,

  -- -------- -- --------
     @xmath      (3.28)
     @xmath      (3.29)
  -- -------- -- --------

###### Proof.

The key to the proof is treating these two sequences as supermartingale
difference sequences and applying law of large numbers for
supermartingale difference sequences (theorem 4.1 and corollary 4.2 in (
3.28 )).

We first look at the sequence @xmath . Let @xmath . We first prove that
@xmath is a supermartingale. Notice that @xmath , i.e. it is measurable
given all the information before frame @xmath , and @xmath .
Furthermore, @xmath @xmath @xmath , where the only inequality follows
from ( 3.27 ). Thus, it follows @xmath is a supermartingale. Next, we
show that the second moment of supermartingale differences, i.e. @xmath
, is deterministically bounded by a fixed constant for any @xmath . This
part of proof is given in Appendix B. Thus, the following holds: @xmath
. Now, applying Lemma 3.3.3 immediately gives ( 3.28 ).

Similarly, we can prove ( 3.29 ) by proving @xmath is a supermartingale
with bounded second moment on differences using ( 3.23 ), ( 3.24 ) and (
3.25 ). The procedure is almost the same as above and we omitted the
details here for brevity. ∎

###### Corollary 3.3.1.

The following ratio of time averages is upper bounded with probability
1,

@xmath .

###### Proof.

From ( 3.28 ), it follows for any @xmath , there exists an @xmath such
that @xmath implies @xmath . Thus, @xmath . Since @xmath is arbitrary,
take @xmath gives the result. ∎

#### 3.3.5 Near optimal time average cost

The ratio of time averages in corollary 3.3.1 and the true time average
share the same bound, which is proved by the following lemma:

###### Lemma 3.3.5.

The following time average is bounded with probability 1,

  -- -- -- --------
           (3.30)
  -- -- -- --------

where @xmath and @xmath .

The idea of the proof is similar to that of basic renewal theory, which
derives upper and lower bounds for each @xmath within any frame @xmath
using corollary 3.3.1 , thereby showing that as @xmath , the upper and
lower bounds meet. See appendix C for details. With the help of this
lemma, we are able to prove the following near optimal performance
theorem:

###### Theorem 3.3.1.

If @xmath , then the time average total cost under the algorithm is near
optimal on the order of @xmath , i.e. with probability 1,

  -- -- -- --------
           (3.31)
  -- -- -- --------

where @xmath , @xmath and @xmath .

See appendix D for details of proof.

### 3.4 Delay improvement via virtualization

#### 3.4.1 Delay improvement

The algorithm in previous sections optimizes time average cost. However,
it can route requests to idle queues, which increases system delay. This
section considers an improvement in the algorithm that maintains the
same average cost guarantees, but reduces delay. This is done by a
“virtualization” technique that reduces from @xmath server request
queues to only one request queue @xmath . Specifically, the same
Algorithm 1 is run, with queue updates ( 3.1 ) for each of the @xmath
queues @xmath . However, the @xmath processes are now virtual queues
rather than actual queues: Their values are only kept in software. Every
slot @xmath , the data center observes the incoming requests @xmath ,
rejection cost @xmath and virtual queue values, making rejection
decision according to ( 3.10 ) as before. The admitted requests are
queued in @xmath . Meanwhile, each server @xmath makes active/idle
decisions observing its own virtual queue @xmath same as before.
Whenever a server is active, it grabs the requests from request queue
@xmath and serves them. This results in an actual queue updating for the
system:

  -- -- -- --------
           (3.32)
  -- -- -- --------

Fig. 5.2 shows this data center architecture.

#### 3.4.2 Performance guarantee

Since this algorithm does not look at the actual queue @xmath , it is
not clear whether or not the actual request queue would be stabilized
under the proposed algorithm. The following lemma answers the question.
For simplicity, we call the system with @xmath queues, where our
algorithm applies, the virtual system, and call the system with only one
queue the actual system.

###### Lemma 3.4.1.

If @xmath and @xmath , then the virtualization technique stabilizes the
queue @xmath with the bound: @xmath .

###### Proof.

Notice that this bound is @xmath times the individual queue bound in
lemma 3.3.1 , we prove the lemma by showing that the sum-up weights
@xmath in the virtual system always dominates the queue length @xmath .
We prove this by induction. The base case is obvious since @xmath .
Suppose at the beginning of time @xmath , @xmath , then, during time
@xmath , we distinguish between the following two cases:

1.  Not all active servers in actual system have requests to serve. This
    case happens if and only if there are not enough requests in @xmath
    to be served, i.e. @xmath . Thus, according to queue updating rule (
    3.32 ), at the beginning of time slot @xmath , there will be no
    request sitting in the actual queue, i.e. @xmath . Hence, it is
    guaranteed that @xmath .

2.  All active servers in actual system have requests to serve. Notice
    that the virtual system and the actual system have exactly the same
    arrivals, rejections and server active/idle states. Thus, the
    following holds, @xmath @xmath @xmath @xmath , where the first
    inequality follows from induction hypothesis as well as the fact
    that @xmath .

Above all, we proved @xmath . Since each @xmath , the lemma follows. ∎

Since the virtual system and the actual system have exactly the same
cost, and it can be shown that the optimal cost in one queue system is
lower bounded by the optimal cost in @xmath queue system, thus, the near
optimal performance is still guaranteed.

### 3.5 Simulation

In this section, we demonstrate the performance of our proposed
algorithm via extensive simulations. The first simulation runs over
i.i.d. traffic. We show that our algorithm indeed achieves @xmath near
optimality with @xmath delay ( @xmath trade-off), which is predicted by
Lemma 3.3.1 and Theorem 3.3.1 . We then apply our algorithm to a real
data center traffic trace with realistic scale, setup time and cost
being the power consumption. We compare the performance of the proposed
algorithm with several other heuristic algorithms and show that our
algorithm indeed delivers lower delay and saves power.

#### 3.5.1 Near optimality in @xmath queues system

In the first simulation, we consider a relative small scale problem with
i.i.d. generated traffic. We set the number of servers @xmath . The
incoming requests @xmath are integers following a uniform distribution
in @xmath . The request rejecting cost @xmath are also integers
following a uniform distribution in @xmath . The maximum admission
amount @xmath and the maximum idle time @xmath . There is only one idle
option @xmath for each server where the idle cost @xmath . The setup
time follows a geometric distribution with mean @xmath , setup cost
@xmath per slot, service cost @xmath per slot, and the service amount
@xmath follows a uniform distribution over integers. The values @xmath
are generated uniform at random within @xmath and specified in table II.

The algorithm is run for 1 million slots in each trial and each plot
takes the average of these 1 million slots. We compare our algorithm to
the optimal stationary algorithm. The optimal stationary algorithm is
computed using linear program [ fox1966markov ] with the full knowledge
of the statistics of requests and rejecting costs.

In Fig. 5.3 , we show that as our tradeoff parameter @xmath gets larger,
the average cost approaches the optimal value and achieves a near
optimal performance. Furthermore, the cost curve drops rapidly when
@xmath is small and becomes relatively flat when @xmath gets large,
thereby demonstrating our @xmath optimality gap in Theorem 3.3.1 . Fig.
5.4 plots the average sum-up queue size @xmath and shows as @xmath gets
larger, the average sum-up queue size becomes larger. We also plot the
sum of individual queue bound from Lemma 3.3.1 for comparison. We can
see that the real queue size grows linearly with @xmath (although the
constant in Lemma 3.3.1 is not tight due to the much better delay we
obtain here), which demonstrates the @xmath delay bound.

We then tune the requests @xmath to be uniform in @xmath and keep other
parameters unchanged. In Fig. 5.5 , we see that since the request rate
gets larger, we need @xmath to be larger in order to obtain the near
optimality, but still, the near optimality gap scales roughly @xmath .
Fig. 4.6 gives the sum-up average queue length in this case. The average
queue length is larger than that of Fig. 5.4 with linear growth with
respect to @xmath .

#### 3.5.2 Real data center traffic trace and performance evaluation

This second considers a simulation on a real data center traffic
obtained from the open source data sets of the paper [ benson2010network
] . The trace is plotted in Fig. 3.8 . We synthesize different data
chunks from the source so that the trace contains both the steady phase
and increasing phase. The total time duration is 2800 seconds with each
slot equal to 20ms. The peak traffic is 2120 requests per 20 ms, and the
time average traffic over this whole time interval is 654 requests per
20 ms.

We consider a data center consisting of 1060 homogeneous servers. We
assume each server has only one sleep state and the service quantity of
each server at each slot follows a Zipf’s law ¹ ¹ 1 The pdf of Zipf’s
law with parameter @xmath is defined as: @xmath . Thus, the mean of the
distribution is @xmath . with parameter @xmath and @xmath . This gives
the service rate of each server equal to @xmath requests per 20ms. So
the full capacity of the data center is able to support the peak
traffic. Zipf’s law is previously introduced to model a wide scope of
physics, biology, computer science and social science phenomenon ( [
newman2005power ] ), and is adopted in various literatures to simulate
the empirical data center service rate ( [ gandhi2013dynamic ,
gandhi2012sleep ] ). The setup time of each server is geometrically
distributed with success probability equal to @xmath . This gives the
mean setup time 1000 slots (20 seconds). This setup time is previously
shown in [ gandhi2012sleep ] to be a typical time duration for a desktop
to recover from the suspend or hibernate state.

Furthermore, to make a fair comparison with several existing algorithms,
we enforce the front end balancer to accept all requests at each time
slot (so the rejection rate is always 0). The only cost in the system is
then the power consumption. We assume that a server consumes 10 W each
slot when active and 0 W each slot when idle. The setup cost is also 10
W per slot. Moreover, we apply the one queue model described in Section
3.4 for all the rest of the simulations. Following the problem
formulation, the maximum idle time of a server for the proposed
algorithm is @xmath , while no such limit is imposed for any other
benchmark algorithms.

We first run our proposed algorithm over the trace with virtualization
(in Section 3.4 ) for different @xmath values. We set the initial
virtual queue backlog @xmath , and keep 20 servers always on. Fig. 3.9
and Fig. 3.10 plots the running average power consumption and
corresponding queue length for @xmath and 1200, respectively. It can be
seen that as @xmath gets large, the average power consumption does not
improve too much but the queue length changes drastically. This
phenomenon results from the @xmath trade-off of our proposed algorithm.
In view of this fact, we choose @xmath which gives a reasonable delay
performance in Fig. 3.10 .

Next, we compare our proposed algorithm with the same initial setup and
@xmath to the following algorithms:

-   Always-on with @xmath active servers and the rest servers staying on
    the sleep mode. Note that 327 servers can support the average
    traffic over the whole interval which is 654 requests per 20 ms.

-   Always-on with full capacity. This corresponds to keeping all 1060
    servers on at every slot.

-   Reactive. This algorithm is developed in [ gandhi2012sleep ] which
    reacts to the current traffic @xmath and maintains @xmath servers
    on. In the simulation, we choose @xmath to be the average of the
    traffic from the latest 10 slots. If the current active server
    @xmath , then, we turn @xmath servers off, otherwise, we turn @xmath
    servers to the setup state.

-   Reactive with extra capacity. This algorithm is similar to Reactive
    except that we introduce a virtual traffic flow of @xmath jobs per
    slot. So during each time slot @xmath , the algorithm maintains
    @xmath servers on.

Fig. 3.11 - 3.13 plots the average power consumption, queue length and
the number of active servers, respectively. It can be seen that all
algorithms perform pretty well during first half of the trace. For the
second half of the trace, the traffic load is increasing. The Always-on
algorithm with mean capacity does not adapt to the traffic so the queue
length blows up quickly. Because of the long setup time, the number of
active servers in the Reactive algorithm fails to catch up with the
increasing traffic so the queue length also blows up. Our proposed
algorithm minimizes the power consumption while stabilizing the queues,
thereby outperforming both the Always-on and the Reactive algorithm.
Note that the Reactive with extra 200 job capacity is able to achieve a
similar delay performance as our proposed algorithm, but with
significant extra power consumption.

Finally, we evaluate the influence of different sleep modes on the
performance. We keep all the setups the same as before and consider the
sleep modes with sleep power consumption equal to 2 W and 4 W per slot,
respectively. Since the Always-on and the Reactive algorithm do not look
at the sleep power consumption, their decisions remain the same as
before, thus, we superpose the queue length of our proposed algorithm
onto the previous Fig. 3.12 and get the queue length comparison in Fig.
3.14 . We see from the plot that increasing the power consumption during
the sleep mode only slightly increases the queue length of our proposed
algorithm. Fig. 3.16 plots the running average power consumption under
different sleep modes. Despite spending more power on the sleep mode,
the proposed algorithm can still save considerable amount of power
compared to other algorithms while keeping the request queue stable.
This shows that our algorithm is empirically robust to the change of
sleep mode.

### 3.6 Additional lemmas and proofs

#### Appendix A— Proof of Lemma 3.2.1

We have ( 3.33 ), as shown at the bottom of this page, holds,

where the first equality follows from the definition @xmath and the
second equality follows from iterated expectations conditioning on
@xmath and @xmath . For simplicity of notations, let

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

then

  -- -- --
        
  -- -- --

Meanwhile, given the queue length @xmath at frame @xmath , denote the
benchmark solution over pure decisions as

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

Then, for any randomized decision on @xmath and @xmath , its realization
within frame @xmath satisfies the following

  -- -------- --
     @xmath   
  -- -------- --

which implies

  -- -------- --
     @xmath   
  -- -------- --

Taking conditional expectation from both sides gives

  -- -- --
        
        
        
  -- -- --

Thus, it is enough to consider pure decisions only, which boils down to
computing ( 3.34 ). This proves the lemma.

#### Appendix B— Proof of Lemma 3.3.4

This section is dedicated to prove that @xmath is bounded. First of all,
since the idle option set @xmath is finite, denote

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

It is obvious that @xmath , @xmath , @xmath , and @xmath . Combining
with the boundedness of queues in lemma 3.3.1 , it follows

  -- -------- -- --
     @xmath      
                 
     @xmath      
                 
  -- -------- -- --

Let @xmath , it follows

  -- -------- --
     @xmath   
  -- -------- --

Thus,

  -- -- --
        
  -- -- --

Notice that @xmath by ( 3.2 ), where @xmath is upper bonded by @xmath
and @xmath has first four moments bounded by assumption 3.1.2 . Thus,
@xmath is bounded by a fixed constant.

#### Appendix C— Proof of Lemma 3.3.5

###### Proof.

Let’s first abbreviate the notation by defining

  -- -------- -------- --
     @xmath   @xmath   
                       
  -- -------- -------- --

For any @xmath , we can bound the partial sums from above by the
following

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is defined in ( 3.15 ), and @xmath . Thus,

  -- -------- -------- --
     @xmath            
              @xmath   
  -- -------- -------- --

where

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

Thus, this implies that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

We then try to work out an upper bound for @xmath and @xmath
respectively.

1.  Bound for @xmath :

      -- -------- -- --
         @xmath      
         @xmath      
      -- -------- -- --

    where the second inequality follows from corollary 3.3.1 . It
    remains to show that

      -- -- -- --------
               (3.35)
      -- -- -- --------

    Since @xmath , it is enough to show that

      -- -------- -- --------
         @xmath      (3.36)
         @xmath      (3.37)
      -- -------- -- --------

    We prove ( 3.37 ), and ( 3.36 ) is similar. Since each @xmath ,
    where @xmath and @xmath has bounded first four moments, the first
    four moments of @xmath must also be bounded and there exists a
    constant @xmath such that

      -- -- --
            
      -- -- --

    For any @xmath , define a sequence of events

      -- -- --
            
      -- -- --

    According to Markov inequality,

      -- -- --
            
      -- -- --

    Thus,

      -- -- --
            
      -- -- --

    By Borel-Cantelli lemma (lemma 1.6.1 in [ Durrett ] ),

      -- -- --
            
      -- -- --

    which implies

      -- -- --
            
      -- -- --

    Since @xmath is arbitrary, this implies ( 3.37 ). Similarly, ( 3.36
    ) can be proved. Thus, ( 3.35 ) holds and

      -- -------- --
         @xmath   
      -- -------- --

2.  Bound for @xmath :

      -- -------- -- --
         @xmath      
         @xmath      
         @xmath      
         @xmath      
      -- -------- -- --

    where the second inequality follows from ( 3.35 ), the third
    inequality follows from corollary 3.3.1 and the last inequality
    follows from the fact that @xmath .

Above all, we proved the lemma. ∎

#### Appendix D— Proof of Theorem 3.3.1

###### Proof.

Define the drift-plus-penalty(DPP) expression @xmath as follows

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

By simple algebra using the queue updating rule ( 3.1 ), we can work out
the upper bound for @xmath as follows,

  -- -------- -------- --
     @xmath            
              @xmath   
     @xmath            
     @xmath            
                       
  -- -------- -------- --

where @xmath , the last inequality follows from adding @xmath and
subtracting @xmath with the fact that the best randomized stationary
algorithm should also satisfy the constraint ( 3.5 ), i.e. @xmath .

Now we take the partial average of @xmath from 0 to @xmath and take
@xmath ,

  -- -------- -- -- --------
     @xmath         
                    
                    (3.38)
  -- -------- -- -- --------

According to ( 3.29 ),

  -- -- -- --------
           (3.39)
  -- -- -- --------

On the other hand,

  -- -------- -- -- --------
                    
     @xmath         
     @xmath         (3.40)
  -- -------- -- -- --------

where @xmath as defined below ( 3.15 ), the first inequality follows
from the fact that for any @xmath ,

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

and the second inequality follows from lemma 3.3.5 . Substitute ( 3.39 )
and ( 3.40 ) into ( 3.38 ) gives

  -- -------- -- -- --------
     @xmath         (3.41)
  -- -------- -- -- --------

Finally, notice that by telescoping sums,

  -- -------- -------- --
              @xmath   
     @xmath            
     @xmath            
  -- -------- -------- --

Substitute above inequality into ( 3.41 ) and divide @xmath from both
sides give the desired result. ∎

## Chapter 4 Power Aware Wireless File Downloading and Restless Bandit
via Renewal Optimization

In this chapter, we look at another application of the renewal
optimization, namely, the wireless file downloading. We start with a
simple single-user file downloading problem and show that this problem
can be characterized by a 2 state Markov decision process (MDP) with
constraints, for which the drift-plus-penalty (DPP) ratio algorithm
(Algorithm 1 ) applies. We then consider a more realistic multi-user
file downloading and show that this problem is a constrained version of
the well-known restless bandit problem, for which we develop a DPP ratio
indexing heuristic based on the coupled renewal optimization.

### 4.1 System model and problem formulation

Consider a wireless access point, such as a base station or femto node,
that delivers files to @xmath different wireless users. The system
operates in slotted time with time slots @xmath . Each user can download
at most one file at a time. File sizes are random and complete delivery
of a file requires a random number of time slots. A new file request is
made by each user at a random time after it finishes its previous
download. Let @xmath represent the binary file state process for user
@xmath . The state @xmath means that user @xmath is currently active
downloading a file, while the state @xmath means that user @xmath is
currently idle.

Idle times are assumed to be independent and geometrically distributed
with parameter @xmath for each user @xmath , so that the average idle
time is @xmath . Active times depend on the random file size and the
transmission decisions that are made. Every slot @xmath , the access
point observes which users are active and decides to serve a subset of
at most @xmath users, where @xmath is the maximum number of simultaneous
transmissions allowed in the system ( @xmath is assumed throughout). The
goal is to maximize a weighted sum of throughput subject to a total
average power constraint.

The file state processes @xmath are coupled controlled Markov chains
that form a total state @xmath that can be viewed as a restless
multi-armed bandit system . Such problems are complex due to the
inherent curse of dimensionality.

We first compute an online optimal algorithm for 1-user systems, i.e.,
the case @xmath . This simple case avoids the curse of dimensionality
and provides valuable intuition. The optimal policy here is computed via
the drift-plus-penalty (DPP) ratio algorithm. The resulting algorithm
makes a greedy transmission decision that affects success probability
and power usage. Next, the algorithm is extended as a low complexity
online heuristic for the @xmath -user problem, which we call the “DPP
ratio indexing”. The heuristic has the following desirable properties:

-   Implementation of the @xmath -user heuristic is as simple as
    comparing indices for @xmath different 1-user problems.

-   The @xmath -user heuristic is analytically shown to meet the desired
    average power constraint.

-   The @xmath -user heuristic is shown in simulation to perform well
    over a wide range of parameters. Specifically, it is very close to
    optimal for example cases where an offline optimal can be computed.

-   The @xmath -user heuristic is shown to be optimal in a special case
    with no power constraint and with certain additional assumptions.
    The optimality proof uses a theory of stochastic coupling for
    queueing systems [ tassiulas1993dynamic ] .

Prior work on wireless optimization uses Lyapunov functions to maximize
throughput in cases where the users are assumed to have an infinite
amount of data to send [ neely2008fairness , eryilmaz2007fair ,
georgiadis2006resource , stolyar2005maximizing , tassiulas1993dynamic ]
, or when data arrives according to a fixed rate process that does not
depend on delays in the network (which necessitates dropping data if the
arrival rate vector is outside of the capacity region, e.g. [
neely2008fairness ] ). These models do not consider the interplay
between arrivals at the transport layer and file delivery at the network
layer. For example, a web user in a coffee shop may want to evaluate the
file she downloaded before initiating another download. The current work
captures this interplay through the binary file state processes @xmath .
This creates a complex problem of coupled Markov chains. This problem is
fundamental to file downloading systems. The modeling and analysis of
these systems is a significant contribution of the current thesis.

To understand this issue, suppose the data arrival rate is fixed and
does not adapt to the service received over the network. If this arrival
rate exceeds network capacity by a factor of two, then at least half of
all data must be dropped. This can result in an unusable data stream,
possibly one that contains every odd-numbered packet. A more practical
model assumes that full files must be downloaded and that new downloads
are only initiated when previous ones are completed. A general model in
this direction would allow each user to download up to @xmath files
simultaneously. This thesis considers the case @xmath , so that each
user is either actively downloading a file, or is idle. ¹ ¹ 1 One way to
allow a user @xmath to download up to @xmath files simultaneously is as
follows: Define @xmath virtual users with separate binary file state
processes. The transition probability from idle to active in each of
these virtual users is @xmath . The conditional rate of total new
arrivals for user @xmath (given that @xmath files are currently in
progress) is then @xmath for @xmath . The resulting system for @xmath
users has a nontrivial Markov structure with @xmath states.

Since the current problem includes both time-average constraints (on
average power expenditure) and instantaneous constraints which restrict
the number of users that can be served on one slot, it is more
complicated than the weakly coupled systems discussed in previous
chapters. More specifically, The latter service restriction is similar
to a traditional restless multi-armed bandit (RMAB) system [
whittle1988restless ] .

RMAB problem considers a population of @xmath parallel MDPs that
continue evolving whether in operation or not (although in different
rules). The goal is to choose the MDPs in operation during each time
slot so as to maximize the expected reward subject to a constraint on
the number of MDPs in operation. The problem is in general complex (see
P-SPACE hardness results in [ papadimitriou1999complexity ] ). A
standard low-complexity heuristic for such problems is the Whittle’s
index technique [ whittle1988restless ] . However, the Whittle’s index
framework applies only when there are two options on each state (active
and passive). Further, it does not consider the additional time average
cost constraints. The DPP ratio indexing algorithm developed in the
current work can be viewed as an alternative indexing scheme that can
always be implemented and that incorporates additional time average
constraints. It is likely that the techniques of the current work can be
extended to other constrained RMAB problems. Prior work in [
tassiulas1993dynamic ] develops a Lyapunov drift method for queue
stability, and work in [ Neely2010 ] develops a drift-plus-penalty (DPP)
ratio method for optimization over renewal systems. The current work is
the first to use these techniques as a low complexity heuristic for
multidimensional Markov problems.

Work in [ tassiulas1993dynamic ] uses the theory of stochastic coupling
to show that a longest connected queue algorithm is delay optimal in a
multi-dimensional queueing system with special symmetric assumptions .
The problem in [ tassiulas1993dynamic ] is different from that of the
current work. However, a similar coupling approach is used below to show
that, for a special case with no power constraint, the DPP ratio
indexing algorithm is throughput optimal in certain asymmetric cases. As
a consequence, the proof shows the policy is also optimal for a
different setting with @xmath servers, @xmath single-buffer queues, and
arbitrary packet arrival rates @xmath .

### 4.2 Single user scenario

Consider a file downloading system that consists of only one user that
repeatedly downloads files. Let @xmath be the file state process of the
user. State “1” means there is a file in the system that has not
completed its download, and “0” means no file is waiting. The length of
each file is independent and is either exponentially distributed or
geometrically distributed (described in more detail below). Let @xmath
denote the expected file size in bits. Time is slotted. At each slot in
which there is an active file for downloading, the user makes a service
decision that affects both the downloading success probability and the
power expenditure. After a file is downloaded, the system goes idle
(state @xmath ) and remains in the idle state for a random amount of
time that is independent and geometrically distributed with parameter
@xmath .

A transmission decision is made on each slot @xmath in which @xmath .
The decision affects the number of bits that are sent, the probability
these bits are successfully received, and the power usage. Let @xmath
denote the decision variable at slot @xmath and let @xmath represent an
abstract action set. The set @xmath can represent a collection of
modulation and coding options for each transmission. Assume also that
@xmath contains an idle action denoted as “0.” The decision @xmath
determines the following two values:

-   The probability of successfully downloading a file @xmath , where
    @xmath with @xmath .

-   The power expenditure @xmath , where @xmath is a nonnegative
    function with @xmath .

The user chooses @xmath whenever @xmath . The user chooses @xmath for
each slot @xmath in which @xmath , with the goal of maximizing
throughput subject to a time average power constraint. The example where
the decision set @xmath is finite can be found in the simulation
experiment section. Here is a simple example where the decision can be
continuous:

###### Example 1.

Let @xmath be the set of all possible power allocation options, i.e.
@xmath where @xmath are constants. Then, @xmath , @xmath and the success
probability of downloading a file can be @xmath .

The problem can be described by a two state Markov decision process with
binary state @xmath . Given @xmath , a file is currently in the system.
This file will finish its download at the end of the slot with
probability @xmath . Hence, the transition probabilities out of state
@xmath are:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (4.1)
     @xmath   @xmath   @xmath      (4.2)
  -- -------- -------- -------- -- -------

Given @xmath , the system is idle and will transition to the active
state in the next slot with probability @xmath , so that:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (4.3)
     @xmath   @xmath   @xmath      (4.4)
  -- -------- -------- -------- -- -------

Define the throughput, measured by bits per slot, as:

  -- -------- --
     @xmath   
  -- -------- --

The file downloading problem reduces to the following:

  -- -- -------- -- -------
        @xmath      (4.5)
        @xmath      (4.6)
        @xmath      (4.7)
                    (4.8)
  -- -- -------- -- -------

where @xmath is a positive constant that determines the desired average
power constraint.

#### 4.2.1 The memoryless file size assumption

The above model assumes that file completion success on slot @xmath
depends only on the transmission decision @xmath , independent of
history. This implicitly assumes that file length distributions have a
memoryless property where the residual file length is independent of the
amount already delivered. Further, it is assumed that if the controller
selects a transmission rate that is larger than the residual bits in the
file, the remaining portion of the transmission is padded with fill bits
. This ensures error events provide no information about the residual
file length beyond the already known 0/1 binary file state. Of course,
error probability might be improved by removing padded bits. However,
this affects only the last transmission of a file and has negligible
impact when expected file size is large in comparison to the amount that
can be transmitted in one slot. Note that padding is not needed in the
special case when all transmissions send one fixed length packet.

The memoryless property holds when each file @xmath has independent
length @xmath that is exponentially distributed with mean length @xmath
bits, so that:

  -- -------- --
     @xmath   
  -- -------- --

For example, suppose the transmission rate @xmath (in units of
bits/slot) and the transmission success probability @xmath are given by
general functions of @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

Then the file completion probability @xmath is the probability that the
residual amount of bits in the file is less than or equal to @xmath ,
and that the transmission of these residual bits is a success. By the
memoryless property of the exponential distribution, the residual file
length is distributed the same as the original file length. Thus:

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (4.9)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Alternatively, history independence holds when each file @xmath consists
of a random number @xmath of fixed length packets, where @xmath is
geometrically distributed with mean @xmath . Assume each transmission
sends exactly one packet, but different power levels affect the
transmission success probability @xmath . Then:

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

The memoryless file length assumption allows the file state to be
modeled by a simple binary-valued process @xmath . However, actual file
sizes may not have an exponential or geometric distribution. One way to
treat general distributions is to approximate the file sizes as being
memoryless by using a @xmath function defined by either ( 4.9 ) or (
4.10 ), formed by matching the average file size @xmath or average
number of packets @xmath . The decisions @xmath are made according to
the algorithm below, but the actual event outcomes that arise from these
decisions are not memoryless. A simulation comparison of this
approximation is provided in Section 4.5 , where it is shown to be
remarkably accurate (see Fig. 4.7 ).

The algorithm in this section optimizes over the class of all algorithms
that do not use residual file length information. This maintains low
complexity by ensuring a user has a binary-valued Markov state @xmath .
While a system controller might know the residual file length,
incorporating this knowledge creates a Markov decision problem with an
infinite number of states (one for each possible value of residual
length) which significantly complicates the scenario.

#### 4.2.2 DPP ratio optimization

This subsection develops an online algorithm for problem ( 4.5 )-( 4.8
). This algorithm follows from Algorithm 1 in Chapter 1 with some
customizations towards this application. First, notice that file state “
@xmath ” is recurrent under any decisions for @xmath . Denote @xmath as
the @xmath -th time when the system returns to state “1.” Define the
renewal frame as the time period between @xmath and @xmath . Define the
frame size :

  -- -------- --
     @xmath   
  -- -------- --

Notice that @xmath for any frame @xmath in which the file does not
complete its download. If the file is completed on frame @xmath , then
@xmath , where @xmath is a geometric random variable with mean @xmath .
Each frame @xmath involves only a single decision @xmath that is made at
the beginning of the frame. Thus, the total power used over the duration
of frame @xmath is:

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

We treat the time average constraint in ( 4.6 ) using a virtual queue
@xmath that is updated every frame @xmath by:

  -- -- -- --------
           (4.12)
  -- -- -- --------

with initial condition @xmath . The algorithm is then parameterized by a
constant @xmath which affects a performance tradeoff. At the beginning
of the @xmath -th renewal frame, the user observes virtual queue @xmath
and chooses @xmath to maximize the following drift-plus-penalty (DPP)
ratio:

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

The numerator of the above ratio adds a “queue drift term” @xmath to the
“current reward term” @xmath . The intuition is that it is desirable to
have a large value of current reward, but it is also desirable to have a
large drift (since this tends to decrease queue size). Creating a
weighted sum of these two terms and dividing by the expected frame size
gives a simple index. The next subsections show that, for the context of
the current work, this index leads to an algorithm that pushes
throughput arbitrarily close to optimal (depending on the chosen @xmath
parameter) with a strong sample path guarantee on average power
expenditure.

The denominator in ( 4.13 ) can easily be computed via the transition
model ( 4.1 )-( 4.4 ):

  -- -- -- --------
           (4.14)
  -- -- -- --------

Thus, ( 4.13 ) is equivalent to

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

This gives the following Algorithm 5 for the single-user case:

###### Algorithm 5.

-    At each time @xmath , the user observes virtual queue @xmath and
    chooses @xmath as the solution to ( 4.15 ) (where ties are broken
    arbitrarily).

-    The value @xmath is computed according to ( 4.12 ) at the end of
    the @xmath -th frame.

The expected performance analysis of this algorithm follows from that of
Section 1.1.4 and we omit the details for brevity. In the following, we
give a stronger probability 1 performance analysis taking into account
the special property of the algorithm in this customized setting.

#### 4.2.3 Average power constraints via queue bounds

In this section, we show that the proposed algorithm makes the virtual
queue deterministically bounded.

###### Lemma 4.2.1.

If there is a constant @xmath such that @xmath for all @xmath , then:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

From ( 4.12 ), we know that for each frame @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Rearranging terms and using @xmath gives:

  -- -------- --
     @xmath   
  -- -------- --

Fix @xmath . Summing over @xmath gives:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The sum power over the first @xmath frames is the same as the sum up to
time @xmath , and so:

  -- -------- --
     @xmath   
  -- -------- --

Dividing by @xmath gives:

  -- -------- --
     @xmath   
  -- -------- --

Taking @xmath , then,

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

Now for each positive integer @xmath , let @xmath be the integer such
that @xmath . Since power is only used at the first slot of a frame, one
has:

  -- -------- --
     @xmath   
  -- -------- --

Taking a @xmath as @xmath and using ( 4.16 ) yields the result. ∎

In order to show that the queue process under our proposed algorithm is
deterministically bounded, we need the following assumption:

###### Assumption 4.2.1.

The following quantities are finite and strictly positive:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Lemma 4.2.2.

Suppose Assumption 4.2.1 holds. If @xmath , then under our algorithm we
have for all @xmath :

  -- -- --
        
  -- -- --

###### Proof.

First, consider the case when @xmath . From ( 4.12 ) and the fact that
@xmath for all @xmath , it is clear the queue can never increase, and so
@xmath for all @xmath .

Next, consider the case when @xmath . We prove the assertion by
induction on @xmath . The result trivially holds for @xmath . Suppose it
holds at @xmath for @xmath , so that:

  -- -------- --
     @xmath   
  -- -------- --

We are going to prove that the same holds for @xmath . There are two
cases:

1.  @xmath . In this case we have by ( 4.12 ):

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

2.  @xmath . In this case, we use proof by contradiction. If @xmath then
    the queue cannot increase, so:

      -- -------- --
         @xmath   
      -- -------- --

    On the other hand, if @xmath then @xmath and so the numerator in (
    4.15 ) satisfies:

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
      -- -------- -------- -------- --

    and so the maximizing ratio in ( 4.15 ) is negative. However, the
    maximizing ratio in ( 4.15 ) cannot be negative because the
    alternative choice @xmath increases the ratio to 0. This
    contradiction implies that we cannot have @xmath .

∎

The above is a sample path result that only assumes parameters satisfy
@xmath , @xmath , and @xmath . Thus, the algorithm meets the average
power constraint even if it uses incorrect values for these parameters.
The next subsection provides a throughput optimality result when these
parameters match the true system values.

#### 4.2.4 Optimality over randomized algorithms

Consider the following class of i.i.d. randomized algorithms : Let
@xmath be non-negative numbers defined for each @xmath , and suppose
they satisfy @xmath . Let @xmath represent a policy that, every slot
@xmath for which @xmath , chooses @xmath by independently selecting
strategy @xmath with probability @xmath . Then @xmath are independent
and identically distributed (i.i.d.) over frames @xmath . Under this
algorithm, it follows by the law of large numbers that the throughput
and power expenditure satisfy (with probability 1):

  -- -------- -------- -- --
     @xmath   @xmath      
     @xmath   @xmath      
  -- -------- -------- -- --

It can be shown that optimality of problem ( 4.5 )-( 4.8 ) can be
achieved over this class. Thus, there exists an i.i.d. randomized
algorithm @xmath that satisfies:

  -- -- -------- -------- -- --------
        @xmath   @xmath      (4.17)
        @xmath   @xmath      (4.18)
  -- -- -------- -------- -- --------

where @xmath is the optimal throughput for the problem ( 4.5 )-( 4.8 ).

#### 4.2.5 Key feature of the drift-plus-penalty ratio

Define @xmath as the system history up to frame @xmath , which includes
which includes the actions taken @xmath frame lengths @xmath , the busy
period in each frame, the idle period in each frame, and the queue value
@xmath (since this is determined by the random events before frame
@xmath ). Consider the algorithm that, on frame @xmath , observes @xmath
and chooses @xmath according to ( 4.15 ). The following key feature of
this algorithm can be shown (see [ Neely2010 ] for related results):

  -- -- --
        
  -- -- --

where @xmath is any (possibly randomized) alternative decision that is
based only on @xmath . This is an intuitive property: By design, the
algorithm in ( 4.15 ) observes @xmath and then chooses a particular
action @xmath to minimize the ratio over all deterministic actions.
Thus, as can be shown, it also minimizes the ratio over all potentially
randomized actions. Using the (randomized) i.i.d. decision @xmath from (
4.17 )-( 4.18 ) in the above and noting that this alternative decision
is independent of @xmath gives:

  -- -- -- --------
           (4.19)
  -- -- -- --------

#### 4.2.6 Performance theorem

###### Theorem 4.2.1.

Suppose Assumption 4.2.1 holds. The proposed algorithm achieves the
constraint @xmath and yields throughput satisfying (with probability 1):

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

where @xmath is a constant. ² ² 2 The constant @xmath is independent of
@xmath and is given in the proof.

###### Proof.

First, for any fixed @xmath , Lemma 4.2.2 implies that the queue is
deterministically bounded. Thus, according to Lemma 4.2.1 , the proposed
algorithm achieves the constraint

  -- -------- --
     @xmath   
  -- -------- --

The rest is devoted to proving the throughput guarantee ( 4.20 ).

Define:

  -- -------- --
     @xmath   
  -- -------- --

We call this a Lyapunov function . Define a frame-based Lyapunov Drift
as:

  -- -------- --
     @xmath   
  -- -------- --

According to ( 4.12 ) we get

  -- -- --
        
  -- -- --

Thus:

  -- -------- --
     @xmath   
  -- -------- --

Taking a conditional expectation of the above given @xmath and recalling
that @xmath includes the information @xmath gives:

  -- -- -- --------
           (4.21)
  -- -- -- --------

where @xmath is a constant that satisfies the following for all possible
histories @xmath :

  -- -- --
        
  -- -- --

Such a constant @xmath exists because the power @xmath is
deterministically bounded due to Assumption 4.2.1 , and the frame sizes
@xmath are bounded in second moment regardless of history according to (
4.14 ).

Adding the “penalty” @xmath to both sides of ( 4.21 ) gives:

  -- -- --
        
        
        
  -- -- --

Expanding @xmath in the denominator of the last term gives:

  -- -- --
        
        
  -- -- --

Substituting ( 4.19 ) into the above expression gives:

  -- -- -- --------
           
           
           (4.22)
  -- -- -- --------

Rearranging gives:

  -- -- -- --------
           (4.23)
  -- -- -- --------

This implies that @xmath is a supermartingale difference sequence.
Furthermore, we already know the queue @xmath is deterministically
bounded, it follows that:

  -- -- --
        
  -- -- --

This, together with ( 4.23 ), implies by Lemma 3.3.3 that (with
probability 1):

  -- -- --
        
  -- -- --

Thus, for any @xmath one has for all sufficiently large @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Rearranging implies that for all sufficiently large @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the final inequality holds because @xmath for all @xmath . Thus:

  -- -------- --
     @xmath   
  -- -------- --

The above holds for all @xmath . Taking a limit as @xmath implies:

  -- -------- --
     @xmath   
  -- -------- --

Notice that @xmath only changes at the boundary of each frame and
remains 0 within the frame. Thus, we can replace the sum over frames
@xmath by a sum over slots @xmath . The desired result follows. ∎

The theorem shows that throughput can be pushed within @xmath of the
optimal value @xmath , where @xmath can be chosen as large as desired to
ensure throughput is arbitrarily close to optimal. The tradeoff is a
queue bound that grows linearly with @xmath according to Lemma 4.2.2 ,
which affects the convergence time required for the constraints to be
close to the desired time averages (as described in the proof of Lemma
4.2.1 ).

### 4.3 Multi-user file downloading

This section considers a multi-user file downloading system that
consists of @xmath single-user subsystems. Each subsystem is similar to
the single-user system described in the previous section. Specifically,
for the @xmath -th user (where @xmath ):

-   The file state process is @xmath .

-   The transmission decision is @xmath , where @xmath is an abstract
    set of transmission options for user @xmath .

-   The power expenditure on slot @xmath is @xmath .

-   The success probability on a slot @xmath for which @xmath is @xmath
    , where @xmath is the function that describes file completion
    probability for user @xmath .

-   The idle period parameter is @xmath .

-   The average file size is @xmath bits.

Assume that the random variables associated with different subsystems
are mutually independent. The resulting Markov decision problem has
@xmath states, as shown in Fig. 4.1 . The transition probabilities for
each active user depends on which users are selected for transmission
and on the corresponding transmission modes. This is a restless bandit
system because there can also be transitions for non-selected users
(specifically, it is possible to transition from inactive to active).

To control the downloading process, there is a central server with only
@xmath threads ( @xmath ), meaning that at most @xmath jobs can be
processed simultaneously . So at each time slot, the server has to make
decisions selecting at most @xmath out of @xmath users to transmit a
portion of their files. These decisions are further restricted by a
global time average power constraint. The goal is to maximize the
aggregate throughput, which is defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are a collection of positive weights that can be used to
prioritize users. Thus, this multi-user file downloading problem reduces
to the following:

  -- -- -------- -- --------
        @xmath      (4.24)
        @xmath      (4.25)
        @xmath      (4.26)
        @xmath      (4.27)
        @xmath      (4.28)
  -- -- -------- -- --------

where the constraints ( 4.27 )-( 4.28 ) hold for all @xmath and @xmath ,
and where @xmath is the indicator function defined as:

  -- -- --
        
  -- -- --

#### 4.3.1 DPP ratio indexing algorithm

This section develops our indexing algorithm for the multi-user case
using the single-user case as a stepping stone. The major difficulty is
the instantaneous constraint @xmath . Temporarily neglecting this
constraint, we use Lyapunov optimization to deal with the time average
power constraint first.

We introduce a virtual queue @xmath , which is again 0 at @xmath .
Instead of updating it on a frame basis, the server updates this queue
every slot as follows:

  -- -- -- --------
           (4.29)
  -- -- -- --------

Define @xmath as the set of users beginning their renewal frames at time
@xmath , so that @xmath for all such users. In general, @xmath is a
subset of @xmath . Define @xmath as the number of users in the set
@xmath .

At each time slot @xmath , the server observes the queue state @xmath
and chooses @xmath in a manner similar to the single-user case.
Specifically, for each user @xmath define:

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

This is similar to the expression ( 4.15 ) used in the single-user
optimization. Call @xmath a reward . Now define an index for each
subsystem @xmath by:

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

which is the maximum possible reward one can get from the @xmath -th
subsystem at time slot @xmath . Thus, it is natural to define the
following myopic algorithm: Find the (at most) @xmath subsystems in
@xmath with the greatest rewards, and serve these with their
corresponding optimal @xmath options in @xmath that maximize @xmath .

###### Algorithm 6.

-    At each time slot @xmath , the server observes virtual queue state
    @xmath and computes the indices using ( 4.31 ) for all @xmath .

-    Activate the @xmath subsystems with greatest indices, using their
    corresponding actions @xmath that maximize @xmath .

-    Update @xmath according to ( 4.29 ) at the end of each slot @xmath
    .

#### 4.3.2 Theoretical performance analysis

In this subsection, we show that the above algorithm always satisfies
the desired time average power constraint. We adopt the following
assumption:

###### Assumption 4.3.1.

The following quantities are finite and strictly positive.

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Lemma 4.3.1.

Suppose Assumption 4.3.1 holds. Then, the queue @xmath is
deterministically bounded under Algorithm 6 . Specifically, we have for
all @xmath :

  -- -- --
        
  -- -- --

###### Proof.

First, consider the case when @xmath . Since @xmath , it is clear from
the updating rule ( 4.29 ) that @xmath will remain 0 for all @xmath .

Next, consider the case when @xmath . We prove the assertion by
induction on @xmath . The result trivially holds for @xmath . Suppose at
@xmath , we have:

  -- -------- --
     @xmath   
  -- -------- --

We are going to prove that the same statement holds for @xmath . We
further divide it into two cases:

1.  @xmath . In this case, since the queue increases by at most @xmath
    on one slot, we have:

      -- -------- --
         @xmath   
      -- -------- --

2.  @xmath . In this case, since @xmath , there is no possibility that
    @xmath unless @xmath . Thus, the DPP ratio indexing algorithm of
    minimizing ( 4.30 ) chooses @xmath for all @xmath . Thus, all
    indices are 0. This implies that @xmath cannot increase, and we get
    @xmath .

∎

###### Theorem 4.3.1.

The proposed DPP ratio indexing algorithm achieves the constraint:

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

First of all, similar to Lemma 4.2.1 , one can show that if @xmath for
some constant @xmath and any @xmath , then, @xmath . Using Lemma 4.3.1
we finish the proof. ∎

### 4.4 Multi-user optimality in a special case

In general, it is very difficult to prove optimality of the above
multi-user algorithm. There are mainly two reasons. The first reason is
that multiple users might renew themselves asynchronously, making it
difficult to define a “renewal frame” for the whole system. Thus, the
proof technique in Theorem 1 is infeasible. The second reason is that,
even without the time average constraint, the problem degenerates into a
standard restless bandit problem where the optimality of indexing is not
guaranteed.

This section considers a special case of the multi-user file downloading
problem where the DPP ratio indexing algorithm is provably optimal. The
special case has no time average power constraint. Further, for each
user @xmath :

-   Each file consists of a random number of fixed length packets with
    mean @xmath .

-   The decision set @xmath , where 0 stands for “idle” and 1 stands for
    “download.” If @xmath , then user @xmath successfully downloads a
    single packet.

-   @xmath .

-   Idle time is geometrically distributed with mean @xmath .

-   The special case @xmath is assumed .

The assumption that the file length and idle time parameters @xmath and
@xmath satisfy @xmath is restrictive. However, there exists certain
queueing system which admits exactly the same markov dynamics as the
system considered here when the assumption holds (described in Section
4.4.1 below). More importantly, it allows us to implement the stochastic
coupling idea to prove the optimality.

The goal is to maximize the sum throughput (in units of packets/slot),
which is defined as:

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

In this special case, the multi-user file downloading problem reduces to
the following:

  -- -- -------- -- --------
        @xmath      (4.33)
        @xmath      (4.34)
        @xmath      (4.35)
        @xmath      (4.36)
        @xmath      (4.37)
  -- -- -------- -- --------

where the equality ( 4.37 ) uses the fact that @xmath . A picture that
illustrates the Markov structure of constraints ( 4.35 )-( 4.37 ) is
given in Fig. 4.2

#### 4.4.1 A system with @xmath single-buffer queues

The above model, with the assumption @xmath , is structurally equivalent
to the following: Consider a system of @xmath single-buffer queues,
@xmath servers, and independent Bernoulli packet arrivals with rates
@xmath to each queue @xmath . This considers packet arrivals rather than
file arrivals , so there are no file length variables and no parameters
@xmath in this interpretation. Let @xmath be the binary-valued vector of
packet arrivals on slot @xmath , assumed to be i.i.d. over slots and
independent in each coordinate. Assume all packets have the same size
and each queue has a single buffer that can store just one packet. Let
@xmath be 1 if queue @xmath has a packet at the beginning of slot @xmath
, and @xmath else. Each server can transmit at most 1 packet per slot.
Let @xmath be 1 if queue @xmath is served on slot @xmath , and @xmath
else. An arrival @xmath occurs at the end of slot @xmath and is accepted
only if queue @xmath is empty at the end of the slot (such as when it
was served on that slot). Packets that are not accepted are dropped. The
Markov dynamics are described by the same figure as before, namely, Fig.
4.2 . Further, the problem of maximizing throughput is given by the same
equations ( 4.33 )-( 4.37 ). Thus, although the variables of the two
problems have different interpretations, the problems are structurally
equivalent. For simplicity of exposition, the remainder of this section
uses this single-buffer queue interpretation.

#### 4.4.2 Optimality of the indexing algorithm

Since there is no power constraint, for any @xmath the DPP ratio
indexing policy ( 4.31 ) in Section 4.3.1 reduces to the following
(using @xmath , @xmath ): If there are fewer than @xmath non-empty
queues, serve all of them. Else, serve the @xmath non-empty queues with
the largest values of @xmath , where:

  -- -------- --
     @xmath   
  -- -------- --

Thus, the DPP ratio indexing algorithm in this context reduces to
serving the (at most @xmath ) non-empty queues with the largest @xmath
values each time slot. For the remainder of this section, this is called
the Max- @xmath policy. The following theorem shows that Max- @xmath is
optimal in this context.

###### Theorem 4.4.1.

The Max- @xmath policy is optimal for the problem ( 4.33 )-( 4.37 ). In
particular, under the single-buffer queue interpretation, it maximizes
throughput over all policies that transmit on each slot @xmath without
knowledge of the arrival vector @xmath .

For the @xmath single-buffer queue interpretation, the total throughput
is equal to the raw arrival rate @xmath minus the packet drop rate.
Intuitively, the reason Max- @xmath is optimal is that it chooses to
leave packets in the queues that are least likely to induce packet
drops. An example comparison of the throughput gap between Max- @xmath
and Min- @xmath policies is given in Section 4.6 .

The proof of Theorem 4.4.1 is divided into two parts. The first part
uses stochastic coupling techniques to prove that Max- @xmath dominates
all alternative work-conserving policies. A policy is work-conserving if
it does not allow any server to be idle when it could be used to serve a
non-empty queue. The second part of the proof shows that throughput
cannot be increased by considering non-work-conserving policies.

#### 4.4.3 Preliminaries on stochastic coupling

Consider two discrete time processes @xmath and @xmath . The notation
@xmath means that @xmath and @xmath are stochastically equivalent , in
that they are described by the same probability law. Formally, this
means that their joint distributions are the same, so for all @xmath and
all @xmath :

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

The notation @xmath means that @xmath is stochastically less than or
equal to @xmath , as defined by the following theorem.

###### Theorem 4.4.2.

( [ tassiulas1993dynamic ] ) The following three statements are
equivalent:

1.  @xmath .

2.  @xmath @xmath for all @xmath , all @xmath , and for all functions
    @xmath that are measurable and nondecreasing in all coordinates.

3.   There exist two stochastic processes @xmath and @xmath on a common
    probability space that satisfy @xmath , @xmath , and @xmath for
    every @xmath .

The following additional notation is used in the proof of Theorem 4.4.1
.

-   Arrival vector @xmath , where @xmath @xmath . Each @xmath is an
    independent binary random variable that takes @xmath w.p. @xmath and
    @xmath w.p. @xmath .

-   Buffer state vector @xmath , where @xmath @xmath . So @xmath if
    queue @xmath has a packet at the beginning of slot @xmath , and
    @xmath else.

-   Total packet process @xmath , where @xmath represents the total
    number of packets in the system on slot @xmath . Since each queue
    can hold at most one packet, we have @xmath for all slots @xmath .

#### 4.4.4 Stochastic ordering of buffer state process

The next lemma is the key to proving Theorem 4.4.1 . The lemma considers
the multi-queue system with a fixed but arbitrary initial buffer state
@xmath . The arrival process @xmath is as defined above. Let @xmath be
the total packet process under the Max- @xmath policy. Let @xmath be the
corresponding process starting from the same initial state @xmath and
having the same arrivals @xmath , but with an arbitrary work-conserving
policy @xmath .

###### Lemma 4.4.1.

The total packet processes @xmath and @xmath satisfy:

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

###### Proof.

Without loss of generality, assume the queues are sorted so that @xmath
. Define @xmath as the buffer state vector under policy @xmath . Define
@xmath as the corresponding buffer states under the Max- @xmath policy.
By assumption the initial states satisfy @xmath . Next, we construct a
third process @xmath with a modified arrival vector process @xmath and a
corresponding buffer state vector @xmath (with the same initial state
@xmath ), which satisfies:

1.  @xmath is also generated from the Max- @xmath policy.

2.  @xmath . Since the total packet process is completely determined by
    the initial state, the scheduling policy, and the arrival process,
    it is enough to construct @xmath so that it is of the same
    probability law as @xmath .

3.  @xmath .

Since the arrival process @xmath is i.i.d. over slots, in order to
guarantee 2) and 3), it is sufficient to construct @xmath coupled with
@xmath for each @xmath so that the following two properties hold for all
@xmath :

-   The random variables @xmath and @xmath have the same probability
    law. Specifically, both produce arrivals according to Bernoulli
    processes that are independent over queues and over time, with
    @xmath for all @xmath .

-   For all @xmath ,

      -- -------- -- --------
         @xmath      (4.39)
      -- -------- -- --------

The construction is based on an induction.

At @xmath we have @xmath . Thus, ( 4.39 ) naturally holds for @xmath .
Now fix @xmath and assume ( 4.39 ) holds for all slots up to time @xmath
. If @xmath , further assume the arrivals @xmath have been constructed
to have the same probability law as @xmath . Since arrivals on slot
@xmath occur at the end of slot @xmath , the arrivals @xmath must be
constructed. We are going to show there exists an @xmath that is coupled
with @xmath so that it has the same probability law and it also ensures
( 4.39 ) holds for @xmath .

Since arrivals occur after the transmitting action, we divide the
analysis into two parts. First, we analyze the temporary buffer states
after the transmitting action but before arrivals occur . Then, we
define arrivals @xmath at the end of slot @xmath to achieve the desired
coupling.

Define @xmath and @xmath as the temporary buffer states right after the
transmitting action at slot @xmath but before arrivals occur under
policy @xmath and policy Max- @xmath , respectively. Thus, for each
queue @xmath :

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.40)
     @xmath   @xmath   @xmath      (4.41)
  -- -------- -------- -------- -- --------

where @xmath and @xmath are the slot @xmath decisions under policy
@xmath and Max- @xmath , respectively. Since ( 4.39 ) holds for @xmath
on slot @xmath , the total number of packets at the start of slot @xmath
under policy @xmath is less than or equal to that of using Max- @xmath .
Since both policies @xmath and Max- @xmath are work-conserving, it is
impossible for policy @xmath to transmit more packets than Max- @xmath
during slot @xmath . This implies:

  -- -------- -- --------
     @xmath      (4.42)
  -- -------- -- --------

Indeed, if @xmath transmits the same number of packets as Max- @xmath on
slot @xmath , then ( 4.42 ) clearly holds. On the other hand, if @xmath
transmits fewer packets than Max- @xmath , it must transmit fewer than
@xmath packets (since @xmath is the number of servers). In this case,
the work-conserving nature of @xmath implies that all non-empty queues
were served, so that @xmath for all @xmath and ( 4.42 ) again holds. We
now claim the following holds:

###### Lemma 4.4.2.

  -- -------- -- --------
     @xmath      (4.43)
  -- -------- -- --------

###### Proof.

See Section 4.6 . ∎

Now let @xmath and @xmath be the subscript of @xmath -th empty temporary
buffer (with order starting from the first queue) corresponding to
@xmath and @xmath , respectively. It follows from ( 4.43 ) that the
@xmath system on slot @xmath has at least as many empty temporary buffer
states as the Max- @xmath policy, and:

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

where @xmath is the the number of empty temporary buffer states under
Max- @xmath at time slot @xmath . Since @xmath if and only if @xmath , (
4.44 ) further implies that

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

Now construct the arrival vector @xmath for the system with the Max-
@xmath policy in the following way:

  -- -------- -------- -- --------
     @xmath   @xmath      (4.46)
     @xmath               (4.47)
  -- -------- -------- -- --------

Notice that ( 4.47 ) uses valid probability distributions because of (
4.45 ). This establishes the slot @xmath arrivals for the Max- @xmath
policy for all of its @xmath queues with empty temporary buffer states.
The slot @xmath arrivals for its queues with non-empty temporary buffers
will be dropped and hence do not affect the queue states on slot @xmath
. Thus, we define arrivals @xmath to be independent of all other
quantities and to be Bernoulli with @xmath for all @xmath in the set:

  -- -------- --
     @xmath   
  -- -------- --

Now we verify that @xmath and @xmath have the same probability law.
First condition on knowledge of @xmath and the particular @xmath and
@xmath values for @xmath . All queues @xmath with non-empty temporary
buffer states on slot @xmath under Max- @xmath were defined to have
arrivals @xmath as independent Bernoulli variables with @xmath . It
remains to verify those queues within @xmath . According to ( 4.47 ),
for any queue @xmath in set @xmath , it follows

  -- -- -------- -------- --
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

and so @xmath for all @xmath . Further, mutual independence of @xmath
implies mutual independence of @xmath . Finally, these quantities are
conditionally independent of events before slot @xmath , given knowledge
of @xmath and the particular @xmath and @xmath values for @xmath . Thus,
conditioned on this knowledge, @xmath and @xmath have the same
probability law. This holds for all possible values of the conditional
knowledge @xmath and @xmath and @xmath . It follows that @xmath and
@xmath have the same (unconditioned) probability law.

Finally, we show that the coupling relations ( 4.46 ) and ( 4.47 )
produce such @xmath satisfying

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

According to ( 4.46 ) and ( 4.47 ),

  -- -------- --
     @xmath   
  -- -------- --

thus,

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

Pick any @xmath . Let @xmath be the number of empty temporary buffers
within the first @xmath queues under policy @xmath , i.e.

  -- -------- --
     @xmath   
  -- -------- --

Similarly define:

  -- -------- --
     @xmath   
  -- -------- --

Then, it follows:

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.50)
     @xmath   @xmath   @xmath      (4.51)
  -- -------- -------- -------- -- --------

We know that @xmath . So there are two cases:

-   If @xmath , then from ( 4.50 ):

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    where the inequality follows from ( 4.43 ) and from ( 4.49 ) with
    @xmath . Thus, ( 5.18 ) holds.

-   If @xmath , then from ( 4.50 ):

      -- -------- -------- --
         @xmath   @xmath   
                  @xmath   
         @xmath   @xmath   
         @xmath   @xmath   
         @xmath   @xmath   
      -- -------- -------- --

    where the first inequality follows from the fact that

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    and the second inequality follows from ( 4.49 ).

Thus, ( 4.39 ) holds for @xmath and the induction step is done. ∎

###### Corollary 4.4.1.

The Max- @xmath policy maximizes throughput within the class of
work-conserving policies.

###### Proof.

Let @xmath be the number of packets transmitted under any
work-conserving policy @xmath on slot @xmath , and let @xmath be the
corresponding process under policy Max- @xmath . Lemma 4.4.1 implies
@xmath . Then:

  -- -- -------- -------- --
        @xmath            
        @xmath   @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

where the inequality follows from Theorem 4.4.2 , with the understanding
that @xmath is a function that is nondecreasing in all coordinates. ∎

#### 4.4.5 Extending to non-work-conserving policies

Corollary 4.4.1 establishes optimality of Max- @xmath over the class of
all work-conserving policies. To complete the proof of Theorem 4.4.1 ,
it remains to show that throughput cannot be increased by allowing for
non-work-conserving policies. It suffices to show that for any
non-work-conserving policy, there exists a work-conserving policy that
gets the same or better throughput. The proof is straightforward and we
give only a proof sketch for brevity. Consider any non-work-conserving
policy @xmath , and let @xmath be its buffer state process on slot
@xmath for each queue @xmath . For the same initial buffer state and
arrival process, define the work-conserving policy @xmath as follows:
Every slot @xmath , policy @xmath initially allocates the @xmath servers
to exactly the same queues as policy @xmath . However, if some of these
queues are empty under policy @xmath , it reallocates those servers to
any non-empty queues that are not yet allocated servers (in keeping with
the work-conserving property). Let @xmath be the buffer state process
for queue @xmath under policy @xmath . It is not difficult to show that
@xmath for all queues @xmath and all slots @xmath . Therefore, on every
slot @xmath , the amount of blocked arrivals under policy @xmath is
always greater than or equal to that under policy @xmath . This implies
the throughput under policy @xmath is less than or equal to that of
policy @xmath .

### 4.5 Simulation experiments

In this section, we demonstrate near optimality of the multi-user DPP
ratio indexing algorithm by extensive simulations. In the first part, we
simulate the case in which the file length distribution is geometric,
and show that the suboptimality gap is extremely small. In the second
part, we test the robustness of our algorithm for more general scenarios
in which the file length distribution is not geometric. For simplicity,
it is assumed throughout that all transmissions send a fixed sized
packet, all files are an integer number of these packets, and that
decisions @xmath affect the success probability of the transmission as
well as the power expenditure.

#### 4.5.1 DPP ratio indexing with geometric file length

In the first simulation we use @xmath , @xmath with action set @xmath ;
The settings are generated randomly and specified in Table I, and the
constraint @xmath .

The algorithm is run for 1 million slots in each trial and each point is
the average of 100 trials. We compare the performance of our algorithm
with the optimal randomized policy. The optimal policy is computed by
constructing composite states (i.e. if there are three users where user
1 is at state 0, user 2 is at state 1 and user 3 is at state 1, we view
011 as a composite state), and then reformulating this MDP into a linear
program (see [ fox1966markov ] ) with @xmath variables and @xmath
constraints.

In Fig. 5.3 , we show that as our tradeoff parameter @xmath gets larger,
the objective value approaches the optimal value and achieves a near
optimal performance. Fig. 5.4 and Fig. 5.5 show that @xmath also affects
the virtual queue size and the constraint gap. As @xmath gets larger,
the average virtual queue size becomes larger and the gap becomes
smaller. We also plot the upper bound of queue size we derived from
Lemma 4.3.1 in Fig. 5.4 , demonstrating that the queue is bounded. In
order to show that @xmath is indeed a trade-off parameter affecting the
convergence time, we plotted Fig. 4.6 . It can be seen from the figure
that as @xmath gets larger, the number of time slots needed for the
running average to roughly converge to the optimal power expenditure
becomes larger.

In the second simulation, we explore the parameter space and demonstrate
that in general the suboptimality gap of our algorithm is negligible.
First, we define the relative error as the following:

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

where @xmath is the objective value after running 1 million slots of our
algorithm and @xmath is the optimal value. We first explore the system
parameters by letting @xmath ’s and @xmath ’s take random numbers within
0 and 1, letting @xmath take random number within 1 and 5, choosing
@xmath and fixing the remaining parameters the same as the last
experiment. We conduct 1000 Monte-Carlo experiments and calculate the
average relative error, which is 0.00083 .

Next, we explore the control parameters by letting the @xmath take
random number within 2 and 4, and letting @xmath values random numbers
between 0 and 1, choosing @xmath and fixing the remaining parameters the
same as the first simulation. The relative error is 0.00057 . Both
experiments show that the suboptimality gap is extremely small.

#### 4.5.2 DPP ratio indexing with non-memoryless file lengths

In this part, we test the sensitivity of the algorithm to different file
length distributions. In particular, the uniform distribution and the
Poisson distribution are implemented respectively, while our algorithm
still treats them as a geometric distribution with same mean. We then
compare their throughputs with the geometric case.

We use @xmath , @xmath with action set @xmath . The settings are
specified in Table II with constraint @xmath . Notice that for geometric
and uniform distribution, the file lengths are taken to be integer
values. The algorithm is run for 1 million slots in each trial and each
point is the average of 100 trials.

While the decisions are made using these values, the affect of these
decisions incorporates the actual (non-memoryless) file sizes. Fig. 4.7
shows the throughput-versus- @xmath relation for the two non-memoryless
cases and the memoryless case with matched means. The performance of all
three is similar. This illustrates that the indexing algorithm is robust
under different file length distributions.

### 4.6 Additional lemmas and proofs

#### 4.6.1 Comparison of Max-@xmath and Min-@xmath

This section shows that different work conserving policies can give
different throughput for the @xmath single-buffer queue problem of
Section 4.4.1 . Suppose we have two single-buffer queues and one server.
Let @xmath be the arrival rates of the i.i.d. Bernoulli arrival
processes for queues 1 and 2. Assume @xmath . There are 4 system states:
@xmath , where state @xmath means queue 1 has @xmath packets and queue 2
has @xmath packets. Consider the (work conserving) policy of giving
queue 1 strict priority over queue 2. This is equivalent to the Max-
@xmath policy when @xmath , and is equivalent to the Min- @xmath policy
when @xmath . Let @xmath be the steady state throughput. Then:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the steady state probability of the resulting discrete
time Markov chain. One can solve the global balance equations to show
that @xmath , so that the Max- @xmath policy has a higher throughput
than the Min- @xmath policy. In particular, it can be shown that:

-   Max- @xmath throughput: @xmath

-   Min- @xmath throughput: @xmath

#### 4.6.2 Proof of Lemma 4.4.2

This section proves that:

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

The case @xmath is already established from ( 4.42 ). Fix @xmath . Since
@xmath cannot transmit more packets than Max- @xmath during slot @xmath
, inequality ( 4.53 ) is proved by considering two cases:

1.  Policy @xmath transmits less packets than policy Max- @xmath . Then
    @xmath transmits less than @xmath packets during slot @xmath . The
    work-conserving nature of @xmath implies all non-empty queues were
    served, so @xmath for all @xmath and ( 4.53 ) holds.

2.  Policy @xmath transmits the same number of packets as policy Max-
    @xmath . In this case, consider the temporary buffer states of the
    last @xmath queues under policy Max- @xmath . If @xmath , then
    clearly the following holds

      -- -------- -- --------
         @xmath      (4.54)
      -- -------- -- --------

    Subtracting ( 4.54 ) from ( 4.42 ) immediately gives ( 4.53 ). If
    @xmath , then all @xmath servers of the Max- @xmath system were
    devoted to serving the largest @xmath queues. So only packets in the
    last @xmath queues could be transmitted by Max- @xmath during the
    slot @xmath . In particular, @xmath for all @xmath , and so (by (
    4.41 )):

      -- -------- -- --------
         @xmath      (4.55)
      -- -------- -- --------

    Thus:

      -- -------- -------- -- --------
         @xmath   @xmath      (4.56)
                  @xmath      (4.57)
                  @xmath      (4.58)
      -- -------- -------- -- --------

    where ( 4.56 ) holds by ( 4.40 ), ( 4.57 ) holds because ( 4.39 ) is
    true on slot @xmath , and the last equality holds by ( 4.55 ). This
    proves ( 4.53 ).

## Chapter 5 Opportunistic Scheduling over Renewal Systems

This chapter considers an opportunistic scheduling problem over a single
renewal system. Different from previous chapters, we consider teh
scenario where at the beginning of each renewal frame, the controller
observes a random event and then chooses an action in response to the
event, which affects the duration of the frame, the amount of resources
used, and a penalty metric. The goal is to make frame-wise decisions so
as to minimize the time average penalty subject to time average resource
constraints. This problem has applications to task processing and
communication in data networks, as well as to certain classes of Markov
decision problems. We formulate the problem as a dynamic fractional
program and propose an adaptive algorithm which uses an empirical
accumulation as a feedback parameter. A key feature of the proposed
algorithm is that it does not require knowledge of the random event
statistics and potentially allows (uncountably) infinite event sets. We
prove the algorithm satisfies all desired constraints and achieves
@xmath near optimality with probability 1.

### 5.1 Introduction

Consider a system that operates over the timeline of real numbers @xmath
. The timeline is divided into back-to-back periods called renewal
frames and the start of each frame is called a renewal (see Fig. 5.1 ).
The system state is refreshed at each renewal. At the start of each
renewal frame @xmath the controller observes a random event @xmath and
then takes an action @xmath from an action set @xmath in response to
@xmath . The pair @xmath affects: (i) the duration of that renewal
frame; (ii) a vector of resource expenditures for that frame; (iii) a
penalty incurred on that frame. The goal is to choose actions over time
to minimize time average penalty subject to time average constraints on
the resources without knowing any statistic of @xmath . We call such a
problem opportunistic scheduling over renewal systems .

#### 5.1.1 Example applications

This problem has applications to task processing in computer networks,
and certain generalizations of Markov decision problems.

-   Task processing networks: Consider a device that processes tasks
    back-to-back. Each renewal period corresponds to the time required
    to complete a single task. The random event @xmath observed
    corresponds to a vector of task parameters, including the type,
    size, and resource requirements for that particular task. The action
    set @xmath consists of different processing mode options, and the
    specific action @xmath determines the processing time, energy
    expenditure, and task quality. In this case, task quality can be
    defined as a negative penalty, and the goal is to maximize time
    average quality subject to power constraints and task completion
    rate constraints. A specific example of this sort is the following
    file downloading problem: Consider a wireless device that repeatedly
    downloads files. The device has two states: active (wants to
    download a file) and idle (does not want to download a file).
    Renewals occur at the start of each new active state. Here, @xmath
    denotes the observed wireless channel state, which affects the
    success probability of downloading a file (and thereby affects the
    transition probability from active to idle). This example is
    discussed further in the simulation section (Section 5.6 ).

-   Hierarchical Markov decision problems: Consider a slotted
    two-timescale Markov decision processes (MDP) over an infinite
    horizon and with constraints on average cost per slot. An MDP is run
    on the lower level, with a special state that is recurrent under any
    sequence of actions. The renewals are defined as revisitation times
    to that state. On a higher level, a random event @xmath is observed
    upon each revisitation to the renewal state on the lower level.
    Then, a decision is made on the higher level in response to @xmath ,
    which in turn affects the transition probability and penalty/cost
    received per slot on the lower level until the next renewal. Such a
    problem is a generalization of classical MDP problem (e.g. [ Ro02 ]
    , [ Be01 ] ) and has been considered previously in [ wernz2013multi
    ] , [ chang2003multitime ] with discrete finite state and full
    information on both levels. A heuristic method is also proposed in [
    wernz2013multi ] when some of the information is unknown. The
    algorithm of the current chapter does not require knowledge of the
    statistics of @xmath and allows the event set @xmath to be
    potentially (uncountably) infinite.

#### 5.1.2 Previous approaches on renewal systems

Most works on optimization over renewal systems consider the simpler
scenario of knowing the probability distribution of @xmath . In such a
case, one can show via the renewal-reward theory that the problem can be
solved (offline) by finding the solution to a linear fractional program.
This idea has been applied to solve MDPs in the seminal work [ Fo66 ] .
Methods for solving linear fractional programs can also be found, for
example, in [ Sc83 , BV04 ] . However, the practical limitations of such
an offline algorithm are twofold: First, if the event set @xmath is
large, then, there are too many probabilities @xmath to estimate and the
corresponding offline optimization problem may be difficult to solve
even if all probabilities are estimated accurately. Second, generic
offline optimization solvers may not take advantage of the special
renewal structure of the system. One notable example is the treatment of
power and delay minimization for a multi-class M/G/1 queue in [ Yao02 ,
LN14 ] , where the renewal structure allows a well known @xmath - @xmath
rule for delay minimization to be extended to treat both power and delay
constraints.

The work in [ Neely2010 , Ne09 ] presents a new drift-plus-penalty (DPP)
ratio algorithm solving renewal optimizations knowing the distribution
of @xmath . The algorithm treats the constraints via virtual queues so
that one only requires to minimize an unconstrained ratio during every
renewal frame. The algorithm provably meets all constraints and achieves
asymptotic near-optimality. The works [ wang2015dynamic ,
urgaonkar2015dynamic ] show that the edge cloud server migration problem
can be formulated as a specific renewal optimization. Using a variant of
the DPP ratio algorithm, they show that solving a simple stochastic
shortest path problem during every renewal frame gives near-optimal
performance. The work [ wei2018asynchronous ] solves a more general
asynchronous optimization over parallel renewal systems, though the
knowledge of the random event statistics is still required. It is worth
noting that the work [ Ne09 ] also proposes a heuristic algorithm when
the distribution of @xmath is not known. That algorithm is partially
analyzed: It is shown that if a certain process converges, then the
algorithm converges to a near-optimal point. However, whether or not
such a process converges is unknown.

#### 5.1.3 Other related works

The renewal optimization problem considered in this chapter is a
generalization of stochastic optimization over fixed time slots. Such
problems are categorized based on whether or not the random event is
observed before the decision is made. Cases where the random event is
observed before taking actions are often referred to as opportunistic
scheduling problems . Over the past decades, many algorithms have been
proposed including max-weight ( [ tassiulas1990stability ,
tassiulas1993dynamic ] ), Lyapunov optimization ( [ eryilmaz2006joint ,
eryilmaz2007fair , Neely2010 , georgiadis2006resource ] ), fluid model
methods ( [ stolyar2005maximizing , eryilmaz2007fair ] ), and dual
subgradient methods ( [ lin2004joint , ribeiro2010ergodic ] ) are often
used.

Cases where the random events are not observed are referred to as online
learning problems . Various algorithms are developed for unconstrained
learning including the weighted majority algorithm ( [
littlestone1994weighted ] ), multiplicative weighting algorithm ( [
freund1999adaptive ] ), following the perturbed leader ( [
hutter2005adaptive ] ) and online gradient descent ( [
zinkevich2003online , hazan2014beyond ] ). The resource constrained
learning problem is studied in [ mahdavi2012trading ] and [
wu2015algorithms ] . Online learning with an underlying MDP structure is
also treated using modified multiplicative weighting ( [ even2005experts
] ) and improved following the perturbed leader ( [ yu2009markov ] ).

#### 5.1.4 Our contributions

In this work, we focus on opportunistic scheduling over renewal systems
and propose a new algorithm that runs online (i.e. takes actions in
response to each observed @xmath ). Unlike prior works, the proposed
algorithm requires neither the statistics of @xmath nor explicit
estimation of them, and is fully analyzed with convergence properties
that hold with probability 1. From a technical perspective, we prove
near-optimality of the algorithm by showing asymptotic stability of a
customized process, relying on a novel construction of exponential
supermartingales which could be of independent interest. We complement
our theoretical results with simulation experiments on a time varying
constrained MDP.

### 5.2 Problem Formulation and Preliminaries

Consider a system where the time line is divided into back-to-back time
periods called frames. At the beginning of frame @xmath ( @xmath ), a
controller observes the realization of a random variable @xmath , which
is an i.i.d. copy of a random variable taking values in a compact set
@xmath with distribution function unknown to the controller. Then, after
observing the random event @xmath , the controller chooses an action
vector @xmath . Then, the tuple @xmath induces the following random
variables:

-   The penalty received during frame @xmath : @xmath .

-   The length of frame @xmath : @xmath .

-   A vector of resource consumptions during frame @xmath : @xmath .

We assume that given @xmath and @xmath at frame @xmath , @xmath is a
random vector independent of the outcomes of previous frames , with
known expectations. We then denote these conditional expectations as

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

which are all deterministic functions of @xmath and @xmath . This
notation is useful when we want to highlight the action @xmath we
choose. The analysis assumes a single action in response to the observed
@xmath at each frame. Nevertheless, an ergodic MDP can fit into this
model by defining the action as a selection of a policy to implement
over that frame so that the corresponding @xmath , @xmath and @xmath are
expectations over the frame under the chosen policy.

Let

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The goal is to minimize the time average penalty subject to @xmath
constraints on resource consumptions. Specifically, we aim to solve the
following fractional programming problem:

  -- -------- -------- -- -------
     @xmath   @xmath      (5.1)
              @xmath      (5.2)
              @xmath      (5.3)
  -- -------- -------- -- -------

where @xmath are nonnegative constants, and both the minimum and
constraint are taken in an almost sure sense. Finally, we use @xmath to
denote the minimum that can be achieved by solving above optimization
problem. For simplicity of notation, let

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

#### 5.2.1 Assumptions

Our main result requires the following assumptions, their importance
will become clear as we proceed. We begin with the following boundedness
assumption:

###### Assumption 5.2.1 (Exponential type).

Given @xmath and @xmath for a fixed @xmath , it holds that @xmath with
probability 1 and @xmath are of exponential type, i.e. there exists a
constant @xmath s.t.

  -- -- --
        
        
        
  -- -- --

where @xmath is a positive constant.

The following proposition is a simple consequence of the above
assumption:

###### Proposition 1.

Suppose Assumption 5.2.1 holds. Let @xmath be any of the three random
variables @xmath , @xmath and @xmath for a fixed @xmath . Then, given
any @xmath and @xmath ,

  -- -- --
        
  -- -- --

The proof follows from the inequality:

  -- -- --
        
  -- -- --

###### Assumption 5.2.2.

There exists a positive constant @xmath large enough so that the optimal
objective of @xmath , denoted as @xmath , falls into @xmath with
probability 1.

###### Remark 5.2.1.

If @xmath , then, we shall find a constant @xmath large enough so that
@xmath . Then, define a new penalty @xmath . It is easy to see that
minimizing @xmath is equivalent to minimizing @xmath and the optimal
objective of the new problem is @xmath , which is nonnegative.

###### Assumption 5.2.3.

Let @xmath be the performance vector under a certain @xmath pair. Then,
for any fixed @xmath , the set of achievable performance vectors over
all @xmath is compact.

In order to state the next assumption, we need the notion of randomized
stationary policy . We start with the definition:

###### Definition 5.2.1 (Randomized stationary policy).

A randomized stationary policy is an algorithm that at the beginning of
each frame @xmath , after observing the random event @xmath , the
controller chooses @xmath with a conditional probability that is the
same for all @xmath .

###### Assumption 5.2.4 (Bounded achievable region).

Let

  -- -- --
        
  -- -- --

be the one-shot average of one randomized stationary policy. Let @xmath
be the set of all achievable one-shot averages @xmath . Then, @xmath is
bounded.

###### Assumption 5.2.5 (@xmath-slackness).

There exists a randomized stationary policy @xmath such that the
following holds,

  -- -- --
        
  -- -- --

where @xmath is a constant.

###### Remark 5.2.2 (Measurability issue).

We implicitly assume the policies for choosing @xmath in reaction to
@xmath result in a measurable @xmath , so that @xmath , @xmath , @xmath
are valid random variables and the expectations in Assumption 5.2.4 and
5.2.5 are well defined. This assumption is mild. For example, when the
sets @xmath and @xmath are finite, it holds for any randomized
stationary policy. More generally, if @xmath and @xmath are measurable
subsets of some separable metric spaces, this holds whenever the
conditional probability in Definition 5.2.1 is “regular” (see [ Durrett
] for discussions on regular conditional probability), and @xmath ,
@xmath , @xmath are continuous functions on @xmath .

### 5.3 An Online Algorithm

We define a vector of virtual queues @xmath which are 0 at @xmath and
updated as follows:

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

The intuition behind this virtual queue idea is that if the algorithm
can stabilize @xmath , then the “arrival rate” @xmath is below “service
rate” @xmath and the constraint is satisfied. The proposed algorithm
then proceeds as in Algorithm 5.1 via two fixed parameters @xmath ,
@xmath , and an additional process @xmath that is initialized to be
@xmath . For any real number @xmath , the notation @xmath stands for
ceil and floor function:

  -- -- --
        
  -- -- --

Note that we can rewrite ( 5.6 ) as the following deterministic form:

  -- -- --
        
  -- -- --

Thus, Algorithm 5.1 proceeds by observing @xmath on each frame @xmath
and then choosing @xmath in @xmath to minimize the above deterministic
function. We can now see that we only use knowledge of current
realization @xmath , not statistics of @xmath . Also, the compactness
assumption (Assumption 5.2.3 ) guarantees that the minimum of ( 5.6 ) is
always achievable.

-   At the beginning of each frame @xmath , the controller observes
    @xmath , @xmath , @xmath and chooses action @xmath to minimize the
    following function:

      -- -- -- -------
               (5.6)
      -- -- -- -------

-   Update @xmath :

      -- -- --
            
      -- -- --

-   Update virtual queues @xmath :

      -- -------- --
         @xmath   
      -- -------- --

Algorithm 5.1 Online renewal optimization:

### 5.4 Feasibility Analysis

In this section, we prove that the proposed algorithm gives a sequence
of actions @xmath which satisfies all desired constraints with
probability 1. Specifically, we show that all virtual queues are stable
with probability 1, in which we leverage an important lemma from [
hajek1982hitting ] to obtain a exponential bound for the norm of @xmath
.

#### 5.4.1 The drift-plus-penalty bound

The start of our proof uses the drift-plus-penalty methodology. For a
general introduction on this topic, see [ neely2012stability ] for more
details. We define the 2-norm function of the virtual queue vector as:

  -- -------- --
     @xmath   
  -- -------- --

Define the Lyapunov drift @xmath as

  -- -- --
        
  -- -- --

Next, define the penalty function at frame @xmath as @xmath , where
@xmath is a fixed trade-off parameter. Then, the drift-plus-penalty
methodology suggests that we can stabilize the virtual queues by
choosing an action @xmath to greedily minimize the following
drift-plus-penalty expression, with the observed @xmath , @xmath and
@xmath :

  -- -- --
        
  -- -- --

The penalty term @xmath uses the @xmath variable, which depends on
events from all previous frames. This penalty does not fit the rubric of
[ neely2012stability ] and convergence of the algorithm does not follow
from prior work. A significant thrust of the current chapter is
convergence analysis under such a penalty function.

In order to obtain an upper bound on @xmath , we square both sides of (
5.5 ) and use the fact that @xmath ,

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

Summing the above over all @xmath and dividing by @xmath gives

  -- -------- --
     @xmath   
  -- -------- --

Adding @xmath to both sides and taking conditional expectations gives

  -- -------- -- -- -------
                    
     @xmath         
     @xmath         (5.8)
  -- -------- -- -- -------

where the last inequality follows from Proposition 1 . Thus, as we have
already seen in Algorithm 5.1 , the proposed algorithm observes the
vector @xmath , the random event @xmath and @xmath at frame @xmath , and
minimizes the right hand side of ( 5.8 ).

#### 5.4.2 Bounds on the virtual queue process and feasibility

In this section, we show how the bound ( 5.8 ) leads to the feasibility
of the proposed algorithm. Define @xmath as the system history
information up until frame @xmath . Formally, @xmath is a filtration
where each @xmath is the @xmath -algebra generated by all the random
variables before frame @xmath . Notice that since @xmath and @xmath
depend only on the events before frame @xmath , @xmath contains both
@xmath and @xmath . The following important lemma gives a stability
criterion for any given real random process with certain negative drift
property:

###### Lemma 5.4.1 (Theorem 2.3 of [hajek1982hitting]).

Let @xmath be a real random process over @xmath satisfying the following
two conditions for a fixed @xmath :

1.   For any @xmath , @xmath , for some @xmath .

2.   Given @xmath , @xmath , with some @xmath .

Suppose further that @xmath is given and finite, then, at every @xmath ,
the following bound holds:

  -- -- --
        
  -- -- --

Thus, in order to show the stability of the virtual queue process, it is
enough to test the above two conditions with @xmath . The following
lemma shows that @xmath satisfies these two conditions:

###### Lemma 5.4.2 (Drift condition).

Let @xmath , then, it satisfies the two conditions in Lemma 5.4.1 with
the following constants:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where @xmath .

The central idea of the proof is to plug the @xmath -slackness policy
specified in Assumption 5.2.5 into the right hand side of ( 5.8 ). A
similar idea has been presented in the Lemma 6 of [ wei2015probabilistic
] under the bounded increment of the virtual queue process. Here, we
generalize the idea to the case where the increment of the virtual
queues contains exponential type random variables @xmath and @xmath .
Note that the boundedness of @xmath is crucial for the argument to hold,
which justifies the truncation of pseudo average in the algorithm. Lemma
5.4.1 is proved in the Appendix 5.7 .

Combining the above two lemmas, we immediately have the following
corollary:

###### Corollary 5.4.1 (Exponential decay).

Given @xmath , the following holds for any @xmath under the proposed
algorithm,

  -- -- -- -------
           (5.9)
  -- -- -- -------

where

  -- -------- --
     @xmath   
  -- -------- --

and @xmath are as defined in Lemma 5.4.2 . Furthermore, we have @xmath ,
i.e. the queue size is @xmath .

The bound on @xmath follows readily from ( 5.9 ) via Jensen’s
inequality. With Corollary 5.4.1 in hand, we can prove the following
theorem:

###### Theorem 5.4.1 (Feasibility).

All constraints in ( 5.1 )-( 5.3 ) are satisfied under the proposed
algorithm with probability 1.

###### Proof of Theorem 5.4.1.

By queue updating rule ( 5.5 ), for any @xmath and any @xmath , one has

  -- -------- --
     @xmath   
  -- -------- --

Fix @xmath as a positive integer. Then, summing over all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath ,

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

Define the event

  -- -------- --
     @xmath   
  -- -------- --

By the Markov inequality and Corollary 5.4.1 , for any @xmath , we have

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where @xmath is defined in Corollary 5.4.1 . Thus, we have

  -- -------- --
     @xmath   
  -- -------- --

Thus, by the Borel-Cantelli lemma [ Durrett ] ,

  -- -- --
        
  -- -- --

Since @xmath is arbitrary, letting @xmath gives

  -- -- --
        
  -- -- --

Finally, taking the @xmath from both sides of ( 5.10 ) and substituting
in the above equation gives the claim. ∎

### 5.5 Optimality Analysis

In this section, we show that the proposed algorithm achieves time
average penalty within @xmath of the optimal objective @xmath . Since
the algorithm meets all the constraints, it follows,

  -- -------- --
     @xmath   
  -- -------- --

Thus, it is enough to prove the following theorem:

###### Theorem 5.5.1 (Near optimality).

For any @xmath and @xmath , the objective value produced by the proposed
algorithm is near optimal with

  -- -------- --
     @xmath   
  -- -------- --

i.e. the algorithm achieves @xmath near optimality.

###### Remark 5.5.1.

Combining Theorem 5.5.1 with Corollary 5.4.1 , we see that the tuning
parameter @xmath plays a trade-off between the sub-optimality and the
virtual queue bound (i.e. the constraint violation). In particular, our
result recovers the classical @xmath trade-off in the work of
opportunistic scheduling [ Neely2010 ] .

In order to prove Theorem 5.5.1 , we introduce the following notation:

  -- -- --
        
        
  -- -- --

#### 5.5.1 Relation between @xmath and @xmath

We start with a preliminary lemma illustrating that the original pseudo
average @xmath behaves almost the same as the tamed pseudo average
@xmath . Note that @xmath can be written as:

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 5.5.1 (Equivalence relation).

For any @xmath ,

1.  @xmath if and only if @xmath .

2.  @xmath if and only if @xmath .

3.  @xmath if and only if @xmath .

4.  @xmath if and only if @xmath .

This lemma is intuitive and the proof is shown in the Appendix 5.7 . We
will prove results on @xmath which extend naturally to @xmath via Lemma
5.5.1 .

The key idea of proving Theorem 5.5.1 is to bound the original pseudo
average process @xmath asymptotically from above by @xmath , which is
Theorem 5.5.2 below. We then prove Theorem 5.5.2 through the following
three steps:

-   We construct a truncated version of @xmath , namely @xmath , which
    has the same limit as @xmath (Lemma 5.5.3 below), so that it is
    enough to show @xmath asymptotically.

-   For the process @xmath , we bound the moments of the hitting time,
    namely, the time interval between two consecutive visits to the
    region @xmath , by constructing a dominating exponential
    supermartingale and bounding its size. (Lemma 5.5.6 and 5.5.7
    below).

-   We show that @xmath only finitely often asymptotically (with
    probability 1) using the bounded moments of the hitting time.

#### 5.5.2 Towards near optimality (I): Truncation

The following lemma states that the optimality of ( 5.1 )-( 5.3 ) is
achievable within the closure of the set of all one-shot averages
specified in Assumption 5.2.4 :

###### Lemma 5.5.2 (Stationary optimality).

Let @xmath be the optimal objective of ( 5.1 )-( 5.3 ). Then, there
exists a tuple @xmath , the closure of @xmath , such that the following
hold:

  -- -------- -- --------
     @xmath      (5.11)
     @xmath      (5.12)
  -- -------- -- --------

i.e. the optimality is achievable within @xmath .

The proof of this lemma is similar to the proof of Theorem 4.5 as well
as Lemma 7.1 of [ Neely2010 ] . We omit the details for brevity.

We start the truncation by picking up an @xmath small enough so that
@xmath . We aim to show @xmath . By Lemma 5.5.1 , it is enough to show
@xmath . The following lemma tells us it is enough to prove it on a
further term-wise truncated version of @xmath .

###### Lemma 5.5.3 (Truncation lemma).

Consider the following alternative pseudo average @xmath obtained by
truncating each summand such that @xmath and

  -- -- --
        
  -- -- --

where @xmath , @xmath is defined in Assumption 5.2.1 and @xmath is
defined in Lemma 5.4.2 . Then, we have

  -- -------- --
     @xmath   
  -- -------- --

###### Proof of Lemma 5.5.3.

Consider any frame @xmath such that there is a discrepancy between the
summand of @xmath and @xmath , i.e.

  -- -- -- --------
           (5.13)
  -- -- -- --------

By the Cauchy-Schwartz inequality, this implies

  -- -- --
        
  -- -- --

Thus, at least one of the following three events happened:

1.  @xmath .

2.  @xmath .

3.  @xmath .

where @xmath is defined in ( 5.4 ). Indeed, the occurence of one of the
three events is necessary for ( 5.13 ) to happen. We then argue that
these three events jointly occur only finitely many times. Thus, as
@xmath , the discrepancies are negligible.

Assume the event @xmath occurs, then, since @xmath , it follows @xmath .
Then, we have

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second to last inequality follows from the Markov inequality
and the last inequality follows from Assumption 5.2.1 .

Assume the event @xmath occurs, then, we have

  -- -------- --
     @xmath   
  -- -------- --

Thus,

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second to last inequality follows from the Markov inequality
and the last inequality follows from Corollary 5.4.1 .

Assume the event @xmath occurs. Again, by Assumption 5.2.1 and the
Markov inequality,

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the last inequality follows from Assumption 5.2.1 again. Now, by a
union bound,

  -- -------- --
     @xmath   
  -- -------- --

and thus,

  -- -- --
        
  -- -- --

By the Borel-Cantelli lemma, we have the joint event @xmath occurs only
finitely many times with probability 1, and our proof is finished. ∎

Lemma 5.5.3 is crucial for the rest of the proof. Specifically, it
creates an alternative sequence @xmath which has the following two
properties:

1.  We know exactly what the upper bound of each of the summands is,
    whereas in @xmath , there is no exact bound for the summand due to
    @xmath and other exponential type random variables.

2.  For any @xmath , we have @xmath . Thus, if @xmath for some @xmath ,
    then, @xmath .

#### 5.5.3 Towards near optimality (II): Exponential supermartingale

The following preliminary lemma demonstrates a negative drift property
for each of the summands in @xmath .

###### Lemma 5.5.4 (Key feature inequality).

For any @xmath , if @xmath , then, we have

  -- -- --
        
  -- -- --

###### Proof of Lemma 5.5.4.

Since the proposed algorithm minimizes ( 5.6 ) over all possible
decisions in @xmath , it must achieve value less than or equal to that
of any randomized stationary algorithm @xmath . This in turn implies,

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

Taking expectation from both sides with respect to @xmath and using the
fact that randomized stationary algorithms are i.i.d. over frames and
independent of @xmath , we have

  -- -- --
        
  -- -- --

for any @xmath . Since @xmath specified in Lemma 5.5.2 is in the closure
of @xmath , we can replace @xmath by the tuple @xmath and the inequality
still holds. This gives

  -- -------- -------- --
                       
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

where the second to last inequality follows from ( 5.11 ) and ( 5.12 ),
and the last inequality follows from @xmath and @xmath . Finally, since
@xmath for any real numbers @xmath , it follows,

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

and the claim follows. ∎

Define @xmath as the frame where @xmath visits the set @xmath for the
@xmath -th time with the following conventions: 1. If @xmath and @xmath
, then we count them as two times. 2. When @xmath , @xmath is equal to
0. Define the hitting time @xmath as

  -- -------- --
     @xmath   
  -- -------- --

The goal is to obtain a moment bound on this quantity when @xmath
(otherwise, this quantity is 1). In order to do so, we introduce a new
process as follows. For any @xmath , define

  -- -- -- --------
           (5.14)
  -- -- -- --------

The following lemma shows that indeed this @xmath is closely related to
@xmath . It plays an important role in proving Lemma 5.5.7 :

###### Lemma 5.5.5.

For any @xmath , if @xmath , then, @xmath .

###### Proof of Lemma 5.5.5.

Suppose @xmath , then, the following holds

  -- -------- --
     @xmath   
  -- -------- --

Thus,

  -- -------- --
     @xmath   
  -- -------- --

Since at the frame @xmath , @xmath , it follows,

  -- -- --
        
  -- -- --

Since @xmath , it follows @xmath and the claim follows. ∎

Recall our goal is to bound the hitting time @xmath of the process
@xmath when @xmath , with a strictly negative drift property as Lemma
5.5.4 . A classical approach analyzing the hitting time of a stochastic
process came from Wald’s construction of martingale for sequential
analysis (see, for example, [ wald1944cumulative ] for details). Later,
[ hajek1982hitting ] extended this idea to analyze the stability of a
queueing system with drift condition by a supermartingale construnction.
Here, we take one step further by considering the following
supermartingale construction based on @xmath :

###### Lemma 5.5.6 (Exponential Supermartingale).

Fix @xmath and @xmath such that @xmath . Define a new random process
@xmath starting from @xmath with

  -- -- --
        
  -- -- --

where for any set @xmath , @xmath is the indicator function which takes
value 1 if @xmath is true and 0 otherwise. For any @xmath , @xmath and
@xmath are defined as follows:

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

Then, the process @xmath is measurable with respect to @xmath , @xmath ,
and furthermore, it is a supermartingale with respect to the filtration
@xmath .

The proof of Lemma 5.5.6 is shown in Appendix 5.7 .

###### Remark 5.5.2.

If the increments @xmath were to be bounded, then, we could adopt the
similar construction as that of [ hajek1982hitting ] . However, in our
scenario @xmath is of the order @xmath , which is increasing and
unbounded. Thus, we need decreasing exponents @xmath and increasing
weights @xmath to account for that. Furthermore, the indicator function
indicates that we are only interested in the scenario @xmath .

The following lemma uses the previous result to bound the conditional
fourth moment of the hitting time @xmath .

###### Lemma 5.5.7.

Given @xmath as in Lemma 5.5.6 , for any @xmath and any @xmath such that
@xmath , there exists a positive constant @xmath , such that

  -- -- --
        
  -- -- --

###### Proof of Lemma 5.5.7.

First of all, from Lemma 5.5.6 gives that @xmath is a supermartingale
starting from @xmath , thus, we have the following chains of
inequalities for any @xmath :

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the first inequality uses the supermartingale property and the
last inequality uses Lemma 5.5.5 that on the set @xmath , @xmath and
@xmath . By definition of @xmath ,

  -- -- --
        
  -- -- --

where the first inequality follows from the definition of @xmath , and
the second inequality follows from the assumption that @xmath , thus,
@xmath and @xmath . Thus, it follows,

  -- -- --
        
  -- -- --

Now, we bound the fourth moment of hitting time:

  -- -------- -------- --
                       
     @xmath            
     @xmath            
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

Thus, it remains to show there exists a constant @xmath on the order
@xmath such that

  -- -------- --
     @xmath   
  -- -------- --

which is given is Appendix 5.8 . This implies there exists a @xmath so
that

  -- -- --
        
  -- -- --

Thus,

  -- -- -- --
           
  -- -- -- --

where the last equality follows from the fact that @xmath . This
finishes the proof. ∎

#### 5.5.4 An asymptotic upper bound on @xmath

So far, we have proved that if we pick any @xmath such that @xmath ,
then, the inter-visiting time has bounded conditional fourth moment. We
aim to show that @xmath with probability 1. By Lemma 5.5.3 , it is
enough to show @xmath . To do so, we need the following Second
Borel-Cantelli lemma:

###### Lemma 5.5.8 (Theorem 5.3.2. of [Durrett]).

Let @xmath be a filtration with @xmath , and @xmath be a sequence of
events with @xmath , then

  -- -- --
        
  -- -- --

###### Theorem 5.5.2 (Asymptotic upper bound).

For any @xmath and @xmath , the following hold,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

###### Proof of Theorem 5.5.2.

First of all, since the inter-hitting time @xmath has finite fourth
moment, each inter-hitting time is finite with probability 1, and thus
the process @xmath will visit @xmath infinitely many times with
probability 1. Then, we pick any @xmath and define the following
sequence of events:

  -- -- -- --------
           (5.15)
  -- -- -- --------

For any fixed @xmath , by Conditional Markov inequality, the following
holds with probability 1:

  -- -------- -------- --
     @xmath            
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the second inequality follows from Lemma 5.5.7 with @xmath , the
third inequality follows from the fact that @xmath and @xmath . The last
inequality follows from the fact that the inter-hitting time takes at
least one frame and thus @xmath .

Choose @xmath and @xmath as is defined in ( 5.15 ). Then, for any @xmath
, we have with probability 1,

  -- -- --
        
  -- -- --

Now by Lemma 5.5.8 ,

  -- -- --
        
  -- -- --

Since the process @xmath visits @xmath infinitely many times with
probability 1,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath is arbitrary, let @xmath gives

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

Finally, we show how this convergence result leads to the bound of
@xmath . According to the updating rule of @xmath , for any frame @xmath
such that @xmath ,

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the first inequality follows from the fact that @xmath . Now, we
take the @xmath from both sides and analyze each single term on the
right hand side:

  -- -------- --
     @xmath   
              
  -- -------- --

where we apply the convergence result ( 5.16 ) in the second line. Thus,

  -- -------- --
     @xmath   
  -- -------- --

By Lemma 5.5.3 we have @xmath . Finally, by Lemma 5.5.1 , and the fact
that @xmath , we have @xmath . Since this holds for any @xmath small
enough, let @xmath finishes the proof. ∎

#### 5.5.5 Finishing the proof of near optimality

With the help of previous analysis on @xmath , we are ready to prove our
main theorem, with the following lemma on strong law of large numbers
for martingale difference sequences:

###### Lemma 5.5.9 (Corollary 4.2 of [neely2012stability]).

Let @xmath be a filtration and let @xmath be a real-valued random
process such that @xmath . Suppose there is a finite constant @xmath
such that @xmath , and

  -- -- --
        
  -- -- --

Then,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof of Theorem 5.5.1.

Recall for any @xmath , the empirical accumulation without ceil and
floor function is

  -- -- --
        
  -- -- --

Dividing both sides by @xmath yields

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

Moving the last term to the left hand side and taking the @xmath from
both sides gives

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the second inequality follows from inequality ( 5.7 ) and
telescoping sums, and the last inequality follows from @xmath , @xmath
and @xmath . Now we use Lemma 5.5.9 with @xmath to bound the second
term. Since @xmath is of exponential type by Assumption 5.2.1 , we know
that @xmath . Furthermore, @xmath . Thus,

  -- -- --
        
  -- -- --

Thus, all assumptions in Lemma 5.5.9 are satisfied and we conclude that

  -- -------- --
     @xmath   
  -- -------- --

This implies,

  -- -- --
        
  -- -- --

By Theorem 5.5.2 , @xmath is asymptotically upper bounded. Since @xmath
and @xmath , it follows @xmath , which goes to infinity as @xmath .
Thus,

  -- -------- --
     @xmath   
  -- -------- --

and thus,

  -- -------- --
     @xmath   
  -- -------- --

By Theorem 5.5.2 again, @xmath is asymptotically upper bounded by @xmath
. Based on this result, it is easy to show the following

  -- -------- --
     @xmath   
  -- -------- --

Thus, we finally get

  -- -------- --
     @xmath   
  -- -------- --

finishing the proof. ∎

### 5.6 Simulation experiments

In this section, we demonstrate the performance of our proposed
algorithm through an application scenario on single user file
downloading. We show that this problem can be formulated as a two state
constrained online MDP and solved using our proposed algorithm.

Consider a slotted time system where @xmath , and one user is repeatedly
downloading files. We use @xmath to denote the system file state at time
slot @xmath . State “1” indicates there is an active file in the system
for downloading and state “0” means there is no file and the system is
idle. Suppose the user can only download 1 file at each time, and the
user cannot observe the file length. Each file contains an integer
number of packets which is independent and geometrically distributed
with expected length equal to 1.

During each time slot where there is an active file for downloading
(i.e. @xmath ), the user first observes the channel state @xmath , which
is the i.i.d. random variable taking values in @xmath with equal
probabilities, and delay penalty @xmath , which is also an i.i.d. random
variable taking values in @xmath with equal probability. Then, the user
makes a service action @xmath . The pair @xmath affects the following
quantities:

-   The success probability of downloading a file at time @xmath :
    @xmath .

-   The resource consumption @xmath at time @xmath . We assume @xmath ,
    @xmath , @xmath and @xmath .

After a file is downloaded, the system goes idle (i.e. @xmath ) and
stays there for a random amount of time that is independent and
geometrically distributed with mean equal to 2. The goal is to minimize
the time average delay penalty subject to a resource constraint that the
time average resource consumption cannot exceed 1.

In [ wei2015power ] , a similar optimization problem is considered but
without random events @xmath and @xmath , which can be formulated as a
two state constrained MDP. Here, using the same logic, we can formulate
our optimization problem as a two state constrained online MDP. Given
@xmath , the file will finish its download at the end of this time slot
with probability @xmath . Thus, the transition probabilities out of
state 1 are:

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

On the other hand, given @xmath , the system is idle and will transition
to the active state in the next slot with probability @xmath :

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

Now, we characterize this online MDP through renewal frames and show
that it can be solved using the proposed algorithm in Section 5.2 .
First, notice that the state “1” is recurrent under any action @xmath .
We denote @xmath as the @xmath -th time slot when the system returns to
state “1”. Define the renewal frame as the time period between @xmath
and @xmath with frame size

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, since the system does not have any control options in state
“0”, the controller makes exactly one decision during each frame and
this decision is made at the beginning of each frame. Thus, we can write
out the optimization problem as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Subsequently, in order to apply our algorithm, we can define the virtual
queue @xmath as @xmath with updating rule

  -- -------- --
     @xmath   
  -- -------- --

Notice that for any particular action @xmath and random event @xmath ,
we can always compute @xmath as

  -- -- -------- --
                 
        @xmath   
  -- -- -------- --

where the second equality follows by substituting @xmath and @xmath .
Thus, for each @xmath , the expression ( 5.6 ) can be computed.

In each of the simulations, each data point is the time average of 2
million slots. We compare the performance of the proposed algorithm with
the optimal randomized policy. The optimal policy is computed by
formulating the MDP into a linear program with the knowledge of the
distribution on @xmath and @xmath . See [ Fo66 ] for details of this
linear program formulation.

In Fig. 5.2 , we plot the performance of our algorithm verses @xmath
parameter for different @xmath value. We see from the plots that as
@xmath gets larger, the time averages approaches the optimal value and
achieves a near optimal performance for @xmath roughly between @xmath
and @xmath . A more obvious relation between performance and @xmath
value is shown in Fig. 5.3 , where we fix @xmath and plot the
performance of the algorithm verses @xmath value. It is clear from the
plots that the algorithm fails whenever @xmath is too small ( @xmath )
or too big ( @xmath ). This meets the statement of Theorem 5.5.1 that
the algorithm works for @xmath .

In Fig. 5.4 , we plot the time average resource consumption verses
@xmath value. We see from the plots that the algorithm is always
feasible for different @xmath ’s and @xmath ’s, which meets the
statement of Theorem 5.4.1 . Also, as @xmath gets larger, the constraint
gap tends to be smaller. In Fig. 5.5 , we plot the average virtual queue
size verses @xmath value. It shows that the average queue size gets
larger as @xmath get larger. To see the implications, recall from the
proof of Theorem 5.4.1 , the inequality ( 5.10 ) implies that the
virtual queue size @xmath affects the rate that the algorithm converges
down to the feasible region. Thus, if the average virtual queue size is
large, then, it takes longer for the algorithm to converge. This
demonstrates that @xmath is indeed a trade-off parameter which trades
the sub-optimality gap for the convergence rate.

### 5.7 Additional proofs

###### Proof of Lemma 5.4.2.

We begin by bounding the difference @xmath for any @xmath :

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the first inequality follows from triangle inequality and the last
inequality follows from the fact that for any @xmath , @xmath . Thus, it
follows,

  -- -- --
        
  -- -- --

which follows from Proposition 1 . Also, we have

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

where the second to last inequality follows by substituting the
definition @xmath and the last inequality follows from Assumption 5.2.1
.

Next, suppose @xmath . Then, since the proposed algorithm minimizes the
term on the right hand side of ( 5.8 ) over all possible decisions at
frame @xmath , it must achieve smaller value on that term compared to
that of @xmath -slackness policy @xmath specified in Assumption 5.2.5 .
Formally, this is

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

where we used the fact that @xmath and @xmath are in @xmath . Substitute
this bound into the right hand side of ( 5.8 ) and take expectation from
both sides regarding @xmath gives

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

Since @xmath , This implies

  -- -------- -------- --
                       
     @xmath            
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the second inequality follows from applying Proposition 1 to bound
@xmath as well as the fact that @xmath , and the third inequality
follows from the @xmath -slackness property as well as the assumption
that @xmath is i.i.d. over slots and hence independent of @xmath . This
further implies

  -- -------- -------- --
                       
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

where we use the fact that @xmath and also the assumption that @xmath .
Now take the square root from both sides gives

  -- -- --
        
  -- -- --

By concavity of @xmath function, we have @xmath , thus,

  -- -- -- --------
           (5.17)
  -- -- -- --------

Finally, we claim that this gives that under the condition @xmath ,

  -- -- -- --------
           (5.18)
  -- -- -- --------

To see this, we expand @xmath using Taylor series as follows:

  -- -------- -------- --
                       
     @xmath            
     @xmath            
     @xmath            
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the first inequality follows from ( 5.17 ), the second inequality
follows from @xmath , and the second to last inequality follows from
Proposition 1 .

Finally, notice that the above quadratic function on @xmath attains the
minimum at the point @xmath with value @xmath , and this function is
strictly decreasing when

  -- -- --
        
  -- -- --

Thus, our choice of

  -- -- --
        
  -- -- --

ensures that @xmath is strictly less than 1 and the proof is finished. ∎

###### Proof of Lemma 5.5.1.

If @xmath for some @xmath , then, @xmath falls into one of the following
three cases:

-   @xmath .

-   @xmath and @xmath .

-   @xmath and @xmath .

Then, we prove the above four properties based on these three cases.

1) If @xmath for some @xmath , then, the first two cases immediately
imply @xmath . If @xmath , then, we have @xmath , which violates the
assumption that @xmath . Thus, the third case is ruled out. On the other
hand, if @xmath , then, obviously, @xmath .

2) If @xmath for some @xmath , then the last two cases immediately imply
@xmath . If @xmath , then, we have @xmath , which violates the
assumption that @xmath . Thus, the first case is ruled out. On the other
hand, if @xmath , then, obviously, @xmath .

3) If @xmath , then, for any @xmath such that @xmath , there exists an
@xmath large enough so that @xmath . Then, by property 2), @xmath ,
which implies @xmath . Let @xmath gives @xmath . One the other hand, if
@xmath , then, obviously, @xmath .

4) If @xmath , then, for any @xmath such that @xmath there exists an
@xmath large enough so that @xmath . Then, by property 1), @xmath ,
which implies @xmath . Let @xmath gives @xmath . One the other hand, if
@xmath , then, obviously, @xmath . ∎

###### Proof of Lemma 5.5.6.

The proof is divided into two parts. The first part contains some
technical preliminaries showing @xmath is measurable respect to @xmath ,
and the second part contains computations to prove the supermartingale
claim.

-    Technical preliminaries: First of all, for any fixed @xmath , since
    @xmath is a random variable on the integers, we need to justify that
    @xmath is indeed a filtration. First, it is obvious that @xmath a
    valid stopping time, i.e.

      -- -------- --
         @xmath   
      -- -------- --

    Then, any @xmath with some constant @xmath is also a valid stopping
    time because

      -- -------- --
         @xmath   
      -- -------- --

    where @xmath . Thus, by definition of stopping time @xmath -algebra
    from [ Durrett ] , we know that for any @xmath , @xmath can be
    written as the collection of all sets @xmath that have @xmath ¹ ¹ 1
    An intuitive interpretation is that when @xmath , the set @xmath is
    contained in the information known until @xmath . . Now, pick @xmath
    as constants, and if a set @xmath , then,

      -- -- --
            
      -- -- --

    Thus, @xmath and @xmath is indeed a filtration.

    Since @xmath is determined by the realization up to frame @xmath ,
    it follows, for any @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    which implies that @xmath . Since @xmath is a filtration, it follows
    @xmath for any @xmath . By the same methodology, we can show that
    @xmath , which in turn implies, @xmath and @xmath . Overall, the
    function @xmath is measurable respect to @xmath .

-    Proof of supermartingale claim: It is obvious that @xmath , thus,
    in order to prove @xmath is a supermartingale, it is enough to show
    that

      -- -- -- --------
               (5.19)
      -- -- -- --------

    First, on the set @xmath , we have

      -- -- --
            
      -- -- --

    It is then sufficient to show the inequality ( 5.19 ) holds on the
    set @xmath . Since

      -- -------- -- --
                     
         @xmath      
         @xmath      
         @xmath      
         @xmath      
         @xmath      
      -- -------- -- --

    where @xmath and @xmath can be moved out of the expectation because
    @xmath and @xmath , and the only inequality follows from the
    following argument: On the set @xmath , @xmath , thus, by Lemma
    5.5.5 , @xmath and using the fact @xmath , we have @xmath . Thus, it
    is sufficient to show that on the set @xmath , we have

      -- -- --
            
      -- -- --

    By Taylor expansion, we have

      -- -------- -- --
                     
         @xmath      
         @xmath      
         @xmath      
      -- -------- -- --

    where the last inequality comes from the following argument: On the
    set @xmath , @xmath , thus, by the definition of @xmath , we have
    @xmath , and Lemma 5.5.1 gives @xmath , then, by Lemma 5.5.4 , we
    have

      -- -- --
            
      -- -- --

    Now, by the assumption that @xmath , we have @xmath , which follows
    from simple algebraic manipulations. Using the fact that @xmath , we
    have

      -- -------- -- --
                     
         @xmath      
         @xmath      
         @xmath      
      -- -------- -- --

    where the final inequality follows by completing the third term back
    to Taylor series which is equal to @xmath . Overall, the inequality
    ( 5.19 ) holds and @xmath is a supermartingale.

∎

### 5.8 Computation of Asymptotics

In this appendix, we show that there exists a constant @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

We first bound @xmath . Let @xmath , then,

  -- -------- -------- --
     @xmath            
              @xmath   
              @xmath   
  -- -------- -------- --

where we used the fact that @xmath . Next, to bound @xmath , we take the
logarithm:

  -- -------- -------- --
              @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the first inequality follows from the first order Taylor
expansion. Since @xmath , we compute the integral, which gives

  -- -- --
        
  -- -- --

Thus,

  -- -------- -------- --
              @xmath   
     @xmath            
     @xmath            
  -- -------- -------- --

where the last inequality follows from the fact that the integrand is
monotonically decreasing when @xmath , thus, the integral dominates the
sum on the tail @xmath . For the part where @xmath , the maximum of the
integrand is bounded by @xmath . Thus, the total difference of such
approximation is bounded by @xmath . Then, we try to estimate the
integral. Notice that

  -- -------- --
     @xmath   
  -- -------- --

we do integration-by-parts, which gives

  -- -------- -------- --
                       
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

Since @xmath and @xmath , we have @xmath , which implies @xmath , thus,

  -- -------- -- --
                 
     @xmath      
  -- -------- -- --

Repeat above procedure 3 more times, we have

  -- -------- -------- --
                       
     @xmath   @xmath   
                       
     @xmath   @xmath   
  -- -------- -------- --

for some @xmath on the order of @xmath (which is @xmath ), where the
second to last inequality follows from @xmath and thus, we replace
@xmath with @xmath and do a direct integration. Overall, we proved the
claim.

## Chapter 6 Online Learning in Weakly Coupled Markov Decision Processes

In this chapter, we consider online learning over weakly coupled Markov
decision processes. We develop a new distributed online algorithm where
each MDP makes its own decision each slot after observing a multiplier
computed from past information. While the scenario is significantly more
challenging than the classical online learning context, the algorithm is
shown to have a tight @xmath regret and constraint violations
simultaneously over a time horizon @xmath .

### 6.1 Problem formulation and related works

This chapter considers online constrained Markov decision processes
(OCMDP) where both the objective and constraint functions can vary each
time slot after the decision is made. We assume a slotted time scenario
with time slots @xmath . The OCMDP consists of @xmath parallel Markov
decision processes with indices @xmath . The @xmath -th MDP has state
space @xmath , action space @xmath , and transition probability matrix
@xmath which depends on the chosen action @xmath . Specifically, @xmath
where

  -- -- --
        
  -- -- --

where @xmath and @xmath are the state and action for system @xmath on
slot @xmath . We assume that both the state space and the action space
are finite for all @xmath .

After each MDP @xmath makes the decision at time @xmath (and assuming
the current state is @xmath and the action is @xmath , the following
information is revealed:

1.  The next state @xmath .

2.  A penalty function @xmath that depends on the current state @xmath
    and the current action @xmath .

3.  A collection of @xmath constraint functions @xmath that depend on
    @xmath and @xmath .

The functions @xmath and @xmath are all bounded mappings from @xmath to
@xmath and represent different types of costs incurred by system @xmath
on slot @xmath (depending on the current state and action). For example,
in a multi-server data center, the different systems @xmath can
represent different servers, the cost function for a particular server
@xmath might represent energy or monetary expenditure for that server,
and the constraint costs for server @xmath can represent negative
rewards such as service rates or qualities. Coupling between the server
systems comes from using all of them to collectively support a common
stream of arriving jobs.

A key aspect of this general problem is that the functions @xmath and
@xmath are unknown until after the slot @xmath decision is made. Thus,
the precise costs incurred by each system are only known at the end of
the slot. For a fixed time horizon of @xmath slots, the overall penalty
and constraint accumulation resulting from a policy @xmath is:

  -- -- -- -------
           (6.1)
  -- -- -- -------

and

  -- -- --
        
  -- -- --

where @xmath represents a given distribution on the initial joint state
vector @xmath . Note that @xmath denotes the state-action pair of the
@xmath th MDP, which is a pair of random variables determined by @xmath
and @xmath . Define a constraint set

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

Define the regret of a policy @xmath with respect to a particular joint
randomized stationary policy @xmath along with an arbitrary starting
state distribution @xmath as:

  -- -------- --
     @xmath   
  -- -------- --

The goal of OCMDP is to choose a policy @xmath so that both the regret
and constraint violations grow sublinearly with respect to @xmath ,
where regret is measured against all feasible joint randomized
stationary policies @xmath .

Here we give a brief review of the works related to online optimization
and online MDPs.

-    Online convex optimization (OCO) : This concerns multi-round cost
    minimization with arbitrarily-varying convex loss functions.
    Specifically, on each slot @xmath the decision maker chooses
    decisions @xmath within a convex set @xmath (before observing the
    loss function @xmath ) in order to minimize the total regret
    compared to the best fixed decision in hindsight, expressed as:

      -- -- --
            
      -- -- --

    See [ hazan2016introduction ] for an introduction to OCO. Zinkevich
    introduced OCO in [ zinkevich2003online ] and shows that an online
    projection gradient descent (OGD) algorithm achieves @xmath regret.
    This @xmath regret is proven to be the best in [ hazan07ML ] ,
    although improved performance is possible if all convex loss
    functions are strongly convex. The OGD decision requires to compute
    a projection of a vector onto a set @xmath . For complicated sets
    @xmath with functional equality constraints, e.g., @xmath , the
    projection can have high complexity. To circumvent the projection,
    work in [ mahdavi2012trading , jenatton2016adaptive , yu2016low ,
    chen2017online ] proposes alternative algorithms with simpler
    per-slot complexity and that satisfy the inequality constraints in
    the long term (rather than on every slot). Recently, new primal-dual
    type algorithms with low complexity are proposed in [
    neely2017online , hao2017onlinestochastic ] to solve more
    challenging OCO with time-varying functional inequality constraints.

-    Online Markov decision processes : This extends OCO to allow
    systems with a more complex Markov structure. This is similar to the
    setup of the current paper of minimizing the expression ( 6.1 ), but
    does not have the constraint set ( 6.2 ). Unlike traditional OCO,
    the current penalty depends not only on the current action and the
    current (unknown) penalty function, but on the current system state
    (which depends on the history of previous actions). Further, the
    number of policies can grow exponentially with the sizes of the
    state and action spaces, so that solutions can be computationally
    intensive. The work [ even2009online ] develops an algorithm in this
    context with @xmath regret. Extended algorithms and regularization
    methods are developed in [ yu2009markov ] [ guan2014online ] [
    dick2014online ] to reduce complexity and improve dependencies on
    the number of states and actions. Online MDP under bandit feedback
    (where the decision maker can only observe the penalty corresponding
    to the chosen action) is considered in [ yu2009markov ] [
    neu2010online ] .

-    Constrained MDPs : This aims to solve classical MDP problems with
    known cost functions but subject to additional constraints on the
    budget or resources. Linear programming methods for MDPs are found,
    for example, in [ altman1999constrained ] , and algorithms beyond LP
    are found in [ neely2011online ] [ caramanis2014efficient ] .
    Formulations closest to our setup appear in recent work on weakly
    coupled MDPs in [ boutilier2016budget ] [ wei2016theory ] that have
    known cost and resource functions.

-    Reinforcement Learning (RL) : This concerns MDPs with some unknown
    parameters (such as unknown functions and transition probabilities).
    Typically, RL makes stronger assumptions than the online setting,
    such as an environment that is unknown but fixed, whereas the
    unknown environment in the online context can change over time.
    Methods for RL are developed in [ bertsekas1995dynamic ] [
    sutton1998reinforcement ] [ lattimore2013sample ] [
    chen2016stochastic ] .

### 6.2 Preliminaries

#### 6.2.1 Basic Definitions

Throughout this paper, given an MDP with state space @xmath and action
space @xmath , a policy @xmath defines a (possibly probabilistic) method
of choosing actions @xmath at state @xmath based on the past
information. We start with some basic definitions of important classes
of policies:

###### Definition 6.2.1.

For an MDP, a randomized stationary policy @xmath defines an algorithm
which, whenever the system is in state @xmath , chooses an action @xmath
according to a fixed conditional probability function @xmath , defined
for all @xmath and @xmath .

###### Definition 6.2.2.

For an MDP, a pure policy @xmath is a randomized stationary policy with
all probabilities equal to either 0 or 1. That is, a pure policy is
defined by a deterministic mapping between states @xmath and actions
@xmath . Whenever the system is in a state @xmath , it always chooses a
particular action @xmath (with probability 1).

Note that if an MDP has a finite state and action space, the set of all
pure policies is also finite. Consider the MDP associated with a
particular system @xmath . For any randomized stationary policy @xmath ,
it holds that @xmath for all @xmath . Define the transition probability
matrix @xmath under policy @xmath to have components as follows:

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

It is easy to verify that @xmath is indeed a stochastic matrix , that
is, it has rows with nonnegative components that sum to 1. Let @xmath be
an (arbitrary) initial distribution for the @xmath -th MDP. Define the
state distribution at time @xmath under @xmath as @xmath . By the Markov
property of the system, we have @xmath . A transition probability matrix
@xmath is ergodic if it gives rise to a Markov chain that is irreducible
and aperiodic. Since the state space is finite, an ergodic matrix @xmath
has a unique stationary distribution denoted @xmath , so that @xmath is
the unique probability vector solving @xmath .

###### Assumption 6.2.1 (Unichain model).

There exists a universal integer @xmath such that for any integer @xmath
and every @xmath , we have the product @xmath is a transition matrix
with strictly positive entries for any sequence of pure policies @xmath
associated with the @xmath th MDP.

###### Remark 6.2.1.

Assumption 6.2.1 implies that each MDP @xmath is ergodic under any pure
policy. This follows by taking @xmath all the same in Assumption 6.2.1 .
Since the transition matrix of any randomized stationary policy can be
formed as a convex combination of those of pure policies, any randomized
stationary policy results in an ergodic MDP for which there is a unique
stationary distribution. Assumption 6.2.1 is easy to check via the
following simple sufficient condition.

###### Proposition 6.2.1.

Assumption 6.2.1 holds if, for every @xmath , there is a fixed ergodic
matrix @xmath (i.e., a transition probability matrix that defines an
irreducible and aperiodic Markov chain) such that for any pure policy
@xmath on MDP @xmath we have the decomposition

  -- -------- --
     @xmath   
  -- -------- --

where @xmath depends on the pure policy @xmath and @xmath is a
stochastic matrix depending on @xmath .

###### Proof.

Fix @xmath and assume every pure policy on MDP @xmath has the above
decomposition. Since there are only finitely many pure policies, there
exists a lower bound @xmath such that @xmath for every pure policy
@xmath . Since @xmath is an ergodic matrix, there exists an integer
@xmath large enough such that @xmath has strictly positive components
for all @xmath . Fix @xmath and let @xmath be any sequence of @xmath
pure policies on MDP @xmath . Then

  -- -- --
        
  -- -- --

where inequality is treated entrywise. The universal integer @xmath can
be taken as the maximum integer @xmath over all @xmath . ∎

###### Definition 6.2.3.

A joint randomized stationary policy @xmath on @xmath parallel MDPs
defines an algorithm which chooses a joint action @xmath given the joint
state @xmath according to a fixed conditional probability @xmath .

The following special class of separable policies can be implemented
separately over each of the @xmath MDPs and plays a role in both
algorithm design and performance analysis.

###### Definition 6.2.4.

A joint randomized stationary policy @xmath is separable if the
conditional probabilities @xmath decompose as a product

  -- -- --
        
  -- -- --

for all @xmath , @xmath .

#### 6.2.2 Technical assumptions

The functions @xmath and @xmath are determined by random processes
defined over @xmath . Specifically, let @xmath be a finite dimensional
vector space. Let @xmath and @xmath be two sequences of random vectors
in @xmath . Then for all @xmath , @xmath , @xmath we have

  -- -- --
        
        
  -- -- --

where @xmath and @xmath formally define the time-varying functions in
terms of the random processes @xmath and @xmath . It is assumed that the
processes @xmath and @xmath are generated at the start of slot @xmath
(before any control actions are taken), and revealed gradually over
time, so that functions @xmath and @xmath are only revealed at the end
of slot @xmath .

###### Remark 6.2.2.

The functions generated at time 0 in this way are also called oblivious
functions because they are not influenced by control actions. Such an
assumption is commonly adopted in previous unconstrained online MDP
works (e.g. [ even2009online ] , [ yu2009markov ] and [ dick2014online ]
). Further, it is also shown in [ yu2009markov ] that without this
assumption, one can choose a sequence of objective functions against the
decision maker in a specifically designed MDP scenario so that one never
achieves the sublinear regret.

The functions are also assumed to be bounded by a universal constant
@xmath , so that:

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

It is assumed that @xmath is independent, identically distributed
(i.i.d.) and independent of @xmath . Hence, the constraint functions can
be arbitrarily correlated on the same slot, but appear i.i.d. over
different slots. On the other hand, no specific model is imposed on
@xmath . Thus, the functions @xmath can be arbitrarily time varying. Let
@xmath be the system information up to time @xmath , then, for any
@xmath , @xmath contains state and action information up to time @xmath
, i.e. @xmath , @xmath , and @xmath and @xmath . Throughout this paper,
we make the following assumptions.

###### Assumption 6.2.2 (Independent transition).

For each MDP, given the state @xmath and action @xmath , the next state
@xmath is independent of all other past information up to time @xmath as
well as the state transition @xmath , i.e., for all @xmath it holds that

  -- -- --
        
  -- -- --

where @xmath contains all past information up to time @xmath .

Intuitively, this assumption means that all MDPs are running
independently in the joint probability space and thus the only coupling
among them comes from the constraints, which reflects the notion of
weakly coupled MDPs in our title. Furthermore, by definition of @xmath ,
given @xmath , the next transition @xmath is also independent of
function paths @xmath and @xmath .

The following assumption states the constraint set is strictly feasible.

###### Assumption 6.2.3 (Slater’s condition).

There exists a real value @xmath and a fixed separable randomized
stationary policy @xmath such that

  -- -- --
        
  -- -- --

where the initial state is @xmath and is the unique stationary
distribution of policy @xmath , and the expectation is taken with
respect to the random initial state and the stochastic function @xmath
(i.e., @xmath ).

Slater’s condition is a common assumption in convergence time analysis
of constrained convex optimization (e.g. [ nedic2009approximate ] , [
bertsekas2009convex ] ). Note that this assumption readily implies the
constraint set @xmath can be achieved by the above randomized stationary
policy. Specifically, take @xmath and @xmath , then, we have

  -- -- --
        
  -- -- --

#### 6.2.3 The state-action polyhedron

In this section, we recall the well-known linear program formulation of
an MDP (see, for example, [ altman1999constrained ] and [ fox1966markov
] ). Consider an MDP with a state space @xmath and an action space
@xmath . Let @xmath be a probability simplex, i.e.

  -- -- --
        
  -- -- --

Given a randomized stationary policy @xmath with stationary state
distribution @xmath , the MDP is a Markov chain with transition matrix
@xmath given by ( 6.3 ). Thus, it must satisfy the following balance
equation:

  -- -------- --
     @xmath   
  -- -------- --

Defining @xmath and substituting the definition of transition
probability ( 6.3 ) into the above equation gives

  -- -- --
        
  -- -- --

The variable @xmath is often interpreted as a stationary probability of
being at state @xmath and taking action @xmath under some randomized
stationary policy. The state action polyhedron @xmath is then defined as

  -- -- --
        
  -- -- --

Given any @xmath , one can recover a randomized stationary policy @xmath
at any state @xmath as

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

Given any fixed penalty function @xmath , the best policy minimizing the
penalty (without constraint) is a randomized stationary policy given by
the solution to the following linear program (LP):

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

where @xmath . Note that for any policy @xmath given by the state-action
pair @xmath according to ( 6.5 ),

  -- -- --
        
  -- -- --

Thus, @xmath is often referred to as the stationary state penalty of
policy @xmath .

It can also be shown that any state-action pair in the set @xmath can be
achieved by a convex combination of state-action vectors of pure
policies, and thus all corner points of the polyhedron @xmath are from
pure policies. As a consequence, the best randomized stationary policy
solving ( 6.6 ) is always a pure policy.

#### 6.2.4 Preliminary results on MDPs

In this section, we give preliminary results regarding the properties of
our weakly coupled MDPs under randomized stationary policies. The proofs
can be found in Appendix 6.6.1 . We start with a lemma on the uniform
mixing of MDPs.

###### Lemma 6.2.1.

Suppose Assumption 6.2.1 and 6.2.2 hold. There exists a positive integer
@xmath and a constant @xmath such that for any two state distributions
@xmath and @xmath ,

  -- -- --
        
  -- -- --

where the supremum is taken with respect to any sequence of @xmath
randomized stationary policies @xmath .

For the @xmath -th MDP, let @xmath be its state-action polyhedron
according to the definition in Section 6.2.3 . For any joint randomized
stationary policy, let @xmath be the marginal state-action probability
vector on the @xmath -th MDP, i.e. for any joint state-action
distribution @xmath where @xmath and @xmath , we have @xmath .

We have the following lemma:

###### Lemma 6.2.2.

Suppose Assumption 6.2.1 and 6.2.2 hold. Consider the product MDP with
product state space @xmath and action space @xmath . Then, for any joint
randomized stationary policy, the following hold:

1.   The product MDP is irreducible and aperiodic.

2.   The marginal stationary state-action probability vector @xmath .

An immediate conclusion we can draw from this lemma is that given any
penalty and constraint functions @xmath and @xmath , @xmath , the
stationary penalty and constraint value of any joint randomized
stationary policy can be expressed as

  -- -- --
        
  -- -- --

with @xmath . This in turn implies such stationary state-action
probabilities @xmath can also be realized via a separable randomized
stationary policy @xmath with

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

and the corresponding stationary penalty and constraint value can also
be achieved via this policy. This fact implies that when considering the
stationary state performance only, the class of separable randomized
stationary policies is large enough to cover all possible stationary
penalty and constraint values.

In particular, let @xmath be the separable randomized stationary policy
associated with the Slater condition (Assumption 6.2.3 ). Using the fact
that the constraint functions @xmath (i.e. @xmath ) are i.i.d.and
Assumption 6.2.2 on independence of probability transitions, we have the
constraint functions @xmath and the state-action pairs at any time
@xmath are mutuallly independent. Thus,

  -- -- --
        
  -- -- --

where @xmath corresponds to @xmath according to ( 6.7 ).

Then, Slater’s condition can be translated to the following: There
exists a sequence of state-action probabilities @xmath from a separable
randomized stationary policy such that @xmath , and

  -- -- -- -------
           (6.8)
  -- -- -- -------

The assumption on separability does not lose generality in the sense
that if there is no separable randomized stationary policy that
satisfies ( 6.8 ), then, there is no joint randomized stationary policy
that satisfies ( 6.8 ) either.

#### 6.2.5 The blessing of slow-update property in online MDPs

The current state of an MDP depends on previous states and actions. As a
consequence, the slot @xmath penalty not only depends on the current
penalty function and current action, but also on the system history.
This complication does not arise in classical online convex optimization
( [ hazan2016introduction ] , [ zinkevich2003online ] ) as there is no
notion of “state” and the slot @xmath penalty depends only on the slot
@xmath penalty function and action.

Now imagine a virtual system where, on each slot @xmath , a policy
@xmath is chosen (rather than an action). Further imagine the MDP
immediately reaching its corresponding stationary distribution @xmath .
Then the states and actions on previous slots do not matter and the slot
@xmath performance depends only on the chosen policy @xmath and on the
current penalty and constraint functions. This imaginary system now has
a structure similar to classical online convex optimization as in the
Zinkevich scenario [ zinkevich2003online ] .

A key feature of online convex optimization algorithms as in [
zinkevich2003online ] is that they update their decision variables
slowly. For a fixed time scale @xmath over which @xmath regret is
desired, the decision variables are typically changed no more than a
distance @xmath from one slot to the next. An important insight in prior
(unconstrained) MDP works(e.g. [ dick2014online ] , [ even2009online ] ,
and [ yu2009markov ] ) is that such slow updates also guarantee the
“approximate” convergence of an MDP to its stationary distribution. As a
consequence, one can design the decision policies under the imaginary
assumption that the system instantly reaches its stationary
distribution, and later bound the error between the true system and the
imaginary system. If the error is on the same order as the desired
@xmath regret, then this approach works. This idea serves as a
cornerstone of our algorithm design of the next section, which treats
the case of multiple weakly coupled systems with both objective
functions and constraint functions.

### 6.3 OCMDP algorithm

Our proposed algorithm is distributed in the sense that each time slot,
each MDP solves its own subproblem and the constraint violations are
controlled by a simple update of global multipliers called “virtual
queues” at the end of each slot. Let @xmath be the state-action
polyhedra of @xmath MDPs, respectively. Let @xmath be a state-action
vector at time slot @xmath . At @xmath , each MDP chooses its initial
state-action vector @xmath resulting from any separable randomized
stationary policy @xmath . For example, one could choose a uniform
policy @xmath , solve the equation @xmath to get a probability vector
@xmath , and obtain @xmath . For each constraint @xmath , let @xmath be
a virtual queue defined over slots @xmath with the initial condition
@xmath , and update equation:

  -- -- -- -------
           (6.9)
  -- -- -- -------

Our algorithm uses two parameters @xmath and @xmath and makes decisions
as follows: At the start of each slot @xmath ,

-   The @xmath -th MDP observes @xmath and chooses @xmath to solve the
    following subproblem:

      -- -- -- --------
               (6.10)
      -- -- -- --------

-   Construct the randomized stationary policy @xmath according to ( 6.5
    ) with @xmath , and choose the action @xmath at @xmath -th MDP
    according to the conditional distribution @xmath .

-   Update the virtual queue @xmath according to ( 6.9 ) for all @xmath
    .

###### Remark 6.3.1.

Note that for any slot @xmath , this algorithm gives a separable
randomized stationary policy, so that each MDP chooses its own policy
based on its own function @xmath , @xmath , and a common multiplier
@xmath . Furthermore, note that ( 6.10 ) is a convex quadratic program
(QP). Standard theory of QP (e.g. [ ye1989extension ] ) shows that the
computation complexity solving ( 6.10 ) is @xmath for each @xmath .
Thus, the total computation complexity over all MDPs during each round
is @xmath .

###### Remark 6.3.2.

The quadratic term @xmath in ( 6.10 ) penalizes the deviation of @xmath
from the previous decision variable @xmath . Thus, under proper choice
of @xmath , the distance between @xmath and @xmath would be very small,
which is the slow update condition we need according to Section 6.2.5 .

The next lemma shows that solving ( 6.10 ) is in fact a projection onto
the state-action polyhedron. For any set @xmath and a vector @xmath ,
define the projection operator @xmath as

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 6.3.1.

Fix an @xmath and @xmath . The @xmath that solves ( 6.10 ) is

  -- -- --
        
  -- -- --

where @xmath

###### Proof.

By definition, we have

  -- -------- -- --
     @xmath      
     @xmath      
                 
     @xmath      
                 
     @xmath      
     @xmath      
  -- -------- -- --

finishing the proof. ∎

#### 6.3.1 Intuition of the algorithm and roadmap of analysis

The intuition of this algorithm follows from the discussion in Section
6.2.5 . Instead of the Markovian regret ( 6.1 ) and constraint set ( 6.2
), we work on the imaginary system that after the decision maker chooses
any joint policy @xmath and the penalty/constraint functions are
revealed, the @xmath parallel Markov chains reach stationary state
distribution right away, with state-action probability vectors @xmath
for @xmath parallel MDPs. Thus there is no Markov state in such a system
anymore and the corresponding stationary penalty and constraint function
value at time @xmath can be expressed as @xmath and @xmath ,
respectively. As a consequence, we are now facing a relatively easier
task of minimizing the following regret:

  -- -- -- --------
           (6.11)
  -- -- -- --------

where @xmath are the state-action probabilities corresponding to the
best fixed joint randomized stationary policy within the following
stationary constraint set

  -- -- -- --------
           (6.12)
  -- -- -- --------

with the assumption that Slater’s condition ( 6.8 ) holds.

To analyze the proposed algorithm, we need to tackle the following two
major challenges:

-    Whether or not the policy decision of the proposed algorithm would
    yield @xmath regret and constraint violation on the imaginary system
    that reaches steady state instantaneously on each slot.

-    Whether the error between the imaginary and true systems can be
    bounded by @xmath .

In the next section, we answer these questions via a multi-stage
analysis piecing together the results of MDPs from Section 6.2.4 with
multiple ingredients from convex analysis and stochastic queue analysis.
We first show the @xmath regret and constraint violation in the
imaginary online linear program incorporating a new regret analysis
procedure with a stochastic drift analysis for queue processes. Then, we
show if the benchmark randomized stationary algorithm always starts from
its stationary state, then, the discrepancy of regrets between the
imaginary and true systems can be controlled via the slow-update
property of the proposed algorithm together with the properties of MDPs
developed in Section 6.2.4 . Finally, for the problem with arbitrary
non-stationary starting state, we reformulate it as a perturbation on
the aforementioned stationary state problem and analyze the perturbation
via Farkas’ Lemma.

### 6.4 Convergence time analysis

#### 6.4.1 Stationary state performance: An online linear program

Let @xmath be the virtual queue vector and @xmath . Define the drift
@xmath .

##### Sample-path analysis

This section develops a couple of bounds given a sequence of penalty
functions @xmath and constraint functions @xmath . The following lemma
provides bounds for virtual queue processes:

###### Lemma 6.4.1.

For any @xmath at @xmath , the following holds under the virtual queue
update ( 6.9 ),

  -- -- --
        
  -- -- --

where @xmath is the constant defined in ( 6.4 ).

###### Proof.

By the queue updating rule ( 6.9 ), for any @xmath ,

  -- -------- -------- --
              @xmath   
     @xmath            
     @xmath            
     @xmath            
     @xmath            
  -- -------- -------- --

Note that the constraint functions are deterministically bounded,

  -- -- --
        
  -- -- --

Substituting this bound into the above queue bound and rearranging the
terms finish the proof. ∎

The next lemma provides a bound for the drift @xmath .

###### Lemma 6.4.2.

For any slot @xmath , we have

  -- -- --
        
  -- -- --

###### Proof.

By definition, we have

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
     @xmath            
  -- -------- -------- --

Note that by the queue update ( 6.9 ), we have

  -- -- --
        
  -- -- --

Substituting this bound into the drift bound finishes the proof. ∎

Consider a convex set @xmath . Recall that for a fixed real number
@xmath , a function @xmath is said to be @xmath -strongly convex , if
@xmath is convex over @xmath . It is easy to see that if @xmath is
convex, @xmath and @xmath , the function @xmath is @xmath -strongly
convex. Furthermore, if the function @xmath is @xmath -strongly convex
that is minimized at a point @xmath , then (see, e.g., Corollary 1 in [
YuNeely17SIOPT ] ):

  -- -------- -- --------
     @xmath      (6.13)
  -- -------- -- --------

The following lemma is a direct consequence of the above strongly convex
result. It also demonstrates the key property of our minimization
subproblem ( 6.10 ).

###### Lemma 6.4.3.

The following bound holds for any @xmath and any fixed @xmath :

  -- -- -- --------
           (6.14)
  -- -- -- --------

This lemma follows easily from the fact that the proposed algorithm (
6.10 ) gives @xmath minimizing the left hand side, which is a strongly
convex function, and then, applying ( 6.13 ), with

  -- -- --
        
  -- -- --

Combining the previous two lemmas gives the following
“drift-plus-penalty” bound.

###### Lemma 6.4.4.

For any fixed @xmath such that @xmath and @xmath , we have the following
bound,

  -- -- -- --------
           (6.15)
  -- -- -- --------

###### Proof.

Using Lemma 6.4.2 and then Lemma 6.4.3 , we obtain

  -- -------- -------- -- --------
                          
     @xmath               
     @xmath               
              @xmath      (6.16)
  -- -------- -------- -- --------

Note that by the queue updating rule ( 6.9 ), we have for any @xmath ,

  -- -- --
        
  -- -- --

and for @xmath , @xmath by the initial condition of the algorithm. Also,
we have for any @xmath ,

  -- -- --
        
  -- -- --

Thus, we have

  -- -- --
        
  -- -- --

Substituting this bound into ( 6.16 ) finishes the proof. ∎

##### Objective bound

###### Theorem 6.4.1.

For any @xmath in the constraint set ( 6.12 ) and any @xmath , the
proposed algorithm has the following stationary state performance bound:

  -- -- --
        
  -- -- --

In particular, choosing @xmath and @xmath gives the @xmath regret

  -- -- --
        
  -- -- --

###### Proof.

First of all, note that @xmath is i.i.d. and independent of all system
history up to @xmath , and thus independent of @xmath . We have

  -- -- -- --------
           (6.17)
  -- -- -- --------

where the last inequality follows from the assumption that @xmath is in
the constraint set ( 6.12 ). Substituting @xmath into ( 6.15 ), taking
expectation with respect to both sides and using ( 6.17 ) give

  -- -- --
        
  -- -- --

where the second inequality follows from ( 6.17 ). Note that for any
@xmath , completing the squares gives

  -- -- --
        
  -- -- --

Substituting this inequality into the previous bound and rearranging the
terms give

  -- -- --
        
  -- -- --

Taking telescoping sums from 1 to @xmath and dividing both sides by
@xmath gives,

  -- -------- -- --
                 
                 
     @xmath      
  -- -------- -- --

where we use the fact that @xmath and @xmath . ∎

##### A drift lemma and its implications

From Lemma 6.4.1 , we know that in order to get the constraint violation
bound, we need to look at the size of the virtual queue @xmath . The
following drift lemma serves as a cornerstone for our goal.

###### Lemma 6.4.5 (Lemma 5 of [hao2017onlinestochastic]).

Let @xmath be a probability space. Let @xmath be a discrete time
stochastic process adapted to a filtration @xmath with @xmath and @xmath
. Suppose there exist integer @xmath , real constants @xmath , @xmath
and @xmath such that

  -- -------- -------- -- --------
     @xmath   @xmath      (6.18)
     @xmath               (6.19)
  -- -------- -------- -- --------

hold for all @xmath . Then, the following holds:

  -- -------- --
     @xmath   
  -- -------- --

Note that a special case of above drift lemma for @xmath dates back to
the seminal paper of Hajek ( [ hajek1982hitting ] ) bounding the size of
a random process with strongly negative drift. Since then, its power has
been demonstrated in various scenarios ranging from steady state queue
bound ( [ eryilmaz2012asymptotically ] ) to feasibility analysis of
stochastic optimization ( [ wei2016online ] ). The current
generalization to a multi-step drift is first considered in [
hao2017onlinestochastic ] .

This lemma is useful in the current context due to the following lemma,
whose proof can be found in Appendix 6.6.2 .

###### Lemma 6.4.6.

Let @xmath be the system history functions up to time @xmath , including
@xmath , @xmath , @xmath , and @xmath is a null set. Let @xmath be an
arbitrary positive integer, then, we have

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

  -- -- --
        
  -- -- --

where @xmath .

Combining the previous two lemmas gives the virtual queue bound as

  -- -- --
        
  -- -- --

We then choose @xmath , @xmath and @xmath , which implies that

  -- -- -- --------
           (6.20)
  -- -- -- --------

where @xmath

##### The slow-update condition and constraint violation

In this section, we prove the slow-update property of the proposed
algorithm, which not only implies the the @xmath constraint violation
bound, but also plays a key role in Markov analysis.

###### Lemma 6.4.7.

The sequence of state-action vectors @xmath satisfies

  -- -- --
        
  -- -- --

In particular,choosing @xmath and @xmath gives a slow-update condition

  -- -- -- --------
           (6.21)
  -- -- -- --------

where @xmath is defined in ( 6.20 ).

###### Proof of Lemma 6.4.7.

First, choosing @xmath in ( 6.14 ) gives

  -- -- --
        
  -- -- --

Rearranging the terms gives

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the second and third inequality follow from Cauchy-Schwarz
inequality. Thus, it follows

  -- -- --
        
  -- -- --

Applying the fact that @xmath , @xmath and taking expectation from both
sides give the first bound in the lemma. The second bound follows
directly from the first bound by further substituting ( 6.20 ). ∎

###### Theorem 6.4.2.

The proposed algorithm has the following stationary state constraint
violation bound:

  -- -- --
        
  -- -- --

where @xmath is defined in ( 6.20 ).

###### Proof.

Taking expectation from both sides of Lemma 6.4.1 gives

  -- -- --
        
  -- -- --

Substituting the bounds ( 6.20 ) and ( 6.21 ) in to the above inequality
gives the desired result. ∎

#### 6.4.2 Markov analysis

So far, we have shown that our algorithm achieves an @xmath regret and
constraint violation simultaneously regarding the stationary online
linear program ( 6.11 ) with constraint set given by ( 6.12 ) in the
imaginary system. In this section, we show how these stationary state
results lead to a tight performance bound on the original true online
MDP problem ( 6.1 ) and ( 6.2 ) comparing to any joint randomized
stationary algorithm starting from its stationary state.

##### Approximate mixing of MDPs

Let @xmath be the set of system history functions up to time @xmath ,
including @xmath , @xmath , @xmath , and @xmath is a null set. Let
@xmath be the stationary state distribution at @xmath -th MDP under the
randomized stationary policy @xmath in the proposed algorithm. Let
@xmath be the true state distribution at time slot @xmath under the
proposed algorithm given the function path @xmath and starting state
@xmath , i.e. for any @xmath , @xmath and @xmath .

The following lemma provides a key estimate on the distance between
stationary distribution and true distribution at each time slot @xmath .
It builds upon the slow-update condition (Lemma 6.4.7 ) of the proposed
algorithm and uniform mixing bound of general MDPs (Lemma 6.2.1 ).

###### Lemma 6.4.8.

Consider the proposed algorithm with @xmath and @xmath . For any initial
state distribution @xmath and any @xmath , we have

  -- -- --
        
  -- -- --

where @xmath and @xmath are mixing parameters defined in Lemma 6.2.1 and
@xmath is an absolute constant defined in ( 6.20 ).

###### Proof of Lemma 6.4.8.

By Lemma 6.4.7 we know that for any @xmath ,

  -- -- --
        
  -- -- --

Thus,

  -- -- --
        
  -- -- --

Since for any @xmath , @xmath it then follows

  -- -- -- --------
           (6.22)
  -- -- -- --------

Now, we use the above relation to bound @xmath for any @xmath .

  -- -------- -- -- --------
                    
     @xmath         
     @xmath         (6.23)
  -- -------- -- -- --------

where the second inequality follows from the slow-update condition (
6.22 ) and the final equality follows from the fact that given the
function path @xmath , the following holds

  -- -- -- --------
           (6.24)
  -- -- -- --------

To see this, note that from the proposed algorithm, the policy @xmath is
determined by @xmath . Thus, by definition of stationary distribution,
given @xmath , we know that @xmath , and it is enough to show that given
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

First of all, the state distribution @xmath is determined by @xmath ,
@xmath and probability transition from @xmath to @xmath , which are in
turn determined by @xmath . Thus, given @xmath , for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where the second inequality follows from the Assumption 6.2.2 , the
third equality follows from the fact that @xmath is determined by @xmath
, thus, for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and the last equality follows from the definition of transition
probability ( 6.3 ). This gives

  -- -------- --
     @xmath   
  -- -------- --

and thus ( 6.24 ) holds.

We can iteratively apply the procedure ( 6.23 ) @xmath times as follows

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the second inequality follows from the nonexpansive property in
@xmath norm of the stochastic matrix @xmath that

  -- -- --
        
  -- -- --

and then using the slow-update condition ( 6.22 ) again. By Lemma 6.2.1
, we have

  -- -- --
        
  -- -- --

Iterating this inequality down to @xmath gives

  -- -------- -- --
                 
                 
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

finishing the proof. ∎

##### Benchmarking against policies starting from stationary state

Combining the results derived so far, we have the following regret bound
regarding any randomized stationary policy @xmath starting from its
stationary state distribution @xmath such that @xmath in the constraint
set @xmath defined in ( 6.2 ).

###### Theorem 6.4.3.

Let @xmath be the sequence of randomized stationary policies resulting
from the proposed algorithm with @xmath and @xmath . Let @xmath be the
starting state of the proposed algorithm. For any randomized stationary
policy @xmath starting from its stationary state distribution @xmath
such that @xmath , we have

  -- -- --
        
        
  -- -- --

###### Proof of Theorem 6.4.3.

First of all, by Lemma 6.2.2 , for any randomized stationary policy
@xmath , there exists some stationary state-action probability vectors
@xmath such that @xmath ,

  -- -- --
        
  -- -- --

and @xmath . As a consequence, @xmath implies @xmath and it follows
@xmath is in the imaginary constraint set @xmath defined in ( 6.12 ).
Thus, we are in a good shape applying Theorem 6.4.1 from imaginary
systems.

We then split @xmath into two terms:

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

By Theorem 6.4.1 , we get

  -- -- -- --------
           (6.25)
  -- -- -- --------

We then bound (I). Consider each time slot @xmath . We have

  -- -- --
        
  -- -- --

  -- -- --
        
  -- -- --

where the first equality follows from the definition of @xmath and the
second equality follows from the following: Given a specific function
path @xmath , the policy @xmath and the true state distribution @xmath
are fixed. Thus, we have,

  -- -- --
        
  -- -- --

Taking the full expectation regarding the function path gives the
result. Thus,

  -- -------- -- --
                 
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the last inequality follows from Lemma 6.4.8 . Thus, it follows,

  -- -------- -- -- --------
     @xmath         
     @xmath         
     @xmath         (6.26)
  -- -------- -- -- --------

Overall, combining ( 6.25 ),( 6.26 ) and substituting the constant
@xmath defined in ( 6.20 ) gives the objective regret bound.

For the constraint violation, we have

  -- -- --
        
  -- -- --

The term (V) can be readily bounded using Theorem 6.4.2 as

  -- -- --
        
  -- -- --

For the term (IV), we have

  -- -- --
        
  -- -- --

  -- -- --
        
  -- -- --

where the first equality follows from the definition of @xmath and the
second equality follows from the following: Given a specific function
path @xmath , the policy @xmath and the true state distribution @xmath
are fixed. Thus, we have,

  -- -- --
        
  -- -- --

Taking the full expectation regarding the function path gives the
result. Then, repeat the same proof as that of ( 6.26 ) gives

  -- -- --
        
  -- -- --

This finishes the proof of constraint violation. ∎

### 6.5 A more general regret bound against policies with arbitrary
starting state

Recall that Theorem 6.4.3 compares the proposed algorithm with any
randomized stationary policy @xmath starting from its stationary state
distribution @xmath , so that @xmath . In this section, we generalize
Theorem 6.4.3 and obtain a bound of the regret against all @xmath where
@xmath is an arbitrary starting state distribution (not necessarily the
stationary state distribution). The main technical difficulty doing such
a generalization is as follows: For any randomized stationary policy
@xmath such that @xmath , let @xmath be the stationary state-action
probabilities such that @xmath and @xmath . For some finite horizon
@xmath , there might exist some “low-cost” starting state distribution
@xmath such that @xmath for some @xmath . As a consequence, one coud
have

  -- -- --
        
  -- -- --

This implies although @xmath is feasible for our true system, its
stationary state-action probabilities @xmath can be infeasible with
respect to the imaginary constraint set ( 6.12 ), and all our analysis
so far fails to cover such randomized stationary policies.

To resolve this issue, we have to “enlarge” the imaginary constraint set
( 6.12 ) so as to cover all state-action probabilities @xmath arising
from any randomized stationary policy @xmath such that @xmath . But a
perturbation of constraint set would result in a perturbation of
objective in the imaginary system also. Our main goal in this section is
to bound such a perturbation and show that the perturbation bound leads
to the final @xmath regret bound.

##### A relaxed constraint set

We begin with a supporting lemma on the uniform mixing time bound over
all joint randomized stationary policies. The proof is given in Appendix
6.6.3 .

###### Lemma 6.5.1.

Consider any randomized stationary policy @xmath in ( 6.2 ) with
arbitrary starting state distribution @xmath . Let @xmath be the
corresponding transition matrix on the product state space. Then, the
following holds

  -- -- -- --------
           (6.27)
  -- -- -- --------

where @xmath is fixed positive constant independent of @xmath .

The following lemma shows a relaxation of @xmath on the imaginary
constraint set ( 6.12 ) is enough to cover all the @xmath discussed at
the beginning of this section. The proof is given in Appendix 6.6.3 .

###### Lemma 6.5.2.

For any @xmath and any randomized stationary policies @xmath in ( 6.2 ),
with arbitrary starting state distribution @xmath and stationary
state-action probability @xmath ,

  -- -- -- --------
           (6.28)
           (6.29)
  -- -- -- --------

where @xmath is an absolute constant. In particular, @xmath is contained
in the following relaxed constraint set

  -- -- --
        
  -- -- --

##### Best stationary performance over the relaxed constraint set

Recall that the best stationary performance in hindsight over all
randomized stationary policies in the constraint set @xmath can be
obtained as the minimum achieved by the following linear program.

  -- -------- -- -- --------
     @xmath         (6.30)
     @xmath         (6.31)
  -- -------- -- -- --------

On the other hand, if we consider all the randomized stationary policies
contained in the original constraint set ( 6.2 ), then, By Lemma 6.5.2 ,
the relaxed constraint set @xmath contains all such policies and the
best stationary performance over this relaxed set comes from the minimum
achieved by the following perturbed linear program:

  -- -------- -- -- --------
     @xmath         (6.32)
     @xmath         (6.33)
  -- -------- -- -- --------

We aim to show that the minimum achieved by ( 6.32 )-( 6.33 ) is not far
away from that of ( 6.30 )-( 6.31 ). In general, such a conclusion is
not true due to the unboundedness of Lagrange multipliers in constrained
optimization. However, since Slater’s condition holds in our case, the
perturbation can be bounded via the following well-known Farkas’ lemma (
[ bertsekas2009convex ] ):

###### Lemma 6.5.3 (Farkas’ Lemma).

Consider a convex program with objective @xmath and constraint function
@xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      (6.34)
     @xmath   @xmath      (6.35)
              @xmath      (6.36)
  -- -------- -------- -- --------

for some convex set @xmath . Let @xmath be one of the solutions to the
above convex program. Suppose there exists @xmath such that @xmath .
Then, there exists a separation hyperplane parametrized by @xmath such
that @xmath and

  -- -------- --
     @xmath   
  -- -------- --

The parameter @xmath is usually referred to as a Lagrange multiplier.
From the geometric perspective, Farkas’ Lemma states that if Slater’s
condition holds, then, there exists a non-vertical separation hyperplane
supported at @xmath and contains the set @xmath on one side. Thus, in
order to bound the perturbation of objective with respect to the
perturbation of constraint level, we need to bound the slope of the
supporting hyperplane from above, which boils down to controlling the
magnitude of the Lagrange multiplier. This is summarized in the
following lemma:

###### Lemma 6.5.4 (Lemma 1 of [nedic2009approximate]).

Consider the convex program ( 6.34 )-( 6.36 ), and define the Lagrange
dual function

  -- -- --
        
  -- -- --

Suppose there exists @xmath such that @xmath for some positive constant
@xmath . Then, the level set @xmath is bounded for any nonnegative
@xmath . Furthermore, we have

  -- -- --
        
  -- -- --

The technical importance of these two lemmas in the current context is
contained in the following corollary.

###### Corollary 6.5.1.

Let @xmath and @xmath be solutions to ( 6.30 )-( 6.31 ) and ( 6.32 )-(
6.33 ), respectively. Then, the following holds

  -- -- --
        
  -- -- --

where @xmath is the constant defined in Assumption 6.2.3 .

###### Proof of Corollary 6.5.1.

Take

  -- -------- --
              
              
     @xmath   
  -- -------- --

and @xmath in Farkas’ Lemma and we have the following display

  -- -- --
        
  -- -- --

for any @xmath and some @xmath . In particular, substituting @xmath into
the above display gives

  -- -- -- -- --------
              
              (6.37)
  -- -- -- -- --------

where the final inequality follows from the fact that @xmath satisfies
the relaxed constraint @xmath and @xmath . Now we need to bound the
magnitude of Lagrange multiplier @xmath . Note that in our scenario,

  -- -- --
        
  -- -- --

and the Lagrange multiplier @xmath is the solution to the maximization
problem

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the dual function defined in Lemma 6.5.4 . thus, it must
be in any super level set @xmath . In particular, taking @xmath in Lemma
6.5.4 and using Slater’s condition ( 6.8 ), we have there exists @xmath
such that

  -- -- --
        
  -- -- --

where the final inequality follows from the deterministic bound of
@xmath by @xmath . Substituting this bound into ( 6.37 ) gives the
desired result. ∎

As a simple consequence of the above corollary, we have our final bound
on the regret and constraint violation regarding any @xmath .

###### Theorem 6.5.1.

Let @xmath be the sequence of randomized stationary policies resulting
from the proposed algorithm with @xmath and @xmath . Let @xmath be the
starting state of the proposed algorithm. For any randomized stationary
policy @xmath starting from the state @xmath such that @xmath , we have

  -- -- --
        
        
  -- -- --

###### Proof.

Let @xmath be the randomized stationary policy corresponding to the
solution @xmath to ( 6.30 )-( 6.31 ) and let @xmath be any randomized
stationary policy such that @xmath . Since @xmath , it follows @xmath .
By Theorem 6.4.3 , we know that

  -- -- --
        
  -- -- --

and @xmath satisfies the bound in the statement. It is then enough to
bound @xmath . We split it in to two terms:

  -- -------- --
     @xmath   
  -- -------- --

By ( 6.28 ) in Lemma 6.5.2 , the term (II) is bounded by @xmath . It
remains to bound the first term. Since @xmath , by Lemma 6.5.2 , the
corresponding state-action probabilities @xmath of @xmath satisfies
@xmath and @xmath is feasible for ( 6.32 )-( 6.33 ). Since @xmath is the
solution to ( 6.32 )-( 6.33 ), we must have

  -- -- --
        
  -- -- --

On the other hand, by Corollary 6.5.1 ,

  -- -- -- --
           
  -- -- -- --

Combining the above two displays gives @xmath and the proof is finished.
∎

### 6.6 Additional lemmas and proofs

#### 6.6.1 Missing proofs in Section 6.2.4

We prove Lemma 6.2.1 and 6.2.2 in this section.

###### Proof of Lemma 6.2.1.

For simplicity of notations, we drop the dependencies on @xmath
throughout this proof. We first show that for any @xmath , where @xmath
is specified in Assumption 6.2.1 , @xmath is a strictly positive
stochastic matrix.

Since the MDP is finite state with a finite action set, the set of all
pure policies (Definition 6.2.2 ) is finite. Let @xmath be probability
transition matrices corresponding to these pure policies. Consider any
sequence of randomized stationary policies @xmath . Then, it follows
their transition matrices can be expressed as convex combinations of
pure policies, i.e.

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath . Thus, we have the following display

  -- -------- -------- -- --------
     @xmath               
     @xmath   @xmath      (6.38)
  -- -------- -------- -- --------

where @xmath ranges over all @xmath configurations.

Since @xmath , it follows ( 6.38 ) is a convex combination of all
possible sequences @xmath . By assumption 6.2.1 , we have @xmath is
strictly positive for any @xmath , and there exists a universal lower
bound @xmath of all entries of @xmath ranging over all configurations in
@xmath . This implies @xmath is also strictly positive with the same
lower bound @xmath for any sequences of randomized stationary policies
@xmath .

Now, we proceed to prove the mixing bound. Choose @xmath and we can
decompose any @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath has each entry equal to @xmath (recall that @xmath is the
number of states which equals the size of the matrix) and @xmath depends
on @xmath . Then, @xmath is also a stochastic matrix (nonnegative and
row sum up to 1) because both @xmath and @xmath are stochastic matrices.
Thus, for any two distribution vectors @xmath and @xmath , we have

  -- -- --
        
  -- -- --

where we use the fact that for distribution vectors

  -- -- --
        
  -- -- --

Since @xmath is a stochastic matrix, it is non-expansive on @xmath
-norm, namely, for any vector @xmath , @xmath . To see this, simply
compute

  -- -- -- --------
           (6.39)
  -- -- -- --------

Overall, we obtain,

  -- -- --
        
  -- -- --

We can then take @xmath to finish the proof. ∎

###### Proof of Lemma 6.2.2.

Since the probability transition matrix of any randomized stationary
policy is a convex combination of those of pure policies, it is enough
to show that the product MDP is irreducible and aperiodic under any
joint pure policy. For simplicity, let @xmath and @xmath . Consider any
joint pure policy @xmath which select a fixed joint action @xmath given
a joint state @xmath , with probability 1. By Assumption 6.2.2 , we have

  -- -------- -- -- --------
                    
     @xmath         
                    
     @xmath         
     @xmath         
     @xmath         (6.40)
  -- -------- -- -- --------

where the second equality follows from the independence relation in
Assumption 6.2.2 . Thus, we obtain the equality,

  -- -- --
        
  -- -- --

Then, the one step transition probability between any two states @xmath
can be computed as

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
     @xmath            
  -- -------- -------- --

where we can remove the summation on @xmath due to the fact that @xmath
is a pure policy. The notation @xmath denotes a fixed mapping from
product state space @xmath to an individual action space @xmath
resulting from the pure policy, and @xmath is the Markov transition
probability from state @xmath to @xmath under the action @xmath . One
can then further compute the @xmath ( @xmath ) step transition
probability from between any two states @xmath as

  -- -------- -- -- --------
     @xmath         
                    
     @xmath         
                    (6.41)
  -- -------- -- -- --------

For any @xmath , the term

  -- -- --
        
  -- -- --

denotes the probability of moving from @xmath to @xmath along a certain
path under a certain sequence of fixed decisions @xmath , @xmath . Let

  -- -- --
        
  -- -- --

be the state path of k-th MDP. One can then change the order of
summation in ( 6.41 ) and sum over state paths of each MDP as follows:

  -- -- --
        
  -- -- --

We would like to exchange the order of the product and the sums so that
we can take the path sum over each individual MDP respectively. However,
the problem is that the transition probabilities are coupled through the
actions. The idea to proceed is to first apply a “hard” decoupling by
taking the infimum of transition probabilities of each MDP over all pure
policies, and use Assumption 6.2.1 , to bound the transition probability
from below uniformly. We have

  -- -------- -- --
     @xmath      
                 
     @xmath      
                 
  -- -------- -- --

where @xmath range over all pure policies, and the second inequality
follows from the fact that fix any path of other MDPs (i.e. @xmath ),
the term

  -- -- --
        
  -- -- --

is the probability of reaching @xmath from @xmath in @xmath steps using
a sequence of actions @xmath , where each action is a deterministic
function of the previous state at the 1-st MDP only. Thus, it dominates
the infimum over all sequences of pure policies @xmath on this MDP.
Similarly, we can decouple the rest of the sums and obtain the follow
display:

  -- -------- -- --
     @xmath      
     @xmath      
  -- -------- -- --

where @xmath denotes the @xmath -th entry of the product matrix @xmath .
Now, by Assumption 6.2.1 , there exists a large enough integer @xmath
such that @xmath is a strictly positive matrix for any sequence of
@xmath randomized stationary policy. As a consequence, the above
probability is strictly positive and ( 6.41 ) is also strictly positive.

This implies, if we choose @xmath , then, starting from any arbitrary
product state @xmath , there is a positive probability of returning to
this state after @xmath steps for all @xmath , which gives the
aperiodicity. Similarly, there is a positive probability of reaching any
other composite state after @xmath steps for all @xmath , which gives
the irreducibility. This implies the product state MDP is irreducible
and aperiodic under any joint pure policy, and thus, any joint
randomized stationary policy.

For the second part of the claim, we consider any randomized stationary
policy @xmath and the corresponding joint transition probability matrix
@xmath , there exists a stationary state-action probability vector
@xmath , such that

  -- -- -- --------
           (6.42)
  -- -- -- --------

Then, the state-action probability of the k-th MDP is @xmath . Thus,

  -- -------- -- --
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --

where the third from the last inequality follows from Assumption 6.2.2 .
This finishes the proof. ∎

#### 6.6.2 Missing proofs in Section 6.4.1

###### Proof of Lemma 6.4.6.

Consider the state-action probabilities @xmath which achieves the
Slater’s condition in ( 6.8 ). First of all, note that @xmath . Then,
using the assumption that @xmath is i.i.d. and independent of all system
information up to @xmath , we have

  -- -- -- --------
           (6.43)
  -- -- -- --------

Now, by the drift-plus-penalty bound ( 6.15 ), with @xmath ,

  -- -------- -------- --
     @xmath            
                       
     @xmath            
              @xmath   
  -- -------- -------- --

where the second inequality follows from Holder’s inequality that

  -- -- --
        
  -- -- --

Summing up the drift from @xmath to @xmath and taking a conditional
expectation @xmath give

  -- -------- -- --
                 
     @xmath      
                 
     @xmath      
  -- -------- -- --

Using the tower property of conditional expectations (further taking
conditional expectations @xmath inside the conditional expectation) and
the bound ( 6.43 ), we have

  -- -------- --
              
              
     @xmath   
  -- -------- --

where the last inequality follows from the queue updating rule ( 6.9 )
that

  -- -- --
        
  -- -- --

Thus, we have

  -- -- --
        
  -- -- --

Suppose @xmath , then, it follows,

  -- -- --
        
  -- -- --

which implies

  -- -- --
        
  -- -- --

Since @xmath , taking square root from both sides using Jensen’
inequality gives

  -- -- --
        
  -- -- --

On the other hand, we always have

  -- -- --
        
  -- -- --

Overall, we finish the proof. ∎

#### 6.6.3 Missing proofs in Section 6.5

###### Proof of Lemma 6.5.1.

Consider any joint randomized stationary policy @xmath and a starting
state probability @xmath on the product state space @xmath . Let @xmath
be the corresponding transition matrix on the product state space. Let
@xmath be the state distribution at time @xmath under @xmath and @xmath
be the stationary state distribution. By Lemma 6.2.2 , we know that this
product state MDP is irreducible and aperiodic (ergodic) under any
randomized stationary policy. In particular, it is ergodic under any
pure policy. Since there are only finitely many pure policies, let
@xmath be probability transition matrices corresponding to these pure
policies. By Proposition 1.7 of [ LevinPeresWilmer2006 ] , for any
@xmath , there exists integer @xmath such that @xmath is strictly
positive for any @xmath . Let

  -- -------- --
     @xmath   
  -- -------- --

then, it follows @xmath is strictly positive uniformly for all @xmath
’s. Let @xmath be the least entry of @xmath over all @xmath ’s.
Following from the fact that the probability transition matrix @xmath is
a convex combination of those of pure policies, i.e. @xmath , we have
@xmath is also strictly positive. To see this, note that

  -- -- --
        
  -- -- --

where the inequality is taken to be entry-wise. Furthermore, the least
entry of @xmath is lower bounded by @xmath uniformly over all joint
randomized stationary policies @xmath , which follows from the fact that
the least entry of @xmath is bounded as

  -- -- --
        
  -- -- --

The rest is a standard bookkeeping argument following from the Markov
chain mixing time theory (Theorem 4.9 of [ LevinPeresWilmer2006 ] ). Let
@xmath be a matrix of the same size as @xmath and each row equal to the
stationary distribution @xmath . Let @xmath . We claim that for any
integer @xmath , and any @xmath ,

  -- -------- -- --------
     @xmath      (6.44)
  -- -------- -- --------

for some stochastic matrix @xmath . We use induction to prove this
claim. First of all, for @xmath , from the fact that @xmath is a
positive matrix and the least entry is uniformly lower bounded by @xmath
over all policies @xmath , we can write @xmath as

  -- -- --
        
  -- -- --

for some stochastic matrix @xmath , where we use the fact that @xmath .
Suppose ( 6.44 ) holds for @xmath , we show that it also holds for
@xmath . Using the fact that @xmath and @xmath for any stochastic matrix
@xmath , we can write out @xmath :

  -- -------- -------- --
     @xmath            
     @xmath            
     @xmath            
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

Thus, ( 6.44 ) holds. For any integer @xmath , we write @xmath for some
integer @xmath and @xmath . Then,

  -- -- --
        
  -- -- --

Let @xmath be the @xmath -th row of @xmath , then, we obtain

  -- -------- --
     @xmath   
  -- -------- --

where we use the fact that the @xmath -norm of the row difference is
bounded by 2. Finally, for any starting state distribution @xmath , we
have

  -- -- --
        
  -- -- --

Take @xmath finishes the proof. ∎

###### Proof of Lemma 6.5.2.

Let @xmath be the joint state distribution at time @xmath under policy
@xmath . Using the fact that @xmath is a fixed policy independent of
@xmath and Assumption 6.2.2 that the probability transition is also
independent of function path given any state and action, the function
@xmath and state-action pair @xmath are mutually independent. Thus, for
any @xmath

  -- -- --
        
  -- -- --

where @xmath and @xmath and the latter expectation is taken with respect
to @xmath (i.e. the random variable @xmath ). On the other hand, by
Lemma 6.2.2 , we know that for any randomized stationary policy @xmath ,
the corresponding stationary state-action probability can be expressed
as @xmath with @xmath . Thus,

  -- -- --
        
  -- -- --

Hence, we can control the difference:

  -- -------- -------- --
                       
     @xmath            
     @xmath   @xmath   
  -- -------- -------- --

where the third inequality follows from Lemma 6.5.1 . Taking @xmath
finishes the proof of ( 6.29 ) and ( 6.28 ) can be proved in a similar
way.

In particular, we have for any randomized stationary policy @xmath that
satisfies the constraint ( 6.2 ), we have

  -- -- -- --
           
           
  -- -- -- --

finishing the proof. ∎
