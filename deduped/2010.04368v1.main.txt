## Chapter 1 Introduction

As for many other computer vision tasks, a recognition model gets as
input the whole observation, e.g., an image or a video, and predicts a
label. For instance, in video/action recognition, a model gets as input
all the information in the video and predicts a label that represents
the most likely category/event happened in that video. Although
understanding such information given the full extent of videos is key to
the success of wide variety of applications, the full observation is not
always available in many real-world cases, such as autonomous
navigation, surveillance and sports analysis. In these application, it
is crucial to reliably predict the future action of a particular
sequence. For instance, an autonomous car should always predict the
intention of all nearby pedestrians a few seconds ahead to e.g., avoid
hitting them.

Video anticipation is one of the key challenges in computer vision and
video analysis. Anticipation is the task of predicting future
representation(s) given limited, partial observation. Having a reliable
anticipation system is highly critical in scenarios where one needs to
react before the action is finalized, such as in pedestrian intention
forecasting system in autonomous vehicles. Sequence anticipation is a
challenging task due to the fact that given limited observation, the
future representation can be highly ambiguous. Given the task, one may
anticipate a coarse, discrete representation per limited observation,
called discrete anticipation, or a fine-grained, continuous
representation, called continuous anticipation. There are also cases
where the future representation can be considered deterministic , i.e.,
expecting one single likely outcome for a given observation, or can be
considered stochastic , where multiple plausible future representations
are likely and correct.

In this chapter, we first provide a definition of the problem as well as
the motivations behind this research. We then summarize our
contributions and the thesis outline.

### 1.1 Problem Definition

###### Discrete and Deterministic Anticipation.

In a discrete anticipation problem, the goal is to predict the desired
outcome @xmath given the partial observation @xmath where @xmath is the
length of the sequence and @xmath is the length of the partial
observation. The desired outcome @xmath can be a binary value
determining whether what will happen in the future is normal or abnormal
or can be a categorical determining the action of interest at time
@xmath . For discrete anticipation, we specifically study the problem of
action anticipation in this thesis. In contrast to the widely studied
problem of recognizing an action given a complete sequence, as
introduced above, action anticipation aims to identify the action from
only partially available videos. As such, it is therefore key to the
success of computer vision applications requiring to react as early as
possible, such as autonomous navigation. Note that, when addressing
discrete anticipation task, we focus on only a deterministic scenario,
where we are interested in predicting only one @xmath that has the
highest likelihood of occurrence.

###### Continuous and Stochastic Anticipation.

In a continuous anticipation task, the goal is to predict a sequence of
desired outcomes @xmath given the partial observation @xmath where
@xmath is the length of the sequence and @xmath is the length of the
partial observation. The main difference to discrete anticipation is one
need to anticipate the representation at every time-step in the future
such that the whole sequence remains natural and plausible. Unlike
discrete anticipation where the outcome representation (e.g., action
label) differs from the input representation (e.g., consecutive RGB
frames), in continuous anticipation these two representation are usually
from the same domain, casting video anticipation problem into future
representation generation. Examples of continuous anticipation is
trajectory prediction, video generation, and human motion prediction,
where in this thesis we focus on the latter. Similar to many other
real-world tasks in continuous anticipation, human motion prediction is
a highly stochastic process: given an observed sequence of poses,
multiple future motions are plausible. Thus, it is crucial to take
stochasticity of this problem into account. Therefore, in this thesis we
focus on stochastic scenario and try to generate multiple likely
continuations of an observed human motion.

### 1.2 Thesis Outline and Contributions

This thesis comprises seven chapters. Apart from this chapter that
introduces the problem and provides our motivations to address the
problem video anticipation, in Chapter 2, we introduce the technical
background of the methods we used in this thesis. This background
material consists of some concepts and theories that are common to many
of the approaches proposed in later chapters, including the
convolutional neural network architectures, recurrent neural networks,
and variational autoencoders. The next four chapters provide details of
our novel techniques for sequence learning for video anticipation.
Particularly, in Chapter 3, we introduce our novel techniques for
encouraging LSTMs to anticipate actions as early as possible. In Chapter
4, we introduce a new anticipation-specific dataset as well as a novel
multi-modal LSTM architecture that generalizes our previous contribution
to an arbitrary number of modalities. In Chapter 5, we address the
problem of human motion prediction with the focus on the stochastic
nature of this problem. Then, in Chapter 6, we propose a novel framework
for generating diverse and semantically high quality sequences given
partial observation. Finally, in Chapter 7, we summarise the main
contributions of the thesis and discuss ongoing and future work stemming
from this research.

A brief summary of each contribution is provided below.

#### 1.2.1 General Action Anticipation

We propose a novel action anticipation framework (that can be also seen
as an early recognition model). In particular, we introduce a novel loss
that encourages making correct predictions very early. Our loss models
the intuition that some actions, such as running and high jump, are
highly ambiguous after seeing only the first few frames, and false
positives should therefore not be penalized too strongly in the early
stages. By contrast, we would like to predict a high probability for the
correct class as early as possible, and thus penalize false negatives
from the beginning of the sequence. Our experiments demonstrate that,
for a given model, our new loss yields significantly higher accuracy
than existing ones on the task of early prediction. We also propose a
novel multi-stage Long Short Term Memory (MS-LSTM) architecture for
action anticipation. This model effectively extracts and jointly
exploits context- and action-aware features. This is in contrast to
existing methods that typically extract either global representations
for the entire image or video sequence, thus not focusing on the action
itself, or localize the feature extraction process to the action itself
via dense trajectories, optical flow or actionness, thus failing to
exploit contextual information [ sadegh2017encouraging ] .

#### 1.2.2 Action Anticipation in Driving Scenarios

We improve and extend our previous contribution by focusing on driving
scenarios, encompassing common the subproblems of anticipating ego car’s
driver maneuvers, front car’s driver maneuver, accidents, violating or
respecting traffic rules, and pedestrian intention, with a fixed,
sensible set of sensors. To this end, we introduce the VIrtual
ENvironment for Action Analysis (VIENA @xmath ) dataset. Altogether,
these subproblems encompass a total of 25 distinct action classes. VIENA
@xmath is acquired using the GTA V video game. It contains more than 15K
full HD, 5s long videos, corresponding to more than 600 samples per
action class, acquired in various driving conditions, weathers,
daytimes, and environments. This amounts to more than 2.25M frames, each
annotated with an action label. These videos are complemented by basic
vehicle dynamics measurements, reflecting well the type of information
that one could have access to in practice ¹ ¹ 1 Our dataset is publicly
available at https://sites.google.com/view/viena2-project/ . We then
benchmark state-of-the-art action anticipation algorithms on VIENA2, and
as another contribution, introduce a new multi-modal, LSTM-based
architecture that generalizes out previous contribution to an arbitrary
number of modalities, together with a new anticipation loss, which
outperforms existing approaches in our driving anticipation scenarios [
aliakbarian2018viena ] .

#### 1.2.3 A Stochastic Conditioning Scheme for Diverse Sequence
Generation

For continuous, stochastic anticipation task, we address the problem of
stochastic human motion prediction. As introduced earlier in this
thesis, human motion prediction aims to forecast the sequence of future
poses of a person given past observations of such poses. To achieve
this, existing methods typically rely on recurrent neural networks
(RNNs) that encode the person’s motion. While they predict reasonable
motions, RNNs are deterministic models and thus cannot account for the
highly stochastic nature of human motion; given the beginning of a
sequence, multiple, diverse futures are plausible. To correctly model
this, it is therefore critical to develop algorithms that can learn the
multiple modes of human motion, even when presented with only
deterministic training samples. We introduce an approach to effectively
learn the stochasticity in human motion. At the heart of our approach
lies the idea of Mix-and-Match perturbations: Instead of combining a
noise vector with the conditioning variables in a deterministic manner
(as usually done in standard practices), we randomly select and perturb
a subset of these variables. By randomly changing this subset at every
iteration, our strategy prevents training from identifying the root of
variations and forces the model to take it into account in the
generation process. This is a highly effective conditioning scheme in
scenarios when (1) we are dealing with a deterministic dataset, i.e.,
one sample per condition, (2) the conditioning signal is very strong and
representative, e.g., the sequence of past observations, and (3) the
model has an expressive decoder that can generate a plausible sample
given only the condition. We utilize Mix-and-Match by incorporating it
into a recurrent encoder-decoder network with a conditional variational
autoencoder (CVAE) block that learns to exploit the perturbations.
Mix-and-Match then acts as the stochastic conditioning scheme instead of
concatenation that usually appears in standard CVAEs [
Aliakbarian_2020_CVPR ] .

#### 1.2.4 Variational Autoencoders with Learned Conditional Priors

In our previous contribution, we identified one limitation of a standard
CVAEs when dealing deterministic datasets and strong conditioning
signals. In this work, we further investigates this problem from a more
theoretical point of view. Specifically, in this contribution we tackle
the task of diverse sequence generation in which all the diversely
generated sequences carry the same semantic as in the conditioning
signal. We observe that in standard CVAE, conditioning and sampling the
latent variable are two independent processes, leading to generating
samples that are not necessarily carry all the contextual information
about the condition. To address this, we propose to explicitly make the
latent variables depend on the observations (the conditioning signal).
To achieve this, we develop a CVAE architecture that learns a
distribution not only of the latent variables, but also of the
observations, the latter acting as prior on the former. By doing so, we
change the variational family of the posterior distribution of the CVAE,
thus, as a side effect, our approach can mitigate posterior collapse to
some extent [ aliakbarian2019sampling ] .

## Chapter 2 Technical Background

In this chapter we introduce the background materials, architectures and
models that have been used in this thesis. We use Convolutional Neural
Networks (CNNs) widely as the feature/representation learning approach
for discrete anticipation task. We employ variants of Recurrent Neural
Networks (RNNs) to effectively model sequences. Finally, since we cast
the problem of continuous anticipation as generating future
representation, we use Variational Autoencoders (VAEs) as our backbone
generative model in this thesis. Below, we review the utilized CNN
architectures, the variants of RNNs used in this thesis, and VAEs to
help the reader better understand the following chapters.

### 2.1 Convolutional Neural Networks

Convolutional neural networks [ karpathy2014large ] , or CNNs in short
and also known as ConvNets, are powerful tools used in many machine
learning and computer vision tasks. With the advances in deep learning
and compute, CNNs achieve the state-of-the-art performance on various
tasks such as image recognition [ simonyan2014very ; karpathy2014large ;
he2016deep ; huang2017densely ; krizhevsky2012imagenet ] , semantic
segmentation [ long2015fully ; noh2015learning ; saleh2017incorporating
; saleh2017bringing ; zhao2017pyramid ] , action recognition [
wang2016temporal ; carreira2017quo ; simonyan2014two ;
feichtenhofer2016convolutional ; wu2019long ; aliakbarian2016deep ] ,
object detection [ redmon2016you ; tychsen2017denet ; ren2015faster ;
girshick2015fast ; girshick2014rich ] , etc. CNNs comprise stack of
layers of different types, including but not limited to convolutional,
pooling, and fully-connected (also known as linear) layers. Common CNN
architectures stack blocks of these operations to perform feature
representation learning which is done by learning trainable parameters
of each layer. Since, in most cases, different layers are stacked on top
of each other, deeper layers perform operations on the output of
shallower layers, thus learning the feature representation in a
hierarchical manner. Hence, it is usually the case where the features
computed by the deeper layers carry out more semantically meaningful
information while the features of shallower layers provide detailed, but
low-level information [ saleh2016built ; bertasius2015deepedge ] . This
characteristic of CNNs allows us to use the output features of deeper
layers as a good representation of the input image [ wang2016temporal ;
carreira2017quo ; simonyan2014two ; feichtenhofer2016convolutional ;
wu2019long ] . In this thesis, we use CNNs as feature extractors for
representing the frames of videos such that it becomes feasible to use
them as the input to the recurrent models.

In this section, we assume that the reader has a knowledge about CNNs
building blocks, layers, and optimization, thus, we only briefly
introduce the special CNN architectures we used later in this thesis.

There are many powerful CNN architectures that shown effective in many
computer vision tasks. Among those, we use VGG16 [ simonyan2014very ]
and DenseNet [ huang2017densely ] architectures in the following
chapters. In this section we describe these two architectures briefly.

###### Vgg16.

Introduced in 2014, VGG16 [ simonyan2014very ] considered to be a very
deep network. This network, as shown in Fig. 2.1 comprises five
convolutional blocks followed by three fully-connected layers. Each
convolutional block contains convolutional, pooling, and nonlinearity
layers. Pre-trained on ImageNet dataset [ krizhevsky2012imagenet ] and
fine-tuned on the target dataset, this network can extract
discriminative features from each image.

In this thesis, followed by the idea of Class Activation Map (CAM) [
zhou2016learning ] , we modified the VGG16 architecture such that is
becomes capable of generating heatmaps that illustrate the most
discriminative part of image in determining a particular class. As
depicted in Fig. 2.2 , one can add a global average pooling (GAP) layer
[ zhou2016learning ] after the fifth convolutional block of the VGG16
architecture. The output of GAP is then used as the input features for a
fully-connected layer that maps the features to the number of classes.
Given such design, one can generate a coarse representation that
determines the most discriminative part of the input simply by
projecting back the weights of the output layer on to the last
convolutional feature maps. More specifically, let @xmath represent the
activation of unit @xmath in the last convolutional layer at the spatial
location @xmath . A score @xmath for each class @xmath can be obtained
by performing GAP to obtain, or each unit @xmath , a feature @xmath ,
followed by a fully-connected layer with the set of weights @xmath .
That is,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

A CAM for a class @xmath at location @xmath can be then computed as

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

which indicates the importance of the activations at location @xmath in
the final score for class @xmath .

###### DenseNet.

Densely-connected convolutional neural network [ huang2017densely ] , or
DenseNet in short, was proposed in 2016 to address the issue of maximum
information and gradient flow. To this end, as illustrated in Fig. 2.3 ,
in DenseNet, every layer gets as input the output of all previous
layers, thus, is connected to every other layers directly. Thus, since
each layer has a direct access to the gradient computed by the loss
directly, the vanishing gradient problem is remedied to a certain
degree.

Another good characteristic of DenseNet is that, unlike simpler
architectures such as VGG16, the output of the deepest layer contains
features at multiple semantic level, i.e., very rich semantic features
as well as low level features representing edges or texture.

### 2.2 Recurrent Neural Networks

Throughout this thesis, we widely use RNNs to learn the sequences in
both cases of discrete and continuous anticipation tasks. In this
section, we first describe the main concept of RNNs briefly, then
discuss the two variants of RNNs, namely Long-Short Term Memory (LSTM) [
hochreiter1997long ] and Gated Recurrent Unit (GRU) [ cho2014learning ]
, which we later use in the following chapters.

#### 2.2.1 Vanilla RNN.

There are many tasks that are sequential in nature and often cannot be
casted as the ones with fixed-length/fixed-size inputs and outputs,
e.g., speech recognition [ graves2013speech ] , machine translation [
sutskever2014sequence ; maruf2019survey ; chung2014empirical ] , and
video analysis [ donahue2015long ] . In a sequential problem it is
crucial that the decision at each time-step being made based on both the
observation at that given time-step and all previous observations since
samples at different time-steps are not independent of each other. RNNs
perform exactly the same task by taking a representation of previous
outputs into account while processing the input of the current time-step
by updating and using its hidden state . At a given time, each RNN cell
takes as input the representation of the current time-step and the
hidden state that has been updated given the representation of all
previous time-steps. Then, it updates the hidden state given the
previous hidden state and the current input, and optionally can generate
an output. Since RNNs require updating the hidden state at each
time-step, they work sequentially.

More specifically, RNNs learn the temporal relations by mapping the
input sequence to hidden states, and then mapping the hidden states to
the desired output. Given @xmath , the input at time @xmath , the hidden
state is updated as

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath is the hidden state at the previous time-step, @xmath is
the nonlinearity function such as @xmath , and @xmath and @xmath are the
set of learnable weights. One can also generate an output, @xmath , at
each time-step by computing

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where @xmath is a set of learnable weights that maps the hidden state to
the output.

Although shown effective in many sequence learning problems, RNNs suffer
from the vanishing/exploding gradient that makes them hard to train for
the task requires learning long-range dependencies. To remedy this issue
LSTM has been proposed.

###### Lstm.

Long-short term memory unit, or LSTM in short, is a variant of RNN which
is proposed to address the problem of vanishing/exploding gradient when
dealing with long sequences. What makes LSTMs applicable for learning
longer-range dependencies is the concept of memory , in which it allows
the model decide when to forget and when to use and update the previous
hidden state given the input at the current time-step. LSTM comes with a
gating of the form

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where @xmath is a recurrent state, @xmath is a gating function, and
@xmath is a full update. Unlike vanilla RNN, such gating assures that
the derivatives of the loss with respect to the recurrent state @xmath
does not vanish.

The recurrent states comprises a cell state , @xmath , and an output
state , @xmath . The gate @xmath modulates if the cell state should be
forgotten and the gate @xmath modulates if the new update should be
taken into account. The gate @xmath modules in the case that output
state should be reset. The structure of these gates and states are

-   Forget gate: @xmath

-   Input gate: @xmath

-   Full cell state update: @xmath

-   Cell state: @xmath

-   Output gate: @xmath

-   Output state: @xmath

where @xmath is the sigmoid activation function.

In standard practice, the forget gate bias @xmath should be initialized
with a large value such that @xmath and the gating has no effect [
gers1999learning ] . Note that a prediction @xmath can be made given the
hidden state @xmath , so that is why the @xmath is called the output
state.

###### Gru.

The LSTM can be simplified in the a gated recurrent unit, or GRU in
short, with a gating for the recurrent state and a reset gate.
Therefore, one can easily have only the following gates and updates

-   Reset gate: @xmath

-   Forget gate: @xmath

-   Full update: @xmath

-   Hidden update: @xmath

### 2.3 Variational Autoencoders

Consider we have a model @xmath , that works with a set of observed
variables @xmath , which we have access to given a dataset, and a set of
unobserved latent variables @xmath , which typically are not available
in the dataset. This model @xmath aims to model the observations, i.e.,
@xmath through a neural network parameterized with @xmath . The
likelihood of the data can then be computed as the marginal distribution
over the observed variables

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

which is also called the marginal likelihood or the evidence of a model.
@xmath is called a deep latent variable model, or DLVM in short, since
its distributions are parameterized by a neural network @xmath . In a
simple case, one can define @xmath as a factorization,

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

In such case, @xmath is called the prior distribution over the latent
variable @xmath . Learning the maximum likelihood in such model is not
straightforward since @xmath is not tractable [ kingma2013auto ] . This
is related to the intractability of the posterior distribution @xmath ,
which can be computed as

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

To solve the maximum likelihood problem, we would like to have @xmath
and @xmath . Using Variational Inference , we aim to approximate the
true posterior @xmath with another distribution @xmath which is computed
with another neural network parameterized with @xmath (called
variational parameters), such that @xmath . Using such approximation,
Variational Autoencoders [ kingma2013auto ] , or VAEs in short, are able
to optimize the marginal likelihood in a tractable way. The optimization
objective of the VAEs is variational lower bound, also known as evidence
lower bound, or ELBO in short. Recall that variational inference aims to
find an approximation of the posterior that represent the true one. One
way to do this is to minimize the divergence between the approximate and
true posterior using Kullback–Leibler divergence [
kullback1951information ] , or KL divergence in short, that is,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

This can be seen as an expectation,

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

The second term above, i.e., the true posterior, according to the Bayes’
theorem, can be written as @xmath . The data distribution @xmath is
independent of the latent variable @xmath , thus, can be pulled out of
the expectation term,

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

By putting the @xmath in the right hand side of the above equation, we
can write,

  -- -------- -- --------
     @xmath      
     @xmath      
                 (2.12)
  -- -------- -- --------

The second expectation term in above equation, according to definitions,
is the KL divergence between the approximate posterior @xmath and the
prior @xmath distributions. Thus, this can be written as

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

In above equation, @xmath is the log-likelihood of the data which we
would like to optimize, @xmath is the KL divergence between the
approximate and the true posterior distributions, which is not tractable
to compute, but, according to definitions, we know that is non-negative,
@xmath is the reconstruction loss, and @xmath is the KL divergence
between the approximate posterior distribution and a prior over the
latent variable. The last term can be seen as a regularization term over
the latent representation. Therefore, the intractability and
non-negativity of @xmath only allows us to optimize the lower bound on
the log-likelihood of the data,

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

which we call variational or evidence lower bound (ELBO).

In practice, VAEs first learn to generate a latent variable @xmath given
the data @xmath , i.e., approximate the posterior distribution @xmath ,
the encoder, whose goal is to model the variation of the data. From this
latent random variable @xmath , VAEs then generate a sample @xmath by
learning @xmath , the decoder, whose goal is to maximize the log
likelihood of the data.

These two networks, i.e., the encoder ( @xmath ) and the decoder (
@xmath ), are trained jointly, using a prior over the latent variable,
as mentioned above. This prior is usually the standard Normal
distribution, @xmath . In practice, the posterior distribution is
approximated by a Gaussian @xmath , whose parameters are output by the
encoder. To facilitate optimization, the reparameterization trick [
kingma2013auto ] is used. That is, the latent variable is computed as
@xmath , where @xmath is a vector sampled from the standard Normal
distribution.

As an extension to VAEs, conditional VAEs, or CVAEs [ sohn2015learning ]
in short, use auxiliary information, i.e., the conditioning variable or
observation, to generate the data @xmath . In the standard setting, both
the encoder and the decoder are conditioned on the conditioning variable
@xmath . That is, the encoder is denoted as @xmath and the decoder as
@xmath . Then, the objective of the model becomes t

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

In practice, conditioning is typically done by concatenation; the input
of the encoder is the concatenation of the data @xmath and the condition
@xmath , i.e., @xmath , and that of the decoder the concatenation of the
latent variable @xmath and the condition @xmath , i.e., @xmath . Thus,
the prior distribution is still @xmath , and the latent variable is
sampled independently of the conditioning one. It is then left to the
decoder to combine the information from the latent and conditioning
variables to generate a data sample.

## Chapter 3 General Action Anticipation

In this chapter, we focus on action anticipation, the task of predicting
one discrete representation, i.e., action label, of a deterministic
future.

In contrast to the widely studied problem of recognizing an action given
a complete sequence, action anticipation aims to identify the action
from only partially available videos. As such, it is therefore key to
the success of computer vision applications requiring to react as early
as possible, such as autonomous navigation. In this chapter, we propose
a new action anticipation method that achieves high prediction accuracy
even in the presence of a very small percentage of a video sequence. To
this end, we develop a multi-stage LSTM architecture that leverages
context-aware and action-aware features, and introduce a novel loss
function that encourages the model to predict the correct class as early
as possible. Our experiments on standard benchmark datasets evidence the
benefits of our approach; We outperform the state-of-the-art action
anticipation methods for early prediction by a relative increase in
accuracy of 22.0% on JHMDB-21, 14.0% on UT-Interaction and 49.9% on
UCF-101.

### 3.1 Introduction

Understanding actions from videos is key to the success of many
real-world applications, such as autonomous navigation and sports
analysis. While great progress has been made to recognize actions from
complete sequences [ donahue2015long ; wang2016temporal ;
fernando2016discriminative ; bilen2016dynamic ; fernando2016rank ;
wang2013action ] in the past decade, action anticipation [ ryoo2011human
; ma2016learning ; soomro2016predicting ] , which aims to predict the
observed action as early as possible, has become a popular research
problem only recently. Anticipation is crucial in scenarios where one
needs to react before the action is finalized, such as to avoid hitting
pedestrians with an autonomous car, or to forecast dangerous situations
in surveillance scenarios.

The key difference between recognition and anticipation lies in the fact
that the methods tackling the latter should predict the correct class as
early as possible, given only a few frames from the beginning of the
video sequence. To address this, several approaches have introduced new
training losses encouraging the score [ soomro2016online ] or the rank [
ma2016learning ] of the correct action to increase with time, or
penalizing increasingly strongly the classification mistakes [
jain2016recurrent ] . In practice, however, the effectiveness of these
losses remains limited for very early prediction, such as from 1% of the
sequence.

In this chapter, we introduce a novel loss that encourages making
correct predictions very early. Specifically, our loss models the
intuition that some actions, such as running and high jump, are highly
ambiguous after seeing only the first few frames, and false positives
should therefore not be penalized too strongly in the early stages. By
contrast, we would like to predict a high probability for the correct
class as early as possible, and thus penalize false negatives from the
beginning of the sequence. Our experiments demonstrate that, for a given
model, our new loss yields significantly higher accuracy than existing
ones on the task of early prediction.

In particular, in this chapter, we also contribute a novel multi-stage
Long Short Term Memory (LSTM) architecture for action anticipation. This
model effectively extracts and jointly exploits context- and
action-aware features (see Fig. 3.1 ). This is in contrast to existing
methods that typically extract either global representations for the
entire image [ diba2016deepcamp ; wang2016temporal ; donahue2015long ]
or video sequence [ tran2015learning ; karpathy2014large ] , thus not
focusing on the action itself, or localize the feature extraction
process to the action itself via dense trajectories [ TrajectoryPooled ;
IDT ; DiscriminativeRankPooling ] , optical flow [ CNN2Stream ; TSN ;
VLAD3 ] or actionness [ actionness ; ActionnessRanking ; ActionTubelets
; FastActionProposal ; OnlineSEEDS ; SpatioTemporalProposal ] , thus
failing to exploit contextual information. To the best of our knowledge,
only two-stream networks [ TwoStreamNIPS ; CNN2Stream ; cheron2015p ;
SpatioTemporalLSTM ] have attempted to jointly leverage both information
types by making use of RGB frames in conjunction with optical flow to
localize the action. Exploiting optical flow, however, does not allow
these methods to explicitly leverage appearance in the localization
process. Furthermore, computing optical flow is typically expensive,
thus significantly increasing the runtime of these methods. By not
relying on optical flow, our method is significantly more efficient: On
a single GPU, our model analyzes a short video (e.g., 50 frames) 14
times faster than [ TwoStreamNIPS ] and [ CNN2Stream ] .

Our model is depicted in Fig. 3.2 . In a first stage, it focuses on the
global, context-aware information by extracting features from the entire
RGB image. The second stage then combines these context-aware features
with action-aware ones obtained by exploiting class-specific
activations, typically corresponding to regions where the action occurs.
In short, our model first extracts the contextual information, and then
merges it with the localized one.

As evidenced by our experiments, our approach significantly outperforms
the state-of-the-art action anticipation methods on all the standard
benchmark datasets that we evaluated on, including UCF-101 [ UCF101 ] ,
UT-Interaction [ ryoo2010overview ] , and JHMDB21 [ JHMDB ] . We further
show that our combination of context- and action-aware features is also
beneficial for the more traditional task of action recognition.
Moreover, we evaluate the effect of optical flow features for both
action recognition and anticipation.

### 3.2 Related Work

The focus of this chapter is twofold: Action anticipation, with a new
loss that encourages correct prediction as early as possible, and action
modeling, with a model that combines context- and action-aware
information using multi-stage LSTMs. Below, we discuss the most relevant
approaches for these two aspects.

#### 3.2.1 Action Anticipation

The idea of action anticipation was introduced by [ ryoo2009spatio ] ,
which models causal relationships to predict human activities. This was
followed by several attempts to model the dynamics of the observed
actions, such as by introducing integral and dynamic bag-of-words [
ryoo2011human ] , using spatial-temporal implicit shape models [
yu2012predicting ] , extracting human body movements via skeleton
information [ zhao2013online ] , and accounting for the complete and
partial history of observed features [ kong2014discriminative ] .

More recently, [ soomro2016predicting ] proposed to make use of binary
SVMs to classify video snippets into sub-action categories and obtain
the final class label in an online manner using dynamic programming. To
overcome the need to train one classifier per sub-action, [
soomro2016online ] extended this approach to using a structural SVM.
Importantly, this work further introduced a new objective function to
encourage the score of the correct action to increase as time
progresses.

While the above-mentioned work made use of handcrafted features, recent
advances have naturally led to the development of deep learning
approaches to action anticipation. In this context, [ ma2016learning ]
proposed to combine a Convolutional Neural Network (CNN) with an LSTM to
model both spatial and temporal information. The authors further
introduced new ranking losses whose goal is to enforce either the score
of the correct class or the margin between the score of the correct
class and that of the best score to be non-decreasing over time.
Similarly, in [ brain4Cars ] , a new loss that penalizes classification
mistakes increasingly strongly over time was introduced in an LSTM-based
framework that used multiple modalities. While the two above-mentioned
methods indeed aim at improving classification accuracy over time, they
do not explicitly encourage making correct predictions as early as
possible. By contrast, while accounting for ambiguities in early stages,
our new loss still aims to prevent false negatives from the beginning of
the sequence.

Instead of predicting the future class label, in [
vondrick2016anticipating ] , the authors proposed to predict the future
visual representation. However, the main motivation for this was to work
with unlabeled videos, and the learned representation is therefore not
always related to the action itself.

#### 3.2.2 Action Modeling

Most recent action approaches extract global representations for the
entire image [ DeepCAMP ; ActionTransformation ; LRCN ] or video
sequence [ 3DCNN ; LargeScaleCNN ] . As such, these methods do not truly
focus on the actions of interest, but rather compute a context-aware
representation. Unfortunately, context does not always bring reliable
information about the action. For example, one can play guitar in a
bedroom, a concert hall or a yard. To overcome this, some methods
localize the feature extraction process by exploiting dense trajectories
[ TrajectoryPooled ; IDT ; DiscriminativeRankPooling ] or optical flow [
VLAD3 ] . Inspired by objectness, the notion of actionness [ actionness
; ActionnessRanking ; ActionTubelets ; FastActionProposal ; OnlineSEEDS
; SpatioTemporalProposal ] has recently also been proposed to localize
the regions where a generic action occurs. The resulting methods can
then be thought of as extracting action-aware representations. In other
words, these methods go to the other extreme and completely discard the
notion of context which can be useful for some actions, such as playing
football on a grass field.

There is nevertheless a third class of methods that aim to leverage
these two types of information [ TwoStreamNIPS ; CNN2Stream ;
cheron2015p ; SpatioTemporalLSTM ; TSN ] . By combining RGB frames and
optical flow in two-stream architectures, these methods truly exploit
context and motion, from which the action can be localized by learning
to distinguish relevant motion. This localization, however, does not
directly exploit appearance. Here, inspired by the success of these
methods, we develop a novel multi-stage network that also leverages
context- and action-aware information. However, we introduce a new
action-aware representation that exploits the RGB data to localize the
action. As such, our approach effectively leverages appearance for
action-aware modeling, and, by avoiding the expensive optical flow
computation, is much more efficient than the above-mentioned two-stream
models. In particular, our model is about 14 times faster than the
state-of-the-art two-stream network of [ CNN2Stream ] and has less
parameters. The reduction in number of parameters is due to the fact
that [ CNN2Stream ] and [ TwoStreamNIPS ] rely on two VGG-like networks
(one for each stream) with a few additional layers (including 3D
convolutions for [8]). By contrast, our model has, in essence, a single
VGG-like architecture, with some additional LSTM layers, which only have
few parameters. Moreover, our work constitutes the first attempt at
explicitly leveraging context- and action-aware information for action
anticipation. Finally, we introduce a novel multi-stage LSTM fusion
strategy to integrate action and context aware features.

### 3.3 Our Approach

Our goal is to predict the class of an action as early as possible, that
is, after having seen only a very small portion of the video sequence.
To this end, we first introduce a new loss function that encourages
making correct predictions very early. We then develop a multi-stage
LSTM model that makes use of this loss function while leveraging both
context- and action-aware information.

#### 3.3.1 A New Loss for Action Anticipation

As argued above, a loss for action anticipation should encourage having
a high score for the correct class as early as possible. However, it
should also account for the fact that, early in the sequence, there can
often be ambiguities between several actions, such as running and high
jump. Below, we introduce a new anticipation loss that follows these two
intuitions.

Specifically, let @xmath encode the true activity label at time @xmath ,
i.e., @xmath if the sample belongs to class @xmath and 0 otherwise, and
@xmath denote the corresponding label predicted by a given model. We
define our new loss as

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath is the number of action classes and @xmath the length
(number of frames) of the input sequence.

This loss function consists of two terms. The first one penalizes false
negatives with the same strength at any point in time. By contrast, the
second one focuses on false positives, and its strength increases
linearly over time, to reach the same weight as that on false negatives.
Therefore, the relative weight of the first term compared to the second
one is larger at the beginning of the sequence. Altogether, this
encourages predicting a high score for the correct class as early as
possible, i.e., preventing false negatives, while accounting for the
potential ambiguities at the beginning of the sequence, which give rise
to false positives. As we see more frames, however, the ambiguities are
removed, and these false positives are encouraged to disappear.

Our new loss matches the desirable properties of an action anticipation
loss. In the next section, we introduce a novel multi-stage architecture
that makes use of this loss.

#### 3.3.2 Multi-stage LSTM Architecture

To tackle action anticipation, we develop the novel multi-stage
recurrent architecture based on LSTMs depicted by Fig. 3.2 . This
architecture consists of a stage-wise combination of context- and
action-aware information. Below, we first discuss our approach to
extracting these two types of information, and then present our complete
multi-stage recurrent network.

##### Context- and Action-aware Modeling

To model the context- and action-aware information, we introduce the
two-stream architecture shown in Fig. 3.3 . The first part of this
network is shared by both streams and, up to conv5-2, corresponds to the
VGG-16 network [ VGG ] , pre-trained on ImageNet for object recognition.
The output of this layer is connected to two sub-models: One for
context-aware features and the other for action-aware ones. We then
train these two sub-models for the same task of action recognition from
a single image using a cross-entropy loss function defined on the output
of each stream. In practice, we found that training the entire model in
an end-to-end manner did not yield a significant improvement over
training only the two sub-models. In our experiments, we therefore opted
for this latter strategy, which is less expensive computationally and
memory-wise. Below, we first discuss the context-aware sub-network and
then turn to the action-aware one.

###### Context-Aware Feature Extraction.

This sub-model is similar to VGG-16 from conv5-3 up to the last
fully-connected layer, with the number of units in the last
fully-connected layer changed from 1000 (original 1000-way ImageNet
classification model) to the number of activities @xmath .

In essence, this sub-model focuses on extracting a deep representation
of the whole scene for each activity and thus incorporates context. We
then take the output of its fc7 layer as our context-aware features.

###### Action-Aware Feature Extraction.

As mentioned before, the context of an action does not always correlate
with the action itself. Our second sub-model therefore aims at
extracting features that focus on the action itself. To this end, we
draw inspiration from the object classification work of [
zhou2015learning ] . At the core of this work lies the idea of Class
Activation Maps (CAMs). In our context, a CAM indicates the regions in
the input image that contribute most to predicting each class label. In
other words, it provides information about the location of an action.
Importantly, this is achieved without requiring any additional
annotations.

More specifically, CAMs are extracted from the activations in the last
convolutional layer in the following manner. Let @xmath represent the
activation of unit @xmath in the last convolutional layer at spatial
location @xmath . A score @xmath for each class @xmath can be obtained
by performing global average pooling [ NetinNetGAP ] to obtain, for each
unit @xmath , a feature @xmath , followed by a linear layer with weights
@xmath . That is, @xmath . A CAM for class @xmath at location @xmath can
then be computed as

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

which indicates the importance of the activations at location @xmath in
the final score for class @xmath .

Here, we propose to make use of the CAMs to extract action-aware
features. To this end, we use the CAMs in conjunction with the output of
the conv5-3 layer of the model. The intuition behind this is that
conv5-3 extracts high-level features that provide a very rich
representation of the image [ UnderstandingCNN ] and typically
correspond to the most discriminative parts of the object [ DeepEdge ;
BuiltinFGBG ] , or, in our case, the action. Therefore, we incorporate a
new layer to our sub-model, whose output can be expressed as

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath . As shown in Fig. 3.4 , this new layer is then followed by
fully-connected ones, and we take our action-aware features as the
output of the corresponding fc7 layer.

##### Sequence Learning for Action Anticipation

To effectively combine the information contained in the context-aware
and action-aware features described above, we design the novel
multi-stage LSTM model depicted by Fig. 3.2 . This model first focuses
on the context-aware features, which encode global information about the
entire image. It then combines the output of this first stage with our
action-aware features to provide a refined class prediction.

To train this model for action anticipation, we make use of our new loss
introduced in Section 3.3.1 . Therefore, ultimately, our network models
long-range temporal information and yields increasingly accurate
predictions as it processes more frames.

Specifically, we write the overall loss of our model as

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where @xmath is the total number of training sequences. This loss
function combines losses corresponding to the context-aware stage and to
the action-aware stage, respectively. Below, we discuss these two stages
in more detail.

###### Learning Context.

The first stage of our model takes as input our context-aware features,
and passes them through a layer of LSTM cells followed by a
fully-connected layer that, via a softmax operation, outputs a
probability for each action class. Let @xmath be the vector of
probabilities for all classes and all time steps predicted by the first
stage for sample @xmath . We then define the loss for a single sample as

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is our new loss defined in Eq. 4.1 , and @xmath is the
ground-truth class label for sample @xmath .

###### Learning Context and Action.

The second stage of our model aims at combining context-aware and
action-aware information. Its structure is the same as that of the first
stage, i.e., a layer of LSTM cells followed by a fully-connected layer
to output class probabilities via a softmax operation. However, its
input merges the output of the first stage with our action-aware
features. This is achieved by concatenating the hidden activations of
the LSTM layer with our action-aware features. We then make use of the
same loss function as before, but defined on the final prediction. For
sample @xmath , this can be expressed as

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where @xmath is the vector of probabilities for all classes predicted by
the second stage.

###### Inference.

At inference, the input RGB frames are forward-propagated through our
model. We therefore obtain a probability vector for each class at each
frame. While one could simply take the probabilities in the current
frame @xmath to obtain the class label at time @xmath , via @xmath , we
propose to increase robustness by leveraging the predictions of all the
frames up to time @xmath . To this end, we make use of an average
pooling of these predictions over time.

### 3.4 Experiments

In this section, we first compare our method with state-of-the-art
techniques on the task of action anticipation, and then analyze various
aspects of our model, such as the influence of the loss function and of
the different feature types. In the supplementary material, we provide
additional experiments to analyze the effectiveness of different LSTM
architectures, and the influence of the number of hidden units and of
our temporal average pooling strategy. We also report the performance of
our method on the task of action recognition from complete videos with
and without optical flow, and action anticipation with optical flow.

#### 3.4.1 Datasets

For our experiments, we made use of the standard UCF-101 [ UCF101 ] ,
UT-Interaction [ ryoo2010overview ] , and JHMDB-21 [ JHMDB ] benchmarks,
which we briefly describe below.

The UCF-101 dataset consists of 13,320 videos (each contains a single
action) of 101 action classes including a broad set of activities such
as sports, playing musical instruments and human-object interaction,
with an average length of 7.2 seconds. UCF-101 is one of the most
challenging datasets due to its large diversity in terms of actions and
to the presence of large variations in camera motion, cluttered
background and illumination conditions. There are three standard
training/test splits for this dataset. In our comparisons to the
state-of-the-art for both action anticipation and recognition, we report
the average accuracy over the three splits. For the detailed analysis of
our model, however, we rely on the first split only.

The JHMDB-21 dataset is another challenging dataset of realistic videos
from various sources, such as movies and web videos, containing 928
videos and 21 action classes. Similarly to UCF-101, in our comparison to
the state-of-the-art, we report the average accuracy over the three
standard splits of data. Similar to UCF-101 dataset, each video contains
one action starting from the beginning of the video.

The UT-Interaction dataset contains videos of continuous executions of 6
human-human interaction classes: shake-hands, point, hug, push, kick and
punch. The dataset contains 20 video sequences whose length is about 1
minute each. Each video contains at least one execution of each
interaction type, providing us with 8 executions of human activities per
video on average. Following the recommended experimental setup, we used
10-fold leave-one-out cross validation for each of the standard two sets
of 10 videos. That is, within each set, we leave one sequence for
testing and use the remaining 9 for training. Following standard
practice, we also made use of the annotations provided with the dataset
to split each video into sequences containing individual actions.

#### 3.4.2 Implementation Details

###### CNN and LSTM Configuration.

The parameters of the CNN were optimized using stochastic gradient
descent with a fixed learning rate of 0.001, a momentum of 0.9, a weight
decay of 0.0005, and mini-batches of size 32. To train our LSTMs, we
similarly used stochastic gradient descent with a fixed learning rate of
0.001, a momentum of 0.9, and mini-batch size of 32. For all LSTMs, we
used 2048 hidden units. To implement our method, we used Python and
Keras [ keras ] . We will make our code publicly available.

###### Training Procedure.

To fine-tune the network on each dataset, we augment the data, so as to
reduce the effect of over-fitting. The input images were randomly
flipped horizontally and rotated by a random amount in the range -8 to 8
degrees. We then extracted crops according to the following procedure:
(1) Compute the maximum cropping rectangle with a given aspect ratio (
@xmath ) that fits inside the input image. (2) Scale the width and
height of the cropping rectangle by a factor randomly selected in the
range @xmath - @xmath . (3) Select a random location for the cropping
rectangle within the original input image and extract the corresponding
subimage. (4) Scale the subimage to @xmath .

After these geometric transformations, we further applied RGB channel
shifting [ wu2015deep ] , followed by randomly adjusting image
brightness, contrast and saturation with a factor @xmath . The
operations are: for brightness, @xmath , for contrast, @xmath , and for
saturation, @xmath .

#### 3.4.3 Comparison to the State-of-the-Art

We compare our approach to the state-of-the-art action anticipation
results reported on each of the three datasets discussed above. We
further complement these state-of-the-art results with additional
baselines that make use of our context-aware features with the loss of
either [ ma2016learning ] or [ brain4Cars ] . Note that a detailed
comparison of different losses within our model is provided in Section
3.4.4 .

Following standard practice, we report the so-called earliest and latest
prediction accuracies. Note, however, that there is no real agreement on
the proportion of frames that the earliest setting corresponds to. For
each dataset, we make use of the proportion that has been employed by
the baselines (i.e., either 20% or 50%). Note also that our approach
relies on at most @xmath frames (with @xmath in practice). Therefore, in
the latest setting, where the baselines rely on the complete sequences,
we only exploit the first @xmath frames. We believe that the fact that
our method significantly outperforms the state-of-the-art in this
setting despite using less information further evidences the
effectiveness of our approach.

###### Jhmdb-21.

The results for the JHMDB-21 dataset are provided in Table 3.1 . In this
case, following the baselines, earliest prediction corresponds to
observing the first 20% of the sequence. Note that we clearly outperform
all the baselines by a significant margin in both the earliest and
latest settings. Remarkably, we also outperform the methods that rely on
additional information as input, such as optical flow [ soomro2016online
; soomro2016predicting ; ma2016learning ] and Fisher vector features
based on Improved Dense Trajectories [ soomro2016online ] . This clearly
demonstrates the benefits of our approach for anticipation.

###### UT-Interaction.

We provide the results for the UT-Interaction dataset in Table 3.2 .
Here, following standard practice, 50% of the sequence was observed for
earliest prediction, and the entire sequence for latest prediction.
Recall that our approach uses at most @xmath frames for prediction in
both settings, while the average length of a complete sequence is around
120 frames. Therefore, as evidenced by the results, our approach yields
significantly higher accuracy despite using considerably less data as
input.

###### Ucf-101.

We finally compare our approach with our two baselines on the UCF-101
dataset. While this is not a standard benchmark for action anticipation,
this experiment is motivated by the fact that this dataset is relatively
large, has many classes, with similarity across different classes, and
contains variations in video capture conditions. Altogether, this makes
it a challenging dataset to anticipate actions, especially when only a
small amount of data is available. The results on this dataset are
provided in Table 3.3 . Here, the earliest setting corresponds to using
the first 2 frames of the sequences, which corresponds to around 1% of
the data. Again, we clearly outperform the two baselines consisting of
exploiting context-aware features with the loss of either [
ma2016learning ] or [ brain4Cars ] . We believe that this further
evidences the benefits of our approach, which leverages both context-
and action-aware features with our new anticipation loss. A detailed
evaluation of the influence of the different feature types and losses is
provided in the next section.

#### 3.4.4 Analysis

In this section, we provide a more detailed analysis of the influence of
our loss function and of the different feature types on anticipation
accuracy. Finally, we also provide a visualization of our different
feature types, to illustrate their respective contributions.

##### Influence of the Loss Function

Throughout the chapter, we have argued that our novel loss, introduced
in Section 3.3.1 , is better-suited to action anticipation than existing
ones. To evaluate this, we trained several versions of our model with
different losses. In particular, as already done in the comparison to
the state-of-the-art above, we replaced our loss with the ranking loss
of [ ma2016learning ] (ranking loss on detection score) and the loss of
[ brain4Cars ] , but this time within our complete multi-stage model,
with both context- and action-aware features.

Furthermore, we made use of the standard cross-entropy ( CE ) loss,
which only accounts for one activity label for each sequence (at time
@xmath ). This loss can be expressed as

  -- -------- -- -------
     @xmath      
     @xmath      (3.7)
  -- -------- -- -------

We then also modified the loss of [ brain4Cars ] , which consists of an
exponentially weighted softmax, with an exponentially weighted
cross-entropy loss ( ECE ), written as

  -- -------- -- -------
     @xmath      
     @xmath      (3.8)
  -- -------- -- -------

The main drawback of this loss comes from the fact that it does not
strongly encourage the model to make correct predictions as early as
possible. To address this issue, we also introduce a linearly growing
loss ( LGL ), defined as

  -- -------- -- -------
     @xmath      
     @xmath      (3.9)
  -- -------- -- -------

While our new loss, introduced in Section 3.3.1 , also makes use of a
linearly-increasing term, it corresponds to the false positives in our
case, as opposed to the false negatives in the LGL. Since some actions
are ambiguous in the first few frames, we find it more intuitive not to
penalize false positives too strongly at the beginning of the sequence.
This intuition is supported by our results below, which show that our
loss yields better results than the LGL.

In Fig. 3.5 , we report the accuracy of the corresponding models as a
function of the number of observed frames on the UCF-101 dataset. Note
that our new loss yields much higher accuracies than the other ones,
particularly when only a few frames of the sequence are observed; With
only 2 frames observed, our loss yields an accuracy similar to the other
losses with 30–40 frames. With 30fps, this essentially means that we can
predict the action 1 second earlier than other methods. The importance
of this result is exemplified by research showing that a large
proportion of vehicle accidents are due to mistakes/misinterpretations
of the scene in the immediate time leading up to the crash [ brain4Cars
; crash ] .

Moreover, in Fig. 3.6 , we report the performance of the corresponding
models as a function of the number of observed frames when using our
average pooling strategy. Note that this strategy can generally be
applied to any action anticipation method and, as shown by comparing
Figs. 3.5 and 3.6 , increases accuracy, especially at very early stages,
which clearly demonstrates its effectiveness. Note that using it in
conjunction with our loss still yields the best results by a significant
margin.

##### Influence of the Features

We then evaluate the importance of the different feature types,
context-aware and action-aware, on anticipation accuracy. To this end,
we compare models trained using each feature type individually with our
model that uses them jointly. For the models using a single feature
type, we made use of a single LSTM to model temporal information. By
contrast, our approach relies on a multi-stage LSTM, which we denote by
MS-LSTM . Note that all models were trained using our new anticipation
loss. The results of this experiment on the UCF-101 dataset are provided
in Table 3.9 . These results clearly evidence the importance of using
both feature types, which consistently outperforms individual ones.

Since we extract action-aware features, and not motion-aware ones, our
approach will not be affected by irrelevant motion. The CNN that
extracts these features learns to focus on the discriminative parts of
the images, thus discarding the irrelevant information. To confirm this,
we conducted an experiment on some classes of UCF-101 that contain
irrelevant motions/multiple actors, such as Baseball pitch, Basketball,
Cricket Shot and Ice dancing. The results of our action-aware and
context-aware frameworks for these classes are: 66.1% vs. 58.2% for
Baseball pitch, 83% vs. 76.4% for Basketball, 65.1% vs. 58% for Cricket
Shot, and 92.3% vs. 91.7% for Ice dancing. This shows that our
action-aware features can effectively discard irrelevant motion/actors
to focus on the relevant one(s).

##### Visualization

Finally, we provide a better intuition of the kind of information each
of our feature types encode (see Fig. 3.7 ). This visualization was
computed by average pooling over the 512 channels of Conv5-3 (of both
the context-aware and action-aware sub-networks). As can be observed in
the figure, our context-aware features have high activations on regions
corresponding to any relevant object in the scene (context). By
contrast, in our action-aware features, high activations correctly
correspond to the focus of the action. Therefore, they can reasonably
localize the parts of the frame that most strongly participate in the
action happening in the video and reduce the noise coming from context.

### 3.5 Comparison to State-of-the-Art Action Recognition Methods

We first compare the results of our approach to state-of-the-art methods
on UCF-101, JHMDB-21 and UT-Interaction in terms of average accuracy
over the standard training and testing partitions. In Table 3.5 , we
provide the results on the UCF-101 dataset. Here, for the comparison to
be fair, we only report the results of the baselines that do not use any
other information than the RGB image and the activity label (we refer
the readers to the baselines’ papers and the survey [ survey ] for more
detail). In other words, while it has been shown that additional,
handcrafted features, such as dense trajectories and optical flow, can
help improve accuracy [ IDT ; TrajectoryPooled ; VLAD3 ; TwoStreamNIPS ;
DynamicNetwork ] , our goal here is to truly evaluate the benefits of
our method, not of these features. Note, however, that, as discussed in
the next section of this supplementary material, our approach can still
benefit from such features. As can be seen from the table, our approach
outperforms all these RGB-based baselines. In Tables 3.6 and 3.7 , we
provide the results for JHMDB-21 and UT-Interaction. Again, we
outperform all the baselines, even though, in this case, some of them
rely on additional information such as optical flow [ FindingActionTubes
; actionness ; soomro2016online ; soomro2016predicting ; ma2016learning
] or IDT Fisher vector features [ soomro2016online ] . We believe that
these experiments show the effectiveness of our approach at tackling the
action recognition problem.

### 3.6 Exploiting Optical Flow

Note that our approach can also be extended into a two-stream
architecture to benefit from optical flow information, as
state-of-the-art action recognition methods do. In particular, to
extract optical flow features, we made use of the pre-trained temporal
network of [ TwoStreamNIPS ] . We then computed the CNN features from a
stack of 20 optical flow frames (10 frames in the @xmath -direction and
10 frames in the @xmath -direction), from @xmath to @xmath at each time
@xmath . As these features are potentially loosely related to the action
(by focusing on motion), we merge them with the input to the second
stage of our multi-stage LSTM. In Table 3.8 , we compare the results of
our modified approach with state-of-the-art methods that also exploit
optical flow. Note that our two-stream approach yields accuracy
comparable to the state-of-the-art.

We also conducted an experiment to evaluate the effectiveness of
incorporating optical flow in our framework for action anticipation. To
handle the case where less than 10 frames are used, we padded the frame
stack with gray images (with values 127.5). Our flow-based approach
achieved 86.8% for earliest and 91.8% for latest prediction on UCF-101,
thus showing that, if runtime is not a concern, optical flow can indeed
help increase the accuracy of our approach.

We further compare our approach with the two-stream network [
TwoStreamNIPS ] , designed for action recognition, applied to the task
of action anticipation. On UCF-101, this model achieved 83.2% for
earliest and 88.6% for latest prediction, which our approach with
optical flow clearly outperforms.

### 3.7 Effect of Different Feature Types

Here, we evaluate the importance of the different feature types,
context-aware and action-aware, on recognition accuracy. To this end, we
compare models trained using each feature type individually with our
model that uses them jointly. For all models, we made use of LSTMs with
2048 units. Recall that our approach relies on a multi-stage LSTM, which
we denote by MS-LSTM . The results of this experiment for different
losses are reported in Table 3.9 . These results clearly evidence the
importance of using both feature types, which consistently outperforms
using individual ones in all settings.

### 3.8 Robustness to the Number of Hidden Units

Based on our experiments, we found that for large datasets such as
UCF-101, the 512 hidden units that some baselines use (e.g. [ LRCN ;
LSTMAction ] ) do not suffice to capture the complexity of the data.
Therefore, to study the influence of the number of units in the LSTM, we
evaluated different versions of our model with 1024 and 2048 hidden
units (since 512 yields poor results and higher numbers, e.g., 4096,
would require too much memory) and trained the model with 80% training
data and validated on the remaining 20%. For a single LSTM, we found
that using 2048 hidden units performs best. For our multi-stage LSTM,
using 2048 hidden units also yields the best results. We also evaluated
the importance of relying on average pooling in the LSTM. The results of
these different versions of our MS-LSTM framework are provided in Table
3.10 . This shows that, typically, more hidden units and average pooling
can improve accuracy slightly.

### Effect of the LSTM Architecture

Finally, we study the effectiveness of our multi-stage LSTM architecture
at merging our two feature types. To this end, we compare the results of
our MS-LSTM with the following baselines: A single-stage LSTM that takes
as input the concatenation of our context-aware and action-aware
features (Concatenation); The use of two parallel LSTMs whose outputs
are merged by concatenation and then fed to a fully-connected layer
(Parallel). A multi-stage LSTM where the two different feature-types are
processed in the reverse order (Swapped), that is, the model processes
the action-aware features first and, in a second stage, combines them
with the context-aware ones; The results of this comparison are provided
in Table 3.11 . Note that both multi-stage LSTMs outperform the
single-stage one and the two parallel LSTMs, thus indicating the
importance of treating the two types of features sequentially.
Interestingly, processing context-aware features first, as we propose,
yields higher accuracy than considering the action-aware ones at the
beginning. This matches our intuition that context-aware features carry
global information about the image and will thus yield noisy results,
which can then be refined by exploiting the action-aware features.

Furthermore, we evaluate a CNN-only version of our approach, where we
removed the LSTM, but kept our average pooling strategy to show the
effect of our MS-LSTM architecture on top of the CNN. On UCF-101, this
achieved 69.53% for earliest and 73.80% for latest prediction. This
shows that, while this CNN-only framework yields reasonable predictions,
our complete approach with our multistage LSTM benefits from explicitly
being trained on multiple frames, thus achieving significantly higher
accuracy (80.5% and 83.4%, respectively). While the LSTM could in
principle learn to perform average pooling, we believe that the lack of
data prevents this from happening.

### 3.9 Conclusion

In this chapter, we have introduced a novel loss function to address
very early action anticipation. Our loss encourages the model to make
correct predictions as early as possible in the input sequence, thus
making it particularly well-suited to action anticipation. Furthermore,
we have introduced a new multi-stage LSTM model that effectively
combines context-aware and action-aware features. Our experiments have
evidenced the benefits of our new loss function over existing ones.
Furthermore, they have shown the importance of exploiting both context-
and action-aware information. Altogether, our approach significantly
outperforms the state-of-the-art in action anticipation on all the
datasets we applied it to. However, all of these datasets are mainly
designed for action recognition task, predicting the action label given
the full extent of the video. We argue that this is not very well-suited
for the task of anticipation since most actions start from the beginning
of the videos. A proper dataset for action anticipation should contain
videos that actions start after a reasonable amount of observation,
e.g., after 25% of the video length. In the next chapter, we focus on
creating such dataset for the very challenging task of action
anticipation in driving scenarios. We also extend the multi-stage LSTM
from processing two modalities (action-aware and context-aware) to
arbitrary number of modalities.

## Chapter 4 Action Anticipation in Driving Scenarios

Following previous chapter, we continue focusing on a discrete and
determinsitic anticipation task. Motivated by the success of our
approach discussed in previous chapter, we move towards a more
challenging and crucial application, action anticipation in driving
scenarios. In such scenarios, anticipation becomes critical since one,
the driver of ego car, other car’s driver, pedestrian, or cyclist, needs
to react before the action is finalized, for instance, where a car needs
to, e.g., avoid hitting pedestrians and respect traffic lights. While
solutions have been proposed to tackle subsets of the driving
anticipation tasks, by making use of diverse, task-specific sensors,
there is no single dataset or framework that addresses them all in a
consistent manner. In this chapter, we therefore introduce a new,
large-scale dataset, called VIENA @xmath , covering 5 generic driving
scenarios, with a total of 25 distinct action classes. It contains more
than 15K full HD, 5s long videos acquired in various driving conditions,
weathers, daytimes and environments, complemented with a common and
realistic set of sensor measurements. This amounts to more than 2.25M
frames, each annotated with an action label, corresponding to 600
samples per action class. We discuss our data acquisition strategy and
the statistics of our dataset, and benchmark state-of-the-art action
anticipation techniques, including a new multi-modal LSTM architecture
with an effective loss function for action anticipation in driving
scenarios.

### 4.1 Introduction

Understanding actions/events from videos is key to the success of many
real-world applications, such as autonomous navigation, surveillance and
sports analysis. While great progress has been made to recognize actions
from complete sequences [ feichtenhofer2016convolutional ; LRCN ; TSN ;
DynamicNetwork ] , action anticipation, which aims to predict the
observed action as early as possible, has only reached a much lesser
degree of maturity [ sadegh2017encouraging ; vondrick2016anticipating ;
soomro2016predicting ] . Nevertheless, anticipation is a crucial
component in scenarios where a system needs to react quickly, such as in
robotics [ koppula2016anticipating ] , and automated driving [
jain2016brain4cars ; liebner2013generic ; li2017unified ] . Its benefits
have also been demonstrated in surveillance settings [
ramanathan2016detecting ; wang2017hierarchical ] .

In this chapter, we focus on the driving scenario. In this context, when
consulting the main actors in the field, may they be from the computer
vision community, the intelligent vehicle one or the automotive
industry, the consensus is that predicting the intentions of a car’s own
driver, for Advanced Driver Assistance Systems (ADAS), remains a
challenging task for a computer, despite being relatively easy for a
human [ dong2017intention ; olabiyi2017driver ; jain2016recurrent ;
jain2016brain4cars ; rasouli2017agreeing ] . Anticipation then becomes
even more complex when one considers the maneuvers of other vehicles and
pedestrians [ klingelschmitt2016probabilistic ; zyner2017long ;
dong2017intention ] . However, it is key to avoiding dangerous
situations, and thus to the success of autonomous driving.

Over the years, the researchers in the field of anticipation for driving
scenarios have focused on specific subproblems of this challenging task,
such as lane change detection [ morris2011lane ; tawari2014looking ] , a
car’s own driver’s intention [ ohn2014head ] or maneuver [ jain2015car ;
jain2016recurrent ; jain2016brain4cars ; olabiyi2017driver ] recognition
and pedestrian intention prediction [ rasouli2017agreeing ;
pool2017using ; li2017unified ; schulz2015controlled ] . Furthermore,
these different subproblems are typically addressed by making use of
different kinds of sensors, without considering the fact that, in
practice, the automotive industry might not be able/willing to
incorporate all these different sensors to address all these different
tasks.

In this chapter, we study the general problem of anticipation in driving
scenarios, encompassing all the subproblems discussed above, and others,
such as other drivers’ intention prediction, with a fixed, sensible set
of sensors. To this end, we introduce the VI rtual EN vironment for A
ction A nalysis (VIENA @xmath ) dataset, covering the five different
subproblems of predicting driver maneuvers, pedestrian intentions, front
car intentions, traffic rule violations, and accidents. Altogether,
these subproblems encompass a total of 25 distinct action classes. VIENA
@xmath was acquired using the GTA V video game [ GTAgame ] . It contains
more than 15K full HD, 5s long videos, corresponding to more than 600
samples per action class, acquired in various driving conditions,
weathers, daytimes, and environments. This amounts to more than 2.25M
frames, each annotated with an action label. These videos are
complemented by basic vehicle dynamics measurements, and therefore
reflect well the type of information that one could have access to in
practice.

Below, we describe how VIENA @xmath was collected and compare its
statistics and properties to existing datasets. We then benchmark
state-of-the-art action anticipation algorithms on VIENA @xmath , and
introduce a new multi-modal, LSTM-based architecture, together with a
new anticipation loss, which outperforms existing approaches in our
driving anticipation scenarios. Finally, we investigate the benefits of
our synthetic data to address anticipation from real images. In short,
our contributions are: (i) a large-scale action anticipation dataset for
general driving scenarios; (ii) a multi-modal action anticipation
architecture.

VIENA @xmath is meant as an extensible dataset that will grow over time
to include not only more data but also additional scenarios. Note that,
for benchmarking purposes, however, we will clearly define training/test
partitions. A similar strategy was followed by other datasets such as
CityScapes, which contains a standard benchmark set but also a large
amount of additional data. VIENA @xmath is publicly available, together
with our benchmark evaluation, our new architecture and our multi-domain
training strategy.

### 4.2 Viena@xmath

VIENA @xmath is a large-scale dataset for action anticipation, and more
generally action analysis, in driving scenarios. While it is generally
acknowledged that anticipation is key to the success of automated
driving, to the best of our knowledge, there is currently no dataset
that covers a wide range of scenarios with a common, yet sensible set of
sensors. Existing datasets focus on specific subproblems, such as driver
maneuvers and pedestrian intentions [ rasouli2017agreeing ;
pool2017using ; kooij2014context ] , and make use of different kinds of
sensors. Furthermore, with the exception of [ jain2016brain4cars ] ,
none of these datasets provide videos whose first few frames do not
already show the action itself or the preparation of the action.

To create VIENA @xmath , we made use of the GTA V video game, whose
publisher allows, under some conditions, for the non-commercial use of
the footage [ GTA_file ] . Beyond the fact that, as shown in [
richterplaying ] via psychophysics experiments, GTA V provides realistic
images that can be captured in varying weather and daytime conditions,
it has the additional benefit of allowing us to cover crucial
anticipation scenarios, such as accidents, for which real-world data
would be virtually impossible to collect.

In this section, we first introduce the different scenarios covered by
VIENA @xmath and discuss the data collection process. We then study the
statistics of VIENA @xmath and compare it against existing datasets.

#### 4.2.1 Scenarios and Data Collection

As illustrated in Fig. 4.2 , VIENA @xmath covers five generic driving
scenarios. These scenarios are all human-centric, i.e., consider the
intentions of humans, but three of them focus on the car’s own driver,
while the other two relate to the environment (i.e., pedestrians and
other cars). These scenarios are:

1.   Driver Maneuvers (DM). This scenario covers the 6 most common
    maneuvers a driver performs while driving: Moving forward (FF),
    stopping (SS), turning (left (LL) and right (RR)) and changing lane
    (left (CL) and right (CR)). Anticipation of such maneuvers as early
    as possible is critical in an ADAS context to avoid dangerous
    situations.

2.   Traffic Rules (TR). This scenario contains sequences depicting the
    car’s own driver either violating or respecting traffic rules, e.g.,
    stopping at (SR) and passing (PR) a red light, driving in the
    (in)correct direction (WD,CD), and driving off-road (DO).
    Forecasting these actions is also crucial for ADAS.

3.   Accidents (AC). In this scenario, we capture the most common
    real-world accident cases [ volvoaccident ] : Accidents with other
    cars (AC), with pedestrians (AP), and with assets (AA), such as
    buildings, traffic signs, light poles and benches, as well as no
    accident (NA). Acquiring such data in the real world is virtually
    infeasible. Nevertheless, these actions are crucial to anticipate
    for ADAS and autonomous driving.

4.   Pedestrian Intentions (PI). This scenario addresses the question of
    whether a pedestrian is going to cross the road (CR), or has stopped
    (SS) but does not want to cross, or is walking along the road (AS)
    (on the sidewalk). We also consider the case where no pedestrian is
    in the scene (NP). As acknowledged in the literature [ pool2017using
    ; schulz2015controlled ; rasouli2017agreeing ] , early understanding
    of pedestrians’ intentions is critical for automated driving.

5.   Front Car Intentions (FCI). The last generic scenario of VIENA
    @xmath aims at anticipating the maneuvers of the front car. This
    knowledge has a strong influence on the behavior to adopt to
    guarantee safety. The classes are same as the ones in Driver
    Maneuver scenario, but for the driver of the front car.

We also consider an additional scenario consisting of the same driver
maneuvers as above but for heavy vehicles, i.e., trucks and buses. In
all these scenarios, for the data to resemble a real driving experience,
we made use of the equipment depicted in Fig. 4.1 , consisting of a
steering wheel with a set of buttons and a gear stick, as well as of a
set of pedals. We then captured images at 30 fps with a single virtual
camera mounted on the vehicle and facing the road forward. Since the
speed of the vehicle is displayed at a specific location in these
images, we also extracted it using an OCR module [ smith2007overview ]
(see supplementary material for more detail on data collection).
Furthermore, we developed an application that records measurements from
the steering wheel. In particular, it gives us access to the steering
angle every 1 microsecond, which allowed us to obtain a value of the
angle synchronized with each image. Our application also lets us obtain
the ground-truth label of each video sequence by recording the driver
input from the steering wheel buttons. This greatly facilitated our
labeling task, compared to [ richterplaying ; richter2016playing ] ,
which had to use a middleware to access the rendering commands from
which the ground-truth labels could be extracted. Ultimately, VIENA
@xmath consists of video sequences with synchronized measurements of
steering angles and speed, and corresponding action labels.

Altogether, VIENA @xmath contains more than 15K full HD videos (with
frame size of @xmath ), corresponding to a total of more than 2.25M
annotated frames. The detailed number of videos for each class and the
proportions of different weather and daytime conditions of VIENA @xmath
are provided in Fig. 4.3 . Each video contains 150 frames captured at 30
frames-per-second depicting a single action from one scenario. The
action occurs in the second half of the video (mostly around the @xmath
second mark), which makes VIENA @xmath well-suited to research on action
anticipation, where one typically needs to see what happens before the
action starts.

Our goal is for VIENA @xmath to be an extensible dataset. Therefore, by
making our source code and toolbox for data collection and annotation
publicly available, we aim to encourage the community to participate and
grow VIENA @xmath . Furthermore, while VIENA @xmath was mainly collected
for the task of action anticipation in driving scenarios, as it contains
full length videos, i.e., videos of a single drive of 30 minutes on
average depicting multiple actions, it can also be used for the tasks of
action recognition and temporal action localization.

#### 4.2.2 Comparison to Other Datasets

The different scenarios and action classes of VIENA @xmath make it
compatible with existing datasets, thus potentially allowing one to use
our synthetic data in conjunction with real images. For instance, the
action labels in the Driver Maneuver scenario correspond to the ones in
Brain4Cars [ jain2016brain4cars ] and in the Toyota Action Dataset [
olabiyi2017driver ] . Similarly, our last two scenarios dealing with
heavy vehicles contain the same labels as in Brain4Cars [
jain2016brain4cars ] . Moreover, the actions in the Pedestrian Intention
scenario corresponds to those in [ ped_benchmark ] . Note, however,
that, to the best of our knowledge, there is no other dataset covering
our Traffic Rules and Front Car Intention scenarios, or containing data
involving heavy vehicles. Similarly, there is no dataset that covers
accidents involving a driver’s own car. In this respect, the most
closely related dataset is DashCam [ chan2016anticipating ] , which
depicts accidents of other cars. Furthermore, VIENA @xmath covers a much
larger diversity of environmental conditions, such as daytime variations
(morning, noon, afternoon, night, midnight), weather variations (clear,
sunny, cloudy, foggy, hazy, rainy, snowy), and location variations
(city, suburbs, highways, industrial, woods), than existing public
datasets. In the supplementary material, we provide examples of each of
these different environmental conditions.

In addition to covering more scenarios and conditions than other driving
anticipation datasets, VIENA @xmath also contains more samples per class
than existing action analysis datasets, both for recognition and
anticipation. As shown in Table 4.1 , with 600 samples per class, VIENA
@xmath outsizes (at least class-wise) the datasets that are considered
large by the community. This is also the case for other synthetic
datasets, such as VIPER [ richterplaying ] , GTA5 [ richter2016playing ]
, Virtual KITTI [ gaidon2016virtual ] , and SYNTHIA [ ros2017semantic ]
, which, by targeting different problems, such as semantic segmentation
for which annotations are more costly to obtain, remain limited in size.
We acknowledge, however, that, since we target driving scenarios, our
dataset cannot match in absolute size more general recognition datasets,
such as Kinetics.

### 4.3 Benchmark Algorithms

In this section, we first discuss the state-of-the-art action analysis
and anticipation methods that we used to benchmark our dataset. We then
introduce a new multi-modal LSTM-based approach to action anticipation,
and finally discuss how we model actions from our images and additional
sensors.

#### 4.3.1 Baseline Methods

The idea of anticipation was introduced in the computer vision community
almost a decade ago by [ ryoo2009spatio ] . While the early methods [
ryoo2011human ; soomro2016predicting ; soomro2016online ] relied on
handcrafted-features, they have now been superseded by end-to-end
learning methods [ ma2016learning ; jain2016brain4cars ;
sadegh2017encouraging ] , focusing on designing new losses better-suited
to anticipation. In particular, the loss of our approach in Chapter 3
has proven highly effective, achieving state-of-the-art results on
several standard benchmarks.

Despite the growing interest of the community in anticipation, action
recognition still remains more thoroughly investigated. Since
recognition algorithms can be converted to performing anticipation by
making them predict a class label at every frame, we include the
state-of-the-art recognition methods in our benchmark. Specifically, we
evaluate the following baselines:

###### Baseline 1: CNN+LSTMs.

The high performance of CNNs in image classification makes them a
natural choice for video analysis, via some modifications. This was
achieved in [ LRCN ] by feeding the frame-wise features of a CNN to an
LSTM model, and taking the output of the last time-step LSTM cell as
prediction. For anticipation, we can then simply consider the prediction
at each frame. We then use the temporal average pooling strategy
introduced in Chapter 3 , which has proven effective to increase the
robustness of the predictor for action anticipation.

###### Baseline 2: Two-Stream Networks.

Baseline 1 only relies on appearance, ignoring motion inherent to video
(by motion, we mean explicit motion information as input, such as
optical flow). Two-stream architectures, such as the one of [
feichtenhofer2016convolutional ] , have achieved state-of-the-art
performance by explicitly accounting for motion. In particular, this is
achieved by taking a stack of 10 externally computed optical flow frames
as input to the second stream. A prediction for each frame can be
obtained by considering the 10 previous frames in the sequence for
optical flow. We also make use of temporal average pooling of the
predictions.

###### Baseline 3: Multi-Stage LSTMs.

The Multi-Stage LSTM (MS-LSTM) discussed in Chapter 3 constitutes the
state of the art in action anticipation. This model jointly exploits
context- and action-aware features that are used in two successive LSTM
stages. As mentioned above, the key to the success of MS-LSTM is its
training loss function. This loss function can be expressed as

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is the ground-truth label of sample @xmath at frame @xmath
, @xmath the corresponding prediction, and @xmath . The first term
encourages the model to predict the correct action at any time, while
the second term accounts for ambiguities between different classes in
the earlier part of the video.

#### 4.3.2 A New Multi-Modal LSTM

While effective, MS-LSTM suffers from the fact that it was specifically
designed to take two modalities as input, the order of which needs to be
manually defined. As such, it does not naturally apply to our more
general scenario, and must be actively modified, in what might be a
sub-optimal manner, to evaluate it with our action descriptors. To
overcome this, we therefore introduce a new multi-modal LSTM (MM-LSTM)
architecture that generalizes the multi-stage architecture, introduced
in Chapter 3 of this thesis, to an arbitrary number of modalities.
Furthermore, our MM-LSTM also aims to learn the importance of each
modality for the prediction.

Specifically, as illustrated in Fig. 4.4 for @xmath modalities, at each
time @xmath , the representations of the @xmath input modalities are
first passed individually into an LSTM with a single hidden layer. The
activations of these @xmath hidden layers are then concatenated into an
@xmath matrix @xmath , which acts as input to a time-distributed
fully-connected layer (FC-Pool). This layer then combines the @xmath
modalities to form a single vector @xmath . This representation is then
passed through another LSTM whose output is concatenated with the
original @xmath via a skip connection. The resulting @xmath matrix is
then compacted into a 1024D vector via another FC-Pool layer. The output
of this FC-Pool layer constitutes the final representation and acts as
input to the classification layer.

The reasoning behind this architecture is the following. The first
FC-Pool layer can learn the importance of each modality. While its
parameters are shared across time, the individual, modality-specific
LSTMs can produce time-varying outputs, thus, together with the FC-Pool
layer, providing the model with the flexibility to change the importance
of each modality over time. In essence, this allows the model to learn
the importance of the modalities dynamically. The second LSTM layer then
models the temporal variations of the combined modalities. The skip
connection and the second FC-Pool layer produce a final representation
that can leverage both the individual, modality-specific representations
and the learned combination of these features.

###### Learning.

To train our model, we make use of the loss of Eq. 4.1 . However, we
modify the weights as @xmath , allowing the influence of the second term
to vary nonlinearly. In practice, we set @xmath and @xmath , yielding
the weight function of Fig. 4.4 . These values were motivated by the
study of [ pentland1999modeling ] , which shows that driving actions
typically undergo the following progression: In a first stage, the
driver is not aware of an action or decides to take an action. In the
next stage, the driver becomes aware of an action or decides to take
one. This portion of the video contains crucial information for
anticipating the upcoming action. In the last portion of the video, the
action has started. In this portion of the video, we do not want to make
a wrong prediction, thus penalizing false positives strongly. Generally
speaking, our sigmoid-based strategy to define the weight reflects the
fact that, in practice and in contrast with many academic datasets, such
as UCF-101 [ soomro2012ucf101 ] and JHMDB-21 [ JhuangICCV2013 ] ,
actions do not start right at the beginning of a video sequence, but at
any point in time, the goal being detecting them as early as possible.

During training, we rely on stage-wise supervision, by introducing an
additional classification layer after the second LSTM block, as
illustrated in Fig. 4.4 . At test time, however, we remove this
intermediate classifier to only keep the final one. We then make use of
the temporal average pooling strategy introduced in Chapter 3 to
accumulate the predictions over time.

#### 4.3.3 Action Modeling

Our MM-LSTM can take as input multiple modalities that provide diverse
and complementary information about the observed data. Here, we briefly
describe the different descriptors that we use in practice.

-    Appearance-based Descriptors. Given a frame at time @xmath , the
    most natural source of information to predict the action is the
    appearance depicted in the image. To encode this information, we
    make use of a slightly modified DenseNet [ huang2016densely ] ,
    pre-trained on ImageNet. See Section 4.3.4 for more detail. Note
    that we also use this DenseNet as appearance-based CNN for Baselines
    1 and 2.

-    Motion-based Descriptors. Motion has proven a useful cue for action
    recognition [ feichtenhofer2017spatiotemporal ;
    feichtenhofer2016convolutional ] . To encode this, we make use of a
    similar architecture as for our appearance-based descriptors, but
    modify it to take as input a stack of optical flows. Specifically,
    we extract optical flow between @xmath consecutive pairs of frames,
    in the range @xmath , and form a @xmath flow stack encoding
    horizontal and vertical flows. We fine-tune the model pre-trained on
    ImageNet for the task of action recognition, and take the output of
    the additional fully-connected layer as our motion-aware descriptor.
    Note that we also use this DenseNet for the motion-based stream of
    Baseline 2.

-    Vehicle Dynamics. In our driving context, we have access to
    additional vehicle dynamics measurements. For each such measurement,
    at each time @xmath , we compute a vector from its value @xmath ,
    its velocity @xmath and its acceleration @xmath . To map these
    vectors to a descriptor of size comparable to the appearance- and
    motion-based ones, inspired by [ fernando2017going ] , we train an
    LSTM with a single hidden layer modeling the correspondence between
    vehicle dynamics and action label. In our dataset, we have two types
    of dynamics measurements, steering angle and speed, which results in
    two additional descriptors.

When evaluating the baselines, we report results of both their standard
version, relying on the descriptors used in the respective papers, and
of modified versions that incorporate the four descriptor types
discussed above. Specifically, for CNN-LSTM, we simply concatenate the
vehicle dynamics descriptors and the motion-based descriptors to the
appearance-based ones. For the Two-Stream baseline, we add a second
two-stream sub-network for the vehicle dynamics and merge it with the
appearance and motion streams by adding a fully-connected layer that
takes as input the concatenation of the representation from the original
two-stream sub-network and from vehicle dynamics two-stream sub-network.
Finally, for MS-LSTM, we add a third stage that takes as input the
concatenation of the second-stage representation with the vehicle
dynamics descriptors.

#### 4.3.4 Implementation Details

We make use of the DenseNet-121 [ huang2016densely ] , pre-trained on
ImageNet, to extract our appearance- and motion-based descriptors.
Specifically, we replace the classifier with a fully-connected layer
with 1024 neurons followed by a classifier with @xmath outputs, where
@xmath is the number of classes. We fine-tune the resulting model using
stochastic gradient descent for @xmath epochs with a fixed learning rate
of @xmath and mini-batches of size @xmath . Recall that, for the
motion-based descriptors, the corresponding DenseNet relies on @xmath
flow stacks as input, which requires us to also replace the first layer
of the network. To initialize the parameters of this layer, we average
the weights over the three channels corresponding to the original RGB
channels, and replicate these average weights @xmath times [ TSN ] . We
found this scheme to perform better than random initialization.

### 4.4 Benchmark Evaluation and Analysis

We now report and analyze the results of our benchmarking experiments.
For these experiments to be as extensive as possible given the available
time, we performed them on a representative subset of VIENA @xmath
containing about 6.5K videos acquired in a large variety of
environmental conditions and covering all 25 classes. This subset
contains 277 samples per class, and thus still outsizes most action
analysis datasets, as can be verified from Table 4.1 . The detailed
statistics of this subset are provided in the supplementary material.

To evaluate the behavior of the algorithms in different conditions, we
defined three different partitions of the data. The first one, which we
refer to as Random in our experiments, consists of randomly assigning
70% of the samples to the training set and the remaining 30% to the test
set. The second partition considers the daytime of the sequences, and is
therefore referred to as Daytime . In this case, the training set is
formed by the day images and the test set by the night ones. The last
partition, Weather , follows the same strategy but based on the
information about weather conditions, i.e., a training set of clear
weather and a test set of rainy/snowy/… weather.

Below, we first present the results of our benchmarking on the Random
partition, and then analyze the challenges related to our new dataset.
We finally evaluate the benefits of our synthetic data for anticipation
from real images, and analyze the bias of VIENA @xmath . Note that
additional results including benchmarking on the other partitions and
ablation studies of our MM-LSTM model are provided in the supplementary
material. Note also that the scenarios and classes acronyms are defined
in Section 4.2.1 .

#### 4.4.1 Action Anticipation on VIENA@xmath

We report the results of our benchmark evaluation on the different
scenarios of VIENA @xmath in Table 4.2 for the original versions of the
baselines, relying on the descriptors used in their respective paper,
and in Table 4.3 for their modified versions that incorporate all
descriptor types. Specifically, we report the recognition accuracies for
all scenarios after every second of the sequences. Note that, in
general, incorporating all descriptor types improves the results.
Furthermore, while the action recognition baselines perform quite well
in some scenarios, such as Accidents and Traffic Rules for the
two-stream model, they are clearly outperformed by the anticipation
methods in the other cases. Altogether, our new MM-LSTM consistently
outperforms the baselines, thus showing the benefits of learning the
dynamic importance of the modalities.

A comparison of the baselines with our approach on the Daytime and
Weather partitions of VIENA @xmath is provided in the supplementary
material. In essence, the conclusions of these experiments are the same
as those drawn above.

#### 4.4.2 Challenges of VIENA@xmath

Based on the results above, we now study what challenges our dataset
brings, such as which classes are the most difficult to predict and
which classes cause the most confusion. We base this analysis on the
per-class accuracies of our MM-LSTM model, which achieved the best
performance in our benchmark. This, we believe, can suggest new
directions to investigate in the future.

Our MM-LSTM per-class accuracies are provided in Table 4.4 , and the
corresponding confusion matrices at the earliest (after seeing 1 second)
and latest (after seeing 5 seconds) predictions in Fig. 4.5 . Below, we
discuss the challenges of the various scenarios.

1.   Driver maneuver: After 1s, most actions are mistaken for Moving
    Forward , which is not surprising since the action has not started
    yet. After 5s, most of the confusion has disappeared, except for
    Changing Lane (left and right), for which the appearance, motion and
    vehicle dynamics are subject to small changes only, thus making this
    action look similar to Moving Forward .

2.   Accident: Our model is able to distinguish No Accident from the
    different accident types early in the sequence. Some confusion
    between the different types of accident remains until after 5s, but
    this would have less impact in practice, as long as an accident is
    predicted.

3.   Traffic rule: As in the maneuver case, there is initially a high
    confusion with Correct Direction , due to the fact that the action
    has not started yet. The confusion is then much reduced as we see
    more information, but Passing a Red Light remains relatively poorly
    predicted.

4.   Pedestrian intention: The most challenging class for early
    prediction in this scenario is Pedestrian Walking along the Road .
    The prediction is nevertheless much improved after 5s.

5.   Front car intention: Once again, at the beginning of the sequence,
    there is much confusion with the Forward class. After 5s, the
    confusion is significantly reduced, with, as in the maneuver case,
    some confusion remaining between the Change lane classes and the
    Forward class, illustrating the subtle differences between these
    actions.

#### 4.4.3 Benefits of VIENA@xmath for Anticipation from Real Images

To evaluate the benefits of our synthetic dataset for anticipation on
real videos, we make use of the JAAD dataset [ rasouli2017agreeing ] for
pedestrian intention recognition, which is better suited to deep
networks than other datasets, such as [ ped_benchmark ] , because of its
larger size (58 videos vs. 346). This dataset is, however, not annotated
with the same classes as we have in VIENA @xmath , as its purpose is to
study pedestrian and driver behaviors at pedestrian crossings. To make
JAAD suitable for our task, we re-annotated its videos according to the
four classes of our Pedestrian Intention scenario, and prepared a
corresponding train/test split. JAAD is also heavily dominated by the
Crossing label, requiring augmentation of both training and test sets to
have a more balanced number of samples per class.

To demonstrate the benefits of VIENA @xmath in real-world applications,
we conduct two sets of experiments: 1) Training on JAAD from scratch,
and 2) Pre-training on VIENA @xmath followed by fine-tuning on JAAD. For
all experiments, we use appearance-based and motion-based features,
which can easily be obtained for JAAD. The results are shown in Table
4.5 . This experiment clearly demonstrates the effectiveness of using
our synthetic dataset that contains photo-realistic samples simulating
real-world scenarios.

Another potential benefit of using synthetic data is that it can reduce
the amount of real data required to train a model. To evaluate this, we
fine-tuned an MM-LSTM trained on VIENA @xmath using a random subset of
JAAD ranging from 20% to 100% of the entire dataset. The accuracies at
every second of the sequence and for different percentages of JAAD data
are shown in Fig. 4.6 . Note that with 60% of real data, our MM-LSTM
pre-trained on VIENA @xmath already outperforms a model trained from
scratch on 100% of the JAAD data. This shows that our synthetic data can
save a considerable amount of labeling effort on real images.

#### 4.4.4 Bias Analysis

For a dataset to be unbiased, it needs to be representative of the
entire application domain it covers, thus being helpful in the presence
of other data from the same application domain. This is what we aimed to
achieve when capturing data in a large diversity of environmental
conditions. Nevertheless, every dataset is subject to some bias. For
example, since our data is synthetic, its appearance differs to some
degree from real images, and the environments we cover are limited by
those of the GTA V video game. However, below, we show empirically that
the bias in VIENA @xmath remains manageable, making it useful beyond
evaluation on VIENA @xmath itself. In fact, the experiments of Section
4.4.3 on real data already showed that performance on other datasets,
such as JAAD, can be improved by making use of VIENA @xmath . To further
evaluate the bias of the visual appearance of our dataset, we relied on
the idea of domain adversarial training introduced in [
ganin2015unsupervised ] . In short, given data from two different
domains, synthetic and real in our case, domain adversarial training
aims to learn a feature extractor, such as a DenseNet, so as to fool a
classifier whose goal is to determine from which domain a sample comes.
If the visual appearance of both domains is similar, such a classifier
should perform poorly. We therefore trained a DenseNet to perform action
classification from a single image using both VIENA @xmath and JAAD
data, while learning a domain classifier to discriminate real samples
from synthetic ones. The performance of the domain classifier quickly
dropped down to chance, i.e., 50%. To make sure that this was not simply
due to failure to effectively train the domain classifier, we then froze
the parameters of the DenseNet while continuing to train the domain
classifier. Its accuracy remained close to chance, thus showing that the
features extracted from both domains were virtually indistinguishable.
Note that the accuracy of action classification improved from 18% to 43%
during the training, thus showing that, while the features are
indistinguishable to the discriminator, they are useful for action
classification.

In our context of synthetic data, another source of bias could arise
from the specific users who captured the data. To analyze this, we
trained an MM-LSTM model from the data acquired by a single user,
covering all classes and all environmental conditions, and tested it on
the data acquired by another user. In Table 4.6 , we compare the average
accuracies of this experiment to those obtained when training and
testing on data from the same user. Note that there is no significant
differences, showing that our data generalizes well to other users.

### 4.5 Benchmark Evaluation on All Splits of VIENA@xmath

We now report our results on the different scenarios of VIENA @xmath .
In Table 4.7 and Table 4.8 , we report the recognition accuracies for
all scenarios after every second of the sequences. Note that some
scenarios are easier to anticipate than others, e.g., accidents vs.
driver maneuvers. Altogether, the trend remains the same as in the
Random split.

### 4.6 Ablation Study for our MM-LSTM

In this section, we evaluate different aspects of our new MM-LSTM model.

###### Effect of Loss Function

We first evaluate the influence of our new loss function on our results.
To this end, we replaced it with the standard cross-entropy loss, and
with the loss of [ sadegh2017encouraging ] , which has proven effective
at action anticipation and corresponds to a linear weight instead of our
nonlinear one. In Table 4.9 , we compare the results of our MM-LSTM
model trained with these losses and with ours on the Accidents scenario
of VIENA @xmath . Note that our loss yields more accurate predictions,
particularly in the early stages of the videos. This confirms the
effectiveness of our false positive weighting strategy, well-suited for
action anticipation.

###### Effect of Different Features

We then analyze the impact of the different features on our results. To
this end, we trained separate MM-LSTM models with a single modality as
input. We then evaluate these models on the Accidents and Front car
intention scenarios of VIENA @xmath . The results are provided in Table
4.10 . Note that, in the former scenario, speed and appearance play an
important role, whereas in the latter one, the steering angle and motion
are the dominant features, closely followed by appearance. This shows
that different scenarios need to rely more strongly on different
features. Nevertheless, using all descriptors jointly always improves
over using them individually, which evidences that our MM-LSTM model can
effectively discover the importance of the different modalities.

###### Effect of the Number of Hidden Units

Furthermore, we study the effect of varying the number of hidden units
in the LSTM blocks of our MM-LSTM model. To this end, we simultaneously
varied this number between 256 and 4096 across all LSTM units in our
model. As can be seen in Table 4.11 , where we provide the detailed
numbers for this analysis for the Random partition, the results are
stable and only marginally improving with more hidden units. However,
the number of parameters of the MM-LSTM model increases drastically, in
this case from @xmath to @xmath . In our experiments in this chapter, we
therefore used 1024 hidden units for all LSTMs, which is a good balance
between performance and number of parameters ( @xmath ).

###### Effect of Input Block

As mentioned earlier, the FC-Pool layer of our MM-LSTM model encodes the
importance of the modalities and is shared across time. While, on its
own, this cannot change the importance of the modalities over time, the
modality-specific LSTMs can adapt their outputs over time so as to serve
this purpose. To further evidence the importance of these two modules in
learning a good combination of the modalities, we performed an
experiment where we deactivated them in turn, and trained models on all
24 permutations of the 4 input modalities. The results in Table 4.12 ,
where we provide the average accuracy and standard deviation over the 24
permutations, show that using both modules not only yields higher
accuracy, but also makes our network virtually unaffected by the order
of the modalities, i.e., very small standard deviation.

###### Comparison to the State of the Art

To evaluate the effectiveness of our model on another dataset, and
compare it to the state-of-the-art anticipation techniques, we make use
of the standard JHMDB-21 benchmark. For this experiment, we followed the
experimental setup of [ sadegh2017encouraging ] as we did in Chapter 3 ,
and used their context-aware and action-aware features instead of our
features, to make the comparison more fair. The results of several
state-of-the-art methods on this dataset are shown in Table 4.13 . Note
that we outperform all these methods. Importantly, we outperform the
multi-stage LSTM introduced in Chapter 3 , which manually defines the
order in which the two feature types should be considered. This, we
believe, truly evidences the benefits of our approach, which can
automatically learn the dynamic importance of the input modalities for
the final prediction. Moreover, we evaluated our model when trained with
the loss function introduced in Chapter 3 . Note that, in the JHMDB-21
dataset, all actions start from the beginning of the videos and continue
to the end. This, however, is not the case in VIENA @xmath , where the
actions begin to occur only during the second half of the video.
Therefore, the loss function introduced in Chapter 3 , which linearly
grows over time, performs better on JHMDB-21, as it is better suited for
such cases, but performs worse on VIENA @xmath , as shown in Table 4.9 .
In other words, the design of the loss function should consider the
nature of the target dataset. JHMDB depicts non-driving activities,
where the actions occur over the entire duration of the video. This is
in fact a case where linearly growing weights better model the way the
action occurs, i.e., linearly over the entire sequence, and thus perform
better. Note, however, that the best accuracy in Table 4.13 is still
achieved by our approach, but by using linear weights, and that our
approach with nonlinear weights follows very closely, thus showing good
robustness to the precise definition of the weights.

### 4.7 Viena@xmath Visualization: Environmental Diversity

To collect data for the different scenarios of VIENA @xmath , we made
use of various vehicles, including heavy vehicles for Driver Maneuver of
heavy vehicles’ driver. To provide diversity in terms of vehicle
conditions, type, etc., we made use of different kinds of cars, such as
old cars that are harder to control during turns or at high speed, and
sport cars that are better-suited when driving at high speed (see Fig.
4.7 ). For heavy vehicles, we collected data with two types of vehicles,
trucks and buses. However, again for the sake of diversity, we made use
of heavy vehicles with different specifications, top speed, and length
(see Fig. 4.8 ).

Note that, for all of our videos, the camera was mounted on the car so
as to provide us with a front view. For heavy vehicles, the camera was
mounted in a similar manner, but providing a higher viewpoint, since
heavy vehicles are taller. In Figs. 4.9 , 4.10 and 4.11 , we provide
example images acquired at different daytimes, i.e., from pre-dawn to
midnight, in different weather conditions and at different locations,
respectively. We believe that the combination of these conditions, i.e.,
different vehicles, daytimes, weathers, and locations, provide a very
diverse set of data which will reduce the effect of dataset bias.

### 4.8 Viena@xmath Visualization: Scenarios

In Fig. 4.12 , we visualize samples from the different classes of
different scenarios of VIENA @xmath .

### 4.9 Conclusion

In this chapter, we have introduced a new large-scale dataset for
general action anticipation in driving scenarios, which covers a broad
range of situations with a common set of sensors. Furthermore, we have
proposed a new MM-LSTM architecture allowing us to learn the importance
of multiple input modalities for action anticipation. Our experimental
evaluation has shown the benefits of our new dataset and of our new
model. This chapter also concludes our contributions on the task of
anticipating a discrete representation of a deterministic future. In the
next chapter, we start studying the problem of anticipating a continuous
representation of a stochastic future, with the focus on diverse human
motion prediction.

## Chapter 5 A Stochastic Conditioning Scheme for Diverse Human Motion
Prediction

In this chapter, we start studying the problem of anticipating
continuous future representations of a stochastic process. That is, for
instance, the case of human motion prediction. Human motion prediction,
the task of predicting future 3D human poses given a sequence of
observed ones, has been mostly treated as a deterministic problem.
However, human motion is a stochastic process: Given an observed
sequence of poses, multiple future motions are plausible. Existing
approaches to modeling this stochasticity typically combine a random
noise vector with information about the previous poses. This
combination, however, is done in a deterministic manner, which gives the
network the flexibility to learn to ignore the random noise.
Alternatively, in this chapter, we propose to stochastically combine the
root of variations with previous pose information, so as to force the
model to take the noise into account. We exploit this idea for motion
prediction by incorporating it into a recurrent encoder-decoder network
with a conditional variational autoencoder block that learns to exploit
the perturbations. Our experiments on two large-scale motion prediction
datasets demonstrate that our model yields high-quality pose sequences
that are much more diverse than those from state-of-the-art stochastic
motion prediction techniques.

### 5.1 Introduction

Human motion prediction aims to forecast the sequence of future poses of
a person given past observations of such poses. To achieve this,
existing methods typically rely on recurrent neural networks (RNNs) that
encode the person’s motion [ martinez2017human ; gui2018adversarial ;
walker2017pose ; kundu2018bihmp ; barsoum2018hp ; pavllo2019modeling ;
pavllo2018quaternet ] . While they predict reasonable motions, RNNs are
deterministic models and thus cannot account for the highly stochastic
nature of human motion; given the beginning of a sequence, multiple,
diverse futures are plausible. To correctly model this, it is therefore
critical to develop algorithms that can learn the multiple modes of
human motion, even when presented with only deterministic training
samples.

Recently, several attempts have been made at modeling the stochastic
nature of human motion [ yan2018mt ; barsoum2018hp ; walker2017pose ;
kundu2018bihmp ; lin2018human ] . These methods rely on sampling a
random vector that is then combined with an encoding of the observed
pose sequence. In essence, this combination is similar to the
conditioning of generative networks; the resulting models aim to
generate an output from a random vector while taking into account
additional information about the content.

While standard conditioning strategies, i.e., concatenating the
condition to the latent variable, may be effective for many tasks, as in
[ yan2016attribute2image ; kulkarni2015deep ; esser2018variational ;
engel2017latent ; bao2017cvae ; larsen2015autoencoding ] , they are
ill-suited for motion prediction. The reason is the following: In other
tasks, the conditioning variable only provides auxiliary information
about the output to produce, such as the fact that a generated face
should be smiling. By contrast, in motion prediction, it typically
contains the core signal to produce the output, i.e., the information
about the previous poses. We empirically observed that, since the
prediction model is trained using deterministic samples (i.e., one
condition per sample), it can then simply learn to ignore the random
vector and still produce a meaningful output based on the conditioning
variable only. In other words, the model can ignore the root of
variations, and thus essentially become deterministic. This problem was
discussed in [ bowman2015generating ] in the context of unconditional
text generation, and we identified it in our own motion prediction
experiments.

We introduce a simple yet effective approach to counteracting this loss
of diversity and thus to generating truly diverse future pose sequences.
At the heart of our approach lies the idea of Mix-and-Match
perturbations: Instead of combining a noise vector with the conditioning
variables in a deterministic manner, we randomly select and perturb a
subset of these variables. By randomly changing this subset at every
iteration, our strategy prevents training from identifying the root of
variations and forces the model to take it into account in the
generation process. Consequently, as supported by our experiments, our
approach produces not only high-quality predictions but also truly
diverse ones.

In short, our contributions in this chapter are (i) a novel way of
imposing diversity into conditional VAEs, called Mix-and-Match
perturbations ; (ii) a new motion prediction model capable of generating
multiple likely future pose sequences from an observed motion; (iii) a
new set of evaluation metrics for quantitatively measuring the quality
and the diversity of generated motions, thus facilitating the comparison
of different stochastic approaches; and (iv) a curriculum learning
paradigm for training generative models that use Mix-and-Match
perturbation as the stochastic conditioning scheme. Despite its
simplicity, curriculum learning of variation is essential to achieve
optimal performance in case of imposing large variations.

### 5.2 Related Work

Deterministic Motion Prediction. Most motion prediction approaches are
based on deterministic models [ pavllo2018quaternet ; pavllo2019modeling
; gui2018adversarial ; jain2016structural ; martinez2017human ;
gui2018few ; fragkiadaki2015recurrent ; ghosh2017learning ;
mao2019learning ] , casting motion prediction as a regression task where
only one outcome is possible given the observations. Due to the success
of RNN-based methods at modeling sequence-to-sequence learning problems,
many attempts have been made to address motion prediction within a
recurrent framework [ martinez2017human ; gui2018adversarial ;
walker2017pose ; kundu2018bihmp ; barsoum2018hp ; pavllo2019modeling ;
pavllo2018quaternet ] . Typically, these approaches try to learn a
mapping from the observed sequence of poses to the future sequence.
Another group of study addresses this problem within feed-forward models
[ mao2019learning ; li2018convolutional ; butepage2017deep ] , either
with fully-connected [ butepage2017deep ] , convolutional [
li2018convolutional ] , or more recently, graph neural networks [
mao2019learning ] . While a deterministic approach may produce accurate
predictions, it fails to reflect the stochastic nature of human motion,
where multiple plausible outcomes can be highly likely for a single
given series of observations. Modeling this diversity is the topic of
this chapter, and we therefore focus the discussion below on the other
methods that have attempted to do so.

Stochastic Motion Prediction. The general trend to incorporate
variations in the predicted motions consists of combining information
about the observed pose sequence with a random vector. In this context,
two types of approaches have been studied: The techniques that directly
incorporate the random vector into the RNN decoder, e.g., as in GANs,
and those that make use of an additional Conditional Variational
Autoencoder (CVAE) [ sohn2015learning ] to learn a latent variable that
acts as the root of variation.

In the first class of methods, [ lin2018human ] sample a random vector
@xmath at each time step and add it to the pose input to the RNN
decoder. By relying on different random vectors at each time step,
however, this strategy is prone to generating discontinuous motions. To
overcome this, [ kundu2018bihmp ] make use of a single random vector to
generate the entire sequence. This vector is both employed to alter the
initialization of the decoder and concatenated with a pose embedding at
each iteration of the RNN. By relying on concatenation as a mean to fuse
the condition and the random vector, these two methods contain
parameters that are specific to the random vector, and thus give the
model the flexibility to ignore this information. In [ barsoum2018hp ] ,
instead of using concatenation, the random vector is added to the hidden
state produced by the RNN encoder. While addition prevents having
parameters that are specific to the random vector, this vector is first
transformed by multiplication with a parameter matrix, and thus can
again be zeroed out so as to remove the source of diversity, as we
observe empirically in Section 5.4.2 .

The second category of stochastic methods introduce an additional CVAE
between the RNN encoder and decoder. This allows them to learn a more
meaningful transformation of the noise, combined with the conditioning
variables, before passing the resulting information to the RNN decoder.
In this context, [ walker2017pose ] propose to directly use the pose as
conditioning variable. As will be shown in our experiments, while this
approach is able to maintain some degree of diversity, albeit less than
ours, it yields motions of lower quality because of its use of
independent random vectors at each time step. In [
butepage2018anticipating ] , an approach similar to that of [
walker2017pose ] is proposed, but with one CVAE per limb. As such, this
method suffers from the same discontinuity problem as [ walker2017pose ;
lin2018human ] . Finally, instead of perturbing the pose, the recent
work of [ yan2018mt ] uses the RNN decoder hidden state as conditioning
variable in the CVAE, concatenating it with the random vector. While
this approach generates high-quality motions, it suffers from the fact
that the CVAE decoder gives the model the flexibility to ignore the
random vector.

Ultimately, both classes of methods suffer from the fact that they allow
the model to ignore the random vector, thus relying entirely on the
conditioning information to generate future poses. Here, we introduce an
effective way to maintain the root of diversity by randomizing the
combination of the random vector with the conditioning variable.

### 5.3 Proposed Method

In this section, we first introduce our Mix-and-Match approach to
introducing diversity in CVAE-based motion prediction. We then describe
the motion prediction architecture we used in our experiments and
propose a novel evaluation metric to quantitatively measure the
diversity and quality of generated motions.

#### 5.3.1 Mix-and-Match Perturbation

The main limitation of prior work in the area of stochastic motion
modeling, such as [ walker2017pose ; barsoum2018hp ; yan2018mt ] , lies
in the way they fuse the random vector with the conditioning variable,
i.e., RNN hidden state or pose, which causes the model to learn to
ignore the randomness and solely exploit the deterministic conditioning
information to generate motion. To overcome this, we propose to make it
harder for the model to decouple the random variable from the
deterministic information. Specifically, we observe that the way the
random variable and the conditioning one are combined in existing
methods is deterministic. We therefore propose to make this process
stochastic.

Similarly to [ yan2018mt ] , we propose to make use of the hidden state
as the conditioning variable and generate a perturbed hidden state by
combining a part of the original hidden state with the random vector.
However, as illustrated in Fig. 5.1 , instead of assigning predefined,
deterministic indices to each piece of information, such as the first
half for the hidden state and the second one for the random vector, we
assign the values of the hidden state to random indices and the random
vector to the complementary ones.

More specifically, as depicted in Fig. 5.1 , a mix-and-match
perturbation takes two vectors of size @xmath as input, say @xmath and
@xmath , and combines them in a stochastic manner. To this end, it
relies on two operations. The first one, called Sampling , chooses
@xmath indices uniformly at random among the @xmath possible values,
given a sampling rate @xmath . Let us denote by @xmath , the resulting
set of indices and by @xmath the complementary set. The second
operation, called Resampling , then creates a new @xmath -dimensional
vector whose values at indices in @xmath are taken as those at
corresponding indices in the first input vector and the others at the
complementary indices, of dimension @xmath , in the second input vector.

#### 5.3.2 M&M Perturbation for Motion Prediction

Let us now describe the way we use our mix-and-match perturbation
strategy for motion prediction. To this end, we first discuss the
network we rely on during inference, and then explain our training
strategy.

Inference. The high-level architecture we use at inference time is
depicted by Fig. 5.2 (Top). It consists of an RNN encoder that takes
@xmath poses @xmath as input and outputs an @xmath -dimensional hidden
vector @xmath . A random @xmath -dimensional portion of this hidden
vector, @xmath , is then combined with an @xmath -dimensional random
vector @xmath via our mix-and-match perturbation strategy. The resulting
@xmath -dimensional output is passed through a small neural network
(i.e., ResBlock2 in Fig. 5.2 ) that reduces its size to @xmath , and
then fused with the remaining @xmath -dimensional portion of the hidden
state, @xmath . This, in turn, is passed through the VAE decoder to
produce the final hidden state @xmath , from which the future poses
@xmath are obtained via the RNN decoder.

Training. During training, we aim to learn both the RNN parameters and
the CVAE ones. Because the CVAE is an auto encoder, it needs to take as
input information about future poses. To this end, we complement our
inference architecture with an additional RNN future encoder, yielding
the training architecture depicted in Fig. 5.2 (Bottom). Note that, in
this architecture, we incorporate an additional mix-and-match
perturbation that fuses the hidden state of the RNN past encoder @xmath
with that of the RNN future encoder @xmath and forms @xmath . This
allows us to condition the VAE encoder in a manner similar to the
decoder. Note that, for each mini batch, we use the same set of sampled
indices for all mix-and-match perturbation steps throughout the network.
Furthermore, following the standard CVAE strategy, during training, the
random vector @xmath is sampled from the approximate posterior
distribution @xmath , whose mean @xmath and covariance matrix @xmath are
produced by the CVAE encoder with parameters @xmath . This, in practice,
is done by the reparameterization technique [ kingma2013auto ] . Note
that, during inference, @xmath since we do not have access to @xmath ,
hence to @xmath and @xmath .

To learn the parameters of our model, we rely on the availability of a
dataset @xmath containing @xmath videos @xmath depicting a human
performing an action. Each video consists of a sequence of @xmath poses,
@xmath , and each pose comprises @xmath joints forming a skeleton,
@xmath . The pose of each joint is represented as a 4D quaternion. Given
this data, we train our model by minimizing a loss function of the form

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

The first term in this loss compares the output of the network with the
ground-truth motion using the squared loss. That is,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath is the predicted 4D quaternion for the @xmath joint at time
@xmath in sample @xmath , and @xmath the corresponding ground-truth one.
The main weakness of this loss is that it treats all joints equally.
However, when working with angles, some joints have a much larger
influence on the pose than others. For example, because of the kinematic
chain, the pose of the shoulder affects that of the rest of the arm,
whereas the pose of the wrists has only a minor effect. To take this
into account, we define our second loss term as the error in 3D space.
That is,

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath is the predicted 3D position of joint @xmath at time @xmath
in sample @xmath and @xmath the corresponding ground-truth one. These 3D
positions can be computed using forward kinematics, as in [
pavllo2018quaternet ; pavllo2019modeling ] . Note that, to compute this
loss, we first perform a global alignment of the predicted pose and the
ground-truth one by rotating the root joint to face [0, 0, 0]. Finally,
following standard practice in training VAEs, we define our third loss
term as the KL divergence

  -- -------- -- -------
     @xmath      
     @xmath      (5.4)
  -- -------- -- -------

where @xmath and @xmath is the length of the diagonal of the covariance
matrix. In practice, since our VAE appears within a recurrent model, we
weigh @xmath by a function @xmath corresponding to the KL annealing
weight of [ bowman2015generating ] . We start from @xmath , forcing the
model to encode as much information in @xmath as possible, and gradually
increase it to @xmath , following a logistic curve.

#### 5.3.3 Curriculum Learning of Variation

The parameter @xmath in our mix-and-match perturbation scheme determines
a trade-off between stochasticity and motion quality. The larger @xmath
, the larger the portion of the original hidden state that will be
perturbed. Thus, the model incorporates more randomness and less
information from the original hidden state. As such, given a large
@xmath , it becomes harder for the model to deliver motion information
from the observation to the future representation since a large portion
of the hidden state is changing randomly. In particular, we observed
that training becomes unstable if we use a large @xmath from the
beginning, with the motion-related loss terms fluctuating while the
prior loss @xmath quickly converges to zero. To overcome this while
still enabling the use of sufficiently large values of @xmath to achieve
high diversity, we introduce the curriculum learning strategy depicted
by Fig. 5.3 . In essence, we initially select @xmath indices in a
deterministic manner and gradually increase the randomness of these
indices as training progresses. More specifically, given a set of @xmath
indices, we replace @xmath indices from the sampled ones with the
corresponding ones from the remaining @xmath indices. Starting from
@xmath , we gradually increase @xmath to the point where all @xmath
indices are sampled uniformly randomly. More details, including the
pseudo-code of this approach, are provided in the supplementary
material. This strategy helps the motion decoder to initially learn and
incorporate information about the observations (as in [ yan2018mt ] ),
yet, in the long run, still prevents it from ignoring the random vector.

#### 5.3.4 Quality and Diversity Metrics

When dealing with multiple plausible motions, or in general diverse
solutions to a problem, evaluation is a challenge. The standard metrics
used for deterministic motion prediction models are ill-suited to this
task, because they typically compare the predictions to the ground
truth, thus inherently penalizing diversity. For multiple motions, two
aspects are important: the diversity and the quality , or realism, of
each individual motion. Prior work typically evaluates these aspects via
human judgement. While human evaluation is highly valuable, and we will
also report human results, it is very costly and time-consuming. Here,
we therefore introduce two metrics that facilitate the quantitative
evaluation of both quality and diversity of generated human motions. We
additionally extend the Inception-Score [ salimans2016improved ] to our
task.

To measure the quality of generated motions, we propose to rely on a
binary classifier trained to discriminate real (ground-truth) samples
from fake (generated) ones. The accuracy of this classifier on the test
set is thus inversely proportional to the quality of the generated
motions. In other words, high-quality motions are those that are not
distinguishable from real ones. Note that we do not rely on adversarial
training, i.e., we do not define a loss based on this classifier when
training our model. To measure the diversity of the generated motions, a
naive approach would consist of relying on the distance between the
generated motion and a reference one. However, generating identical
motions that are all far from the reference one would therefore yield a
high value, while not reflecting diversity. To prevent this, we propose
to make use of the average distance between all pairs of generated
motions. A similar idea has been investigated to measure the diversity
of solutions in other domains [ yuan2019diverse ;
yang2018diversitysensitive ] .

The quality and diversity metrics can reliably evaluate a stochastic
motion prediction model. While providing valuable information, drawing
conclusion about the performance of a model is always easier with a
single measure. To this end, we extend the Inception-Score (IS) [
salimans2016improved ] used to measure the quality of images produced by
a generative model. Our extension to IS is twofold: (1) Inspired by [
huang2018multimodal ] , we extend IS to the conditional case, where the
condition provides the core signal to generate the sample; (2) Our
extended IS measures the quality and diversity of sequential solutions.
To this end, we first train a strong skeleton-based action classifier [
li2018co ] on ground-truth motions. With then compute the IS of each of
the multiple motions generated for a given condition (observed motion),
and report the mean IS and its standard deviation over all conditions.
The reason behind reporting the mean IS over all conditions is to
evaluating the diversity of generated motions given each observation.
Note that studying IS only makes it hard to evaluate quality and
diversity separately, and thus we still believe that all three metrics
are required. Importantly, we show empirically that our proposed metrics
are in line with human judgement, at considerably lower cost.

### 5.4 Experiments

We now evaluate the effectiveness of our approach at generating multiple
plausible motions. To this end, we use Human3.6M [ h36m_pami ] and the
CMU Mocap dataset ¹ ¹ 1 Available at http://mocap.cs.cmu.edu/ . , two
large publicly available motion capture datasets. In this section, we
introduce the baselines and give information about the implementation
details and evaluation metrics. We then provide all the experimental
results.

###### Baselines.

We compare our Mix-and-Match approach with the different means of
imposing variation in motion prediction discussed in Section 5.2 , i.e.,
concatenating the hidden state to a learned latent variable, Yan et al.,
[ yan2018mt ] , concatenating the pose to a learned latent variable at
each time-step, Walker et al., [ walker2017pose ] , and adding a
(transformed) random noise to the hidden state, Barsoum et al., [
barsoum2018hp ] . For the comparison to be fair, we use 16 frames (i.e.,
640ms) as observation to generate the next 60 frames (i.e., 2.4sec) for
all baselines. All models are trained with the same motion
representation, annealing strategy, backbone network, and losses, except
for Barsoum et al., [ barsoum2018hp ] which cannot make use of @xmath .

###### Implementation Details.

The motion encoders and decoders in our model are single layer GRU [
cho2014learning ] networks, comprising 1024 hidden units each. For the
decoders, we use a teacher forcing technique [ williams1989learning ] to
decode motion. At each time-step, the network chooses with probability
@xmath whether to use its own output at the previous time-step or the
ground-truth pose as input. We initialize @xmath , and decrease it
linearly at each training epoch such that, after a certain number of
epochs, the model becomes completely autoregressive, i.e., uses only its
own output as input to the next time-step. We train our model on a
single GPU with the Adam optimizer [ kingma2014adam ] for 100K
iterations. We use a learning rate of 0.001 and a mini-batch size of 64.
To avoid exploding gradients, we use the gradient-clipping technique of
[ pascanu2013difficulty ] for all layers in the network. We implemented
our model using the Pytorch framework of [ paszke2017automatic ] .

###### Evaluation Metrics.

In addition to the metrics discussed in Section 5.3.4 , we also report
the standard ELBO metric (approximated by the reconstruction loss and
the KL on the test set) and the sampling loss (S-MSE) of our approach
and the state-of-the-art stochastic motion prediction techniques.
However, evaluating only against one ground-truth motion (i.e., one
sample from multi-modal distribution), as in MSE or S-MSE, can lead to a
high score for one sample while penalizing other plausible modes. This
behavior is undesirable since it cannot differentiate a multi-modal
solution from a good, but uni-modal one. Similarly, the metrics in [
yan2018mt ] or the approximate ELBO only evaluate quality given one
single ground truth. While the ground truth has high quality, there
exist multiple high quality continuations of an observation, which our
proposed metric accounts for. As discussed in Section 5.3.4 , we
evaluate the quality and diversity of the predicted motions. Note, these
metrics should be considered together, since each one taken separately
does not provide a complete picture of how well a model can predict
multiple plausible future motions. For example, a model can generate
diverse but unnatural motions, or, conversely, realistic but identical
motions. To evaluate quality, as discussed in Section 5.3.4 , we use a
recurrent binary classifier whose task is to determine whether a sample
comes from the ground-truth data or was generated by the model. We train
such a classifier for each method, using 25K samples generated at
different training steps together with 25K real samples, forming a
binary dataset of 50K motions for each method. To evaluate diversity, as
discussed in Section 5.3.4 , we compute the mean Euclidean distance from
each motion to all other @xmath motions when generating @xmath motions.
To compute IS, we trained an action classifier [ li2018co ] with 50K
real motions. We then compute the IS for @xmath samples per condition
for 50 different conditions. We followed Section 5.3.4 to report IS.
Furthermore, we also performed a human evaluation to measure the quality
of the motions generated by each method. To this end, we asked eight
users to rate the quality of 50 motions generated by each method, for a
total of 200 motions. The ratings were defined on a scale of 1-5, 1
representing a low-quality motion and 5 a high-quality, realistic one.
We then scaled the values to the range 0-50 to make them comparable with
those of the binary classifier.

#### 5.4.1 Comparison to the State-of-the-Art

In this section, we quantitatively compare our approach to the
state-of-the-art stochastic motion prediction techniques in terms of
approximate ELBO, Diversity, Quality, and IS on a held-out test set, as
well as the training KL term at convergence. Table 5.1 shows the results
on the Human3.6M and CMU Mocap datasets.

These results show that Mix-and-Match is highly capable of learning the
variation in human motion while maintaining a good motion quality. This
is shown by IS, Diversity, and Quality metrics, which should be
considered together. It is also evidenced by the low reconstruction loss
and higher KL term on the test set. The training KL term at convergence
also shows that, in Mix-and-Match, the posterior does not collapse to
the prior distribution, i.e., the model does not ignore the latent
variable. While the MSE of our approach is slightly higher than that of
Yan et al., [ yan2018mt ] on Human3.6M and Barsoum et al., [
barsoum2018hp ] on the CMU Mocap dataset, we effectively exploit the
latent variables, as demonstrated by the KL term on the test set, the IS
and diversity metric and the qualitative results provided in Fig. 5.4
and in the supplementary material. As evidenced by the examples of
diverse motions generated by our model in Fig. 5.4 , given a single
observation, Mix-and-Match is able to generate diverse, but natural
motions ² ² 2 See the video of our results in the supplementary
material. .

#### 5.4.2 Analysis on Diversity and Quality

To provide a deeper understanding of our approach, we evaluate different
aspects of Mix-and-Match. All these experiments were done on Human3.6M.
In the following, we first analyze the diversity in the hidden state
space, i.e., the first part of the model where variation is imposed. We
then evaluate the quality and diversity of prediction when tested at
different stages of the training. We also perform a human evaluation on
the quality of the generated motions, comparing it with our inexpensive,
automatic quality metric. Finally, we compare Mix-and-Match with other
stochastic techniques in terms of sampling error (S-MSE), i.e., by
computing the error of the best of @xmath generated motions given the
ground-truth one. More experiments and visualizations are provided in
the supplementary material.

###### Diversity in Hidden State Space.

In Fig. 5.5 , we plot the diversity of the representations used as input
to the RNN decoders of [ yan2018mt ] and [ barsoum2018hp ] , two
state-of-the-art methods that are closest in spirit to our approach.
Here, diversity is measured as the average pairwise distance across the
@xmath representations produced for a single series of observations. We
report the mean diversity over 50 samples and the corresponding standard
deviation. As can be seen from the figure, the diversity of [ yan2018mt
] and [ barsoum2018hp ] decreases as training progresses, thus
supporting our observation that these models learn to ignore the
perturbations. As evidenced by the black curve, which shows an
increasing diversity as training progresses, our approach produces not
only high-quality predictions but also truly diverse ones. The gradual
but steady increase in diversity of our approach is due to our
curriculum learning strategy described in Section 5.3.3 . Without it,
training is less stable, with large diversity variations.

###### Diversity and Quality in Motion Space.

Now, we thoroughly compare our approach with state-of-the-art stochastic
motion prediction models in terms of quality and diversity. The results
of the metrics of Section 5.3.4 are provided in Fig. 5.6 (Left and
Middle) and those of the human evaluation in Fig. 5.6 (Right). Below, we
analyze the results of the different models.

As can be seen from Fig. 5.6 , [ yan2018mt ] tends to ignore the random
variable @xmath , thus ignoring the root of variation. As a consequence,
it achieves a low diversity, much lower than ours, but produces samples
of high quality, albeit almost identical, which is also shown in
qualitatively in Fig. 3 of the supplementary material. We empirically
observed that the magnitude of the weights acting on @xmath to be orders
of magnitude smaller than that of acting on the condition, 0.008 versus
232.85 respectively. Note that this decrease in diversity occurs after
16K iterations, indicating that the model takes time to identify the
part of the hidden state that contains the randomness. Nevertheless, at
iteration 16K, prediction quality is low, and thus one could not simply
stop training at this stage. Note that the lack of diversity of [
yan2018mt ] is also evidenced by Fig. 5.5 . As can be verified in Fig.
5.6 (Right), where [ yan2018mt ] appears in a region of high quality but
low diversity, the results of human evaluation match those of our
classifier-based quality metric.

Fig. 5.6 also evidences the limited diversity of the motions produced by
[ barsoum2018hp ] despite its use of random noise during inference. Note
that the authors of [ barsoum2018hp ] mentioned in their chapter that
the random noise was added to the hidden state. Only by studying their
publicly available code ³ ³ 3 https://github.com/ebarsoum/hpgan did we
understand the precise way this combination was done. In fact, the
addition relies on a parametric, linear transformation of the noise
vector. That is, the perturbed hidden state is obtained as @xmath .
Because the parameters @xmath are learned , the model has the
flexibility to ignore @xmath (the magnitude of @xmath is in the order of
@xmath ), which causes the behavior observed in Figs. 5.6 and 5.5 . Note
that the authors of [ barsoum2018hp ] acknowledged that, despite their
best efforts, they noticed very little variations between predictions
obtained with different @xmath values. By depicting [ barsoum2018hp ] in
a region of high quality but low diversity, the human evaluation results
in Fig. 5.6 (Right) again match those of our classifier-based quality
metric.

As can be seen in Fig. 5.6 (Left and Middle), [ walker2017pose ]
produces motions with higher diversity than [ barsoum2018hp ; yan2018mt
] , but of much lower quality. The main reason behind this is that the
random vectors that are concatenated to the poses at each time-step are
sampled independently of each other, which translates to discontinuities
in the generated motions. Human evaluation in Fig. 5.6 (Right) further
confirms that [ walker2017pose ] ’s results lie in a low-quality,
medium-diversity region.

The success of our approach is confirmed by Fig. 5.6 (Left and Middle).
Our model generates diverse motions, even after a long training time,
and the quality of these motions is high. While this quality is slightly
lower than that of [ barsoum2018hp ; yan2018mt ] when looking at our
classifier-based metric, it is rated higher by IS and humans, as can be
verified from Fig. 5.6 (Right) and Table 5.1 . Altogether, these results
confirm the ability of our approach to generate highly diverse yet
realistic motions.

###### Evaluating the Sampling Error.

We now quantitatively compare our approach with other stochastic
baselines in terms of sampling error (aka S-MSE). To this end, we follow
the evaluation setting of deterministic motion prediction (as in [
fragkiadaki2015recurrent ; pavllo2019modeling ; pavllo2018quaternet ;
martinez2017human ; gui2018adversarial ] ) which allows further
comparisons to deterministic baselines. We report the standard metric,
i.e., the Euclidean distance between the generated and ground-truth
Euler angles (aka MAE). To evaluate this metric for our method and the
stochastic motion prediction models, which generate multiple, diverse
predictions, we make use of the best sample among the @xmath generated
ones with @xmath for the stochastic baselines and for our approach. This
evaluation procedure aims to show that, among the @xmath generated
motions, at least one is close to the ground truth. As shown in Table
5.2 , by providing higher diversity, our approach outperforms the
baselines. Similarly, in Table 5.3 , we compare the best of @xmath
sampled motions for our approach with the deterministic motion
prediction techniques. Note that the goal of this experiment is not to
provide a fair comparison to deterministic models, but to show that,
among the diverse set of motions generated by our model, there exists at
least one motion that is very close to the ground-truth one. The point
of bringing the MAE of other deterministic methods, is to show how good
deterministic models, with sophisticated architectures and complicated
loss functions, perform on this task.

### 5.5 Conclusion

In this chapter, we have proposed an effective way of perturbing the
hidden state of an RNN such that it becomes capable of learning the
multiple modes of human motions. Our evaluation of quality and
diversity, based on both new quantitative metrics and human judgment,
have evidenced that our approach outperforms existing stochastic
methods. Generating diverse plausible motions given limited observations
has many applications, especially when the motions are generated in an
action-agnostic manner, as done here. For instance, our model can be
used for human action forecasting [ rodriguez2018action ;
aliakbarian2016deep ; shi2018action ] , as well as the methods
introduced in Chapter 3 and Chapter 4, where one seeks to anticipate the
action as early as possible, or for motion inpainting, where, given
partial observations, one aims to generate multiple in-between
solutions. Although Mix-and-Match can successfully encourage diversity
in the generate motions, it does not account for the fact that all
generated motions should convey the semantic information exist in the
past observation. This could be particularly important in certain
applications, such as automatic animation generation. In the next
chapter, we introduce another VAE-based framework that not only
encourages diversity, but also encourages generating semantically
plausible motions.

## Chapter 6 Better Motion Generation via Variational Autoencoders with
Learned Conditional Priors

Following previous chapter, we continue studying the problem of
anticipating continuous representations of a stochastic future, with the
focus on diverse human motion prediction. Similar to previous chapter,
in this context, a popular approach consists of using Conditional
Variational Autoencoders (CVAEs). In this chapter, we identify a major
weakness of the CVAE frameworks used in existing techniques: During
training, they rely on a global prior on the latent variable, thus
making it independent of the conditioning signal. At inference, this
translates to sampling latent variables that do not match the given CVAE
condition. In this chapter, we directly address this by conditioning the
sampling of the latent variable on the CVAE condition, thus encouraging
it to carry relevant information. Our experiments demonstrate that our
approach not only yields samples of higher quality while retaining the
semantic information contained in the observed 3D pose sequence, but
also helps the model to avoid the posterior collapse, a known problem of
VAEs with expressive decoders. We additionally show the generality of
our approach by using it for diverse image captioning.

### 6.1 Introduction

Human motion prediction is the task of forecasting plausible 3D human
motion continuation(s) given a sequence of past 3D human poses. To
address this problem, prior work mostly relies on recurrent
encoder-decoder architectures, where the encoder processes the observed
motion, and the decoder generates a single estimated future trajectory
given the encoded representation of the past [ martinez2017human ;
gui2018adversarial ; walker2017pose ; kundu2018bihmp ; barsoum2018hp ;
pavllo2019modeling ; pavllo2018quaternet ; wei2019motion ] . While this
approach yields valid future motion, it tends to ignore the fact that
human motion is stochastic in nature; given one single observation,
multiple diverse continuations of the motion are likely and plausible.
The lack of stochasticity of these encoder-decoder methods ensues from
the fact that both the network operations and the sequences in the
training dataset are deterministic. In this chapter, we introduce an
approach to modeling this stochasticity by learning multiple modes of
human motion.

Recent attempts that account for human motion stochasticity rely on
combining a random vector with an encoding of the observed pose sequence
[ butepage2018anticipating ; yan2018mt ; barsoum2018hp ; walker2017pose
; kundu2018bihmp ; lin2018human ; aliakbarian2019MixAndMatch ] . In
particular, the state-of-the-art approaches to diverse human motion
prediction [ aliakbarian2019MixAndMatch ; yan2018mt ] make use of
conditional variational auto-encoders (CVAEs). In this chapter, we argue
that standard CVAEs are ill-suited to this task for the following
reason.

In essence, VAEs utilize neural networks to learn the distribution of
the data. To this end, VAEs first learn to generate a latent variable
@xmath given the data @xmath , i.e., approximate the posterior
distribution @xmath , where @xmath are the parameters of a neural
network, the encoder, whose goal is to model the variation of the data.
From this latent random variable @xmath , VAEs then generate a new
sample @xmath by learning @xmath , where @xmath denotes the parameters
of another neural network, the decoder, whose goal is to maximize the
log likelihood of the data. These two networks, i.e., the encoder and
the decoder, are trained jointly, using a prior over the latent
variable. By using a variational approximation of the posterior,
training translates to maximizing the variational lower bound of the log
likelihood with respect to the parameters @xmath and @xmath , given by

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

where the second term on the right hand side encodes the KL divergence
between the posterior @xmath and a chosen prior distribution @xmath . As
an extension to VAEs, CVAEs use auxiliary information, i.e., the
conditioning variable or observation, to generate the data @xmath . In
the standard setting, both the encoder and the decoder are conditioned
on the conditioning variable @xmath . That is, the encoder becomes
@xmath and the decoder @xmath . Then, in theory, the objective of the
model should become

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

In practice, however, the prior distribution of the latent variable is
still assumed to be independent of @xmath , i.e., @xmath . As
illustrated by Fig. 6.1 , at test time, this translates to sampling a
latent variable from a region of the prior that is unlikely to be highly
correlated with the (observed) condition.

In this chapter, we overcome this limitation by explicitly making the
sampling of the latent variable depend on the condition. In other words,
instead of using @xmath as prior distribution, we truly use @xmath .
This not only respects the theory behind the design of CVAEs, but, as we
empirically demonstrate, leads to generating motions of higher quality,
that preserve the context of the conditioning signal, i.e., the observed
past motion. To achieve this, we develop a CVAE architecture that learns
a distribution not only of the latent variable but also of the
conditioning one. We then use this distribution as a prior over the
latent variable, making its sampling explicitly dependent on the
condition. As such, we name our method LCP-VAE , for L earned C
onditional P rior.

Our experiments show that not only does LCP-VAE yield a much wider
variety of plausible samples than state-of-the-art stochastic motion
prediction methods, but it also preserves the semantic information of
the condition, such as the type of action performed by the person,
without explicitly exploiting this information. We also show that, by
unifying latent variable sampling and conditioning, we can mitigate the
posterior collapse problem, a well-known issue for VAEs with expressive
decoders [ bowman2015generating ; yang2017improved ; kim2018semi ;
gulrajani2016pixelvae ; liu2019cyclical ; semeniuta2017hybrid ;
zhao2017infovae ; tolstikhin2017wasserstein ; chen2016variational ;
alemi2017fixing ; he2019lagging ; li2019surprisingly ; goyal2017z ;
lucas2018auxiliary ; dieng2018avoiding ; van2017neural ;
guu2018generating ; xu2018spherical ; davidson2018hyperspherical ;
razavi2019preventing ] , but unexplored for CVAEs. For more detail on
this problem, we refer the reader to Appendix A. Finally, we show that
our approach generalizes to the task of diverse image captioning.

### 6.2 Related Work

Because of space limitation, in this section, we focus on the human
motion prediction literature. We nonetheless review of the literature on
image captioning work and posterior collapse in Appendix A.

Most motion prediction methods are based on deterministic models [
pavllo2018quaternet ; wei2019motion ; pavllo2019modeling ;
gui2018adversarial ; jain2016structural ; martinez2017human ; gui2018few
; fragkiadaki2015recurrent ; ghosh2017learning ] , casting motion
prediction as a regression task where only one outcome is possible given
the observation. While this may produce accurate predictions, it fails
to reflect the stochastic nature of human motion, where multiple futures
can be highly likely for a single given series of observations. Modeling
stochasticity is the topic of this chapter, and we therefore focus the
discussion below on the other methods that have attempted to do so.

The general trend to incorporate variations in the predicted motions
consists of combining information about the observed pose sequence with
a random vector. In this context, two types of approaches have been
studied: The techniques that directly incorporate the random vector into
the RNN decoder and those that make use of an additional CVAE. In the
first class of methods, [ lin2018human ] samples a random vector @xmath
at each time step and adds it to the pose input of the RNN decoder. By
relying on different random vectors at each time step, however, this
strategy is prone to generating discontinuous motions. To overcome this,
[ kundu2018bihmp ] makes use of a single random vector to generate the
entire sequence. As we will show in our experiments, by relying on
concatenation, these two methods contain parameters that are specific to
the random vector, and thus give the model the flexibility to ignore
this information. In [ barsoum2018hp ] , instead of using concatenation,
the random vector is added to the hidden state produced by the RNN
encoder. While addition prevents having parameters that are specific to
the random vector, this vector is first transformed by multiplication
with a learnable parameter matrix, and thus can again be zeroed out so
as to remove the source of diversity, as observed in our experiments.

The second category of stochastic methods introduce an additional CVAE
between the RNN encoder and decoder. In this context, [ walker2017pose ]
proposes to directly use the pose as conditioning variable. As will be
shown in our experiments, while this approach is able to maintain some
degree of diversity, albeit less than ours, it yields motions of lower
quality because of its use of independent random vectors at each time
step. Instead of perturbing the pose, [ yan2018mt ] uses the RNN decoder
hidden state as conditioning variable in the CVAE, concatenating it with
the random vector. While this approach generates high-quality motions,
it suffers from the fact that the CVAE decoder gives the model the
flexibility to ignore the random vector, which therefore yields
low-diversity outputs. To overcome this, [ aliakbarian2019MixAndMatch ]
perturbs the hidden states via a stochastic Mix-and-Match operation
instead of concatenation. Through such a perturbation, the decoder is
not able decouple the noise and the condition. However, since the
perturbation is not learned and is a non-parametric operation, the
quality and the context of the generated motion are inferior to those
obtained with our approach. More importantly, all of the above-mentioned
CVAE-based approaches use priors that are independent of the condition.
We will show in our experiments that such designs are ill-suited for
human motion prediction. By contrast, our approach uses a conditional
prior and is thus able to generate diverse motions of higher quality,
carrying the contextual information of the conditioning signal.

### 6.3 Unifying Sampling Latent Variable and Conditioning

In this section, we introduce our approach as a general framework with a
new conditioning scheme for CVAEs that is capable of generating diverse
and plausible samples, where the latent variables are sampled from an
appropriate region of the prior distribution. In essence, our framework
consists of two autoencoders, one acting on the conditioning signal and
the other on the samples we wish to model. The latent representation of
the condition then serves as a conditioning variable to generate samples
from a learned distribution.

As discussed above, we are interested in problems that are stochastic in
nature; given one condition, multiple plausible and natural samples are
likely. However, for complicated tasks such as motion prediction, the
training data is typically insufficiently sampled, in that, for any
given condition, the dataset contains only a single sample, in effect
making the data appear deterministic. For instance, in motion
prediction, we never observed twice the same past motion with two
different future ones. Moreover, for such problems, the condition
provides the core signal to generate a good sample, even in a
deterministic model. Therefore, it is highly likely that a CVAE trained
for this task learns to ignore the latent variable and rely only on the
condition to produce its output [ aliakbarian2019MixAndMatch ; yan2018mt
] . This relates to the posterior collapse problem in strongly
conditioned VAEs [ aliakbarian2019MixAndMatch ] , as discussed in more
detail in Appendix A. Below, we address this by forcing the sampling of
the random latent variable to depend on the conditioning one. By making
this dependency explicit, we (1) sample an informative latent variable
given the condition, and thus generate a sample of higher quality, and
(2) prevent the network from ignoring the latent variable in the
presence of a strong condition, thus enabling it to generate diverse
outputs.

Note that conditioning the VAE encoder via standard strategies, e.g.,
concatenation, is perfectly fine, since the two inputs to the encoder,
i.e., the data and the condition, are deterministic and useful to
compress the sample into the latent space. However, conditioning the VAE
decoder requires special care, which is what we focus on below.

#### 6.3.1 Stochastically Conditioning the Decoder

We propose to make the sampling of the latent variable from the
prior/posterior distribution explicitly depend on the condition instead
of treating these two variables as independent. To this end, we first
learn the distribution of the condition via a simple VAE, which we refer
to as CS-VAE because this VAE acts on the conditioning signal. The goal
of CS-VAE is to reconstruct the condition, e.g., the observed past
motion, given its latent representation. We take the prior of CS-VAE as
a standard Normal distribution @xmath . Following Kingma and Welling [
kingma2013auto ] , this allows us to approximate the CS-VAE posterior
with another sample from a Normal distribution @xmath via the
reparametrization trick

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

where @xmath and @xmath are the parameter vectors of the posterior
distribution generated by the VAE encoder, and thus @xmath .

Following the same strategy for the data VAE translates to treating the
conditioning and the data latent variables independently, which we seek
to avoid. Therefore, as illustrated in Fig. 6.2 (Bottom), we instead
define the LCP-VAE posterior not as directly normally distributed but
conditioned on the posterior of CS-VAE . To this end, we extend the
standard reparameterization trick as

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (6.4)
  -- -------- -------- -- -------

where @xmath comes from Eq. 6.3 , and @xmath and @xmath are the
parameter vectors generated by the LCP-VAE encoder. In fact, @xmath in
Eq. 6.3 is a sample from the scaled and translated version of @xmath
given @xmath and @xmath , and @xmath in Eq. 6.4 is a sample from the
scaled and translated version of @xmath given @xmath and @xmath . Since
we have access to the observations during both training and testing, we
always sample @xmath from the condition posterior. As @xmath is sampled
given @xmath , one expects the latent variable @xmath to carry
information about the strong condition, and thus a sample generated from
@xmath to correspond to a plausible sample given the condition. This
extended reparameterization trick lets us sample one single informative
latent variable that contains information about both the data and the
conditioning signal. This further allows us to avoid conditioning the
LCP-VAE decoder by concatenating the latent variable with a
deterministic representation of the condition. As will be shown in our
experiments, and in more detail in Appendix A, our sampling strategy not
only yields higher-quality motions that retain the semantic information
contained in the conditioning signal, but also helps the model to avoid
posterior collapse. However, it changes the variational family of the
LCP-VAE posterior. In fact, the posterior is no longer @xmath , but a
Gaussian distribution with mean @xmath and covariance matrix @xmath .
This will be accounted for when designing the KL divergence loss
discussed below.

#### 6.3.2 Learning

To learn the parameters of our model, we rely on the availability of a
dataset @xmath containing @xmath training samples @xmath . Each training
sample is a pair of condition and data sample. For CS-VAE , which learns
the distribution of the condition, we define the loss as the KL
divergence between its posterior and the standard Gaussian prior, that
is,

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

where @xmath is the dimension of the latent variable @xmath . By
contrast, for LCP-VAE , we define the loss as the KL divergence between
the posterior of LCP-VAE and the posterior of CS-VAE , i.e., of the
condition. To this end, we freeze the weights of CS-VAE before computing
the KL divergence, since we do not want to move the posterior of the
condition but that of the data. The KL divergence is then computed as
the divergence between two multivariate Normal distributions, encoded by
their mean vectors and covariance matrices as

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

Let @xmath , @xmath , @xmath be the dimensionality of the latent space
and @xmath the trace of a square matrix. The loss in Eq. 6.6 can be
written as ¹ ¹ 1 See Appendix A for more detail on the KL divergence
between two multivariate Gaussians and the derivation of Eq. 6.7 .

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

After computing the loss in Eq. 6.7 , we unfreeze CS-VAE and update it
with its previous gradient. Trying to match the posterior of LCP-VAE to
that of CS-VAE allows us to effectively use our extended
reparameterization trick in Eq. 6.4 . Furthermore, we use the standard
reconstruction loss for both CS-VAE and LCP-VAE , thus minimizing the
mean squared error (MSE) in the case of human motion prediction and the
negative log-likelihood (NLL) in the case of image captioning.

We refer to the reconstruction losses as @xmath and @xmath for CS-VAE
and LCP-VAE , respectively. Thus, our complete loss is

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

In practice, since the nature of our data is sequential (e.g., sequence
of human poses in human motion prediction and sequence of words in image
captioning), the LCP-VAE encoder is a recurrent model. Thus, we weigh
the KL divergence terms by a function @xmath corresponding to the KL
annealing weight of [ bowman2015generating ] . We start from @xmath ,
forcing the model to encode as much information in @xmath as possible,
and gradually increase it to @xmath during training, following a
logistic curve. We then continue training with @xmath .

In short, our method can be interpreted as a simple yet effective
framework (designed for CVAEs) for altering the variational family of
the posterior such that (1) a latent variable from this posterior
distribution is explicitly sampled given the condition, both during
training and inference (as illustrated in Fig. 6.3 ), and (2) the model
is much less likely to suffer from posterior collapse because the
mismatch between the posterior and prior distributions makes it harder
for learning to drive the KL divergence of Eq. 6.7 towards zero.

### 6.4 Experiments

In this chapter, we mainly focus on stochastic human motion prediction,
where the goal is to generate diverse and plausible continuations of
given past observations. Additionally, to show that our LCP-VAE
generalizes to other domains, we tackle the problem of stochastic image
captioning, where, given an image representation, the task is to
generate diverse yet related captions.

#### 6.4.1 Diverse Human Motion Prediction

###### Datasets.

To evaluate the effectiveness of our approach on the task of stochastic
human motion prediction, we use the Human3.6M [ h36m_pami ] and CMU
MoCap ² ² 2 Available at http://mocap.cs.cmu.edu/ . datasets, two large
publicly-available motion capture (mocap) datasets. Human3.6M comprises
more than 800 long indoor motion sequences performed by 11 subjects,
leading to 3.6M frames. Each frame contains a person annotated with 3D
joint positions and rotation matrices for all 32 joints. In our
experiments, for our approach and the replicated VAE-based baselines, we
represent each joint in 4D quaternion space. We follow the standard
preprocessing and evaluation settings used in [ martinez2017human ;
gui2018adversarial ; pavllo2018quaternet ; jain2016structural ] . The
CMU MoCap dataset is another large-scale motion capture dataset covering
diverse human activities, such as jumping, running, walking, and playing
basketball. Each frame contains a person annotated with 3D joint
rotation matrices for all 38 joints. As for Human3.6M, and following
standard practice [ wei2019motion ; li2018convolutional ] , we represent
each joint in 4D quaternion space. We also evaluate our approach on the
real-world Penn Action dataset [ zhang2013actemes ] , which contains
2326 sequences of 15 different actions, where for each person, 13 joints
are annotated in 2D space. The results on Penn Action are provided in
Appendix A.

###### Evaluation Metrics.

To quantitatively evaluate our approach and other stochastic motion
prediction baselines [ yan2018mt ; barsoum2018hp ; walker2017pose ;
aliakbarian2019MixAndMatch ] , we report the reconstruction error,
commonly referred to as ELBO, along with the KL-divergence on the
held-out test set. Additionally, we report quality [
aliakbarian2019MixAndMatch ] and diversity [ yang2018diversitysensitive
; aliakbarian2019MixAndMatch ; yuan2019diverse ] metrics, which should
be considered together. Specifically, to measure the diversity of the
motions generated by a stochastic model, we make use of the average
distance between all pairs of the @xmath motions generated from the same
observation. To measure quality, we train a binary classifier [
aliakbarian2019MixAndMatch ] to discriminate real (ground-truth) samples
from fake (generated) ones. The accuracy of this classifier on the test
set is inversely proportional to the quality of the generated motions.
Furthermore, we report a context metric measured as the performance of a
strong action classifier [ li2018co ] trained on ground-truth motions.
Specifically, the classifier is tested on each of the @xmath motions
generated from each observation. For @xmath observations and @xmath
continuations per observation, the accuracy is measured by computing the
argmax over each prediction’s probability vector, and we report context
as the mean class accuracy on the @xmath motions. Finally, we report the
training KL at convergence to show that no posterior collapse occurred.
For all metrics, we use @xmath motions per test observation. For all
experiments, we use 16 frames (i.e., 640ms) as observation to generate
the next 60 frames (i.e., 2.4sec).

###### Evaluating Stochasticity.

In Table 6.1 , we compare our approach (whose detailed architecture is
described in Appendix A) with the state-of-the-art stochastic motion
prediction models [ yan2018mt ; aliakbarian2019MixAndMatch ;
walker2017pose ; barsoum2018hp ] . Note that one should consider the
reported metrics jointly to truly evaluate a stochastic model. For
instance, while MT-VAE [ yan2018mt ] and HP-GAN [ barsoum2018hp ]
generate high-quality motions, they are not diverse. Conversely, while
Pose-Knows [ walker2017pose ] generates diverse motions, they are of low
quality. By contrast, our approach generates both high quality and
diverse motions. This is also the case of Mix-and-Match [
aliakbarian2019MixAndMatch ] , which, however, preserves much less
context. In fact, none of the baselines effectively conveys the context
of the observation to the generated motions. As shown in Table 6.2 , the
upper bound for context on Human3.6M is 0.60 (i.e., the classifier [
li2018co ] performance given the ground-truth motions). Our approach
yields a context of 0.54 when given only about 20% of the data. We
observe a similar behavior on the CMU MoCap dataset, shown in Table 6.2
. Altogether, as also supported by the qualitative results of Fig. 6.4
and in Appendix A, our approach yields diverse, high-quality and
context-preserving predictions. This is further evidenced by the t-SNE [
maaten2008visualizing ] plots of Fig. 6.5 , where different samples of
various actions are better separated for our approach than for, e.g.,
MT-VAE [ yan2018mt ] . For further discussion of the baselines, a deeper
insight of their behavior under different evaluation metrics, and a
discussion of the relation of each method’s performance to posterior
collapse, we refer the reader to Appendix A.

###### Evaluating Sampling Quality.

To further evaluate the sampling quality, we evaluate stochastic
baselines using the standard mean angle error (MAE) metric in Euler
space. To this end, we use the best of the @xmath generated motions for
each observation (referred to as S-MSE in [ yan2018mt ] ). A model that
generates more diverse motions has higher chances of producing a motion
close to the ground-truth one. As shown in Table 6.3 , this is the case
with our approach and Mix-and-Match [ aliakbarian2019MixAndMatch ] ,
which both yield higher diversity. However, our approach performs better
thanks to its context-preserving latent representation and its higher
quality of the generated motions.

In Table 6.4 , we compare our approach with the state-of-the-art
deterministic motion prediction models [ martinez2017human ;
jain2016structural ; gui2018few ; fragkiadaki2015recurrent ;
gui2018adversarial ] using the MAE metric in Euler space. To have a fair
comparison, we generate one motion per observation by setting the latent
variable to the distribution mode, i.e., @xmath . This allows us to
generate a plausible motion without having access to the ground truth.
To compare against the deterministic baselines, we follow the standard
setting, and thus use 50 frames (i.e., 2sec) as observation to generate
the next 25 frames (i.e., 1sec). Surprisingly, despite having a very
simple motion decoder architecture (one-layer GRU network) with a very
simple reconstruction loss function (MSE), this motion-from-mode
strategy yields results that are competitive with those of the baselines
that use sophisticated architectures and advanced loss functions. We
argue that learning a good, context-preserving latent representation of
human motion is the contributing factor to the success of our approach.
This, however, could be used in conjunction with sophisticated motion
decoders and reconstruction losses, which we leave for future research.

In Appendix A, we study alternative designs to condition the VAE encoder
and decoder.

#### 6.4.2 Diverse Image Captioning

For the task of conditional text generation, we focus on stochastic
image captioning. To demonstrate the effectiveness of our approach, we
report results on the MSCOCO [ lin2014microsoft ] captioning task, with
the original train/test splits of 83K and 41K images, respectively. The
MSCOCO dataset has five captions per image. However, we make it
deterministic by removing four captions per image, yielding a
Deterministic-MSCOCO captioning dataset. Note that the goal of this
experiment is not to advance the state of the art in image captioning,
but rather to explore the effectiveness of our approach on a different
task where we have strong conditioning signal and an expressive decoder
in the presence of a deterministic dataset. A brief review of the recent
work on diverse text generation is given in Appendix A.

We compare LCP-VAE (with the architecture described in Appendix A) with
a standard CVAE and with its autoregressive, non-variational counterpart
³ ³ 3 Note that LCP-VAE is agnostic to the choice of data
encoder/decoder architecture. We leave the use of more sophisticated
architectures for future research. . For quantitative evaluation, we
report the ELBO (the negative log-likelihood), along with the
KL-divergence and the Perplexity of the reconstructed captions on the
held-out test set. We also quantitatively measure the diversity, the
quality, and the context of sampled captions. To measure the context, we
rely on the BLEU1 score, making sure that the sampled captions represent
elements that appear in the image. For CVAE and LCP-VAE , we compute the
average BLEU1 score for @xmath captions sampled per image and report the
mean over the images. To measure diversity, we compte the BLEU4 score
between every pair of @xmath sampled captions per image. The smaller the
BLEU4 is, the more diverse the captions are. The diversity metric is
then 1-BLEU4, i.e., the higher the better. To measure quality, we use a
metric similar to that in our human motion prediction experiments,
obtained by training a binary classifier to discriminate real
(ground-truth) captions from fake (generated) ones. The accuracy of this
classifier on the test set is inversely proportional to the quality of
the generated captions. We provide qualitative examples for all the
methods in Fig. 6.6 and in Appendix A. As shown in Table 6.5 , a CVAE
learns to ignore the latent variable as it can minimize the caption
reconstruction loss given solely the image representation. By doing so,
all the generated captions at test time are identical, despite sampling
multiple latent variables. This can be further seen in the ELBO and
Perplexity of the reconstructed captions. We expect a model that takes
as input the captions and the image to have a much lower reconstruction
loss compared to the autoregressive baseline, which takes only the image
as input. However, this is not the case with CVAE, indicating that the
connection between the encoder and the decoder, i.e., the latent
variable, does not carry essential information about the input caption.
However, the quality of the generated sample is reasonably good. This is
also evidenced by the qualitative evaluations in Appendix A. By
contrast, LCP-VAE effectively handles this situation by unifying the
sampling of the latent variable and the conditioning, leading to diverse
but high quality captions, as reflected by the ELBO of our approach in
Table 6.5 and the qualitative results in Appendix A. Additional
quantitative evaluations and ablation studies for image captioning are
provided in Appendix A.

### 6.5 Conclusion

In this chapter, we have studied the problem of conditionally generating
diverse sequences with a focus on scenarios where the conditioning
signal is strong enough such that an expressive decoder can generate
plausible samples from it only. We have addressed this problem by
forcing the sampling of the latent variable to depend on the
conditioning one, which contrasts with standard CVAEs. By making this
dependency explicit, the model receives a latent variable that carries
information about the condition during both training and test time. This
further prevents the network from ignoring the latent variable in the
presence of a strong condition, thus enabling it to generate diverse
outputs. To demonstrate the effectiveness of our approach, we have
investigated two application domains: Stochastic human motion prediction
and diverse image captioning. In both cases, our LCP-VAE model was able
to generate diverse and plausible samples, as well as to retain
contextual information, leading to semantically-meaningful predictions.
We believe our approach will have great impact on practical applications
such as pedestrian intention forecasting aliakbarian2018viena2 ,
trajectory forecasting and human tracking saleh2020artist ;
kosaraju2019social . In the future, we will also apply our approach to
other problems that rely on strong conditions, such as image inpainting
and super-resolution, for which only deterministic datasets are
available.

## Chapter 7 Conclusion

In this thesis we have tackled the problem video anticipation by
learning sequential representations. We have started from anticipating a
discrete representation of a deterministic future, e.g., as in action
anticipation and then move toward a more complex task of anticipating
multiple plausible continuous representations of a stochastic task,
e.g., human motion prediction.

The major contributions of this thesis are outlined below:

1.  We propose a novel action anticipation framework (that can be also
    seen as an early recognition model). In particular, we introduce a
    novel loss that encourages making correct predictions very early.
    Our loss models the intuition that some actions, such as running and
    high jump, are highly ambiguous after seeing only the first few
    frames, and false positives should therefore not be penalized too
    strongly in the early stages. By contrast, we would like to predict
    a high probability for the correct class as early as possible, and
    thus penalize false negatives from the beginning of the sequence.
    Our experiments demonstrate that, for a given model, our new loss
    yields significantly higher accuracy than existing ones on the task
    of early prediction. We also propose a novel multi-stage Long Short
    Term Memory (MS-LSTM) architecture for action anticipation. This
    model effectively extracts and jointly exploits context- and
    action-aware features. This is in contrast to existing methods that
    typically extract either global representations for the entire image
    or video sequence, thus not focusing on the action itself, or
    localize the feature extraction process to the action itself via
    dense trajectories, optical flow or actionness, thus failing to
    exploit contextual information. This work has been published in ICCV
    2017, Venice, Italy.

2.  We improve and extend our previous contribution by focusing on
    driving scenarios, encompassing common the subproblems of
    anticipating ego car’s driver maneuvers, front car’s driver
    maneuver, accidents, violating or respecting traffic rules, and
    pedestrian intention, with a fixed, sensible set of sensors. To this
    end, we introduce the VIrtual ENvironment for Action Analysis (VIENA
    @xmath ) dataset. Altogether, these subproblems encompass a total of
    25 distinct action classes. VIENA @xmath is acquired using the GTA V
    video game. It contains more than 15K full HD, 5s long videos,
    corresponding to more than 600 samples per action class, acquired in
    various driving conditions, weathers, daytimes, and environments.
    This amounts to more than 2.25M frames, each annotated with an
    action label. These videos are complemented by basic vehicle
    dynamics measurements, reflecting well the type of information that
    one could have access to in practice ¹ ¹ 1 Our dataset is publicly
    available at https://sites.google.com/view/viena2-project/ . We then
    benchmark state-of-the-art action anticipation algorithms on VIENA2,
    and as another contribution, introduce a new multi-modal, LSTM-based
    architecture that generalizes out previous contribution to an
    arbitrary number of modalities, together with a new anticipation
    loss, which outperforms existing approaches in our driving
    anticipation scenarios. This work has been published in ACCV 2018,
    Perth, Australia.

3.  For continuous, stochastic anticipation task, we address the problem
    of stochastic human motion prediction. As introduced earlier in this
    thesis, human motion prediction aims to forecast the sequence of
    future poses of a person given past observations of such poses. To
    achieve this, existing methods typically rely on recurrent neural
    networks (RNNs) that encode the person’s motion. While they predict
    reasonable motions, RNNs are deterministic models and thus cannot
    account for the highly stochastic nature of human motion; given the
    beginning of a sequence, multiple, diverse futures are plausible. To
    correctly model this, it is therefore critical to develop algorithms
    that can learn the multiple modes of human motion, even when
    presented with only deterministic training samples. We introduce an
    approach to effectively learn the stochasticity in human motion. At
    the heart of our approach lies the idea of Mix-and-Match
    perturbations: Instead of combining a noise vector with the
    conditioning variables in a deterministic manner (as usually done in
    standard practices), we randomly select and perturb a subset of
    these variables. By randomly changing this subset at every
    iteration, our strategy prevents training from identifying the root
    of variations and forces the model to take it into account in the
    generation process. This is a highly effective conditioning scheme
    in scenarios when (1) we are dealing with a deterministic dataset,
    i.e., one sample per condition, (2) the conditioning signal is very
    strong and representative, e.g., the sequence of past observations,
    and (3) the model has an expressive decoder that can generate a
    plausible sample given only the condition. We utilize Mix-and-Match
    by incorporating it into a recurrent encoder-decoder network with a
    conditional variational autoencoder (CVAE) block that learns to
    exploit the perturbations. Mix-and-Match then acts as the stochastic
    conditioning scheme instead of concatenation that usually appears in
    standard CVAEs. This work has been published in CVPR 2020, Seattle,
    Washington, USA.

4.  In our previous contribution, we identified one limitation of a
    standard CVAEs when dealing deterministic datasets and strong
    conditioning signals. In this work, we further investigates this
    problem from a more theoretical point of view. Specifically, in this
    contribution we tackle the task of diverse sequence generation in
    which all the diversely generated sequences carry the same semantic
    as in the conditioning signal. We observe that in standard CVAE,
    conditioning and sampling the latent variable are two independent
    processes, leading to generating samples that are not necessarily
    carry all the contextual information about the condition. To address
    this, we propose to explicitly make the latent variables depend on
    the observations (the conditioning signal). To achieve this, we
    develop a CVAE architecture that learns a distribution not only of
    the latent variables, but also of the observations, the latter
    acting as prior on the former. By doing so, we change the
    variational family of the posterior distribution of the CVAE, thus,
    as a side effect, our approach can mitigate posterior collapse to
    some extent. This work will be submitted to CVPR 2021.

### 7.1 Future Work

In this thesis we focused on the task of video anticipation. Although
there has been great progress in recent years to address the task of
action anticipation, diverse human motion prediction remained a
relatively less studied problem. Below, we list some potential future
direction based on our research, mostly focusing on diverse sequence
generation:

-   Incorporating sequential latent variables: In our proposed
    frameworks, we use a single latent variable to represent a sequence,
    e.g., human motion or an image caption. However, a more expressive
    alternative could be a sequential latent variable, considering a
    latent variable for the representations at each time-step.
    Specifically, instead of having @xmath sampled from the approximate
    posterior @xmath , we consider a sequence of latent variables @xmath
    where @xmath is sampled from @xmath and given @xmath . We believe
    that this not only provides a more expressive latent space, it
    allows us to have more control on the variations at different
    time-steps.

-   Incorporating Auxiliary information: Our proposed methods for
    diverse human motion prediction are completely unsupervised; no
    extra annotation/feedback are required to train our models. However,
    one can certainly benefit from additional information to guide the
    latent space to learn better (more semantically meaningful)
    representations. For instance, in the case of human motion
    prediction, the motion capture datasets are often come with
    additional action label; the action that the subject is performing
    while his/her motion is being captured. One can utilize this
    information with a shallow action classifier on top of the latent
    space, forcing the latent space to learn not only the information
    that is useful for motion reconstruction, but also the ones that are
    discriminative enough to distinguish multiple actions.

-   Going beyond poses: Our motion prediction approaches works on
    abstract pose representation of the human at each time-step. That is
    a representation of e.g., 32 body joints represented in either 3D
    position or 4D Quaternion representation. One potential future
    direction is to evaluate how our methods perform when dealing with
    very high dimensional data, e.g., video frames or fine-grained human
    shapes.

## Appendix A Appendix A

### a.1 Technical Background on Evidence Lower Bound

Our goal is to solve a maximum likelihood problem. To this end, as
discussed in the main paper, we rely on Variational Inference , which
approximates the true posterior @xmath with another distribution @xmath
. This distribution is computed via another neural network parameterized
by @xmath (called variational parameters), such that @xmath . Using such
an approximation, Variational Autoencoders [ kingma2013auto ] , or VAEs
in short, are able to optimize the marginal likelihood in a tractable
way. The optimization objective of the VAEs is a variational lower
bound, also known as evidence lower bound, or ELBO in short.

Specifically, to find an approximation of the posterior that represents
the true one, variational inference minimizes the Kullback-Leibler (KL)
divergence between the approximate and true posteriors. This divergence
can be written as

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

This can further be seen as an expectation, yielding

  -- -------- -- -------
     @xmath      
     @xmath      (A.2)
  -- -------- -- -------

According to Bayes’ theorem, the second term above, i.e., the true
posterior, can be written as @xmath . The data distribution @xmath is
independent of the latent variable @xmath , and can thus be pulled out
of the expectation term, giving

  -- -------- -- -------
     @xmath      (A.3)
  -- -------- -- -------

By moving the @xmath term to the right-hand side of the above equation,
we can write

  -- -------- -- -------
     @xmath      
     @xmath      
                 (A.4)
  -- -------- -- -------

The second expectation term in the resulting equation is, by definition,
the KL divergence between the approximate posterior @xmath and the prior
@xmath . This lets us write

  -- -------- -- -------
     @xmath      (A.5)
  -- -------- -- -------

In this equation, @xmath is the log-likelihood of the data, which we
would like to maximize; @xmath is the KL divergence between the
approximate and the true posterior distributions, which, while not
computable, is by definition non-negative; @xmath is the reconstruction
loss; and @xmath is the KL divergence between the approximate posterior
distribution and a prior over the latent variable. This last term can be
seen as a regularizer of the latent representation. Altogether, the
intractability and non-negativity of @xmath only allows us to optimize a
lower bound of the log-likelihood of the data, given by

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

This is referred to as variational or evidence lower bound (ELBO).

### a.2 Derivation of LCP-VAE’s KL Divergence Loss

In our approach, the model encourages the posterior of LCP-VAE to be
close to the one of CS-VAE . In general, the KL divergence between two
distributions @xmath and @xmath is defined as

  -- -------- -- -------
     @xmath      (A.7)
  -- -------- -- -------

Let us now consider the case where the distributions are multivariate
Gaussians @xmath in @xmath , where @xmath , with @xmath and @xmath are
@xmath -dimensional vectors predicted by the encoder network of the VAE.
The density function of such a distribution is

  -- -------- -- -------
     @xmath      (A.8)
  -- -------- -- -------

Thus, the KL divergence between two multivariate Gaussians is computed
as

  -- -------- -- -------
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      
     @xmath      (A.9)
  -- -------- -- -------

where @xmath denotes the trace operator. In Eq. A.9 , the covariance
matrix @xmath and mean @xmath correspond to distribution @xmath and the
covariance matrix @xmath and mean @xmath correspond to distribution
@xmath . Given this result, we can then compute the KL divergence of the
LCP-VAE and the posterior distribution with mean @xmath and covariance
matrix @xmath . Let @xmath , @xmath , and @xmath be the dimensionality
of the latent space. The loss in Eq. 6 of the main paper can then be
written as

  -- -------- -- --------
     @xmath      
     @xmath      (A.10)
  -- -------- -- --------

Since @xmath , @xmath will be cancelled out in the @xmath term, which
yields

  -- -------- -- --------
     @xmath      (A.11)
  -- -------- -- --------

### a.3 Mitigating Posterior Collapse: Related Work

Deep generative models offer promising results in generating diverse,
realistic samples, such as images, text, motion, and sound, from purely
unlabeled data. One example of such successful generative models are
variational autoencoders [ kingma2013auto ] (VAEs), the stochastic
variant of autoencoders, which, thanks to strong and expressive
decoders, can generate high-quality samples. Training such models,
however, may often result in posterior collapse: the posterior
distribution @xmath of the latent variable @xmath given the input @xmath
becomes equal to the prior distribution, resulting in a latent variable
carrying no information about the input. In other words, the model
learns to ignore the latent variable.

The most common approaches to tackling posterior collapse consist of
weighing the KL divergence between the posterior and prior during
training by an annealing function [ bowman2015generating ;
yang2017improved ; kim2018semi ; gulrajani2016pixelvae ; liu2019cyclical
] , weakening the decoder [ semeniuta2017hybrid ; zhao2017infovae ] , or
changing the training objective [ zhao2017infovae ;
tolstikhin2017wasserstein ] . All of them are based on the perspective
that the solution to posterior collapse can be found in a good local
optimum in terms of evidence lower bound [ chen2016variational ;
alemi2017fixing ] . However, they each suffer from drawbacks: Any
annealing weight that does not become and remain equal to one at some
point during training yields an improper statistical model; weakening
the decoder tends to degrade the quality of the generated samples;
changing the objective does not optimize the true variational lower
bound. As an alternative, some methods modify the training strategy to
more strongly encourage the inference network to approximate the model’s
true posterior [ he2019lagging ; li2019surprisingly ] . Other methods
add auxiliary tasks either with non-autoregressive models [
lucas2018auxiliary ] or that exploit the latent variable [ goyal2017z ;
lucas2018auxiliary ; dieng2018avoiding ] . While this encourages the
latent variable to carry some information, it may not be directly useful
for the main task. Alternatively, several techniques incorporate
constraints in VAEs. In this context, VQ-VAE [ van2017neural ]
introduces a discrete latent variable obtained by vector quantization of
the latent one that, given a uniform prior over the outcome, yields a
fixed KL divergence equal to log @xmath , with @xmath the size of the
codebook; several recent works use the von Mises-Fisher distribution to
obtain a fixed KL divergence, thus mitigating the posterior collapse
problem [ guu2018generating ; xu2018spherical ;
davidson2018hyperspherical ] ; more recently, delta-VAE [
razavi2019preventing ] modifies the posterior such that it maintains a
minimum distance between the prior and the posterior.

Although successful at handling the posterior collapse in the presence
of expressive decoders, e.g., LSTM, GRUs, PixelCNN, all of these
approaches were designed for standard VAEs, not conditional VAEs
(CVAEs). As such, they do not address the problem of mitigating the
influence of a strong conditioning signal in ignoring the latent
variable, which is our focus here. In this context, we observed that a
strong condition provides enough information for an expressive decoder
to reconstruct the data, thus allowing the decoder to ignore the latent
variable at no loss in reconstruction quality.

### a.4 Further Discussion on the Performance of Stochastic Baselines

The MT-VAE model [ yan2018mt ] tends to ignore the random variable
@xmath , thus ignoring the root of variation. As a consequence, it
achieves a low diversity, much lower than ours, but produces samples of
high quality, albeit almost identical (see the diversity of the
generated motions in quantitative evaluations). We empirically observed
this by analysing the weights acting on the latent variable @xmath and
the ones acting on the conditioning signal (i.e., the hidden state of
the past motion). We observed the magnitude of the weights acting on
@xmath to be orders of magnitude smaller than that of acting on the
condition, 0.008 versus 232.85, respectively. To further confirm that
the MT-VAE ignores the latent variable, we performed an additional
experiment where, at test time, we sampled each element of the random
vector independently from @xmath instead of from the prior @xmath . This
led to neither loss of quality nor increase of diversity of the
generated motions.

Our experiments with the HP-GAN model [ barsoum2018hp ] evidence the
limited diversity of the sampled motions despite its use of random noise
during inference. Note that the authors of [ barsoum2018hp ] mentioned
in their paper that the random noise was added to the hidden state. Only
by studying their publicly available code ¹ ¹ 1
https://github.com/ebarsoum/hpgan did we understand the precise way this
combination was done. In fact, the addition relies on a parametric,
linear transformation of the noise vector. That is, the perturbed hidden
state is obtained as

  -- -------- -- --------
     @xmath      (A.12)
  -- -------- -- --------

Because the parameters @xmath are learned , the model has the
flexibility to ignore @xmath , which leads to the low diversity of
sampled motions. Note that the authors of [ barsoum2018hp ] acknowledged
that, despite their best efforts, they noticed very little variation
between predictions obtained with different @xmath values. We further
analysed this phenomenon and observed that the magnitude of @xmath is in
the order of @xmath , confirming the fact that it provides the model
with the flexibility to ignore @xmath . Since the perturbation is
ignored, however, the quality of the generated motions is high.

Pose-Knows [ walker2017pose ] , produces motions with higher diversity
than the aforementioned two baselines, but of much lower quality. The
main reason behind this is that the random vectors that are concatenated
to the poses at each time-step are sampled independently of each other,
which translates to discontinuities in the generated motions. This
problem might be mitigated by sampling the noise in a time-dependent,
autoregressive manner, as in [ videoFlow ] for video generation. Doing
so, however, goes beyond the scope of our analysis.

The Mix-and-Match approach [ aliakbarian2019MixAndMatch ] yields sampled
motions with higher diversity and reasonable quality. The architecture
of Mix-and-Match is very close to that of MT-VAE, but replaces the
deterministic concatenation operation with a stochastic perturbation of
the hidden state with the noise. Through such a perturbation, the
decoder is not able decouple the noise and the condition, by contrast
with concatenation. However, since the perturbation is not learned and
is a non-parametric operation, the quality of the generated motion is
lower than ours and of other baselines (except for Pose-Knows). We see
the Mix-and-Match perturbation as a workaround to the posterior collapse
problem, which nonetheless sacrifices the quality and the context of the
sampled motions.

### a.5 Ablation Study on Different Means of Conditioning

In addition to the experiments in the main paper, we also study various
designs to condition the VAE encoder and decoder. As discussed before,
conditioning the VAE encoder can be safely done via concatenating two
deterministic sources of information, i.e., the representations of the
past and the future, since both sources are useful to compress the
future motion into the latent space. In Table A.1 , we use both a
deterministic representation of the observation, @xmath , and a
stochastic one, @xmath , as a conditioning variable for the encoder.
Similarly, we compare the use of either of these variables via
concatenation with that of our modified reparameterization trick
(explained in the main paper). This shows that, to condition the
decoder, reparameterization is highly effective at addressing posterior
collapse. Furthermore, for the encoder, a deterministic condition works
better than a stochastic one. When both the encoder and decoder are
conditioned via deterministic conditioning variables, i.e., row 2 in
Table A.1 , the model learns to ignore the latent variable and rely
solely on the condition, as evidenced by the KL term tending to zero.

### a.6 Experimental Results on the Penn Action Dataset

As a complementary experiment, we evaluate our approach on the Penn
Action dataset, which contains 2326 sequences of 15 different actions,
where for each person, 13 joints are annotated in 2D space. Most
sequences have less than 50 frames and the task is to generate the next
35 frames given the first 15. Results are provided in Table A.2 . Note
that the upper bound for the Context metric is 0.74, i.e., the
classification performance given the Penn Action ground-truth motions.

### a.7 Stochastic Human Motion Prediction Architecture

Our motion prediction model follows the architecture depicted in Fig. 2
of the main paper. Below, we describe the architecture of each component
in our model. Note that human poses, consisting of 32 joints in case of
the Human3.6M dataset, are represented in 4D quaternion space. Thus,
each pose at each time-step is represented with a vector of size @xmath
. All the tensor sizes described below ignore the mini-batch dimension
for simplicity.

The observed motion encoder , or CS-VAE motion encoder, is a single
layer GRU [ chung2014empirical ] network with 1024 hidden units. If the
observation sequence has length @xmath , the observed motion encoder
maps @xmath into a single hidden representation of size @xmath , i.e.,
the hidden state of the last time-step. This hidden state, @xmath , acts
as the condition to the LCP-VAE encoder and the direct input to the
CS-VAE encoder.

CS-VAE , similarly to any variational autoencoder, has an encoder and a
decoder. The CS-VAE encoder is a fully-connected network with ReLU
non-linearities, mapping the hidden state of the motion encoder, i.e.,
@xmath , to an embedding of size @xmath . Then, to generate the mean and
standard deviation vectors, we use two fully connected branches. They
map the embedding of size @xmath to a mean vector of size @xmath and a
standard deviation vector of size @xmath , where 128 is the length of
the latent variable. Note that we apply a ReLU non-linearity to the
vector of standard deviations to ensure that it is non-negative. We then
use the reparameterization trick [ kingma2013auto ] to sample a latent
variable of size @xmath . The CS-VAE decoder consists of multiple
fully-connected layers, mapping the latent variable to a variable of
size @xmath , acting as the initial hidden state of the observed motion
decoder. Note that we apply a Tanh non-linearity to the generated hidden
state to mimic the properties of a GRU hidden state.

The observed motion decoder , or CS-VAE motion decoder, is similar to
its motion encoder, except for the fact that it reconstructs the motion
autoregressively. Additionally, it is initialized with the reconstructed
hidden state, i.e., the output of the CS-VAE decoder. The output of each
GRU cell at each time-step is then fed to a fully-connected layer,
mapping the GRU output to a vector of size @xmath , which represents a
human pose with 32 joints in 4D quaternion space. To decode the motions,
we use a teacher forcing technique [ williams1989learning ] during
training. At each time-step, the network chooses with probability @xmath
whether to use its own output at the previous time-step or the
ground-truth pose as input. We initialize @xmath , and decrease it
linearly at each training epoch such that, after a certain number of
epochs, the model becomes completely autoregressive, i.e., uses only its
own output as input to the next time-step. Note that, at test time, the
motions are generated completely autoregressively, i.e., with @xmath .

Note that the future motion encoder and decoder have exactly the same
architectures as the observed motion ones. The only difference is their
input, where the future motion is represented by poses from @xmath to
@xmath in a sequence. In the following, we describe the architecture of
LCP-VAE for motion prediction.

LCP-VAE is a conditional variational encoder. Its encoder’s input is a
representation of future motion, i.e., the last hidden state of the
future motion encoder, @xmath , conditioned on @xmath . The conditioning
is done by concatenation, thus the input to the encoder is a
representation of size @xmath . The LCP-VAE encoder, similarly to the
CS-VAE encoder, maps its input representation to an embedding of size
@xmath . Then, to generate the mean and standard deviation vectors, we
use two fully connected branches, mapping the embedding of size @xmath
to a mean vector of size @xmath and a standard deviation vector of size
@xmath , where 128 is the length of the latent variable. Note that we
apply a ReLU non-linearity to the vector of standard deviations to
ensure that it is non-negative. To sample the latent variable, we use
our extended reparameterization trick, explained in the main paper. This
unifies the conditioning and sampling of the latent variable. Then,
similarly to CS-VAE , the latent variable is fed to the LCP-VAE decoder,
which is a fully connected network that maps the latent representation
of size @xmath to a reconstructed hidden state of size @xmath for future
motion prediction. Note that we apply a Tanh non-linearity to the
generated hidden state to mimic the properties of a GRU hidden state.

### a.8 Diverse Image Captioning Architecture

Our diverse image captioning model follows the architecture depicted in
Fig.2 of the main paper. Below, we describe the architecture of each
component in our model. Note that all tensor sizes described below
ignore the mini-batch dimension for simplicity.

The image encoder is a ResNet152 [ he2016deep ] pretrained on ImageNet [
krizhevsky2012imagenet ] . Given the encoder, the conditioning signal is
a @xmath feature representation. Note that, to avoid an undesirable
equilibrium in the reconstruction loss of the CS-VAE , we freeze the
ResNet152 during training.

CS-VAE is a standard variational autoencoder. Its encoder maps the input
representation of size @xmath to an embedded representation of size
@xmath . Then, to generate the mean and standard deviation vectors, we
use two fully connected branches, mapping the embedding of size @xmath
to a mean vector of size @xmath and a standard deviation vector of size
@xmath , where 256 is the length of the latent variable. The decoder of
the CS-VAE maps the sampled latent variable of size @xmath to a
representation of size @xmath . The generated representation acts as a
reconstructed image representation. During training, we learn the
reconstruction by computing the smoothed @xmath loss between the
generated representation and the image feature of the frozen ResNet152.

The caption encoder is a single layer GRU network with a hidden size of
1024. Each word in the caption is represented using a randomly
initialized embedding layer that maps each word to a representation of
size @xmath .

LCP-VAE is a conditional variational autoencoder. As input to its
encoder, we first concatenate the image representation of size @xmath to
the caption representation of size @xmath . The encoder then maps this
representation to an embedded representation of size @xmath . Then, to
generate the mean and standard deviation vectors, we use two fully
connected branches, mapping the embedding of size @xmath to a mean
vector of size @xmath and a standard deviation vector of size @xmath ,
where 256 is the length of the latent variable. To sample the latent
variable, we make use of our extended reparameterization trick,
explained in the main paper. This unifies the conditioning and sampling
of the latent variable. The LCP-VAE decoder then maps this latent
representation to a vector of size @xmath via a fully-connected layer.
We then apply batch normalization [ ioffe2015batch ] on the
representation, which acts as the first token to the caption decoder.

The caption decoder is also a single layer GRU network with a hidden
size of 1024. Its first token is the representation generated by the
LCP-VAE decoder, while the remaining tokens are the words in the
corresponding caption. To decode the caption, we use a teacher forcing
technique during training. At each time-step, the network chooses with
probability @xmath whether to use its own output at the previous
time-step or the ground-truth token as input. We initialize @xmath , and
decrease it linearly at each training epoch such that, after a certain
number of epochs, the model becomes completely autoregressive, i.e.,
uses only its own output as input to the next time-step. Note that, at
test time, the captions are generated completely autoregressively, i.e.,
with @xmath .

### a.9 Diverse Text Generation: Related Work

A number of studies utilize generative models for language modeling. For
instance, [ fang2019implicit ] uses VAEs and LSTMs in an unconditional
language modeling problem, where posterior collapse may occur if the VAE
is not trained well. To handle the problem of posterior collapse in
language modeling, the authors of [ fang2019implicit ] try to directly
match the aggregated posterior to the prior. This can be considered an
extension of variational autoencoders with a regularization to maximize
the mutual information, addressing the posterior collapse. VAEs are also
used for language modeling in [ li2019surprisingly ] . In this context,
it was observed that VAEs make it hard to find a good balance between
language modeling and representation learning. To improve the training
of VAEs in such scenarios, the authors of [ li2019surprisingly ] first
pretrain the inference network in an autoencoder fashion, such that the
inference network learns a good representation of the data in a
deterministic manner. Then, they train the whole VAE while considering a
weight for the KL term during training. However, the second step
modifies the way VAEs optimize the variational lower bound. Furthermore,
this approach prevents the model from being trained end-to-end. Unlike
these approaches, our method considers the case of conditional text
generation, where the conditioning signal (the image to be captioned in
our case) is strong enough such that the caption generator can rely
solely on that.

The recent work of [ cho2019mixture ] proposes to separate
diversification from generation for the tasks of sequence generation and
language modeling. The diversification stage uses a mixture of experts
(MoE) to sample different binary masks on the source sequence for
diverse content selection. The generation stage uses a standard
encoder-decoder model taking as input the selected content from the
source sequence. While effective at generating diverse sequences, this
approach relies heavily on the selection part, where one needs to select
the most important information in the source to generate the target
sequence. Thus, the diversity of the generated target sequence depends
on the diversity of the selected parts of the source sequence.
Similarly, the authors of [ shen2019mixture ] utilize an MoE for the
task of diverse machine translation. While this task is considered to be
a diverse text generation one, with indeed diverse translations
generated from each source sentence, the methods addressing it rely on
the availablity of the a stochastic dataset, i.e., having access to
multiple target sequences for each source sentence during training. By
contrast, here, we design an approach that works with deterministic
datasets, as those available for human motion prediction.

### a.10 Ablation Study on Diverse Image Captioning

In addition to the experiments in the main paper, in Table A.3 , we
evaluate our approach, the autoregressive baseline and the CVAE, in
terms of BLEU scores, i.e., BLEU1, BLEU2, BLEU3, and BLEU4 of the
captions generated at test time. For the autoregressive baseline, the
model generates one caption per image, which makes it straightforward to
compute the BLEU scores. For the CVAE, we consider the best BLEU score
among all @xmath sampled captions, according to the best-matching
ground-truth caption. For our model, we consider the caption from the
mode, i.e., the one sampled from @xmath . Although the caption sampled
from LCP-VAE is not chosen based on the best match with the ground-truth
caption, it yields promising quality in terms of BLEU scores. For the
sake of completeness and fairness, we also provide the results obtained
with the best of @xmath captions for our approach, which outperform all
the baselines.

### a.11 Additional Motion Prediction Qualitative Results

In this section we provide additional qualitative results for our
LCP-VAE approach, illustrated in Fig. A.1 to Fig. A.5 .

### a.12 Additional Image Captioning Qualitative Results

In this section we provide additional qualitative results for our
LCP-VAE approach, illustrated in Fig. A.6 to Fig. A.10 .
