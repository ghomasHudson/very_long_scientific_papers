Chapter 1: Background and Motivation

I discuss the general setting of my thesis. I define the problem I am
going to study, its motivation and its relevance. Moreover, I describe
briefly the structure of the dissertation and, finally I state my
original contributions

## I Introduction

In the orthodox scientific community, it is commonly accepted that laws
of physics reflect laws of nature. In this Thesis we uphold a different
line of thinking: laws of physics consist of rules designed for
processing information about the world for the purpose of describing
and, to a certain extent, understanding natural phenomena. The form of
the laws of physics should reflect the form of rules for manipulating
information.

This unorthodox point of view has important consequences for the laws of
physics themselves: the identification of the relevant
information-constraints of a particular phenomenon implies that the
rules for processing information can take over, and thus, the laws of
physics that govern the specific phenomenon should be determined. In
principle, the field of applicability of the ideas explored in this
Thesis is not limited to physics. This novel line of thinking may be
useful to various scientific disciplines since all subject matters are
investigated by applying the same universal methods and tools of
reasoning.

One of our major line of research concerns the possibility that laws of
physics can be derived from rules of inference. In the case of a
positive answer, we expect that physics should use the same tools, the
same kind of language that has been found useful for inference. We are
well aware that most of our basic physics theories make use of the
concepts of probability, of entropy, and of geometry.

We know that the evidence supporting the fact that laws of physics can
be derived from rules of inference is already considerable. Indeed, most
of the formal structure of statistical mechanics can already be derived
from principles of inference jaynes . Moreover, the derivation of
quantum mechanics as information physics is already quite developed
catichaQM . Finally, it seems plausible that even Einstein’s theory of
gravity (GR, general relativity) might be derived from a more
fundamental statistical geometrodynamics in analogy to the way in which
thermodynamics can be derived from a microscopic statistical mechanics
catichaGR .

It is known that any attempt to build a unified theory of all forces is
problematic: these problems arise from the difficulties of incorporating
the classical theory of gravity with quantum theories of
electromagnetic, weak and strong forces. Few situations, where
gravitational and quantum phenomena coexist in a non-trivial way, can be
studied in some detail. It just happens that in these situations (for
instance, Hawking’s black hole evaporation waldino ) thermodynamics
plays an essential role. These considerations lead to believe that
concepts such as entropy should play a key-role in any successful
theoretical construct attempting to unify all fundamental interactions
in nature .

The relation between physics and nature is more complicated than has
been usually assumed. An explicit admission of such a statement is
represented by the recognition that statistical physics and quantum
physics are theories of inference. This scientific awareness is leading
to a gradual acceptance of the fact that the task of theoretical physics
is that of putting some order in our understanding of natural phenomena,
not to write the ultimate equations of the universe.

In this Thesis, we want to carry on this line of thought and explore the
possibility that laws of physics become rules for the consistent
manipulation of information: laws of physics work not because they are
laws of nature but rather because they are laws of inference and they
have been designed to work. Therefore my first line of investigation
will be the following:

  First line of research  

     I would like to push the possibility to extend the notion that laws
    of physics can be derived from rules of inference into new
    unexplored territories. I would like to construct new entropic
    dynamical models that reproduce recognizable laws of physics using
    merely the tools of inference (probability, information geometry and
    entropy).

However, I have to select the laws of physics that I want to reproduce.

It is known there is no unified characterization of chaos in classical
and quantum dynamics. In the Riemannian casetti and Finslerian cipriani
(a Finsler metric is obtained from a Riemannian metric by relaxing the
requirement that the metric be quadratic on each tangent space)
geometrodynamical approach to chaos in classical Hamiltonian systems, an
active field of research concerns the possibility of finding a rigorous
relation among the sectional curvature, the Lyapunov exponents, and the
Kolmogorov-Sinai dynamical entropy (i. e. the sum of positive Lyapunov
exponents) kawabe . The largest Lyapunov exponent characterizes the
degree of chaoticity of a dynamical system and, if positive, it measures
the mean instability rate of nearby trajectories averaged along a
sufficiently long reference trajectory. Moreover, it is known that
classical chaotic systems are distinguished by their exponential
sensitivity to initial conditions and that the absence of this property
in quantum systems has lead to a number of different criteria being
proposed for quantum chaos. Exponential decay of fidelity,
hypersensitivity to perturbation, and the Zurek-Paz quantum chaos
criterion of linear von Neumann’s entropy growth zurek are some examples
caves . These criteria accurately predict chaos in the classical limit,
but it is not clear that they behave the same far from the classical
realm.

I chose the second line of research below for the following three
reasons: i) lack of a unifying understanding of chaotic phenomena in
classical and quantum physics; ii) test the potential mathematical power
of these information-geometric tools at my disposal; test the potential
predicting power of entropic dynamical models.

  Second line of research  

     I would like to push the possibility to derive, explain and
    understand classical and quantum criteria of chaos from rules of
    inference. I would like to construct new entropic chaotic dynamical
    models that reproduce recognizable laws of mathematical-physics
    using merely the tools of inference (probability, information
    geometry and entropy).

## Ii Problems under investigation, their relevance and original
contributions

First I review the basic elements of the maximum relative entropy
formalism (ME method) and recall the basics of Riemannian geometry with
special focus to its application to probability theory (this is known as
Information Geometry, IG). IG and ME are the fundamental tools that
Prof. Ariel Caticha has used to build a form of information-constrained
dynamics on statistical manifolds to investigate the possibility that
Einstein’s general theory of gravity (or any classical or quantum theory
of physics) may emerge as a macroscopic manifestation of an underlying
microscopic statistical structure. This dynamics is known in the
literature as Entropic Dynamics (ED). Therefore, since ED is an
important element of this thesis, I review the key-points of such
dynamics, emphasizing the most relevant points that I use in my own
information geometrodynamical approach to chaos (IGAC). Of course,
before introducing my IGAC, I briefly review the basics of the
conventional Riemannian geometrodynamics approach to chaos and discussed
the notion of chaos in physics in general. After this long background
information that is needed because of the originality and novelty of
these topics, I begin with my original contributions.

First, two chaotic entropic dynamical models are considered. The
geometric structure of the statistical manifolds underlying these models
is studied. It is found that in both cases, the resulting metric
manifolds are negatively curved. Moreover, the geodesics on each
manifold are described by hyperbolic trajectories. A detailed analysis
based on the Jacobi-Levi-Civita equation for geodesic spread (JLC
equation) is used to show that the hyperbolicity of the manifolds leads
to chaotic exponential instability. A comparison between the two models
leads to a relation among scalar curvature of the manifold ( @xmath ),
Jacobi field intensity ( @xmath ) and information geometrodynamical
entropy (IGE, @xmath ). The IGE entropy is proposed as a brand new
measure of chaoticity.

  First Contribution  

    cafaro1 ; cafaro2 ; cafaro3 : I suggest that these three quantities,
    @xmath , @xmath , and @xmath are useful indicators of chaoticity for
    chaotic dynamical systems on curved statistical manifolds.
    Furthermore, I suggest a classical  information-geometric criterion
    of linear information geometrodynamical entropy growth in analogy
    with the Zurek-Paz quantum chaos criterion.

Second, in collaboration with Prof. Ariel Caticha, I show that the ED
formalism is not purely an abstract mathematical framework; it is indeed
a general theoretical scheme where conventional Newtonian dynamics is
obtained as a special limiting case.

  Second Contribution  

    caticha-cafaro : The reproduction of the Newtonian dynamics from
    first principles of probable inference and information geometric
    methods is another original contribution of my work .

Third, I extend my study of chaotic systems (information
geometrodynamical approach to chaos, IGAC) to an ED Gaussian model
describing an arbitrary system of @xmath degrees of freedom. It is shown
that the hyperbolicity of a non-maximally symmetric @xmath -dimensional
statistical manifold @xmath underlying the ED Gaussian model leads to
linear information-geometrodynamical entropy growth and to exponential
divergence of the Jacobi vector field intensity, quantum and classical
features of chaos respectively. As a special physical application, the
information geometrodynamical scheme is applied to investigate the
chaotic properties of a set of @xmath -uncoupled three-dimensional
anisotropic inverted harmonic oscillators (IHOs) characterized by an
Ohmic distributed frequency spectrum and I show that the asymptotic
behavior of the information-geometrodynamical entropy is characterized
by linear growth. Finally the anisotropy of the statistical manifold
underlying such physical system and its relationship with the spectrum
of frequencies of the oscillators are studied.

  Third Contribution  

    cafaro4 I compute the asymptotic temporal behavior of the
    information geometrodynamical entropy of a set of @xmath -uncoupled
    three-dimensional anisotropic inverted harmonic oscillators (IHOs)
    characterized by an Ohmic distributed frequency spectrum and I
    suggest the classical information-geometric analogue of the
    Zurek-Paz quantum chaos criterion in its classical reversible limit
    .

I am aware that several points in my IGAC need deeper understanding and
analysis, however I hope that my work convincingly shows that:

  Point 1  

    Laws of physics are deeply geometrical because they are practical
    rules to process information about the world and geometry is the
    most natural tool to carry out that task. The notion that laws of
    physics are not laws of nature but rules of inference seems
    outrageous but cannot be simply dismissed. Indeed, it deserves
    serious attention and further research.

  Point 2  

    This is a novel and unorthodox research area and there are many
    risks and criticisms cafaro5 . I believe the information
    geometrodynamical approach to chaos may be useful in providing a
    unifying criterion of chaos of both classical and quantum varieties,
    thus deserving further research and developments.

Chapter 2: Maximum entropy methods and information geometry

I review the basic information physics and mathematical tools employed
in my work. First, I review the major aspects of the Maximum Entropy
method (ME method), a unique general theory of inductive inference.
Second, I recall some basic elements of conventional Riemannian
differential geometry useful for the understanding of standard
approaches to the geometrical study of chaos. Finally, I introduce the
basics of Riemannian geometry applied to probability theory, namely
Information Geometry (IG). IG and ME are the basic tools that I need to
introduce in order to allow the reader to follow the description of the
constrained information dynamics on curved statistical manifolds
(entropic dynamics, ED).

## Iii Introduction

Inference is the process of drawing conclusions from available
information. Information is whatever constraints rational beliefs
caticha(aip07) .

When the information available is sufficient to make unequivocal, unique
assessments of truth we speak of making deductions: on the basis of this
or that information we deduce that a certain proposition is true. The
method of reasoning leading to deductive inferences is called logic.
Situations where the available information is insufficient to reach such
certainty lie outside the realm of logic. In these cases we speak of
making a probable inference, and the method of reasoning is probability
theory. An alternative name is ” inductive inference ”. The word
”induction” refers to the process of using limited information about a
few special cases to draw conclusions about more general situations.

The main goal of inductive inference is to update from a prior
probability distribution to a posterior distribution when new relevant
information becomes available. Updating methods should be systematic and
objective. The most important updating methods are the Bayesian updating
method sivia and the ME method caticha(aip07) ; shore ; skilling ;
caticha(aip04) ; caticha(book) ; caticha-giffin(aip06) .

The ME method is a generalization of Jaynes’ method of maximum entropy,
MaxEnt method jaynes . Jaynes’ method of maximum entropy is a method to
assign probabilities on the basis of partial testable information.
Testable information is sufficient to make a prediction and predictions
can be tested. MaxEnt arises as a rule to assign a probability
distribution, however it can be extended to a full-fledged method for
inductive inference. The extended method will henceforth be abbreviated
as ME. In Jaynes’s MaxEnt method, it is shown that statistical mechanics
and thus thermodynamics are theories of inference. MaxEnt can be
interpreted as a special case of ME when one updates from a uniform
prior using the Gibbs-Shannon entropy.

The nature of the information being processed dictates the choice
between the Bayesian updating method and the ME method. Bayes’ theorem
jaynes should be used to update our beliefs about the values of certain
quantities @xmath on the basis of information about the observed values
of other quantities @xmath - the data- and of the known relation between
them- the conditional distribution @xmath . If @xmath are the prior
beliefs, the updated or posterior distribution is given by @xmath . The
Bayesian method of updating is a consequence of the product rule for
probabilities and therefore it is limited to situations where it makes
sense to define the joint probability of @xmath and @xmath , @xmath . On
the other hand, the ME method is designed for updating from a prior to a
posterior probability distribution when the information to be processed
takes the form of constraints on the family of acceptable posterior
distributions. Although the terms ”prior” and ”posterior” are normally
used only in the context of Bayes’ theorem, we will adopt the same
terminology when using the ME method since we are concerned with the
same goal of processing information to update from a prior to a
posterior probability distribution. As a final remark, we point out that
in general it is meaningless to use Bayes’ theorem to process
information in the form of constraints, and conversely, it is
meaningless to process data using ME. However, there are special cases
where the same piece of information can be both interpreted as data and
as constraint, In such cases, both methods can be used and it can be
shown that they agree.

## Iv What is the Maximum Relative Entropy Formalism?

Consider a multidimensional discrete or continuous variable @xmath .
Assume the prior probability distribution @xmath describes the
uncertainty about @xmath . When new relevant information becomes
available, our goal is to update from @xmath to a posterior probability
distribution @xmath . Information appears in the form of constraints and
usually could be given in terms of expected values. The problem consists
in selecting the proper @xmath among all those posterior probability
distributions within the family defined by the available relevant
constraints.

Skilling skilling suggested that in order to select the posterior @xmath
it seems reasonable to rank the candidate distributions in order of
increasing preference. The ranking must be transitive: if distribution
@xmath is preferred over distribution @xmath , and @xmath is preferred
over @xmath , then @xmath is preferred over @xmath . To each @xmath is
assigned a real number @xmath , which we will henceforth call entropy,
in such a way that if @xmath is preferred over @xmath , then @xmath .
The probability distribution that maximizes the entropy @xmath will be
the selected posterior distribution @xmath . Therefore, it becomes
evident to conclude that the Maximum Entropy method (ME) is a
variational method involving entropies which are real numbers.

Moreover, to define the ranking scheme, a functional form of @xmath must
be chosen. Recall that the purpose of the ME method is to update from
priors to posteriors and that the ranking scheme must depend on the
particular prior @xmath and therefore the entropy @xmath must be a
functional of both @xmath and @xmath . Thus the entropy @xmath , @xmath
produces a ranking of the distributions @xmath relative to the given
prior @xmath : @xmath , @xmath is the entropy of @xmath relative to
@xmath . Accordingly @xmath , @xmath is commonly called relative
entropy. The modifier ”relative” is redundant and will be dropped since
all entropies are relative, even when relative to a uniform
distribution. Moreover, since we deal with incomplete information the ME
method cannot be deductive: the method must be inductive. Therefore, we
may find useful to use those special cases where we know what the
preferred distribution should be to eliminate those entropy functionals
@xmath , @xmath that fail to provide the right update. In general, the
known special cases are called the axioms of the theory. Since they
define what makes one distribution preferable over another, they play a
very crucial role in the ME updating method.

In what follows, we will briefly present the three axioms of the ME
method. The axioms do not refer to merely three cases; any induction
from such a weak foundation would hardly be reliable. The reason the
axioms are convincing and so constraining is that they refer to three
infinitely large classes of known special cases. Additional details and
proofs are given in caticha(aip04) ; caticha-giffin(aip06) .

Axiom 1: Locality. Local information has local effects .

Assume the information to be processed does not refer to a special
subdomain @xmath of the space @xmath of @xmath ’s. The PMU ( Principle
of Minimal Updating (PMU): Beliefs should be updated only to the extent
required by the new information ) requires we do not change our minds
about @xmath in the absence of any new available information about
@xmath . Therefore, we design the inference method so that the prior
probability of @xmath conditional on @xmath @xmath , @xmath , is not
updated. The selected conditional posterior is @xmath . The consequence
of axiom @xmath is that non-overlapping domains of @xmath contribute
additively to the entropy. Dropping multiplicative factors and additive
terms that do not affect the overall ranking, the entropy functional
becomes

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where @xmath is some unknown function.

Axiom 2: Coordinate invariance. The system of coordinates carries no
information .

Any of a variety of coordinate systems can be used to label the points
@xmath . The freedom to change coordinates should not affect the ranking
of the distributions. The consequence of axiom @xmath is that @xmath ,
@xmath can be written in terms of coordinate invariants such as @xmath
and @xmath , and @xmath :

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

(Multiplicative factors and additive terms that do not affect the
overall ranking have been dropped.) Thus the unknown function @xmath ,
@xmath , @xmath has been replaced by @xmath . The unknown functions
become now the density @xmath and the function @xmath with two
arguments. Next we determine the density @xmath by invoking the locality
axiom @xmath once again.

Axiom 1 (special case): When there is no new information there is no
reason to change one’s mind .

The domain @xmath in axiom @xmath coincides with the whole space @xmath
when no new information is available. The conditional probabilities
@xmath should not be updated and the selected posterior distribution
coincides with

the prior, @xmath . The consequence is that @xmath , @xmath becomes,

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

Axiom 3: Consistency for independent subsystems. When a system is
composed of subsystems that are known to be independent it should not
matter whether the inference procedure treats them separately or
jointly.

Assume the information on two independent subsystems @xmath and @xmath
that are treated separately is such that the prior distributions @xmath
and @xmath are respectively updated to @xmath and @xmath . When treated
as a single system the joint prior is @xmath and the family of potential
posteriors is @xmath , @xmath . The entropy functional must be such that
the selected posterior is @xmath . The consequence of axiom @xmath for
this particular case of just two subsystems is that entropies are
restricted to the one-parameter family given by

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

Multiplicative factors and additive terms that do not affect the overall
ranking scheme can be freely chosen. The @xmath case reproduces the
usual logarithmic relative entropy,

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

[Use @xmath in ( 4 ) and let @xmath @xmath to get ( 5 ).]

In caticha-giffin(aip06) it was argued that the index @xmath has to be
the same for all systems. Consistency requires that @xmath must be a
universal constant. From the success of statistical mechanics as a
theory of inference it was inferred that the value of this constant must
be @xmath @xmath leading to the logarithmic entropy, eq.( 5 ).

In conclusion, the ME updating method can be summarized as follows
caticha(aip07) :

The ME method : The objective is to update from a prior distribution
@xmath to a posterior distribution @xmath given the information that the
posterior lies within a certain family of distributions @xmath . The
selected posterior @xmath is that which maximizes the entropy @xmath ,
@xmath . Since prior information is valuable, the functional @xmath ,
@xmath has been chosen so that beliefs are updated only to the extent
required by the new information. No interpretation for @xmath , @xmath
is given and none is needed.

## V Elements of Riemannian Geometry

In this section I briefly recall some essential concepts and notations
of Riemannian differential geometry which are used in this dissertation.
The present section is only meant to facilitate the reader to follow the
work presented in the next Chapters, so that my discussion will not be a
rigorous treatment of the subject. For a more elaborate discussion, I
refer the reader to references in casetti , to textbooks of general
relativity landau ; wald ; de felice or to a more mathematically
oriented introductions to the subject given in do carmo . Finally, a
comprehensive and rigorous treatment, which goes far beyond what is
needed to follow the exposition in this Thesis, can be found in
Kobayashi and Nomizu kobayashi . As a side remark, I would like to
emphasize that each work appearing in reference casetti has deeply
shaped my own personal point of view concerning the relevance of
geometry in the study of chaos. However, the author’s main concern in
casetti was the investigation of classical chaos in the Riemannian and
Finslerian geometric frameworks . My personal objective is to
investigate both classical and quantum aspects of chaoticity in a hybrid
information-geometric framework being aware that, thus far, no
application of Finslerian geometry to probability theory is available in
the literature private .

### v.1 Notes on Riemannian manifolds

A differentiable manifold @xmath is a set that can be covered with a
collection, either finite or denumerable, of charts , such that each
point of @xmath is represented at least on one chart, and the different
charts are differentiably connected to each other. A chart is a set of
coordinates on the manifold, i.e., it is a set of @xmath real numbers
@xmath , . . . , @xmath which denote the ”position” of a point on the
manifold. The number @xmath of coordinates of a chart is the same for
each connected part of the manifold (and for the whole manifold if the
latter is connected, i.e., it cannot be split in two disjoint parts
which are still manifolds); such a number is called the dimension of the
manifold @xmath . The union of the charts on @xmath is called an atlas
of @xmath .

#### v.1.1 Tangent vectors and tensors

A possible way to define a vector is using curves on the manifold @xmath
. Given a curve @xmath in @xmath , represented in local coordinates by
the parametric equations @xmath , we define a tangent vector at @xmath
as the velocity vector of the curve in @xmath , i.e.,

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

so that the @xmath components of the tangent vector @xmath are given by

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

The set of all the tangent vectors of @xmath in @xmath is a linear
space, referred to as the tangent space of @xmath in @xmath , and
denoted by @xmath . Each tangent space is isomorphic to an @xmath
-dimensional Euclidean space. Given a chart @xmath , . . . , @xmath in a
neighborhood of @xmath , a basis @xmath , . . . , @xmath of @xmath can
be defined, so that a generic vector @xmath is expressed as a sum of the
@xmath ’s weighted by its components,

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

The basis @xmath is called a coordinate basis of @xmath , and its
vectors @xmath are often denoted  by @xmath (the origin of this notation
is in the fact that vectors can be defined as directional derivatives on
@xmath ). The basis depends on the chart: choosing another chart, @xmath
, . , @xmath , we get another basis @xmath . The components of @xmath in
the two different bases are connected by the following rule,

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

referred to as the vector transformation rule . Indeed, one can define a
vector as a quantity whose components transform according to ( 9 ). The
union of all the tangent spaces @xmath of the manifold @xmath ,

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

is a @xmath -dimensional manifold and is referred to as the tangent
bundle of @xmath .

A vector field @xmath on @xmath is an assignment of a vector @xmath at
each point @xmath . If @xmath is a smooth function,

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

is a real number for each @xmath , i.e., @xmath is a function on @xmath
. If such a function is smooth, @xmath is called a smooth vector field
on @xmath . The curves @xmath which satisfy the differential equations

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

are called the trajectories of the field @xmath , and the mapping @xmath
which maps any point @xmath of @xmath along the trajectory of @xmath
emanating from @xmath is called the flow of @xmath . Given two vector
fields @xmath , @xmath , one can define the commutator as the vector
field @xmath , @xmath such that

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

i.e., in terms of the local components,

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

We note that, if @xmath is a coordinate basis,

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

and that, conversely, given @xmath nonvanishing and commuting vector
fields that are linearly independent, there always exists a chart for
which these vector fields are a coordinate basis.

Tangent vectors are not the only vector-like quantities that can be
defined on a manifold @xmath : there are also cotangent vectors, which
can be defined as follows. Let us recall that the dual space @xmath of a
vector space @xmath is the space of linear maps from @xmath to the real
numbers. Given a basis of @xmath , @xmath , a basis of @xmath , { @xmath
}, called the dual basis , is defined by

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

The dual space of @xmath , @xmath , is called the cotangent bundle of
@xmath . Its elements are called cotangent vectors , or sometimes
covariant vectors (while the tangent vectors are sometimes denoted as
contravariant vectors ). The dual basis elements are usually denoted as
@xmath ,…, @xmath , i.e., @xmath is such that @xmath . The components
@xmath of cotangent vectors transform according to the rule

  -- -------- --
     @xmath   
  -- -------- --

to be compared with ( 9 ). The common rule is to use subscripts to
denote the components of dual vectors and superscripts for those of
vectors.

A @xmath , @xmath - tensor @xmath over a vector space @xmath is a
multilinear map

  -- -------- -- ------
     @xmath      (17)
  -- -------- -- ------

i.e., acting on @xmath dual vectors and @xmath vectors, @xmath yields a
number, and it does so in such a manner that if we fix all but one of
the vectors or dual vectors, it is a linear map in the remaining
variable. A @xmath , @xmath tensor is a scalar, a @xmath , @xmath tensor
is a vector, and a @xmath , @xmath tensor is a dual vector. The space
@xmath , @xmath of the tensors of type @xmath , @xmath is a linear
space; a @xmath , @xmath -tensor is defined once its action on @xmath
vectors of the dual basis and on @xmath vectors of the basis is known,
and since there are @xmath independent ways of choosing these basis
vectors, @xmath , @xmath is a @xmath -dimensional linear space. Two
natural operations can be defined on tensors. The first one is called
contraction with respect to the @xmath -th (dual vector) and the @xmath
-th (vector) arguments and is a map

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

defined by

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

The contracted tensor @xmath is independent of the choice of the basis,
so that the contraction is a well-defined, invariant, operation. The
second operation is the tensor product , which maps an element @xmath ,
@xmath @xmath @xmath , @xmath into an element of @xmath , @xmath , i.e.,
two tensors @xmath and @xmath into a new tensor, denoted by @xmath ,
defined as follows: given @xmath dual vectors @xmath , . . . , v @xmath
and @xmath vectors @xmath , . . . , @xmath , then

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

The tensor product allows one to construct a basis for @xmath , @xmath
starting from a basis @xmath of @xmath and its dual basis @xmath : such
a basis is given by the @xmath tensors @xmath ⋅ @xmath ⋅ @xmath ⋅ @xmath
⋅ @xmath ⋅ @xmath ⋅ @xmath . Thus, every tensor @xmath @xmath , @xmath
allows a decomposition

  -- -------- -- ------
     @xmath      (21)
  -- -------- -- ------

the numbers @xmath are called the components of @xmath in the basis
@xmath . The components of the contracted tensor @xmath are

  -- -------- -- ------
     @xmath      (22)
  -- -------- -- ------

and, the components of the tensor product @xmath are

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

All these results are valid for a generic vector space, so that they
hold in particular for the vector spaces of the tangent bundle @xmath of
@xmath , over which tensors (and, analogously to vector fields, tensor
fields) can be defined exactly as above.

#### v.1.2 Metrics on Riemannian manifolds

The length element @xmath (the infinitesimal square distance, the
metric) on @xmath can be defined at each point @xmath by means of a
@xmath , @xmath -tensor @xmath , provided it is symmetric , i.e., @xmath
, @xmath , @xmath , and nondegenerate , i.e., @xmath , @xmath @xmath if
and only if @xmath . In fact, a @xmath with these properties induces on
the tangent bundle @xmath a nondegenerate quadratic form (called the
scalar product ),

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

Then it is possible to measure lengths on the manifold. A manifold
@xmath , equipped with a scalar product, is called a (pseudo)Riemannian
manifold, and the scalar product is referred to as a (pseudo)Riemannian
structure on @xmath . If the quadratic form ( 24 ) is positive-definite,
then one speaks of a (proper) Riemannian metric. In the latter case the
squared length element is always positive. For instance, one can define
the length of a curve as

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

The curves @xmath which are extremals of the length functional are
called the geodesics of @xmath .

In a coordinate basis, we can expand the metric @xmath as

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

so that one defines the invariant (squared) length element on the
manifold, in local coordinates, as

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

The scalar product of two vectors @xmath and @xmath is given, in terms
of @xmath , by

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

In the above equation we have made use of the fact that @xmath
establishes a one-to-one correspondence between vectors and dual
vectors, i.e., in components,

  -- -------- -- ------
     @xmath      (29)
  -- -------- -- ------

For this reason, the components of the inverse metric @xmath are simply
denoted by @xmath , instead of @xmath , and allow to pass from dual
vector (covariant) components to vector (contravariant) components:

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

This operation of raising and lowering the indices can be applied not
only to vector, but also to tensor components. This allows us to pass
from @xmath , @xmath tensor components to the corresponding @xmath ,
@xmath tensor components and vice versa. What does not change in the
operation is the sum @xmath which is called the rank (or the order ) of
the tensor.

### v.2 Covariant differentiation on Riemannian manifolds

Differential calculus on Riemannian manifolds is complicated by the fact
that ordinary derivatives do not map vectors into vectors, i.e., the
ordinary derivatives of the components of a vector @xmath , @xmath ,
taken for instance at a point @xmath along a given curve @xmath , are
not the components of a vector in @xmath , because they do not transform
according to the rule ( 9 ). The geometric origin of this fact is that
the parallel transport of a vector from a point @xmath to a point @xmath
on a non-Euclidean manifold depends on the path chosen to join @xmath
and @xmath . Since in order to define the derivative of a vector at
@xmath , we have to move that vector from @xmath to a neighboring point
along a curve and then parallel-transport it back to the original point
in order to measure the difference, we need a definition of parallel
transport to define a derivative; conversely, given a (consistent)
derivative, i.e., a derivative which maps vectors into vectors, one
could define the parallel transport by imposing that a vector is
parallel transported along a curve if its derivative along the curve is
zero. The two ways are conceptually equivalent: we follow the first way,
by introducing the notion of a connection and then using it to define
the derivative operator. Such a derivative will be referred to as the
covariant derivative .

A (linear) connection on the manifold @xmath is a map @xmath such that,
given two vector fields (one could also consider tensor fields, but for
the sake of simplicity we define connections using vectors) @xmath and
@xmath , it yields a third field @xmath with the following properties:

1. @xmath is bilinear in @xmath and @xmath , i.e., @xmath @xmath and
@xmath ;

2. @xmath ;

3. (Leibnitz rule) @xmath , where @xmath is the ordinary directional
derivative in the direction of @xmath .

The parallel transport of a vector @xmath along a curve @xmath , whose
tangent vector field is @xmath , is then defined as the (unique) vector
field @xmath along @xmath such that

1. @xmath ;

2. @xmath along @xmath .

The notion of covariant derivative now immediately follows: the
covariant derivative @xmath of @xmath along @xmath is given by the
vector field

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

On the basis of equation ( 31 ), with a certain abuse of language, one
often refers to @xmath as the covariant derivative of @xmath along
@xmath , where @xmath and @xmath are generic vector fields. Among all
the possible linear connections, and given a metric @xmath , there is
one and only one which (i) is symmetric , i.e.,

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

and (ii) conserves the scalar product, i.e., the scalar product of two
parallel vector fields @xmath and @xmath is constant along @xmath ,

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

Such a linear connection is obviously the natural one on a Riemannian
manifold, and is referred to as the Levi-Civita (or Riemannian )
connection. Whenever we refer to a covariant derivative without any
specification, we mean the covariant derivative induced by the
Riemannian connection. The components of the Riemannian connection
@xmath with respect to a coordinate basis @xmath are the Christoffel
symbols , given by

  -- -------- -- ------
     @xmath      (34)
  -- -------- -- ------

and are given, in terms of the derivatives of the components of the
metric, by the following formula

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

where @xmath . The expression in local coordinates of the covariant
derivative ( 31 ) of a vector field @xmath is then

  -- -------- -- ------
     @xmath      (36)
  -- -------- -- ------

#### v.2.1 Geodesic Equation

The geodesics are defined as the curves of extremal length on the
manifold and can also be defined as self-parallel curves , i.e., curves
such that the tangent vector @xmath is always parallel transported. Thus
geodesics are the curves @xmath which satisfy the equation (referred to
as the geodesic equation )

  -- -------- -- ------
     @xmath      (37)
  -- -------- -- ------

whose expression in local coordinates follows from ( 36 ), and is

  -- -------- -- ------
     @xmath      (38)
  -- -------- -- ------

provided @xmath is an affine parameter. Since the norm of the tangent
vector @xmath of a geodesic is constant, @xmath , the arc length of a
geodesic is proportional to the parameter:

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

When the parameter is actually the arc length, i.e., @xmath , we say
that the geodesic is normalized . Whenever we consider a geodesic, we
assume it is normalized, if not explicitly stated otherwise. This means
that equations ( 38 ) are nothing but the Euler-Lagrange equations for
the length functional along a curve @xmath parametrized by the arc
length,

  -- -------- -- ------
     @xmath      (40)
  -- -------- -- ------

Given a congruence of geodesics @xmath on @xmath , there exists a unique
vector field @xmath on @xmath such that its trajectories are @xmath ,
@xmath . Such a vector field @xmath is called the geodesic field and its
flow @xmath , @xmath the geodesic flow on @xmath .

### v.3 The curvature tensor

A way of measuring how much a Riemannian manifold @xmath , @xmath
deviates from being Euclidean is by use of the curvature tensor . This
quantity, also known as the Riemann-Christoffel tensor , is a tensor of
order @xmath defined as

  -- -------- -- ------
     @xmath      (41)
  -- -------- -- ------

where @xmath is the Levi-Civita connection of @xmath . Observe that if
@xmath , then @xmath , @xmath for all the pairs of tangent vectors
@xmath , @xmath , because of the commutativity of the ordinary
derivatives. In addition, @xmath measures the noncommutativity of the
covariant derivative: in fact, if we choose a coordinate system @xmath
,…, @xmath , we have, since @xmath ,

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

In local coordinates, the components of the Riemann curvature tensor
(considered here as a @xmath , @xmath -tensor) are given by

  -- -------- -- ------
     @xmath      (43)
  -- -------- -- ------

Thus, given a metric @xmath , the curvature @xmath is uniquely defined.
A manifold @xmath , @xmath is called flat when the curvature tensor
vanishes.

Given a positive function @xmath , a conformal transformation is the
transformation

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

where @xmath is the metric tensor. Two Riemannian manifolds are said
conformally related if they are linked by a conformal transformation. In
particular, a manifold is @xmath , @xmath conformally flat if it is
possible to find a conformal transformation that sends @xmath into a
flat metric. Conformally flat manifolds exhibit some remarkable
simplifications for the calculation of the curvature tensor components
(see goldberg ).

Closely related to the curvature tensor is the sectional — or Riemannian
— curvature, which we define now. Let us consider two vectors @xmath ,
@xmath @xmath , and let us put

  -- -------- -- ------
     @xmath      (45)
  -- -------- -- ------

which is the area of the two-dimensional parallelogram determined by
@xmath and @xmath . If @xmath the vectors @xmath , @xmath span a
two-dimensional subspace @xmath . We define the sectional curvature at
the point @xmath relative to @xmath , as the quantity:

  -- -- -- ------
           (46)
  -- -- -- ------

which can be shown to be independent of the choice of the two vectors
@xmath , @xmath . In local coordinates, ( 46 ) becomes

  -- -------- -- ------
     @xmath      (47)
  -- -------- -- ------

The knowledge of @xmath for the @xmath planes @xmath spanned by a
maximal set of linearly independent vectors completely determines @xmath
at @xmath .

If dim @xmath then @xmath coincides with the Gaussian curvature of the
surface, i.e., with the product of the reciprocals of two curvature
radii.

A manifold is called isotropic if @xmath , @xmath does not depend on the
choice of the plane @xmath . A remarkable result — Schur’s theorem do
carmo — is that in this case @xmath is also constant, i.e. it does not
depend on the point @xmath either.

Some “averages” of the sectional curvatures are very important. The
Ricci curvature @xmath at @xmath in the direction @xmath is defined as
the sum of the sectional curvatures at @xmath relative to the planes
determined by @xmath and the @xmath directions orthogonal to @xmath ,
i.e., if @xmath ,…, @xmath , @xmath is an orthonormal basis of @xmath
and @xmath is the plane spanned by @xmath and @xmath ,

  -- -------- -- ------
     @xmath      (48)
  -- -------- -- ------

The scalar curvature @xmath at P is the sum of the @xmath Ricci
curvatures at @xmath ,

  -- -------- -- ------
     @xmath      (49)
  -- -------- -- ------

In terms of the components of the curvature tensor, such curvatures can
be defined as follows (in the following formulae, we drop the dependence
on @xmath , because it is understood that the components are local
quantities). We first define the Ricci tensor as the two-tensor whose
components, @xmath , are obtained by contracting the first and the third
indices of the Riemann tensor,

  -- -------- -- ------
     @xmath      (50)
  -- -------- -- ------

then,

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

The right hand side of ( 51 ) is called ”saturation” of @xmath with
@xmath . The scalar curvature can be obtained as the trace of the Ricci
tensor,

  -- -------- -- ------
     @xmath      (52)
  -- -------- -- ------

In the case of a constant curvature — or isotropic — manifold, the
components of the Riemann curvature tensor have the remarkably simple
form

  -- -------- -- ------
     @xmath      (53)
  -- -------- -- ------

where @xmath is the constant sectional curvature, so that the components
of the Ricci tensor are

  -- -------- -- ------
     @xmath      (54)
  -- -------- -- ------

and all the above defined curvatures are constants, and are related by

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

### v.4 The Jacobi-Levi-Civita equation

A brief derivation of the JLC (Jacobi-Levi-Civita) equation is presented
in this subsection. We will proceed as follows: first, we will define
the geodesic separation vector field @xmath , then we will show that the
field @xmath is actually a Jacobi field, i.e., obeys the Jacobi
equation.

Let us define a geodesic congruence as a family of geodesics @xmath ,
@xmath issuing from a neighborhood @xmath of a manifold point, smoothly
dependent on the parameter @xmath , and let us fix a reference geodesic
@xmath . Denote then by @xmath the vector field tangent to @xmath in
@xmath , i.e., the velocity vector field whose components are

  -- -------- -- ------
     @xmath      (56)
  -- -------- -- ------

and by @xmath the vector field tangent in @xmath to the curves @xmath
for a fixed @xmath , i.e., the vector field of components

  -- -------- -- ------
     @xmath      (57)
  -- -------- -- ------

The field @xmath will be referred to as the geodesic separation field ,
and measures the distance between nearby geodesics. Let us now show that
@xmath is a Jacobi field. First of all, we notice that the field @xmath
commutes with @xmath , i.e., @xmath , @xmath . In fact, from the
definition of the commutator ( 14 ) and from the definitions of @xmath ,
( 57 ), and of @xmath , ( 56 ), we have

  -- -------- -- ------
     @xmath      (58)
  -- -------- -- ------

and using again ( 57 ) and ( 56 ), we find that

  -- -------- -- ------
     @xmath      (59)
  -- -------- -- ------

so that @xmath , @xmath . Now, let us compute the second covariant
derivative of the field @xmath , @xmath . First of all, let us recall
that our covariant derivative comes from a Levi-Civita connection, which
is symmetric (see ( 32 )), so that

  -- -------- -- ------
     @xmath      (60)
  -- -------- -- ------

and having just shown that @xmath , @xmath , we can write

  -- -------- -- ------
     @xmath      (61)
  -- -------- -- ------

Now, using this result, and the fact that @xmath because @xmath is a
geodesic, we can write

  -- -------- -- ------
     @xmath      (62)
  -- -------- -- ------

from which, using the definition of the curvature tensor ( 41 ) and,
again, the vanishing of the commutator @xmath , @xmath , we get

  -- -------- -- ------
     @xmath      (63)
  -- -------- -- ------

which is nothing but the Jacobi equation written in compact notation. In
an explicit way the ( 63 ) can be written as,

  -- -------- -- ------
     @xmath      (64)
  -- -------- -- ------

It is worth noticing that the normal component @xmath of @xmath , i.e.,
the component of @xmath orthogonal to @xmath along the geodesic @xmath ,
is again a Jacobi field, since we can always write @xmath : one
immediately finds then that the velocity @xmath satisfies the Jacobi
equation, so that @xmath must obey the same equation. This can allow us
to restrict ourselves to the study of the normal Jacobi fields.

## Vi What is Information Geometry?

In the present section, I describe some of the basics of information
geometry (IG). IG is the result of applying conventional Riemannian
geometry to probability theory. Although interest in this subject can be
traced back to the late 1960’s, it reached maturity only through the
work of Amari in the 1980’s amari . The development of the field of
information geometry can only be said to have just begun.

### vi.1 Notes on Information Geometry

IG began as an investigation of the natural differential geometric
structure possessed by families of probability distributions. As a
rather simple example, consider the set of normal distributions with
mean @xmath and variance @xmath :

  -- -------- -- ------
     @xmath      (65)
  -- -------- -- ------

By specifying @xmath we determine a particular normal distribution, and
therefore the set may be viewed as a @xmath -dimensional space
(manifold) which has @xmath as a coordinate system. However, this is not
an Euclidean space, but rather a Riemannian space with a metric which
naturally follows from the underlying properties of the probability
distributions. Probability distributions are the fundamental element
over which fields such as statistics, stochastic processes, and
information theory are developed. Therefore, the geometric structure of
the space of probability distributions must play a fundamental role in
these information sciences. In fact, considering statistical estimation
from a differential geometric viewpoint has provided statistics with a
new analytic tool which has allowed several previously open problems to
be solved; information geometry has already established itself within
the field of statistics. In the fields of information theory, stochastic
processes, and systems, information geometry is being currently applied
to allow the investigation of hitherto unexplored possibilities.

The utility of information geometry, however, is not limited to these
fields. It has, for example, been applied productively to areas such as
statistical physics and mathematical theory underlying neural networks.

From a mathematical point of view, quantum mechanics may be constructed
as an extension of probability theory, and it is possible to generalize
many concepts in probability theory to their quantum equivalents. The
framework of information geometry for statistical models may also be
extended to the quantum mechanical setting. A variety of important works
related to differential geometrical aspects of quantum mechanics have so
far been made by many researchers. However, the study of quantum
information geometry has just started and we are far from getting its
whole perspective at present.

One of the fundamental questions information geometry helps to answer
is: ”Given two probability distributions, is it possible to define a
notion of ”distance” between them?”. This chapter, which introduces some
of the basic concepts of information geometry, does not presuppose any
knowledge of the theory of probability and distributions. Unfortunately
however, it does require some knowledge of Riemannian geometry.

### vi.2 Families of probability distributions as statistical manifolds

For our purposes, a probability distribution over some field (or set)
@xmath is a distribution

  -- -------- -- ------
     @xmath      (66)
  -- -------- -- ------

such that

  -- -------- -- ------
     @xmath      (67)
  -- -------- -- ------

In what follows, we will consider families of such distributions. In
most cases these families will be parametrized by a set of continuous
parameters @xmath that take values in some open interval @xmath and we
write @xmath to denote members of the family. For any fixed @xmath ,
@xmath is the mapping from @xmath to @xmath with

  -- -------- -- ------
     @xmath      (68)
  -- -------- -- ------

In information geometry, one extends a family of distributions @xmath ,

  -- -------- -- ------
     @xmath      (69)
  -- -------- -- ------

to a manifold @xmath such that the points in @xmath are in a one to one
relation with the distributions in @xmath . In doing so, one hopes to
gain some insight into the structure of such a family. For example, one
might hope to discover a reasonable measure of ”nearness” of two
distributions in the family. Having made the link between families of
distributions and manifolds, one can try to identify which objects in
the language of distributions naturally correspond to objects in the
language of manifolds and vice versa. The most important objects in the
language of manifolds are tangent vectors. The tangent space @xmath at
the point in @xmath with coordinates @xmath is seen to be isomorphic to
the vector space spanned by the model parameters (function from @xmath
to @xmath ) @xmath . This space is called @xmath . Therefore, a vector
field @xmath ,

  -- -------- -- ------
     @xmath      (70)
  -- -------- -- ------

is equivalent to a model parameter @xmath ,

  -- -------- -- ------
     @xmath      (71)
  -- -------- -- ------

Just as @xmath is the space of continuously differentiable mappings that
assigns some vector @xmath to each point @xmath , @xmath assigns a model
parameter @xmath .

In view of the above equivalence, we will not find necessary to
distinguish between the vector field @xmath and the corresponding random
variable @xmath . Equation ( 71 ) is called the @xmath - representation
of the vector field @xmath . It is clearly possible to use some other
basis of functionals of @xmath instead of @xmath . Our present choice
has the advantage that the @xmath - representation of a vector has zero
expectation value,

  -- -------- -- ------
     @xmath      (72)
  -- -------- -- ------

Using other functionals can be useful, and in fact the @xmath -
representation turns out to be just one member of the family of @xmath -
representations @xmath .

### vi.3 Distances between distributions: the Fisher-Rao information
metric

In several applications we are interested in distances between
distributions. For example, given a distribution @xmath and a
submanifold @xmath we may wish to find the distribution @xmath that is
”nearest” to @xmath in some sense. In order to fulfill such a task we
need a notion of distance on manifolds of distributions . In other
words, we need a metric.

Consider a family of probability distributions @xmath defined in terms
of some parameters @xmath with @xmath ,…, @xmath . The space of these
distributions constitutes a manifold, the points of which are the
distributions and, the parameters @xmath are convenient set of
coordinates. The structure of such manifolds is studied by introducing
conventional differential geometrical notions. Ultimately, the problem
is to quantify the extent to which we can distinguish between two
neighboring probability distributions @xmath and @xmath . If @xmath is
small enough the distributions overlap considerably, it is easy to
confuse them and we are inclined to say that the distributions are near.
More specifically we seek a real positive number that provides a
quantitative measure of the extent to which the two distributions can be
distinguished. When we interpret this measure of distinguishability as a
distance- the information metric- the manifold of distributions
immediately acquires a geometric structure and we can proceed to study
it using the mathematical techniques of differential geometry. It
appears that the introduction of geometrical methods is the natural way
to study spaces of probability distributions, to study how one changes
one’s mind and effectively moves in such a space as a result of
acquiring information. Perhaps, this is the reason why the models we
develop to describe the world are so heavily geometrical.

We are looking for a quantitative measure of the extent that two
probability distributions @xmath and @xmath can be distinguished. An
appealing and intuitive way to approach this problem is the following.
Consider the relative difference,

  -- -------- -- ------
     @xmath      (73)
  -- -------- -- ------

It might seem that the expected value of the relative difference is a
good candidate. However, it is not because it vanishes identically (see
( 72 )),

  -- -------- -- ------
     @xmath      (74)
  -- -------- -- ------

Instead, the variance does not vanish and therefore it is a good choice,

  -- -------- -- ------
     @xmath      (75)
  -- -------- -- ------

This is the measure of distinguishability for which we are searching; a
small value of @xmath means the points @xmath and @xmath are difficult
to distinguish. The matrix @xmath is called the Fisher information
matrix fisher . Thus far, no notion of distance has been introduced on
the space of states. In general, it is said that the reason it is
difficult to distinguish between two points is that they happen to be
too close together. However, it is very tempting to invert the logic and
assert that the two points @xmath and @xmath must be very close together
because they are difficult to distinguish. Moreover, notice that @xmath
is positive since it is a variance and it vanishes only when @xmath
vanishes. Therefore it is natural to interpret @xmath as the metric
tensor of a Riemannian space rao . It is known as the Fisher-Rao
information metric. This metric is a suitable metric for manifolds of
distributions and it is given by,

  -- -------- -- ------
     @xmath      (76)
  -- -------- -- ------

with @xmath and @xmath . Notice that @xmath . Obviously, the information
metric also defines an inner product: for two vector fields @xmath and
@xmath , we have

  -- -------- -- ------
     @xmath      (77)
  -- -------- -- ------

Rao recognized that @xmath is a metric in the space of probability
distributions and this recognition gave rise to the subject of
information geometry amari . This heuristic argument presents a
disadvantage, namely it does not make explicit a crucial property of the
Fisher-Rao metric: except for an overall multiplicative constant this
Riemannian metric is unique cencov ; campbell . The coordinates @xmath
are quite arbitrary and one can freely switch from one set to another.
It is then easy to check that @xmath are the components of a tensor,
that is, the distance @xmath is an invariant, a scalar. Incidentally,
@xmath is also dimensionless. At first sight, the definition ( 76 ) may
seem rather ad hoc. However, it has been proven amari ; rao ; corcuera
to be unique in having the following very appealing properties:

1.  @xmath is invariant under reparametrizations of the sample space
    @xmath ;

2.  @xmath is covariant under reparametrizations of the manifold @xmath
    (the parameter space).

The uniqueness of the Fisher-Rao information metric is the most
remarkable aspect about this metric: except for a constant scale factor,
it is the only Riemannian metric that adequately takes into account that
points of the manifold are not structureless; that is, that they are
probability distributions. As said before, a proof of such a result is
given in cencov . Once the information metric is given, then connection
coefficients, curvatures and other aspects of the geometry can be
computed in the conventional way.

### vi.4 Volume elements in curved statistical manifolds

Once the distances among probability distributions have been assigned, a
natural next step is to obtain measures for extended regions in the
space of distributions.

Consider an @xmath -dimensional volume of the statistical manifold
@xmath of distributions @xmath labelled by parameters @xmath with @xmath
,…, @xmath . The parameters @xmath are coordinates for the point @xmath
and in these coordinates it may not be obvious how to write down an
expression for a volume element @xmath . However, within a sufficiently
small region (volume element) any curved space looks flat. Curved spaces
are ”locally flat”. The idea then is rather simple: within that very
small region, we should use Cartesian coordinates and the metric takes a
very simple form, it is the identity matrix @xmath . In locally
Cartesian coordinates @xmath the volume element is simply given by the
product

  -- -------- -- ------
     @xmath      (78)
  -- -------- -- ------

which, in terms of the old coordinates, is

  -- -------- -- ------
     @xmath      (79)
  -- -------- -- ------

The problem consists in calculating the Jacobian @xmath of the
transformation that takes the metric @xmath into its Euclidean form
@xmath .

Let the new coordinates be defined by @xmath . A small change in @xmath
corresponds to a small change in @xmath ,

  -- -------- -- ------
     @xmath      (80)
  -- -------- -- ------

and the Jacobian is given by the determinant of the matrix @xmath ,

  -- -------- -- ------
     @xmath      (81)
  -- -------- -- ------

The distance between two neighboring points is the same whether we
compute it in terms of the old or the new coordinates,

  -- -------- -- ------
     @xmath      (82)
  -- -------- -- ------

Therefore the relation between the old and the new metric is,

  -- -------- -- ------
     @xmath      (83)
  -- -------- -- ------

Taking the determinant of ( 83 ), we obtain

  -- -------- -- ------
     @xmath      (84)
  -- -------- -- ------

and, therefore

  -- -------- -- ------
     @xmath      (85)
  -- -------- -- ------

Finally, we have succeeded in expressing the volume element totally in
terms of the coordinates @xmath and the known metric @xmath ,

  -- -------- -- ------
     @xmath      (86)
  -- -------- -- ------

The volume of any extended region on the manifold is given by,

  -- -------- -- ------
     @xmath      (87)
  -- -------- -- ------

As a final remark, note that @xmath is a scalar quantity and therefore
is invariant under general coordinate transformations, @xmath ,
preserving orientation. The square root of the metric tensor transforms
as xavier ,

  -- -------- -- ------
     @xmath      (88)
  -- -------- -- ------

and the flat infinitesimal volume element @xmath transforms as

  -- -------- -- ------
     @xmath      (89)
  -- -------- -- ------

Thus, from ( 88 ) and ( 89 ), we obtain

  -- -------- -- ------
     @xmath      (90)
  -- -------- -- ------

Equation ( 90 ) implies that the infinitesimal statistical volume
element is invariant under general coordinate transformations that
preserve orientation, that is with positive Jacobian.

## Vii ME and IG at work: a simple example

In what follows, I will briefly illustrate a couple of examples where
the ME method is employed. Further examples can be found in sivia .

Let the microstates of a physical system be labelled by @xmath , and let
@xmath be the number of microstates in the range @xmath . We assume that
a state of the system, a macrostate, is defined by the known expected
values @xmath of some @xmath variables @xmath with @xmath ,…., @xmath ,

  -- -------- -- ------
     @xmath      (91)
  -- -------- -- ------

This limited information will certainly not be sufficient to answer all
questions that one could conceivably ask about the system. Choosing the
right set of variables @xmath is perhaps the most difficult problem in
statistical mechanics. A crucial assumption is that ( 91 ) is not just
any random information, instead it is the right information for our
purposes.

It is convenient to think of each state as a point in an @xmath
-dimensional statistical manifold; the numerical values @xmath
associated to each point form a convenient set of coordinates. The ME
method allows us to associate a probability distribution to each point
in the space of states. The probability distribution @xmath that best
reflects the prior information contained in @xmath , updated by the
information @xmath , is obtained by maximizing the relative logarithmic
entropy

  -- -------- -- ------
     @xmath      (92)
  -- -------- -- ------

subject to the constraints ( 91 ) and to the normalization constraint
@xmath . Upon setting @xmath where @xmath is given by

  -- -------- -- ------
     @xmath      (93)
  -- -------- -- ------

and using the normalization constraint, we obtain

  -- -------- -- ------
     @xmath      (94)
  -- -------- -- ------

The partition function @xmath and the Lagrange multipliers @xmath are
defined as,

  -- -------- -- ------
     @xmath      (95)
  -- -------- -- ------

The maximized value of entropy is given by

  -- -------- -- ------
     @xmath      (96)
  -- -------- -- ------

For the sake of clarity, let us consider the following special case:
assume the normalization and information constraints are given by

  -- -------- -- ------
     @xmath      (97)
  -- -------- -- ------

where @xmath . Upon maximizing @xmath (where we have assumed a uniform
prior, @xmath ) subject to the above constraints, we obtain

  -- -------- -- ------
     @xmath      (98)
  -- -------- -- ------

The probability distribution @xmath , @xmath in ( 98 ) is the well-known
Gaussian probability distribution. As a last step, let us calculate the
Fisher-Rao information metric associated with a statistical manifold of
Gaussians. It is known that amari ,

  -- -------- -- ------
     @xmath      (99)
  -- -------- -- ------

where,

  -- -------- -- -------
     @xmath      (100)
  -- -------- -- -------

Substituting ( 98 ) in ( 100 ), after some algebra, we obtain

  -- -------- -- -------
     @xmath      (101)
  -- -------- -- -------

Finally, substituting ( 101 ) in ( 99 ), we get

  -- -------- -- -------
     @xmath      (102)
  -- -------- -- -------

with a line element @xmath given by

  -- -------- -- -------
     @xmath      (103)
  -- -------- -- -------

The Gaussian distribution is quite remarkable, it applies to a wide
variety of problems such as the distribution of errors affecting
experimental data, the distribution of velocities of molecules in gases
and liquids, the distribution of fluctuations of thermodynamical
quantities, and so on. Basically, it arises whenever we deal with
macroscopic variable that is the result of adding a large number of
small independent contributions. Gaussian distributions can be derived
as the distributions that codify information about the mean (first
moment) and the variance (second moment) while remaining maximally
ignorant about everything else. Gaussian distributions are successful
when third and higher moments are irrelevant.

Chapter 3: Information-constrained dynamics, part I: Entropic Dynamics

In this Chapter, I describe (following reference catichaED ) the basic
aspects of ”Entropic Dynamics”, a theoretical construct developed to
investigate the possibility that Einstein’s general theory of gravity
(or more generally, any classical or quantum theory of physics) may
emerge as a macroscopic manifestation of an underlying microscopic
statistical structure. ED is the starting point of the major
contribution of this thesis (the IGAC, information geometrodynamical
approach to chaos) and therefore I must revisit it. I emphasize the most
relevant points that I will use in my own IGAC and suggest further
improvements that, indeed, will be the object of Chapters 6 and 7.

## Viii Introduction

In Caticha’s Entropic Dynamics catichaED ,  the author explores the
possibility that the laws of physics might be laws of inference rather
than laws of nature. He explores what sort of dynamics can one derive
from well-established rules of inference. Specifically, given relevant
information codified in the initial and the final states, the problem is
to study the trajectory that the system is expected to follow. It turns
out the solution to this problem follows from a principle of inference,
the principle of maximum entropy, and not from a principle of physics.
The entropic dynamics derived this way exhibits some remarkable formal
similarities with other generally covariant theories such as general
relativity.

Dynamics is the study of changes occurring in nature. For instance,
thermodynamics deals with changes between states of equilibrium and
addresses the question of which final states can be reached from any
given initial state. Mechanics is the study of changes known as motion,
chemistry considers chemical reactions, quantum mechanics deals with
transitions between quantum states, and so on. In all of these examples,
the objective is predicting or explaining the observed changes on the
basis of relevant available information that is codified in the ”states”
of the system. In some cases the final state can be predicted with
certainty, in others the information available is incomplete and we can
only assign probabilities. Thermodynamics holds a very special place
among all these forms of dynamics. Thanks to the development of
statistical mechanics by Maxwell, Boltzmann, Gibbs and others, and
thanks to Jaynes’ work jaynes , thermodynamics became the first clear
example of a fundamental physical theory that could be derived from
general principles of probable inference. An appropriate choice of which
states one is considering, plus well-known principles of inference note
1 , namely, consistency, objectivity, universality and honesty lead to
the derivation of the entire theory of thermodynamics. These principles
lead to a unique set of rules for processing information: these are the
rules of probability theory cox and the method of maximum entropy jaynes
; shore . There are strong indications that quantum mechanics can be
deduced from principles of inference caticha pla . Many features of the
theory follow from the correct identification of the subject matter plus
general principles of inference. If the ”fundamental” theory of quantum
mechanics can be derived in this way, then it is possible that other
forms of dynamics might ultimately reflect laws of inference rather than
laws of nature. The fundamental equations of change, or motion, or
evolution would follow from probabilistic and entropic arguments in the
case that dynamics reflects laws of inference. The discovery of new
dynamical laws would be reduced to the discovery of what is the
necessary information for carrying out correct inferences. This is a
very important point. Unfortunately, this search for the right variables
has always been and remains to this day the major obstacle in the
understanding of new phenomena.

In his work (ED), Caticha explores the possible connection between the
fundamental laws of physics and the theory of probable inference. He
explores the possibility that dynamics can be derived from inference and
rather than starting with a known dynamical theory and attempting to
derive it, he proceeds in the opposite direction. His ED arises simply
from well-established rules of inference! In next section, the notation
is introduced, the space of states is defined, and a brief review
concerning the introduction of a natural quantitative measure of the
change involved in going from one state to another turns the space of
states into a metric space caticha IED is presented. (Such metric
structures have been found useful in statistical inference, where the
subject is known as Information Geometry amari , and in physics, to
study both equilibrium weinhold and nonequilibrium thermodynamics balian
.) Typically, once the kinematics appropriate to a certain motion has
been selected, one proceeds to define the dynamics by additional
postulates. This is precisely the option Caticha wanted to avoid: in the
dynamics developed here there are no such postulates. The equations of
motion follow from an assumption about what information is relevant and
sufficient to predict the motion.

In a previous work (irreversible entropic dynamics) caticha IED ,
Caticha considered a similar problem. Assuming that the system evolves
from a given initial state to other states, he studied the trajectory
that the system is expected to follow. In this problem, the existence of
a trajectory is assumed and, in addition, it is assumed that information
about the initial state is sufficient to determine it. The application
of a principle of inference, the method of maximum entropy (ME), to the
only information available, the initial state and the recognition that
motion occurred leads to the dynamical law. The resulting ”entropic”
dynamics is very simple: the system moves irreversibly and continuously
along the entropy gradient. However, the question of whether the actual
trajectory is the expected one remains unanswered and it depends on
whether the information encoded in the initial state happened to be
sufficient for prediction. For many systems more information is needed,
even for those for which the dynamics is reversible. In the reversible
case, assuming that the system evolves from a given initial state to a
given final state, the objective is to study what trajectory is the
system expected to follow. Again, it is implicitly assumed that there is
a trajectory, that in moving from one state to another the system will
pass through a continuous set of intermediate states. Again, the
equation of motion follows from a principle of inference, the principle
of maximum entropy, and not from a principle of physics. (For a brief
account of the ME method in a form that is convenient for our current
purpose see previous Chapter). In the resulting ”entropic” dynamics, the
system moves along a geodesic in the space of states. The geometry of
the space of states is curved and possibly quite complicated. Important
features of this entropic dynamics are explored in section 4.

## Ix The Fisher-Rao information metric

In this section, a quantitative description of the notion of change is
briefly reviewed (for more details see caticha IED ). First, change can
be measured by distinguishability since the larger the change involved
in going from one state to another, the easier it is to distinguish
between them. Next, using the ME method one assigns a probability
distribution to each state. This transforms the problem of
distinguishing between two states into the problem of distinguishing
between the corresponding probability distributions. The extent to which
one distribution can be distinguished from another is given by the
distance between them as measured by the Fisher-Rao information metric
amari ; fisher ; rao . Thus, change is measured by distinguishability
which is measured by distance. Let the microstates of a physical system
be labelled by @xmath , and let @xmath be the number of microstates in
the range @xmath . We assume that a state of the system (i.e., a
macrostate) is defined by the expected values @xmath of some @xmath
appropriately chosen variables @xmath ( @xmath , @xmath , . . . , @xmath
),

  -- -------- -- -------
     @xmath      (104)
  -- -------- -- -------

A very important assumption is that the selected variables codify all
the information relevant to answering the particular questions under
investigation. Again, we emphasize there is no systematic procedure to
choose the right variables. The selection of relevant variables is made
on the basis of intuition guided by experiment. Essentially, it is a
matter of trial and error. The variables should include those that can
be controlled or observed experimentally, but there are cases where
others must also be included. For instance, the success of equilibrium
thermodynamics originates from the fact that a few variables are
sufficient to describe a static situation, and being few, these
variables are easy to identify. On the other hand, in fluid dynamics the
selection is more difficult. One must include many more variables, such
as the local densities of particles, momentum, and energy, that are
neither controlled nor usually observed. The states of the system form
an @xmath -dimensional manifold with coordinates given by the numerical
values @xmath . A probability distribution @xmath is associated with
each state. In order to obtain the distribution that best reflects the
prior information contained in @xmath updated by the information @xmath
, we maximize the logarithmic relative entropy

  -- -------- -- -------
     @xmath      (105)
  -- -------- -- -------

subject to the constraints ( 104 ). The distribution obtained this way
is

  -- -------- -- -------
     @xmath      (106)
  -- -------- -- -------

where the partition function @xmath and the Lagrange multipliers @xmath
are given by

  -- -------- -- -------
     @xmath      (107)
  -- -------- -- -------

Furthermore, the change involved in going from state @xmath to the state
@xmath can be measured by the extent to which the two distributions can
be distinguished. As discussed in amari , except for an overall
multiplicative constant, the measure of distinguishability is given by
the ”distance” @xmath between @xmath and @xmath ,

  -- -------- -- -------
     @xmath      (108)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (109)
  -- -------- -- -------

is the Fisher-Rao metric fisher ; rao . This metric is unique, it is the
only Riemannian metric that adequately reflects the fact that the states
@xmath are probability distributions, not ”structureless points”.

## X Entropic dynamics

The main objective of ED is deriving the expected trajectory of the
system, assuming it evolves from a given initial state to a given final
state. The entropic dynamical framework implicitly assumes that there
exists a trajectory or, stated otherwise, that large changes are the
result of a continuous succession of very many small changes. Therefore,
the problem of studying large changes becomes the much simpler problem
of studying small changes. Focusing on small changes and assuming that
the change in going from the initial state @xmath to the final state
@xmath is small enough, the distance @xmath between such states becomes

  -- -------- -- -------
     @xmath      (110)
  -- -------- -- -------

In what follows, we explain how to find which states are expected to lie
on the trajectory between @xmath and @xmath . First, in going from the
initial to the final state the system must pass through a halfway point,
that is, a state @xmath that is equidistant from @xmath and @xmath .
Chosen the halfway state, the expected trajectory of the system is
determined. Indeed, there is nothing special about halfway states.
Similarly, we could have argued that in going from the initial to the
final state the system must first traverse a third of the way, that is,
it must pass through a state that is twice as distant from @xmath as it
is from @xmath . In general, the system must pass through an
intermediate states @xmath such that, having already moved a distance
@xmath away from the initial @xmath , there remains a distance @xmath to
be covered to reach the final @xmath . Halfway states have @xmath @xmath
, ”third of the way” states have @xmath @xmath , and so on. Each
different value of @xmath provides a different criterion to select the
trajectory. If there are several ways to determine an (assumed) existing
trajectory, consistency requires that all these ways should agree. The
selected trajectory must be independent of @xmath . Stated otherwise,
the main ED problem becomes the following: ”Initially, the system is in
state @xmath and new information is given to us. The system has moved to
one of the neighboring states in the family @xmath . The problem becomes
selecting the proper @xmath ”. This new formulation of the ED problem is
precisely the kind of problem to be tackled using the ME method.
Following caticha fluctuations and what was reported in Chapter 2, we
recall that the ME method is a method for processing information. It
allows us to go from an old set of beliefs, described by the prior
probability distribution, to a new set of beliefs, described by the
posterior distribution, when the available information is just a
specification of the family of distributions from which the posterior
must be selected note 2 . Usually, this family of posteriors is defined
by the known expected values of some relevant variables. This is not
necessary and the information-constraints need not be linear
functionals. In ED, constraints are defined geometrically. Whenever one
contemplates using the ME method, it is important to specify which
entropy should be maximized. The selection of a distribution @xmath
requires that the entropies to be considered must be of the form

  -- -------- -- -------
     @xmath      (111)
  -- -------- -- -------

Equation ( 111 ) defines the entropy of @xmath relative to the prior
@xmath . The interpretation of @xmath as the prior follows from the
logic behind the ME method itself. As a side remark, following reference
caticha fluctuations , I would like to recall that in the absence of new
information there is no reason to change one’s mind. The selected
posterior distribution should coincide with the prior distribution when
there are no constraints. Since the distribution that maximizes @xmath
subject to no constraints is @xmath , we must set @xmath equal to the
prior. That said, let us return to our ED problem. Assuming we know that
the system is initially in state @xmath and we are not given the
information that the system moved. Therefore, we have no reason to
believe that any change has occurred. The prior @xmath should be chosen
so that the maximization of @xmath subject to no constraints leads to
the posterior @xmath @xmath . The correct choice is @xmath @xmath .
Instead, assuming we know that the system is initially in state @xmath
and we are given the information that the system moved to one of the
neighboring states in the family @xmath , then the correct selection of
the posterior probability distribution is obtained by maximizing the
entropy

  -- -------- -- -------
     @xmath      (112)
  -- -------- -- -------

subject to the constraint @xmath . For the sake of simplicity, let us
write @xmath @xmath @xmath @xmath and @xmath so that @xmath becomes

  -- -------- -- -------
     @xmath      (113)
  -- -------- -- -------

and the distances @xmath and @xmath from @xmath to @xmath and @xmath are
defined as

  -- -------- -- -------
     @xmath      (114)
  -- -------- -- -------

In order to maximize @xmath under variations of @xmath subject to the
constraint

  -- -------- -- -------
     @xmath      (115)
  -- -------- -- -------

we introduce a Lagrange multiplier @xmath ,

  -- -------- -- -------
     @xmath      (116)
  -- -------- -- -------

After some algebra, it can shown that

  -- -------- -- -------
     @xmath      (117)
  -- -------- -- -------

The multiplier @xmath and the quantity @xmath are determined
substituting back into the constraint ( 115 ). From ( 114 ) we obtain
@xmath and @xmath , and therefore note 3

  -- -------- -- -------
     @xmath      (118)
  -- -------- -- -------

Therefore, the intermediate state @xmath selected by the maximum entropy
method must satisfy the following relation

  -- -------- -- -------
     @xmath      (119)
  -- -------- -- -------

The geometrical interpretation of ( 119 ) is straightforward: the
triangle defined by the points @xmath , @xmath , and @xmath degenerates
into a straight line. This is sufficient to determine a short segment of
the trajectory: all intermediate states lie on the straight line between
@xmath and @xmath . The generalization beyond short trajectories is
immediate: if any three nearby points along a curve lie on a straight
line the curve is a geodesic . This result is independent of the
arbitrarily chosen value @xmath so the potential consistency problem we
mentioned before does not arise. Summarizing, the answer to the ED
problem is the following catichaED : ”The expected trajectory is the
geodesic that passes through the given initial and final states”. As a
final remark, we would like to point out that in ED the motion is
predicted on the basis of a ”principle of inference”, the principle of
maximum entropy, and not from a ”principle of physics”. ED is derived in
an unusual way and one should expect some unusual features. Indeed,
unusual features arise as soon as one asks any question concerning time.
For example, ED determines the vector tangent to the trajectory @xmath ,
but not the actual ”velocity” @xmath . This becomes clear since there is
no reference to an external time @xmath nowhere in the ED problem nor in
any implicit background information. In order to find a relation between
the distance @xmath along the trajectory and the external time @xmath ,
additional information is required. In conventional forms of dynamics
(ED is not a conventional form of dynamics) this information is
implicitly encoded in a ”principle of physics”, in the Hamiltonian which
fixes the evolution of a system relative to external clocks. However,
the ED problem does not mention any external universe. The only clock
available is the system itself, and the problem becomes that of deciding
how this clock should be read. For instance, one of the variables @xmath
could be chosen as a clock variable and it could be arbitrarily called
intrinsic time. Intrinsic time should be defined so that motion looks
simple. Intrinsic time @xmath may be considered as quantified change. A
natural definition for the intrinsic time @xmath consists in stipulating
that the system moves with unit velocity, then @xmath is given by the
distance @xmath itself, @xmath . A very special consequence of this
definition of intrinsic time is that intervals between events along the
trajectory are not known a priori. Intervals are determined only after
the equations of motion are solved and the actual trajectory is
determined. ED shares this peculiar feature with the theory of General
Relativity (GR). In GR, as in ED, there is no reference to an external
time. For instance, it is known that in GR the proper time interval
along any curve between the initial and final three-dimensional
geometries of space representing the given initial and final states is
only determined after solving the Einstein equations of motion bsw . A
serious impediment in understanding the classical theory of gravity is
caused by the absence of an external time york , since there is no clear
understanding about which variables represent the true gravitational
degrees of freedom. This absence of an external time gives rise to
problems also in formulating a quantum theory of gravity kuchar ,
because of difficulties in defining equal-time commutators. In the
following section, following reference catichaED , we point out some
further formal similarities between ED and GR by presenting a Lagrangian
and Hamiltonian formulation of Entropic Dynamics.

## Xi Canonical formalism for entropic dynamics

ED can be derived from an ”action” principle. The trajectory of the
system is a geodesic and therefore the ”action” is the length itself

  -- -------- -- -------
     @xmath      (120)
  -- -------- -- -------

where @xmath is an arbitrary parameter along the trajectory. The
Lagrangian @xmath is given by

  -- -------- -- -------
     @xmath      (121)
  -- -------- -- -------

The action @xmath is invariant under reparametrizations @xmath provided
the end points are left unchanged, @xmath and @xmath . Indeed, when the
transformation is infinitesimal, @xmath , the corresponding change in
the action @xmath becomes,

  -- -------- -- -------
     @xmath      (122)
  -- -------- -- -------

This change in equation ( 122 ) vanishes provided @xmath . As a side
remark, we would like to point out that ED shares with GR the fact that
both are generally covariant theories. Furthermore, as emphasized in
claudio , there is an important distinction between the symmetries of a
generally covariant theory such as GR and the internal symmetries of a
proper gauge theory. The action of a generally covariant theory (such as
ED) is invariant under those reparametrizations that are restricted to
map the boundary onto itself. For proper internal gauge transformations
there are no such restrictions.

Recall that the standard Hamilton’s principle of least action for a
nonrelativistic particle demands extremizing the action

  -- -------- -- -------
     @xmath      (123)
  -- -------- -- -------

where @xmath is ”physical” time, and the interval between initial and
final states @xmath is given. Furthermore, recall that Jacobi’s
principle of least action for a particle with energy @xmath moving in a
potential @xmath determines the trajectory by extremizing the action

  -- -------- -- -------
     @xmath      (124)
  -- -------- -- -------

In Jacobi’s principle there is no reference to any time @xmath . The
time interval between initial and final states is not given, and the
parameter @xmath is unphysical and arbitrary. The determination of the
temporal evolution along the trajectory requires an additional
constraint,

  -- -------- -- -------
     @xmath      (125)
  -- -------- -- -------

Therefore, we emphasize that the ED action ( 120 ) is an action of the
Jacobi type, not of the Hamiltonian type. The natural choice for a
supplementary constraint that defines @xmath and determines the
evolution along the trajectory is given by

  -- -------- -- -------
     @xmath      (126)
  -- -------- -- -------

It is known that GR is also described by a Jacobi-type action brown and
this leads to an additional formal similarity between GR and ED. In
order to explore this similarity further it is convenient to construct
the canonical Hamiltonian version of Jacobi’s action. The canonical
momenta @xmath are defined as

  -- -------- -- -------
     @xmath      (127)
  -- -------- -- -------

and have unit magnitude,

  -- -------- -- -------
     @xmath      (128)
  -- -------- -- -------

The canonical Hamiltonian @xmath vanishes identically,

  -- -------- -- -------
     @xmath      (129)
  -- -------- -- -------

because the Lagrangian is homogeneous of first degree in the @xmath ’s.
From a physics point of view, this is evident since the generator of
time evolution (Hamiltonian) can be expected to vanish whenever there is
no external time with respect to which the system could possibly evolve.
We are led to consider the canonical action

  -- -------- -- -------
     @xmath      (130)
  -- -------- -- -------

subject to the constrained variations of the momenta @xmath ( 128 ).
Therefore, the correct variational principle requires to extremize the
action @xmath given by

  -- -------- -- -------
     @xmath      (131)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (132)
  -- -------- -- -------

and @xmath are Lagrange multipliers that enforce the constraint

  -- -------- -- -------
     @xmath      (133)
  -- -------- -- -------

for each value of @xmath . For later convenience, the overall factor of
@xmath in ( 132 ) is introduced (it amounts to rescaling @xmath ).
Variation of @xmath with respect to @xmath , @xmath , and @xmath leads
to the equations of motion,

  -- -------- -- -------
     @xmath      (134)
  -- -------- -- -------

and ( 133 ). Of course, there is no equation of motion for @xmath and it
must be determined from the constraint. It follows that,

  -- -------- -- -------
     @xmath      (135)
  -- -------- -- -------

which, using the supplementary condition ( 126 ), implies

  -- -------- -- -------
     @xmath      (136)
  -- -------- -- -------

The lapse function would be the analogue of @xmath in GR. Accordingly,
the analogue of ( 133 ) in GR is called the Hamiltonian constraint. The
quantity @xmath describes the increase of ”intrinsic” time per unit
increase of the unphysical parameter @xmath . In terms of @xmath the
equations of motion are given by

  -- -------- -- -------
     @xmath      (137)
  -- -------- -- -------

In generally covariant theories (such as GR and ED) there is no
canonical Hamiltonian (it vanishes identically) but there are
constraints. Information-constraints play the role of generators of
evolution and change in the unorthodox entropic dynamics.

## Xii Conclusions

In this Chapter, following reference catichaED , we tried to describe
the philosophy underlying the ED theoretical construct. We emphasized
that entropic dynamics is formally similar to other generally covariant
theories: the dynamics is reversible, the trajectories are geodesics,
the system supplies its own notion of an intrinsic time, the motion can
be derived from a variational principle that turns out to be of the form
of Jacobi’s action principle rather than the more familiar principle of
Hamilton. Furthermore, we pointed out that ”the canonical Hamiltonian
formulation of ED is an example of a constrained information-dynamics
where the information-constraints play the role of generators of
evolution” . In conclusion, as a general remark, it would be worthwhile
emphasizing that a reasonable physical theory must satisfy two key
requirements: the first is that it must provide us with a set of
mathematical models, the second is that the theory must identify real
physical systems to which the models might possibly apply. The ED
proposed in catichaED satisfies the first requirement, but it fails with
respect to the second. There are formal similarities with GR and whether
Einstein’s theory of gravity will in the end turn out to be an example
of ED is at this point no more than a speculation. A more definite
answer may be achieved once the still unsettled problem of identifying
those variables that describe the true degrees of freedom of the
gravitational field is resolved york ; kuchar .

In what follows in this Thesis, I briefly explain the main idea that
allowed Caticha and I to make progress in such ED theoretical construct.
It is known that Caticha’s ultimate goal is to develop statistical
geometrodynamics. The problem of GR is twofold: one is how geometry
evolves, and the other is how particles move in a given geometry. My
work on chaos focuses on how particles move in a given geometry and
neglects the other problem (the evolution of the geometry). The
realization that there exist two separate and distinct problems was a
turning point in my research and lead to an unexpected result that I
present in the next Chapters. Especially, in Chapter 6, I will argue
that ED may lead to conventional physical theories such as Newtonian
dynamics.

Chapter 4: The notion of chaos in physics

The notion of chaos in classical and quantum physics is introduced. The
Zurek-Paz quantum chaos criterion of linear entropy growth (von Neumann
entropy) is described. Moreover, I briefly review the basics of the
conventional Riemannian geometric approach to chaos. Finally, the notion
of Kolmogorov-Sinai dynamical entropy and that of Lyapunov exponents is
introduced.

## Xiii Classical Chaotic Dynamics

Since the time of Laplace and until rather recently most physicists
believed that, given dynamic equations and initial conditions, the
behavior of any macroscopic system can be reliably predicted. This
confidence in the deterministic nature of classical physics amazingly
coexisted with the experience of a large number of phenomena indicating
the opposite: fluid turbulence, various kinds of plasma instabilities,
games of chances (roulette, for example), and so on. The lack of
predictability in this cases was attributed to ”inessential” features,
such as uncertainty of the initial conditions, the influence of
uncontrolled external disturbances, or the participation of a very large
number of degrees of freedom, which made predictions practically
impossible.

Although the discovery of quantum mechanics destroyed the belief in the
deterministic nature of physical systems at the microscopic scale, this
was thought to be of no consequence for macroscopic systems that are
well described by classical physics. Meanwhile, the understanding of the
limited predictive capabilities of classical physics gathered force,
especially due to the advent of electronic computers that permitted the
systematic study of nonlinear dynamical systems. The importance of the
remarkable work of H. Poincaré poincare1 , who recognized the
non-integrability of even simple dynamical systems and their chaotic
properties, was not immediately recognized. Whereas, M. Born born , for
example, believed that the lack in long-term predictability of classical
motions could be attributed to errors in initial data, the famous study
of Fermi, Pasta and Ulam fermi1 surprisingly showed that not all
nonlinear dynamical systems are characterized by stochastic behavior.

Eventually, mainly through the efforts of mathematicians, a
qualitatively new concept of the nature of nonintegrable dynamical
systems, namely local instability of the trajectories of the majority of
nonlinear systems, was established. Numerous simple, apparently
deterministic dynamical systems are characterized by extremely irregular
and ultimately unpredictable motion, exclusively governed by the
internal dynamics of the system. This chaotic behavior, which is in no
way associated with the influence of external noise or uncertainty in
the initial conditions, can be truly called ”dynamical stochasticity”.
Dynamical chaos is characteristic of many nonlinear dynamical systems in
different branches of physics and other sciences: astronomy, chemistry,
biology, meteorology, end even econometrics. In fact, chaotic behavior
in mesoscopic systems is often the rule rather than the exception. This
statement has a precise mathematical foundation in the famous Siegel
theorem siegel : the nonitegrable Hamiltonians are dense among all
analytic Hamiltonians (Hamiltonians that are infinitely differentiable
and are locally described by a convergent power series zampieri ), but
the integrable Hamiltonians are not . This allows us to say that
nonintegrable systems are more abundant than the integrable ones.

Nonlinear dynamics was developed to its present form mainly by the
efforts of mathematicians, such as Poincaré, Birkhoff, Siegel,
Kolmogorov, Arnold, Moser, and Sinai.

As physicists we must be grateful for their far-sighted contributions,
and we still have to explore the full implications of their deep insight
into the physical world.

At the beginning of the last century, H. Poincaré poincare observed that
a fully deterministic dynamics does not necessarily imply explicit
predictions on the evolution of a dynamical system. This can be
considered a milestone in the approach to the study of dynamical chaos.
The content of the work by H. Poincaré and J. Hadamard is much more
conceptually deep and subtle than I have resumed here. Anyway, chaos as
an effect of instability of orbits in dynamical systems has remained for
a long time a sort of pure mathematical subject.

Only in the fifties, the Kolmogorov-Arnold-Moser theorem (KAM) kam and
the numerical experiment on a chain of nonlinearly coupled oscillators
by Fermi-Pasta-Ulam fermi have stressed again the fundamental relevance
of dynamical chaos not only on a mathematical, but also on a physical
ground. Later on, the works by E. Lorenz lorenz , M. Henon and C. Heiles
henon and B. V. Chirikov chirikov have provided new insights on the
origin of chaotic behaviors in dissipative as well as in conservative
systems. The main conceptual improvement is the observation that
dynamical chaos is not necessarily a consequence of the many degrees of
freedom present in a system; on the other hand such a system, at the
same time, can display, under certain conditions, ordered and very
complex behaviors.

It is known that nonabelian gauge theories show a chaotic behavior in
the classical limit. This manifests itself mainly in the rapid
divergence of gauge fields configurations initially adjacent in
configuration space which leads to a kind of saturation; a resemblance
of a thermal state. Chaotic phenomena play an important role in the
world of fundamental interactions, contributing to properties such as
quark confinement, chiral symmetry breaking, and particle reactions at
very high energy biro

### xiii.1 Integrable Dynamical Systems

The use of the @xmath -dimensional Hamiltonian phase space @xmath ,
where @xmath , @xmath are the generalized coordinates and momenta,
respectively, provides the ideal framework for the discussion of the
concepts of integrability and local instability of trajectories. Assume
that the Hamiltonian @xmath is given in terms of analytic functions of
the @xmath and @xmath . From Hamiltonian equations

  -- -------- -- -------
     @xmath      (138)
  -- -------- -- -------

it then follows Liouville’s theorem:

  -- -------- -- -------
     @xmath      (139)
  -- -------- -- -------

where @xmath is the constant phase space distribution along any
trajectory in phase space. It is worthwhile mentioning that this
particular property of Hamiltonian dynamics does not prevent the
occurrence of dynamical stochastic motion (chaotic motion).

The integration of ( 138 ) depends on the existence, and identification,
of so-called integrals of motion . In the Hamiltonian formalism, the
time-dependence of dynamical quantities is conveniently expressed in
terms of Poisson brackets. For some function @xmath the total time
derivative upon using ( 138 ) is given by

  -- -------- -- -------
     @xmath      (140)
  -- -------- -- -------

where @xmath is called the Poisson bracket of @xmath and @xmath . If the
function @xmath is explicitly time-independent, it is an integral (or
constant) of motion if its Poisson bracket with the Hamiltonian @xmath
vanishes. Evidently, for explicitly time-independent Hamiltonians,
@xmath itself is a constant of motion, i.e. the total energy of the
system is conserved.

A Hamiltonian system is said to be integrable , if there exist precisely
@xmath independent isolating integrals of motion @xmath :

  -- -------- -- -------
     @xmath      (141)
  -- -------- -- -------

where @xmath denotes the constant value of @xmath . The independence of
the quantities @xmath can be defined in terms of their mutual Poisson
brackets:

  -- -------- -- -------
     @xmath      (142)
  -- -------- -- -------

Usually, @xmath is taken as one of the constants of motion; this then
implies the time-independence of the @xmath : @xmath .

Examples of integrable dynamical systems are all systems with a single
degree of freedom described by an analytic Hamiltonian @xmath and all
systems with @xmath degrees of freedom that are described by linear
equations of motion. Such systems can be reduced to @xmath decoupled
normal modes by linear transformation. There are examples of nonlinear
dynamical systems that are integrable. A well-known example of a
nonlinear integrable system is the so-called Toda chain toda
corresponding to a chain of particles coupled by exponential two-body
potentials. For a closed chain of three particles, the Toda Hamiltonian
is:

  -- -------- -- -------
     @xmath      (143)
  -- -------- -- -------

Another familiar example of a completely integrable system is the
two-body Kepler problem. Besides the constants of motion determined by
space-time symmetry (energy and angular momentum), there exists a third
integral of motion, the Runge-Lenz vector (as a result of a hidden
dynamical @xmath symmetry). The existence of this symmetry is the reason
why the (elliptical) Kepler orbits are closed and the perihelion does
not precess.

### xiii.2 Non-integrable (chaotic) dynamical system

A dynamical system that is not integrable is called non-integrable. For
instance, turbulent dynamical systems are non-integrable. However,
turbulence and chaos are not synonyms! Turbulence is an example of a
spatio-temporal complexity . Spatio-temporal complex dynamics is not yet
completely understood, both from an experimental and a theoretical point
of view. Turbulent motions are indeed chaotic, but chaotic motions need
not be turbulent. Chaos may involve only a small number of degrees of
freedom, that is it can be narrow band in space and/or time. There are
numerous examples of chaotic systems characterized by temporal
complexity but spatial simplicity, like the Lorenz’s system. Turbulence
is different because it is always complex both in space and time.
Turbulent motions are not time reversible even though it is governed by
classical mechanics, i. e. Newton’s dynamics, which is time reversible.

#### xiii.2.1 Chaos in the weather: the Lorenz model

Sensitivity to initial conditions is what causes the seemingly
unpredictable, long-term evolution of chaotic motion, because even a
tiny error in the measurement of the initial conditions of a real
dynamical system leads rapidly to a lack of predictability of its
long-term behavior. As we cannot measure any real dynamical system with
infinite precision, the long term prediction of chaotic motion in such
systems is impossible, even if we know their equations of motion
exactly.

The sensitive dependence on initial conditions of chaotic systems is
more popularly known as the butterfly effect addison . This phenomenon
was first discovered by Edward Lorenz during his investigation into a
system of coupled ordinary differential equations (ODEs) used as a
simplified model of 2D thermal convection. known as Rayleigh-Benard
convection landau(fluids) . These equations are now called the Lorenz
equations, or Lorenz model.

In the Rayleigh-Benard convection there are two confining plates, a
bottom plate at temperature @xmath and a top plate at temperature @xmath
. For small temperature differences between the two plates, heat is
conducted through the stationary fluid between the plates. However, when
@xmath becomes large enough, buoyancy forces within the heated fluid
overcome internal fluid viscosity and a pattern of counter-rotating,
steady recirculating vortices is set up between the plates. Lorenz
noticed that, in his simplified mathematical model of Rayleigh-Benard
convection, very small differences in the initial conditions blew up and
quickly led to enormous differences in the final behavior. Lorenz
reasoned that if this type of behavior could occur in such a simple
dynamical system, then it may also be possible in a much more complex
physical system involving convection: the weather system. Thus, a very
small perturbation, caused for instance by a butterfly flapping its
wings, would lead rapidly to a complete change in future weather
patterns. The Lorenz equations are

  -- -------- -- -------
     @xmath      (144)
  -- -------- -- -------

This system has two nonlinearities, the @xmath term and the @xmath term,
and exhibits both periodic and chaotic motion depending upon the values
of the control parameters @xmath , @xmath and @xmath . The parameter
@xmath is the Prandtl number which relates the energy losses within the
fluid due to viscosity to those due to thermal conduction; @xmath
corresponds to the dimensionless measure of the temperature difference
between the plates known as the Rayleigh number; @xmath is related to
the ratio of the vertical height of the fluid layer to the horizontal
extent of the convective rolls within it. Note also that the variables
@xmath , @xmath , @xmath are not spatial coordinates but rather
represent the convective overturning greenT , horizontal temperature
variation, and vertical temperature variation respectively.

## Xiv What is Quantum Chaos?

The problem of quantum chaos arose from the attempts to understand the
very peculiar phenomenon of classical dynamical chaos in terms of
quantum mechanics. Preliminary investigations immediately unveiled a
very deep difficulty related to the fact that the two crucial properties
of classical mechanics necessary for dynamical chaos to occur
(continuous spectrum of motion and continuous phase space) are violated
in quantum mechanics. Indeed, the energy and frequency spectra of any
quantum motion, bounded in phase space, are always discrete. According
to the existing theory of dynamical systems such motion corresponds to
the limiting case of regular motion. The ultimate origin of this
fundamental quantum property is discreteness of the phase space itself
or, in modern mathematical language, a non-commutative geometry of the
latter. This is the very basis of all quantum physics directly related
to the fundamental uncertainty principle which implies a finite size of
an elementary phase-space cell,

  -- -------- -- -------
     @xmath      (145)
  -- -------- -- -------

The naive resolution of this difficulty would be the absence of any
quantum chaos. For this reason it was even proposed to use the term
”quantum chaology” berry87 which essentially means the study of the
absence of chaos in quantum mechanics. If the above conclusions were
true, a sharp contradiction would arise with the correspondence
principle which requires the transition from quantum to classical
mechanics for all phenomena including the new one: dynamical chaos. Does
this really mean a failure of the correspondence principle as some
authors insist ford ? If it were so quantum chaos would, indeed, be a
great discovery since it would mean that classical mechanics is not the
limiting case of quantum mechanics but a distinct theory. Unfortunately,
there exists a less radical (but also interesting and important)
resolution of this difficulty.

A recent breakthrough in the understanding of quantum chaos has been
achieved, particularly, due to a new philosophy which, either explicitly
or implicitly, is generally accepted; namely the whole physical problem
of quantum dynamics is considered as divided into two qualitatively
different parts:

  1  

    proper quantum dynamics as described by specific dynamical
    variables, the wavefunction @xmath ; and

  2  

    quantum measurement including the recording of the result and hence
    the collapse of the wavefunction @xmath .

The first part is described by some deterministic equation, for example,
the Schrodinger equation and naturally belongs to the general theory of
dynamical systems. The problem is well posed and this allows for
extensive studies.

The second part still remains very vague to the extent that there is no
common agreement even on the question whether this is a real physical
problem or an ill-posed one so that the Copenhagen interpretation of
quantum mechanics gives satisfactory answers to all the admissible
questions. In any event, there exists as yet no dynamical description of
quantum measurement including the @xmath -collapse.

The absence of a classical-like chaos is true for the above mentioned
first part of quantum dynamics only. Quantum measurement as far as the
result is concerned, is a random process: all quantum measurements to
date are thermodynamically irreversible. They are accompanied by an
increase in entropy of the measured system together with the measuring
apparatus. This irreversibility can be used to locate the boundary
between the classical and quantum domains. However, there are good
reasons to believe that this randomness can be interpreted as a
particular manifestation of dynamical chaos percival .

The separation of the first part of quantum dynamics, which is very
natural from a mathematical point of view, was introduced by Schrodinger
who, however, certainly underestimated the importance of the second part
in physics.

### xiv.1 The Zurek-Paz Quantum Chaos Criterion

In what follows, I will briefly describe the Zurek-Paz quantum chaos
criterion of von Neumann’s linear entropy growth.

#### xiv.1.1 Introduction

It is now well-known that nonlinearity in classical systems generically
leads to chaotic behavior. A necessary, but not sufficient,
characteristic of this is a sensitive dependence of the orbits on
initial conditions. Within this context the quantum analogue does not
exist. Classical and quantum mechanics, however, are not very different
when the dynamics of classical systems is rephrased in terms of the
linear Liouville equation for phase space distributions. The sensitive
dependence on initial conditions is mirrored, in this formulation, by
the linear increase with time of the coarse-grained Gibbs entropy which
is also known as the Shannon entropy. The quantum analogue of the
Shannon entropy is the von Neumann entropy @xmath @xmath defined by

  -- -------- -- -------
     @xmath      (146)
  -- -------- -- -------

@xmath being the density matrix of the system. For a Hamiltonian system
unitary evolution implies

  -- -------- --
     @xmath   
  -- -------- --

An ingredient which is crucial in quantum mechanics is measurement.
Classically measurement on a system can be made such that there is an
arbitrarily small disturbance on the system. In quantum mechanics this
is not the case as can be appreciated from the Heisenberg uncertainty
relations. The role of the environment in the quantum evolution of
chaotic systems is well-known sarkar . This line of thought suggests
that a more natural way to restore the quantum classical correspondence
is to consider physical systems not as being isolated from the rest of
the universe but as undergoing constant and varied interactions with it.
This is an inescapable fact of life and must be accounted for, at least
approximately, in any attempt to describe the real quantum behavior of
microscopic and macroscopic objects zurek . The destruction of phase
coherence in a quantum system because of the continuous monitoring of
its state by internal kolovsky and external zurek degrees of freedom is
a process known as decoherence . In other words, decoherence is the loss
of phase coherence between the set of preferred quantum states in the
Hilbert space of the system due to the interaction with the environment.

The coupling of a quantum system to an environment turns on the
decoherence process which leads to the emergence of classicality. In
conclusion, classicality is an emergent property of an open quantum
system (during a measurement process, information gets transformed from
quantum to classical).

#### xiv.1.2 A toy model: the inverted harmonic oscillator

In a rigorous examination of the entropy approach to the
classical-quantum correspondence problem Zurek and Paz zurek have
considered the completely tractable model of an inverted harmonic
oscillator coupled to a high temperature (harmonic) bath. The
Hamiltonian of the combined system is

  -- -------- -- -------
     @xmath      (147)
  -- -------- -- -------

where @xmath is the inverted oscillator Hamiltonian

  -- -------- -- -------
     @xmath      (148)
  -- -------- -- -------

@xmath is the Hamiltonian of the chosen bath with canonical commutation
relations @xmath ,

  -- -------- -- -------
     @xmath      (149)
  -- -------- -- -------

and @xmath is the Hamiltonian of interaction describing the (possibly
time-dependent) coupling of the inverted harmonic oscillator, through
its position variable of each of the environmental oscillators,

  -- -------- -- -------
     @xmath      (150)
  -- -------- -- -------

The potential energy function in ( 148 ) is an inverted parabola with
its apex at the origin. It is a model of instability in classical
mechanics and the phase space dynamics governed by this Hamiltonian is
an excellent model of a hyperbolic fixed point giannoni . It is
certainly not a chaotic system — it lacks the folding requirement — but
the parameter @xmath is analogous to a Lyapunov exponent in a genuinely
chaotic system. This is because it induces the exponential rate of
divergence (convergence) of nearby points on the unstable (stable)
manifold in its @xmath -dimensional phase space. These linear manifolds
intersect at the origin, i.e. at the only fixed point of the dynamics.
Let us consider the time evolution of a particle moving in the inverted
oscillator potential @xmath , but now let us also consider it to be
coupled through its position @xmath to the position variables of each
oscillator in the infinite set which we use as a model of a thermal bath
at a high temperature. Further choosing the distribution of frequencies
of this set to be of an Ohmic type zurek1 it is possible to derive a
master equation for the reduced density matrix, @xmath , which describes
the state of the particle at any time. The quantum Liouville equation
reads zurek ; caldeira

  -- -------- -- -------
     @xmath      (151)
  -- -------- -- -------

where @xmath is the reduced density matrix in the position
representation, @xmath and @xmath describes the strength of coupling to
the environment and serves as a dissipation parameter. Upon making the
weak coupling assumption of @xmath , Zurek and Paz have solved the
equation corresponding to the above for the Wigner function. This task
is made considerably easier by the fact that the form of the potential
for the inverted oscillator implies that all the quantum correction
terms vanish identically and the quantum Liouville equation in Wigner’s
representation becomes

  -- -------- -- -------
     @xmath      (152)
  -- -------- -- -------

For more general potentials, @xmath rd or higher order derivatives of
@xmath would appear in ( 152 ). On calculating the rate of change of von
Neumann entropy it can be shown that zurek

  -- -------- -- -------
     @xmath      (153)
  -- -------- -- -------

that is,

  -- -------- -- -------
     @xmath      (154)
  -- -------- -- -------

where @xmath is the von Neumann entropy of the system, @xmath is the
reduced density matrix of the system at time @xmath and @xmath is the
same as in ( 148 ). Here @xmath is a constant dependent on the initial
choice of density matrix. The quantum entropy production rate is
determined by the classical instability parameter @xmath . Given that
the classical Lyapunov exponent to which @xmath is analogous is equal to
the Kolmogorov-Sinai (KS) entropy of the system, this is indeed a
remarkable characterization. It suggests that after a time, a quantum,
classically chaotic system loses information to the environment at a
rate determined entirely by the rate at which the classical system loses
information as a result of its dynamics, namely, the KS entropy. Notice
that the KS entropy is not really an entropy. It is an entropy per unit
time, or an ”entropy rate”. Further details on this last remark will
appear at the end of this Chapter.

The inverted harmonic oscillator is intended as a model of instability
and, in fact, the dynamical behavior in phase space is dominated by a
hyperbolic point at the origin. The unstable and stable directions and
the rate at which initial phase space distributions expand and contract
in these directions, respectively, are determined by @xmath . In this
sense we call @xmath an instability parameter analogous to a Lyapunov
exponent in a classical chaotic system. Indeed, at any point on a
trajectory the sum of the Lyapunov exponents is zero. For a chaotic
trajectory there must be at least two nonzero Lyapunov exponents.

The Zurek-Paz conjecture is not free from criticisms miller . Equation (
154 ) has been derived with the help of various simplifying assumptions.
Their conjecture is supported by the fact that hyperbolic points, such
as that exhibited at the origin in the inverted oscillator model, are
ubiquitous in the phase space of a chaotic system. The inverted harmonic
oscillator model is, therefore, a possible representation of the local
behavior in chaotic classical evolution.

However, there are a number of reasons why we should question any
conclusions drawn as to the implications for a real chaotic system based
on so simple a model. First, there are no quantum corrections to the
Wigner function evolution for this quadratic potential (derivatives of
third and higher order vanish, and with them the quantum corrections).
The model does not allow for these influences on the dynamics, which,
though small in the presence of an environment in comparison to the
classical terms, nonetheless are generally always present. Moreover,
Zurek and Paz let the dissipation parameter @xmath while keeping @xmath
constant. This means, essentially, increasing the environmental
temperature and/or the mass of the particle. However, the dissipation
term can never be dropped completely since this would entail setting
@xmath identically, implying decoupling from the environment and hence
@xmath too. A third and related model-dependent assumption concerns the
choice of a thermal bath as environment. Assumed are such features as a
very large or even infinite number of degrees of freedom in the bath,
the special choice of the density of frequencies of the oscillators
which comprise it and the independent, non-interacting nature of these
oscillators.

Moreover, phase-space considerations may be addressed as well. The
stable and unstable manifolds associated with all hyperbolic points in
Hamiltonian chaotic systems intersect one another and those intersection
points are associated with other hyperbolic points (hyperbolic points
are sometimes called saddle points; they are fixed points in phase space
with the property that the trajectories are hyperbolae around them tel
). In this way homoclinic and heteroclinic points are formed (homoclinic
points are intersections of the stable and unstable manifolds of the
same cycle point while heteroclinic points are intersections of the
stable and unstable manifolds of different cycle points tel ; arecchi ).
The stable and unstable manifolds of the inverted oscillator intersect
only at the hyperbolic origin in phase space. Clearly, therefore, the
effect that the complicated distribution of homoclinic points might have
on the open dynamics is not taken into account. Neither, of course, is
the effect of heteroclinic points.

The inverted oscillator model is not chaotic. It has fixed stable and
unstable directions which intersect at the origin of phase space, the
only fixed point of the dynamics. Thus, it does not take into account
the effect of elliptic points, homoclinic points, heteroclinic points,
stable islands or cantori tel (cantori are invariant Cantor sets in the
irregular or stochastic region of phase space remaining after
destruction of KAM surfaces and create partial barriers to transport in
chaotic regions) on the open quantum dynamics tabor . In short, it is
not a good model of the extremely complex mixed phase spaces in which
trajectories of generic Hamiltonian systems evolve. Importantly, too,
the inverted oscillator model does not take into account the folding
mechanism which, along with stretching, characterizes classical chaos.
Indeed, the fixed direction of the stable and unstable manifolds are
quite inadequate to represent the rapid change in the direction of the
local stable and unstable manifolds typically seen in genuinely chaotic
systems. In ignoring this essential ingredient of chaos one is, in
effect, ignoring the fact that the directions of squeezing and
contraction change rapidly along a typical trajectory. Farini et al.
farini have illustrated the dangers of ignoring the folding effect by
studying a driven particle in a quartic double well potential in the
absence of an environment (see habib for a study with an environment).

Notwithstanding these objections, however, the inverted oscillator
remains a tractable model of instability both for a closed system and
for an open system in the presence of an environment. As such, it
deserves attention for the insights it might give regarding the
qualitative and maybe quantitative behavior of genuine, open quantum
analogs of classically chaotic systems.

The purpose of studying such an elementary system is to build up some
degree of intuition as to the behavior of quantum chaotic systems
coupled to an environment. A priori the claims of applicability of an
inverted oscillator to modeling a chaotic system should be treated with
caution. To a certain degree, the results for the oscillator can serve
as a guide to actual quantum behavior in chaotic systems. There is
indeed some value in using the inverted harmonic oscillator as a toy
model of instability in open quantum systems. The conjectures that
follow from it should, however, be tested in more systems that are
classically chaotic.

## Xv Geometry and Chaos

The investigations on the occurrence of regular and chaotic behavior in
@xmath -dimensional dynamical systems are performed with a variety of
methods and mathematical tools. Recently, this ensemble widened with the
inclusion of the Riemannian and Finslerian geometric approaches casetti
; cipriani .

As other new approaches, this tool has been suggested and applied to the
study of stability properties of general dynamical systems, in the hope
to bring the phenomenological analysis of their possibly chaotic
behavior back to an inquiry directed towards an explanation, at least
qualitative, of the mechanisms responsible for the onset of chaos.
Within the framework of the geometrical picture, this explanation was
sought through a possible link between a change in the curvature
properties of the underlying manifold and a modification of the
qualitative dynamical behavior of the system. Within the Hamiltonian
approach, the ingredients needed to make chaos lie basically on the
presence of stretching and folding of dynamical trajectories; i.e., in
the existence of a s trong dependence on initial conditions , which,
together with a bound on the extension of the phase space, yield to a
substantial unpredictability on the long time evolution of a system.
Usually, the strong dependence is detected looking at the occurrence of
an exponentially fast increase of the separation between initially
arbitrarily close trajectories. To have true chaos, this last property
must be however supplemented by the compactness of the ambient space
where dynamical trajectories live, this simply in order to discard
trivial exponential growths due to the unboundedness of the volume at
disposal of the dynamical system. Stated otherwise, the folding is
necessary in order to have a dynamics actually able to mix the
trajectories, making practically impossible, after a finite interval of
time, to discriminate between trajectories which were very nearby each
other at the initial time. When the space isn’t compact, even in
presence of strong dependence on initial conditions, it could be
possible, in some instances (though not always), to distinguish among
different trajectories originating within a small distance and then
evolved subject to exponential instability. In the geometric description
of dynamics, the recipe to find chaos is essentially the same, with some
minor differences, which nevertheless prove sometimes to be very
relevant for the understanding of the qualitative behavior of the
system. When the geometrization procedure is accomplished, the study of
dynamical trajectories is brought back to the analysis of a geodesic
flow on a suitable manifold @xmath . In order to define chaos, @xmath
should be compact, and geodesics on it have to deviate exponentially
fast.

## Xvi Riemannian geometrization of Hamiltonian dynamics

Consider a classical Hamiltonian dynamical systems with @xmath degrees
of freedom, confined in a finite volume (usually systems defined on a
lattice are considered), whose Hamiltonian is of the form

  -- -------- -- -------
     @xmath      (155)
  -- -------- -- -------

where the @xmath ’s and the @xmath ’s are, respectively, the coordinates
and the conjugate momenta of the system. Our emphasis is on systems with
a large number of degrees of freedom. The dynamics of the system ( 155 )
is defined in the @xmath -dimensional phase space spanned by the @xmath
’s and the @xmath ’s. It is possible to relate the dynamical and the
statistical properties of the system ( 155 ) with the geometrical and
topological properties of the phase space where the dynamical
trajectories of the system live casetti . It turns out that as long as
we consider Hamiltonians of the form ( 155 ) we can restrict ourselves
to the study of the geometry and the topology of the @xmath -dimensional
configuration space (actually, an enlarged configuration space with two
extra dimensions may also be considered) without losing information. In
fact, the dynamical trajectories can be seen as geodesics of the
configuration space, provided the latter has been endowed with a
suitable metric.

A Hamiltonian system whose kinetic energy is a quadratic form in the
velocities is referred to as a natural Hamiltonian system. Every
Newtonian system, that is a system of particles interacting through
forces derived from a potential, i.e. of the form ( 155 ), belongs to
this class. The trajectories of a natural system can be seen as
geodesics of a suitable Riemannian manifold. This classical result is
based on a variational formulation of dynamics. In fact Hamilton’s
principle states that the motions of a Hamiltonian system are the
extrema of the functional (Hamiltonian action @xmath )

  -- -------- -- -------
     @xmath      (156)
  -- -------- -- -------

where @xmath is the Lagrangian function of the system, and the geodesics
of a Riemannian manifold are the extrema of the length functional

  -- -------- -- -------
     @xmath      (157)
  -- -------- -- -------

where @xmath is the arc-length parameter. Once a connection between
length and action is established, by means of a suitable choice of the
metric, it will be possible to identify the geodesics with the physical
trajectories.

### xvi.1 Geometry and dynamics

Even if we restrict ourselves to the case of natural systems, the
Riemannian formulation of classical dynamics is far from unique. There
are many possible choices for the ambient space and its metric. The most
commonly known choice — dating back to the nineteenth century — is the
so-called Jacobi metric on the configuration space of the system.
Actually this was the geometric framework of Krylov’s work krilov .
There are other possibilities, for instance a metric originally
introduced by Eisenhart on an enlarged configuration space-time
eisenhart , but this will not be discussed here. The choice of the
metric to be used will be dictated mainly by convenience. These choices
certainly do not contain all the possibilities of geometrizing
conservative dynamics. For instance, with regard to systems whose
kinetic energy is not quadratic in the velocities — the classical
example is a particle subject to conservative as well as
velocity-dependent forces, such as the Lorentz force — it is impossible
to give a Riemannian geometrization, but becomes possible in the more
general framework of a Finsler geometry cipriani . However, we will not
consider this here, and restrict ourselves to standard Hamiltonian
systems.

#### xvi.1.1 The Jacobi metric tensor

Consider an autonomous dynamical system, i.e., a system with
interactions which do not explicitly depend on time, whose Lagrangian
can be written as

  -- -------- -- -------
     @xmath      (158)
  -- -------- -- -------

where the dot stands for a derivative with respect to the parameter on
which the @xmath ’s depend (such a parameter is the time @xmath here,
but could also be the arc-length @xmath ).

The Hamiltonian @xmath is an integral of motion, whose value, the energy
@xmath , is a conserved quantity. Hence Hamilton’s principle can be cast
in Maupertuis’ form arnold : the natural motions of the system are the
stationary paths in the configuration space @xmath for the functional

  -- -------- -- -------
     @xmath      (159)
  -- -------- -- -------

among all the isoenergetic curves, i.e. the curves @xmath connecting the
initial and final points parametrized so that the Hamiltonian @xmath ,
@xmath is a constant equal to the energy @xmath . The fact that the
curves must be isoenergetic with energy @xmath implies that the
accessible part of the configuration space is not the whole @xmath , but
only the subspace @xmath defined by

  -- -------- -- -------
     @xmath      (160)
  -- -------- -- -------

In fact a curve @xmath that lies outside @xmath will never be
parametrizable in such a way that the energy is @xmath , because @xmath
will then pass through points where @xmath and the kinetic energy is
positive.

The kinetic energy @xmath is a homogeneous function of degree two in the
velocities, hence Euler’s theorem implies that

  -- -------- -- -------
     @xmath      (161)
  -- -------- -- -------

and Maupertuis’ principle reads as

  -- -------- -- -------
     @xmath      (162)
  -- -------- -- -------

The configuration space @xmath of a dynamical system with @xmath degrees
of freedom has a differentiable manifold structure, and the Lagrangian
coordinates ( @xmath , . . . , @xmath ) can be regarded as local
coordinates on @xmath . The latter becomes a Riemannian manifold once a
proper metric is defined. Consider systems of the form ( 155 ) where the
kinetic energy matrix is given by @xmath . If we write

  -- -------- -- -------
     @xmath      (163)
  -- -------- -- -------

then, recalling that @xmath , equation ( 162 ) becomes

  -- -------- -- -------
     @xmath      (164)
  -- -------- -- -------

so that the motions are the geodesics of @xmath provided @xmath is the
arc-length element, i.e., the metric on @xmath is given by the tensor
whose components are just the @xmath defined in ( 163 ). This metric is
referred to as the Jacobi metric , and its arc-length element is

  -- -------- -- -------
     @xmath      (165)
  -- -------- -- -------

The geodesic equations written in the local coordinate frame ( @xmath ,
. . . , @xmath ) are

  -- -------- -- -------
     @xmath      (166)
  -- -------- -- -------

where @xmath is the covariant derivative along the curve @xmath , @xmath
is the velocity vector of the geodesic and the @xmath are the
Christoffel symbols. Using the definition of the Christoffel symbols, it
is straightforward to show that ( 166 ) becomes

  -- -------- -- -------
     @xmath      (167)
  -- -------- -- -------

whence, using ( 165 ), Newton’s equations are recovered,

  -- -------- -- -------
     @xmath      (168)
  -- -------- -- -------

Note that the Jacobi metric is obtained by a conformal change of the
kinetic energy metric @xmath . In fact the general result for the
Riemannian geometrization of natural Hamiltonian dynamics ong states
that given a dynamical system on a Riemannian manifold @xmath , @xmath ,
i.e., a dynamical system whose Lagrangian is

  -- -------- -- -------
     @xmath      (169)
  -- -------- -- -------

then it is always possible to find a conformal transformation of the
metric,

  -- -------- -- -------
     @xmath      (170)
  -- -------- -- -------

such that the geodesics of @xmath , @xmath are the trajectories of the
original dynamical system; this transformation is defined by

  -- -------- -- -------
     @xmath      (171)
  -- -------- -- -------

### xvi.2 Stability and curvature

The study of the stability of the trajectories of a dynamical system
finds a natural framework in the geometrization of the dynamics since it
links the latter with the stability of the geodesics; this stability is
completely determined by the curvature of the manifold, as shown below.

Studying the stability of the dynamics means determining the evolution
of perturbations of a given trajectory. This implies that one should
follow the evolution of the linearized (tangent) flow along the
reference trajectory. For a Newtonian system, writing the perturbed
trajectory as

  -- -------- -- -------
     @xmath      (172)
  -- -------- -- -------

substituting this expression in the equations of motion

  -- -------- -- -------
     @xmath      (173)
  -- -------- -- -------

and retaining terms up to first order in the @xmath ’s, one finds that
the perturbation obeys the so-called tangent dynamics equation which
reads as

  -- -------- -- -------
     @xmath      (174)
  -- -------- -- -------

This equation should be solved together with the dynamics in order to
determine the stability or instability of the trajectory: when the norm
of the perturbations grows exponentially, the trajectory is unstable,
otherwise it is stable. Let us now translate the stability problem into
geometric language. By writing, in close analogy to what has been done
above in the case of dynamical systems, a perturbed geodesic as

  -- -------- -- -------
     @xmath      (175)
  -- -------- -- -------

and then inserting this expression in the equation for the geodesics (
166 ), one finds that the evolution of the perturbation vector @xmath is
given by the following equation:

  -- -------- -- -------
     @xmath      (176)
  -- -------- -- -------

where @xmath are the components of the Riemann curvature tensor.
Equation ( 176 ) is referred to as the Jacobi equation, and the tangent
vector field @xmath as the Jacobi field. This equation was first studied
by Levi-Civita and is also often referred to as the equation of Jacobi
and Levi-Civita.

The remarkable fact is that the evolution of @xmath — and then the
stability or instability of the geodesic — is completely determined by
the curvature of the manifold. Therefore, if the metric is induced by a
physical system, as in the case of Jacobi or Eisenhart metrics, such an
equation links the stability or instability of the trajectories to the
curvature of the ambient manifold.

### xvi.3 Mechanical manifolds and curvature

In this subsection, we have to give explicit expressions for the
curvature of the mechanical manifolds, i.e., of those manifolds whose
Riemannian structure is induced by the dynamics via the Jacobi or the
Eisenhart metric.

We already observed that the Jacobi metric is a conformal deformation of
the kinetic-energy metric, whose components are given by the kinetic
energy matrix @xmath . In the case of systems whose kinetic energy
matrix is diagonal, this means that the Jacobi metric is conformally
flat. This greatly simplifies the computation of curvatures. It is
convenient to define then a symmetric tensor @xmath whose components are
ong

  -- -------- -- -------
     @xmath      (177)
  -- -------- -- -------

where @xmath is the potential, @xmath is the energy, and @xmath and
@xmath stand for the Euclidean gradient and norm, respectively. The
curvature of @xmath , @xmath can be expressed through @xmath . In fact,
the components of the Riemann tensor are

  -- -------- -- -------
     @xmath      (178)
  -- -------- -- -------

By contraction of the first and third indices, we obtain the Ricci
tensor, whose components are

  -- -------- -- -------
     @xmath      (179)
  -- -------- -- -------

and by a further contraction we obtain the scalar curvature

  -- -------- -- -------
     @xmath      (180)
  -- -------- -- -------

where @xmath . To summarize, the dynamical trajectories of a Hamiltonian
system of the form ( 155 ) can be seen as geodesics of the configuration
space once a suitable metric is defined. The general relationship which
holds between dynamical and geometrical quantities regardless of the
precise choice of the metric can be sketched as follows:

  -- -- -- -------
           (181)
  -- -- -- -------

Furthermore, the stability of the dynamical trajectories can be mapped
onto the stability of the geodesics, which is completely determined by
the curvature of the manifold.

### xvi.4 Integrability and Killing Vectors

In the Riemannian geometrodynamical approach to chaos (Jacobi
geometrodynamics), the strategy consists in making use of the
Hamiltonian formulation of the dynamical system and then in reducing the
dynamics to a geodesic flow. This reduction is performed at the level of
the least action principle. The most fascinating feature of this
approach is that the problem (often very complicated) of dynamics is
reduced to geometrical properties of a single object — the manifold on
which geodesic flow is induced. In the Jacobi reformulation, all of the
dynamical information is collected into a single geometric object in
which all the available manifest symmetries are retained.

For example, the sensitive dependence of trajectories on initial
conditions, which is a key ingredient of chaos, can be investigated
starting from the equation of geodesic equation. The integrability of
the system, instead, is connected with the existence of Killing vectors
and tensors on this manifold biesiada ; uggla . Consider an @xmath
-dimensional manifold @xmath with metric tensor @xmath . Any @xmath
-vector @xmath that satisfies the Killing equation

  -- -------- -- -------
     @xmath      (182)
  -- -------- -- -------

is said to form a Killing vector of the metric @xmath . The quantity
@xmath is the Lie derivative of the metric tensor @xmath along @xmath
while the semi-colon in ( 182 ) represents the standard covariant
derivative on curved manifolds. For the Lie derivative @xmath to vanish,
requires that the geometry to be unchanged as one moves in the @xmath
-direction, that is, @xmath represents a direction of symmetry of @xmath
. Killing vectors provide the first integrals of the geodesic equation .
Killing equations in general are enormously difficult to solve.
Construction of Killing vectors is a relatively simple task in the case
of conformally flat spaces (which is the case in the majority of
mechanical problems). For instance, a compact manifold @xmath with
negative Ricci curvatures has no nontrivial Killing vector field
(Theorem of Bochner, frankel ). As a simple example, consider the
Poincaré upper half plane @xmath with the Poincaré line element @xmath .
Since the metric coefficients are independent of @xmath , @xmath is a
Killing vector field. The vector @xmath has a length @xmath that tends
to infinity as we approach the @xmath -axis @xmath .

An @xmath -dimensional manifold @xmath is said to be maximally symmetric
if it has @xmath Killing vectors. The most familiar examples of
maximally symmetric spaces of Euclidean signatures are the @xmath
-dimensional Euclidean space @xmath and the @xmath -dimensional spheres
@xmath .  For Euclidean signatures, the flat maximally symmetric spaces
are planes or appropriate higher-dimensional generalizations, while the
positively curved ones are spheres. Maximally symmetric Euclidean spaces
of negative curvature are hyperboloids. There are, of course, maximally
symmetric manifolds with Lorentzian signatures. The maximally symmetric
spacetime with @xmath is simply Minkowski space. The positively curved
maximally symmetric spacetime is called de Sitter space, while that with
negative curvature is labeled anti-de Sitter space. In particular, any
space with vanishing curvature tensor is maximally symmetric; the
converse, however, is not true. If a manifold is maximally symmetric,
the curvature is the same everywhere and the same in every direction.
Hence, if we know the curvature of a maximally symmetric space at one
point, we know it everywhere. Indeed, there are only a small number of
possible maximally symmetric spaces; they are classified by scalar
curvature @xmath (which will be constant everywhere), the dimensionality
@xmath , the metric signature, and perhaps some discrete pieces of
information relating to the global topology.

In any maximally symmetric space @xmath , at any point, in any
coordinate system,

  -- -------- -- -------
     @xmath      (183)
  -- -------- -- -------

It may be useful to introduce the Weyl projective curvature tensor
@xmath defined as,

  -- -- -- -------
           (184)
  -- -- -- -------

The Weyl projective curvature tensor should not to be confused with
Weyl’s conformal curvature tensor de felice . Weyl’s projective tensor
@xmath measures the deviation from isotropy of a given manifold. For an
isotropic manifold @xmath . As a final remark, we emphasize that for a
maximally symmetric manifold (isotropic manifold), the following
relations among the scalar curvature @xmath , Ricci curvature tensor
@xmath and Gaussian scalar curvature @xmath follow de felice ,

  -- -------- -- -------
     @xmath      (185)
  -- -------- -- -------

and,

  -- -------- -- -------
     @xmath      (186)
  -- -------- -- -------

The quantities @xmath in ( 185 ) are the sectional curvatures of planes
spanned by pairs of orthonormal basis elements. For two arbitrary
vectors @xmath and @xmath , the sectional curvature @xmath is defined as
weinberg ,

  -- -------- -- -------
     @xmath      (187)
  -- -------- -- -------

## Xvii Lyapunov exponents and the Kolmogorov-Sinai entropy

In strict mathematical terms, chaotic motion is defined in terms of the
long-term exponential divergence of neighboring trajectories in phase
space. Neighboring orbits of integrable systems, on the other hand, are
either performing stable oscillations around each other or diverge at
most as a finite power of time.

The rate of exponential divergence is quantitatively measured by the
positive Lyapunov exponents @xmath . These exponents can be introduced
in the context of general dynamical systems governed by the first-order
differential equations,

  -- -------- -- -------
     @xmath      (188)
  -- -------- -- -------

Given a solution @xmath of ( 188 ), we can linearize the equations of
motion around this reference orbit and obtain a set of linear
differential equations for the deviations @xmath :

  -- -------- -- -------
     @xmath      (189)
  -- -------- -- -------

The length of the vector @xmath ,

  -- -------- -- -------
     @xmath      (190)
  -- -------- -- -------

provides a measure of the divergence of the two neighboring trajectories
@xmath and @xmath . The maximal Lyapunov exponent @xmath is defined as
the long-time average of its logarithmic growth rate:

  -- -- -- -------
           (191)
  -- -- -- -------

The double limit is required because the accessible phase space is
usually bounded, and hence @xmath cannot continue to grow forever, given
a fixed initial distance @xmath . Regions of phase space for which
@xmath exhibit sensitive dependence on the initial conditions. An
infinitesimal change in the initial data results in macroscopic
deviations after a sufficiently long time:

  -- -------- -- -------
     @xmath      (192)
  -- -------- -- -------

The exponential instability of motion means positive maximal Lyapunov
exponent @xmath .

An attractor is a subset of the manifold @xmath toward which almost all
sufficiently close trajectories converge asymptotically, covering it
densely as the time goes on tel . Strange attractors are called chaotic
attractors . Chaotic attractors have at least one finite positive
Lyapunov exponent. On the other hand, random (noisy) attractors have an
infinite positive Lyapunov exponent, as no correlation exists between
one point on the trajectory and the next (no matter how close they are).

If we calculate the Lyapunov exponent for orthogonal directions of
maximum divergence in phase space, we obtain a set of Lyapunov exponents
@xmath ,…., @xmath where @xmath is the dimension of the phase space.
This set of Lyapunov exponents is known as the Lyapunov spectrum and is
usually ordered from the largest positive Lyapunov exponent @xmath ,
down to the largest negative exponent, @xmath , i. e. maximum divergence
to maximum convergence.

The reason why the exponentially unstable motion is called chaotic is
that almost all trajectories are unpredictable in the following sense:
according to the Alekseev-Brudno theorem alekseev in the algorithmic
theory of dynamical systems, the information @xmath associated with a
segment of a trajectory of length @xmath is equal asymptotically to
pesin

  -- -------- -- -------
     @xmath      (193)
  -- -------- -- -------

where @xmath is the sum of all positive Lyapunov exponents and @xmath is
the so-called Kolmogorov-Sinai dynamical entropy (KS entropy; indeed,
@xmath is not really an entropy but an entropy per unit time, or an
”entropy rate”), a statistical indicator of chaos . The quantity @xmath
in ( 193 ) is formally called the Kolmogorov algorithmic complexity
kolmogorov-c and, in other words, we may say that the Alekseev-Brudno
theorem states that ” the KS entropy measures the algorithmic complexity
of classical trajectories ” benattaman . In computer science, the
Kolmogorov complexity @xmath of a string @xmath with respect to a
universal computer @xmath (Turing machine) is defined as benattaman ,

  -- -------- -- -------
     @xmath      (194)
  -- -------- -- -------

and it represents the minimum length over all binary programs @xmath
that print @xmath and halt. Thus, @xmath is the shortest description
length of @xmath over all descriptions interpreted by the computer
@xmath . Equation ( 193 ) shows that in order to predict each new
segment of a chaotic trajectory, one needs an additional information
proportional to the length of this segment and independent of the full
previous length of trajectory (trajectories correspond to infinitely
long strings @xmath ). This means that this information cannot be
extracted from observation of the previous motion, even an infinitely
long one! If the instability is not exponential but, for example, only a
power law, then the required information per unit time is inversely
proportional to the full previous length of the trajectory and,
asymptotically, the prediction becomes possible. The important condition
@xmath , which characterizes chaotic motion, is not invariant with
respect to the change of time variable. All Lyapunov exponents of an
integrable system are zero. Conversely, the existence of a single
positive Lyapunov exponent demonstrates the nonintegrability of a
dynamical system.

Chaos is characterized by the positivity of at least one Lyapunov
exponent of the system and therefore the significance of the concept of
chaos depends essentially on the invariance of the Lyapunov exponents.
It is known that under Lorentz transformations, @xmath and @xmath
change, but their positivity is preserved for chaotic systems zheng .
Under Rindler transformations birrell , @xmath and @xmath change in such
a way that systems, which are chaotic for an accelerated Rindler
observer, can be nonchaotic for an inertial Minkowski observer.
Therefore, the concept of chaos is observer-dependent zheng .
Furthermore, a Lyapunov exponent (which is a ”per time” measure of
having exponential separation of nearby trajectories in ”time”) is not
invariant under transformations of the ”time” coordinate! The fact that
the Lyapunov exponents are strongly gauge dependent quantities leads to
additional problems connected with the characterization of chaos,
especially in general relativity (where a gauge invariant measure of
chaoticity is still missing). Therefore, the risk of inventing chaotic
solutions in an artificial way is present and, because of that, extra
care is needed in order to characterize ”true chaos” rugh-le .

Chapter 5: Curvature, entropy and Jacobi fields

In this Chapter, we study chaos in the context of two models the
dynamics of which is entropic dynamics on curved statistical manifolds.
Two chaotic entropic dynamical models are considered. The geometric
structure of the statistical manifolds underlying these models is
studied. It is found that in both cases, the resulting metric manifolds
are negatively curved. Moreover, the geodesics on each manifold are
described by hyperbolic trajectories. A detailed analysis based on the
Jacobi-Levi-Civita equation for geodesic spread (JLC equation) is used
to show that the hyperbolicity of the manifolds leads to chaotic
exponential instability. A comparison between the two models leads to a
relation among scalar curvature of the manifold ( @xmath ), Jacobi field
intensity ( @xmath ) and information geometrodynamical entropy (IGE,
@xmath ). The IGE is a convenient new tool we introduce to study chaotic
entropic dynamics. We propose the IGE entropy as a new measure of
chaoticity. These three quantities, @xmath , @xmath , and @xmath are
suggested as useful indicators of chaoticity on curved statistical
manifolds. Indeed, in analogy to the Zurek-Paz quantum chaos criterion
(in its classical reversible limit), a classical information-geometric
chaos criterion of linear IGE growth is suggested.

## Xviii Introduction

Entropic Dynamics (ED) caticha-ED is a theoretical framework constructed
on statistical manifolds to explore the possibility that the laws of
physics, either classical or quantum, might be laws of inference rather
than laws of nature. It is known that thermodynamics can be obtained by
means of statistical mechanics which can be considered a form of
statistical inference jaynes rather than a pure ”physical” theory.
Indeed, even some features of quantum physics can be derived from
principles of inference caticha-pla . Recent research considers the
possibility that Einstein’s theory of gravity is derivable from general
principles of inductive inference caticha-piombino . Unfortunately, the
search for the correct variables that encode relevant information about
a system is a major obstacle in the description and understanding of its
evolution. The manner in which relevant variables are selected is not
straightforward. This selection is made, in most cases, on the basis of
intuition guided by experiment. The Maximum relative Entropy (ME) method
caticha-giffin is used to construct ED models. The ME method is designed
to be a tool of inductive inference. It is used for updating from a
prior to a posterior probability distribution when new information in
the form of constraints becomes available. We use known techniques
caticha-ED to show that this principle leads to equations that are
analogous to equations of motion. Information is processed using ME
methods in the framework of Information Geometry (IG) amari that is,
Riemannian geometry applied to probability theory. In our approach,
probability theory is a form of generalized logic of plausible
inference. It should apply in principle, to any situation where we lack
sufficient information to permit deductive reasoning.

In this Chapter, we focus on two special entropic dynamical models. In
the first model @xmath , we consider an hypothetical system whose
microstates span a @xmath space labelled by the variables @xmath and
@xmath . We assume that the only testable information pertaining to the
quantities @xmath and @xmath consists of the expectation values @xmath
and the variance @xmath . In the second model @xmath , we consider a
@xmath space of microstates labelled by the variables @xmath and @xmath
. In in this case, we assume that the only testable information
pertaining to the quantities @xmath and @xmath consist of the
expectation values @xmath and of the variances @xmath and @xmath . Our
models may be extended to more elaborate systems (highly constrained
dynamics) where higher dimensions are considered. However, for the sake
of clarity, we restrict our considerations to the above relatively
simple cases. Given two known boundary macrostates, we investigate the
possible trajectories of systems on the manifolds. The geometric
structure of the manifolds underlying the models is studied. The metric
tensor, Christoffel connections coefficients, Ricci and Riemann
curvature tensors are calculated in both cases and it is shown that in
both cases the dynamics takes place on negatively curved manifolds. The
geodesics of the dynamical models are hyperbolic trajectories on the
manifolds. A detailed study of the stability of such geodesics is
presented using the equation of geodesic deviation (Jacobi equation).
The notion of statistical volume elements is introduced to investigate
the asymptotic behavior of a one-parameter family of neighboring
geodesics. It is shown that the behavior of geodesics on such manifolds
is characterized by exponential instability that leads to chaotic
scenarios on the manifolds. These conclusions are supported by the
asymptotic behavior of the Jacobi vector field intensity. Finally, a
relation among entropy-like quantities, instability and curvature in the
two models is presented.

## Xix Curved Statistical Manifolds

In the case of ED1, a measure of distinguishability among the states of
the system is achieved by assigning a probability distribution @xmath to
each state defined by expected values @xmath , @xmath , @xmath of the
variables @xmath , @xmath and @xmath . In the case of ED2, one assigns a
probability distribution @xmath to each state defined by expected values
@xmath , @xmath , @xmath , @xmath of the variables @xmath , @xmath ,
@xmath and @xmath . The process of assigning a probability distribution
to each state provides the statistical manifolds of the ED models with a
metric structure. Specifically, the Fisher-Rao information metric fisher
defined in ( 201 ) is used to quantify the distinguishability of
probability distributions @xmath that live on the manifold (the family
of distributions @xmath is as a manifold, each distribution @xmath is a
point with coordinates @xmath where @xmath labels the macrovariables).
As such, the Fisher-Rao metric assigns an IG to the space of states.

### xix.1 The Statistical Manifold @xmath

Consider a hypothetical physical system evolving over a two-dimensional
space. The variables @xmath and @xmath label the @xmath space of
microstates of the system. We assume that all information relevant to
the dynamical evolution of the system is contained in the probability
distributions. For this reason, no other information (such as external
fields) is required.  We assume that the only testable information
pertaining to the quantities @xmath and @xmath consists of the
expectation values @xmath and the variance @xmath . Therefore, these
 three expected values define the @xmath space of macrostates @xmath of
the ED1 model. Each macrostate may be thought as a point of a
three-dimensional statistical manifold with coordinates given by the
numerical values of the expectations @xmath , @xmath , @xmath . The
available information can be written in the form of the following
constraint equations,

  -- -------- -- -------
     @xmath      (195)
  -- -------- -- -------

where @xmath , @xmath and @xmath . The probability distributions @xmath
and @xmath are constrained by the conditions of normalization,

  -- -------- -- -------
     @xmath      (196)
  -- -------- -- -------

Information theory identifies the exponential distribution as the
maximum entropy distribution if only the expectation value is known. The
Gaussian distribution is identified as the maximum entropy distribution
if only the expectation value and the variance are known (see the simple
example presented in Chapter 2). ME methods allow us to associate a
probability distribution @xmath to each point in the space of states.
The distribution that best reflects the information contained in the
prior distribution @xmath updated by the constraints @xmath , @xmath ,
@xmath is obtained by maximizing the relative entropy

  -- -------- -- -------
     @xmath      (197)
  -- -------- -- -------

where @xmath is the uniform prior probability distribution. The prior
@xmath is set to be uniform since we assume the lack of initial
available information about the system (postulate of equal a priori
probabilities). Upon maximizing ( 197 ), given the constraints ( 195 )
and ( 196 ), we obtain

  -- -------- -- -------
     @xmath      (198)
  -- -------- -- -------

where @xmath , @xmath and @xmath . The probability distribution ( 198 )
encodes the available information concerning the system and @xmath
becomes,

  -- -------- -- -------
     @xmath      (199)
  -- -------- -- -------

where @xmath and @xmath . Note that we have assumed uncoupled
constraints between the microvariables @xmath and @xmath . In other
words, we assumed that information about correlations between the
microvariables did not need to be tracked. This assumption leads to the
simplified product rule in ( 198 ). Coupled constraints however, would
lead to a generalized product rule in ( 198 ) and to a metric tensor (
201 ) with non-trivial off-diagonal elements (covariance terms).
Correlation terms may be fictitious. They may arise for instance from
coordinate transformations. On the other hand, correlations may arise
from interaction of the system with external fields. Such scenarios
would require more delicate analysis.

#### xix.1.1 The Metric Tensor on @xmath

We cannot determine the evolution of microstates of the system since the
available information is insufficient. Instead we can study the distance
between two total distributions with parameters @xmath , @xmath , @xmath
and @xmath , @xmath , @xmath . Once the states of the system have been
defined, the next step concerns the problem of quantifying the notion of
change in going from the state @xmath to the state @xmath . For our
purpose a convenient measure of change is distance. The measure we seek
is given by the dimensionless ”distance” @xmath between @xmath and
@xmath :

  -- -------- -- -------
     @xmath      (200)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (201)
  -- -------- -- -------

is the Fisher-Rao information metric. Substituting ( 198 ) into ( 201 )
, the metric @xmath on @xmath becomes,

  -- -------- -- -------
     @xmath      (202)
  -- -------- -- -------

Substituting ( 202 ) into ( 200 ), the ”length” element reads,

  -- -------- -- -------
     @xmath      (203)
  -- -------- -- -------

Notice that the metric structure of @xmath is an emergent structure and
is not itself fundamental. It arises only after assigning a probability
distribution @xmath to each state @xmath .

#### xix.1.2 The Curvature of @xmath

In this paragraph we calculate the statistical curvature @xmath . This
is achieved via application of standard differential geometric methods
to the space of probability distributions @xmath . Recall the
definitions of the Ricci tensor @xmath and Riemann curvature tensor
@xmath ,

  -- -------- -- -------
     @xmath      (204)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (205)
  -- -------- -- -------

The Ricci scalar @xmath is obtained from ( 204 ) or ( 205 ) via
appropriate contraction with the metric tensor @xmath in ( 202 ), namely

  -- -------- -- -------
     @xmath      (206)
  -- -------- -- -------

where @xmath so that @xmath diag @xmath , @xmath , @xmath . The
Christoffel symbols @xmath appearing in ( 204 ) and ( 205 ) are defined
by,

  -- -------- -- -------
     @xmath      (207)
  -- -------- -- -------

Substituting ( 202 ) into ( 207 ), we calculate the non-vanishing
components of the connection coefficients,

  -- -------- -- -------
     @xmath      (208)
  -- -------- -- -------

By substituting ( 208 ) in ( 204 ) we determine the Ricci tensor
components,

  -- -------- -- -------
     @xmath      (209)
  -- -------- -- -------

The non-vanishing Riemann tensor component is,

  -- -------- -- -------
     @xmath      (210)
  -- -------- -- -------

Finally, by substituting ( 209 ) or ( 210 ) into ( 206 ) and using
@xmath we obtain the Ricci scalar,

  -- -------- -- -------
     @xmath      (211)
  -- -------- -- -------

From ( 211 ) we conclude that @xmath is a manifold of constant negative
@xmath curvature. We remark that the scalar curvature of @xmath arises
from the presence of the Gaussian distribution. Instead, the exponential
distribution does not contribute to @xmath .

### xix.2 The Statistical Manifold @xmath

In this case we assume that the @xmath space of microstates of the
system is labelled by the variables @xmath and @xmath . We assume, as in
subsection @xmath , that all information relevant to the dynamical
evolution of the system is contained in the probability distributions.
Moreover, we assume that the only testable information pertaining to the
quantities @xmath and @xmath consist of the expectation values @xmath
and of the variances @xmath and @xmath . Therefore, these four expected
values define the @xmath space of macrostates @xmath of the ED2 model.
Each macrostate may be thought as a point of a four-dimensional
statistical manifold with coordinates given by the numerical values of
the expectations @xmath , @xmath , @xmath , @xmath . We emphasize the
fact that entropic dynamic is not defined on the space of microstates
but on the space of macrostates. The available information can be
written in the form of the following constraint equations,

  -- -- -- -------
           (212)
  -- -- -- -------

where @xmath , @xmath , @xmath and @xmath . The probability
distributions @xmath and @xmath are constrained by the conditions of
normalization,

  -- -------- -- -------
     @xmath      (213)
  -- -------- -- -------

The distribution that best reflects the information contained in the
uniform prior distribution @xmath updated by the constraints @xmath ,
@xmath , @xmath , @xmath is obtained by maximizing the relative entropy

  -- -------- -- -------
     @xmath      (214)
  -- -------- -- -------

Upon maximizing ( 214 ), given the constraints ( 212 ) and ( 213 ), we
obtain

  -- -------- -- -------
     @xmath      (215)
  -- -------- -- -------

The probability distribution ( 215 ) encodes the available information
concerning the system and @xmath becomes,

  -- -------- -- -------
     @xmath      (216)
  -- -------- -- -------

where @xmath and @xmath .

#### xix.2.1 The Metric Tensor on @xmath

Proceeding as in @xmath , we determine the metric on @xmath .
Substituting ( 215 ) into ( 201 ), the metric @xmath on @xmath becomes,

  -- -------- -- -------
     @xmath      (217)
  -- -------- -- -------

Finally, substituting ( 217 ) into ( 200 ), the ”length” element reads,

  -- -------- -- -------
     @xmath      (218)
  -- -------- -- -------

#### xix.2.2 The Curvature of @xmath

Proceeding as in @xmath , we calculate the statistical curvature @xmath
of @xmath . Notice that @xmath diag @xmath , @xmath , @xmath , @xmath .
Substituting ( 217 ) into ( 207 ), the non-vanishing components of the
connection coefficients become,

  -- -------- -- -------
     @xmath      (219)
  -- -------- -- -------

By substituting ( 219 ) in ( 204 ) we determine the Ricci tensor
components,

  -- -------- -- -------
     @xmath      (220)
  -- -------- -- -------

The non-vanishing Riemann tensor components are,

  -- -------- -- -------
     @xmath      (221)
  -- -------- -- -------

Finally, by substituting ( 220 ) or ( 221 ) into ( 206 ) and using
@xmath , we obtain the Ricci scalar,

  -- -------- -- -------
     @xmath      (222)
  -- -------- -- -------

From ( 222 ) we conclude that @xmath is a manifold of uncoupled Gaussian
probability distributions of constant negative @xmath curvature.

## Xx The ED Models

The dynamics can be derived from a standard principle of least action
(Maupertuis- Euler-Lagrange-Jacobi-type) caticha-ED ; arnold . The main
differences are that the dynamics being considered here are defined on a
space of probability distributions @xmath , not on an ordinary linear
space @xmath . Also, the standard coordinates @xmath of the system are
replaced by statistical macrovariables @xmath .

Given the initial macrostate and that the system evolves to a fixed
final macrostate, we investigate the expected trajectories of the ED
models on @xmath and @xmath . The classical dynamics of a particle can
be derived from the principle of least action in the
Maupertuis-Euler-Lagrange-Jacobi form arnold ,

  -- -------- -- -------
     @xmath      (223)
  -- -------- -- -------

where @xmath are the coordinates of the system, @xmath is an affine
parameter along the trajectory and @xmath is a functional defined as

  -- -------- -- -------
     @xmath      (224)
  -- -------- -- -------

For a non-relativistic system, the energy @xmath is,

  -- -------- -- -------
     @xmath      (225)
  -- -------- -- -------

where the coefficients @xmath are the reduced mass matrix coefficients
and @xmath is the time derivative of the canonical coordinate @xmath .
We now seek the expected trajectory of the system assuming it evolves
from @xmath @xmath to @xmath . Such a system moves along a geodesic in
the space of states, which is a curved manifold with the appropriately
chosen metric caticha-ED . Since the trajectory of the system is a
geodesic, the ED-action is itself the length; that is,

  -- -------- -- -------
     @xmath      (226)
  -- -------- -- -------

where @xmath and @xmath , @xmath is the Lagrangian of the system,

  -- -------- -- -------
     @xmath      (227)
  -- -------- -- -------

A useful choice for @xmath is one satisfying the condition @xmath .
Therefore, we formally identify the affine parameter @xmath with the
temporal evolution parameter @xmath , @xmath . Performing a standard
calculus of variations with @xmath , we obtain

  -- -------- -- -------
     @xmath      (228)
  -- -------- -- -------

Note that from ( 228 ), @xmath @xmath . This differential equation shows
that if @xmath for a particular @xmath then the corresponding @xmath is
conserved. This suggests to interpret @xmath as momenta. Equations ( 228
) and ( 207 ) lead to the geodesic equation,

  -- -------- -- -------
     @xmath      (229)
  -- -------- -- -------

Observe that ( 229 ) are nonlinear , second order coupled ordinary
differential equations. These equations describe a dynamics that is
reversible and their solution is the trajectory between an initial and a
final macrostate. The trajectory can be equally well traversed in both
directions.

### xx.1 Model ED1: Geodesics on @xmath

We seek the explicit form of ( 229 ) for ED1. Substituting ( 208 ) in (
229 ), we obtain,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (230)
  -- -------- -- -------

Integrating this set of differential equations, we obtain

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (231)
  -- -------- -- -------

The integration constants arising from the exponential contribution to
the geodesic equations are @xmath and @xmath with @xmath . The
integration constants arising from the Gaussian contribution to the
geodesic equations are @xmath , @xmath , @xmath and @xmath . In total,
there are the six real integration constants since there are six initial
conditions: three for the initial values of the macrovariables labelling
points on @xmath and three for the initial values of the first
derivative of the macrovariables. However, the normalization constraint
@xmath leads to five independent initial conditions.

Note that the set of equations ( 231 ) parametrizes the evolution
surface of  the statistical submanifold @xmath of @xmath ,

  -- -------- -- -------
     @xmath      (232)
  -- -------- -- -------

The set of points in @xmath are the expected intermediate macrostates of
the system in its evolution from a given initial macrostate @xmath to a
given final macrostate @xmath .  In other words, the measure of @xmath
describes the portion of statistical volume in configuration space
accessed by the system in its information-constrained evolution between
two given points of the manifold @xmath . A different set of initial
conditions would lead to consider a different submanifold @xmath ,
@xmath .

### xx.2 Model ED2: Geodesics on @xmath

We seek the explicit form of ( 229 ) for ED2. Substituting ( 219 ) in (
229 ), we obtain,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (233)
  -- -------- -- -------

Integrating this set of differential equations, we obtain

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (234)
  -- -------- -- -------

The eight integration constants @xmath , @xmath , @xmath , @xmath ,
@xmath , @xmath , @xmath and @xmath assume real values and they are not
independent from each other. They satisfy the normalization condition
@xmath . Furthermore, they are functions of the initial values of the
macrovariable @xmath and of its first derivative @xmath .

Again, note that the set of equations ( 234 ) parametrizes the evolution
surface of  the statistical submanifold @xmath of @xmath ,

  -- -------- -- -------
     @xmath      (235)
  -- -------- -- -------

The set of points in @xmath are the expected intermediate macrostates of
the system in its evolution from a given initial macrostate @xmath to a
given final macrostate @xmath .  In other words, the measure of @xmath
describes the portion of statistical volume in configuration space
accessed by the system in its information-constrained evolution between
two given points of the manifold @xmath . A different set of initial
conditions would lead to consider a different submanifold @xmath ,
@xmath .

## Xxi Chaotic Instability in the ED Models

The Riemannian curvature of a manifold is closely connected with the
behavior of the geodesics on it, i.e., with the motion of the
corresponding dynamical system arnold . If the Riemannian curvature of a
manifold is positive (as on a sphere or ellipsoid), then the nearby
geodesics oscillate about one another in most cases; whereas if the
curvature is negative (as on the surface of a hyperboloid of one sheet),
geodesics rapidly diverge from one another.

### xxi.1 Instability in ED1

In this subsection, the stability of ED1 is considered. It is shown that
neighboring trajectories are exponentially unstable under small
perturbations of initial conditions. In the rest of the Chapter, we only
assume that in ED1 we have @xmath ; in ED2, we assume we have @xmath .
We make this choice because we are selecting the quantity ” @xmath ” as
the common parameter (the one-parameter characterizing families of
geodesics on configuration spaces) labelling different geodesics on
manifolds @xmath and @xmath and our objective is to compare the degree
of chaoticity of both dynamics (ED1 and ED2) on their respective
underlying curved statistical manifolds.

At this stage of our discussion, the parameter ” @xmath ” is a quantity
extracted from the continuous entropic dynamical evolution equations (
231 ) and ( 234 ). Of course, a different set of initial conditions
would lead to a different parameter @xmath . Once the choice is made, we
assume that @xmath is roughly constant over accessible region of
configuration space. This assumption would not be new in the literature,
see reference cavicchio for example. In the following sections, we will
discover that @xmath plays the role of a standard Lyapunov exponent.

We could show that @xmath does play the role of a Lyapunov exponent
already at this stage of our discussion. Lyapunov exponents are
asymptotic quantities: they are defined in the limit as time approaches
infinity. The finite Lyapunov exponent in the direction @xmath of a
trajectory @xmath satisfying the differential equation @xmath with
@xmath and with initial condition @xmath is defined as lyapunov ,

  -- -------- -- -------
     @xmath      (236)
  -- -------- -- -------

The brackets @xmath in ( 236 ) denote the standard scalar product in
@xmath , @xmath is the asymptotically regular fundamental matrix of the
differential equation @xmath verhulst . In the ED1 model, the
differential equation to consider is

  -- -------- -- -------
     @xmath      (237)
  -- -------- -- -------

where @xmath are given in ( 231 ). In the asymptotic limit, the @xmath
matrix @xmath can be approximated by a diagonal matrix with constant
coefficients, @xmath diag @xmath . A straightforward calculation would
lead to an asymptotically regular @xmath fundamental matrix

  -- -------- -- -------
     @xmath      (238)
  -- -------- -- -------

with @xmath , @xmath , @xmath , @xmath . Therefore, equation ( 236 )
would lead to the following interesting result

  -- -------- -- -------
     @xmath      (239)
  -- -------- -- -------

We have shown that, under our assumptions, the leading Lyapunov exponent
@xmath is given by @xmath .

#### xxi.1.1 The Geodesic Length @xmath

Consider the one-parameter family of geodesics @xmath where @xmath are
solutions of ( 230 ), the ”selector parameter” @xmath tells which
geodesic is being considered and the affine parameter @xmath tells where
is the point being considered on a given geodesic. The length of
geodesics in @xmath is defined as,

  -- -------- -- -------
     @xmath      (240)
  -- -------- -- -------

where @xmath is given in ( 202 ). Substituting ( 231 ) in ( 240 ) and
considering the asymptotic expression of @xmath , we obtain

  -- -------- -- -------
     @xmath      (241)
  -- -------- -- -------

In evaluating ( 240 ), we have not imposed the conventional
normalization condition @xmath . Indeed, in our case @xmath constant;
what matters is that we will use this very same normalization constant
in evaluating the length of geodesics in @xmath so that we may compare
both lengths using the same ”meter” (statistical affine parameter @xmath
).

In order to roughly investigate the asymptotic behavior of two
neighboring geodesics labelled by the parameters @xmath and @xmath , we
consider the following difference,

  -- -------- -- -------
     @xmath      (242)
  -- -------- -- -------

It is clear that @xmath diverges, that is, the lengths of two
neighboring geodesics with slightly different parameters @xmath and
@xmath differ in a remarkable way as the evolution parameter @xmath
@xmath . This hints at the onset of instability of the hyperbolic
trajectories on @xmath .

#### xxi.1.2 Evolution of Volumes @xmath on the Statistical Manifold
@xmath

The instability of ED1 can be further explored by studying the behavior
of the one-parameter family of statistical volume elements @xmath .
Recall that @xmath is the space of probability distributions @xmath
labeled by parameters @xmath , @xmath , @xmath . These parameters are
the coordinates of the point @xmath , and in these coordinates a @xmath
volume element @xmath reads

  -- -------- -- -------
     @xmath      (243)
  -- -------- -- -------

where in the ED1 model here presented, @xmath . Hence, the volume
element @xmath is given by,

  -- -------- -- -------
     @xmath      (244)
  -- -------- -- -------

The volume increase of an extended region of @xmath is given by,

  -- -------- -- -------
     @xmath      (245)
  -- -------- -- -------

Integrating ( 245 ) using ( 231 ), we obtain

  -- -------- -- -------
     @xmath      (246)
  -- -------- -- -------

The quantity that actually encodes relevant information about the
stability of neighboring volume elements is the average volume @xmath ,

  -- -- -- -------
           (247)
  -- -- -- -------

For convenience, let us rename @xmath . Therefore, the asymptotic
expansion of @xmath for @xmath @xmath reads,

  -- -------- -- -------
     @xmath      (248)
  -- -------- -- -------

This asymptotic evolution in ( 248 ) describes the exponential increase
of average volume elements on @xmath . The exponential instability
characteristic of chaos forces the system to rapidly explore large areas
(volumes) of the statistical manifolds. It is interesting to note that
this asymptotic behavior appears also in the conventional description of
quantum chaos zurek-paz where the entropy increases linearly at a rate
determined by the Lyapunov exponents ruelle . The linear entropy
increase as a quantum chaos criterion was introduced by Zurek and Paz.
In our information-geometric approach a relevant variable that will be
useful for comparison of the two different degrees of instability
characterizing the two ED models is the relative entropy-like quantity
defined as,

  -- -------- -- -------
     @xmath      (249)
  -- -------- -- -------

Substituting ( 248 ) in ( 249 ) and considering the asymptotic limit
@xmath , we obtain

  -- -------- -- -------
     @xmath      (250)
  -- -------- -- -------

Notice that the late-time limit @xmath is necessary to describe chaos
bhattacharya . Furthermore, the study of the asymptotic behavior of
average quantities is very common in chaotic dynamics cavicchio ;
bhattacharya . The entropy-like quantity @xmath in ( 250 ) may be
interpreted as the asymptotic limit of the natural logarithm of a
statistical weight @xmath . defined on @xmath . Equation ( 250 ) is the
information-geometric analog of the Zurek-Paz chaos criterion. As a
final remark, we would like to emphasize that the connection between
@xmath and @xmath may be compared to the connection between the KS
entropy and the ”physical” entropy (the entropy of the second law of
thermodynamics) latora ; the steps we used to construct the IGE resemble
the ones that Toda and Ikeda used to propose a quantal Lyapunov exponent
ikeda .

#### xxi.1.3 The Jacobi Vector Field @xmath

We study the behavior of the one-parameter family of neighboring
geodesics @xmath where,

  -- -------- -------- -- -------
     @xmath   @xmath      (251)
                          
     @xmath   @xmath      
  -- -------- -------- -- -------

The relative geodesic spread is characterized by the Jacobi equation de
felice ; mtw ,

  -- -------- -- -------
     @xmath      (252)
  -- -------- -- -------

where @xmath , @xmath , @xmath and,

  -- -------- -- -------
     @xmath      (253)
  -- -------- -- -------

Equation ( 252 ) forms a system of three coupled ordinary differential
equations linear in the components of the deviation vector field ( 253 )
but nonlinear in derivatives of the metric ( 201 ). It describes the
linearized geodesic flow: the linearization ignores the relative
velocity of the geodesics. When the geodesics are neighboring but their
relative velocity is arbitrary, the corresponding geodesic deviation
equation is the so-called generalized Jacobi equation chicone . The
nonlinearity is due to the existence of velocity-dependent terms in the
system.

Neighboring geodesics accelerate relative to each other with a rate
directly measured by the curvature tensor @xmath . The Riemann tensor
is, within this information-geometric framework, an indicator of the
strength of ”statistical tidal forces”. Multiplying both sides of ( 252
) by @xmath and using the standard symmetry properties of the Riemann
curvature tensor, the geodesic deviation equation becomes,

  -- -------- -- -------
     @xmath      (254)
  -- -------- -- -------

Recall that the covariant derivative @xmath in ( 252 ) is defined as,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (255)
  -- -------- -------- -- -------

and that the only non-vanishing Riemann tensor component is @xmath .
Therefore, the three differential equations for the geodesic deviation
are,

  -- -------- -- -------
     @xmath      (256)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      
     @xmath      (257)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      
     @xmath      (258)
  -- -------- -- -------

Substituting ( 208 ), ( 210 ) and ( 251 ) in equations ( 256 ), ( 257 )
and ( 258 ) and considering the asymptotic limit @xmath , the geodesic
deviation equations become,

  -- -------- -- -------
     @xmath      (259)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (260)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (261)
  -- -------- -- -------

Neglecting the exponentially decaying terms in @xmath in ( 260 ) and (
261 ) and assuming that,

  -- -------- -- -------
     @xmath      (262)
  -- -------- -- -------

the geodesic deviation equations finally become,

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (263)
  -- -------- -- -------

Note that in order to prove that our assumptions in ( 262 ) are correct,
we will check a posteriori its consistency. Integrating the system of
differential equations ( 263 ), we obtain

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- -- -------
     @xmath      (264)
  -- -------- -- -------

where @xmath , @xmath ,…, @xmath are integration constants. Note that
conditions ( 262 ) are satisfied and therefore our assumption are
compatible with the solutions obtained. Finally, consider the vector
field components @xmath defined in ( 253 ) and its magnitude @xmath ,

  -- -------- -- -------
     @xmath      (265)
  -- -------- -- -------

The magnitude @xmath is called the Jacobi field intensity. In our case (
265 ) becomes,

  -- -------- -- -------
     @xmath      (266)
  -- -------- -- -------

Substituting ( 251 ) and ( 264 ) in ( 266 ), and keeping the leading
term in the asymptotic expansion in @xmath , we obtain

  -- -------- -- -------
     @xmath      (267)
  -- -------- -- -------

where the constant coefficient @xmath @xmath encodes information about
initial conditions and depends on the model parameter @xmath . We
conclude that the geodesic spread on @xmath is described by means of an
exponentially divergent Jacobi vector field intensity @xmath . It is
known that classical chaotic systems exhibit exponential sensitivity to
initial conditions. This characterization, quantified in terms of
Lyapunov exponents, is an important ingredient in any conventional
definition of classical chaos. In our approach, the quantity @xmath with
@xmath given in ( 267 ) would play the role of the conventional Lyapunov
exponents.

### xxi.2 Instability in ED2

In this subsection, the instability of the geodesics on @xmath is
studied. We proceed as in section @xmath .

#### xxi.2.1 The Geodesic Length @xmath

Consider the one-parameter family of geodesics @xmath where @xmath are
solutions of ( 233 ). The length of geodesics in @xmath is defined as,

  -- -------- -- -------
     @xmath      (268)
  -- -------- -- -------

where @xmath defined in ( 217 ). Substituting ( 234 ) in ( 268 ) and
considering the asymptotic limit of @xmath when @xmath , we obtain

  -- -------- -- -------
     @xmath      (269)
  -- -------- -- -------

In evaluating ( 269 ), we have not imposed the conventional
normalization condition @xmath . Indeed, in our case @xmath constant;
what matters is that we have used the very same normalization constant
used in evaluating the length of geodesics in @xmath so that we may
compare both lengths using the same ”meter” (statistical affine
parameter @xmath ). In order to roughly investigate the asymptotic
behavior of two neighboring geodesics labelled by the parameters @xmath
and @xmath , we consider the following difference,

  -- -------- -- -------
     @xmath      (270)
  -- -------- -- -------

It is clear that @xmath diverges, that is the lengths of two neighboring
geodesics with slightly different parameters @xmath and @xmath differ in
a significant way as the evolution parameter @xmath . This hints at the
onset of instability of the hyperbolic trajectories on @xmath .

#### xxi.2.2 Evolution of Volumes @xmath on the Statistical Manifold
@xmath

The instability of ED2 can be explored by studying the behavior of the
one-parameter family of statistical volume elements @xmath . Recall that
@xmath is the space of probability distributions @xmath labeled by
parameters @xmath , @xmath , @xmath , @xmath . These parameters are the
coordinates of the point @xmath , and in these coordinates a @xmath
infinitesimal volume element @xmath reads,

  -- -------- -- -------
     @xmath      (271)
  -- -------- -- -------

where in the ED2 model here presented, @xmath . Hence, the infinitesimal
volume element @xmath is given by,

  -- -------- -- -------
     @xmath      (272)
  -- -------- -- -------

The volume of an extended region of @xmath is defined by,

  -- -------- -- -------
     @xmath      (273)
  -- -------- -- -------

Integrating ( 273 ) and using ( 234 ), we obtain

  -- -------- -- -------
     @xmath      (274)
  -- -------- -- -------

The accessible volume, on average, on the configuration space @xmath is
@xmath ,

  -- -- -- -------
           (275)
  -- -- -- -------

For convenience, let us rename @xmath . Therefore, the asymptotic
expansion of @xmath for @xmath @xmath reads,

  -- -------- -- -------
     @xmath      (276)
  -- -------- -- -------

In analogy to ( 249 ) we introduce,

  -- -------- -- -------
     @xmath      (277)
  -- -------- -- -------

Substituting ( 276 ) in ( 277 ) and considering its asymptotic limit, we
obtain

  -- -------- -- -------
     @xmath      (278)
  -- -------- -- -------

#### xxi.2.3 The Jacobi Vector Field @xmath

We proceed as in @xmath . Study the behavior of the one-parameter @xmath
family of neighboring geodesics on @xmath , @xmath with

  -- -------- -- -------
     @xmath      (279)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (280)
  -- -------- -- -------

Note that because we will compare the two Jacobi fields @xmath on @xmath
and @xmath on @xmath , we assume the same initial conditions as
considered in @xmath . Recall that the non-vanishing Riemann tensor
components are @xmath and @xmath given in ( 221 ). Therefore two of the
four differential equations describing the geodesic spread are,

  -- -------- -- -------
     @xmath      
     @xmath      (281)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      
     @xmath      (282)
  -- -------- -- -------

The other two equations can be obtained from ( 281 ) and ( 282 )
substituting the index @xmath with @xmath and @xmath with @xmath . Thus,
we will limit our considerations just to the above two equations. Using
equations ( 219 ), ( 221 ), ( 279 ) and ( 280 ) in ( 281 ) and ( 282 )
and considering the asymptotic limit @xmath , the two equations of
geodesic deviation become,

  -- -------- -- -------
     @xmath      (283)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (284)
  -- -------- -- -------

Neglecting the exponentially decaying terms in @xmath in ( 283 ) and (
284 ) and assuming

  -- -------- -- -------
     @xmath      (285)
  -- -------- -- -------

the geodesic deviation equations in ( 283 ) and ( 284 ) become,

  -- -------- -- -------
     @xmath      (286)
  -- -------- -- -------

The consistency of the assumptions in ( 285 ) will be checked a
posteriori after integrating equations in ( 286 ). It follows that the
geodesics spread on @xmath is described by the temporal evolution of the
following deviation vector components,

  -- -------- -------- -- -------
     @xmath   @xmath      (287)
     @xmath   @xmath      
  -- -------- -------- -- -------

where @xmath , @xmath ,…, @xmath are integration constants. Note that
@xmath and @xmath in ( 287 ) equal @xmath in ( 264 ). Furthermore, note
that these solutions above are compatible with the assumptions in ( 285
). Finally, consider the Jacobi vector field intensity @xmath on @xmath
,

  -- -------- -- -------
     @xmath      (288)
  -- -------- -- -------

Substituting ( 279 ), ( 280 ) and ( 287 ) in ( 288 ) and keeping the
leading term in the asymptotic expansion in @xmath , we obtain

  -- -------- -- -------
     @xmath      (289)
  -- -------- -- -------

where the constant coefficient @xmath @xmath encodes information about
initial conditions and it depends on the model parameter @xmath . We
conclude that the geodesic spread on @xmath is described by means of an
exponentially divergent Jacobi vector field intensity @xmath .

## Xxii Statistical Curvature, Jacobi Field Intensity and Entropy-like
Quantities

Many problems in mathematical statistics, information theory and in
stochastic processes can be tackled using differential geometric methods
on curved statistical manifolds. For instance, an important class of
statistical manifolds is that arising from the exponential family amari
and one particular family is that of gamma probability distributions.
These distributions have been shown arwini to have important uniqueness
properties for stochastic processes. In this Chapter, two statistical
manifolds of negative curvature @xmath and @xmath have been considered.
They are representations of smooth families of probability distributions
(exponentials and Gaussians for @xmath , Gaussians for @xmath ). They
represent the ”arena” where the entropic dynamics takes place. The
instability of the trajectories of the ED1 and ED2 on @xmath and @xmath
respectively, has been studied using the statistical weight @xmath
defined on the curved manifold @xmath and the Jacobi vector field
intensity @xmath . Does our analysis lead to any possible further
understanding of the role of statistical curvature in physics and
statistics? We argue that it does.

The role of curvature in physics is fairly well understood. It encodes
information about the field strengths for all the four fundamental
interactions in nature. The curvature plays a key role in the Riemannian
geometric approach to chaos casetti . In this approach, the study of the
Hamiltonian dynamics is reduced to the investigation of geometrical
properties of the manifold on which geodesic flow is induced. For
instance, the stability or local instability of geodesic flows depends
on the sectional curvature properties of the suitable defined metric
manifold. The sectional curvature brings the same qualitative and
quantitative information that is provided by the Lyapunov exponents in
the conventional approach. Furthermore, the integrability of the system
is connected with existence of Killing vectors on the manifold. However,
a rigorous relation among curvature, Lyapunov exponents and
Kolmogorov-Sinai entropy kawabe is still under investigation. In
addition, there does not exist a well defined unifying characterization
of chaos in classical and quantum physics caves due to fundamental
differences between the two theories. Even in other fields of research
(for instance, statistical inference) the role of curvature is not well
understood. The meaning of statistical curvature for a one-parameter
model in inference theory was introduced in efron . Curvature served as
an important tool in the asymptotic theory of statistical estimation.
The higher the scalar curvature at a given point on the manifold, the
more difficult it is to do estimation at that point rodriguez . Our
analysis may be useful to shed light in statistical inference theory as
well.

Recall that the entropy-like quantity @xmath is the asymptotic limit of
the natural logarithm of the average of the statistical volume @xmath
associated to the evolution of the geodesics on @xmath . Considering
equations ( 250 ) and ( 278 ), we obtain,

  -- -------- -- -------
     @xmath      (290)
  -- -------- -- -------

Furthermore, the relationship between the statistical curvatures on the
curved manifolds @xmath and @xmath is,

  -- -------- -- -------
     @xmath      (291)
  -- -------- -- -------

In view of ( 290 ) and ( 291 ), it follows that there is a direct
proportionality between the curvature @xmath and the asymptotic
expression for the entropy-like quantity @xmath characterizing the ED on
manifolds @xmath with @xmath , @xmath , namely

  -- -------- -- -------
     @xmath      (292)
  -- -------- -- -------

Moreover, from ( 267 ) and ( 289 ), we obtain the following relation,

  -- -------- -- -------
     @xmath      (293)
  -- -------- -- -------

The two manifolds @xmath and @xmath are exponentially unstable and the
intensity of Jacobi vector field @xmath of manifold @xmath with
curvature @xmath is asymptotically twice the intensity of the Jacobi
vector field @xmath of manifold @xmath with curvature @xmath .
Considering ( 291 ) and ( 293 ), we obtain

  -- -------- -- -------
     @xmath      (294)
  -- -------- -- -------

It seems there exists a direct proportionality between the curvature
@xmath and the intensity of the Jacobi field @xmath characterizing the
degree of chaoticity of a statistical manifold of negative curvature
@xmath . Finally, comparison of ( 292 ) and ( 294 ) leads to the formal
link between curvature, entropy and chaoticity:

  -- -------- -- -------
     @xmath      (295)
  -- -------- -- -------

Though several points need deeper understanding and analysis, we hope
that our work shows that this information-geometric approach may be
useful in providing a unifying framework to study chaos on statistical
manifolds underlying entropic dynamical models.

## Xxiii Conclusions

Two chaotic entropic dynamical models have been considered: a @xmath and
@xmath statistical manifold @xmath and @xmath respectively. These
manifolds serve as the stage on which the entropic dynamics takes place.
In the former case, macro-coordinates on the manifold are represented by
the expectation values of microvariables associated with Gaussian and
exponential probability distributions. In the latter case,
macro-coordinates are expectation values of microvariables associated
with two Gaussians distributions. The geometric structure of @xmath and
@xmath was studied in detail. It was shown that @xmath is a curved
manifold of constant negative curvature @xmath while @xmath has constant
negative curvature @xmath . The geodesics of the ED models are
hyperbolic curves on @xmath @xmath . A study of the stability of
geodesics on @xmath and @xmath was presented. The notion of statistical
volume elements was introduced to investigate the asymptotic behavior of
a one-parameter family of neighboring volumes @xmath . An
information-geometric analog of the Zurek-Paz chaos criterion was
presented . It was shown that the behavior of geodesics is characterized
by exponential instability that leads to chaotic scenarios on the curved
statistical manifolds. These conclusions are supported by a study based
on the geodesic deviation equations and on the asymptotic behavior of
the Jacobi vector field intensity @xmath on @xmath and @xmath . A
Lyapunov exponent analog similar to that appearing in the Riemannian
geometric approach was suggested as an indicator of chaos. On the basis
of our analysis a relationship among an entropy-like quantity,
chaoticity and curvature in the two models ED1 and ED2 is proposed,
suggesting to interpret the statistical curvature as a measure of the
entropic dynamical chaoticity.

The implications of this work are twofold. Firstly, it helps to
understand the possible future use of the statistical curvature in
modelling real processes by relating it to conventionally accepted
quantities such as entropy (be it the KS entropy, the Shannon entropy,
the IGE entropy (that we have introduced in this Chapter), the
Kolmogorov complexity, the von Neumann entropy, the quantum dynamical
entropy, etc. etc.) and chaos. On the other hand, it serves to cast what
is already known in physics regarding curvature in a new light as a
consequence of its proposed link with inference. Finally we remark that
based on the results obtained from the chosen ED models, it is not
unreasonable to think that should the correct variables describing the
true degrees of freedom of a physical system be identified, perhaps
deeper insights into the foundations of models of physics and reasoning
(and their relationship to each other) may be uncovered.

Chapter 6: Information-constrained dynamics, Part II: Newtonian entropic
dynamics

In collaboration with Prof. Ariel Caticha, I show that the ED formalism
is not purely a mathematical framework; it is indeed a general
theoretical scheme where conventional Newtonian dynamics is obtained as
a special limiting case. Newtonian dynamics is derived from prior
information codified into an appropriate statistical model. The basic
assumption is that there is an irreducible uncertainty in the location
of particles so that the state of a particle is defined by a probability
distribution. The corresponding configuration space is a statistical
manifold the geometry of which is defined by the information metric. The
trajectory follows from a principle of inference, the method of Maximum
Entropy. No additional ”physical” postulates such as an equation of
motion, or an action principle, nor the concepts of momentum and of
phase space, not even the notion of time, need to be postulated. The
resulting entropic dynamics reproduces the Newtonian dynamics of any
number of particles interacting among themselves and with external
fields. Both the mass of the particles and their interactions are
explained in terms of the underlying statistical manifold.

## Xxiv Introduction

In this Chapter, we use well established principles of inference to
derive Newtonian dynamics from relevant prior information codified into
a statistical model caticha-cafaro . We do not assume equations of
motion or principles of least action. Moreover, neither the concept of
momentum nor that of the associated phase space is assumed. Indeed, not
even the notion of an absolute Newtonian time is postulated. Firstly, we
construct a suitable statistical model of the space of states of a
system of particles. The statistical configuration space is
automatically endowed with a geometry and this information geometry
turns out to be unique amari ; cencov . Secondly, we tackle the
dynamics: given the initial and the final states, we investigate what
trajectory the system is expected to follow. In the conventional
approach one postulates an equation of motion or an action principle
that presumably reflects a ”law of nature”. However, in our theoretical
framework, the dynamics follows from a principle of inference, the
method of Maximum (relative) Entropy, ME catichaME . We show that with a
suitable choice of the statistical manifold the resulting ”entropic
dynamics” caticha02 ; caticha03 reproduces Newtonian dynamics, or more
properly, Newtonian entropic dynamics (NED).

## Xxv Configuration space as a statistical manifold

Let us consider a single particle moving in space @xmath : the
configuration space is a three dimensional manifold @xmath with some
unknown metric tensor @xmath . Our main assumption is that there is a
certain fuzziness to space @xmath ; there is an irreducible uncertainty
in the location of the particle. Thus, the assertion “the particle is at
the point @xmath ” means that its ”true” position @xmath is somewhere in
the vicinity of @xmath . This leads us to associate a probability
distribution @xmath to each point @xmath and the configuration space is
thus transformed into a statistical manifold @xmath : a point @xmath is
no longer a structureless dot but a probability distribution. Remarkably
there is a unique measure of the extent to which the distribution at
@xmath can be distinguished from the neighboring distribution at @xmath
. It is the information metric of Fisher and Rao amari . Thus, physical
space, when viewed as a statistical manifold, inherits a metric
structure from the distributions @xmath . We will assume that the
originally unspecified metric @xmath is precisely the information metric
induced by the distributions @xmath .

### xxv.1 The Gaussian Model and the Covariance Problem

Given a manifold @xmath of probability distributions @xmath , the
problem is to find the corresponding information metric @xmath . This is
commonly called the direct problem . Its solution might be laborious but
it is quite mechanical. In this Chapter, we are tackling what is called
the inverse problem : constructing a statistical manifold @xmath with a
given metric tensor @xmath . In caticha03 , it was proposed that a
Gaussian model,

  -- -------- -- -------
     @xmath      (296)
  -- -------- -- -------

where @xmath , encodes the physically relevant information, which
consists of an estimate of the particle position,

  -- -------- -- -------
     @xmath      (297)
  -- -------- -- -------

and of its uncertainty, given by the covariance matrix @xmath ,

  -- -------- -- -------
     @xmath      (298)
  -- -------- -- -------

where @xmath @xmath is the inverse of @xmath , @xmath . It is worthwhile
noticing that the expected values in eqs.( 297 ) and ( 298 ) are not
covariant under coordinate transformations. Indeed, the transformation
@xmath does not lead to @xmath because in general @xmath except when
uncertainties are small. Our Gaussian model can at best be an
approximation valid when @xmath is sharply localized in a very small
region within which curvature effects are negligible. Fortunately, this
is all we need for our present purpose. The information distance between
@xmath and @xmath is calculated from (see e.g., amari )

  -- -------- -- -------
     @xmath      (299)
  -- -------- -- -------

with @xmath and @xmath . Consider the nine-dimensional space of
Gaussians

  -- -------- -- -------
     @xmath      (300)
  -- -------- -- -------

Here the parameters @xmath include the three @xmath plus six independent
elements of the symmetric matrix @xmath . Eq.( 299 ) defines the
information distance between @xmath , @xmath and @xmath , @xmath as

  -- -------- -- -------
     @xmath      (301)
  -- -------- -- -------

where @xmath , @xmath and, @xmath with @xmath . Therefore,

  -- -------- -- -------
     @xmath      (302)
  -- -------- -- -------

This is the metric of the full nine-dimensional manifold, but it is not
what we need. What we want is the metric of the embedded
three-dimensional submanifold where @xmath is some function of @xmath .
The tensorial behavior of @xmath in equation ( 296 ) is our major
concern. As we said, the notion of expected value is not covariant. Our
approach is an approximation that is valid when the position uncertainty
is much smaller than the local radius of curvature of the manifold; or,
alternatively, the effects of space curvature are negligible within the
region where @xmath is appreciable. Moreover, @xmath is a tensor while
@xmath does not behave as a tensor (in general). However, it can be
shown that special cases exist where the tensorial behavior of @xmath
can be restored. Namely, if we consider exclusively linear change of
coordinates @xmath such that ,

  -- -------- -- -------
     @xmath      (303)
  -- -------- -- -------

that is, if we assume that the new set of coordinates @xmath are not
allowed to depend on the old set @xmath in a nonlinear way, than we may
conclude that @xmath transforms as a tensor. In these very restrictive
conditions, we may conclude that @xmath has tensorial behavior provided
that the new coordinates are not allowed to depend on the old
coordinates (source coordinates) in a nonlinear way.

### xxv.2 The Gaussian-like Model and the Solution to the Covariance
Problem

One of the major points in the indirect problem is choosing the correct
variance-covariance field tensor @xmath that leads to a given metric
tensor @xmath on @xmath . We want to devise fully covariant models. This
can be achieved by constructing Gaussian-like probability distributions
that, in the limit of small uncertainties, approximate the Gaussian
distributions in ( 296 ). Such a distribution may be defined as,

  -- -------- -- -------
     @xmath      (304)
  -- -------- -- -------

The quantity @xmath in ( 304 ) is the determinant of the positive
definite metric tensor field @xmath satisfying the relation,

  -- -------- -- -------
     @xmath      (305)
  -- -------- -- -------

The quantity @xmath is a scalar field, @xmath , @xmath is the @xmath
-length along the @xmath -geodesic from the point @xmath to the point
@xmath and @xmath is a normalization constant. The proposed probability
distribution in ( 304 ) is a manifestly covariant quantity: the
normalization constant @xmath , the @xmath -length @xmath , @xmath , the
scalar field @xmath and, @xmath are all invariant quantities. We remark
that the space of spherically symmetric Gaussians with prescribed @xmath
,

  -- -------- -- -------
     @xmath      (306)
  -- -------- -- -------

is a special case of the distributions defined in ( 304 ). In this case,
the variance-covariance matrix in ( 296 ) is diagonal and proportional
to the unit matrix,

  -- -------- -- -------
     @xmath      (307)
  -- -------- -- -------

Differentiating @xmath and using the fact that @xmath with @xmath , we
get

  -- -------- -- -------
     @xmath      (308)
  -- -------- -- -------

Substituting ( 308 ) in ( 302 ), we obtain

  -- -- -- -------
           (309)
  -- -- -- -------

which, using @xmath , simplifies to

  -- -------- -- -------
     @xmath      (310)
  -- -------- -- -------

In the case of probability distributions ( 306 ), the metric tensor
@xmath on @xmath becomes,

  -- -------- -- -------
     @xmath      (311)
  -- -------- -- -------

If the @xmath field varies very slowly

  -- -------- -- -------
     @xmath      (312)
  -- -------- -- -------

then to first order the metric @xmath is independent of the derivatives
@xmath ,

  -- -------- -- -------
     @xmath      (313)
  -- -------- -- -------

and the metric tensor @xmath is just a conformal transformation of flat
metric @xmath ,

  -- -------- -- -------
     @xmath      (314)
  -- -------- -- -------

We point out that condition ( 312 ) is compatible with the definition
given in ( 323 ), @xmath @xmath . Rewriting @xmath in terms of @xmath ,
equation ( 312 ) reads,

  -- -------- -- -------
     @xmath      (315)
  -- -------- -- -------

Therefore, we are allowed to consider very small @xmath by simply making
the parameter @xmath small and keeping @xmath ( @xmath would be the
information geometric analog of a conservative force appearing in
Newton’s second law) arbitrarily big. For small @xmath , the
distribution in ( 304 ) can be written as,

  -- -------- -- -------
     @xmath      (316)
  -- -------- -- -------

In local Cartesian coordinates (LCC), @xmath , we obtain

  -- -------- -- -------
     @xmath      (317)
  -- -------- -- -------

and ( 316 ) becomes,

  -- -- -- -------
           (318)
  -- -- -- -------

In the case of probability distributions ( 318 ), the metric tensor
@xmath on @xmath becomes,

  -- -------- -- -------
     @xmath      (319)
  -- -------- -- -------

Since ( 319 ) is a tensorial equation, its validity is preserved in
arbitrary coordinates system and,

  -- -------- -- -------
     @xmath      (320)
  -- -------- -- -------

the tensorial relation between @xmath and @xmath is covariantly
preserved. In conclusion, in this subsection we provided a solution to
the inverse problem suggesting a fully covariant Gaussian-like model of
probability distributions @xmath forming a curved statistical manifold
@xmath with a given metric tensor given, in the limit of small
uncertainties, by @xmath in ( 320 ). The possibility to extend this
result in regions of high curvature, such as near singularities, remains
to be ascertained.

## Xxvi Newtonian Entropic Dynamics of a single particle

Our objective is to construct ED models on statistical manifolds of
Gaussian-like probability distributions presented in the previous
section. We follow the work presented in Chapter 3. Assume there exists
a continuous path, the key question is to find the trajectory the system
is expected to follow, given an initial and a final state. A large
change is the result of a succession of very many small changes and
therefore we only need to determine the properties of a short segment of
the trajectory. The idea behind entropic dynamics is that as the system
moves from a point @xmath to a neighboring point @xmath it must pass
through a halfway point. caticha02 . The basic dynamical question can
now be rephrased as follows: the system is initially described by the
probability distribution @xmath and we are given the information that it
has moved to one of the neighboring states in the family @xmath where
the @xmath lie on the plane halfway between the initial @xmath and the
final @xmath . Which @xmath do we select? The answer is given by the
method of maximum (relative) entropy, ME. The selected distribution is
that which maximizes the entropy of @xmath relative to the prior @xmath
subject to the constraint that @xmath is equidistant from @xmath and
@xmath . The result is that the selected @xmath minimizes the distance
to @xmath and therefore the three points @xmath , @xmath and @xmath lie
on a straight line. Since any three neighboring points along the
trajectory must line up, the trajectory predicted by entropic dynamics
is the geodesic that minimizes the length

  -- -------- -- -------
     @xmath      (321)
  -- -------- -- -------

where @xmath is any parameter that labels points along the curve, @xmath
. In entropic dynamics, the minimal-length geodesics represent the only
family of curves that is singled out as special. The construction of
useful physics models does not require any additional structure and
therefore none will be introduced. The simplest statistical model is a
three-dimensional manifold of spherically symmetric Gaussians with
constant variance @xmath . From ( 319 ) follows that the corresponding
information metric is

  -- -------- -- -------
     @xmath      (322)
  -- -------- -- -------

where @xmath is the familiar metric of flat Euclidean space. We point
out that already in such a simple model entropic dynamics reproduces the
familiar straight line trajectories that are commonly associated with
Galilean inertial motion. However, non-trivial entropic dynamical models
require some curvature. For instance, consider the model of spherically
symmetric Gaussians where the variance is a non-uniform scalar field
@xmath . It is convenient to write the corresponding information metric
@xmath as the Euclidean metric eq.( 322 ) modulated by a positive
conformal factor @xmath ,

  -- -------- -- -------
     @xmath      (323)
  -- -------- -- -------

The conformal factor @xmath defines an angle-preserving transformation
(conformal transformation) and its effect is a local dilation. It is
useful to rewrite the length eq.( 321 ) with the metric ( 323 ) in the
form

  -- -------- -- -------
     @xmath      (324)
  -- -------- -- -------

with a ”Lagrangian” function @xmath given by,

  -- -------- -- -------
     @xmath      (325)
  -- -------- -- -------

The geodesics follow from the Lagrange equations,

  -- -------- -- -------
     @xmath      (326)
  -- -------- -- -------

that is, substituting ( 325 ) in ( 326 ),

  -- -------- -- -------
     @xmath      (327)
  -- -------- -- -------

These equations can be simplified considerably once we notice that the
parameter @xmath is arbitrary. Let us replace the original @xmath with a
new parameter @xmath defined as

  -- -------- -- -------
     @xmath      (328)
  -- -------- -- -------

In terms of the new @xmath the equation of motion ( 327 ) becomes,

  -- -------- -- -------
     @xmath      (329)
  -- -------- -- -------

In order to allow us the possibility of considering the limit of small
uncertainties, @xmath , we redefine the relation between @xmath and
@xmath introducing a new parameter @xmath ( @xmath has the dimensions of
a time) such that,

  -- -------- -- -------
     @xmath      (330)
  -- -------- -- -------

From eq.( 330 ) the new @xmath is such that

  -- -------- -- -------
     @xmath      (331)
  -- -------- -- -------

Eqs.( 329 ) and ( 331 ) are equivalent to Newtonian dynamics. To make it
explicit we introduce a ”mass” @xmath and a ”potential” @xmath through a
mere change of notation,

  -- -------- -- -------
     @xmath      (332)
  -- -------- -- -------

The parameter @xmath has been introduced because of dimensional analysis
convenience ( @xmath has the dimensions of an energy; @xmath has the
dimensions of a length) and the constant @xmath reflects the freedom to
add a constant to the potential. The result is the Newtonian
information-dynamical equation,

  -- -------- -- -------
     @xmath      (333)
  -- -------- -- -------

and energy conservation,

  -- -------- -- -------
     @xmath      (334)
  -- -------- -- -------

Thus, the constant @xmath is interpreted as energy. We have just derived
@xmath purely from principles of inference applied to the relevant
information codified into a statistical model! From eq.( 321 ) onwards
our inference approach is formally identical to the Jacobi action
principle of classical mechanics lanczos but we did not need to know
this. Within our theoretical construct, both the mass @xmath of the
particles and their interactions are explained in terms of an
irreducible uncertainty of their positions. Masses and interactions are
features of the curved statistical manifold underlying the
information-dynamics. Even though our formalism describes a
non-relativistic model, there already appears a ”unification” between
mass @xmath and potential energy @xmath : they are different aspects of
the same thing, the particle’s ”intrinsic” position uncertainty @xmath
modulated throughout space by the field @xmath . The derivation
presented in this section illustrates the main idea but has two
important limitations. First, it applies to a single particle with a
fixed constant energy @xmath and this means that we consider only
isolated systems. Second, even though we have identified a convenient
parameter @xmath , we do not know that it actually represents
”true” time. It could be that @xmath is the universal Newtonian time. It
could be that @xmath is just a parameter that applies only to one
particular isolated particle. The original formulation in terms of the
”Jacobi” action, eq.( 324 ), is completely timeless. Therefore the
appearance of time is obscure. The solution to both these problems
emerges as we apply the formalism to the motion of the only system known
to be completely isolated: the whole universe. In this case, the fact
that the energy @xmath is a fixed constant does not represent a
restriction. Moreover, since the preferred time parameter would be
associated to the whole universe, it would not be at all inappropriate
to call it the universal time.

## Xxvii Newtonian Entropic Dynamics of the Whole Universe

Our theoretical scheme may be generalized to arbitrary @xmath -particles
interacting with external potentials and also with each other
caticha-cafaro . However, to simplify our notation we will consider a
universe that consists of @xmath particles. For the @xmath -particle
system the position @xmath , @xmath is denoted by @xmath coordinates
@xmath with @xmath , @xmath ,.., @xmath . Let @xmath , @xmath with
@xmath , @xmath , @xmath for particle 1 and @xmath , @xmath , @xmath for
particle 2. A point in the @xmath configuration space is a Gaussian
distribution,

  -- -------- -- -------
     @xmath      (335)
  -- -------- -- -------

The simplest model for two (possibly non-identical) particles assigns
uniform variances @xmath and @xmath to each particle. The corresponding
metric, analogous to eq.( 322 ), is

  -- -------- -- -------
     @xmath      (336)
  -- -------- -- -------

where @xmath is a constant @xmath diagonal matrix,

  -- -------- -- -------
     @xmath      (337)
  -- -------- -- -------

where each entry represents a @xmath matrix. The metric @xmath describes
a flat space; the trajectories are familiar ”straight” lines and the
particles move independently of each other; they do not interact.
However, non-trivial dynamics requires the introduction of curvature and
the simplest way to do this is through an overall conformal field @xmath
with @xmath , @xmath . We propose

  -- -------- -- -------
     @xmath      (338)
  -- -------- -- -------

The equation of motion for the @xmath universe is the geodesic that
minimizes

  -- -------- -- -------
     @xmath      (339)
  -- -------- -- -------

where @xmath , @xmath and @xmath . The Lagrange equations yield,

  -- -- -- -------
           (340)
  -- -- -- -------

which suggests introducing a new parameter @xmath defined by

  -- -------- -- -------
     @xmath      (341)
  -- -------- -- -------

In terms of the new parameter the equations of motion are

  -- -------- -- -------
     @xmath      (342)
  -- -------- -- -------

where @xmath is a diagonal matrix. Equation ( 342 ) becomes

  -- -------- -- -------
     @xmath      (343)
  -- -------- -- -------

for each of the particles, @xmath , @xmath . The motion of particle 1
depends on the location of particle 2: these are interacting particles!
The new time parameter @xmath , eq.( 341 ), is such that

  -- -------- -- -------
     @xmath      (344)
  -- -------- -- -------

As before, the equivalence to Newtonian dynamics is made explicit by a
change of notation,

  -- -------- -- -------
     @xmath      (345)
  -- -------- -- -------

The result is

  -- -------- -- -------
     @xmath      (346)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (347)
  -- -------- -- -------

The constant @xmath in ( 347 ) is the total energy of the universe and
there are no restrictions on the energy of individual subsystems. The
choice for the conformal factor @xmath , @xmath is quite general. For
instance, we may consider @xmath , @xmath ,

  -- -------- -- -------
     @xmath      (348)
  -- -------- -- -------

so the particles can interact with external potentials @xmath and @xmath
and also with each other through @xmath , @xmath . The definition of the
parameter @xmath requires taking into account all the particles in the
universe. We started with a completely timeless theory, eq.( 339 ), and
in fact, no external time has been introduced. What we have is a
convenient @xmath parameter associated to the change of the total
system, which in this case is the whole universe. The universe is its
own clock and it measures universal time. It is worthwhile noticing that
the reparametrization that allowed us to introduce a Newtonian time was
possible only because the same conformal factor @xmath applies equally
to all particles.

### xxvii.1 Remarks on Newtonian Entropic Dynamics

Newtonian entropic dynamics offers a new perspective on the concept of
mass and interactions. In order to see this, notice that since @xmath in
( 338 ) is diagonal the distribution ( 335 ) is a product,

  -- -------- -- -------
     @xmath      (349)
  -- -------- -- -------

Although the model represents interacting particles, the distribution is
a product: the uncertain variables @xmath and @xmath are statistically
independent. The coupling arises through the conditioning on @xmath ,
@xmath . Focusing our attention on particle 1 (similar remarks also
apply to particle 2), we note the distribution @xmath , @xmath is a
spherically symmetric Gaussian,

  -- -------- -- -------
     @xmath      (350)
  -- -------- -- -------

The uncertainty in the position of particle 1 is given by @xmath ,
@xmath . The mass @xmath is interpreted in terms of a uniform background
contribution to the uncertainty. Mass is a manifestation of an
uncertainty in location; higher mass reflects a lower uncertainty. On
the other hand, interactions arise from the non-uniformity of @xmath ,
@xmath that depends on the location of other particles through the
modulating field @xmath , @xmath . It is amusing to note that even
though this is a non-relativistic model there already appears a
”unification” between mass and (potential) energy: they are different
aspects of the same thing, the position uncertainty.

## Xxviii Conclusions

In this Chapter, we have shown that the tools of inference- probability,
information geometry and entropy- are sufficiently rich that one can
construct entropic dynamics models that reproduce recognizable laws of
physics. Indeed, preliminary steps towards an entropic dynamics approach
to general relativity appeared in caticha03 . Moreover, an entropic
dynamics approach to the study of chaos has already lead to interesting
results cafaro01 ; cafaro02 ; cafaro03 .  An extended version of these
results will be presented in Chapters 7 and 8. We emphasize that NED has
limited applicability being a nonrelativistic model. Our theoretical
construct invokes two metrics: there is the metric of flat
three-dimensional Euclidean space, @xmath , that appears in the kinetic
energies and there is the information metric @xmath that accounts for
mass @xmath and interactions @xmath and applies to the curved
configuration space @xmath . This is a reflection of the fact that a
system of @xmath particles is described as a point in a @xmath
-dimensional configuration space @xmath instead of @xmath points living
within the same evolving three-dimensional space @xmath . Furthermore,
we the choice of the modulating field @xmath does not arise from first
principles, it is merely an educated guess. This is no different from
Newton’s dictum hypothesis non fingo : a choice of @xmath is justified
by its explanatory success. Finally, at this very premature point in the
development of our Newtonian entropic dynamics, we do not offer any
physical insight about the underlying fuzziness of space. This will be
one of our major concerns in future works.

Chapter 7: Information geometrodynamical approach to chaos: An
Application

I extend my study of chaotic systems (information geometrodynamical
approach to chaos, IGAC) to an ED Gaussian model describing an arbitrary
system of @xmath degrees of freedom. It is shown that the hyperbolicity
of a non-maximally symmetric @xmath -dimensional statistical manifold
@xmath underlying the ED Gaussian model leads to linear
information-geometrodynamical entropy growth and to exponential growth
of the Jacobi vector field intensity, quantum and classical features of
chaos respectively. As a special physical application, the information
geometrodynamical scheme is applied to investigate the chaotic
properties of a set of @xmath -uncoupled three-dimensional anisotropic
inverted harmonic oscillators (IHOs) characterized by an Ohmic
distributed frequency spectrum. I show that the asymptotic temporal
behavior of the information geometrodynamical entropy of such a system
exhibits linear growth and I suggest the system studied may be
considered to be the classical information-geometric analogue of the
Zurek-Paz quantum chaos criterion in its classical reversible limit.

## Xxix Introduction

The lack of a unified characterization of chaos in classical and quantum
dynamics is well-known. In the Riemannian casetti and Finslerian
cipriani (a Finsler metric is obtained from a Riemannian metric by
relaxing the requirement that the metric be quadratic on each tangent
space) geometrodynamical approach to chaos in classical Hamiltonian
systems, an active field of research concerns the possibility of finding
a rigorous relation among the sectional curvature, the Lyapunov
exponents, and the Kolmogorov-Sinai dynamical entropy (i.e. the sum of
positive Lyapunov exponents) kawabe . The largest Lyapunov exponent
characterizes the degree of chaoticity of a dynamical system and, if
positive, it measures the mean instability rate of nearby trajectories
averaged along a sufficiently long reference trajectory. Moreover, it is
known that classical chaotic systems are distinguished by their
exponential sensitivity to initial conditions and that the absence of
this property in quantum systems has lead to a number of different
criteria being proposed for quantum chaos. Exponential decay of
fidelity, hypersensitivity to perturbation, and the Zurek-Paz quantum
chaos criterion of linear von Neumann’s entropy growth zurek are some
examples caves . These criteria accurately predict chaos in the
classical limit, but it is not clear that they behave the same far from
the classical realm. The present work makes use of Entropic Dynamics
(ED) caticha1 . ED is a theoretical framework that arises from the
combination of inductive inference (Maximum relative Entropy Methods,
caticha2 ) and Information Geometry (Riemannian geometry applied to
probability theory) (IG) amari . As such, ED is constructed on
statistical manifolds. It is developed to investigate the possibility
that laws of physics - either classical or quantum - might reflect laws
of inference rather than laws of nature. This Chapter contains works
that follow up a series of my works cafaro1 ; cafaro2 ; cafaro3 . In
this Chapter, the ED theoretical framework is used to explore the
possibility of constructing a unified characterization of classical and
quantum chaos. We investigate a system whose microstates @xmath are
characterized by @xmath degrees of freedom @xmath . Each degree of
freedom is Gaussian-distributed and it is described by two pieces of
relevant information, its mean expected value and its variance. This
leads to consider an ED model on a non-maximally symmetric @xmath
-dimensional statistical manifold @xmath . It is shown that @xmath
possesses a constant negative Ricci curvature that is proportional to
the number of degrees of freedom of the system, @xmath . It is shown
that the system explores statistical volume elements on @xmath at an
exponential rate. We define a dynamical information-geometric entropy
@xmath of the system and we show it increases linearly in time
(statistical evolution parameter) and is moreover, proportional to the
number of degrees of freedom of the system. The geodesics on @xmath are
hyperbolic trajectories. Using the Jacobi-Levi-Civita (JLC) equation for
geodesic spread, it is shown that the Jacobi vector field intensity
@xmath diverges exponentially and is proportional to the number of
degrees of freedom of the system. Thus, @xmath , @xmath and @xmath are
proportional to the number of Gaussian-distributed microstates of the
system. This proportionality leads to conclude there is a substantial
link among these information-geometric indicators of chaoticity.
Finally, as a special physical application, the information
geometrodynamical scheme is applied to investigate the chaotic
properties of a set of @xmath -uncoupled three-dimensional anisotropic
inverted harmonic oscillators (IHOs) characterized by an Ohmic
distributed frequency spectrum. We study the three-dimensional IHOs for
the sake of generality. However we also illustrate the main idea in a
simpler example studying the IGAC (Information Geometrodynamical
Approach to Chaos) of two uncoupled inverted one-dimensional harmonic
oscillators. I show that the asymptotic temporal behavior of the
information geometrodynamical entropy of such a system presents linear
growth and I suggest the system studied may be considered the classical
information-geometric analogue of the Zurek-Paz quantum chaos criterion
in its classical reversible limit.

## Xxx Specification of the Gaussian ED-model

Maximum relative Entropy (ME) methods are used to construct an ED model
that follows from an assumption about what information is relevant to
predict the evolution of the system. Given a known initial macrostate
(probability distribution) and that the system evolves to a final known
macrostate, the possible trajectories of the system are examined. A
notion of distance between two probability distributions is provided by
IG. As shown in fisher ; rao this distance is quantified by the
Fisher-Rao information metric tensor.

We consider an ED model whose microstates span a @xmath -dimensional
space labelled by the variables @xmath with @xmath , @xmath ,…., @xmath
and @xmath with @xmath , @xmath , @xmath . We assume the only testable
information pertaining to the quantities @xmath consists of the
expectation values @xmath and variance @xmath . The set of these
expectation values define the @xmath -dimensional space of macrostates
of the system. A measure of distinguishability among the states of the
ED model is obtained by assigning a probability distribution @xmath to
each macrostate @xmath where @xmath with @xmath , @xmath , @xmath ,
@xmath and @xmath , @xmath , @xmath . The process of assigning a
probability distribution to each state endows @xmath with a metric
structure. Specifically, the Fisher-Rao information metric defined in (
362 ) is a measure of distinguishability among macrostates. It assigns
an IG to the space of states.

### xxx.1 The Gaussian statistical manifold @xmath

We consider an arbitrary system evolving over a @xmath -dimensional
space. The variables @xmath label the @xmath -dimensional space of
microstates of the system. All information relevant to the dynamical
evolution of the system is assumed to be contained in the probability
distributions. For this reason, no other information is required. Each
macrostate may be viewed as a point of a @xmath -dimensional statistical
manifold with coordinates given by the numerical values of the
expectations @xmath and @xmath . The available information is contained
in the following @xmath information constraint equations,

  -- -- -- -------
           (351)
  -- -- -- -------

where @xmath and @xmath with @xmath , @xmath , @xmath , @xmath and
@xmath , @xmath , @xmath . The probability distributions @xmath are
constrained by the conditions of normalization,

  -- -------- -- -------
     @xmath      (352)
  -- -------- -- -------

The Gaussian distribution is identified by information theory as the
maximum entropy distribution if only the expectation value and the
variance are known. ME methods allows to associate a probability
distribution @xmath to each point in the space of states @xmath . The
distribution that best reflects the information contained in the prior
distribution @xmath updated by the information @xmath is obtained by
maximizing the relative entropy

  -- -------- -- -------
     @xmath      (353)
  -- -------- -- -------

As a working hypothesis, the prior @xmath is set to be uniform since we
assume the lack of prior available information about the system
(postulate of equal a priori probabilities). Upon maximizing ( 353 ),
given the constraints ( 351 ) and ( 352 ), we obtain

  -- -------- -- -------
     @xmath      (354)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (355)
  -- -------- -- -------

and @xmath , @xmath . For the rest of the Chapter, unless stated
otherwise, the statistical manifold @xmath will be defined by the
following expression,

  -- -------- -- -------
     @xmath      (356)
  -- -------- -- -------

The probability distribution ( 354 ) encodes the available information
concerning the system. Note we assumed uncoupled constraints among
microvariables @xmath . In other words, we assumed that information
about correlations between the microvariables need not to be tracked.
This assumption leads to the simplified product rule ( 354 ). However,
coupled constraints would lead to a generalized product rule in ( 354 )
and to a metric tensor ( 362 ) with non-trivial off-diagonal elements
(covariance terms). For instance, the total probability distribution
@xmath of two dependent Gaussian distributed microvariables @xmath and
@xmath reads

  -- -------- -- -------
     @xmath      (357)
     @xmath      
  -- -------- -- -------

where @xmath is the correlation coefficient given by

  -- -- -- -------
           (358)
  -- -- -- -------

The metric induced by ( 357 ) is obtained by use of ( 362 ), the result
being

  -- -- -- -------
           (359)
  -- -- -- -------

where @xmath , @xmath , @xmath , @xmath , @xmath . The Ricci curvature
scalar associated with manifold characterized by ( 359 ) is given by

  -- -------- -- -------
     @xmath      (360)
  -- -------- -- -------

It is clear that in the limit @xmath , the off-diagonal elements of
@xmath vanish and the scalar @xmath reduces to the result obtained in
cafaro2 , namely @xmath . Correlation terms may be fictitious. They may
arise for instance from coordinate transformations. On the other hand,
correlations may arise from external fields in which the system is
immersed. In such situations, correlations among @xmath effectively
describe interaction between the microvariables and the external fields.
Such generalizations would require more delicate analysis. Before
proceeding, a comment is in order. Most probability distributions arise
from the maximum entropy formalism as a result of simple statements
concerning averages (Gaussians, exponential, binomial, etc.). Not all
distribution are generated in this manner however. Some distributions
are generated by combining the results of simple cases (multinomial from
a binomial) while others are found as a result of a change of variables
(Cauchy distribution). For instance, the Weibull and Wigner-Dyson
distributions can be obtained from an exponential distribution as a
result of a power law transformation wigner-dyson .

#### xxx.1.1 Metric structure of @xmath

We cannot determine the evolution of microstates of the system since the
available information is insufficient. Not only is the information
available insufficient but we also do not know the equation of motion.
In fact there is no standard ”equation of motion”. Instead we can ask:
how close are the two total distributions with parameters @xmath ,
@xmath and @xmath , @xmath ? Once the states of the system have been
defined, the next step concerns the problem of quantifying the notion of
change from the state @xmath to the state @xmath . A convenient measure
of change is distance. The measure we seek is given by the dimensionless
distance @xmath between @xmath and @xmath ,

  -- -------- -- -------
     @xmath      (361)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (362)
  -- -------- -- -------

is the Fisher-Rao information metric. Substituting ( 354 ) into ( 362 ),
the metric @xmath on @xmath becomes a @xmath matrix @xmath made up of
@xmath blocks @xmath with dimension @xmath given by,

  -- -------- -- -------
     @xmath      (363)
  -- -------- -- -------

with @xmath , @xmath , @xmath , @xmath and @xmath . From ( 362 ), the
”length” element ( 361 ) reads,

  -- -- -- -------
           (364)
  -- -- -- -------

We bring attention to the fact that the metric structure of @xmath is an
emergent (not fundamental) structure. It arises only after assigning a
probability distribution @xmath to each state @xmath .

#### xxx.1.2 Curvature of @xmath

Given the Fisher-Rao information metric, we use standard differential
geometry methods applied to the space of probability distributions to
characterize the geometric properties of @xmath . Recall that the Ricci
scalar curvature @xmath is given by,

  -- -------- -- -------
     @xmath      (365)
  -- -------- -- -------

where @xmath so that @xmath . The Ricci tensor @xmath is given by,

  -- -------- -- -------
     @xmath      (366)
  -- -------- -- -------

The Christoffel symbols @xmath appearing in the Ricci tensor are defined
in the standard manner as,

  -- -------- -- -------
     @xmath      (367)
  -- -------- -- -------

Using ( 363 ) and the definitions given above, we can show that the
Ricci scalar curvature becomes

  -- -------- -- -------
     @xmath      (368)
  -- -------- -- -------

The scalar curvature is the sum of all sectional curvatures of planes
spanned by pairs of orthonormal basis elements @xmath of the tangent
space @xmath with @xmath ,

  -- -------- -- -------
     @xmath      (369)
  -- -------- -- -------

where @xmath . Notice that the sectional curvatures completely determine
the curvature tensor. From ( 368 ) we conclude that @xmath is a @xmath
-dimensional statistical manifold of constant negative Ricci scalar
curvature. A detailed analysis on the calculation of Christoffel
connection coefficients using the ED formalism for a four-dimensional
manifold of Gaussians can be found in cafaro2 .

#### xxx.1.3 Anisotropy and Compactness

It can be shown that @xmath is not a maximally symmetric
multidimensional manifold. The first way this can be understood is from
the fact that the Weyl Projective curvature tensor goldberg (or the
anisotropy tensor) @xmath defined by

  -- -------- -- -------
     @xmath      (370)
  -- -------- -- -------

with @xmath in the present case, is non-vanishing. In ( 370 ), the
quantity @xmath is the Riemann curvature tensor defined in the usual
manner by

  -- -------- -- -------
     @xmath      (371)
  -- -------- -- -------

Considerations regarding the negativity of the Ricci curvature as a
strong criterion of dynamical instability and the necessity of
compactness of @xmath in ”true” chaotic dynamical systems is under
investigation cafaro4 .

The issue of symmetry of @xmath can alternatively be understood from
consideration of the sectional curvature. In view of ( 369 ), the
negativity of the Ricci scalar implies the existence of expanding
directions in the configuration space manifold @xmath . Indeed, from (
368 ) one may conclude that negative principal curvatures (extrema of
sectional curvatures) dominate over positive ones. Thus, the negativity
of the Ricci scalar is only a sufficient (not necessary) condition for
local instability of geodesic flow. For this reason, the negativity of
the scalar provides a strong criterion of local instability. Scenarios
may arise where negative sectional curvatures are present, but the
positive ones could prevail in the sum so that the Ricci scalar is
non-negative despite the instability in the flow in those directions.
Consequently, the signs of the sectional curvatures are of primary
significance for the proper characterization of chaos.

Yet another useful way to understand the anisotropy of the @xmath is the
following. It is known that in @xmath dimensions, there are at most
@xmath independent Killing vectors (directions of symmetry of the
manifold). Since @xmath is not a pseudosphere, the information metric
tensor does not admit the maximum number of Killing vectors @xmath
defined as

  -- -------- -- -------
     @xmath      (372)
  -- -------- -- -------

where @xmath , given by

  -- -------- -- -------
     @xmath      (373)
  -- -------- -- -------

is the covariant derivative operator with respect to the connection
@xmath defined in ( 367 ). The Lie derivative @xmath of the tensor field
@xmath along a given direction @xmath measures the intrinsic variation
of the field along that direction (that is, the metric tensor is Lie
transported along the Killing vector) clarke . Locally, a maximally
symmetric space of Euclidean signature is either a plane, a sphere, or a
hyperboloid, depending on the sign of @xmath . In our case, none of
these scenarios occur. As will be seen in what follows, this fact has a
significant impact on the integration of the geodesic deviation equation
on @xmath . At this juncture, we emphasize it is known that the
anisotropy of the manifold underlying system dynamics plays a crucial
role in the mechanism of instability. In particular, fluctuating
sectional curvatures require also that the manifold be anisotropic.
However, the connection between curvature variations along geodesics and
anisotropy is far from clear and is currently under investigation.

Krylov was the first to emphasize krylov the use of @xmath as an
instability criterion in the context of an @xmath -body system (a gas)
interacting via Van der Waals forces, with the ultimate hope to
understand the relaxation process in a gas. However, Krylov neglected
the problem of compactness of the configuration space manifold which is
important for making inferences about exponential mixing of geodesic
flows pellicott . Mixing provides statistical independence of different
parts of a trajectory. This is the condition for application of
probability theory that allows to calculate statistical properties such
as diffusion, relaxation etc. Why is compactness so significant in the
characterization of chaos? True chaos should be identified by the
occurrence of two crucial features: 1) strong dependence on initial
conditions and exponential divergence of the Jacobi vector field
intensity, i.e., stretching of dynamical trajectories; 2) compactness of
the configuration space manifold, i.e., folding of dynamical
trajectories. Compactness cipriani ; jost is required in order to
discard trivial exponential growths due to the unboundedness of the
”volume” available to the dynamical system. In other words, the folding
is necessary to have a dynamics actually able to mix the trajectories,
making practically impossible, after a finite interval of time, to
discriminate between trajectories which were very nearby each other at
the initial time. When the space is not compact, even in presence of
strong dependence on initial conditions, it could be possible in some
instances (though not always), to distinguish among different
trajectories originating within a small distance and then evolved
subject to exponential instability.

The statistical manifold @xmath defined in ( 356 ) is compact provided
that the parameter space @xmath is compact. This can be seen as follows.
It is known from IG that there is a one-to-one relation between elements
of the statistical manifold and the parameter space. More precisely, the
statistical manifold @xmath is homeomorphic to the parameter space
@xmath . This implies the existence of a continuous, bijective map
@xmath ,

  -- -------- -- -------
     @xmath      (374)
  -- -------- -- -------

where @xmath . The inverse image @xmath is the so-called homeomorphism
map. In addition, since homeomorphisms preserve compactness, it is
sufficient to restrict ourselves to a compact subspace of the parameter
space @xmath in order to ensure that @xmath is itself compact. In our
specific case, the accessible region of the statistical manifold @xmath
is given by

  -- -------- -- -------
     @xmath      (375)
  -- -------- -- -------

where the @xmath dimensional @xmath is defined as

  -- -------- -- -------
     @xmath      (376)
  -- -------- -- -------

Assuming @xmath , from equations ( 380 ), we obtain

  -- -------- -- -------
     @xmath      (377)
  -- -------- -- -------

In conclusion, @xmath is compact provided we restrict ourselves to
consider only the accessible macrostates on @xmath .

## Xxxi Canonical formalism for the Gaussian ED-model

Jacobi was the first one to develop a technique of classical mechanics
where a Hamiltonian system is geometrized by transforming it into a
geodesic flow on a suitable manifold with a convenient Riemannian metric
jacobi . The two key steps in obtaining the geometrization of a
Hamiltonian system are the introduction of a conformal transformation of
the metric and the rescaling of the time parameter biesiada1 (and, for
more details, Chapter 6). The reformulation of dynamics in terms of a
geodesic problem allows the application of a wide range of well-known
geometrical techniques in the investigation of the solution space and
properties of equations of motions. The power of the Jacobi
reformulation is that all of the dynamical information is collected into
a single geometric object - the manifold on which geodesic flow is
induced - in which all the available manifest symmetries are retained.
For instance, integrability of the system is connected with the
existence of Killing vectors and tensors on this manifold biesiada2 ;
uggla .

In this section we study the trajectories of the system on @xmath . We
emphasize ED can be derived from a standard principle of least action
(of Maupertuis-Euler-Lagrange-Jacobi type) caticha1 ; arnold . The main
differences are that the dynamics being considered here, namely ED, is
defined on a space of probability distributions @xmath , not on an
ordinary vectorial space @xmath and the standard coordinates @xmath of
the system are replaced by statistical macrovariables @xmath . The
geodesic equations for the macrovariables of the Gaussian ED model are
given by,

  -- -------- -- -------
     @xmath      (378)
  -- -------- -- -------

with @xmath , @xmath ,…, @xmath . The geodesic equations are nonlinear
second order coupled ordinary differential equations. They describe a
reversible dynamics whose solution is the trajectory between an initial
and a final macrostate. The trajectory can be equally well traversed in
both directions.

### xxxi.1 Geodesics on @xmath

We determine the explicit form of ( 378 ) for the pairs of statistical
coordinates @xmath , @xmath . Substituting the expression of the
Christoffel connection coefficients into ( 378 ), the geodesic equations
for the macrovariables @xmath and @xmath associated to the microstate
@xmath become,

  -- -------- -- -------
     @xmath      (379)
  -- -------- -- -------

with @xmath , @xmath , @xmath , @xmath and @xmath , @xmath , @xmath .
The @xmath Gaussians are not coupled to each other, however each
Gaussian is characterized by coupled macrovariables @xmath and @xmath .
Equation ( 379 ) describes a set of coupled ordinary differential
equations, whose solutions are

  -- -- --
        
  -- -- --

  -- -- -- -------
           (380)
  -- -- -- -------

The quantities @xmath , @xmath , @xmath , @xmath are real integration
constants that can be evaluated upon specification of boundary
conditions. We are interested in the stability of the trajectories on
@xmath . It is known arnold that the Riemannian curvature of a manifold
is intimately related to the behavior of geodesics on it. If the
Riemannian curvature of a manifold is negative, geodesics (initially
parallel) rapidly diverge from one another. We observe that since every
maximal geodesic (one that cannot be extended to any larger interval) is
well-defined for all temporal parameters @xmath , @xmath constitute a
geodesically complete manifold lee . It is therefore a natural setting
within which one may consider global questions and search for a weak
criterion of chaos cipriani .

## Xxxii Exponential divergence of the Jacobi vector field intensity

The actual interest of the Riemannian formulation of the dynamics stems
from the possibility of studying the instability of natural motions
through the instability of geodesics of a suitable manifold, a
circumstance that has several advantages. First of all a powerful
mathematical tool exists to investigate the stability or instability of
a geodesic flow: the Jacobi-Levi-Civita equation for geodesic spread
carmo . The JLC-equation describes covariantly how nearby geodesics
locally scatter. It is a familiar object both in Riemannian geometry and
theoretical physics (it is of fundamental interest in experimental
General Relativity). Moreover the JLC-equation relates the stability or
instability of a geodesic flow with curvature properties of the ambient
manifold, thus opening a wide and largely unexplored field of
investigation of the connections among geometry, topology and geodesic
instability, hence chaos.

Consider the behavior of the one-parameter (at fixed @xmath and @xmath ,
@xmath ) family of neighboring geodesics @xmath where

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

with @xmath , @xmath , @xmath , @xmath and @xmath , @xmath , @xmath .
The relative geodesic spread on a (non-maximally symmetric) curved
manifold as @xmath is characterized by the Jacobi-Levi-Civita equation,
the natural tool to tackle dynamical chaos clarke ; carmo ,

  -- -------- -- -------
     @xmath      (382)
  -- -------- -- -------

where the Jacobi vector field @xmath is defined as,

  -- -------- -- -------
     @xmath      (383)
  -- -------- -- -------

Notice that the JLC-equation appears intractable already at rather small
@xmath . For isotropic manifolds, the JLC-equation can be reduced to the
simple form,

  -- -------- -- -------
     @xmath      (384)
  -- -------- -- -------

where @xmath is the constant value assumed throughout the manifold by
the sectional curvature. The sectional curvature of manifold @xmath is
the @xmath -dimensional generalization of the Gaussian curvature of
two-dimensional surfaces of @xmath . If @xmath , unstable solutions of
equation ( 384 ) assumes the form

  -- -------- -- -------
     @xmath      (385)
  -- -------- -- -------

once the initial conditions are assigned as @xmath , @xmath and @xmath .
Equation ( 382 ) forms a system of @xmath coupled ordinary differential
equations linear in the components of the deviation vector field ( 383 )
but nonlinear in derivatives of the metric ( 362 ). It describes the
linearized geodesic flow: the linearization ignores the relative
velocity of the geodesics. When the geodesics are neighboring but their
relative velocity is arbitrary, the corresponding geodesic deviation
equation is the so-called generalized Jacobi equation chicone ;
hodgkinson . The nonlinearity is due to the existence of
velocity-dependent terms in the system. Neighboring geodesics accelerate
relative to each other with a rate directly measured by the curvature
tensor @xmath . Substituting ( LABEL:solns ) in ( 382 ) and neglecting
the exponentially decaying terms in @xmath and its derivatives,
integration of ( 382 ) leads to the following asymptotic expression of
the Jacobi vector field intensity,

  -- -------- -- -------
     @xmath      (386)
  -- -------- -- -------

As a side remark, we point out that if we consider the special case
where different Gaussians are characterized by the same initial
conditions leading to the same @xmath @xmath @xmath , @xmath , @xmath ,
@xmath and @xmath , @xmath ,.., @xmath , equation ( 386 ) becomes,

  -- -------- -- -------
     @xmath      (387)
  -- -------- -- -------

We conclude that the geodesic spread on @xmath is described by means of
an exponentially divergent Jacobi vector field intensity @xmath , a
classical feature of chaos. In our approach the quantity @xmath ,

  -- -- -- -------
           (388)
  -- -- -- -------

would play the role of the conventional Lyapunov exponents.

## Xxxiii Linearity of the information geometrodynamical entropy

The statistical manifold @xmath is the space of probability
distributions @xmath labeled by @xmath statistical parameters @xmath .
These parameters are the coordinates for the point @xmath , and in these
coordinates a volume element @xmath reads,

  -- -------- -- -------
     @xmath      (389)
  -- -------- -- -------

The volume of an extended region @xmath of @xmath is defined by,

  -- -------- -- -------
     @xmath      (390)
  -- -------- -- -------

where @xmath and @xmath are given in ( LABEL:solns ). The quantity that
encodes relevant information about the stability of neighboring volume
elements is the average volume @xmath ,

  -- -------- -- -------
     @xmath      (391)
  -- -------- -- -------

Again, as a side remark, we point out that if we consider the special
case where different Gaussians are characterized by the same initial
conditions, equation ( 386 ) becomes,

  -- -------- -- -------
     @xmath      (392)
  -- -------- -- -------

This asymptotic regime of evolution in ( 391 ) describes the exponential
increase of average volume elements on @xmath . The exponential
instability characteristic of chaos forces the system to rapidly explore
large areas (volumes) of the statistical manifold. It is interesting to
note that this asymptotic behavior appears also in the conventional
description of quantum chaos where the entropy increases linearly at a
rate determined by the Lyapunov exponents ruelle . The linear increase
of entropy as a quantum chaos criterion was introduced by Zurek and Paz
zurek . In our information-geometric approach a relevant quantity that
can be useful to study the degree of instability characterizing the ED
model is the information-geometric entropy defined as,

  -- -------- -- -------
     @xmath      (393)
  -- -------- -- -------

Substituting ( 391 ) in ( 393 ), we obtain

  -- -- -- -------
           (394)
  -- -- -- -------

In the special case of same initial conditions for different Gaussians,
( 394 ) becomes,

  -- -------- -- -------
     @xmath      (395)
  -- -------- -- -------

The entropy @xmath in ( 394 ) is the asymptotic limit of the natural
logarithm of the statistical weight @xmath defined on @xmath . Its
linear growth in time is reminiscent of the aforementioned quantum chaos
criterion. Indeed, equation ( 394 ) may be considered the
information-geometric analog of the Zurek-Paz chaos criterion.

In conclusion, we have shown that

  -- -------- -- -------
     @xmath      (396)
  -- -------- -- -------

Each indicator of chaos behaves as expected: @xmath is negative (this is
a sufficient but not necessary condition for chaos), @xmath grows
exponentially in @xmath and, @xmath grows linearly in @xmath and is
proportional to the sum of positive Lyapunov exponents of the system.
Furthermore, it is reasonable to state that the temporal complexity
(chaoticity) of a system ought to grow linearly as a function of the
number of its variables feldy and it seems reasonable to assume that
this complexity should not depend on the special choice of the initial
conditions of the system but only on its dynamical evolution. The
selection of a special set of initial conditions should not affect the
degree of chaoticity of a dynamical system. Because of these
considerations, we are allowed to choose any special set of convenient
initial conditions and evaluate the behavior of the indicators of chaos
in such special case. As we have showed, assuming the same initial
conditions for different Gaussians, we obtain

  -- -------- -- -------
     @xmath      (397)
  -- -------- -- -------

The Ricci scalar curvature @xmath grows as a function of the number of
the microvariables of the system, the information-geometric entropy
@xmath grows linearly as a function of the microvariables of the system
and the Jacobi vector field intensity @xmath grows exponentially as a
function of the microvariables of the system: @xmath , @xmath and @xmath
behave as proper indicators of chaoticity and are proportional to the
number of Gaussian-distributed microstates of the system. This
proportionality leads to the conclusion that there exists a formal link
among these information-geometric measures of chaoticity. Formally,

  -- -- -- -------
           (398)
  -- -- -- -------

Equation ( 398 ), together with the information-geometric analog of the
Zurek-Paz quantum chaos criterion, equation ( 394 ), represent the
fundamental results of this work. We are aware that equation ( 398 ) is
reliable in the restrictive assumption of Gaussianity and for very
special initial conditions. However, we believe that with some
additional technical machinery, more general conclusions can be achieved
and this connection among indicators of chaoticity may be strengthened.
Furthermore, we believe our theoretical modelling scheme may be used to
describe actual systems where transitions from quantum to classical
chaos scenario occur, but this requires additional analysis. In the
following section, we briefly consider some similarities among the von
Neumann, Kolmogorov-Sinai and Information-Geometrodynamical entropies.

## Xxxiv On the von Neumann, Kolmogorov-Sinai and information
geometrodynamical Entropies

In conventional approaches to chaos, the notion of entropy is
introduced, in both classical and quantum physics, as the missing
information about the systems fine-grained state jaynes ; caves .
Following the first work in reference caves , we consider a classical
system and suppose that the phase space is partitioned into very
fine-grained cells of uniform volume @xmath , labelled by an index
@xmath . If one does not know which cell the system occupies, one
assigns probabilities @xmath to the various cells; equivalently, in the
limit of infinitesimal cells, one can use a phase-space density @xmath .
Then, in a classical chaotic evolution, the asymptotic expression of the
information needed to characterize a particular coarse-grained
trajectory out to time @xmath is given by the Shannon information
entropy (measured in bits) caves ,

  -- -- -- -------
           (399)
  -- -- -- -------

where @xmath is the phase-space density and @xmath is the probability
for the corresponding coarse-grained trajectory. @xmath is the missing
information about which fine-grained cell the system occupies. Equation
( 399 ) can be explained with the following approximate reasoning caves
: the number of pieces in the partition of the evolved pattern grows as
@xmath (i.e., @xmath @xmath ), each piece having approximately the same
phase-space volume and, therefore, the same probability @xmath . That
said, equation ( 399 ) follows in a straightforward way. However, we
think this picture is not the most clear one. We believe that within our
IGAC, a better and clearer understanding of what actually is happening
may be achieved. The quantity @xmath represents the linear rate of
information increase and it is called the Kolmogorov-Sinai entropy (or
metric entropy). @xmath quantifies the degree of classical chaos (for a
more detailed discussion about @xmath , see Chapter 4). It is worthwhile
emphasizing that the quantity that grows asymptotically as @xmath is
really the average of the information on the left side of equation ( 399
). This distinction can be ignored however, if we assume that the
chaotic system has roughly constant Lyapunov exponents over the
accessible region of phase space.

In quantum mechanics the fine-grained alternatives are normalized state
vectors in Hilbert space. From a set of probabilities for various state
vectors, one can construct a density operator

  -- -------- -- -------
     @xmath      (400)
  -- -------- -- -------

The normalization of the density operator, @xmath , implies that the
eigenvalues make up a normalized probability distribution. The von
Neumann entropy (natural generalization of both Boltzmann’s and
Shannon’s entropy) of the density operator @xmath (measured in bits)
stenholm ; caves2 ,

  -- -------- -- -------
     @xmath      (401)
  -- -------- -- -------

can be thought of as the missing information about which eigenvector the
system is in. Entropy quantifies the degree of unpredictability about
the system’s fine-grained state. In quantum mechanics, the von Neumann
entropy plays a role analogous to that played by the Shannon entropy in
classical probability theory. They are both monotone functionals of the
state. The von Neumann entropy reduces to the Shannon entropy for
diagonal density matrices. However, in general the von Neumann entropy
is a subtler object than its classical counterpart. The quantity @xmath
in ( 401 ) can be interpreted as the non-commutative (quantum theory is
a non-commutative probability theory) quantum analog of the
Kolmogorov-Sinai dynamical entropy, the so-called quantum dynamical
entropy benny . Examples of quantum dynamical entropies applied to
quantum chaos and quantum information theory are the Alicki-Fannes (AF)
alicki entropy and the Connes-Narnhofer-Thirring (CNT) connes entropy.
Both the AF and CNT entropy coincide with the KS entropy on classical
dynamical systems. They also coincide on finite-dimensional quantum
systems. However, they differ when moving from finite to infinite
quantum systems. Furthermore, recall that decoherence is the loss of
phase coherence between the set of preferred quantum states in the
Hilbert space of the system due to the interaction with the environment.
Moreover, decoherence induces transitions from quantum to classical
systems. Therefore, classicality is an emergent property of an open
quantum system. Motivated by such considerations, Zurek and Paz
investigated implications of the process of decoherence for quantum
chaos.

They considered a chaotic system, a single unstable harmonic oscillator
characterized by a potential @xmath ( @xmath is the Lyapunov exponent),
coupled to an external environment. In the reversible classical limit
zurek2 , the von Neumann entropy of such a system increases linearly at
a rate determined by the Lyapunov exponent,

  -- -------- -- -------
     @xmath      (402)
  -- -------- -- -------

Notice that the consideration of @xmath uncoupled identical unstable
harmonic oscillators characterized by potentials @xmath @xmath would
simply lead to

  -- -------- -- -------
     @xmath      (403)
  -- -------- -- -------

The resemblance of equations ( 394 ) and ( 403 ) is remarkable. In what
follows, we apply our information geometrical method to a set of two (
@xmath ) uncoupled inverted anisotropic harmonic oscillators and show we
obtain asymptotic linear IGE growth. The case for an arbitrary @xmath
-set in three dimensions is presented in the Appendix.

## Xxxv The information geometry of a set of @xmath-uncoupled inverted
harmonic oscillators (IHO)

In this section, our objective is to characterize chaotic properties of
a set of two one-dimensional inverted harmonic oscillators, each with
different frequency @xmath . We will study the asymptotic behavior of
the geometrodynamical entropy and the functional dependence of the Ricci
scalar curvature of the @xmath -dimensional manifold @xmath underlying
the ED model of the IHOs on the frequencies @xmath , @xmath , @xmath .
In Chapter 6, we explored the possibility of using well established
principles of inference to derive Newtonian dynamics from relevant prior
information codified into an appropriate statistical manifold cafaro5 .
In what follows, we introduce the basics of the general formalism for a
set of @xmath IHOs. This approach is similar (mathematically but not
conceptually) to the geometrization of Newtonian dynamics used in the
Riemannian geometrodynamical to chaos casetti ; biesiada3 .

### xxxv.1 Informational geometrization of Newtonian dynamics

In what follows, we apply the general formalism developed in Chapter 6
to our specific problem under consideration. The system under
investigation has @xmath degrees of freedom and a point on the @xmath
dimensional configuration space manifold @xmath is parametrized by the
@xmath Lagrangian coordinates @xmath . Moreover, the system under
investigation is described by the Lagrangian @xmath ,

  -- -------- -- -------
     @xmath      (404)
  -- -------- -- -------

so that the Hamiltonian function @xmath is a constant of motion. For the
sake of simplicity, let us set @xmath . According to the principle of
stationary action - in the form of Maupertuis - among all the possible
isoenergetic paths @xmath with fixed end points, the paths that make
vanish the first variation of the action functional

  -- -------- -- -------
     @xmath      (405)
  -- -------- -- -------

are natural motions. As the kinetic energy @xmath is a homogeneous
function of degree two, we have @xmath , and Maupertuis’ principle reads

  -- -------- -- -------
     @xmath      (406)
  -- -------- -- -------

The manifold @xmath is naturally given a proper Riemannian structure. In
fact, let us consider the matrix

  -- -------- -- -------
     @xmath      (407)
  -- -------- -- -------

so that Maupertuis’ principle becomes

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (408)
  -- -------- -------- -- -------

thus motions are geodesics of @xmath , provided we define @xmath as its
arclength. The metric tensor @xmath of @xmath is then defined by

  -- -------- -- -------
     @xmath      (409)
  -- -------- -- -------

where @xmath is a natural base of @xmath - the cotangent space at the
point @xmath - in the local chart @xmath . This is known as the Jacobi
metric (or kinetic energy metric). Denoting by @xmath the canonical
Levi-Civita connection, the geodesic equation is defined as stewart ,

  -- -------- -- -------
     @xmath      (410)
  -- -------- -- -------

where @xmath is the tangent vector of the allowed paths @xmath at
constant energy @xmath . In the local chart @xmath , equation ( 410 )
becomes,

  -- -------- -- -------
     @xmath      (411)
  -- -------- -- -------

where the Christoffel coefficients are the components of @xmath defined
by

  -- -------- -- -------
     @xmath      (412)
  -- -------- -- -------

with @xmath . Since @xmath , from the geodesic equation we obtain

  -- -------- -- -------
     @xmath      (413)
  -- -------- -- -------

whereupon using @xmath , we verify that ( 413 ) reduces to

  -- -------- -- -------
     @xmath      (414)
  -- -------- -- -------

Equation ( 414 ) are Newton’s equations. It is worthwhile emphasizing
that the transformation to geodesic motion on a curved statistical
manifold is obtained in two key steps: the conformal transformation of
the metric , @xmath @xmath and, the rescaling of the temporal evolution
parameter , @xmath .

### xxxv.2 Two uncoupled inverted one-dimensional harmonic oscillators

As a simple physical example, we examine the IG associated with a set of
two one dimensional IHOs. In this case, the metric tensor @xmath
appearing in ( 407 ) takes the form

  -- -------- -- -------
     @xmath      (415)
  -- -------- -- -------

where the function @xmath is given by,

  -- -------- -- -------
     @xmath      (416)
  -- -------- -- -------

Hence the metric tensor @xmath on @xmath becomes,

  -- -------- -- -------
     @xmath      (417)
  -- -------- -- -------

Using the standard definition of the Ricci scalar ( 365 ), we obtain

  -- -------- -- -------
     @xmath      (418)
  -- -------- -- -------

For @xmath , the scalar curvature ( 418 ) is always negative,

  -- -- -- -------
           (419)
  -- -- -- -------

However, in presence of distinct frequency values, @xmath , it is
possible to properly choose the @xmath ’s so that @xmath becomes either
negative or positive. In addition, we notice that the manifold
underlying the IHO model is anisotropic since its associated Weyl
projective curvature tensor components are non-vanishing. For the
special case, @xmath , we obtain

  -- -------- -- -------
     @xmath      (420)
  -- -------- -- -------

Clearly, the frequency parameter @xmath drives the degree of anisotropy
of the statistical manifold @xmath and, as expected, in the limit of
vanishing @xmath , we recover the flat ( @xmath ), isotropic ( @xmath )
Euclidean manifold characterized by metric @xmath . This result is a
concrete example of the fact that conformal transformations change the
degree of anisotropy of the ambient statistical manifold underlying the
Newtonian dynamics. Our only remaining task is to compute the
information geometrodynamical entropy @xmath , defined as

  -- -- -- -------
           (421)
  -- -- -- -------

The quantity @xmath appearing in ( 421 ) is the average volume element,
defined by

  -- -- -- -------
           (422)
  -- -- -- -------

with the statistical volume element @xmath given by

  -- -------- -------- -- -------
     @xmath   @xmath      (423)
                          
              @xmath      
  -- -------- -------- -- -------

Recall that the two Newtonian equations of motion for each inverted
harmonic oscillator are given by,

  -- -------- -- -------
     @xmath      (424)
  -- -------- -- -------

Hence, the asymptotic behavior of such macrovariables on manifold @xmath
is given by,

  -- -------- -- -------
     @xmath      (425)
  -- -------- -- -------

Substituting @xmath and @xmath into ( 423 ), we obtain

  -- -------- -- -------
     @xmath      (426)
  -- -------- -- -------

By direct computation, we find the average of ( 426 ) is given by,

  -- -------- -- -------
     @xmath      (427)
  -- -------- -- -------

Assuming as a working hypothesis that @xmath , we obtain

  -- -------- -- -------
     @xmath      (428)
  -- -------- -- -------

Finally, substituting ( 428 ) in ( 421 ), we obtain

  -- -------- -- -------
     @xmath      (429)
  -- -------- -- -------

It is clear that the information-geometrodynamical entropy @xmath
exhibits classical linear behavior in the asymptotic limit, with
proportionality coefficient @xmath @xmath ,

  -- -------- -- -------
     @xmath      (430)
  -- -------- -- -------

Equation ( 430 ) expresses the asymptotic linear growth of our
information geometrodynamical entropy for the IHO system considered.
This result (for @xmath ) extends the result of Zurek-Paz ( 403 ) in a
classical information-geometric setting. This result, together with my
previous works cafaro2 ; cafaro3 lend substantial support for the IGAC
approach advocated in the present Chapter.

## Xxxvi Conclusions

A Gaussian ED statistical model has been constructed on a @xmath
-dimensional statistical manifold @xmath . The macro-coordinates on the
manifold are represented by the expectation values of microvariables
associated with Gaussian distributions. The geometric structure of
@xmath was studied in detail. It was shown that @xmath is a curved
manifold of constant negative Ricci curvature @xmath . The geodesics of
the ED model are hyperbolic curves on @xmath . A study of the stability
of geodesics on @xmath was presented. The notion of statistical volume
elements was introduced to investigate the asymptotic behavior of a
one-parameter family of neighboring volumes @xmath . An
information-geometric analog of the Zurek-Paz chaos criterion was
suggested. It was shown that the behavior of geodesics is characterized
by exponential instability that leads to chaotic scenarios on the curved
statistical manifold. These conclusions are supported by a study based
on the geodesic deviation equations and on the asymptotic behavior of
the Jacobi vector field intensity @xmath on @xmath . A Lyapunov exponent
analog similar to that appearing in the Riemannian geometric approach to
chaos was suggested as an indicator of chaoticity. On the basis of our
analysis a relationship among an entropy-like quantity, chaoticity and
curvature is proposed, suggesting to interpret the statistical curvature
as a measure of the entropic dynamical chaoticity.

The results obtained in this work are significant, in our opinion, since
a rigorous relation among curvature, Lyapunov exponents and
Kolmogorov-Sinai entropy is still under investigation kawabe . In
addition, there does not exist a well defined unifying characterization
of chaos in classical and quantum physics caves due to fundamental
differences between the two theories. In addition, the role of curvature
in statistical inference is even less understood. The meaning of
statistical curvature for a one-parameter model in inference theory was
introduced in efron . Curvature served as an important tool in the
asymptotic theory of statistical estimation. Therefore the implications
of this work is twofold. Firstly, it helps understanding possible future
use of the statistical curvature in modelling real processes by relating
it to conventionally accepted quantities such as entropy and chaos. On
the other hand, it serves to cast what is already known in physics
regarding curvature in a new light as a consequence of its proposed link
with inference.

As a simple physical example, we considered the information-geometry
@xmath associated with a set of two inverted harmonic oscillators. It
was determined that in the limit of a flat frequency spectrum ( @xmath
), the scalar curvature @xmath is constantly negative. In the case of
distinct frequencies, i.e., @xmath , it is possible - for appropriate
choices of @xmath and @xmath - to obtain either negative or positive
values of @xmath . Moreover, it was shown that @xmath is an anisotropic
manifold since the Weyl projective curvature tensor has a non-vanishing
component @xmath . It was found that the information geometrodynamical
entropy of the IHO system exhibits asymptotic linear growth. This IHO
example is generalized to arbitrary values of @xmath in the Appendix.

The descriptions of a classical chaotic system of arbitrary interacting
degrees of freedom, deviations from Gaussianity and chaoticity arising
from fluctuations of positively curved statistical manifolds are being
investigated cafaro4 .

## Xxxvii Appendix

### xxxvii.1 The set of @xmath uncoupled inverted anisotropic
three-dimensional harmonic oscillators

#### xxxvii.1.1 Ohmic frequency spectrum

We now generalize the results obtained in this Chapter for a set of
@xmath IHOs. The information metric on the @xmath -dimensional
statistical manifold @xmath is given by

  -- -------- -- -------
     @xmath      (431)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (432)
  -- -------- -- -------

The information geometrodynamical entropy @xmath is defined as

  -- -------- -- -------
     @xmath      (433)
  -- -------- -- -------

where the average volume element @xmath is given by

  -- -------- -- -------
     @xmath      (434)
  -- -------- -- -------

and the statistical volume element @xmath is defined as

  -- -------- -- -------
     @xmath      (435)
  -- -------- -- -------

Substituting ( 434 ) and ( 435 ) in ( 433 ) we obtain the general
expression for @xmath ,

  -- -------- -- -------
     @xmath      (436)
  -- -------- -- -------

To evaluate ( 436 ) we observe @xmath can be written as

  -- -------- -------- -- -------
     @xmath   @xmath      
                          
              @xmath      
                          
              @xmath      (437)
  -- -------- -------- -- -------

Since the @xmath -Newtonian equations of motions for each IHO are given
by

  -- -------- -- -------
     @xmath      (438)
  -- -------- -- -------

the asymptotic behavior of such macrovariables on manifold @xmath is
given by

  -- -------- -- -------
     @xmath      (439)
  -- -------- -- -------

We therefore obtain

  -- -------- -- -------
     @xmath      (440)
  -- -------- -- -------

Upon averaging ( 440 ) we find

  -- -- -- -------
           (441)
  -- -- -- -------

where @xmath . As a working hypothesis, we assume @xmath @xmath , @xmath
,.., @xmath . Furthermore, assume that @xmath so that the spectrum of
frequencies becomes continuum and, as an additional working hypothesis,
assume this spectrum is linearly distributed (Ohmic frequency spectrum),

  -- -------- -- -------
     @xmath      (442)
  -- -------- -- -------

Therefore, we obtain

  -- -------- -- -------
     @xmath      (443)
  -- -------- -- -------

Finally, substituting ( 443 ) into ( 433 ), we obtain the remarkable
result

  -- -------- -- -------
     @xmath      (444)
  -- -------- -- -------

Equation ( 444 ) displays the asymptotic, linear information
geometrodynamical entropy growth of the generalized @xmath -set of
inverted harmonic oscillators and extends the result of Zurek-Paz to an
arbitrary set of anisotropic inverted harmonic oscillators zurek in a
classical information-geometric setting.

The Ohmic frequency spectrum case leads to asymptotic IGE growth.
However, in this case, we are not able to compactify the parameter space
of our statistical model. The compactification of the parameter space
(and therefore of the statistical manifold) is required for true chaos
where the folding mechanism must be present (indeed, the lack of such
feature has been one of the most important criticisms to the Zurek-Paz
model). The folding mechanism required for true chaos is not even
restored in a statistical sense (averaging over @xmath and @xmath ),

  -- -------- -- -------
     @xmath      (445)
  -- -------- -- -------

This missing feature may lead us to consider in the near future the
possibility of considering other frequency spectra.

Chapter 8: Concluding Remarks and Future Research Directions

I present concluding remarks emphasizing strengths and weakness of my
approach and I address possible further research directions.

## Xxxviii Concluding Remarks

In this doctoral dissertation, I considered two important questions:

First, are laws of physics practical rules to process information about
the world using geometrical methods? Are laws of physics rules of
inference?

Second, since a unifying framework to describe chaotic dynamics in
classical and quantum domains is missing, is it possible to construct a
new information-geometric model, to develop new tools so that a unifying
framework is provided or, at least, new insights and new understandings
are given?

After setting the scene of my thesis and after stating the problem and
its motivations, I reviewed the basic elements of the maximum relative
entropy formalism (ME method) and recall the basics of Riemannian
geometry with special focus to its application to probability theory
(this is known as Information Geometry, IG). IG and ME are the
fundamental tools that Prof. Ariel Caticha has used to build a form of
information-constrained dynamics on statistical manifolds to investigate
the possibility that Einstein’s general theory of gravity (or any
classical or quantum theory of physics) may emerge as a macroscopic
manifestation of an underlying microscopic statistical structure. This
dynamics is known in the literature as Entropic Dynamics (ED).
Therefore, since ED was an important element of this thesis, I reviewed
the key-points of such dynamics, emphasizing the most relevant points
that I used in my own information geometrodynamical approach to chaos
(IGAC). Of course, before introducing my IGAC, I briefly reviewed the
basics of the conventional Riemannian geometrodynamics approach to chaos
and discussed the notion of chaos in physics in general. After this long
background information that was needed because of the originality and
novelty of these topics, I started with my original contributions. Two
entropic dynamical models are considered. The geometric structure of the
statistical manifolds underlying these models is studied. It is found
that in both cases, the resulting metric manifolds are negatively
curved. Moreover, the geodesics on each manifold are described by
hyperbolic trajectories. A detailed analysis based on the
Jacobi-Levi-Civita equation for geodesic spread (JLC equation) is used
to show that the hyperbolicity of the manifolds leads to chaotic
exponential instability. A comparison between the two models leads to a
relation among scalar curvature of the manifold ( @xmath ), Jacobi field
intensity ( @xmath ) and information geometrodynamical entropy (IGE,
@xmath ). The IGE entropy is proposed as a brand new measure of
chaoticity.

  First Contribution  

    cafaro1 ; cafaro2 ; cafaro3 : I suggest that these three quantities,
    @xmath , @xmath , and @xmath are useful indicators of chaoticity for
    chaotic dynamical systems on curved statistical manifolds.
    Furthermore, I suggest a classical  information-geometric criterion
    of linear information geometrodynamical entropy growth in analogy
    with the Zurek-Paz quantum chaos criterion.

In collaboration with Prof. Ariel Caticha, I show that the ED formalism
is not purely a mathematical framework; it is indeed a general
theoretical scheme where conventional Newtonian dynamics can be obtained
as a special limiting case. Newtonian dynamics is derived from prior
information codified into an appropriate statistical model. The basic
assumption is that there is an irreducible uncertainty in the location
of particles so that the state of a particle is defined by a probability
distribution. The corresponding configuration space is a statistical
manifold the geometry of which is defined by the information metric. The
trajectory follows from a principle of inference, the method of Maximum
Entropy. No additional ”physical” postulates such as an equation of
motion, or an action principle, nor the concepts of momentum and of
phase space, not even the notion of time, need to be postulated. The
resulting entropic dynamics reproduces the Newtonian dynamics of any
number of particles interacting among themselves and with external
fields. Both the mass of the particles and their interactions are
explained as a consequence of the underlying statistical manifold.

  Second Contribution  

    caticha-cafaro : The derivation of the Newtonian dynamics from first
    principles of probable inference and information geometric methods
    is another original contribution of my work in collaboration with
    Prof. Ariel Caticha .

Third, I extend my study of chaotic systems (information
geometrodynamical approach to chaos, IGAC) to an ED Gaussian model
describing an arbitrary system of @xmath degrees of freedom. It is shown
that the hyperbolicity of a non-maximally symmetric @xmath -dimensional
statistical manifold @xmath underlying the ED Gaussian model leads to
linear information-geometrodynamical entropy growth and to exponential
divergence of the Jacobi vector field intensity. As a special physical
application, the information geometrodynamical scheme is applied to
investigate the chaotic properties of a set of @xmath -uncoupled
three-dimensional anisotropic inverted harmonic oscillators coupled to
an internal environment and I show that the asymptotic behavior of the
information-geometrodynamical entropy is characterized by linear growth.
Finally, considerations concerning the anisotropy of the statistical
manifold underlying such physical system and its relationship with the
spectrum of frequencies of the oscillators are carried out.

  Third Contribution  

    cafaro4 : I compute the asymptotic temporal behavior of the
    information geometrodynamical entropy of a set of @xmath -uncoupled
    three-dimensional anisotropic inverted harmonic oscillators (IHOs)
    characterized by an Ohmic distributed frequency spectrum and I
    suggest the classical information-geometric analogue of the
    Zurek-Paz quantum chaos criterion in its classical reversible limit
    .

I am aware that several points in my IGAC need deeper understanding and
analysis, however I hope that my work convincingly shows that this
information-geometric approach may be useful in providing a unifying
criterion of chaos, thus deserving further research and developments.

## Xxxix Future Research Directions

It is evident that my work requires additional improvements and deeper
understanding of some of its results in view of its comparison to more
recent results obtained in more orthodox approaches.

I am working on the possibility of extending the IGAC to chaotic systems
with an arbitrary number of interacting microscopic degrees of freedom.
This would lead to the problem of considering an information-geometric
line element with non trivial off-diagonal matrix elements. This is
relevant because it allow us to study not only macroscopic chaos but
also microscopic dynamics characterizing several known chaotic Newtonian
dynamical systems. In addition, this would increase the understanding of
the relationship between the microscopic and macroscopic behaviors of a
physical system.

I am considering the IGAC arising from arbitrary (non-uniform) prior
probability distributions together with possible deviations from
Gaussianity and chaoticity arising from fluctuations of positively
curved statistical manifolds. I would like to take into consideration
the study of more realistic physical, biological, complex systems in
general (brain dynamics, finance, etc.). I am searching for a better
understanding of the possible relevance of my formalism to key concepts
in modern quantum information theory: chaos, decoherence and
entanglement. I am thinking about the possibility of employing my IGAC
to describe situations of transitions from quantum to classical chaotic
physical systems.

These last two points require a generalization of the IGAC in an
appropriate manner to facilitate the study of quantum mechanical
systems. In collaboration with Dr. Saleem Ali, I already started some
additional works in this direction. We are trying to construct a quantum
Hilbert space from a classical curved statistical manifold. This
extension is implemented via the introduction of complex and symplectic
structure tensors that are compatible with the underlying Riemannian
geometry induced by the Fisher-Rao information metric. Vectors on the
resulting manifold @xmath are interpreted as state vectors associated
with points (i.e., probability density functions) on @xmath .

As a final remark, I would like to point out that my primary concern in
the near future is refining some of my latest controversial work on
regular and chaotic quantum energy level statistics and, possibly,
extending the application of the IGAC to soft chaos regimes cafaro5 .

###### Acknowledgements. There are many people I wish to thank. Friends,
relatives, colleagues, professors and financial sponsors: I am grateful
to you all. I am deeply indebted to my supervisor, Prof. Ariel Caticha,
for allowing me the freedom to pursue my own interests. I am grateful to
all the dissertation Committee members: Prof. Ariel Caticha, Prof. Akira
Inomata, Prof. John Kimball, Prof. Kevin Knuth and Prof. Carlos
Rodriguez. I thank Prof. Ali Mohammad-Djafari (Paris, France), Prof.
Kevin Knuth (Saratoga, USA), Prof. Stefano Mancini and Prof. Fabio
Marchesoni (Erice, Italy) for allowing me the opportunity to present my
work on chaos at their Conferences. I thank Prof. Carlo Bradaschia and
Dr. Giancarlo Cella (my former advisor and co-advisor at the University
of Pisa, Italy) for their interest in my current research and for having
me back there to present my works. I am grateful to Prof. Frederic
Barbaresco (Thales Air Defence, Surface Radar Business Line, France),
Prof. Rafael Gutierrez (Director of the Centre for Complex Systems,
Columbia-South America), Prof. Michael Frey (Department of Mathematics,
Bucknell University, Pennsylvania- USA) and Prof. Yasunori Nishimori
(National Institute of Advanced Industrial Science and Technology,
Tsukaba-Japan) for their deep interest in my work presented in this
thesis. I thank the co-authors of some of my works, Dr. Saleem Ali,
Prof. Salvatore Capozziello, Dr. Christian Corda and Mr. Adom Giffin.
Finally, I wish to express my immense gratitude to the people who were
with me from the beginning of this long journey: my unconditioned
sponsors, my parents Peppino (from Bellona, Italy) and Carmelina (from
Vitulazio, Italy) and, my brother Joe.
