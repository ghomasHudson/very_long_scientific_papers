# Acknowledgements

I would first and foremost like to thank my supervisor Alban Ponse for
the big amounts of time and energy he put into guiding me through this
project. His advice has been invaluable to me and his enthusiasm has
been a huge motivation for me throughout.

A thank you also goes out to Jan van Eijck for pointing me in the right
direction halfway through the project.

Finally I would like to thank my entire thesis committee, consisting of
Alban Ponse, Paul Dekker, Jan van Eijck, Sara Uckelman and Benedikt
Löwe, for taking the time to read and grade my thesis.

— Lars Wortel, August 2011

###### Contents

-    1 Introduction
    -    1.1 What are side effects?
    -    1.2 What are steering fragments?
    -    1.3 Related work
    -    1.4 Overview of this thesis
-    2 Preliminaries
    -    2.1 Introduction
    -    2.2 Toy language
    -    2.3 Propositional Dynamic Logic
    -    2.4 Quantified Dynamic Logic
-    3 Modifying QDL to DLA @xmath
    -    3.1 Introducing DLA @xmath
    -    3.2 A working example
    -    3.3 Re-introducing WHILE
        -    3.3.1 The WHILE command
        -    3.3.2 WHILE in DLA @xmath
        -    3.3.3 Looping behavior and abnormal termination
-    4 Terminology
    -    4.1 Formulas, instructions and programs
    -    4.2 Normal forms of formulas
    -    4.3 Deterministic programs and canonical forms
-    5 The logic of formulas in DLA @xmath
    -    5.1 Proposition algebra
    -    5.2 Short-Circuit Logics
    -    5.3 Repetition-Proof Short-Circuit Logic
-    6 A treatment of side effects
    -    6.1 Introduction
    -    6.2 Side effects in single instructions
    -    6.3 Side effects in basic instructions
    -    6.4 Side effects in programs
    -    6.5 Side effects outside steering fragments
-    7 A classification of side effects
    -    7.1 Introduction
    -    7.2 Marginal side effects
        -    7.2.1 Introduction
        -    7.2.2 Marginal side effects in single instructions
        -    7.2.3 Marginal side effects caused by primitive formulas
    -    7.3 Other classes of side effects
-    8 A case study: Program Algebra
    -    8.1 Program Algebra
        -    8.1.1 Basics of PGA
        -    8.1.2 Behavior extraction
        -    8.1.3 Extensions of PGA
    -    8.2 Logical connectives in PGA
        -    8.2.1 Introduction
        -    8.2.2 Implementation of SCLAnd and SCLOr
        -    8.2.3 Complex Steering Fragments
        -    8.2.4 Negation
        -    8.2.5 Other instructions
    -    8.3 Detecting side effects in PGA
    -    8.4 A working example
-    9 Conclusions and future work

## Chapter 1 Introduction

### 1.1 What are side effects?

In programming practice, side effects are a well-known phenomenon, even
though nobody seems to have an exact definition of what they are. To get
a basic idea, here are some examples from natural language and
programming that should explain the intuition behind side effects.

Suppose you and your wife have come to an agreement regarding grocery
shopping. Upon leaving for work, she told you that “if I don’t call, you
do not have to do the shopping”. Later that day, she calls you to tell
you something completely different, for instance that she is pregnant.
This call now has as side effect that you no longer know whether you
have to do grocery shopping or not, even though the meaning of the call
itself was something completely different.

Another example is taken from [ 9 ] . Suppose someone tells you that
“Phoebe is waiting in front of your door, and you don’t know it!” This
is a perfectly fine thing to say, but you cannot say it twice because
then it will no longer be true that you don’t know that Phoebe is
waiting (after all, you were just told). Here, the side effect is that
your knowledge gets updated by the sentence, which makes the latter part
of that sentence, which is a statement about your knowledge, false.

As said, in programming practice, side effects are a well-known
phenomenon. Logically, they are interesting because the possible
presence of side effects in a program instruction sequence invalidates
principles of propositional logic such as commutativity ( @xmath ) and
idempotency ( @xmath ). The textbook example is the following program:

    x:=1
    if (x:=x+1 and x=2) then y

Here the operator @xmath stands for assignment and @xmath for an
equality test. Assuming an assignment instruction always succeeds (that
is, yields the reply            true           ), in the above example
the test @xmath , where @xmath is the instruction
           x:=x+1           and @xmath the instruction
           x=2           , will succeed and therefore, @xmath will be
executed. However, should the order of those instructions be reversed (
@xmath ), this no longer will be the case. The reason is that the
instruction @xmath has a side effect: apart from returning
           true           , it also increments the variable @xmath with
@xmath , thus making it @xmath . If @xmath is executed before @xmath ,
the test in @xmath (            x=2           ) will yield
           true           . Otherwise, it will yield
           false           .

It is easy to see that should @xmath be executed twice, the end result
will also be            false           . Therefore, for @xmath , we
have that @xmath .

### 1.2 What are steering fragments?

Now that I have given a rough idea of what side effects are, the reader
is probably wondering about the second part of my thesis title: that of
steering fragments. A steering fragment or test is a program fragment
which is concerned with the control flow of the execution of that
program. To be exact, a steering fragment will use the evaluation result
of a formula (which is a Boolean) and depending on the outcome, will
steer further execution of the program. Thus, a steering fragment
consists of two parts: a formula and a control part which decides what
to do with the evaluation result of that formula. Throughout this
thesis, I will be using the terms steering fragment and test
interchangeably.

The formula in a steering fragment can either be a primitive or a
compound formula. The components of a compound formula are usually
connected via logical connectives such as @xmath and @xmath , or involve
negation. If the formula of a steering fragment is compound, we say that
the steering fragment is a complex steering fragment .

We have already seen a classical example of a (complex) steering
fragment in the previous section: the if @xmath then instruction. In the
example above, the formula is a compound formula with @xmath and @xmath
as its components, connected via the logical connective @xmath . The
control part of this steering fragment consists of if and then and the
prescription to execute @xmath if evaluation of
           x:=x+1 and x=2           yields true.

### 1.3 Related work

The main contribution of this thesis is to construct a formal model of
side effects in dynamic logic. Because of that, I only had limited time
and space to properly research related work done in this area. Despite
that, I will briefly describe some references I have come across
throughout this project.

Currently, a formal definition of side effects appears to be missing in
literature. That is not to say that side effects have been completely
ignored. Attempts have been made to create a logic which admits the
possibility of side effects by Bergstra and Ponse [ 5 ] . Furthermore,
an initial, informal classification of side effects has been presented
by Bergstra in [ 1 ] . I will return to those references later in this
thesis.

Black and Windley have made an attempt to reason in a setting with side
effects in [ 7 , 8 ] . In their goal to verify a secure application
written in C using Hoare axiomatic semantics to express the correctness
of program statements, they encountered the problem of side effects
occurring in the evaluation of some C-expressions. They solved the
problem by creating extra inference rules which essentially separate the
evaluation of the side effect from the evaluation of the main
expression.

Also working with C is Norrish in [ 17 ] . He presents a formal
semantics for C and he, too, runs into side effects in the process.
Norrish claims that a semantics gives a program meaning by describing
the way in which it changes a program state. Such a program state would
both include the computer’s memory as well as what is commonly known as
the environment (types of variables, mapping of variable names to
addresses in memory etc.). Norrish claims that in C, changes to the
former come about through the actions of side effects, which are created
by evaluating certain expression forms such as assignments. Norrish’
formal semantics for C is able to handle these side effects.

Böhm presents a different style of axiomatic definitions for programming
languages [ 6 ] . Whereas other authors such as Black and Windley above
use Hoare axiomatic semantics which bases the logic on the notion of
pre- or postcondition, Böhm uses the value of a programming language
expression as the underlying primitive. He relies on the fact that the
underlying programming language is an expression language such as Algol
68 [ 21 ] . Expressions are allowed to have arbitrary side effects and
the notions of statement and expression coincide. Böhm claims that his
formalism is just as intuitive as Hoare-style logic and that the notion
of ‘easy axiomatizability’ — which is a major measurement of the quality
of a programming language — is a matter of a choice of formalism, which
in turn is arbitrary.

In this thesis I will develop a variant of Dynamic Logic to model side
effects. Dynamic Logic is used for a wide range of applications, ranging
from modelling key constructs of imperative programming to developing
dynamic semantic theories for natural language. An early overview of
dynamic logic is given by Harel in [ 15 ] . More recently, Van Eijck and
Stokhof have given an extensive overview of various systems of dynamic
logic in [ 11 ] .

### 1.4 Overview of this thesis

Intuitively, a side effect of a propositional statement is a change in
state of a program or model other than the effect (or change in state)
it was initially executed for. In this thesis I will present a system
that makes this intuition explicit.

First, in Chapter 2 I will present the preliminaries on which my system,
that can model program instructions and their effect on program states,
is based. This system, which I present in Chapter 3 , will be a modified
version of Quantified Dynamic Logic, overviews of which can be found in
[ 15 , 11 ] .

After introducing some terminology and exploring the logic behind this
system in Chapters 4 and 5 , I can formally define side effects, which I
will do in Chapter 6 . In Chapter 7 I will proceed to giving a
classification of side effects, introducing marginal side effects as the
most important class.

In Chapter 8 I will present a case study to see this definition of side
effects in action. For this I will use an — again slightly modified —
version of Program Algebra [ 3 ] . I will end this thesis with some
conclusions and some pointers for future work.

## Chapter 2 Preliminaries

### 2.1 Introduction

In order to say something useful about side effects, we need a formal
definition. Such a definition can be found using dynamic logics. The
basic idea here is that the update of a program instruction is the
change in program state it causes. This allows us to introduce an
expected and an actual evaluation of a program instruction. The expected
evaluation of a program instruction is the change you would expect a
program instruction to make to the program state upon evaluation. This
may differ, however, from the actual evaluation, namely when a side
effect occurs when actually evaluating the program instruction. The side
effect of a program instruction then is defined as the difference in
expected and actual evaluation of a program instruction.

To flesh this out in a formal definition, we first need a system that is
able to model program states and program instructions. Quantified
Dynamic Logic (QDL) is such a system. QDL was developed by Harel [ 14 ]
and Goldblatt [ 13 ] . It can be seen as a first order version of
Propositional Dynamic Logic (PDL), which was developed by Pratt in [ 19
, 20 ] . Much of the overview of both PDL and QDL I will give below is
taken from the overview of dynamic logic by Van Eijck and Stokhof [ 11 ]
.

Dynamic logic can be viewed as dealing with the logic of action and the
result of action [ 11 ] . Although various kinds of actions can be
modelled with it, one is of particular interest for us: the actions
performed on computers, i.e. computations. In essence, these are actions
that change the memory state of a machine, or on a somewhat higher level
the program state of a computer program.

Regardless of what kinds of actions are modelled, the core of dynamic
logic can in many cases be characterized in a similar way via the logic
of ‘labelled transistion systems’. A labelled transition system or LTS
over a signature @xmath , with @xmath a set of propositions and @xmath a
set of actions, is a triple @xmath where @xmath is a set of states,
@xmath is a valuation function and @xmath is a set of labelled
transitions (one binary relation on @xmath for each label @xmath ).

There are various versions of dynamic logic. Before I will introduce two
of these, I will first describe the setting I will be using in my
examples. This setting consists of a toy programming language that is
expressive enough to model the working examples I need to discuss side
effects.

### 2.2 Toy language

My toy language should be able to handle assignments and steering
fragments. The steering fragment can possibly be complex, so our toy
language should be able to handle compound formulas: multiple formulas
(such as equality tests) connected via logical connectives. In
particular, I will be using short-circuit left and ( @xmath ) and
short-circuit left or ( @xmath ) as connectives. Finally, assignments
should be allowed in tests as well: they are, in line with what one
would expect, defined to always return            true           .

As toy language I will first present the WHILE language defined by Van
Eijck in [ 11 ] . We will see soon enough that we will actually need
more functionality than it offers, but it will serve us well in the
introduction of PDL, QDL and the illustration of the problems we will
run into.

The WHILE language works on natural numbers and defines arithmetic
expressions, Boolean expressions and programming commands. Arithmetic
expressions @xmath with @xmath ranging over numerals and @xmath over
variables from a set @xmath are defined as follows:

  -- -- --
        
  -- -- --

Boolean expressions are defined as:

  -- -------- --
     @xmath   
  -- -------- --

Finally, we define the following programming commands:

  -- -------- --
     @xmath   
  -- -------- --

For the sake of simplicity, we will postpone the introduction of the
WHILE command until after we have presented our modified system in
Chapter 3 .

The semantics of the arithmetic expressions are fairly self-explanatory.
We assume that every numeral @xmath in @xmath has an interpretation
@xmath and let @xmath be a mapping from @xmath to @xmath . We then have
the following interpretations of the arithmetic expressions, relative to
initial valuation or initial program state @xmath :

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
                       
  -- -------- -------- --

The semantics of the Boolean expressions are standard as well, writing
@xmath for true and @xmath for false:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The semantics of the commands of the toy language can be given in
various styles. Here I take a look at a variant called structural
operational semantics [ 11 ] . It is specified using a transition system
from pairs of a state and a command, to either a state or again a state
and a (new) command.

First I will give the transitions for the assignment command. It looks
like this, where we write @xmath for the valuation which is like
valuation @xmath except for the variable @xmath , which has been mapped
to @xmath :

  -- -------- --
     @xmath   
  -- -------- --

Here we have the pair of state @xmath and the assignment command @xmath
at the start of the transition. After the transition, we only have a new
state left, since the execution of this command has finished in a single
step.

The SKIP command does nothing: it does not change the state and it
finishes in a single step.

  -- -------- --
     @xmath   
  -- -------- --

In structural operational semantics, there are two rules for sequential
composition, one for when program @xmath finishes in a single step and
one for which it does not.

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Finally, we have the rules for conditional action. There are two
(similar) rules, depending on the outcome of the test:

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

### 2.3 Propositional Dynamic Logic

Now that I have introduced the toy language, it is time to take a look
at the first version of dynamic logic we are interested in:
Propositional Dynamic Logic (PDL in short). The language of PDL consists
of formulas @xmath (based on basic propositions @xmath ) and programs
@xmath (based on basic actions @xmath ):

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

As the name suggests, PDL is based on propositional logic. This means
that the usual properties such as associativity and duality are valid
and will be used throughout. Furthermore, we can use the following
abbreviations:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The relational composition @xmath of binary relations @xmath on state
set @xmath is given by:

  -- -------- --
     @xmath   
  -- -------- --

The @xmath -fold composition @xmath of a binary relation @xmath on
@xmath with itself is recursively defined as follows, with @xmath the
identity relation on @xmath :

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Finally, the reflexive transitive closure of @xmath is given by:

  -- -------- --
     @xmath   
  -- -------- --

To define the semantics of PDL over basic propositions @xmath and basic
actions @xmath , we need the labelled transistion system @xmath for
signature @xmath . The formulas of PDL are interpreted as subsets of
@xmath , the actions as binary relations on @xmath . This leads to the
following interpretations:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The programming constructs in our toy language are expressed in PDL as
follows:

  -- -------- -------- --
              @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Although PDL is a powerful logic, it is not enough yet to properly model
the toy language we need. The reason for that is the need for
assignments. Since assignments change relational structures, the
appropriate assertion language is first order predicate logic, and not
propositional logic [ 11 ] . So instead of PDL, which as the name
suggests uses propositional logic, we need a version of dynamic logic
that uses first order predicate logic. This is where Quantified Dynamic
Logic (QDL in short) comes in.

### 2.4 Quantified Dynamic Logic

The language of QDL consists of terms @xmath , formulas @xmath and
programs @xmath . For functions @xmath and relational symbols @xmath we
have:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

In the case of natural numbers, examples of @xmath are @xmath etc. and
examples of @xmath are @xmath and @xmath . The same abbreviations as in
PDL are used, most notably @xmath and @xmath .

The random assignment ( @xmath ) does not increase the expressive power
of QDL [ 11 ] . It can, however, be nicely used to express the universal
and existential quantifier:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The pair @xmath is called a first order signature. A model for such a
signature is a structure of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a non-empty set, the @xmath are interpretations in
@xmath for the members of @xmath and the @xmath similarly are the
interpretations in @xmath for the members of @xmath . Now let @xmath be
the set of variables of the language. Interpretation of terms in @xmath
is defined relative to an initial valuation @xmath :

  -- -------- -- --------
     @xmath      (QDL1)
     @xmath      (QDL2)
  -- -------- -- --------

Truth in @xmath for formulas is defined by simultaneous recursion, where
@xmath then means that @xmath differs at most from @xmath on the
assignment it gives to variable @xmath :

  -- -------- -------- -- --------
     @xmath   @xmath      (QDL3)
     @xmath   @xmath      (QDL4)
     @xmath   @xmath      (QDL5)
     @xmath   @xmath      (QDL6)
     @xmath   @xmath      (QDL7)
     @xmath   @xmath      (QDL8)
     @xmath   @xmath      (QDL9)
  -- -------- -------- -- --------

The same goes for the relational meaning in @xmath for programs:

  -- -------- -------- -- ---------
     @xmath   @xmath      (QDL10)
     @xmath   @xmath      (QDL11)
     @xmath   @xmath      (QDL12)
     @xmath   @xmath      (QDL13)
     @xmath   @xmath      (QDL14)
  -- -------- -------- -- ---------

The above definition makes concatenation ( @xmath ) an associative
operator:

  -- -------- --
     @xmath   
  -- -------- --

As a convention, we omit the brackets wherever possible.

Although QDL goes a long way to modelling our toy language and program
states, we are not quite there yet. The modifications we have to make
come to light when we examine the expressive power of QDL. QDL currently
has more expressive power than it has semantics defined for. This
problem surfaces when the modality operator is nested within a test,
like this:

  -- -------- --
     @xmath   
  -- -------- --

This is the program @xmath , with @xmath , @xmath and @xmath . As the
semantics of QDL are currently defined, the program @xmath will make a
change to an initial valuation @xmath if it is interpreted in it,
returning valuation @xmath where the assignment @xmath had for variable
@xmath will be expressed by @xmath . This is expressed by QDL10.
However, the current semantics only assign relational meaning to a test
instruction @xmath as long as @xmath , as expressed by QDL11.

Another similar example is the following:

  -- -- --
        
  -- -- --

Although this situation should be similar as above, it is not: because
the program state gets changed twice, QDL now is able to assign
semantics to this program since the program state gets returned to the
original state by the second program instruction (and we therefore have
@xmath ).

So, not only can we devise even a very simple correct QDL-program for
which there are no semantics defined, we can also give a very similar
example for which QDL does define semantics. Not only does that somewhat
erratic behavior seem undesirable, but the nature of the examples here
present us with a problem when we are considering side effects. Exactly
for the situations in which side effects occur, namely when an
instruction in a test causes a change in the program state, there are no
semantics defined in QDL. Therefore, I am going to have to modify QDL so
that it does define semantics in those situations.

## Chapter 3 Modifying QDL to DLA@xmath

### 3.1 Introducing DLA@xmath

In this chapter I will present Dynamic Logic with Assignments as
Formulas , or DLA @xmath in short, the resulting dynamic logic after
making two major modifications to QDL. The modifications I will make are
such that DLA @xmath can model the specific kinds of constructions that
we are interested in. This means that, like the name suggests, we have
to introduce semantics for assignments in formulas. Furthermore, we will
drop or modify some other QDL-instructions that we do not need. Because
of that DLA @xmath evades the problem of QDL mentioned in Section 2.4 of
the previous chapter and one other problem I will get back to in Section
3.3 . Before I introduce DLA @xmath , however, I will show the
modifications that need to be done to Van Eijck’s WHILE language so that
it can model the instructions we need.

In the WHILE language, Boolean expressions are assumed to cause no state
change upon evaluation. However, for our purpose this is inadequate. We
want to allow assignments in tests as well and they cause a state
change. This warrants the first modification to the WHILE language and
its semantics: assignments are allowed in Boolean expressions. The
second modification is that the Boolean OR function will be replaced by
a short-circuit version:

  -- -- --
        
  -- -- --

The new semantics for Boolean expressions are like the semantics defined
by Van Eijck, with as major difference that there are now semantics
defined for assignments:

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, Boolean expressions now might introduce a state change, so
every command containing a Boolean expression (which for now only is the
IF THEN ELSE command) should account for that. In structural operational
semantics, we take a look at how the Boolean expression changes the
state and perform the remaining actions in that new state:

  -- -------- --
     @xmath   
  -- -------- --

And similar for the case that @xmath .

As said, there is one more thing that needs to be modified in the
language above. In order to be properly able to reason about side
effects, the order in which the tests get executed is important. Because
of that, the OR construct in Boolean expressions needs to be replaced by
a short-circuit directed version:

  -- -- --
        
  -- -- --

We will make use of its dual, the short-circuit left and ( @xmath ) too.
It is defined similarly as above. As a convention, from here on @xmath
and @xmath can be used interchangeably in definitions, unless explicitly
stated otherwise. Both @xmath as well as @xmath are associative. We
again omit brackets wherever possible.

All we have left to define now is the state change a Boolean can cause.
This is defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

Missing in the above WHILE language are the random assignment and the
existential quantifier. This is because I have decided to drop them. The
reason for that is that they can cause non-deterministic behavior and in
this thesis, we are not interested in the (side effects of)
non-deterministic programs. In fact it is questionable whether we can
say anything about side effects in non-deterministic programs, but I
will return to that in my possibilities for future work in Chapter 9 .
Aside from that, in our context of (imperative) programs, the random
assignment is an unusual concept at best. The same goes for the formula
@xmath .

With those modifications to the toy language in mind, we can take a look
at the similar modifications that need to be made to QDL. In the
resulting dynamic logic DLA @xmath , we keep the same terms:

  -- -------- --
     @xmath   
  -- -------- --

In DLA @xmath we of course drop the random assignment and existential
quantifier, too. By dropping them, we lose the quantified character of
QDL. Because of that, the resulting logic is no longer called a
quantified dynamic logic. The first major change to QDL, besides the
absence of the random assignment and the existential quantifier, is that
I replace the @xmath command with the weaker @xmath :

  -- -- --
        
  -- -- --

This modification explicitly expresses the possibility of assignments in
formulas. All other programs, however, are no longer allowed in
formulas. Because of this modification we will avoid a number of
problems that QDL has, while keeping the desired functionality that
there should be room for assignments in formulas. I will address these
problems in detail in Section 3.3 .

We have also replaced the @xmath connective with its short-circuit
variant ( @xmath ) and for convenience, have explicitly introduced its
dual ( @xmath ). We will return to the motivation for this change at the
end of this chapter.

We also need to replace the QDL-formula associated with this command
(QDL9). The truth in @xmath for the new command is defined as follows:

  -- -------- -- --------
     @xmath      (DLA9)
  -- -------- -- --------

It should come as no surprise that this always succeeds, since
assignments always succeed and yield            true           . Since
this formula always succeeds, we replaced the possibility modality (
@xmath ) for the necessity modality ( @xmath ). The reason we keep this
formula in the form of a modality at all (and not just @xmath ), is
because formulas of this form can change the initial valuation. This is
in sharp contrast to the basic formulas @xmath and @xmath , which do not
change the initial valuation and are typically not modalities. Because
of that, it is unintuitive to write the assignment formula as @xmath .

On a side note: in our toy language we do simply write @xmath for the
assignment, regardless of where it occurs. This is because in the world
of (imperative) programming, assignments are allowed in steering
fragments.

We will see below that we are going to accept possible state changes in
formulas, in contrast to the original QDL versions. For this we will use
a mechanism to determine when a state change happens, that is, a
function that returns the program(s) that are encountered when
evaluating a formula @xmath . This function is defined as follows:

###### Definition 1.

The program extraction function @xmath returns for formula @xmath the
program(s) that are encountered when evaluating the formula given modal
@xmath and initial valuation @xmath . It is defined recursively as
follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
              @xmath   
     @xmath   @xmath   
  -- -------- -------- --

In the first three cases, no programs are encountered. Therefore, the
program extraction function returns the empty program ( @xmath ). The
formula @xmath is transparent, that is, it returns any program
encountered in its subformula @xmath . Because of the short-circuit
character of @xmath and @xmath , a case distinction is made here: in
case of @xmath , @xmath will not be evaluated if @xmath yields true,
therefore only the program(s) encountered in @xmath will be returned.
Otherwise, the result is a concatenation of the program(s) encountered
in @xmath and @xmath . Obviously, for @xmath the opposite is the case
and this clause is derivable from the previous one using duality.
Finally, if the formula is an assignment, the program equivalent of that
assignment is returned.

Because the evaluation of a formula now can cause a state change, the
original definition for the truth in @xmath of @xmath (QDL7) is no
longer valid. In case @xmath contains an assignment, @xmath must be
evaluated in a different valuation, namely the one resulting after
evaluating @xmath in the initial valuation:

  -- -- -- ---------
           (DLA7a)
  -- -- -- ---------

Since we have added @xmath to formulas as well, we also explicitly have
to define the truth in @xmath for @xmath , which is similar to the
updated definition of @xmath :

  -- -- -- ---------
           (DLA7b)
  -- -- -- ---------

Although @xmath and @xmath use short-circuit evaluation, we do not
explicitly have to define them as such above because we will make sure,
via the program extraction function and an updated version of QDL11 (see
below), that the valuation does not change as a result of @xmath when
@xmath is true (in case of @xmath ) or false (in case of @xmath ).

We can now turn our attention to programs in DLA @xmath . Besides the
absence of the random assignment, what a program @xmath can be does not
change:

  -- -------- --
     @xmath   
  -- -------- --

To remedy the problem that more things can be expressed in QDL than
there are semantics for, we need, as mentioned earlier, to accept that a
state change can occur when evaluating a program containing formulas. In
the case of QDL, that only is the test instruction, given semantics
earlier in QDL11. So, as second major change we need to replace QDL11
by:

  -- -------- -- ---------
     @xmath      (DLA11)
  -- -------- -- ---------

The choice here is in place to avoid looping behavior when evaluating
@xmath .

The definitions above make extensive use of the empty program ( @xmath
). In what follows, it will be handy to know that the empty program is
truly empty. In particular, we would like to have @xmath and @xmath . I
will prove that below.

###### Lemma 3.1.1.

For any program @xmath , initial valuation @xmath , output valuation
@xmath and model @xmath

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

The proof follows from the above defined QDL-axioms:

  -- -------- --
     @xmath   
  -- -------- --

Since we have @xmath iff @xmath and @xmath , and since the latter is
always true, we have

  -- -------- --
     @xmath   
  -- -------- --

∎

###### Lemma 3.1.2.

For any program @xmath , initial valuation @xmath , output valuation
@xmath and model @xmath

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

Similar as for Lemma 3.1.1 . ∎

The change to QDL11 has remedied the problem that there are expressions
in QDL for which there are no semantics defined. Of course I made a
second major change — namely replacing @xmath by @xmath . The reason for
that will come to light as soon as I will reintroduce the WHILE command
in Section 3.3 . Before I will do that, however, I will first discuss a
working example to provide some more insight into the inner workings of
DLA @xmath .

### 3.2 A working example

In this section I will present a working example to illustrate how DLA
@xmath works. I will use the following program, presented here in our
toy language:

  -- -------- --
     @xmath   
              
     @xmath   
     @xmath   
  -- -------- --

In DLA @xmath , this translates to:

  -- -------- --
     @xmath   
              
     @xmath   
              
  -- -------- --

The valuations @xmath are defined for all variables @xmath , i.e. they
are total functions. Usually we are only interested in a small number of
variables, e.g. @xmath and @xmath , in which case we talk about a
valuation @xmath such that @xmath , or if valuation @xmath is an update
of valuation @xmath , @xmath (which is a shorthand for @xmath ). In all
examples we discuss we take for @xmath the model of the natural numbers
and we use numerals to denote its elements.

Since we are working on natural numbers, as constants we have @xmath
ranging over numerals, as functions we have @xmath and @xmath , and as
extra relation we have @xmath . Our model @xmath contains those
constants, functions and relations. Assume we have an initial valuation
@xmath that sets @xmath and @xmath to @xmath : @xmath . We will now
first show how the program in our toy language gets evaluated using the
structural operational semantics we provided in Chapter 2 :

  -- -- --
        
        
  -- -- --

We now need to know if @xmath . We can easily see that it is and
furthermore updates the valuation again by incrementing @xmath by @xmath
. Thus we get as valuation @xmath and we can finish our evaluation as
follows:

  -- -------- --
     @xmath   
  -- -------- --

Having seen how our example program evaluates using the semantics for
our toy language, we can turn our attention to the evaluation using DLA
@xmath . We need to ask ourselves if @xmath exists (with @xmath the
program above), that is, if there is a valuation @xmath that models the
state of the program after being executed on initial valuation @xmath .

Schematically, @xmath can be broken down as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

The break-down above paves the way to evaluate @xmath using the DLA
@xmath -axioms given in the previous sections. We start by applying
QDL12:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

We find @xmath by evaluating @xmath using QDL10 and QDL1:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Now we need to evaluate @xmath . Using QDL13, we get:

  -- -------- --
     @xmath   
  -- -------- --

First we turn our attention to @xmath . Using QDL12 again we get @xmath
such that @xmath and @xmath . To evaluate the former, we need to use our
own rule DLA11. Here we need the program extraction function @xmath for
the first time:

  -- -------- -- --
     @xmath      
                 
                 
  -- -------- -- --

We will first have a look at the program extraction function @xmath .
Below we will see how it calculates the programs that are encountered
while evaluating the formula @xmath :

  -- -- -------- --
        @xmath   
        @xmath   
  -- -- -------- --

Therefore, we have:

  -- -------- -------- --
     @xmath            
                       
              @xmath   
  -- -------- -------- --

The first of these two, @xmath , nicely shows why we need an updated
version of @xmath and @xmath . As we already noticed the test @xmath
contains a program (the assignment @xmath ) and therefore the state
(valuation) changes. As we will see, this will change the outcome of the
second part of the test. We need DLA7b and our program extraction
function @xmath here:

  -- -- -------- --
        @xmath   
        @xmath   
  -- -- -------- --

@xmath is defined by DLA8 to be always true. Applying QDL10 on @xmath
will give us @xmath . We can then apply QDL5 on @xmath :

  -- -------- --
     @xmath   
  -- -------- --

We can easily see (using QDL1) that @xmath . Therefore, we have @xmath
and thus @xmath .

We now need to finish the evaluation of DLA11 by evaluating @xmath .
This can again be done using QDL10 and gives us @xmath . Because the
test @xmath has now succeeded, we can continue to the evaluation of
@xmath . This will give us @xmath . Having already established that
@xmath succeeds, we also know that @xmath will not succeed. Therefore,
we are done with the evaluation of this program @xmath , getting that
@xmath with @xmath is indeed possible with @xmath .

### 3.3 Re-introducing WHILE

In Section 2.2 I introduced our toy language, which was like Van Eijck’s
WHILE language, but without a WHILE (or: guarded iteration) programming
command. Now that we have seen DLA @xmath in action in our simplified
toy language, it is time to re-introduce the WHILE command. After doing
that, we will see that the re-introduction of WHILE raises some more
issues that warrant the second modification I made to QDL, namely
replacing the formula @xmath with @xmath .

#### 3.3.1 The WHILE command

The WHILE command takes the form WHILE @xmath DO @xmath . The complete
list of programming commands in our toy language then is:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

In structural operational semantics, the semantics for the guarded
iteration are as follows. There are two options: if the guard ( @xmath )
is not satisfied, command @xmath is not executed. Instead, the command
finishes, with as only (possible) change the change that the evaluation
of guard @xmath has made to the state:

  -- -------- --
     @xmath   
  -- -------- --

If the guard is satisfied, the rule becomes a little more complicated
because command @xmath gets executed in a state which is possibly
changed by guard @xmath . Like before, we have two cases: one for which
@xmath finishes in a single step and one for which it does not.

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

#### 3.3.2 WHILE in DLA@xmath

In PDL, and therefore QDL and DLA @xmath , WHILE is expressed as
follows:

  -- -------- --
     @xmath   
  -- -------- --

Thanks to the updated rule for @xmath (DLA11), DLA @xmath is able to
handle programs with WHILE perfectly. To see how this works, consider
the following example:

  -- -------- --
     @xmath   
     @xmath   
              
     @xmath   
  -- -------- --

In DLA @xmath , this translates to:

  -- -------- --
     @xmath   
     @xmath   
              
              
  -- -------- --

After the first two commands, we have @xmath . We now need to look at
how the @xmath operator is evaluated. QDL14 states that @xmath iff
@xmath or @xmath . This means that @xmath is either executed not at all
(in which case @xmath ) or at least once. In our case, @xmath .

The first option is that @xmath is executed not at all, in which case
@xmath . However, under this valuation @xmath there is no possible
valuation @xmath after evaluation of the next program command ( @xmath
). In other words, @xmath is false. Therefore, we have to turn our
attention to the other option given by the @xmath command, which is
@xmath . For the evaluation of this we first need QDL12, which tells us
that there has to be an @xmath such that @xmath and @xmath . In Section
3.2 we have already seen how @xmath evaluates; it will succeed and
result in a new valuation @xmath .

Now we need to evaluate @xmath again, but this time with a different
initial valuation (namely @xmath ). This loop continues until we arrive
at a valuation @xmath for which the final program command (the test
@xmath ) will succeed. In our example, this happens in the second
iteration, when we have @xmath , giving us a resulting valuation @xmath
, which is exactly what we would expect given this WHILE loop.

#### 3.3.3 Looping behavior and abnormal termination

An interesting problem regarding the WHILE language and QDL is that
WHILE @xmath DO SKIP (looping behavior) and ABORT (abnormal termination)
are indistinguishable. In some semantics, such as natural semantics,
this is also the case [ 11 ] . In structural operational semantics,
however, there is an (infinite) derivation sequence for WHILE @xmath DO
SKIP, whereas there is no derivation sequence for ABORT.

Using the standard lemma that @xmath (cf. [ 15 , 11 ] ) we can prove the
equivalence of WHILE @xmath DO SKIP and ABORT in QDL. To do so, we need
to ask if @xmath .

###### Theorem 3.3.1.

In QDL, looping behavior and abnormal termination are equivalent: for
any @xmath

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

We will work out the left part first:

  -- -------- --
     @xmath   
  -- -------- --

So we have @xmath with @xmath . Truth of the former in a random model
@xmath and for an initial valuation @xmath is defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

Furthermore we have

  -- -------- --
     @xmath   
  -- -------- --

We have seen in the previous section how such a formula evaluates; after
one iteration we will have @xmath , with @xmath , as one of the options
the @xmath command gives us. Finally we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

This is always the case, so indeed there is an @xmath such that @xmath
(namely @xmath ). Therefore, determining the truth of @xmath comes down
to determining the truth of @xmath , which is @xmath .

Since that is exactly the right hand side of the equation we started out
with, we indeed have that

  -- -------- --
     @xmath   
  -- -------- --

∎

Not being able to distinguish between looping behavior and abnormal
termination seems undesirable. It is because of this that I have decided
to drop the @xmath formulas and replace it by the weaker, but less
problematic formulas @xmath . Looping behaviour can now no longer be
proven to be equivalent to abnormal termination. Furthermore, we avoid
problems with formulas that require infinite evaluations, such as @xmath
.

Because looping behavior and abnormal termination can no longer be
proven equal in DLA @xmath , the relational meaning of DLA @xmath
-instructions now is an instance of the structural operational semantics
we defined for our toy language, with the valuations as ‘states’.
Naturally, this is what we want, since it expresses that DLA @xmath is a
fully defined system that has the behavior we would expect given our toy
language.

This modification also underlines the usefulness of the switch to
short-circuit versions of the logical connectives ( @xmath and its dual
@xmath ). In QDL, the steering fragment of the program

  -- -------- --
     @xmath   
  -- -------- --

can be expressed using @xmath . In DLA @xmath such an expression now no
longer is allowed. However, having @xmath and @xmath in DLA @xmath
allows us to provide a perhaps even more natural translation of this
program, namely @xmath . The full evaluation versions of these logical
connectives ( @xmath and @xmath ) would not do, because the order of the
program instructions is important here. As we will see in Chapter 4 , we
do not need @xmath and @xmath in DLA @xmath , but the fact they provide
natural translations of this kind, together with the fact that having
logical connectives defined is standard in dynamic logic, is reason
enough to keep them.

## Chapter 4 Terminology

In this chapter I will present the terminology I will be using in the
remainder of this thesis. In particular, I will present a more
fine-grained breakdown of the definitions for formulas, instructions and
programs. Furthermore, I will introduce a property of formulas called
normal form and use that to prove yet another property of DLA @xmath
regarding complex steering fragments. Next, I will introduce a subclass
of programs called deterministic programs. Finally, I will introduce a
property of deterministic programs called canonical form.

### 4.1 Formulas, instructions and programs

In this section I will present the more fine-grained breakdown of the
definitions for formulas, instructions and programs.

###### Definition 2.

Formulas can either be primitive or compound formulas. Primitive
formulas are written as @xmath and defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

Compound formulas are written as @xmath and defined similarly, but with
negation and short-circuit disjunction and conjunction as addition:

  -- -- --
        
  -- -- --

###### Definition 3.

Instructions can either be single instructions or basic instructions.
Single instructions are written as @xmath and defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

Basic instructions are written as @xmath and have a little less
restrictive definition regarding tests:

  -- -------- --
     @xmath   
  -- -------- --

This means that single instructions form a subset of basic instructions:

  -- -------- --
     @xmath   
  -- -------- --

###### Definition 4.

Programs are written as @xmath and consist of one or more basic
instructions joined by either concatenation ( @xmath ), union ( @xmath )
or repetition ( @xmath ):

  -- -------- --
     @xmath   
  -- -------- --

### 4.2 Normal forms of formulas

In this section I will introduce a property of formulas called normal
form and use that to prove a property of DLA @xmath regarding complex
steering fragments. I will start with the former.

###### Definition 5.

A formula is said to be in its normal form iff all negations (if any)
that occur in the formula are on atomic level, that is if the negations
only have primitive formulas as their argument (i.e. are of the form
@xmath ).

###### Proposition 1.

Any formula can be rewritten into its normal form such that its
relational meaning is preserved.

###### Proof.

Left-sequential versions of De Morgan’s laws are valid for formulas (we
come back to this point in Chapter 5 ): given model @xmath and initial
valuation @xmath we prove that

  -- -- --
        
  -- -- --

For @xmath , first assume that @xmath , thus @xmath for @xmath , thus
@xmath , and thus @xmath . If @xmath , then @xmath , and thus also
@xmath .

In order to show @xmath , first assume that @xmath , thus @xmath , thus
@xmath . If @xmath , then @xmath for @xmath , so @xmath , and thus
@xmath .

The dual statement can also easily be proved. ∎

The set of side effects caused by the evaluation of a formula does not
change under rewritings of this kind. Using normal forms, we can derive
an interesting property of DLA @xmath :

###### Proposition 2.

Let @xmath be a formula. The program @xmath can be rewritten to a form
in which only primitive formulas or negations thereof occur in tests,
such that its relational meaning is preserved.

###### Proof.

Let @xmath be a normal form of @xmath and assume @xmath is not a
primitive formula or the negation thereof. Then, @xmath either is of the
form @xmath or @xmath . For conjunctions, it is easy to see that the
program @xmath can be rewritten as meant in the proposition:

  -- -- --
        
  -- -- --

We can assume by induction that @xmath and @xmath has been rewritten
into a form in which only primitive formulas and negations occur, too.
We now need to prove that these programs have the same relational
meaning, that is given model @xmath and initial valuation @xmath

  -- -- --
        
  -- -- --

If @xmath , then @xmath does not exist in both cases. If, for @xmath ,
@xmath , @xmath does not exist in both cases either. Otherwise, on the
left hand side, we get @xmath by applying DLA11:

  -- -- --
        
  -- -- --

which by definition of the program extraction function, since @xmath ,
equals

  -- -------- --
     @xmath   
  -- -------- --

On the right hand side, we get @xmath by first applying QDL12, then
applying DLA11 twice and finally applying QDL12 again:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

For disjunctions, the rewritten version is slightly more complex:

  -- -- --
        
  -- -- --

We can prove that given model @xmath and initial valuation @xmath

  -- -- --
        
  -- -- --

in a similar fashion as above. If @xmath , then in both cases @xmath is
obtained by

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then if for @xmath , @xmath , in both cases @xmath does not
exist. If @xmath , then on the left hand side @xmath is obtained via

  -- -- --
        
  -- -- --

And on the right hand side, @xmath is obtained by

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

∎

On a side note, a similar result can be obtained for QDL. Here the
program @xmath can be rewritten to

  -- -------- --
     @xmath   
  -- -------- --

The differences between the DLA @xmath version of the same rule are
there because QDL uses full evaluation. Therefore, @xmath has to be
evaluated even when @xmath is true, although @xmath does not have to be
true anymore.

### 4.3 Deterministic programs and canonical forms

Defining side effects for entire programs can be complicated. This is
because two composition operators, namely union and repetition, can be
non-deterministic. We are, however, not interested in (the side effects
of) non-deterministic programs, even though they can be expressed in DLA
@xmath . ¹ ¹ 1 In fact, as we already mentioned in Chapter 2 , we can
ask ourselves if it is reasonable to talk about side effects in
non-deterministic programs. We have left this question for future work.
To be exact, we are only interested in if @xmath then @xmath else
constructions and while constructions, which in DLA @xmath are expressed
as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

To formally specify this, we introduce deterministic programs , which
cf. [ 14 , 11 ] are defined as follows:

###### Definition 6.

A deterministic program @xmath is a DLA @xmath -program in one of the
following forms:

  -- -------- --
     @xmath   
  -- -------- --

There are two interesting properties of deterministic programs. The
first is regarding programs of the form @xmath . In this case there will
only ever be exactly one situation in which the program gets evaluated.
² ² 2 That is unless we are dealing with an infinite loop, but in that
case the program has no evaluation and we are not interested in those.
After all, there is exactly one repetition loop for which the test
@xmath succeeds, but will fail the next time it is evaluated. We can
formalize this intuition in the following proposition:

###### Proposition 3.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . There is a unique @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath .

###### Proof.

We first prove that there is at least one @xmath for which the above
equation holds. Assume such an @xmath does not exist. This means that
@xmath can never be evaluated, which is a contradiction with our
requirement that there is a valuation @xmath such that @xmath .

Next, we have to prove that there is at most one such @xmath . Let
@xmath be the valuation such that @xmath . By writing this out and then
applying DLA11, we know that for @xmath , we have @xmath . Therefore,
for valuation @xmath with @xmath we cannot evaluate @xmath and thus
there is no @xmath for which the above equivalence holds.

We know that for @xmath , we have @xmath . This automatically means that
for @xmath , the above equivalence will not hold either, since we cannot
satisfy @xmath . Thus, we have exactly one @xmath . ∎

The second interesting property of a deterministic program is the
following:

###### Definition 7.

A deterministic program @xmath is said to be in canonical form if only
concatenations occur as composition operators.

This property is going to be very useful, because we can prove that
given an initial valuation @xmath , any program has a unique canonical
form that has the same behavior:

###### Proposition 4.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . There is a unique deterministic program @xmath in canonical
form such that

  -- -------- --
     @xmath   
  -- -------- --

and @xmath executes the same basic instructions and the same number of
basic instructions as @xmath .

###### Proof.

If @xmath , then @xmath depends on the truth of @xmath :

  -- -------- --
     @xmath   
  -- -------- --

By induction we can assume that @xmath and @xmath are the canonical
forms of @xmath and @xmath (if these are not empty), respectively. The
truth of @xmath follows directly from QDL13 in this case.

If @xmath , we need to use @xmath as meant in Proposition 3 :

  -- -------- --
     @xmath   
  -- -------- --

Once again we can assume by induction that @xmath is the canonical form
of @xmath (once again if @xmath is not empty). The truth of @xmath now
follows directly from Proposition 3 .

It is easy to see that in both these cases, @xmath executes the same
basic instructions as @xmath . It is also easy to see that @xmath is
unique: we cannot add instructions using union or repetition because
then @xmath will no longer be in canonical form and we cannot add
instructions using concatenation because those instructions will be
executed, which violates the requirement that @xmath only executes the
same basic instructions as @xmath . We cannot alter or remove
instructions in @xmath either because all instructions in @xmath get
executed, so altering or removing one would also violate said
requirement. ∎

## Chapter 5 The logic of formulas in DLA@xmath

Now that we have DLA @xmath defined and shown how it works, it is time
to examine the logic of formulas a little closer. As we have mentioned
before, we are making use of short-circuit versions of the @xmath and
@xmath connectives, i.e. connectives that prescribe short-circuit
evaluation. In [ 5 ] , different flavours of short-circuit logics
(logics that can be defined by short-circuit evaluation) are identified.
In this chapter we will give a short overview of these and present the
short-circuit logic that underlies the formulas in DLA @xmath , which
turns out to be repetition-proof short-circuit logic (RPSCL).

### 5.1 Proposition algebra

Short-circuit logic can be defined using proposition algebra , an
algebra that has short-circuit evaluation as its natural semantics.
Proposition algebra is introduced by Bergstra and Ponse in [ 4 ] and
makes use of Hoare’s ternary connective @xmath , which is called the
conditional [ 16 ] . A more common expression for this conditional is if
y then x else z , with @xmath and @xmath ranging of propositional
statements (including propositional variables). Throughout this thesis,
we will use atom as a shorthand for propositional variable.

Using a signature which includes this conditional, @xmath , the
following set of axioms for proposition algebra can be defined:

  -- -------- -------- -- -------
     @xmath   @xmath      (CP1)
     @xmath   @xmath      (CP2)
     @xmath   @xmath      (CP3)
     @xmath   @xmath      (CP4)
  -- -------- -------- -- -------

In the earlier mentioned paper [ 4 ] , varieties of so-called valuation
algebras are defined that serve the interpretation of a logic over
@xmath by means of short-circuit evaluation. The evaluation of the
conditional @xmath is then as follows: first @xmath gets evaluated. That
yields either @xmath , in which case the final evaluation result is
determined by the evaluation of @xmath , or @xmath , in which case the
same goes for @xmath .

All varieties mentioned in [ 4 ] satisfy the above four axioms. The most
distinguishing variety is called the variety of free reactive valuations
and is axiomatized by exactly the four axioms above (further referred to
as conditional propositions (CP)) and nothing more. The associated
valuation congruence is called free valuation congruence and written as
@xmath . Thus, for each pair of closed terms ¹ ¹ 1 Terms that may
contain atoms, but not variables. @xmath over @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Using the conditional, we can define negation ( @xmath ),
left-sequential conjunction ( @xmath ) and left-sequential disjunction (
@xmath ) as follows:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

The above defined connectives are associative and each other’s dual. In
CP, it is not possible to express the conditional @xmath using any set
of Boolean connectives, such as @xmath and @xmath [ 4 ] .

By adding axioms to CP, it can be strengthened. The signature and axioms
of one such extension are called memorizing CP . We write CP @xmath for
this extension that is obtained by adding the axiom CPmem to CP. This
axiom expresses that the first evaluation value of @xmath is memorized:

  -- -------- -------- -- ---------
     @xmath   @xmath      (CPmem)
  -- -------- -------- -- ---------

With @xmath and by replacing @xmath by @xmath we get the contraction
law:

  -- -------- --
     @xmath   
  -- -------- --

A consequence of contraction is the idempotence of @xmath . Furthermore,
CP @xmath is the least identifying extension of CP for which the
conditional can be expressed using negation, conjunction and
disjunction. To be exact, the following holds in CP @xmath :

  -- -- --
        
  -- -- --

We write @xmath (memorizing valuation congruence) for the valuation
congruence axiomatized by @xmath .

Another extension of CP, the most identifying one distinguised in [ 4 ]
, is defined by adding both the contraction law and the axiom below,
which expresses how the order of @xmath and @xmath can be swapped, to
CP:

  -- -------- -- ----------
     @xmath      (CPstat)
  -- -------- -- ----------

The signature and axioms of this extension, for which we write CP @xmath
, are called static CP . We write @xmath ( static valuation congruence )
for the valuation congruence axiomatized by @xmath . A consequence in
@xmath is @xmath , which can be used to derive the commutativity of
@xmath : @xmath .

CP @xmath is the most identifying extension of CP because it is
‘equivalent with’ propositional logic, that is, all tautologies in
propositional logic can be proved in CP @xmath using the above
translations of its common connectives [ 5 ] .

### 5.2 Short-Circuit Logics

In this section we will present the definition of short-circuit logic
and its most basic form, free short-circuit logic (FSCL). The
definitions are given using module algebra [ 2 ] . In module algebra,
@xmath is the operation that exports the signature @xmath from module
@xmath while declaring other signature elements hidden. Using this
operation, short-circuit logics are defined as follows:

###### Definition 8.

A short-circuit logic is a logic that implies the consequences of the
module expression

  -- -- -------- --
                 
        @xmath   
                 
  -- -- -------- --

Thus, the conditional composition is declared to be an auxiliary
operator. In SCL, @xmath can be used as a shorthand for @xmath . After
all, we have that

  -- -------- --
     @xmath   
  -- -------- --

With this definition, we can immediately define the most basic
short-circuit logic we distinguish:

###### Definition 9.

(free short-circuit logic) is the short-circuit logic that implies no
other consequences than those of the module expression .

Using these definitions we can provide equations that are derivable from
. The question whether a finite axiomatization of FSCL with only
sequential conjunction, negation and @xmath exists, is open, but the
following set of equations for FSCL is proposed in [5]: ² ² 2 In [5] it
is stated that the authors did not find any equations derivable from
FSCL but not from EqFSCL.

  -- -------- -------- -- ---------
     @xmath   @xmath      (SCL1)
                          (SCL2)
     @xmath   @xmath      (SCL3)
              @xmath      (SCL4)
              @xmath      (SCL5)
              @xmath      (SCL6)
                          (SCL7)
                          (SCL8)
                          (SCL9)
                          (SCL10)
  -- -------- -------- -- ---------

Note that equations SCL2 and SCL3 imply a left-sequential version of De
Morgan’s laws.

An important equation that is absent is the following:

  -- -- --
        
  -- -- --

This is what we would expect, since evaluation of @xmath (with @xmath a
closed term) can generate a side effect that is absent in the evaluation
of @xmath , although we know that evaluation of @xmath always yields
@xmath .

We now have the most basic short-circuit logic and some of its equations
defined, but of course there also is a “most liberal” short-circuit
logic below propositional logic. This logic is based on memorizing CP
and satisfies idempotence of @xmath (and @xmath ), but not its
commutativity. It is defined as follows:

###### Definition 10.

(memorizing short-circuit logic) is the short-circuit logic that implies
no other consequences than those of the module expression

  -- -- -------- --
        @xmath   
        @xmath   
                 
  -- -- -------- --

For the set of axioms EqMSCL, intuitions and an example, and a
completeness proof of MSCL we refer the reader to [ 5 ] . Adding the
axiom @xmath to MSCL, or equivalently, the axiom @xmath to CP @xmath ,
yields so-called static short-circuit logic (SSCL), which is equivalent
with propositional logic (be it in sequential notation and defined by
short-circuit evaluation).

###### Definition 11.

(static short-circuit logic) is the short-circuit logic that implies no
other consequences than those of the module expression

  -- -- -------- --
        @xmath   
        @xmath   
        @xmath   
                 
  -- -- -------- --

### 5.3 Repetition-Proof Short-Circuit Logic

With both the most basic as well as the most liberal short-circuit logic
we distinguish defined, we can present the variant of short-circuit
logic that we are interested in because it underlies the logic of
formulas in DLA @xmath : repetition-proof short-circuit logic (RPSCL).
This SCL-variant stems from an axiomatization of proposition algebra
called repetition-proof CP (CP @xmath ) that is in between and @xmath
and involves explicit reference to a set @xmath of atoms (propositional
variables).

The axiom system CP @xmath is defined as the extension of CP with the
following two axiom schemes (for @xmath ), which imply that any
subsequent evaluation result of an atom @xmath equals the current one:

  -- -------- -------- -- ---------
     @xmath   @xmath      (CPrp1)
     @xmath   @xmath      (CPrp2)
  -- -------- -------- -- ---------

We write @xmath to denote the set of these axioms schemes in the format
of module algebra. In CP @xmath the conditional cannot be expressed in
terms of @xmath , @xmath and @xmath : in [ 4 ] it is shown that the
propositional statement @xmath (for atoms @xmath ) cannot be expressed
modulo repetition-proof valuation congruence, that is, the valuation
congruence axiomatized by CP @xmath . The definition of RPSCL then
becomes:

###### Definition 12.

(repetition-proof short-circuit logic) is the short-circuit logic that
implies no other consequences than those of the module expression

  -- -- -------- --
        @xmath   
        @xmath   
                 
  -- -- -------- --

The equations defined by include those that are defined by as well as
for @xmath :

  -- -- -- -- --------
              (RP1)
              (RP2)
              (RP3)
              (RP4)
              (RP5)
              (RP6)
              (RP7)
              (RP8)
              (RP9)
              (RP10)
              (RP11)
              (RP12)
  -- -- -- -- --------

It is an open question whether the equations SCL1-SCL10 and the equation
schemes RP1-RP12 axiomatize RPSCL, but it will be shown below that RPSCL
is the logic that models equivalence of formulas in DLA @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

For this reason, we add the conditional @xmath and the constant @xmath
to DLA @xmath (thus making @xmath and @xmath definable). In order to
decide whether different DLA @xmath formulas are equivalent, just
translate these to CP @xmath and decide their equivalence (either by
axiomatic reasoning or by checking their repetition-proof valuation
congruence). So, we extend the formulas in DLA @xmath in order to
characterize the logic that models their equivalence. In this extension
of DLA @xmath , which we baptize DLCA @xmath (for Dynamic Logic with the
Conditional and Assignments as Formulas), truth in @xmath relative an
initial valuation @xmath for the conditional is defined as follows:

  -- -- -- --------
           (DLCA)
  -- -- -- --------

This means that we need an extra equation for the program extraction
function @xmath too which handles the conditional. For model @xmath ,
initial valuation @xmath and @xmath

  -- -------- --
     @xmath   
  -- -------- --

In the remainder of this section we consider formulas over this
signature, thus formulas over @xmath composed with @xmath . Below we
will prove for all mentioned axioms that they are valid in DLCA @xmath .

###### Proposition 5.

Let @xmath be a model for DLCA @xmath . The axiom CP1, that is

  -- -------- -- -------
     @xmath      (CP1)
  -- -------- -- -------

is valid in @xmath .

###### Proof.

Let @xmath be arbitrary formulas and let @xmath be an initial valuation.
Regardless of @xmath , we have @xmath (by QDL3), so by DLCA, we get
@xmath iff for @xmath , @xmath . Since @xmath , we indeed have that
@xmath iff @xmath . ∎

###### Proposition 6.

Let @xmath be a model for DLCA @xmath . The axiom CP2, that is

  -- -------- -- -------
     @xmath      (CP2)
  -- -------- -- -------

is valid in @xmath .

###### Proof.

Let @xmath be arbitrary formulas and let @xmath be an initial valuation.
@xmath is a shorthand for @xmath , so we first need QDL6, which states
that @xmath iff not @xmath , which is never the case. So for any initial
valuation @xmath , @xmath is false. Thus by DLCA, we get @xmath iff for
@xmath , @xmath . Since @xmath , we indeed have that @xmath iff @xmath .
∎

###### Proposition 7.

Let @xmath be a model for DLCA @xmath . The axiom CP3, that is

  -- -------- -- -------
     @xmath      (CP3)
  -- -------- -- -------

is valid in @xmath .

###### Proof.

Let @xmath be an arbitrary formula and let @xmath be an initial
valuation. If @xmath then by DLCA we get for @xmath , @xmath , which
also is true. If @xmath then by DLCA we obtain @xmath (note that also in
this case, @xmath is defined), which also is false. Thus @xmath iff
@xmath and hence the axiom CP3 is valid. ∎

###### Proposition 8.

Let @xmath be a model for DLCA @xmath . The axiom CP4, that is

  -- -------- -- -------
     @xmath      (CP4)
  -- -------- -- -------

is valid in @xmath .

###### Proof.

Let @xmath be arbitrary formulas and let @xmath be an initial valuation.
We are going to have to show that

  -- -------- --
     @xmath   
  -- -------- --

We have to apply DLCA multiple times here. By applying it to the left
hand side we get for @xmath

  -- -------- --
     @xmath   
  -- -------- --

By applying DLCA again to @xmath we get for @xmath

  -- -------- --
     @xmath   
  -- -------- --

So if @xmath and @xmath , we get @xmath . If on the other hand @xmath
but @xmath , we also get @xmath . In all other situations we get @xmath
.

Let us now consider the right hand side of the equation. Here we get for
@xmath :

  -- -- --
        
  -- -- --

Let us first turn our attention to the situation where @xmath . We need
to apply DLCA again and get for @xmath

  -- -------- --
     @xmath   
  -- -------- --

In the situation where @xmath , we get for @xmath

  -- -------- --
     @xmath   
  -- -------- --

So on the right hand side, if @xmath and @xmath , we get @xmath . If
@xmath but @xmath , we also get @xmath . In the other situations we get
either @xmath or @xmath .

To prove that is the same result as on the left-hand side, we need to
prove that @xmath , @xmath if @xmath , and @xmath if @xmath . The last
two statements seem contradictory, but as we will see @xmath can
actually take two different valuations depending on the truth of @xmath
. The mentioned variations are all determined using the program
extraction function. To recap, we have the following:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

We can immediately see that @xmath . Using the updated definition for
the program extraction function we get that

  -- -------- --
     @xmath   
  -- -------- --

Using the new rule for the conditional, we get that:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

To determine if @xmath , we need to have @xmath and we need to evaluate:

  -- -------- --
     @xmath   
  -- -------- --

By QDL12, we know that is equivalent to

  -- -------- --
     @xmath   
  -- -------- --

So indeed we have that if @xmath , then @xmath . Using the same
argument, we get that if @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

Therefore, if @xmath then @xmath . ∎

With those four axioms proven, we already know for a fact that the logic
of formulas in DLA @xmath indeed is a short-circuit logic. To prove that
it is a repetition-free short-circuit logic, we need to prove the axiom
schemes CPrp1 and CPrp2, too. Those axiom schemes make use of atoms
@xmath .

###### Proposition 9.

Let @xmath be a model for DLCA @xmath . The axiom CPrp1, that is

  -- -------- -- ---------
     @xmath      (CPrp1)
  -- -------- -- ---------

is valid in @xmath .

###### Proof.

Let @xmath be arbitrary formulas and @xmath an initial valuation. @xmath
can either be true or false. If it is false, both the left hand side and
the right hand side, by DLCA, are determined for @xmath by @xmath . If
it is true, the question if @xmath is asked. We have to prove that for
every atom @xmath , the reply to this will be the same as the reply to
@xmath (namely, true), that is:

  -- -------- --
     @xmath   
  -- -------- --

Recall that @xmath can be of the forms @xmath . For the first two atoms
we can immediately see our claim is true, since @xmath and therefore
@xmath . For @xmath the claim immediately follows from DLA9: it is,
regardless of the valuation, always true. ∎

###### Proposition 10.

Let @xmath be a model for DLCA @xmath . The axiom CPrp2, that is

  -- -------- -- ---------
     @xmath      (CPrp2)
  -- -------- -- ---------

is valid in @xmath .

###### Proof.

This is the symmetric variant of CPrp1 and proven similarly. ∎

By proving the validity of these axiom schemes in DLCA @xmath we have
proven that the equations SCL1-SCL10 together with RP1-RP12 are axioms
for formulas in DLCA @xmath . CP @xmath indeed is the most identifying
extension of CP which is valid for formulas. After all, the first more
identifying extension we distinguish is CP @xmath ( contractive CP ) [ 5
] , from which amongst others the following weak contraction rule can be
derived: for @xmath

  -- -- --
        
  -- -- --

Clearly this is not valid for DLA @xmath -formulas such as @xmath .

## Chapter 6 A treatment of side effects

### 6.1 Introduction

Now that we have defined a system to model program instructions and
program states, we can return to our original problem: that of formally
defining side effects. Like I said in Section 2.1 , the basic idea is
that a side effect has occurred in the execution of a program if there
is a difference between the actual evaluation and the expected
evaluation of a program given an initial valuation.

We can immediately see however, that we cannot build a definition of
side effects based on the actual and expected evaluation of an entire
program. Such a definition will get into trouble when there are multiple
side effects, especially if those cancel each other out or reinforce
each other. Consider for example the following program:

  -- -------- --
     @xmath   
  -- -------- --

If we are only going to look at the entire program, we will detect one
side effect here, that has incremented the value of @xmath by two.
However, it appears to be more acceptable to say that two side effects
have occurred, that happen to affect the same value.

It gets even more interesting if there is a formula in between the two
clauses above and the clauses themself cancel each other out:

  -- -- --
        
  -- -- --

If we again only look at the entire program, we will detect no side
effects (unless side effects occur in @xmath ). However, because @xmath
might use or modify @xmath as well, it seems we will have to pay
attention to the side effect of the first clause, even though it will be
cancelled out on by the last clause.

So instead of building a definition of side effects by looking only at
the actual and expected evaluation of an entire program, we are going to
build it up starting at the instruction level.

### 6.2 Side effects in single instructions

As said, we are going to use a bottom-up approach to define side
effects, so we will first define side effects for single instructions,
then move up to basic instructions and end with a full definition of
side effects for programs.

The idea is that the side effect of a single instruction is the
difference between the actual and expected evaluation of a single
instruction. This difference is essentially the difference between the
resulting valuations after, respectively, the actual and expected
evaluation of the single instruction. The difference between two
valuations is defined as follows:

###### Definition 13.

Given a model @xmath , the difference between valuations @xmath and
@xmath is defined as those variables that have a different assignment in
@xmath and @xmath :

  -- -------- --
     @xmath   
  -- -------- --

This notion of difference is not symmetric.

We already know what the actual evaluation of a single instruction is:
for this we can use DLA @xmath . This leaves us to define the expected
evaluation. For this we need to know for each single instruction how we
expect it to evaluate, that is, what changes we expect it to make to the
initial valuation. We have the following expectations of each single
instruction:

-    Assignments change the initial valuation by updating the variable
    assignment of the variable under consideration to the
    (interpretation of the) new variable assignment.

-    Tests do not change the initial valuation: they only yield @xmath
    or @xmath and steer the rest of the program accordingly.

We need the following equations for determining the expected evaluation
@xmath of a single instruction:

  -- -------- -------- -- -------
     @xmath               (EV1)
     @xmath   @xmath      (EV2)
     @xmath   @xmath      (EV3)
     @xmath               (EV4)
  -- -------- -------- -- -------

  -- -------- -------- -- -------
     @xmath   @xmath      (EV5)
     @xmath   @xmath      (EV6)
  -- -------- -------- -- -------

Now that we have the actual and the expected evaluation of a single
instruction, we can define its side effects. As said, this is going to
be the difference between the two resulting valuations.

###### Definition 14.

Let @xmath be a single instruction. Let model @xmath be given and let
@xmath be an initial valuation. Furthermore, let @xmath be a valuation
such that @xmath and let @xmath be a valuation such that @xmath . The
set of side effects of single instruction @xmath given model @xmath and
initial valuation @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

It is important to note that the valuations @xmath and @xmath as meant
in the above definition may not exist. We are not interested in those
situations, however. If @xmath and @xmath do exist, they are unique.
Also note that @xmath returns the variable assignment of valuation
@xmath if there is a difference with the variable assignment of
valuation @xmath . Thus, the set of side effects is defined as a set
containing those variables that have a different assignment after the
actual and expected valuation, with as assignments the ones the
variables actually get (that is, the assignments they will have after
evaluating the single instruction with the actual evaluation).

We will illustrate this with two examples. First, consider the single
instruction @xmath , evaluated under model @xmath in initial valuation
@xmath with @xmath . We want to know if this causes a side effect, so we
need to know the actual evaluation and the expected evaluation. To
calculate the actual evaluation, we need to know if @xmath and if yes,
for which valuation @xmath . The equations for DLA @xmath immediately
give us the answer, in this case via QDL10: @xmath . So we get @xmath .

Getting the expected evaluation works in a similar fashion, but instead
of DLA @xmath we now use the equations above to evaluate @xmath . Since
the equation for evaluating an assignment (EV5) is the same as QDL10, we
now get the exact same expected evaluation as the actual evaluation.
Thus we get @xmath and therefore @xmath . We can immediately see that
this results in the set of side effects being empty:

  -- -------- --
     @xmath   
  -- -------- --

This is of course what we would expect: an assignment should not have a
side effect if it does not occur in a steering fragment. Let us now
consider an example where we do expect a side effect: namely if an
assignment does occur in a steering fragment: @xmath . We use the same
initial valuation @xmath . First we try to find the actual evaluation
again, which we do by evaluating @xmath . We now need DLA11, which tells
us that (in this case) @xmath iff @xmath and @xmath . Both evaluate to
true, the latter with @xmath .

The expected update once again takes us to the equations above; we need
to determine @xmath such that @xmath . For tests, the demands are fairly
simple: @xmath and @xmath (see EV6). The latter is by EV4 defined to be
always true. As a result, we get @xmath . Thus we get the following set
of side effects:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Again, this is exactly what we want: since we expect formulas to only
yield true or false, the change this formula makes to the program state
upon evaluation is a side effect.

### 6.3 Side effects in basic instructions

With side effects for single instructions defined, we can move up a step
to side effects in basic instructions. The difference between single and
basic instructions is that in basic instructions, complex steering
fragments are allowed. This means that we are going to have to define
how side effects are handled in tests that contain a disjunction (
@xmath ), conjunction ( @xmath ) or negation ( @xmath ). The idea is
that the set of side effects of the whole formula is the union of the
sets of side effects of its primitive parts. However, we also have to
pay attention to the short-circuit character of @xmath . Only the
primitive formulas that get evaluated can contribute to the set of side
effects.

With this in mind, we can give the definition for side effects in
(possibly) complex steering fragments. Like before, we are only
interested in the side effects if the test actually succeeds. We need to
define this for disjunctions, conjuctions and negations:

###### Definition 15.

Let @xmath be a disjunction. Let model @xmath and initial valuation
@xmath be given, with @xmath and where @xmath is in its normal form.
Furthermore, let @xmath be the valuation after evaluation of formula
@xmath , that is, @xmath . The set of side effects @xmath is defined as:

  -- -------- --
     @xmath   
  -- -------- --

The case distinction is in place because of the short-circuit character
of @xmath . For the definition of its dual @xmath we do not need this
case distinction, because since we are again only interested in the side
effects if the (entire) formula succeeds, all the formulas in the
conjunction have to yield true. Therefore, the definition for
conjunction is a bit easier:

###### Definition 16.

Let @xmath be a conjunction. Let model @xmath and initial valuation
@xmath be given, with @xmath and where @xmath is in its normal form.
Furthermore, let @xmath be the valuation after evaluation of primitive
formula @xmath , that is, @xmath . The set of side effects @xmath is
defined as:

  -- -------- --
     @xmath   
  -- -------- --

The recursive definitions for disjunction and conjunction work because
eventually, a primitive formula will be encountered, for which the side
effects are already defined. Unfortunately, we cannot use a similar
construction for negation. This is because the side effects in a
primitive formula are only defined if that formula yields true upon
evaluation, so we cannot simply treat negation as a transparent operator
(that is, it is typically not true that @xmath ). So we will have to
define negation the hard way instead. Because we are using formulas in
normal form in the other definitions, we only have to define negation
for primitive formulas:

###### Definition 17.

Let @xmath be a negation. Let model @xmath be given and let @xmath be an
initial valuation. Furthermore, let @xmath be a valuation such that
@xmath and let @xmath be a valuation such that @xmath . The set of side
effects of basic instruction @xmath given model @xmath and initial
valuation @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

Now that we have a definition for side effects in (complex) steering
fragments, the extension of our definition of side effects in single
instructions to side effects in basic instructions is trivial:

###### Definition 18.

Let @xmath be a basic instruction. Let model @xmath and initial
valuation @xmath be given and let @xmath be a valuation such that @xmath
. The set of side effects @xmath is defined as:

  -- -------- --
     @xmath   
  -- -------- --

We can illustrate this with a simple, yet interesting example. Consider
the following basic instruction: @xmath with initial valuation @xmath
such that @xmath . In this situation we have two side effects that
happen to cancel each other out. The resulting valuation after the
actual evaluation of this basic instruction will be the same as the
initial valuation @xmath .

First we observe that the formula in this basic instruction is in its
normal form, a trivial observation since no negations occur in it. There
are two primitive formulas in this conjunction, so the set of side
effects is:

  -- -- -------- --
        @xmath   
                 
  -- -- -------- --

Here @xmath is determined by @xmath , so we get @xmath . We have already
seen in the previous section how the parts of the union above evaluate,
so we get:

  -- -- -------- --
        @xmath   
        @xmath   
  -- -- -------- --

So with this definition we have avoided the trap of not detecting any
side effects when there are two side effects that cancel each other out.
Instead we have two side effects here, the last of which happens to
restore the valuation of @xmath to its original one.

### 6.4 Side effects in programs

If we are going to extend our definition to that of side effects in
programs, we are going to have to define how concatenation, union and
repetition are handled.

Defining side effects for entire programs is more complicated than
defining side effects for single and basic instructions. This is because
two composition operators, namely union and repetition, can be
non-deterministic. As we have mentioned before, however, we are only
interested in (the side effects of) deterministic programs. This leaves
us to define how side effects are calculated for the composition
operators of deterministic programs. For concatenation, this is trivial.
We once again require that the entire program can be evaluated with the
given initial valuation. The set of side effects of a program then is
the union of the side effects in its basic instructions that are
executed given some initial valuation:

###### Definition 19.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . Furthermore, let @xmath be the valuation such that @xmath . The
set of side effects @xmath is defined by:

  -- -------- --
     @xmath   
  -- -------- --

This works in a similar fashion as the definition of side effects in
complex steering fragments. We can return now to our example given in
the Introduction of this chapter: @xmath . The above definition indeed
avoids the trap presented there, namely that this program only yields a
single side effect. To see this, consider initial valuation @xmath such
that @xmath . We will then get @xmath and therefore @xmath , so the set
of side effects becomes:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Similarly, side effects that cancel each other out, such as in @xmath
will now perfectly be detected, resulting for the same initial valuation
@xmath in a set of side effects @xmath .

Another interesting observation is that the transformation as defined in
Proposition 2 , which eliminates occurences of @xmath and @xmath in
steering fragments, not only preserves the relational meaning, but also
the side effects of such a steering fragment. The programs @xmath and
its transformed version @xmath are an illustration of this: we can
easily see that both have the same set of side effects.

With concatenation defined, we can move on to the next composition
operators: union and repetition. For this we can use the property that
given an initial valuation, every (terminating) deterministic program
has a unique canonical form that executes the same basic instructions
(see Proposition 4 in Chapter 4 ). This makes the definition of side
effects for programs containing a union or repetition straight-forward:

###### Definition 20.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . Furthermore, let @xmath be the deterministic program in
canonical form as meant in Proposition 4 . The set of side effects
@xmath is defined by:

  -- -------- --
     @xmath   
  -- -------- --

We can illustrate how this works by returning to our running example,
discussed in detail in Section 3.2 :

  -- -------- --
     @xmath   
              
     @xmath   
     @xmath   
  -- -------- --

In DLA @xmath , this translates to the following deterministic program
@xmath :

  -- -------- --
     @xmath   
              
     @xmath   
              
  -- -------- --

We have already seen that for @xmath , there is a valuation @xmath such
that @xmath (namely @xmath ). We can break this program down as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We want to know the set of side effects in this program. This is
determined as follows:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

where we get @xmath by evaluating @xmath . Thus, @xmath . We can easily
see that the first set of side effects @xmath . The interesting part is
the second set of side effects, since we now have a deterministic
program of the form @xmath . Here @xmath and @xmath .

We now have to ask ourselves what the canonical form of @xmath given
valuation @xmath is. This is determined by the outcome of the test

  -- -- --
        
  -- -- --

It is easy to see that this yields true. Thus, the canonical form @xmath
of @xmath is

  -- -------- --
     @xmath   
  -- -------- --

Therefore according to our definition, for @xmath :

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We can once again immediately see that the second set of side effects
@xmath . The first set of side effects is determined in a similar
fashion as in the example in the previous section. In the end, it gives
us:

  -- -------- -------- --
     @xmath            
              @xmath   
  -- -------- -------- --

So we again get a union of two sets of side effects, where we get @xmath
by evaluating @xmath . Thus, @xmath . It should be clear by now that the
first set of side effects contains one side effect, namely @xmath ,
whereas the latter does not contain any side effects. This gives us as
final set of side effects:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

This is exactly the side effect we have come to expect from our running
example.

We can now move on to an example of side effects in programs containing
a repetition. Recall that repetition is defined as follows:

  -- -------- -- ---------
     @xmath      (QDL14)
  -- -------- -- ---------

So, @xmath either gets executed not at all or at least once. The form of
programs we are interested in is

  -- -------- --
     @xmath   
  -- -------- --

In this case there will only ever be exactly one situation in which the
program gets evaluated (see Proposition 3 in Chapter 4 ). Our definition
of canonical forms tells us that given an initial valuation @xmath and
@xmath as meant in Proposition 3 , the canonical form @xmath of @xmath
is

  -- -------- --
     @xmath   
  -- -------- --

Using this we get the following set of side effects of a deterministic
program of the above form:

  -- -------- --
     @xmath   
  -- -------- --

As an example of this, we can return to a slightly modified version of
the example we gave in Section 3.3.2 .

  -- -------- --
     @xmath   
     @xmath   
              
     @xmath   
  -- -------- --

In DLA @xmath , this translates to the following deterministic program
@xmath given model @xmath and initial valuation @xmath such that @xmath
:

  -- -- --
        
  -- -- --

Clearly this is a deterministic program in the form we are interested in
and there is a valuation @xmath such that @xmath . In this case we have
@xmath with @xmath . To get the canonical form @xmath of @xmath , we
need to find the iteration @xmath for which @xmath will succeed, but for
which the test will not succeed another time. This will be for @xmath .
After all, after three iterations we will have valuation @xmath . With
this valuation, the test @xmath will fail, or to put it formally: @xmath
. This means that we will get the following set of side effects:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Is this the result we would expect? The answer is yes. It is clear that
for each time the test is evaluated, a side effect occurs. The test is
performed four times: three times it succeeds (after which the program
executes the body of its loop) and the fourth time it fails, but not
after updating the valuation of @xmath . The program evaluates with as
final valuation @xmath .

### 6.5 Side effects outside steering fragments

The keen observer will have noticed by now that under our current
definition, side effects can only occur in steering fragments. I have
been going through quite some trouble, however, to make my definitions
of side effects as general as possible. Even though in this thesis I am
only interested in side effects in steering fragments, I am fully aware
that views can differ on what the main effect and what the side effect
of an instruction is. That may either be a matter of opinion or a matter
of necessity, as in different systems, the same instruction may have a
side effect in one system and not in the other.

The way my definitions of side effects ¹ ¹ 1 As well as the definitions
of classes of side effects presented in Chapter 7 . are built up, one
need only change the expected evaluation of an instruction in order to
change if it is viewed as a side effect in a certain context. Consider,
for example, the sometimes accepted view that an assignment causes a
side effect, no matter where it occurs in a program. This view is for
example expressed by Norrish in [ 17 ] . The only change we would need
to make to our system to incorporate that view is a change to the
expected evaluation of the assignment, which would then become:

  -- -------- --
     @xmath   
  -- -------- --

The consequence of this in our current setting would be that the
expected evaluation of every program always has a resulting valuation
@xmath that is equal to the initial valuation @xmath , since only
assignments can make changes to a valuation currently and by the above
definition we do not expect any assignment to do so, wherever it occurs
in the program. As a consequence, any change to the valuation (caused by
the actual evaluation) will automatically be a side effect.

It is almost as simple to add new instructions to our setting. I
definitely do not want to claim that the instructions I have defined in
DLA @xmath are exhaustive, so this need may arise. If we were, for
instance, to re-introduce the random assignment @xmath , all we would
have to do was to define the actual and expected evaluation of this. The
actual evaluation is already given by Harel in [ 14 ] and Van Eijck in [
11 ] :

  -- -------- --
     @xmath   
  -- -------- --

If we also would want to allow random assignments in tests, we would
have to add a rule for that as well, similar to the one already in place
for normal assignments:

  -- -------- --
     @xmath   
  -- -------- --

The definition of the expected evaluation is dictated by what we really
expect the random assignment to do. This can be the same as what it
actually does, in which case we have to define the expected evaluation
to be the same as the actual evaluation above:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

If we expect random assignments to do something different, all we have
to do is define the expected evaluation accordingly. This expected
evaluation can literally be anything: from simply not updating the
valuation at all to always setting a completely unrelated variable to
42:

  -- -------- --
     @xmath   
  -- -------- --

On a side note, this example poses some interesting questions about
‘negative’ side effects. Under our current definition, setting the above
mentioned variable to 42 registers as a side effect, but in a somewhat
strange fashion. After all @xmath is a single instruction and for @xmath
and @xmath , @xmath . There will actually be two differences between
valuations @xmath and @xmath here: the actual evaluation updates
variable @xmath , whereas the expected evaluation leaves @xmath alone
but does update the variable the answer to life, the universe and
everything . Both variables will show up in the set of side effects,
both with the assignment the actual evaluation has assigned to them.

This fails to capture what has actually happened here: after all, not
only did an unexpected change to the initial valuation happen (a
‘regular’ side effect), but an expected change also did not happen (a
‘negative’ side effect). At least part of the information what should
have happened is lost, namely the value the variable the answer to life,
the universe and everything was supposed to get. ² ² 2 Which is quite a
shame, considering the trouble it cost to get it. It is an open question
if we should even allow these somewhat odd situations where the actual
evaluation does something completely different than we expect, thereby
generating a negative side effect. We leave this question, as well as
the question how we should handle these situations if we do choose to
allow them, for future work.

## Chapter 7 A classification of side effects

### 7.1 Introduction

In this chapter we will take a closer look at side effects in steering
fragments. In particular, we will give a classification of side effects.
This classification gives us a measure of the impact of a side effect.

As we have already mentioned in our introduction in Chapter 1 , Bergstra
has given an informal classification of side effects in [ 1 ] . Bergstra
makes a distinction between steering instructions and working
instructions. This distinction is based on a setting called Program
Algebra (PGA). In PGA, there is no distinction between formulas and
single instructions other than formulas, which is why the proposed
distinction by Bergstra is meaningful in that setting. Every basic
instruction @xmath in PGA yields a Boolean reply upon execution and can
therefore be made into a positive or negative test instruction @xmath or
@xmath . Naturally, this cannot be done in our setting of DLA @xmath ,
so instead of giving an overview of Bergstra’s paper, I will just
present the major classes of side effects Bergstra distinguishes and
what they come down to in our setting.

Bergstra’s first class of side effects is what he calls ‘trivial side
effects’. By this he means side effects that can only be found in e.g.
consequences for the length of the program or its running time. We are
usually not interested in those kinds of side effects, which is exactly
why Bergstra calls them trivial and why we would say that no side
effects occur at all. An instruction that only returns a meaningful
Boolean reply (that is, a Boolean reply that may differ depending on the
valuation the instruction is evaluated in) is an instruction that only
has trivial side effects. Examples of such instructions are the
comparision instructions such as ( @xmath ) or ( @xmath ). These
instructions can be turned into meaningful test instructions by
prefixing them with a @xmath or @xmath symbol. We will return to this in
our explanation of PGA in Chapter 8 . In our terms, these kinds of
instructions can only be formulas, occuring in steering fragments such
as @xmath or @xmath . To be precise, they can only be formulas that have
the same actual and expected evaluation, and thus no side effects.

The above described situation, where only trivial side effects occur, is
one extreme. The other extreme is when an instruction always yields the
same Boolean reply, regardless of when it is executed. Bergstra says
that in that case, only ‘trivial Boolean results’ occur and that the
instruction should be classified as a working instruction (that is, a
single instruction not being a formula). In our setting this is also
true with one notable exception: that of assignments. As we know,
assignments always return true, so their Boolean result is trivial.
Still, we allow them in formulas, too. If an instruction with trivial
Boolean results occurs outside a formula, its only relevance would be
its effect other than the Boolean reply, in which case you can hardly
call that effect a side effect. If it occurs in a formula, however, the
Boolean result — albeit trivial — does have relevance, so the effect
other than the Boolean reply can indeed be called a side effect. This is
exactly what happens in our setting.

What the classification between steering instructions and working
instructions gives us in the end, is a recommendation on how to use a
particular kind of instruction. Instructions such as comparision (
@xmath ), that only give a Boolean reply, have no meaning as a working
instruction and therefore ideally should only occur in steering
fragments. Other instructions such as assignment ( @xmath ) can be both
steering instructions as well as working instructions and can thus occur
both inside as well as outside steering fragments. Finally, instructions
such as writing to the screen (            write x           ) do not
return a meaningful Boolean reply and should therefore ideally not occur
in steering fragments.

### 7.2 Marginal side effects

#### 7.2.1 Introduction

Having seen the base class of side effects, we can move on to the next
level, that of marginal side effects . The intuition behind a marginal
side effect is fairly simple: the side effect of a single instruction is
marginal if the remainder of the execution of the program is unaffected
by the occurrence of the side effect. The following program is a typical
example of one where a marginal side effect occurs:

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath can be any (deterministic) program. The side effect occurs
in the test. However, since the variable @xmath is no longer used in the
remainder of the program (which only consists of the single instruction
@xmath ), the remainder of the program is unaffected by the occurrence
of the side effect. Therefore, this side effect is marginal.

So what if @xmath does occur in the remainder of the program, for
example in this program:

  -- -------- --
     @xmath   
  -- -------- --

This is a typical example of a program in which the occuring side effect
is not marginal. The reason is that the assignment in the remainder of
the program ( @xmath ) has a different effect on the variable @xmath
than when it would have had if the side effect had not occurred. For
instance, for initial valuation @xmath such that @xmath (and assuming
@xmath does not occur in @xmath ), the assignment maps @xmath to @xmath
. If the side effect had not occurred, it would have had a different
effect on @xmath (namely, it would have mapped it to @xmath ).

Another typical example of a program in which an occuring side effect is
not marginal is our running example:

  -- -- --
        
  -- -- --

Here @xmath can again be any deterministic program and the side effect
occurs in the same place as in our first example. However, the test is
now a complex test and in the second part of the test, @xmath is used.
Suppose the valuation after evaluation of @xmath is @xmath such that
@xmath . The second part of the test ( @xmath ) will now give a
different reply if a side effect does not occur in the first part (or if
that side effect would have affected a different variable). As a result,
the remainder of the program is affected by the side effect: it will be
executed differently if a side effect occurs.

Perhaps the answer to the question if the side effect is marginal is
less clear when the initial valuation in the previous example would not
have been @xmath with @xmath , but for example with @xmath . It is still
the case that the variable @xmath , that is affected by a side effect,
is used again in the remainder of the program, but now it does not
change the outcome of the (complex) test. Is that side effect still not
marginal then? The same question can be posed about the following
example:

  -- -------- --
     @xmath   
  -- -------- --

Regardless of initial valuation @xmath , at the end of this program
(assuming @xmath terminates), @xmath will always be mapped to 42. So is
the side effect in the test marginal or not? The answer can be found by
checking if the remainder of the program is executed in the same way, or
more formally: if the actual update of the remainder of the program is
the same regardless of whether a side effect has occurred. In both our
last examples, the answer to that last question is yes. After all, in
the first example the test @xmath will fail whether @xmath has been
incremented first or not, and in the second example @xmath will always
be mapped to @xmath , again regardless of the side effect that
incremented @xmath earlier. Therefore, the side effects in the discussed
instructions are marginal.

#### 7.2.2 Marginal side effects in single instructions

Although the intuition of marginal side effects should be clear enough
by now, formally defining it is tricky because we have to define
precisely what the remainder of a (deterministic) program @xmath given a
single instruction @xmath and an initial valuation @xmath is. Before we
can define that, we also need to know the history of that same program
given single instruction @xmath , which is loosely described as those
(single or basic) instructions that have already been evaluated when
@xmath is about to get evaluated.

In what follows we are going to assume that in a certain deterministic
program @xmath a single instruction @xmath occurs that is causing a side
effect. Furthermore, we are going to use that given initial valuation
@xmath , any deterministic program has a unique canonical form that has
the same behavior (see Proposition 4 in Chapter 4 ). Defining the
history and remainder of a deterministic program is straight-forward if
that program is in canonical form. Also, we can actually immediately
give a more general definition than what we need here, namely the
history and remainder of a deterministic program given a basic
instruction. This extra generality will come in handy later on.

###### Definition 21.

Let @xmath be a deterministic program in canonical form. Let model
@xmath and initial valuation @xmath be given and let @xmath be the
valuation such that @xmath . Let @xmath be a basic instruction occuring
in @xmath , that is, @xmath is of the form @xmath , with @xmath and
@xmath being possibly empty deterministic programs in canonical form.
The history of program @xmath given basic instruction @xmath is defined
as:

  -- -------- --
     @xmath   
  -- -------- --

The remainder of program @xmath given basic instruction @xmath is
defined as:

  -- -------- --
     @xmath   
  -- -------- --

Using Proposition 4 the extension of the definitions of history and
remainder of a program to all deterministic programs (not just the ones
in canonical form) is trivial:

###### Definition 22.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . Furthermore, let @xmath be the deterministic program in
canonical form as meant in Proposition 4 . The history of program @xmath
given basic instruction @xmath is defined as:

  -- -------- --
     @xmath   
  -- -------- --

The remainder of program @xmath given basic instruction @xmath is
defined as:

  -- -------- --
     @xmath   
  -- -------- --

With definitions for the history and the remainder of a program in hand,
we can define marginal side effects. According to our intuition, a side
effect should be marginal if the evaluation of the remainder of the
program is the same regardless of whether the side effect occurred. We
can tell if that is the case by evaluating the remainder of the program
with two different valuations: one in which the single instruction in
which the side effect occurs has been evaluated using the actual
evaluation, and one in which is has been evaluated using the expected
evaluation. ¹ ¹ 1 We now need to restrict ourselves again to single
instructions because the expected evaluation is (currently) undefined
for complex steering fragments. If the only difference between those two
valuations is exactly the side effect that occurred in the single
instruction, or if there is no difference between those two valuations
at all, then we can say that the evaluation of the remainder of the
program has been the same. This is formally defined as follows:

###### Definition 23.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . Let @xmath be a single instruction in program @xmath causing a
side effect, that is, for @xmath , @xmath . Let @xmath be the valuation
such that @xmath and let @xmath be the valuation such that @xmath . The
side effect in @xmath is marginal iff for @xmath

  -- -------- --
     @xmath   
  -- -------- --

So what happens here exactly? To show this, we return to the examples we
have given earlier in this section. First, consider the program @xmath ,
with initial valuation @xmath such that @xmath . We can observe that
@xmath is in canonical form. In this program, a side effect occurs in
the single instruction @xmath . So is this side effect marginal or not?
Here we have the following:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

As we can see, the valuations @xmath and @xmath are the same. Using our
current definition of the expected evaluation, this will always be the
case, so we could just use valuation @xmath here. However, as I have
said in Section 6.5 of Chapter 6 , I want to keep generality in the
definitions of side effects. We might want to change the definition of
the expected evaluation in the future or add new instructions or
connectives that do modify the initial valuation. Therefore, we use
valuation @xmath , the resulting valuation after evaluating the single
instruction @xmath with the expected evaluation.

To determine if the side effects are marginal, we have to ask ourselves
if

  -- -------- --
     @xmath   
  -- -------- --

We know how to calculate the set of side effects; it is @xmath . In this
case, @xmath is @xmath too, so the side effect occurring in @xmath is
marginal, which is what we want. We can also clearly see in this case
that it is no coincidence that we are testing @xmath and not @xmath : we
need the valuation that is the result of evaluating the single
instruction using the actual evaluation in order to properly compare
this with the set of side effects.

We can now take a look at an example in which the side effect should not
be marginal. Consider the program @xmath , with initial valuation @xmath
such that @xmath . This program is in canonical form too and the side
effect occurs in the same single instruction @xmath . This time we get
the following:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

We have the same set of side effects: @xmath . However, @xmath now is
@xmath . Therefore the side effect is not marginal, which is again what
we would expect.

We have given a third example which closely resembles the ones we have
discussed above, namely @xmath . If we take the same initial valuation
@xmath as above, everything except the remainder of the program given
@xmath will be the same:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

With this example we can see why our definition of marginal side effects
allows the difference between @xmath and @xmath to be @xmath , too. We
have seen before that in situations like these, the side effects should
be marginal, and by allowing the difference to be @xmath , that indeed
is the case.

#### 7.2.3 Marginal side effects caused by primitive formulas

As we have seen, our current definition of marginal side effects is
capable of determining whether a side effect occurring in a single
instruction is marginal or not. We still have to define marginal side
effects for basic instructions. In particular, we need to have a
definition for the situation in which a primitive formula in a complex
test causes a side effect ² ² 2 We say that a primitive formula causes a
side effect here because a side effect cannot occur in a primitive
formula. It can, however, occur in a single or basic instruction which
tests that formula. and in that same test, the variable affected by that
side effect is used again, such as in the following program: @xmath . In
order to define how to determine if a side effect is marginal or not in
these situations, we need to extend our definitions of the history and
remainder of a program such that it not only works given a single
instruction, but also given a primitive formula. Before we can give that
definition, we first need to define the history and remainder of a
compound formula given a primitive formula. We are once again only
interested in those two concepts if the primitive formula @xmath gets
evaluated.

To get an idea of what the history and the remainder of a compound
formula given a primitive formula should be, consider the following
example:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
                       
  -- -------- -------- --

In this example, the history of @xmath given @xmath and given model
@xmath and initial valuation @xmath is empty. The remainder, however, is
not:

  -- -------- --
     @xmath   
  -- -------- --

Notice that this remainder should be empty if @xmath would have been
true.

The history of a formula of course is not always empty. To illustrate
that, we will first introduce a notational convention.

###### Notation.

We will write @xmath to refer to the primitive formula @xmath occurring
in formula @xmath at a specific position.

As an example of this, compare the formulas @xmath and @xmath . The
difference between the formulas @xmath and @xmath is in the instance of
primitive formula @xmath we are referring to.

Let @xmath and @xmath as in the example above. Now consider the
following example:

  -- -- --
        
  -- -- --

Here the history of @xmath given @xmath and given model M and initial
valuation @xmath such that @xmath is not empty:

  -- -------- --
     @xmath   
  -- -------- --

Now that we have given an intuition what the history and remainder of a
formula given a primitive formula and an initial valuation are going to
be, we can move on to giving the actual definitions. In what follows we
will assume that the @xmath in @xmath is in normal form and that the
specific primitive formula @xmath actually appears exactly once in
formula @xmath (although other instances of @xmath may occur in the
formula). @xmath can take the following forms:

  -- -- --
        
  -- -- --

Here @xmath is the same as @xmath . For each of these forms, we will
have to define how the history and the remainder is calculated.

###### Definition 24.

Let @xmath be a formula of one of the above forms. Let model @xmath and
initial valuation @xmath be given. Let @xmath be a primitive formula
occurring in @xmath such that @xmath gets evaluated during the
evaluation of @xmath given initial valuation @xmath . The history of
formula @xmath given primitive formula @xmath is defined as:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
              @xmath   
                       
              @xmath   
                       
  -- -------- -------- --

The remainder of formula @xmath given primitive formula @xmath is
defined as:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
                       
              @xmath   
                       
              @xmath   
  -- -------- -------- --

The reason we are only interested in the history and remainder of a
primitive formula if that formula is actually evaluated, is
straight-forward: we use these definitions to calculate the side effects
caused by that primitive formula and those side effects only exist if
the primitive formula is evaluated. As straight-forward as this is, the
restriction is an important one. Because we know that @xmath gets
evaluated (not be be confused with ‘yielding true’), we do not have to
take potentially troublesome formulas into account such as @xmath .

The above definitions make the history and remainder of a formula given
a primitive formula, partial functions. To see in which situations the
history and remainder are defined and for which they are not, consider
the following formula:

  -- -- --
        
  -- -- --

Now assume we want to know the history of @xmath given @xmath . This
history @xmath is only defined if @xmath gets evaluated, which in turn
only is the case if we have a initial valuation @xmath such that @xmath
. For all initial valuations @xmath such that @xmath , the history of
@xmath given @xmath is undefined. If we would be interested in the
history of @xmath given @xmath , the situation would be reversed: in
that case the history @xmath is only undefined with initial valuation
@xmath such that @xmath .

That the history (and the remainder) is undefined in these cases is not
problematic because as said, we are going to use these definitions to
check if the side effects caused by @xmath are marginal and @xmath can
only cause side effects if it gets evaluated.

Using these definitions, we can move on to define the history and
remainder of a program given a primitive formula:

###### Definition 25.

Let @xmath be a deterministic program in canonical form. Let model
@xmath and initial valuation @xmath be given and let @xmath be the
valuation such that @xmath . Let @xmath be a test occurring in program
@xmath , where @xmath is a formula in normal form. Finally, let @xmath
be a primitive formula occuring in @xmath such that @xmath gets
evaluated during the evaluation of @xmath given initial valuation @xmath
. The history of program @xmath given primitive formula @xmath is, for
@xmath , defined as:

  -- -------- --
     @xmath   
  -- -------- --

The remainder of program @xmath given primitive formula @xmath is
defined as:

  -- -------- --
     @xmath   
  -- -------- --

The final step is to give a definition to determine if a side effect
occurring in a primitive formula is marginal. Given the above, this
definition should not be surprising:

###### Definition 26.

Let @xmath be a deterministic program. Let model @xmath and initial
valuation @xmath be given and let @xmath be the valuation such that
@xmath . Let @xmath be a primitive formula in program @xmath causing one
of the side effects of @xmath . Let @xmath be the valuation such that
@xmath . Let @xmath be the valuation such that @xmath or @xmath and let
@xmath be the valuation such that @xmath or @xmath . ³ ³ 3 This
distinction is necessary because we can only evaluate a test if its
argument yields true. @xmath might actually yield false if @xmath is
part of a larger formula @xmath that despite that yields true, such as
@xmath such that @xmath . Thus, we need either @xmath or @xmath . The
side effect caused by @xmath is marginal iff for @xmath

  -- -------- --
     @xmath   
  -- -------- --

To show how this works, we return to the example given in the beginning
of this section: @xmath , with initial valuation @xmath such that @xmath
. Here the primitive formula @xmath causes a side effect. We can now use
our definition to find out if that side effect is marginal. For that, we
first need the history of @xmath given primitive formula @xmath . To
calculate @xmath , we first observe that @xmath is in normal form. This
gives us a go to use Definition 25 . This definition tells us to first
calculate valuation @xmath , which we get by evaluating @xmath . Here
@xmath is a basic instruction, so we can use Definition 22 to calculate
it. We have seen before how that evaluates:

  -- -------- --
     @xmath   
  -- -------- --

Thus we get @xmath , so @xmath .

All we need to do now to get the history we are looking for, is the
history of formula @xmath given primitive formula @xmath : @xmath . We
can use Definition 24 here and are in the situation where @xmath . Here
@xmath and @xmath , so as history we get:

  -- -------- -------- --
     @xmath            
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Thus, the history of program @xmath given primitive formula @xmath is:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

With the information above we can also immediately calculate the
remainder of formula @xmath given primitive formula @xmath :

  -- -------- -- --
     @xmath      
                 
                 
                 
  -- -------- -- --

Then all we need to determine the remainder of program @xmath given
primitive formula @xmath is the remainder of program @xmath given basic
instruction @xmath . To see how this evaluates, see the previous
section. We can use Definition 22 for this again and get:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

So the remainder of program @xmath given primitive formula @xmath is:

  -- -------- -------- --
     @xmath   @xmath   
                       
  -- -------- -------- --

Now that we have the history and the remainder of @xmath given @xmath ,
we can finally determine if the side effect occurring in @xmath is
marginal. To quickly recap, we have:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath            
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath            
  -- -------- -------- --

Here we have an example where we do not even have to determine if @xmath
is the same as @xmath , because there is no valuation @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

This is because for valuation @xmath the test @xmath will fail.
Therefore, the side effect in @xmath is ‘automatically’ not marginal,
which is indeed what we wanted.

### 7.3 Other classes of side effects

There are two more classes of side effects that I want to discuss. The
first is the class detectible side effects . According to Bergstra, a
side effect in an instruction is detectible if the fact that that side
effect has occured can be measured by means of a steering fragment
containing that instruction [ 1 ] . This is the most general class of
side effects: in my terms, any difference between the actual and the
expected evaluation of a single instruction is a detectible side effect.

The presence of detectible side effects suggests there are
non-detectible side effects as well. This can indeed be the case. A side
effect is undetectible if the evaluation of a (single) instruction
causing a side effect would normally change the program state, but
because of the specific initial valuation, it does not. As a simple
example, consider the single instruction @xmath . Under any initial
valuation @xmath this would change the program state and cause a side
effect, with one exception: namely if @xmath . We can formally define
this as follows:

###### Definition 27.

Let @xmath be a single instruction in model @xmath under initial
valuation @xmath , updating the valuation of a variable @xmath . ⁴ ⁴ 4
In DLA @xmath , this would mean that @xmath either is @xmath or @xmath .
Furthermore, let @xmath . @xmath contains an undetectible side effect
iff for @xmath such that @xmath :

  -- -------- --
     @xmath   
  -- -------- --

It remains to be seen whether these non-detectible side effects are
worth our attention. After all, not being able to detect side effects
suggests that the presence of the side effects does not make much
difference, in any case not to the further execution of the program.
Possible exceptions to this are the execution speed or the efficiency of
the program, especially if there are a lot of undetectible side effects.

In contrast to non-detectible side effects, marginal side effects can
potentially be very useful because they can occur far more often. Like
non-detectible side effects, they are a measure of the impact of a side
effect. If a side effect is marginal, that means that the rest of the
program is unaffected by it and therefore, the side effect is
essentially pretty harmless. One could at this point imagine a claim
that a program in which only marginal side effects occur can be
considered a well-written program, whereas a program in which
non-marginal side effects occur is one that should probably be rewritten
to avoid unexpected behavior. We will leave further investigation of
this claim for future work, however.

## Chapter 8 A case study: Program Algebra

In Chapter 6 , I presented the system I will be using for the treatment
of side effects. In this chapter I will provide a case study to see my
system in action. For this, we will use Program Algebra (PGA) [ 3 ] .
Since PGA is a basic framework for sequential programming, it provides
an ideal case study for our treatment of side effects. By showing how
side effects are determined in the very general setting of PGA, we are
essentially showing how they are dealt with on a host of different, more
specific programming languages.

I will first summarize PGA and explain how we can use it. Next, some
extensions necessary for our purpose will be presented. Finally, I will
present some examples to see in full how my system deals with side
effects.

### 8.1 Program Algebra

#### 8.1.1 Basics of PGA

PGA is built from a set @xmath of basic instructions (not to be confused
with the DLA @xmath -notion by the same name), which are regarded as
indivisible units. Basic instructions always provide a Boolean reply,
which may be used for program control (i.e. in steering fragments).
There are two composition constructs: concatenation and repetition. If
@xmath and @xmath are programs, then so is their concatenation @xmath
and its repetition @xmath . PGA has the following primitive
instructions:

-    Basic instruction Basic instructions are typically notated as
                   a,b,               …. As said they generate a Boolean
    value. Especially important for our purpose is that their associated
    behavior may modify a (program) state.

-    Termination instruction This instruction, notated as @xmath ,
    terminates the program.

-    Test instruction Test instructions come in two flavours: the
    positive test instruction, notated as @xmath (where @xmath is a
    basic instruction), and its negative counterpart, @xmath . For the
    positive test instruction, @xmath is evaluated and if it yields
                   true               , all remaining instructions are
    executed. If it yields                false               , the next
    instruction is skipped and evaluation continues with the instruction
    after that. For the negative test instruction, this is the other way
    around.

-    Forward jump instruction A jump instruction, notated as @xmath
    where @xmath can be any natural number. This instruction prescribes
    a jump to @xmath instructions from the current one. If @xmath , the
    program jumps to the same instruction and inaction occurs. If @xmath
    , the program jumps to the next instruction (so this is essentially
    useless). If @xmath , the next instruction is skipped and the
    program proceeds with the one after that, and so on.

If two programs execute identical sequences of instructions, instruction
sequence congruence holds between them. This can be axiomatized by the
following four axioms:

  -- -------- -------- -- --------
     @xmath   @xmath      (PGA1)
     @xmath   @xmath      (PGA2)
     @xmath   @xmath      (PGA3)
     @xmath   @xmath      (PGA4)
  -- -------- -------- -- --------

The first canonical form of a PGA program is then defined to be a PGA
program which is in one of the following two forms:

1.  @xmath not containing a repetition

2.  @xmath , with both @xmath and @xmath not containing a repetition

Any PGA program can be rewritten into a first canonical form using the
above four equations. The next four axiom schemes for PGA deal with the
simplification of chained jumps:

  -- -------- -------- -- --------
     @xmath   @xmath      (PGA5)
     @xmath   @xmath      (PGA6)
     @xmath   @xmath      (PGA7)
     @xmath               
     @xmath   @xmath      (PGA8)
  -- -------- -------- -- --------

Programs are considered to be structurally congruent if they can be
proven equal using the axioms PGA1-8.

The second canonical form of a PGA program is defined to be a PGA
program in first canonical form for which additionally the following
holds:

1.  There are no chained jumps

2.  Counters used for a jump into the repeating part of the expression
    are as short as possible

Each PGA expression can be rewritten into a shortest structurally
equivalent second canonical form using the above eight equations [ 3 ] .

#### 8.1.2 Behavior extraction

The previous section describes the forms a PGA program can take. In this
section I will explain the behavioral semantics defined in [ 3 ] . The
process of determining the behavior of a PGA program given its
instructions is called behavior extraction . The behavioral semantics
itself is based on thread algebra, TA in short.

Like PGA, TA has a set @xmath of basic instructions, which in this
setting are referred to as actions. Furthermore, it has the following
two constants and two composition mechanisms:

-    Termination This is notated as                S               (for
    Stop) and terminates the behavior.

-    Divergent behavior This is notated as
                   D               (for Divergence). Divergence (or
    inaction) means there no longer is active behavior. For instance,
    infinite jump loops cause divergent behavior since the program only
    makes jumps and does not perform any actions.

-    Postconditional composition This is notated as @xmath and means
    that first @xmath is executed; if its reply is
                   true               then the behavior proceeds with
    @xmath , otherwise it proceeds with @xmath .

-    Action prefix This is notated as @xmath and is a shorthand for
    @xmath : regardless of the reply of @xmath , the behavior will
    proceed with @xmath .

As said, behavior extraction determines the behavior of a PGA program
given its instructions. For that, the behavior extraction operator,
notated as @xmath , is defined. If a program ends without an explicit
termination instruction, it is defined to end in inaction by the
following equation:

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

A termination instruction followed by other instructions ends in
termination and nothing else, which is defined by the following
equation:

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

Behavior extraction is further defined by the following equations
dealing with the composition mechanisms:

  -- -------- -- -------
     @xmath      (8.3)
     @xmath      (8.4)
     @xmath      (8.5)
  -- -------- -- -------

The jump instruction requires a set of equations as well. The first
equation defines that a jump instruction which is jumping to itself
leads to inaction. The second and third define how a jump instruction
can skip subsequent instructions.

  -- -------- -- -------
     @xmath      (8.6)
     @xmath      (8.7)
     @xmath      (8.8)
  -- -------- -- -------

#### 8.1.3 Extensions of PGA

PGA is a most basic framework [ 18 ] . However, there are many
extensions that introduce more ‘advanced’ programming features such as
goto’s and backward jump instructions. Via projections, each of these
extensions can be projected to PGA in such a way that the resulting
PGA-program is behaviorally equivalent to the original program. Examples
of such extensions are PGLB , in which PGA is extended with a backward
jump instruction (             \            @xmath ) and PGLB @xmath ,
in which PGLB is further extended with a label catch instruction (
@xmath ) and an absolute goto instruction ( @xmath ).

Of particular interest for our purpose is the extension of PGA with the
unit instruction operator (PGA @xmath ), introduced in [ 18 ] . The idea
of the unit instruction operator, notated as @xmath , is to wrap a
sequence of instructions into a single unit of length 1. That way, a
more flexible style of PGA-programming is possible. In particular,
programs of the form

    if a then {
        b, c, d
    } else {
        f, g, h
    }

now have a more intuitive translation: @xmath . ¹ ¹ 1 The jump is
necessary to prevent the instructions @xmath , @xmath and @xmath from
being executed when @xmath yields true. Because, thanks to the unit
instruction operator, the instructions @xmath , @xmath , @xmath and
@xmath are viewed as a single instruction, the execution of those is
skipped when @xmath yields             false            .

### 8.2 Logical connectives in PGA

#### 8.2.1 Introduction

As mentioned in Section 8.1 , in PGA a lot of basic notations for
assembly-like programming languages are defined, especially with its
extension with unit instruction operators (PGA @xmath ) [ 18 ] .
However, one important basic notation is missing: that of complex tests,
of the form             if(a and b) then c            . As we have seen,
currently there are positive and negative test instructions in PGA,
which can only test the Boolean reply of a single instruction. More
complex constructions such as the one in the working example of Section
3.2 are however very common in programming practice and also appear in
research papers such as [ 1 ] , where they are referred to as complex
steering fragments. This means that for our purpose, PGA will have to be
extended to accommodate for complex steering fragments. I will do so
below.

Atomic steering fragments (that is, steering fragments containing only
one instruction) are already present in PGA in the form of the positive
and negative test instruction ( @xmath and @xmath respectively). If we
were to extend this with complex steering fragments, an obvious notation
would be @xmath and @xmath . The question now is what forms @xmath can
take and what it means to have such a complex test.

Since the instructions in the steering fragment need to produce a
Boolean reply, the answer to the question above in my opinion should be
that a complex test can only be meaningful if all the instructions in
the complex test may be used to determine the reply. It is not necessary
that all instructions are always used to determine the reply: for
instance when using short-circuit evaluation, in some situations not all
components of a complex test have to be (and therefore are not) used.
However, my claim here is that if a certain instruction is never
necessary to determine the Boolean reply of the whole steering fragment,
then is should not be in the steering fragment.

Currently, PGA has two composition constructs (composition and
repetition). Neither of those define anything, however, about the
Boolean value of multiple instructions. That is, the Boolean value of
@xmath and of @xmath is undefined. The intuitive way to determine the
Boolean reply of a sequence of instructions is via logical connections
such as And ( @xmath ) and Or ( @xmath ). However, these are not present
yet in PGA. This means that I will have to introduce them in an
extension of PGA @xmath , which we baptize PGA @xmath .

Before I do so, however, I need to say something more about the type of
And and Or I will be using. There are multiple flavours available:

-    Logical And / Or These versions are notated as @xmath and @xmath ,
    respectively. They use full evaluation and the order of evaluation
    is undefined.

-    Short-circuit Left And / Or These versions are the ones we use in
    DLA @xmath (see Chapter 6 ). They are notated as @xmath and @xmath .
    From here on I will refer to them as SCLAnd and SCLOr. They use
    short-circuit evaluation and are therefore not commutative. The left
    conjunct or disjunct is evaluated first. There naturally are
    right-hand versions as well, but I will not be using them.

-    Logical Left And / Or These versions are a combination of the other
    two: they use full evaluation, but the left conjunct or disjunct is
    evaluated first. I will notate this as @xmath and @xmath ,
    respectively and refer to them as LLAnd and LLOr. I will not discuss
    right-hand versions.

The latter two are interesting for our purpose, because they are very
suitable to demonstrate side effects. However, since we currently only
have SCLAnd and SCLOr at our disposal in DLA @xmath , I will concentrate
on those connectives. Although LLAnd and LLOr can be added to both PGA
and DLA @xmath , this would raise more questions than it answers, for
instance with regard to the logic which would then be behind the system,
which is why we leave it for future work.

The above connectives will almost always be used in combination with
either a positive or a negative test. This will be written as @xmath
(and similar for the negative test and the @xmath connective).

#### 8.2.2 Implementation of SCLAnd and SCLOr

If I am to introduce the mentioned logical connectives in PGA @xmath , I
will have to be able to project this extention into PGA. Since the
projection of PGA @xmath to PGA is already given in [ 18 ] , it is
sufficient to project PGA @xmath to PGA @xmath to show that the former
can be projected to PGA. Below is a proposal of a projection of the
SCLAnd ( @xmath ) connective from PGA @xmath to PGA @xmath , for @xmath
:

  -- -- -- -------
           (8.9)
  -- -- -- -------

To see why this projection works, consider the following example:
suppose we have the sequence @xmath with @xmath . This means that if
@xmath and @xmath are true, @xmath and @xmath will be executed.
Otherwise, only @xmath will be executed. In PGA @xmath this sequence
would be @xmath . The projection to PGA @xmath would then be @xmath
@xmath . If @xmath is false, the execution skips the unit and executes
the jump instruction, ending up executing @xmath . If @xmath is true,
the unit is entered, starting with the test @xmath . If @xmath is false,
the execution again arrives at the same jump as before, skipping @xmath
and executing @xmath . If @xmath is true, a different jump is executed
which makes the program jump to @xmath first and only then moves on to
@xmath , which is exactly the desired behaviour.

The entire projection is wrapped in a unit because, as we will see
later, the SCLAnd and other operators we define here also are to be
considered units. Therefore, a program sequence prior to (or after) the
operators discussed here cannot jump into the execution of that
operator. By wrapping the projection into a unit I ensure that cannot
happen after the projection either.

For the SCLOr connective, the projection is a little easier. It looks
like this, again for @xmath :

  -- -- -- --------
           (8.10)
  -- -- -- --------

To see why this projection works, consider the same example as above:
@xmath , but now with @xmath . So, if @xmath and / or @xmath are true,
@xmath and @xmath should be executed. If they are both false, only
@xmath should be executed. In PGA @xmath this looks like this: @xmath .
The projection to PGA @xmath then is @xmath . So, if @xmath is true,
execution skips testing @xmath and moves on directly to @xmath . If
@xmath is false, @xmath is tested first. If @xmath is also false,
execution skips @xmath and @xmath is executed. If @xmath is true, @xmath
gets executed first: exactly the desired behaviour.

So far, we have only been considering programs of the form @xmath , that
is, with a positive test. Of course, we also have the negative test
instruction. For a negative test, the projection of SCLAnd resembles
that of SCLOr. This comes as no surprise since SCLAnd and SCLOr are each
other’s dual. It looks like this, again for @xmath :

  -- -- -- --------
           (8.11)
  -- -- -- --------

The projection of @xmath for a negative test resembles the projection of
@xmath for a positive test:

  -- -- -- --------
           (8.12)
  -- -- -- --------

#### 8.2.3 Complex Steering Fragments

The implementations in the previous section work for steering fragments
containing a single logical connective (that is, with disjuncts or
conjuncts @xmath ). However, we also need to define what happens for
larger complex steering fragments (for instance @xmath )). In order to
accommodate this, we need one more property for the @xmath and @xmath
operators in PGA: they have to be treated as units. If we do this, we
can give a recursive definition for the projection, with as base cases
the ones given in the previous sections.

In what follows, the formulas @xmath and @xmath can take the following
form:

  -- -- -- --------
           (8.13)
  -- -- -- --------

As we can see, this includes negation. For more on negation, see the
next section. We get the following projections:

  -- -- -------- --
        @xmath   
        @xmath   
        @xmath   
        @xmath   
  -- -- -------- --

This works as follows. Consider the example @xmath , with @xmath . In
PGA @xmath this would be written as:

  -- -- -- --------
           (8.14)
  -- -- -- --------

We can use our new recursive definition of @xmath and get:

  -- -- -------- --
        @xmath   
                 
        @xmath   
  -- -- -------- --

The projections left now are base cases of @xmath and @xmath ,
respectively. Thus, we get

  -- -------- -------- --
              @xmath   
                       
              @xmath   
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

An interesting question is whether these projections make @xmath an
associative operator. To find out, we compare the above with the example
@xmath where this time @xmath . We get:

  -- -------- -------- --
                       
              @xmath   
              @xmath   
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We can use behavior extraction to check if these programs are behavioral
equivalent. It turns out that both programs indeed have the same
behavior:

  -- -------- --
     @xmath   
  -- -------- --

Thus, we can conclude that @xmath is associative in PGA @xmath , as we
would expect given SCL7. We can analyze @xmath in a similar manner.

#### 8.2.4 Negation

Now that we have the projections for positive and negative tests
defined, we can turn our attention to one more operator that is common
both in programming practice and in logic: negation. In PGA, negation is
absent, so we need to define it here. Not all instructions or sequences
of instructions can be negated: after all, there is no intuition for the
meaning of the negation of a certain behavior. We can, however, negate
basic instructions: by this we mean its Boolean reply changes value.
Sequences of instructions consisting of the operators I have defined
above can be negated as well, which I will write as @xmath . First, I
define the following standard projection rules:

  -- -------- -- --------
     @xmath      (8.15)
     @xmath      (8.16)
     @xmath      (8.17)
  -- -------- -- --------

Now that we have this, we need to take a look at how negation interacts
with the @xmath and @xmath connectives. In particular, we are interested
in what happens if one or both of the instructions in such a connective
are negated. For this, the De Morgan’s laws will come in handy:

  -- -- -- --------
           (8.18)
           (8.19)
  -- -- -- --------

With the above equations in combination with the equations 8.15 - 8.17 ,
we already have the projections for two possible cases (namely when no
instructions are negated and when both instructions are negated). That
leaves us two other cases for both @xmath and @xmath : one in which the
first instruction is negated, and one in which the other is. Below are
the projections of these cases:

  -- -- -- --------
           (8.20)
           (8.21)
           (8.22)
           (8.23)
  -- -- -- --------

For more on the @xmath and @xmath connectives and the rules that apply
to them, see the paper by Bergstra and Ponse on short-circuit logic [ 5
] as well as Chapter 5 .

#### 8.2.5 Other instructions

In the previous subsections we have seen what the projections of the new
logical connectives in PGA @xmath to PGA @xmath look like. To complete
the list of projections, we have to define the projections for the
‘regular’ instructions, as well as how concatenation and repetition are
projected. This is trivial, since these ‘regular’ instructions are the
same in PGA @xmath and PGA @xmath . We get for @xmath and PGA @xmath
-programs @xmath

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

### 8.3 Detecting side effects in PGA

In this section I will show how to detect side effects in a PGA @xmath
program using our treatment of side effects. In essence, all we have to
do is translate the PGA @xmath program to an equivalent DLA @xmath
-program, which can then be used to determine the side effects that
occur.

To recap, we have the following operators in PGA @xmath that have to be
translated:

-   Concatenation ( @xmath )

-   Repetition ( @xmath )

-   Unit instruction operator ( @xmath )

-   Termination ( @xmath )

-   Positive and negative tests ( @xmath )

-   Only in tests: conjunction, disjunction and negation ( @xmath ,
    @xmath )

There are two notable differences between PGA @xmath and DLA @xmath .
The first is that in PGA @xmath a program unsuccessfully terminates
unless explicitly instructed otherwise by the termination instruction,
whereas in DLA @xmath the default is a successful termination. This is
an issue that has to be addressed to properly translate PGA @xmath to
DLA @xmath and the best way to do this, is to add the termination
instruction to DLA @xmath . This illustrates the point I made in Section
6.5 in Chapter 6 : the instructions I defined so far in DLA @xmath are
by no means exhaustive and new instructions may have to be added to
them. This can usually be done by simply defining the actual and
expected evaluation of the new instruction.

The nature of the termination instruction requires us to do a little
more than just that. After all, the termination instruction has a
control element to it: just like for instance the test instruction it
has an influence on which instructions are to be evaluated next. To be
exact, no instructions are to be evaluated next when a termination
instruction is encountered during evaluation of a program. Because of
this, we have to slightly modify the concatenation operator in DLA
@xmath too when we introduce the termination instruction. We baptize the
extension of DLA @xmath with the termination instruction DLTA @xmath
(for Dynamic Logic with Termination and Assignment in Formulas).

The equation for the relational meaning of @xmath in a given model
@xmath and initial valuation @xmath is straight-forward. Execution
simply finishes with the same resulting valuation as the initial
valuation:

  -- -------- -- ----------
     @xmath      (DLTA15)
  -- -------- -- ----------

The updated rule for concatenation has to express that when a
termination instruction is encountered, nothing should be evaluated
afterwards. We use a case distinction for this on the first instruction
of a concatenation:

  -- -------- -------- -- ----------
     @xmath   @xmath      (DLTA12)
  -- -------- -------- -- ----------

We only define the termination instruction in the setting of
deterministic programs here. This is sufficient because this is the only
setting we are currently interested in. DLTA12 replaces QDL12, but keeps
the associative character of concatenation intact:

  -- -------- --
     @xmath   
  -- -------- --

The addition of the termination instruction allows us to easily express
PGA @xmath -programs such as @xmath in DLTA @xmath . They would
otherwise have caused a problem because there would have been no easy
way to stop the evaluation of the program from continuing to evaluating
@xmath , which it of course is not supposed to do if @xmath yields true.

The other notable difference between PGA @xmath and DLA @xmath is that
in the former, anything can be used as a basic instruction. That
includes what we refer to in DLA @xmath as primitive formulas such as
@xmath or @xmath . In PGA the execution of an instruction always
succeeds, even if the Boolean reply that it generates, is false. To
model this in DLTA @xmath , we have to add the primitive formulas @xmath
to the set of instructions, as follows:

  -- -------- --
     @xmath   
  -- -------- --

The relational meaning in @xmath given initial valuation @xmath for
these new instructions is simply that they always succeed without
modifying @xmath :

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

With the termination instruction and the formulas-as-instructions
defined, we can take a first look at the mapping from PGA @xmath to DLTA
@xmath . For this we define a translation function @xmath PGA @xmath
@xmath DLTA @xmath . We define this translation function for PGA
programs in first or second canonical form only; this is sufficient
because as we have seen, every PGA program can be rewritten to first and
second canonical form.

First, we define the set @xmath of basic instructions in PGA to be equal
to the set of primitive formulas and single instructions, not being
tests, in DLA @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the set of single instructions not being tests. In
DLA @xmath , this set only consists of the assignment instruction @xmath
.

For finite sequences of instructions with length @xmath , @xmath and
@xmath , and @xmath a formula as meant in section 8.2.3 , @xmath is
defined as follows:

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

Here we can clearly see what effect it has that PGA @xmath has
unsuccessful termination as its default. We have to explicitly introduce
unsuccessful termination in DLTA @xmath by adding @xmath (a test that
always fails) at the end of every instruction. Furthermore, notice the
unit instruction operator that here has length @xmath , but is
transparent when it has to be translated and thus becomes a sequence of
instructions with length @xmath that is potentially larger than @xmath .
Finally, notice that there is no need to translate possibly compound
formulas @xmath . This is because formulas have the exact same syntax in
PGA @xmath and DLTA @xmath .

Next, we can show the definition of @xmath for finite sequences of
instructions with length @xmath . For @xmath , @xmath and @xmath a
formula as meant in section 8.2.3 , we have

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

With the above translation rules, we can now translate finite PGA @xmath
-programs to their DLTA @xmath -versions. A complete translation would
require a translation of repetition as well. This, however, is quite a
complex task. The reason for that becomes clear when considering
examples like these:

  -- -------- --
     @xmath   
     @xmath   
     @xmath   
  -- -------- --

Because of the behavior of @xmath , we get into trouble here if we
attempt to use the regular translation. The problem is that @xmath can
possibly skip the first instruction of the next repetition loop, which
is behavior that is hard to translate without explicitly introducing
this variant of repetition ( @xmath ) in DLA @xmath . The same problem
arises with the jump instruction. At first glance, the best solution
there is to introduce the jump instruction to DLA @xmath as well. In
that case the second canonical form of PGA-programs comes in handy, as
it is designed to manipulate expressions with repetition such that no
infinite jumps occur.

Since this case study is meant as a relatively clear example of how to
use DLA @xmath to model side effects in other systems such as PGA, it is
beyond our interest here to present these rather complex translations of
repetition. Instead, we restrict ourselves to finite PGA @xmath
-programs and leave the relational semantics for DLA @xmath which models
side effects, as the basis for future work on PGA involving repetition.

### 8.4 A working example

In this section I will present a working example of the translation from
finite PGA @xmath -programs, which we write as PGA @xmath , to DLTA
@xmath . In addition, I will show that we get sufficiently similar
results if we first translate PGA @xmath to DLTA @xmath compared to
first projecting PGA @xmath to PGA @xmath and then translating that to
DLTA @xmath . To be exact, we are going to show that the following
diagram defines a program transformation @xmath on finite deterministic
programs in DLTA @xmath :

  -- -- --
        
  -- -- --

Here @xmath is a reduction function on DLTA @xmath that yields
deterministic DLTA @xmath -programs where occurrences of @xmath and
@xmath have been eliminated.

For the working example, we return to a variant of our running example.
Consider the PGA @xmath -program

  -- -- --
        
  -- -- --

where @xmath suggests a write command. This is a program of the form

  -- -- --
        
  -- -- --

with @xmath and @xmath . Thus, we get the following translation, where
we for clarity have underlined the instruction that we are going to
translate next:

  -- -- --
        
        
        
        
        
        
        
  -- -- --

So there we have it: if we replace the shorthands with their original
instructions or formulas again, we get the following DLTA @xmath
-program, which we baptize @xmath :

  -- -------- -------- --
     @xmath            
              @xmath   
                       
  -- -------- -------- --

Clearly, given model @xmath , @xmath implies that @xmath . So, if @xmath
, the instruction @xmath is executed, after which the program
terminates, while for @xmath , the instruction @xmath is executed after
which the program terminates.

Now let @xmath , so

  -- -------- --
     @xmath   
  -- -------- --

We compute

  -- -------- -------- --
     @xmath   @xmath   
                       
              @xmath   
              @xmath   
  -- -------- -------- --

Note that for each model @xmath and initial valuation @xmath , @xmath ,
so

  -- -------- --
     @xmath   
  -- -------- --

Thus, writing @xmath for the rightmost deterministic DLTA @xmath
-program, we find

  -- -------- --
     @xmath   
  -- -------- --

We now need to ask ourselves if @xmath is ‘sufficiently similar’ to the
earlier derived @xmath . Intuitively, we would say that in this working
example, this indeed is the case. After all, @xmath always yields true,
so the truth of @xmath depends solely on the Boolean reply that @xmath
yields. It therefore does not matter if we lift @xmath out of the union,
which is essentially what we have done in the case of @xmath .

We can call two programs ‘sufficiently similar’ if they evaluate the
same single instructions, not being tests, or primitive formulas in the
same order. We can formalize that notion with the following proposition:

###### Proposition 11.

Let @xmath be a program in PGA @xmath , let @xmath and let @xmath . Let
model @xmath be given and let @xmath be an initial valuation such that
there exists a valuation @xmath such that @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

and the same single instructions, not being tests, and primitive
formulas are evaluated in the same order during evaluation of @xmath and
@xmath given @xmath .

As said, we do not consider repetition as program constructor in our
case study. Furthermore, our model of side effects is limited to
terminating programs, as opposed to programs that can either end in
termination or in divergence. A proof of this proposition might be
found, but is for these reasons perhaps not very much to the point. In
Chapter 9 (Conclusions) we return to this issue.

It is, however, worthwhile to check the proposition for our working
example. Recall that we have the following @xmath and @xmath :

  -- -------- -------- --
     @xmath            
              @xmath   
                       
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

It is not hard to check in this case that for any model @xmath and
initial valuation @xmath such that @xmath can be evaluated, @xmath . It
is also easy to see that the same single instructions, not being tests,
and primitive formulas are evaluated (in the same order). After all,
@xmath , first evaluates the primitive formulas @xmath and @xmath and
uses those to determine the reply of @xmath . Depending on the reply, it
then either evaluates the single instructions @xmath and , or @xmath and
.

Almost the same goes for @xmath . It first evaluates the primitive
formula @xmath and depending on the reply (which happens to be always
true), either stops evaluation (which therefore is never the case) or
continues with the evaluation of primitive formula @xmath . Depending on
the reply, it like @xmath then either evaluates the single instructions
@xmath and , or @xmath and . So at least in our working example,
Proposition 11 holds.

In a similar way, we can analyze the PGA @xmath -program

  -- -- --
        
  -- -- --

We can compute @xmath :

  -- -------- -- --
     @xmath      
                 
                 
                 
                 
  -- -------- -- --

We once again define @xmath , so

  -- -------- --
     @xmath   
  -- -------- --

We compute

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

We can directly eliminate a situation: @xmath is false for any initial
valuation @xmath . Thus, writing @xmath for the second part of the
topmost union:

  -- -------- --
     @xmath   
  -- -------- --

we get given model @xmath for any initial valuation @xmath

  -- -------- --
     @xmath   
  -- -------- --

We can check in similar fashion as before that Proposition 11 holds (for
any initial valuation @xmath ). We can conclude that at least for these
working examples, the mentioned proposition is valid. As said, we leave
the proof for future work.

This case study started from the abstract approach to attempt
decomposition of complex steering fragments in instruction sequences in
PGA @xmath as advocated in [ 5 ] . We show that we can apply this
approach to a rather concrete instance in imperative programming (namely
the set @xmath of basic instructions given in this chapter) and we
obtain some interesting results. In the first place, it inspired our
definition of DLTA @xmath and the analysis and classification of side
effects as discussed in this thesis. Secondly, by the preservation
property formulated in Proposition 11 , it justifies our proposal for
the projection function . It is an interesting result that we are able
to show that the projection , which does not have to anything to do with
valuations, preserves the relational semantics (and therefore side
effects) of a program via the diagram at the beginning of this section,
which is based on a very natural translation.

## Chapter 9 Conclusions and future work

In this thesis I have given a formal definition of side effects. I have
done so by modifying a system for modelling program instructions and
program states, Quantified Dynamic Logic, to a system called DLA @xmath
(Dynamic Logic with Assignments as Formulas), which in contrast to QDL
allows assignments in formulas and makes use of short-circuit
evaluation. I have shown the underlying logic in those formulas to be a
variant of short-circuit logic called repetition-proof short-circuit
logic.

Using DLA @xmath I have defined the actual and the expected evaluation
of a single instruction. The side effects are then defined to be the
difference between the two. I have given rules for composing those side
effects in single instructions, thus scaling up our definition of side
effects to a definition of side effects in deterministic DLA @xmath
-programs. Using this definition I have given a classification of side
effects, introducing as most important class that of marginal side
effects. Finally, I have shown how to use our system for calculating the
side effects in a real system such as PGA.

Our definition gives us an intuitive way to calculate the side effects
in a program. Because of the definition in terms of actual and expected
evaluation, one can easily adapt the system to ones own needs without
having to change the definition of side effects. All one has to do is
update the expected evaluation of a single instruction, or if an
entirely new single instruction is added to the system, define the
actual and expected evaluation for it.

In Chapter 5 we have seen how a sound axiomatization of the formulas in
DLA @xmath can be given using the signature @xmath . I have not used
this signature in the first place because I wanted to stick to the
conventions in dynamic logic. It is noteworthy, however, that this
alternative and possibly more elegant signature exists, especially
because an axiomatization can be given for it.

The definition of side effects given here can point the way to a lot
more research. I can see future work being done in the following areas:

-   I do not want to claim that the instructions I have defined in DLA
    @xmath are exhaustive. Finding out what possible other instructions
    might have to be added to DLA @xmath can be an interesting project.

-   Another possible subject for future work is the issue of ‘negative’
    side effects I briefly touched upon in Section 6.5 . It is an open
    question whether or not we should allow situations in which
    ‘negative’ side effects occur and if so, how we should handle them.

-   In this thesis, we have mostly been looking at imperative programs.
    It should be interesting to see if our definition can be extended
    to, for example, functional programs. Perhaps the work done by Van
    Eijck in [ 10 ] , in which he defines functional programs making use
    of program states, can be used for this.

-   Another interesting question, which has been raised before in
    Chapters 2 and 6 , is that of side effects in non-deterministic
    programs. It warrants further research if it is reasonable to talk
    about side effects there. One can imagine that if the set of side
    effects in all possibilities of a non-deterministic program are the
    same, the side effects of the whole can be defined as exactly that
    set. What needs to be done if that’s not the case however, or if we
    should even want to define side effects of such programs, are open
    questions.

-   In Chapter 7 , the concept of marginal side effects was introduced
    and the suggestion was made that this notion can be linked to claims
    about how well-written a program is. I have not pursued such claims,
    but can imagine further research being done in that area.

-   To develop a direct modelling of side effects for the variant of PGA
    discussed in Chapter 8 , one can introduce valuation functions as
    program states and define a relational meaning that separates
    termination from deadlock/inaction, say

      -- -------- --
         @xmath   
      -- -------- --

    The idea of this would be to evaluate @xmath as far as possible,
    which is a reasonable requirement if @xmath is in second canonical
    form. In addition, we could define a termination predicate, e.g.
    Term @xmath , which states that @xmath terminates for initial
    valuation @xmath . Using this we could define a “behavioral
    equivalence” on programs @xmath and @xmath as follows:

      -- -------- --
         @xmath   
      -- -------- --

    Using this, Proposition 11 can probably be proven, especially
    considering the in Chapter 4 proven property of DLA @xmath that any
    program can be rewritten into a form in which its steering fragments
    only contain primitive formulas and their negations.

-   Also mentioned in Chapter 8 is the possibility to introduce extra
    logical operators, namely Logical Left And (LLAnd) and its dual
    Logical Left Or (LLOr). Introducing these in DLA @xmath is fairly
    straight-forward: one only needs to define its truth in @xmath :

      -- -------- -- -- ---------
         @xmath         (DLA7c)
         @xmath         (DLA7d)
      -- -------- -- -- ---------

    as well as update the program extraction function:

      -- -------- --
         @xmath   
      -- -------- --

    To introduce the same operator in PGA @xmath , projection functions
    in the same style as the ones given in Chapter 8 for SCLAnd and
    SCLOr need to be defined.

-   Another possible matter for further study is whether side effects
    can be used in natural language. In the Introduction, we have
    already seen that they can occur in the pregnant wife example, where
    your wife told you to do the grocery shopping if she did not call
    you, which she later did, but to tell you that she was pregnant.
    Possibly there is a role for side effects when explaining
    misunderstandings. There is no doubt that side effects can be the
    cause of misunderstandings. The pregnant wife example illustrates
    that: you could decide to do grocery shopping to be on the safe side
    after her call, claiming her call indicated you might have to shop,
    only to run into your wife at the store also shopping (who, of
    course, didn’t want to convey the message that you should shop at
    all).

    When we take the Dynamic Epistemic Logic system mentioned in [ 12 ]
    , the knowledge of two communicating agents is captured by an
    epistemic state, one for each agent. The agents also have an
    epistemic state for what they think is the (relevant) knowledge of
    the other agent with whom they are in conversation. A
    misunderstanding has occurred when an agent updates his own
    epistemic state in a different way than the other agents expects him
    to. There are a lot of ways in which this can happen, but relevant
    for us is that one of those ways is, when a side effect from an
    utterance occurs of which one of the agents is not aware.

    If one of the agents is aware of the side effect and also of the
    fact the other agent might not be aware of it, it may be recommended
    to point out this side effect to the other agent. In our example of
    the pregnant wife calling, this would mean that you would have to
    ask your wife on the phone that the fact she called leaves you in
    doubt about the grocery shopping. Naturally, though, we recommend a
    more enthusiastic response to the news she is pregnant first.