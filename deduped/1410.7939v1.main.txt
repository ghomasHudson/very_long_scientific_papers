##### Contents

-    1 The Higgs-boson sector of the MSSM at tree-level
-    2 The scalar quark sector and SQCD at tree-level
-    3 Higher-order Higgs-boson mass corrections within the real MSSM
-    4 The two loop frontier
-    5 Two and more loop integrals with multiple scales
-    6 Introduction of Feynman parametrization
-    7 The virtues of a Mellin-Barnes representation
-    8 The method of differential equations
-    9 Further analytic developments
-    10 Motivation for adopting a numerical approach
-    11 Conceptual idea
    -    11.1 Generation of primary sectors
    -    11.2 Iterated sector decomposition
    -    11.3 Extraction of the poles
    -    11.4 Calculation of the pole coefficients
-    12 The choice of algorithm
    -    12.1 Goals
    -    12.2 A heuristic algorithm for slim results
    -    12.3 Algorithms guaranteed to stop
-    13 Euclidean vs. physical kinematics
-    14 Landau equations
-    15 Deformation of the integration contour
    -    15.1 Cauchy theorem
    -    15.2 Deformation
        -    15.2.1 Deformation studies
        -    15.2.2 Pinch singularities
-    16 Functionality
    -    16.0.1 Loop integrals and integrals of similar structure
    -    16.0.2 General parametric integrals
-    17 Characteristic features
    -    17.1 Loop integrals
    -    17.2 Parametric integrals
    -    17.3 Implementation of Contour deformation
-    18 Additional capabilities
    -    18.1 Evaluation of user-defined functions with arbitrary
        kinematics
    -    18.2 Topology-based construction of the integrand
    -    18.3 Looping over ranges of parameters
    -    18.4 Integration-by-parts relations
    -    18.5 Leaving functions implicit during the algebraic part
    -    18.6 Assessing the reliability of the numerical result
    -    18.7 Automated remapping to one endpoint
    -    18.8 A word on the numerical integration
-    19 Operational sequence
-    20 Selection of checks and examples
    -    20.1 A two-loop three-point function
        -    20.1.1 Producing data files for sets of numerical values
    -    20.2 Massive tensor two-loop two-point functions
-    21 Future developments
-    22 Analytical preparation of the non-planar seven propagator
    integral @xmath
    -    22.1 Integration in a sub-loop
    -    22.2 Backwards transformation
        -    22.2.1 Preconditions
        -    22.2.2 Application to the @xmath
-    23 Expected thresholds from the Landau equations
-    24 Numerical evaluation
    -    24.1 The @xmath diagram
    -    24.2 The @xmath diagram
    -    24.3 Planar seven-propagator all massive graph @xmath
    -    24.4 Non-planar seven-propagator all massive graph @xmath
    -    24.5 Non-planar six-propagator diagram
-    25 Summary
-    26 Dominant momentum-dependent two-loop QCD corrections
-    27 Self-energy diagrams
-    28 Renormalization
    -    28.1 Two-loop counter terms for the renormalization at @xmath
    -    28.2 Renormalization at the sub-loop level
-    29 Treatment of the integrals
    -    29.1 Analytically known integrals
    -    29.2 Analytically unknown integrals
        -    29.2.1 Numerical computation of two-point two-loop
            integrals
-    30 Evaluation of the additional shifts to the Higgs-boson masses
    -    30.1 Phenomenological motivation for two different scenarios
    -    30.2 Renormalized @xmath self-energies
        -    30.2.1 Scenario 1: @xmath scenario
        -    30.2.2 Scenario 2: Light stop scenario
    -    30.3 Neutral @xmath -even Higgs-boson mass corrections
        -    30.3.1 Implementation in the program FeynHiggs
        -    30.3.2 Scenario 1: @xmath scenario
        -    30.3.3 Scenario 2: Light stop scenario
-    31 Summary and Perspectives
-    A Analytical formulae
    -    A.1 One-loop representations
        -    A.1.1 One-loop tadpole
        -    A.1.2 One-loop bubble
        -    A.1.3 Derivative of the one-loop bubble
    -    A.2 Two-loop representations
        -    A.2.1 Two-loop vacuum diagram
        -    A.2.2 Two-loop two-point three-propagator (sunrise) diagram
        -    A.2.3 Two-loop two-point four-propagator diagram
        -    A.2.4 Two-loop two-point five-propagator diagram
-    B SecDec User Manual
    -    B.1 Installation
        -    B.1.1 Loop setup
        -    B.1.2 General setup
        -    B.1.3 User-defined setup
    -    B.2 Operation
    -    B.3 Program input parameters
    -    B.4 Input for the definition of the integrand
    -    B.5 Topology based construction of an integrand
    -    B.6 Utilization of the user-defined setup
    -    B.7 Looping over ranges of parameters
    -    B.8 Leaving functions implicit during the algebraic part

## Chapter \thechapter Introduction

Hadron colliders have set the stage to a whole new era of discovery.
With the increasing wealth of high energy collision data, physics up to
the TeV scale is being explored. Within the theoretical framework of the
Standard Model of particle physics (SM) [ 8 , 9 , 10 , 11 , 12 , 13 , 14
] most of the observations made by past and present collider experiments
can successfully be described. Its predictive power has lead to the
discovery of almost all of its constituents. These are three families of
quarks and leptons, four gauge bosons mediating the electroweak and
strong interaction, and the simplest manifestation of the
Brout-Englert-Higgs mechanism [ 15 , 16 , 17 , 18 ] - the Higgs-boson.
Although the discovery of the latter is still not fully confirmed, a
particle behaving like the Standard Model Higgs-boson has recently been
observed [ 19 , 20 ] in the ATLAS and CMS experiments at the Large
Hadron Collider (LHC). The characteristics of this new boson with a mass
around @xmath GeV have been determined already rather accurately [ 21 ,
22 , 23 , 24 ] . If deviations with respect to the SM characteristics
are found with the collation of more data, this particle must be
interpreted within a different model. There are already several other
reasons to search for an embedding of the Standard Model as an effective
theory into a more general theoretical framework. Apart from the fact
that gravity is not incorporated, the indirect observation of dark
matter [ 25 , 26 , 27 ] does not find a description in the Standard
Model either. Furthermore, the predicted violation of the @xmath
symmetry is not large enough as to explain the observed excess of matter
over antimatter in the universe. More peculiarities are related to the
newly found boson. If it indeed is the SM Higgs-boson, it is discussed [
28 , 29 , 30 , 31 ] that the electroweak vacuum of the Standard Model
may not be absolutely stable and its low mass can only be accommodated
for by assuming an unnatural amount of fine-tuning [ 32 ] . Ideas for
models beyond the Standard Model in which the newly found boson is
realized range from interpreting it as a dilaton [ 33 , 34 ] or in the
framework of a composite Higgs model [ 35 , 36 ] . A different proposal
for a new framework is formulated as a supersymmetric extension to the
Standard Model, in particular the Minimal Supersymmetric Standard Model
(MSSM) [ 37 , 38 , 39 ] . It has been broadly discussed over the last
few decades.

The motivation for supersymmetry (SUSY) [ 40 , 41 , 42 , 43 , 44 , 45 ,
46 ] is twofold. On the one hand, it can provide for a solution to the
fine-tuning and the hierarchy problem, achieve a gauge coupling
unification and moreover accommodate for a dark matter candidate. On the
other hand, it allows for the embedding of present observations into a
more generalized mathematical framework. Supersymmetry arises as the
only possible extension to the Poincaré algebra [ 47 ] , evading the
no-go-theorem found by Coleman and Mandula [ 48 ] . In supersymmetric
theories, all known fermionic particles of the Standard Model are
assigned a scalar superpartner and all bosonic SM particles a fermionic
one. The Standard Model contains one scalar doublet. In renormalizable
supersymmetric models, the necessity for an even number of scalar
doublets arises. At least two Higgs doublets are required, to give mass
to respectively both, up-type and down-type particles and scalar
particles (sparticles) [ 49 , 50 , 51 , 52 , 53 , 54 , 55 ] . The MSSM
contains two scalar doublets which conserve hypercharge gauge
invariance. Due to this invariance, all up-type particles and sparticles
couple exclusively to one scalar doublet, while all down-type
(s)particles couple to the other doublet. This evades constraints from
flavor-changing neutral currents (FCNCs), as was pointed out by Glashow
and Weinberg [ 56 ] . The two scalar doublets of the MSSM give rise to
five physical Higgs-bosons. In lowest order, these are the light and
heavy @xmath -even, the @xmath -odd, and the charged Higgs-bosons. While
the mass of the Higgs-boson remains a free input parameter in the
Standard Model, it is predicted within the MSSM. Associating the newly
observed boson with the lightest @xmath -even Higgs-boson @xmath , the
upper bound on its predicted mass @xmath at leading order (LO) is given
by the @xmath gauge boson mass. This would already have lead to the
exclusion of the MSSM at past collider experiments. Yet, higher-order
quantum corrections to the MSSM Higgs-boson masses lead to a shift in
the upper limit towards @xmath GeV.

Higher-order corrections are not only decisive in the precise prediction
of physics beyond the Standard Model, but are of proven importance in
the understanding of SM processes at colliders. The state of the art of
higher-order corrections to Standard Model processes and a future wish
list is summarized in the proceedings of the 2013 Les Houches workshop [
57 ] . The more accurate predictions are desired, the more involved the
calculations become. Leading order theoretical predictions can most
commonly not meet the current experimental precision. The calculation of
perturbative corrections at next-to-leading order (NLO) in the strong or
electroweak coupling constant has reached an impressive level of
automation meanwhile. Corrections beyond NLO accuracy still require
quite some effort, both on the conceptual and on the technical side
before they can be performed in a largely automated way. There are a few
processes measured at the LHC where the need for next-to-next-to leading
order (NNLO) QCD predictions arises. One of them is top-quark pair
production. Top-quark pair production is vital for the precise
measurements of the top-quark properties but also enters into other
measurements, e.g., of parton distributions. At the LHC top quarks are
produced so numerously that they also constitute a significant
background to new physics signals. It is therefore crucial to understand
this background properly to be able to discriminate the signal. A full
NNLO prediction for the total cross section of top-quark pair production
is known in a semi-numerical form [ 58 ] along with many partial results
in semi-numerical and analytical form [ 59 , 60 , 61 , 62 , 63 , 64 , 65
, 66 , 67 , 68 , 69 , 70 , 71 ] . Soft gluon and Coulomb effects also
have been taken into account beyond the next-to-leading logarithmic
accuracy and have been combined with fixed order results to come up with
predictions as precise as possible [ 72 , 73 , 74 , 75 , 76 , 77 , 78 ,
79 ] . Among the key ingredients of the full NNLO calculation are
complicated two-loop integrals entering the virtual corrections.
Analytical expressions for these are known for diagrams dependent on
relatively few mass scales [ 62 , 61 , 60 , 80 , 69 , 81 ] . As soon as
several mass scales are involved, numerical methods to calculate
multi-loop integrals become increasingly important.

The brief outline of this thesis is as follows: In Chapters id1 - id1 ,
the basic concepts of the author’s work presented in this thesis are
established. In Chapter id1 , the developed version 2 of the program
SecDec is described, laying the foundation for two applications
presented in Chapters id1 and id1 .

More comprehensively, Chapter id1 covers an introduction of the
tree-level Higgs-boson sector of the MSSM. The scalar quark (squark)
sector is discussed as well, focussing on strong and Yukawa-type
interactions. Afterwards, a motivation for higher-order corrections to
the Higgs-boson masses is discussed, along with an introduction to their
computation. The latter involves the evaluation of two-loop integrals
with multiple scales, leading to mass thresholds. Different methods to
approach multi-loop multi-scale integrals are reviewed in Chapter id1 ,
before motivating the pursuit of a universal numerical approach using
Feynman parameterization. The method of sector decomposition is used for
the disentanglement of overlapping ultraviolet (UV), collinear and
infrared (IR) singularities, as discussed in Chapter id1 . Various
algorithms performing differently with respect to this task are also
reviewed.

In Chapter id1 , the appearance of thresholds is discussed. To compute
integrable thresholds, the integrand needs to be analytically continued
to the complex plane. Towards this aim, a deformation of the integration
contour, applicable in numerical calculations, is explained. Finally,
studies by the author are presented which tune the analytical
continuation further towards a stable evaluation of integrals containing
thresholds.

In Chapter id1 , the features incorporated in an upgrade of the
open-source program SecDec are presented. Based on the concepts
introduced in Chapters id1 - id1 , SecDec allows the automated numerical
computation of multi-loop multi-scale integrals, in addition to an
evaluation of more general parametric integrals. Restricted to
non-physical kinematics in version 1, the extension to physical
kinematics including thresholds is achieved in version 2 of the program.
The upgraded features are presented along with diverse other
improvements.

In Chapter id1 , the full power of the program SecDec is shown in an
application to massive non-planar two-loop four-point functions, among
them various ones where analytical results are unknown. Several of the
topologies shown are computed in a fully automated way. For one topology
which is of particular interest in the top-quark pair production at
NNLO, analytical transformations beforehand are shown, improving on the
numerical stability. In particular, the integration of one Feynman
parameter of a sub-loop is found to be beneficial. Furthermore, a
transformation first introduced by the author and collaborators, proves
to allow for a simplification of the singularity structure, leading to a
reduction in the number of sub-functions to be integrated. The
transformation is presented in detail.

In Chapter id1 , the calculation of the dominant neutral @xmath -even
MSSM Higgs-boson mass corrections at the two-loop order including
momentum dependence is presented. This requires the calculation of
two-loop self-energies with a proper renormalization at the two-loop
level, using an overall mixed on-shell and @xmath scheme for the
renormalization. The program SecDec is used in the evaluation of
analytically unaccessible integrals. The mass shifts resulting from the
additional momentum-dependent contributions are presented.

The conclusions are given in Chapter id1 .

## Chapter \thechapter Higgs-bosons in the MSSM

In the following, the Higgs-boson, quark and scalar quark (squark)
sector of the MSSM are introduced. The tree-level mass matrices are
derived from the MSSM Lagrangian. All interactions appearing in the
two-loop corrections to the MSSM Higgs-boson masses discussed in Chap.
id1 are shown as well. This includes supersymmetric QCD (SCQD)
interactions. The following two sections, Sec. 1 and 2 , are based on
Refs. [ 82 , 83 , 84 , 85 , 38 , 86 ] . Afterwards, the current status
of higher-order Higgs-boson mass corrections in the MSSM is reviewed in
Sec. 3 .

### 1 The Higgs-boson sector of the MSSM at tree-level

The Higgs-boson sector of the MSSM with real parameters (rMSSM) is part
of the full MSSM Lagrangian and consists of the following four
components

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where @xmath contains the free-field kinetic terms, @xmath is derived
from the Higgs-boson potential, and @xmath is the gauge-fixing term.
With the introduction of a gauge-fixing term, unphysical degrees of
freedom arise which are compensated by Faddeev-Popov ghost terms [ 87 ]
in @xmath . The interaction part of the Higgs-boson sector Lagrangian
can be summarized as

  -- -------- -------- -- -----
     @xmath   @xmath      
                          
              @xmath      
              @xmath      (2)
  -- -------- -------- -- -----

All physical neutral and charged Higgs-boson fields are referred to with
the index H, the index V is short for vector boson fields, @xmath and
@xmath denote the Standard Model quarks and leptons, @xmath denotes all
squarks and scalar leptons (sleptons) and @xmath symbolizes the
neutralinos and charginos. Adopting the Feynman-’t Hooft gauge, all
ghost contributions vanish.

The MSSM requires two doublets @xmath and @xmath of complex scalar
fields, which are conventionally written in terms of their components as
follows,

  -- -------- -------- -- -----
     @xmath   @xmath      (3)
     @xmath   @xmath      (4)
  -- -------- -------- -- -----

with an associated hypercharge @xmath and @xmath , respectively. Their
vacuum expectation values are given by @xmath and @xmath , respectively.
The fields @xmath and @xmath are still unphysical, but are brought into
the physical basis

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

via orthogonal transformations of the type

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

giving rise to the particle spectrum of physical Higgs- and unphysical
Goldstone-bosons, compare Tab. 1 .

The kinetic part of the MSSM Higgs-boson sector Lagrangian reads

  -- -------- -------- -- -----
     @xmath   @xmath      (7)
  -- -------- -------- -- -----

Note the index @xmath instead of @xmath in Eq. ( 7 ). It is introduced
to distinguish between the @xmath - @xmath and the @xmath - @xmath
basis.

The potential part of the rMSSM Higgs-boson sector Lagrangian @xmath can
be written in terms of the supersymmetric F- and D-term contributions

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

and

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

where @xmath from now on. In contrast to the Standard Model, the
Higgs-boson self-couplings in the MSSM, resulting from Eq. ( 9 ), are
determined through the gauge coupling constants. The dagger in Eqs. ( 8
) and ( 9 ) indicates Hermitian adjoints, @xmath is the higgsino
(fermionic superpartner of the Higgs-boson) mass parameter, @xmath is
the SU(2) @xmath and @xmath the U(1) @xmath coupling constant. The
coupling constants @xmath and @xmath are related to the electric charge
@xmath and the electro-weak mixing angle @xmath of the Standard Model by

  -- -- -- ------
           (10)
  -- -- -- ------

Due to the non-observation of supersymmetric partners to the Standard
Model particles, supersymmetry must be broken. Various supersymmetry
breaking mechanisms can be considered [ 49 , 50 , 51 , 88 , 52 , 53 , 54
, 55 ] . In the MSSM, explicit breaking terms [ 51 , 88 ] parameterize
the effect of SUSY breaking. In order to accommodate for a solution to
the hierarchy problem, these terms may not introduce additional
quadratic divergences. They must have mass dimension less than four.
These so called soft supersymmetry breaking terms are added to the MSSM
Higgs-boson potential

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

where @xmath from now on, and with @xmath ¹ ¹ 1 Note the convention:
@xmath , with @xmath and @xmath . being totally antisymmetric, resulting
in an overall MSSM Higgs-boson potential of

  -- -------- -- -- ------
     @xmath         (12)
  -- -------- -- -- ------

Relating the MSSM Higgs-boson potential to the MSSM Lagrangian by

  -- -------- -------- -- ------
     @xmath   @xmath      (13)
  -- -------- -------- -- ------

where the F-term part of the potential is integrated over the auxiliary
superspace component @xmath , while the D-term part of the potential is
integrated over both superspace components @xmath and @xmath . In the
following, the notation

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

is adopted.

Until now, the MSSM Higgs-boson potential, Eq. ( 12 ), contains four
free parameters: @xmath , @xmath , @xmath and @xmath . Exploiting the
fact that the two vacuum expectation values @xmath and @xmath need to
minimize the potential and be nonzero at the same time, the following
necessary minimization conditions

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

are required to hold. The minimization conditions originate from the
equations of motion, compare Eq. ( 56 ). In Eq. ( 15 ), the linear part
of the rMSSM potential reads

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (16)
  -- -------- -------- -- ------

where @xmath . It should be noted that there are no contributions from
the fields @xmath and @xmath to the linear part of the potential. This
is due to the fact that the MSSM potential is @xmath -conserving,
meaning that it is invariant under the consecutive application of a
charge conjugation @xmath and a parity transformation @xmath . Without
imposing the minimization condition just yet, and writing the
coefficients to the fields @xmath in Eq. ( 16 ) as tadpole parameters
@xmath instead, the parameters @xmath and @xmath can be expressed in
terms of experimentally accessible quantities ( @xmath , @xmath , @xmath
, @xmath ) and the parameter @xmath

  -- -------- -------- -- ------
     @xmath   @xmath      (17)
     @xmath   @xmath      (18)
  -- -------- -------- -- ------

Consequently, after making use of Eq. ( 15 ), the tadpole parameters
@xmath vanish

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

Turning to the part of the MSSM Higgs-boson potential which is bilinear
in the fields, the mass matrices @xmath , @xmath and @xmath of the
scalar fields of the neutral @xmath -even, the neutral @xmath -odd and,
respectively, the charged Higgs- and Goldstone-bosons can be identified

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

where @xmath and @xmath . The tree-level mass matrix of the neutral
@xmath -odd bosons reads

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (21a)
              @xmath      (21b)
  -- -------- -------- -- -------

where in the last step, the relations of Eqs. ( 17 - 19 ) were used.
Afterwards, the mass matrix can be made diagonal

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (22a)
              @xmath      (22b)
  -- -------- -------- -- -------

resulting with the tree-level relation for the physical neutral @xmath
-odd Higgs-boson mass @xmath . Note that the tree-level @xmath -boson
mass @xmath does not attain any dependence on the Standard Model
vector-boson masses @xmath or @xmath . Defining the vacuum expectation
values @xmath and @xmath as

  -- -- -- ------
           (23)
  -- -- -- ------

the following relation results

  -- -------- -- ------
     @xmath      (24)
  -- -------- -- ------

The lower and upper bound on the angle @xmath result from the assumption
that @xmath and @xmath are real and positive, compare Ref. [ 82 ] .
Hereby, all free mass parameters are expressed in terms of physical
observables and the MSSM Higgs-boson potential is fixed by the
parameters @xmath , @xmath , @xmath , @xmath and @xmath . With this
knowledge in mind, the masses of the neutral @xmath -even Higgs-bosons
can be derived from the mass matrix

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (25a)
              @xmath      (25b)
  -- -------- -------- -- -------

being written in terms of the three parameters @xmath , @xmath and the
angle @xmath . After bringing the mass matrix into diagonal form, the
physical @xmath -even Higgs-boson masses read

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

The mass of the light CP-even Higgs-boson is therefore bound from above
through the relation @xmath .
For completeness, the masses of the charged Higgs-bosons can be derived
via the mass matrix of the charged-boson components

  -- -------- -------- -- ------
     @xmath   @xmath      (27)
  -- -------- -------- -- ------

as

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (28a)
              @xmath      (28b)
  -- -------- -------- -- -------

where relations Eqs. ( 17 - 18 ) are again useful. Similar to the SM
relations, the gauge-boson masses are given by

  -- -- -- ------
           (29)
  -- -- -- ------

The rich phenomenological implications of the real MSSM can be explored
further, when studying the dependence on the angles @xmath and @xmath .
The angle @xmath is linked to the vacuum expectation values through
Eq. ( 24 ). In turn, the angle @xmath can be determined from the
rotation of Eq. ( 25b ) into the physical basis. The following basic
relation among the two angles holds [ 83 ]

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

Many more relations among the angles can be found, compare Ref. [ 82 ,
83 ] . When expressing the couplings in terms of these, they can be
formulated as angle suppression factors with respect to Standard Model
Higgs-boson couplings to, e.g., vector bosons

  -- -------- -- ------
     @xmath      (31)
     @xmath      (32)
  -- -------- -- ------

where @xmath denotes the Standard Model Higgs-boson and @xmath the MSSM
Higgs-bosons.

Besides, in order not to be a toy model, the features of the Standard
Model must be reproduced in the MSSM, at least in certain parametric
limits. This is fulfilled in the decoupling limit, taking the limit
@xmath . Then, the physical Higgs-bosons @xmath , @xmath and @xmath
decouple from the theory and the Standard Model Higgs-boson sector
consisting of a single physical @xmath -even scalar @xmath results.
Additionally, a SUSY mass scale much larger than the electro-weak scale
can be assumed. Then, @xmath becomes indistinguishable from the
Higgs-boson @xmath of the Standard Model, since all Standard Model
tree-level and loop-induced couplings to Standard Model gauge bosons and
fermions are reproduced. A decoupling may also occur independent of the
@xmath boson mass in other regions of the MSSM parameter space. For a
discussion, see Refs. [ 85 , 89 ] .

### 2 The scalar quark sector and SQCD at tree-level

In light of the calculation to be discussed in detail in Chap. id1 , an
introduction of the tree-level quark and squark interactions is
necessary, including those of supersymmetric Quantum Chromodynamics
(SQCD). Interactions with the Higgs-boson sector are also discussed.

The relevant parts of the MSSM Lagrangian regarding SQCD, the fermion
@xmath and the scalar fermion (sfermion) @xmath sector, reads

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

where the first two terms on the righthand side of Eq. ( 33 ) are the
free field equations for the squarks and sleptons and the quarks and
leptons, respectively. The last two terms are the QCD and SQCD gauge
field contributions. They can be combined as follows

  -- -------- -- ------
     @xmath      (34)
  -- -------- -- ------

where @xmath in the first term on the righthand side is the
supersymmetric SU(3) Yang-Mills field-strength tensor defined as

  -- -------- -- ------
     @xmath      (35)
  -- -------- -- ------

compare Ref. [ 90 ] . Here, the @xmath denote the generators of SU(3)
@xmath , @xmath is the strong coupling constant and the @xmath represent
the gluon and gluino fields, compare Tab. 2 .

The @xmath and @xmath are the covariant derivatives with respect to the
superspace coordinates and are defined as

  -- -------- -- ------
     @xmath      (36)
  -- -------- -- ------

While the gluon is massless, the gluino acquires a mass term from
explicit but soft supersymmetry breaking

  -- -------- -- ------
     @xmath      (37)
  -- -------- -- ------

This type of mass term can rather generically be introduced for all
fermions of a supergauge multiplet, compare Refs. [ 51 , 88 ] .
Following Ref. [ 82 ] , the third and fourth term in Eq. ( 33 ) can be
split into F-terms, D-terms and soft breaking terms. Additionally,
interactions between the gluino @xmath , a quark and a squark must be
taken into account. Hence, the full potential reads

  -- -------- -------- -- ------
     @xmath   @xmath      (38)
  -- -------- -------- -- ------

The F-term contributions are

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      
              @xmath      (39)
  -- -------- -------- -- ------

where Yukawa interactions of the type @xmath , @xmath and @xmath can be
read off. While the @xmath and @xmath denote the scalar superfields, see
Tab. 2 , the Yukawa couplings read

  -- -- -- ------
           (40)
  -- -- -- ------

The D-terms contributing to the scalar potential Eq. ( 38 ) read

  -- -------- -------- -- ------
     @xmath   @xmath      
     @xmath   @xmath      
     @xmath   @xmath      (41)
  -- -------- -------- -- ------

where @xmath , @xmath and @xmath are the hypercharges of the respective
superfield. The squarks couple not only weakly to each other, but also
strongly via

  -- -------- -- ------
     @xmath      (42)
  -- -------- -- ------

compare e.g. Ref. [ 91 ] . The soft-breaking part of the potential reads

  -- -------- -------- -- ------
     @xmath   @xmath      
     @xmath   @xmath      (43)
  -- -------- -------- -- ------

where the products @xmath denote the trilinear couplings of the
Higgs-bosons to the squarks. In the following, @xmath is chosen by
convention. @xmath is the mass parameter of the left-hand sparticles,
@xmath and @xmath are the mass parameters of the righthand up-type and
down-type sparticles, respectively. The soft breaking terms lead at most
to a logarithmically divergent behavior and gauge invariance is ensured,
see Refs. [ 51 , 88 ] . The slepton fields @xmath are omitted here but
can be included as well with an appropriate choice of hypercharges. The
Yukawa interaction of the quarks and leptons reads

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

where @xmath is the Yukawa coupling to the leptons. The quark and lepton
masses purely arise from the Yukawa interactions, therefore the quark
and lepton mass matrices can be directly deducted from the Yukawa terms.
The interaction of two quarks with a gluon reads

  -- -------- -- ------
     @xmath      (45)
  -- -------- -- ------

where @xmath and @xmath are color indices. Now, the squark-squark gluon
interaction is

  -- -------- -- ------
     @xmath      (46)
  -- -------- -- ------

where the sums run over both left- and righthanded components, compare
Ref. [ 38 ] . Finally, the squark-quark-gluino interaction reads

  -- -------- -- ------
     @xmath      (47)
  -- -------- -- ------

where the relative minus sign comes in with the negative sign of the
color generator @xmath of the color antitriplets. Likewise, there are
electroweak quark-squark-gaugino interactions. They are not listed here
because they are not needed in the calculation of Chap. id1 .

The squark masses are composed of the soft breaking terms, but also the
F- and D-terms of the squark potential, when the Higgs-bosons acquire
vacuum expectation values. Altogether, the massive part of the squark
sector in the MSSM reads

  -- -------- -------- -- ------
     @xmath   @xmath      (48)
  -- -------- -------- -- ------

where the up-type squark mass matrix is given by

  -- -------- -- ------
     @xmath      (49)
  -- -------- -- ------

with @xmath from previous definitions in Eq. ( 23 ) and Eq. ( 40 ). The
parameter @xmath is taken to be real in the rMSSM. A similar mass matrix
can be set up for the down-type squarks from the previously described
parts of the Lagrangian. Using further the definitions of Eq. ( 10 ) and
of the hypercharges below Eq. ( 41 ), the up-type squark mass matrix can
be written as

  -- -------- -- ------
     @xmath      (50)
  -- -------- -- ------

with @xmath and where @xmath denotes their charge and @xmath the third
component of the isospin of the up-type squark, respectively. For
completeness, the down-type squark mass matrix is also given,

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

with @xmath . The squark mass matrices can be rotated into the physical
basis

  -- -------- -------- -- ------
     @xmath   @xmath      (52)
  -- -------- -------- -- ------

with the physical squark mass eigenstates @xmath and @xmath . The new
mass eigenstates are related to the unphysical masses via an orthogonal
transformation

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (53a)
              @xmath      (53b)
  -- -------- -------- -- -------

where the unitarity matrix

  -- -------- -- ------
     @xmath      (54)
  -- -------- -- ------

is parametrized by the mixing angle @xmath .
Matching the two mass matrices in Eq. ( 50 ) and Eq. ( 53b ), an
expression for the parameter @xmath can be formulated as follows

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

### 3 Higher-order Higgs-boson mass corrections within the real MSSM

With the light neutral @xmath -even Higgs-boson tree-level mass @xmath
being limited to @xmath at most, compare Eq. ( 26 ), the MSSM could
already have been excluded at LEP due to the lack of its observation.
Yet, higher-order self-energy corrections shifted the upper bound on the
light Higgs-boson mass considerably. There are mainly three different
methods to approach higher-order mass corrections. They can be combined
as well. An exact calculation, invariant under different gauge-fixing
terms, is achieved using the Feynman-diagrammatic (FD) approach, where
the self-energy diagrams are evaluated explicitly. The second method
uses an effective potential approximation further developed for higher
loop calculations, compare Ref. [ 92 ] . In this approach, the scalar
Higgs-boson fields are expanded around their vacuum expectation values.
This allows for the computation of the higher-loop effective potential,
involving vacuum diagrams of the given loop order. The results are
compact, but also of limited accuracy. As this approach expands around a
constant value of the fields, the momentum dependence of the two-loop
self-energies cannot be taken into account. A third approach uses
effective Lagrangians capturing the dynamics and symmetries of a system
in generic terms, while the phenomenology is contained in their
coefficients. Effective Lagrangians are often used in model-independent
analyses.

In the calculation presented in Chap. id1 of this thesis, the
Feynman-diagrammatic approach is adopted. In this approach, higher order
mass corrections are computed by allowing for perturbations to the
propagators of the fields @xmath which result from the solutions to the
equations of motion. For completeness

  -- -------- -- ------
     @xmath      (56)
  -- -------- -- ------

where @xmath and where @xmath . Firstly, the equations of motion require
that the terms of the MSSM potential part of the Lagrangian linear in
the @xmath -even Higgs-boson fields @xmath , see Eq. ( 16 ), must
vanish. For this condition to be met, all higher-order corrections up to
@xmath order need to be seen as canceling. At tree level, the terms
linear in the fields are tadpole coefficients. Higher orders include
additional propagators to tadpole lines in terms of loops, where the
coefficients are termed @xmath . In conclusion, the statement reads

  -- -------- -- ------
     @xmath      (57)
  -- -------- -- ------

Secondly, the bilinear free field and potential parts of the Lagrangian,
Eq. ( 7 ) and Eqs. ( 8 )-( 11 ), lead to the following contributions to
the equations of motion

  -- -- -- ------
           (58)
  -- -- -- ------

Hence, those terms bilinear in the same field get solutions to the
equations of motion in terms of causal Green functions including a
momentum and a massive part, while the solutions to terms bilinear in
two different fields contain only a massive part. Computing higher
orders in perturbation theory corresponds to adding one-particle
irreducible terms @xmath to the propagators [ 93 ]

  -- -- -------- -- ------
        @xmath      (59)
        @xmath      
        @xmath      (60)
        @xmath      (61)
  -- -- -------- -- ------

where the bold-faced propagator is the one corrected to all orders in
perturbation theory and where each one-particle irreducible term can be
split into its different orders,

  -- -------- -------- -- ------
     @xmath   @xmath      (62)
  -- -------- -------- -- ------

A geometric series relation is used in the last step from Eq. ( 60 ) to
Eq. ( 61 ). The cases where different fields enter the time ordered
two-point correlation functions can be treated similarly. Assuming that
the self-energy corrections in Eq. ( 62 ) can be renormalized, the
renormalized self-energies @xmath enter as corrections to the inverse
propagator matrix of the field @xmath ,

  -- -------- -- ------
     @xmath      (63)
  -- -------- -- ------

The loop-corrected masses @xmath and @xmath are determined by the real
parts of the propagator matrix of the field @xmath . This is equivalent
to solving the equation

  -- -------- -- ------
     @xmath      (64)
  -- -------- -- ------

The status of the available self-energy corrections to the real MSSM can
be summarized as follows.

At the one-loop level, the full corrections to the MSSM Higgs-boson
masses are known, including gauge bosons contributions and momentum
dependence, see Refs. [ 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 ] .

At two loops, a full result using an effective potential approach is
known [ 102 , 103 ] . Preceding works used a two-loop renormalization
group equation (RGE) improved one-loop effective potential approach [
104 , 105 , 106 , 107 , 108 ] , or a two-loop effective potential
approach [ 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 ] .
Furthermore, explicit computations have been performed in the
Feynman-dia-grammatic approach, neglecting gauge contributions and
assuming vanishing external momentum [ 117 , 118 , 119 , 120 ] . The
latter cover the dominant corrections of the order @xmath and @xmath and
the sub-dominant two-loop contributions of the order @xmath , @xmath and
@xmath . The orders are given in terms of the coupling factors entering
the vertices of the loop diagrams. These are the strong coupling
constant @xmath and the Yukawa couplings @xmath and @xmath of the top
and the bottom quark, respectively. The relative size of a correction
can be estimated a priori by assessing the relative size of its
couplings. Due to supersymmetry, the Yukawa couplings for the quarks and
squarks are equivalent @xmath . The soft supersymmetry breaking terms
contributing in the coupling of the Higgs-bosons to the squarks are
proportional to the Standard Model Yukawa couplings as well, compare
Eq. ( 43 ). Therefore, no distinction between @xmath and @xmath is
needed.

Complementary to the Feynman-diagrammatic approach, higher-order
corrections to the Higgs-boson masses have been found using the
effective Lagrangian approach [ 121 , 122 , 123 , 124 , 125 , 126 ] .
Other studies aim towards a combination of the existing two-loop results
obtained in different approaches, see Refs. [ 110 , 111 , 112 , 127 ,
128 ] .

Regarding the third order, the dominant corrections of the order @xmath
are available [ 129 , 130 , 131 ] , where gauge contributions and a
non-vanishing external momentum still need to be incorporated in future
calculations. Also, third and higher order resummation effects have
recently been taken into account [ 132 , 133 ] .

Remarkably, all these higher-order corrections lead to an approximate
upper bound of @xmath GeV, where the maximal value for the light
Higgs-boson mass depends sensitively on the precise value of the
top-quark mass, compare Refs. [ 119 , 128 ] .
While the one-loop corrections relocate the upper bound towards higher
masses, the two-loop corrections enter with competing signs and the
three-loop corrections further stabilize the mass, entering with both
signs. The overall dominant corrections come from those self-energies
involving the top quark and the scalar top (stop) quarks.

The remaining contributions of higher orders can be estimated based on
the already available results. To reach a theoretical precision matching
or even surpassing the experimental one, corrections beyond the above
mentioned ones must be taken into account. At two loops, the remaining
uncertainties originate from neglecting gauge contributions and momentum
dependence [ 134 ] . Therefore, the calculation of the momentum
dependent two-loop corrections to the MSSM Higgs-boson self-energies is
of interest. The momentum dependence at the one-loop level is known to
generally amount to less than 2 GeV, compare Ref. [ 135 ] . The dominant
corrections at the two-loop level have been calculated adopting a full
@xmath renormalization scheme, see Refs. [ 136 , 137 , 138 ] .
Higher-order corrections to the tree-level MSSM Higgs-boson masses can
only be applied consistently if they are computed within the same
renormalization scheme. It is therefore interesting to analyze the
momentum-dependent contribution again, but using an on-shell
renormalization for all masses entering the calculation. Although the
calculation becomes more involved with this renormalization scheme
choice, the benefit of being able to incorporate those corrections into
the public program FeynHiggs [ 139 , 140 , 141 , 142 ] and thereby
making the corrections readily available, is outweighing.

## Chapter \thechapter Multi-scale integrals beyond one loop

This chapter explores diverse techniques regarding the computation of
multi-scale integrals beyond one loop. In particular, a focus is laid on
two-loop calculations, where fully differential phenomenological
predictions start to emerge for a variety of processes. The chapter
culminates in a motivation for the usage of Feynman parameterization in
a tool to compute multi-loop multi-scale integrals in an automated way.

### 4 The two loop frontier

Due to the high energies at present hadron colliders, processes at very
small distances can be resolved. At the Tevatron and the LHC, these
dominantly involve quarks and gluons (partons). While the Tevatron is a
proton-antiproton collider, the dominant production channel has @xmath
pairs in the initial state, at the LHC the @xmath channel is the
dominant initial state. Compared to @xmath annihilation processes at
past colliders this is one additional external leg when higher orders
are computed. This greatly increases the complexity of such a
computation.

At next-to-leading order ² ² 2 It should be noted that the leading order
usually encompasses tree-level diagrams, but there are LO processes
(e.g. Higgs production in gluon fusion) which start with loop diagrams.
in perturbative QCD, the frontier is looking at final states with many
particles and matching them with a parton shower. The motivation to go
to NNLO accuracy is at least twofold. First, the dependence on the
renormalization scale is expected to be reduced. Second, a process has
more partons in the final state. This initiates the reconstruction of
the parton shower, thus approaching an experimental jet reconstruction.

The forefront at NNLO is the computation of four-point processes, where
various complications can arise, one being a complicated singularity
structure of individual diagrams, and the other the involvement of
internal and external masses leading to multi-scale problems. This can
render the evaluation of sometimes just single diagrams a highly
non-trivial task. While multiple legs and scales can already be very
complicated at NLO, at NNLO completely new challenges arise in addition
when generalizing the techniques employed at NLO to NNLO and diverse
conceptual differences must be acknowledged.

Beyond NLO, the renormalization procedure for the cancellation of
ultraviolet (UV) singularities involves not only counter-terms of the
loop order to be considered but also counter-terms of lower loop order
with insertions. Finding the full but minimal set of diagrams is non
trivial but was solved and further developed by Bogoliubov, Parasiuk,
Hepp and Zimmermann, summarized in the (BPHZ) theorem, see Refs. [ 143 ,
144 , 145 ] .

Furthermore, a mixture of real radiation and virtual corrections enter a
cross section at NNLO, in addition to double real and double virtual
contributions. The typical ingredients of an NNLO calculation are
depicted in Fig. 1 . Given the fact that now three pieces enter the
calculation which are all part of different phase space dimensions, the
need for more discriminating and refined subtraction schemes emerges.
At NLO, subtraction schemes are already well established, see Refs. [
146 , 147 , 148 ] . At NNLO there are various subtraction schemes
available, all with different aims and capabilities. After the
introduction of @xmath subtraction by Catani and Grazzini [ 149 , 150 ]
, the idea of using the sector decomposition algorithm [ 151 , 152 ] for
a complete NNLO calculation was originally proposed by Heinrich [ 153 ]
. It was taken up and further developed into a full subtraction scheme
in Refs. [ 154 , 155 , 156 ] , and first applied to a full process in
Ref. [ 157 ] . The idea proposed by Czakon to combine sector
decomposition applied to real emission integrals with phase-space
partitioning from FKS subtractions, lead to the sector improved residue
subtraction [ 158 ] , successfully applied, e.g., in Refs. [ 65 , 159 ]
. Further, the antenna factorization introduced in Refs. [ 160 , 161 ,
162 ] was explicitly worked out and applied to full NNLO processes [ 163
, 164 , 165 , 166 ] . Lastly, a direct generalization of dipole
subtraction to NNLO processes was presented by Somogyi, Trocsanyi et al.
[ 167 , 168 ] .

While for the real radiation the computation of more and more legs is of
interest, for the virtual contributions higher-order loop integrals need
to be solved. In light of the fact that the number of diagrams
contributing to higher-order processes increases tremendously from one
order to the next, and the diagrams themselves become more and more
complicated, it is desirable to find highly automatable procedures to
tackle these. At NLO, tools towards this aim are already highly
developed and sophisticated. The procedure of generating the real
radiation and loop amplitudes contributing to a full process is
automated to a large extent by programs like aMC@NLO [ 169 ] , BlackHat
[ 170 ] , FeynArts, FormCalc & LoopTools [ 171 ] , GoSam [ 172 ] ,
HELAC-NLO [ 173 ] , HERWIG++ [ 174 ] , Matchbox [ 175 ] , MCFM [ 176 ] ,
NJet [ 177 ] Open Loops [ 178 ] , POWHEG [ 179 ] , RECOLA [ 180 ] ,
Sherpa [ 181 ] , VBFNLO [ 182 ] . There are diverse tools allowing for
an automated generation of the pure Feynman amplitudes. The programs
with loop capabilities are FeynArts [ 183 , 184 ] or QGRAF [ 185 ] .

The full basis of irreducible master integrals is known at one-loop
order. It comprises scalar pentagon, box, triangle, bubble and tadpole
diagrams which are known analytically or can be computed numerically [
186 , 187 , 188 , 189 ] . Beyond one loop, the set of irreducible
integrals, so called master integrals, is not known which makes the
decomposition more difficult. Master integrals beyond one loop can have
irreducible numerators which need to be evaluated in addition. In the
reduction of multi-loop amplitudes to a set of resulting master
integrals, integration by parts relations [ 190 , 191 , 192 ] and
identities resulting from Lorentz invariance [ 193 ] are indispensable.
The former are based on the fact that the integral over the total
derivative with respect to any loop momentum @xmath vanishes in
dimensional regularization

  -- -------- -- ------
     @xmath      (65)
  -- -------- -- ------

where the integrand @xmath may contain any combination of propagators,
scalar products and loop momentum vectors. Together with the
exploitation of Lorentz invariance by

  -- -------- -- ------
     @xmath      (66)
  -- -------- -- ------

where @xmath is the number of external momenta @xmath and @xmath the
internal masses, the reduction to master integrals can be achieved. The
full reduction into less complicated integrals can be done if the number
of constraints matches or exceeds the number of unknown integrals.

In the following, the general structure of the resulting master
integrals is shown and different methods to solve them are described.

### 5 Two and more loop integrals with multiple scales

The difficulty of such multi-loop integrals has led to their extensive
study and the development of various specialized integration techniques.
In the following, a general multi-loop integral is introduced before
presenting a variety of techniques to tackle these.

A general Feynman loop integral @xmath at @xmath loops with @xmath
propagators, where the propagators @xmath can have in principle
arbitrary powers @xmath and mass @xmath , has the following
representation in momentum space

  -- -------- -------- -- ------
     @xmath   @xmath      (67)
     @xmath   @xmath      (68)
  -- -------- -------- -- ------

where the @xmath are linear combinations of external momenta @xmath and
loop momenta @xmath . While the rank @xmath of the integral is indicated
by the number of loop momenta appearing in the numerator, the indices
@xmath denote which of the @xmath loop momenta belongs to which Lorentz
index @xmath . The factor of @xmath in @xmath is chosen by convention to
remove any dependence on @xmath after the integration over the loop
momenta within Feynman parameterization. The renormalization scale is
denoted by @xmath . The @xmath in Eq. ( 68 ) results from the solutions
of the field equations in terms of causal Green functions.

The integral is regulated dimensionally meaning that the integer
dimension number @xmath is shifted by an infinitesimal quantity @xmath
to the new dimension variable @xmath . Infrared or ultraviolet poles
then appear as poles in the regulator @xmath . Careful distinction has
to be made between ultraviolet ( @xmath ) and infrared regulators (
@xmath ). Formally, the theory is first renormalized dimensionally by
going a little below the integer number @xmath of space-time dimensions
and afterwards analytically continued to a little value above @xmath to
regulate the mass singularities, see Ref. [ 194 ] for a comprehensive
discussion. A modified version of the dimensional regularization is the
dimensional reduction (DR). It is of particular interest in SUSY
calculations, since it preserves global gauge invariance and
supersymmetry, also at the two loop level [ 195 ] . Adopting this
regularization scheme first introduced and applied in Refs. [ 196 , 197
] , all external particles and all gamma matrices @xmath appearing in
the couplings are treated as quasi four-dimensional, while the loop
integrals are computed in @xmath dimensions. For comparison, the
introduction of a cutoff @xmath to the propagators to regulate UV
divergences corresponds to the Pauli-Villars regularization.

The complexity of the evaluation of loop integrals generally increases
with the number of loops and the number of legs. Massive internal lines
further increase the level of complexity by raising the number of
involved scales. All masses and invariants formed from external momenta
are summarized in the term “kinematic invariants”. The sum of
independent kinematic variables corresponds to the number of scales
involved in a diagram. Already at one-loop many-scale integrals are hard
to compute. Multi-loop integrals involving multiple scales are
particularly demanding. Further complexity arises with the non-planarity
of graphs. Such diagrams only appear beyond one loop. These additional
complications make the evaluation of multi-loop integrals an extremely
non-trivial task and shrewd and refined techniques need to be employed
to tackle these. Analytical techniques are very advanced, but when it
comes to automation they very often still reach their limit. Numerical
methods are in general easier to automate but issues here are the speed
and the accuracy.

In the following, the main technical approaches towards the evaluation
of multi-loop and multi-scale integrals are reviewed. The main recent
developments are sketched before concluding with a motivation for the
method chosen to be investigated in this thesis.

### 6 Introduction of Feynman parametrization

A first method to deal with Feynman loop integrals is to introduce
Feynman parameters to every propagator. To prove that this technique
indeed works, it is descriptive to have a look at a two-propagator
example [ 93 ]

  -- -------- -- ------
     @xmath      (69)
  -- -------- -- ------

with the propagators @xmath and @xmath and the Feynman parameters @xmath
and @xmath . Generalizing this idea to multiple propagators is
straightforward [ 152 ]

  -- -------- -- ------
     @xmath      (70)
  -- -------- -- ------

where @xmath . An equivalent representation was derived by Schwinger.
Parameters are usually referred to as Schwinger variables when they are
assumed to have values between zero and infinity, while values between
zero and one are associated with Feynman parameterization. The general
form of a multi-loop integral reads

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (71)
  -- -------- -------- -- ------

where the propagators are written in terms bilinear in the loop momenta
with coefficients contained in the matrix @xmath , terms linear in the
loop momenta with coefficients @xmath and remaining terms included in
@xmath depending on the masses, external momenta and the Feynman
parameters only. To be able to integrate out the loop momenta, the terms
bilinear and linear in the loop momenta need to be brought into a
quadratic form by a shift

  -- -------- -- ------
     @xmath      (72)
  -- -------- -- ------

The integration of the quadratic form is then straightforward. After
also integrating out the radial coordinates the general loop integral
reads

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      
              @xmath      (73)
  -- -------- -------- -- ------

where

  -- -------- -------- -- ------
     @xmath   @xmath      (74)
     @xmath   @xmath      (75)
  -- -------- -------- -- ------

Note the sign of the infinitesimal imaginary part in Eq. ( 74 ). It
results from factoring an overall minus sign into the prefactor during
Wick rotation. Each loop momentum @xmath in the numerator is associated
with a fixed Lorentz index @xmath . The @xmath refer to the combination
of both indices @xmath . For rank @xmath , the second line of Eq. ( 73 )
reduces to the factor @xmath , containing overall UV divergences if
present. In the case of @xmath , the product @xmath does not contribute
and @xmath . For loop integrals of higher rank @xmath , products of the
matrix element @xmath with the metric tensor @xmath contribute as a sum
of all possible combinations with the vectors @xmath in the double
indices @xmath . As an example of how to read the notation involving the
metric tensors, for rank @xmath and correspondingly @xmath it is

  -- -------- -- ------
     @xmath      (76)
  -- -------- -- ------

The functions @xmath and @xmath in Eq. ( 73 ) are the first and second
Symanzik polynomial, respectively, and are homogeneous (in the Feynman
parameters). @xmath is positive semi-definite and @xmath is negative
semi-definite when all propagators are massless.

The two functions can also be obtained using a graph-theoretical method,
where the polynomials are constructed from topological cuts of the
corresponding Feynman graph. For the construction of the function @xmath
, @xmath lines of the graph are cut, whereas @xmath lines are cut to
arrive at the function @xmath , see Refs. [ 198 , 199 , 152 ] . To
illustrate this for a simple example, assume a massless one-loop box
with all external legs being light-like, compare Fig. 2 . Then, the
first Symanzik polynomial @xmath is constructed from adding up all
possible @xmath line cuts of propagators which lead to a tree-level
diagram. Each cut propagator contributes with its Feynman parameter, if
more than one propagator needs to be cut (which happens for @xmath ),
products of Feynman parameters, all of the same degree, enter the
function @xmath . In the case of the one-loop box with massless
propagators, this reads

The second Symanzik polynomial @xmath is constructed from adding up all
possible @xmath line cuts of propagators. All @xmath cut propagators
contribute with their Feynman parameter and the squared momentum flowing
into the resulting tree. In the case of the one-loop box with massless
propagators, the function @xmath then reads

While the prefactor @xmath in Eq. ( 73 ) contains overall ultraviolet
divergences if present, the vanishing of the function @xmath is related
to ultraviolet sub-divergences of the graph. The second Symanzik
polynomial contains the occurring infrared singularities. The occurrence
of these depends not only on the topology as in the UV case, but on the
kinematics as well. If some of the kinematic invariants are zero, e.g.
when some external momenta are light-like, the vanishing of @xmath may
induce an infrared (IR) divergence. Therefore general theorems about the
IR singularity structure of multi-loop integrals are sparse. For
practical purposes sector decomposition can provide information about
the singularity structure and numerical results, because it offers a
constructive algorithm to extract the poles in @xmath . When
generalizing the kinematic invariants to physical space, the second
Symanzik polynomial can also vanish when linear combinations of Feynman
parameters and kinematic invariants vanish. A clever deformation of the
integration contour to the complex plane helps dealing with these
physical poles and the integration over thresholds.
For a diagram with only massless propagators, the function @xmath does
not contain any squares in the Feynman parameters. If massive internal
lines are present, terms of the type

  -- -------- -- ------
     @xmath      (77)
  -- -------- -- ------

appear. These are the source of complexity when it comes to the
analytical evaluation of multi-scale integrals, as many methods used for
the simplifcation of an integrand can no longer be applied.
This opens the field for a numerical treatment of multi-scale integrals,
where squared Feynman parameters are not a bottleneck to the
calculation. Additionally, the introduction of Feynman parameters is
highly automatable paving the way towards a very general solution to a
large class of multi-scale integrals with arbitrary kinematics.
The problems occurring with this method and their solution will be
explained in the next two chapters. Beforehand, several other approaches
towards solving multi-loop integrals will be reviewed.

### 7 The virtues of a Mellin-Barnes representation

In analogy to Eq. ( 69 ), the main transformation to arrive at a
Mellin-Barnes representation can be summarized in one line

  -- -------- -- ------
     @xmath      (78)
  -- -------- -- ------

with the difference, that now a sum in the denominator is transformed
into a product. The sum on the left-hand side can either be a massive
propagator or a sum of two propagators after Feynman parametrization. In
the first case, massive propagators are expressible as a continuous
superposition of massless propagators. Considering that the massive
propagators introduce squares in the Feynman parameters, see Sec. 6 ,
this transformation can be very beneficial.

In general, a factorization of the type Eq. ( 78 ) can be used to
achieve a representation of loop integrals in terms of gamma functions,
which are in general easier to evaluate. This benefit comes at the cost
of extra Mellin integrations. Within their integration domain, poles in
the variable @xmath can occur. Taking these into account, the
integration contour must always be chosen such that the poles with a
@xmath dependence are placed left of the contour and the poles with a
@xmath dependence are situated on the right-hand side with respect to
the contour. Closing the contour to the right and taking a series of
residues, the integral can be evaluated. Yet, finding the appropriate
contour is non-trivial.

With the computation of the planar [ 200 ] and non-planar [ 201 ]
massless two-loop four-point diagram, Smirnov and Tausk pioneered the
utilization of a Mellin-Barnes representation finding an appropriate
choice of contours for physical kinematics including thresholds. Several
software packages became available automating the analytical procedure
where possible, see Refs. [ 202 , 203 , 204 ] . The more scales are
involved, the less easy it is to arrive at a fully analytical result.
Numerical approaches have also been considered [ 205 , 206 , 207 ] ,
putting much effort in the automation of a proper analytical
continuation of the integrand. Very recently an idea by Pilipp [ 208 ]
was implemented with in combination with Feynman parametrization to
treat such contours in an automated way, see Ref. [ 207 ] . It works as
follows: The second Symanzik polynomial @xmath is decomposed as

  -- -------- -- ------
     @xmath      (79)
  -- -------- -- ------

where a small coefficient to terms in @xmath is extracted into the
parameter @xmath and where all terms contained in @xmath and @xmath are
sufficiently large as not to contribute to a singularity. The
Mellin-Barnes representation is then introduced for the product

  -- -------- -------- -- ------
     @xmath   @xmath      (80)
  -- -------- -------- -- ------

After the application of sector decomposition which will be described in
detail in Chap. id1 , the functions @xmath and @xmath are constant in
the Feynman parameters @xmath , so that the singularity structure is
revealed in the exponents of the factorized @xmath

  -- -------- -- ------
     @xmath      (81)
  -- -------- -- ------

where the @xmath enters with the Feynman parametrization, and where
@xmath , @xmath and @xmath are some coefficients resulting from the
division into sectors. Allowing only those integrals where @xmath , the
integration contour of the integral over the variable @xmath can be
closed to the right, allowing for the application of Cauchy’s integral
theorem. The residues arising from terms of the type @xmath after @xmath
integration need to be taken into account. Afterwards, an expansion in
the parameter @xmath can be performed.

The usage of a Mellin-Barnes representation can be very beneficial in
diverse contexts and can even be applied in an automated way to the
physical region with the computation of asymptotic expansions in the
Feynman integrals. Yet, a fully automated approach in all regions of
phase space is difficult.

### 8 The method of differential equations

As it turns out, a representation for the master integrals resulting
from the reduction can also be found by setting up differential
equations in all kinematic invariants and solving them with the
appropriate boundary conditions. The method was first introduced by
Kotikov who related massive loop integrals to massless ones, see Ref. [
209 ] and then generalized to differential equations in all kinematic
invariants, see Refs. [ 210 , 211 ] .
Taking the derivative of an integral with respect to one of its
invariants yields linear combinations of integrals with at most one
additional propagator in the denominator and one additional scalar
product in the numerator. The derivatives of the invariants @xmath can
be expressed in terms of derivatives in the external momenta, e.g. for
box diagrams

  -- -------- -- ------
     @xmath      (82)
  -- -------- -- ------

This generates similar expressions as those resulting from IBP relations
mentioned in Sec. 4 and Lorenz invariance identities (LI) introduced by
Gehrmann and Remiddi, see Ref. [ 193 ] . Using the IBP and LI relations,
the integrals which received an additional propagator or scalar product
can be reduced again to such an extent as to result in a system of
differential equations. In the example of box diagrams, one of the
equations reads

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (83)
  -- -------- -------- -- ------

where @xmath is an integral of a diagram with four external legs of
@xmath different propagators and @xmath scalar products and where the
function @xmath is rational. The function @xmath plays the role of an
inhomogeneous term and the integral contained in it, @xmath , is one
differing propagator short compared to the @xmath integral. The boundary
conditions can be derived from kinematical limits, e.g. a vanishing
invariant @xmath ,

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (84)
  -- -------- -------- -- ------

where @xmath . Afterwards, the differential equations can be solved by
introducing an integrating factor @xmath of the type

  -- -------- -- ------
     @xmath      (85)
  -- -------- -- ------

yielding solutions to the inhomogeneous equation

  -- -------- -------- -- ------
     @xmath   @xmath      
              @xmath      (86)
  -- -------- -------- -- ------

where the integral over the function @xmath and @xmath is either known
or relatively easy to integrate and where the constant @xmath is chosen
such that it matches the boundary conditions.
The nice feature of this technique is that it can be applied to
arbitrary multi-loop integrals with arbitrary scales. However, the
current bottleneck is related to the appearance of elliptic integrals.

These already appear in the rather simple but all-massive two-loop
bubble with different masses, see Fig. 3 . After the developments
summarized in this thesis, such an integral is easily treated
numerically for in principle arbitrary kinematics and in a fully
automated way.

### 9 Further analytic developments

Apart from the multi-purpose techniques already mentioned in the
previous sections, there are many more specialized tricks to attack a
special class of multi-loop integrals on the one hand, and other ideas
based on long-known mathematical concepts to simplify the result on the
other hand. Furthermore, intensive exploration of diverse mathematical
concepts uncovered new criteria and underlying structures to easier
access scattering amplitudes.

A presentation of analytic results in terms of generalized
hypergeometric functions has been found to yield very compact results.
The evaluation of special cases of hypergeometric functions, namely
generalized Lauricella functions involving elliptic integrals are not
accessible by present analytical techniques. In contrast, for all
results expressible in terms of generalized harmonic polylogarithms
(GHPLs) a fast, accurate and stable numerical evaluation of the
analytical expressions can be found. GHPLs are generalizations of
harmonic polylogarithms [ 212 ] , introduced in Refs. [ 213 , 214 ] and
applied in innumerable phenomenological applications. They are not all
independent and relations among them can become very complicated. A
systematic approach to govern the complexity of such relations is
therefore highly desirable in the study of multi-loop integrals. One
such approach is the formulation of results in terms of symbols. The
concept, introduced by Zagier and Gonachrov in Refs. [ 215 , 216 , 217 ]
, allows for particularly simple and elegant expressions. After the
symbol calculus was applied in the context of @xmath supersymmetric
Yang-Mills (SYM) theory, see Refs. [ 218 , 219 , 220 , 221 , 222 , 223 ,
224 , 225 , 226 , 227 , 228 ] , it was found to be also applicable to
diverse phenomenological problems, see Refs. [ 229 , 230 , 231 , 232 ,
233 , 81 , 234 , 80 , 235 , 236 ] . The coproduct, as a generalization
of the symbol, allowed for the conservation of information on constants
with an associated weight, as was pointed out in Refs. [ 237 , 238 , 231
] .

As already mentioned in Sec. 4 , beyond one-loop the basis of master
integrals is not fixed. Finding criteria for an optimal basis was
therefore a major breakthrough in the computation of multi-loop
amplitudes. These were introduced by Henn, see Ref. [ 239 ] , and
further explored and applied in Refs. [ 240 , 241 , 242 , 243 , 244 ,
245 ] . They lead to a straightforward iterative solution of the
differential equations in the dimensional regulator @xmath .

While the formulation of results in terms of much simpler
representations is of vital importance in pushing the frontier towards
the computation of higher loop integrals, the introduction of completely
different approaches to the computation of the master integrals forms
the second pillar in multi-loop computations. Conceptually, every
mathematical object constituting a loop integral can be reformulated
using a different approach which is more suitable in a specific
calculation. Master integrals entering at higher order in perturbation
theory can be approached from a graph theoretical point of view, an
algebraic or a geometric point of view, to just name a few of the many
areas of interplay between physics and mathematics that lead to
attractive solutions to yet unsolved integral representations. The
liberation from calculations in strictly 4 dimensions, for example, lead
to plural ingenious approaches. The old concept of an infinitesimal
@xmath shift used in dimensional regularization is predominantly used in
the computation of loop integrals. It is also an old concept to shift
the dimension by positive or negative integer numbers, see Refs. [ 246 ,
198 , 247 , 248 , 249 , 250 ] , but one may also benefit from an
integral representation adopting a negative dimension. The latter was
developed in Refs. [ 251 , 252 , 253 , 254 ] and successfully applied in
the computation of a massless two-loop five-propagator diagram, where
the expression of a sub-loop is derived using negative dimensions, see
Ref. [ 255 ] . An abstraction to scalar one-loop vertex functions
including internal masses, off-shell legs and arbitrary propagator
powers was achieved for general dimensions, see Ref. [ 256 ] .
Another concept centers around the analysis of discontinuities across
the branch cuts of Feynman integrals. In the traditional approach, the
integral might be reconstructed directly from one of its discontinuities
using a dispersion relation, see Refs. [ 257 , 258 , 259 , 260 ] . This
technique can be generalized to the application of sequential unitarity
cuts in different channels, reconstructing one- and multi-loop
integrals, see Ref. [ 235 ] .
Furthermore, the criterion of linear reducibility of a graph has been
studied over the past few years and recently used in the computation of
diverse examples, compare Refs. [ 261 , 262 , 263 , 264 , 265 , 266 ,
267 , 268 ] . The examples, all being linearly reducible in the Feynman
parameters, can be integrated sequentially and analytical results can be
given in terms of multiple polylogarithms.

### 10 Motivation for adopting a numerical approach

One may realize that the quality of an employed technique to tackle
multi-leg, -loop and -scale integrals on the one hand lies in its
applicability to very generic cases of loop integrals, and on the other
hand in the achieved accuracy within a given time span in addition to
control over the parametric dependences. A fully automated elegant
analytical approach to compute all possibly existing loop integrals
would therefore be the perfect solution. Yet, analytical methods are
still struggling with the appearance of elliptic integrals, entering
already in rather simple two-loop diagrams, while numerical approaches
need a better ratio of speed to accuracy to compete with the elegance of
analytical results. The two main pillars therefore mutually enrich each
other and methods including analytical and numerical approaches push the
boundaries of what is computable with present techniques.

While the achievements using analytical methods were analyzed in the
previous sections, there are diverse groups who contributed highly
non-trivial results to significant phenomenological applications taking
up a numerical approach, compare e.g. Refs. [ 269 , 270 , 271 , 272 ,
273 , 274 , 275 , 276 , 277 , 58 , 159 , 278 ] .

In the work summarized in this thesis, a highly automated numerical
approach is adopted, filling the gap of the missing automated evaluation
of multi-loop multi-scale integrals including thresholds. To this end, a
representation of the integrals of interest in terms of plain Feynman
parameters is used. Due to its generality, the Feynman parametrization
can serve as the most universal approach to a numerical treatment of
integrals with arbitrary kinematics. Divergences are regulated
dimensionally and are factorized using the method of sector
decomposition. The program SecDec version 1 already implemented the
automated formulation of integrals in terms of Feynman parameterization,
integration of loop momenta and a series expansion in the dimensional
regulator @xmath , where the coefficients to each order in @xmath are
integrated numerically. The upgrade of this program to be able to deal
with mass thresholds within the integration region is one of the main
achievements of the work presented in this thesis. The advancement is
accomplished by an automated analytical continuation of the Feynman
parametrized integrand, building on work presented in Refs. [ 279 , 280
, 281 , 274 , 275 ] . With the resulting version 2 of SecDec , valuable
predictions and checks can be done, regardless of the number of scales
involved. Contrary to analytical methods, it can even be beneficial to
include more scales. While purely finite integrals with multiple scales
are hard to access with analytical methods, it is comparatively easy
using the numerical approach. Finally, it turns out that not only is the
developed tool useful for checks against analytical results, but it has
also proven powerful in computing analytically unaccessible integrals
for phenomenological applications.

## Chapter \thechapter The method of sector decomposition

As described in the introduction, a theory can be ultra-violet and
infrared divergent, The idea of renormalization is to subtract the
divergent parts and thereby make the theory finite. Finding the right
subtraction terms for the UV divergent parts was a long standing problem
whose solution resulted in the BPHZ theorem. To this end Hepp used a
decomposition of higher order loop diagrams into sectors to disentangle
overlapping UV divergences [ 144 ] . Thirty years later, the idea was
taken up by Denner and Roth for a disentanglement of UV divergences [
282 ] .

The application to infrared singularities and a systematic treatment of
these to arbitrary loop order using sector decomposition was pointed out
by Binoth and Heinrich [ 151 ] . It serves as a local subtraction
procedure to separate infrared divergences from individual diagrams.

### 11 Conceptual idea

The algorithm to find the subtraction terms of individual graphs works
in three main steps. In the first step, the singular components of an
integral are disentangled by iteratively decomposing the integral into
sectors. In a second step, the pole coefficients to each order in the
poles of the dimensional regulator @xmath are extracted. In the last
step, the coefficients containing kinematic invariants and Feynman
parameters are integrated analytically or numerically if an analytical
treatment is not accessible.

The idea of sector decomposition is essential for the first step of the
algorithm. It is based on splitting the integration region to achieve a
disentanglement of the singularities. As a simple example, consider the
following integral

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (87a)
              @xmath      (87b)
              @xmath      (87c)
              @xmath      (87d)
              @xmath      (87e)
  -- -------- -------- -- -------

where overlapping divergences appear in the limit of a vanishing of both
Feynman parameters @xmath in the first line. In the second line, see
Eq. ( 87b ), the integration region is split into one part where @xmath
is always bigger than @xmath and a second part where the hierarchy is
reversed. The splitting can be translated into a change of the
integration boundaries, compare Eq. ( 87c ). Using the transformation

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (88a)
     @xmath   @xmath      (88b)
  -- -------- -------- -- -------

in the first integral on the righthand side of Eq. ( 87c ) and the
transformation

  -- -------- -------- -- -------
                          
     @xmath   @xmath      (89a)
     @xmath   @xmath      (89b)
  -- -------- -------- -- -------

in the second integral on the righthand side of Eq. ( 87c ), both
integrals are remapped onto the unit hypercube, compare Eq. ( 87d ). The
transformations in Eqs. ( 88 ) and ( 89 ) are known in the mathematical
literature as blowing-up an affine @xmath -dimensional space, compare
e.g. Ref. [ 283 ] . The number of variables participating in this
blowing-up is two, therefore @xmath in this example. The blowing-up
leads to two integrals with disentangled (non-overlapping)
singularities, compare Eq. ( 87e ).

In the following, all three steps of the algorithm are discussed in more
detail. When treating Feynman loop integrals, a decomposition into
primary sectors is beneficial and performed before the iterated
decomposition into sub-sectors. The description of the algorithm is
restricted to scalar multi-loop integrals for better readability, but
the extension to multi-loop tensor integrals is straightforward. The
discussion of the algorithm is based on Refs. [ 151 , 152 ] .

#### 11.1 Generation of primary sectors

A general loop integral in Feynman parametrization, compare Eq. ( 6 ),
contains a @xmath -distribution, which can be formulated in various
ways. To arrive at the simplest representation for a subsequent iterated
sector decomposition, the representation of the @xmath -distribution is
chosen such that a definite hierarchy is introduced after integration,
where one Feynman parameter @xmath out of @xmath is always larger than
the rest. To this end, the @xmath dimensional unit hypercube is split
into @xmath sectors. In each of these so called primary sectors, one
Feynman parameter @xmath is chosen to be larger than all others

  -- -------- -- ------
     @xmath      (90)
  -- -------- -- ------

where @xmath is the Heaviside step function with values

  -- -------- -- ------
     @xmath      (91)
  -- -------- -- ------

In the distribution sense, it is a generalized function defined as

  -- -------- -- ------
     @xmath      (92)
  -- -------- -- ------

where the derivative of @xmath with respect to @xmath gives the Dirac
@xmath -distribution. After the decomposition into primary sectors, the
integral @xmath is split into @xmath integrals @xmath with @xmath the
upper integration boundary of all integrals over @xmath , compare Eq. (
87c ). In the next step, all integrals are remapped to the unit
hypercube by using a blowing up transformation on the Feynman parameters

  -- -------- -- ------
     @xmath      (93)
  -- -------- -- ------

The homogeneity of the functions @xmath and @xmath lead to a scaling
behavior of @xmath and @xmath , where @xmath is the number of loops of
the diagram, see Sec. 6 . Taking into consideration all powers in @xmath
appearing in one sector

  -- -------- -------- -- ------
     @xmath   @xmath      (94)
              @xmath      (95)
              @xmath      (96)
  -- -------- -------- -- ------

an overall factor of @xmath remains. The integration of the @xmath
-distribution

  -- -------- -- ------
     @xmath      (97)
  -- -------- -- ------

then yields primary sectors of the type

  -- -------- -- ------
     @xmath      (98)
  -- -------- -- ------

where the @xmath are connected with the full integral @xmath by

  -- -------- -- ------
     @xmath      (99)
  -- -------- -- ------

#### 11.2 Iterated sector decomposition

In the one-loop case, the first Symanzik sub-sector polynomials @xmath
are already brought into the form @xmath after the decomposition into
primary sectors. This is different at higher loop order @xmath and also
for the second Symanzik sub-sector polynomials @xmath . An iterative
procedure allows for the successive disentanglement of all
singularities. It follows three steps which are performed until
completion.

At first, a minimal set of parameters @xmath is assigned which leads to
a vanishing of the primary sector functions @xmath and @xmath in the
limit of vanishing elements of @xmath . The success of the decomposition
is dependent on the choice of @xmath which is by no means unique.

Then, the defined @xmath -dimensional cube is split into sub-sectors

  -- -------- -- -------
     @xmath      (100)
  -- -------- -- -------

Next, the integration boundaries are transformed back to the unit cube
by applying a blowing up once more, leading to the following
transformation rules for the Feynman parameters

  -- -------- -- -------
     @xmath      (101)
  -- -------- -- -------

At least one of the functions @xmath and @xmath factorize in the
parameter @xmath with the exponent of @xmath or @xmath , respectively.
Taking the additional Jacobian factor of @xmath into account, exponents
of the type @xmath result for each integration parameter @xmath . @xmath
and @xmath are numbers independent of the regulator @xmath . The
resulting sub-sector integrals are of the form

  -- -------- -- -------
     @xmath      (102)
  -- -------- -- -------

The three steps are repeated creating further sub-sectors @xmath and
@xmath , until no further set @xmath can be found after @xmath
iterations which leads to a vanishing of the sub-sector functions. This
is the case when they contain a constant term in form of a @xmath in the
case of the first, and in form of a kinematic invariant in the case of
the second Symanzik polynomial

  -- -------- -------- -- -------
     @xmath   @xmath      (103)
     @xmath   @xmath      (104)
  -- -------- -------- -- -------

where @xmath and @xmath are polynomials in the Feynman parameters and
where kinematic invariants including masses are termed @xmath and @xmath
.
The singular behavior is now contained in the exponent @xmath and all
overlapping divergences are disentangled.

#### 11.3 Extraction of the poles

It is now possible to find subtraction terms to extract poles in a
Laurent series in the regulator @xmath . Each obtained sub-sector
integrand and all variables @xmath with exponents @xmath can be written
in the general form

  -- -------- -- -------
     @xmath      (105)
  -- -------- -- -------

where @xmath is a function of the decomposed sub-sector functions @xmath
and @xmath . If the Feynman parameter is of positive or vanishing
exponent, @xmath , the integration is finite in the regulator @xmath and
no subtraction is needed. In all other cases, the integration will lead
to a logarithmic pole for @xmath or a higher pole if @xmath and in the
limit of a vanishing Feynman parameter @xmath . To expand around the
pole, an expansion into a Taylor series around @xmath can be performed

  -- -------- -- -------
     @xmath      (106)
  -- -------- -- -------

where @xmath denotes the remainder term which does not contain any poles
in the parameter @xmath by construction and where

  -- -- -- -------
           (107)
  -- -- -- -------

Reinserting Eq. ( 106 ) into Eq. ( 105 ) the only terms depending on the
variable @xmath are powers of it and the remainder polynomial @xmath .
Expanding the whole integrand into plus-distributions using the identity

  -- -------- -- -------
     @xmath      (108)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (109)
  -- -------- -- -------

the integration over @xmath can be performed straightforwardly for the
first term on the righthand side of Eq. ( 106 ), resulting with only the
integration left in the finite remainder term

  -- -------- -- -------
     @xmath      (110)
  -- -------- -- -------

In the case of a logarithmic divergence, the sub-sector integrand with
poles subtracted in the variable @xmath would read

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      
              @xmath      (111)
  -- -------- -------- -- -------

#### 11.4 Calculation of the pole coefficients

After repetition of these subtraction steps for all variables @xmath and
all obtained sub-sectors, nested sums result where each summand can be
dependent on the regulator @xmath . The whole expression can be expanded
in @xmath yielding a Laurent series with coefficients @xmath for each of
the @xmath sub-sector integrals of the @xmath -th primary sector

  -- -------- -- -------
     @xmath      (112)
  -- -------- -- -------

which again enter the full result for a (scalar) loop diagram as

  -- -------- -- -------
     @xmath      (113)
  -- -------- -- -------

### 12 The choice of algorithm

#### 12.1 Goals

The best suited sector decomposition algorithm may differ in view of the
two aspects, applicability and simplicity of the result. One algorithm
may be applicable to every multi-loop diagram, but result in very
complicated expressions. Another one may lead to relatively simple
expressions, but is not guaranteed to stop.

A sector decomposition algorithm does not stop, as soon as it runs into
an infinite recursion. The appearance of such can be exemplified
assuming the following function

  -- -------- -- -------
     @xmath      (114)
  -- -------- -- -------

When decomposing it first in the variables @xmath and @xmath , two
sub-sectors with opposite hierarchy ³ ³ 3 For an equal sign in Eqs. (
115 )-( 118 ), the Jacobian determinants need to be considered as well.

  -- -------- -------- -- -------
     @xmath   @xmath      (115)
     @xmath   @xmath      (116)
  -- -------- -------- -- -------

are created by rescaling the Feynman parameter @xmath in the first
sub-sector of function @xmath , and @xmath in the second sub-sector.
Choosing the sub-sector @xmath and the set @xmath of Feynman parameters,
the initial functional results

  -- -------- -------- -- -------
     @xmath   @xmath      (117)
     @xmath   @xmath      (118)
  -- -------- -------- -- -------

augmented by an additional factor of Feynman parameters. The set @xmath
would instead lead to a termination of the algorithm. If chosen in an
inconvenient way, decomposition sequences can complicate integrand
functions or even lead to an infinite recursion. The occurrence of the
latter limits the applicability. Manifestations of the former have a
direct influence on the numerical convergence.

The fewer decomposition steps are needed in an iterative algorithm, the
fewer sub-sectors are produced and the smaller the powers of factorized
integration parameters are.

#### 12.2 A heuristic algorithm for slim results

A first algorithm, which is also the one employed in the program SecDec
, is completely heuristic. First, the primary sector decomposition is
performed as described in Sec. 11.1 . Then, each individual primary
sector is iteratively decomposed into sub-sectors until both Symanzik
polynomials are finite for vanishing Feynman parameters. The procedure
works as follows:

-   Determine which of the two polynomials @xmath and @xmath of the
    primary sector @xmath turns zero in the limit of vanishing Feynman
    parameters. Find the best decomposition set @xmath for this
    function. If both polynomials nullify, find the best decomposition
    set for @xmath .

-   Compute all possible subsets of Feynman parameters contained in one
    primary sector.

-   Find the smallest set @xmath that nullifies the function detected in
    1). If two or more such sets have equal but minimal length, proceed
    with step 4), otherwise continue with step 5). The smallest set must
    contain more than one Feynman parameter, otherwise it would
    factorize from the polynomials @xmath and/or @xmath .

-   Discover which of the minimal sets maximizes the number of vanishing
    sub-sector polynomials. If there are several minimal sets leading to
    the same maximal number of vanishing polynomials, analyze the powers
    in the Feynman parameters of each nullifying set for the function
    detected in 1). Choose the set with lowest powers in the vanishing
    Feynman parameters.

-   Divide both, @xmath and @xmath into sub-sectors using the
    encountered best minimal set.

The procedure is iterated and more sub-sectors are produced as long as
there is a set that nullifies @xmath or/and @xmath .
This heuristic strategy is found to produce the least sectors compared
to strategies described separately by Bogner, Weinzierl and Smirnov,
Tentyukov, compare Ref. [ 284 ] and Ref. [ 285 ] , respectively. Yet,
the algorithm is not guaranteed to stop. The probability for running
into an infinite recursion as described in the previous section can be
reduced by introducing an additional heuristic strategy to the
algorithm. In the latter, a pre-decomposition is carried out for all
those Feynman parameters appearing quadratically or in higher powers in
the primary sectors. No selection of a subset of Feynman parameters is
performed. This treatment has proven to be very beneficial in many
cases, especially in the computation of two-loop integrals with massive
internal lines, but can lead to an increase in the number of produced
sectors if it is always carried out. Applied to inconveniently chosen
sectors of complicated integrals, this additional strategy can even
introduce higher spurious negative powers in the factorized Feynman
parameters.

#### 12.3 Algorithms guaranteed to stop

There are examples of three-loop diagrams which cannot be treated with
the heuristic algorithms described in the previous chapter due to the
occurrence of infinite recursion. To this end, it is interesting to find
algorithms which are guaranteed to stop, regardless of the numbers of
sectors produced.

The possibly simplest algorithm to that matter is the one introduced by
Hepp [ 144 ] , where @xmath sectors are produced due to the fact that
each sector is split in all Feynman variables @xmath , thereby always
choosing the maximal decomposition set. Although this strategy will
eventually terminate, the amount of sectors produced is by far too
large. The problem can be solved differently by formulating it in terms
of the polyhedra game introduced by Hironaka where the player A is
supposed to win over player B after a finite number of moves and
independent of the reaction of player B, see Ref. [ 286 ] . The relation
to sector decomposition was found by Bogner and Weinzierl who also
analyzed three strategies leading to the termination of the sector
decomposition algorithm, see Ref. [ 284 ] . The first strategy analyzed
there is based on work by Zeilinger [ 287 ] , the second on a strategy
by the mathematician Spivakovsky [ 288 ] , and the third strategy is
inspired by a proof of Encinas and Hauser, see Ref. [ 289 ] . The
strategies are all based on enforcing a sequence of decreasing
decomposition sets of Feynman parameters used for each step in the
iterated decomposition into sub-sectors. It was found, that the
heuristic strategy always wins over the terminating algorithms in terms
of the numbers of sectors produced, see Ref. [ 284 , 290 ] .
This situation does not change with the introduction of another strategy
@xmath found by Smirnov and Tentyukov, although it results with less
sectors than the previously mentioned terminating strategies, see Ref. [
285 ] . The strategy involves the computation of normal vectors to
facets of the convex hull of all weights, where the weights are found by
the exponents in the Feynman parameters of each monomial in the
sub-sector polynomial. If there is no facet which would lead to a, with
respect to the lexicographical ordering, smaller set of Feynman
parameters to be decomposed in the next step, the decomposition is
finished using strategy which is guaranteed to stop, e.g. the one based
on work by Zeilinger. It was also found that strategy @xmath produces
the same number of sectors as the strategy based on Speer sectors [ 291
] , which can process more information about the graph to be computed.
The introduction of Speer sectors leads to a higher efficiency in the
sector decomposition, regarding the speed and the memory intensity, see
Refs. [ 292 , 293 ] .

Having introduced all these strategies, it would be nice to have a
strategy which produces comparatively few sectors with regard to the
heuristic strategy and is guaranteed to terminate in a finite number of
steps. Such a strategy was introduced by Kaneko and Ueda [ 294 , 295 ] .
They take a deterministic approach and reformulate the primary sectors
using convex and combinatorial geometry. By the construction of
intersections of dual cones to convex polyhedral cones a unique
decomposition of the integration region can be found for each
polynomial. Some of the cones may still be too complicated for
integration, therefore they can be cut into simplices using
triangulation. The total number of sectors produced depends on the
triangulation algorithm. For the latter, there are many implementations
available in the literature, see e.g. Refs. [ 296 , 297 ] . Using the
first of the two, the resulting number of sectors is found to be even
smaller compared to the heuristic strategy and the algorithm is, by
construction, always guaranteed to stop, see Ref. [ 295 ] . The drawback
is that the resulting functions are more complicated compared to the
integrands resulting from the heuristic strategy. This is due to the
fact that it is not an iterative algorithm. While currently only the
heuristic strategy, augmented by the option of applying a
pre-decomposition, is implemented in the program SecDec , the algorithm
of Kaneko and Ueda will be included in the next improved version of the
program, see Sec. 21 .

## Chapter \thechapter Singularity structure of Feynman integrals

### 13 Euclidean vs. physical kinematics

In Sec. 6 it was already pointed out that both Symanzik polynomials are
of definite sign when computing integrals in the Euclidean region. This
implies that the energy component of the external momenta lies on the
imaginary axis, leading to negative values in the kinematic invariants
@xmath and @xmath and an overall positive contribution in the second
Symanzik polynomial @xmath . To verify this, compare e.g. with the
one-loop box example in Sec. 6 . Then, together with masses entering
with a positive sign, @xmath is positive semi-definite. After the
application of sector decomposition, all possible singularities
appearing in @xmath are factorized leading to only positive definite
integrands.

Switching to physical kinematics, the invariants formed from external
momenta can be real and four-momentum conservation

  -- -------- -- -------
     @xmath      (119)
  -- -------- -- -------

must hold, where @xmath is the number of external legs of which @xmath
are linearly independent. Due to this fact, @xmath is no longer definite
and further singularities, though integrable, can occur within the
integration region.

An intuitive example are production thresholds which appear as internal
particles go on-shell. This means that the overall incoming external
momentum reaches any sum of masses of internal propagators potentially
leading to physical final states. A simple example to demonstrate this
is a two-loop two-point function with three internal masses, see Fig. 5
. The three-particle-cut discontinuity occurs for

  -- -------- -- -------
     @xmath      (120)
  -- -------- -- -------

The analytical determination of the threshold locations gets more and
more complicated, the more external legs and propagators are involved.
Writing an integral in Feynman parametrization, thresholds may be
parametrized at the integrand level, as a combination of kinematic
invariants and Feynman parameters. The full set of thresholds can be
determined solving the Landau equations of an integrand.

### 14 Landau equations

The Landau equations

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (121a)
     @xmath   @xmath      (121b)
  -- -------- -------- -- --------

give the necessary (but not sufficient) conditions for a divergence, see
Refs. [ 257 , 298 , 299 ] . In accordance with the notation of Sec. 5 ,
the @xmath are linear combinations of external momenta @xmath and loop
momenta @xmath , @xmath is the number of propagators and @xmath the
number of loops. Paraphrasing Eqs. ( 121a ), either the propagator
@xmath or their respective Feynman parameter @xmath must vanish to
potentially contribute to a singularity. Only if Eqs. ( 121b ),
involving a derivative by the loop momenta, vanish simultaneously, the
conditions for a Landau singularity are fulfilled.

A solution to the system with @xmath gives the leading Landau
singularity, which is not integrable for @xmath when @xmath , and real
values of masses and momenta. Those singularities where the vanishing of
one Feynman parameter leads to a singular behavior are termed
sub-leading Landau singularities. These correspond to the thresholds of
a subgraph as a vanishing Feynman parameter can be associated with the
removal of one propagator and the junction of two vertices. These
singularities are integrable and of logarithmic or square-root type.

The Landau equations can be solved by contracting the momenta of Eq. (
121b ) with those loop and external momenta the equation depends on, to
get a system of equations which can be solved by using the constraints
arising from Eq. ( 121a ). A nice example analysis can be found in Ref.
[ 271 ] . Another example is shown in Sec. 23 .
The Landau equations can also be formulated as

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (122a)
     @xmath   @xmath      (122b)
  -- -------- -------- -- --------

after having integrated out the loop momenta, see Ref. [ 300 ] . The
leading Landau singularity is again given by the solution to the system
of equations assuming an empty set of vanishing Feynman parameters.
How we deal with these singularities will be described in the following
section.

### 15 Deformation of the integration contour

#### 15.1 Cauchy theorem

Unless the function @xmath is of definite sign for all possible values
of invariants and Feynman parameters, the denominator of a multi-loop
integral will vanish within the integration region on a hypersurface
given by the solutions of the Landau equations. To avoid the
non-physical poles on the real axis, the Cauchy theorem

  -- -------- -- -------
     @xmath      (123)
  -- -------- -- -------

can be exploited, where @xmath . To be able to use the theorem, the
original integrand, depending only on the real coordinates @xmath , is
analytically continued to the complex plane. The coordinate
transformation reads

  -- -------- -- -------
     @xmath      (124)
  -- -------- -- -------

where the new complex coordinates @xmath describe a path parametrized by
the variables @xmath . With a given description of the coordinates
@xmath , the Cauchy theorem in Eq. ( 123 ) can be formulated. It is
valid in this form as long as the deformation is in accordance with the
causal @xmath prescription of the Feynman propagators, as the region
enclosed by the integration contour then does not contain any singular
points, compare Fig. 6 . It is important to keep in mind, that no poles
should be crossed while changing the integration path, otherwise Eq. (
123 ) is no longer valid.
Finding the right analytical continuation to the coordinates @xmath is
equivalent to finding the proper deformation to the integration contour.
It is the crucial step for the success of this method and will be
treated in the following.

#### 15.2 Deformation

The aim is to find a clever deformation which is well suited for an
automated application in numerical calculations. For its realization, a
good parametrization of the complex variables @xmath in Eq. ( 124 ) must
be found which on the one hand preserves the causal @xmath prescription,
and on the other ensures all physical thresholds to appear as such in
the result. As the latter are contained in the Landau equations, an
inclusion of these in the deformation is desirable. It is therefore
required that all Landau equations, Eqs. ( 122 ), are realized when the
deformed function @xmath vanishes. Furthermore, the @xmath prescription
for the Feynman propagators requires that the contour deformation to the
complex plane is chosen such that the infinitesimal imaginary part is
conserved. The negative sign of the imaginary part of the second
Symanzik polynomial @xmath was discussed in Sec. 6 . For real masses and
Mandelstam invariants @xmath , the following Ansatz [ 279 , 281 , 280 ]
is therefore convenient

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (125)
  -- -------- -------- -- -------

where @xmath is an arbitrary real and positive parameter. A closed
integration contour is guaranteed by the factors @xmath and @xmath ,
keeping the endpoints fixed. From Eq. ( 125 ), the negative sign of the
imaginary part is only guaranteed if the derivative by @xmath is not
negative. Assuming the overall deformation to be small, the analytic
continuation of the integrand can be expanded into a series

  -- -------- -- -------
     @xmath      (126)
  -- -------- -- -------

where the expansion is done individually in each component @xmath . The
physically motivated requirement that all Landau equations, Eqs. ( 122
), be fulfilled is met in Eq. ( 126 ), when the deformed integrand
@xmath vanishes. Furthermore, the imaginary part of @xmath is always
negative due to an ever positive @xmath term. While the absolute size of
the derivative parts are determined by the diagrams to be computed,
@xmath is chosen to be a free parameter determining the scale of the
deformation. Following the analysis of Ref. [ 279 ] , the Ansatz for the
analytical continuation must guarantee a full cancellation of
singularities in subtraction terms present in the remainder term of
Eq. ( 110 ), see also Eq. ( 111 ). If the analytic continuation is done
only after computing the subtraction terms, one Feynman parameter is
deformed, while the one of the subtraction term is not. Assume the
deformation of a function @xmath depending on one Feynman parameter

  -- -------- -- -------
     @xmath      (127)
  -- -------- -- -------

Analytic continuation of the parameter @xmath yields

  -- -------- -- -------
     @xmath      (128)
  -- -------- -- -------

The subtraction term @xmath was set up to cancel the soft singularity in
the real part. This parameterization can introduce spurious poles in the
imaginary part, which are not taken care of in the limit of @xmath ,
unless @xmath vanishes faster than linear in the Feynman parameter
@xmath . If the deformation vanishes faster than linear in the Feynman
parameter @xmath , the imaginary part vanishes faster than the real
part, resulting in the original subtraction term. This condition is no
longer necessary, when the analytic continuation is done prior to the
construction of the subtraction terms. It is due to this analysis that
the analytic continuation of each Feynman parameter is done right after
the iterated sector decomposition procedure.

In summary, unless a kinematic point fulfills all Landau equations,
where both @xmath and its derivatives with respect to @xmath vanish, the
deformation of the integration contour leads to a well behaved integral
at the points where only the function @xmath vanishes.

An implementation and further analysis of this deformation for numerical
calculations has already been worked out in Refs. [ 274 , 275 ] . To
assure a high numerical stability of the evaluation of multi-loop
integrals, necessary to make the implementation publicly available,
supplementary studies of the deformation are necessary which are
presented in the following.

##### 15.2.1 Deformation studies

The aim of these deformation studies is to find an optimal procedure for
an optimal choice for the parameter @xmath which guarantees a good
behavior of the integrand. To this end, the terms of order @xmath ,
@xmath and @xmath are analyzed, assuming a decreasing effect in higher
orders, as is expected from a convergent Taylor series expansion.

The analytic continuation of @xmath to the third power in the
deformation reads

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (129)
              @xmath      (130)
  -- -------- -------- -- -------

which uncovers two non-trivial aspects of the deformation. One leads to
the fact that the term proportional to @xmath contributes to the real
part of @xmath and the other to an ambiguity in the sign of the
imaginary part.

To show the effect on the real part, it is descriptive to look at the
specific but simple example of the massive one-loop bubble, where the
leading Landau singularity is well known to be situated at @xmath when
@xmath . The function @xmath of the one-loop bubble reads

  -- -------- -- -------
     @xmath      (131)
  -- -------- -- -------

The real part of @xmath after the analytical continuation is shown in
Fig. 7 , where a point above threshold was chosen, with a mass @xmath
and @xmath and assuming arbitrary units. From its basic geometric
properties, it is known that the derivative of @xmath in Eq. ( 125 ) is
smallest in the extrema and largest where the slope is maximal. Around
@xmath , the function @xmath is almost, but not exactly, vanishing. The
size of the deformation coming from the derivative of @xmath is shown
for @xmath . One can notice that choosing a rather small @xmath the
function @xmath never vanishes except at the endpoints of the
integration region @xmath , while for the cases of @xmath the function
additionally vanishes in four points. In principle, this should not be a
problem, as the imaginary part is not vanishing in any point beyond the
end-points, see Fig. 8 . But the larger the value for @xmath is chosen,
the closer the points where the real part is zero, get to the endpoints,
where also the imaginary part is small. This can easily lead to
numerical instabilities, so the parameter @xmath should not be chosen
too large.

It should be noted that a value for @xmath is still viable, as long as
the overall deformation is small. Otherwise, the series in the
deformation Eq. ( 126 ) is no longer converging, leading to a wrong sign
of the imaginary part.

After having settled that the deformation parameter should not be chosen
too large, it must be found that it should neither be chosen to small,
see Fig. 9 . Though never striking zero, the modulus is extremely small
for @xmath , in particular in the vicinity of @xmath , which is bad for
the numerical convergence. A maximization of the modulus of the function
@xmath close to the critical points where it becomes minimal stabilizes
the numerical evaluation.

In the case of the one-loop bubble, the term of order @xmath in Eq. (
130 ) is zero. This is not the case for more complicated integrals. In
order for this term not to grow dominant and by that spoil the overall
minus sign of the imaginary part, either lambda must be chosen below one
or the terms proportional to the derivative must be @xmath . This task
can be accomplished by performing a small sampling of the derivative
terms for each Feynman parameter in various values and divide the
parameter @xmath by the maximally achieved value for the derivative

  -- -------- -- -------
     @xmath      (132)
  -- -------- -- -------

Then, the derivative parts are roughly normalized to one and the scale
of the deformation is again dominated by the value for the parameter
@xmath .

In order to further prevent the deformation to become too large,
valuable information on the right choice of @xmath can be extracted from
the analysis of the complex argument

  -- -------- -- -------
     @xmath      (133)
  -- -------- -- -------

where the numerator contains the coefficient of the imaginary part of
order @xmath , see Fig. 10 . Minimizing the complex argument @xmath
improves the numerical convergence in the whole integration region when
kinematically far from a critical point. When the imaginary part is
relatively small compared to the real part, the terms of order @xmath
contributing to the real part cannot become too large and those terms
going with @xmath cannot spoil the overall sign of the imaginary part.
This can be advantageous for speeding up a calculation and is especially
interesting in the case of highly fluctuating integrands. Close to a
threshold, this additional check is however not advisable because it
clashes with the maximization of the modulus of @xmath .

For more technical details about the implementation of the deformation,
see Sec. 17.3 .

##### 15.2.2 Pinch singularities

If a singularity falls together with an endpoint of the integration
region, it is trapped and no proper deformation of the integration
contour is possible. The same applies to the case where two
singularities fall together, where one singular point could only be
bypassed deforming the contour into the direction of the other
singularity. The result is a pinch in the integration contour. Both,
pinch singularities and singularities at the endpoint of the integration
region, are described by the Landau equations, compare Eqs. ( 122 ).

With the introduction of the analytical continuation to the complex
plane and the deformation of the integration contour, integrable
singularities do not appear as divergences in the coefficients of the
Laurent series in @xmath . Nevertheless, the method leads to numerical
instabilities in the vicinity of either a pinch singularity or a
singularity at the endpoint of the integration region. This is due to
the fact that the deformation of the integration contour becomes
negligible. Returning to the one-loop bubble of Eq. ( 131 ), this
behavior can be observed in Fig. 9 , where the modulus of the function
@xmath can become very small. The evaluation time and accuracy of an
integral close to such a singular point heavily depends on the chosen
value for @xmath and the numerical integrator.

## Chapter \thechapter Extension of the program SecDec to physical
kinematics

In the following, the public program SecDec version 2 [ 1 , 4 , 5 , 2 ,
6 , 7 ] is presented. SecDec is a program for the numerical evaluation
of multi-scale multi-loop and multi-dimensional polynomial parameter
integrals. It is based on the sector decomposition algorithm described
in Chap. id1 , where dimensionally regulated singularities are
extracted. Even though their coefficients are available in algebraic
form, they are usually too complicated to be integrated analytically.
Therefore the final computation of the coefficients to each order in the
regulator @xmath is done by Monte Carlo integration. To deal with
integrable singularities due to mass thresholds, the integration contour
is deformed to the complex plane. Before the extension to arbitrary
kinematics was achieved with SecDec version 2, a first version of the
program was publicly available [ 301 ] . Other public implementations of
the sector decomposition algorithm working in the Euclidean space are
introduced in Refs. [ 284 , 285 , 293 , 204 , 296 ] . Recently, a new
version of the program Fiesta has become available [ 207 ] , including
interesting and valuable new features. The extension to arbitrary
kinematics was also achieved there, though their approach is heavily
based on the one employed in SecDec , as mentioned in their publication.

The structure of this chapter is as follows: in Sec. 16 , the
functionality of the program SecDec version 2 is reviewed. Its
characteristic features are explained in Sec. 17 , further capabilities
are elaborated in Sec. 18 . The operational sequence of the program is
shown in Sec. 19 , before studying two examples and discussing the
computation times in Sec. 20 . Prospective future developments are
discussed in Sec. 21 .

### 16 Functionality

In this section, the functionality of the program SecDec is described.
The program has two main branches, one where the computation of any loop
integral or integral with a similar structure is possible. The user can
start from a diagram knowing the propagators involved, or can even feed
some of their own functions into the program. All other steps including
the output of the final result are performed in a fully automated way.
In the other branch, more general parametric functions can be computed,
including the special feature that additional finite functions can be
left symbolic until shortly before numerical integration.

Up to the step of the final integration, the coefficients are computed
in a fully analytical way, where all kinematic invariants are left
symbolic by default. This feature allows for a fast evaluation of
multiple kinematic points, as only the integration step is left to be
done if any of the kinematic invariants change. If a user is interested
in just the result for one particular diagram for one set of kinematics,
it is also possible to insert kinematic values in the beginning.

Version 2 of SecDec contains the following new features, which will be
described in detail in the next sections.

##### 16.0.1 Loop integrals and integrals of similar structure

-   Multi-scale loop integrals can be evaluated without restricting the
    kinematics to the Euclidean region. This has been achieved by
    performing a (numerical) contour integration in the complex plane.
    The program automatically tries to find an optimal deformation of
    the integration path. In addition, a kinematic threshold can be
    defined symbolically. Above this threshold, a complex contribution
    is expected and the deformation of the integration contour is
    automatically switched on.

-   For scalar multi-loop integrals, the integrand can be constructed
    from the topological cuts of the diagram. The user only has to
    provide the vertices and the propagator masses, but does not have to
    provide the momentum flow. ⁴ ⁴ 4 This new feature has been
    implemented in collaboration with J. Carter.

-   Tensor integrals can be evaluated with (in principle) no limitation
    on the rank. This means that a numerical approach in certain cases
    can help to alleviate or even avoid the procedure of amplitude
    reduction to master integrals. ¹ ¹ footnotemark: 1

-   Another new feature is the option to apply the sector decomposition
    algorithm and subsequent contour deformation on user-defined
    functions which do not necessarily have the form of standard loop
    integrals, but have a simliar structure.

-   The files for the numerical integration of functions amenable to
    contour deformation (multi-scale multi-loop integrals, user-defined
    functions) are written in C @xmath rather than Fortran. For
    integrations in Euclidean space, the user can choose between using
    Fortran or C @xmath .

-   A parallelization of the algebraic part for Mathematica versions 7
    and higher is possible if multiple cores are available. This is of
    special interest when computing very complicated multi-scale
    multi-loop integrals, see e.g. Chap. id1 .

-   A rescaling of all kinematic invariants by the absolute value of the
    largest invariant can be chosen to achieve a faster convergence of
    the numerical result.

-   Looping over ranges of parameter values is automated, allowing scans
    over different kinematic configurations within one topology. ¹ ¹
    footnotemark: 1

-   A stable and recent version of the Cuba library [ 302 , 303 ] , Cuba
    -3.2, allowing for a parallelized numerical integration is added to
    the program and used by default.

##### 16.0.2 General parametric integrals

-   The user can define additional (finite) functions at a symbolic
    level. These can be specified later, after the integrand has been
    transformed into a set of finite parameter integrals, for each order
    in @xmath . ¹ ¹ footnotemark: 1

-   Looping over ranges of parameter values is automated, allowing scans
    over parameter sets for more general polynomial functions. ¹ ¹
    footnotemark: 1

Below, these new features are described in more detail, but also see
Appendix B for a user manual. Comprehensive documentation can be found
with the code itself, available at http://secdec.hepforge.org .

### 17 Characteristic features

#### 17.1 Loop integrals

The program is capable of integrating general loop and multi-loop
diagrams, including kinematic thresholds, using Feynman parameters. In
accordance with Sec. 6 , such a loop integral is composed of five parts,
the two Symanzik polynomials @xmath and @xmath , the numerator, the
@xmath -distribution and the powers of factorizing Feynman parameters.
While the numerator in a scalar integral is equal to unity, it can
contain contractions of loop momenta with each other or with external
momenta when the integral is of higher rank. While any kinematic
invariant or scalar factor is treated as a constant in the numerator,
loop momenta appearing as contractions in the numerator influence the
singularity structure of the integrand, compare Eq. ( 73 ).

Tensor integrals up to in principle arbitrary rank can be computed with
SecDec by evaluating the coefficient functions of the Lorentz decomposed
tensors. Take for example, the Lorentz decomposition of a one-loop
two-point (bubble) integral @xmath of rank @xmath with one external
momentum @xmath

  -- -------- -- -------
     @xmath      (134)
  -- -------- -- -------

where the coefficient function @xmath reads

  -- -------- -- -------
     @xmath      (135)
  -- -------- -- -------

with @xmath . In the case of @xmath , the coefficient function @xmath
can be computed with SecDec , thereby delivering a result for the whole
tensor integral.

The algorithms in SecDec are not restricted by any loop order, tensor
rank or the number of scales. Provided with the information on the
diagram to be computed SecDec calculates the Laurent series up to the
desired order in the regulator @xmath in a fully automated way. For the
iterated sector decomposition two heuristic strategies, described in
Sec. 12.2 , are available. A diagram is specified by its propagators,
loop momenta and irreducible numerators, and by the number of external
legs and their on-shell conditions. The on-shell conditions become of
special importance when external legs are light-like. This is the
minimal information needed. Yet, SecDec has several options allowing for
a more efficient evaluation tailored to specific integrals and/or a
customization of the output of the results. One of the features of
SecDec is that all kinematic invariants are left symbolic up to the
numerical integration. This allows for a fast evaluation of integrals of
the same topology for different sets of kinematic values. If only one
kinematic point is of interest, it can be beneficial to set the values
for the invariants already in the beginning to allow for an additional
simplification of the integrands prior to numerical integration. This
feature is included in SecDec as well, by abuse of the on-shell
conditions, see App. B.4 . Furthermore, SecDec allows for the choice of
the desired prefactor and the maximal order in the regulator @xmath to
be computed. Among further options regarding the numerical integration,
see Sec. 18.8 , SecDec arranges for a user-adjustment of the
contour-deformation parameters for calculations in the physical region,
see Sec. 17.3 . A removal of spurious divergences can be achieved using
integration by parts, see Sec. 18.4 .

#### 17.2 Parametric integrals

The program SecDec can also factorise singularities from parameter
integrals which are more general than those in multi-loop integrals. The
only restrictions are firstly that the integration domain should be a
unit hypercube, and secondly singularities should reside only at the
upper and/or lower integration boundary, i.e. at zero or one. Contour
deformation is not available for more general parametric functions,
because it requires the sign of the imaginary part to be known a priori
in order not to give a wrong result. Currently the singularities are
assumed to be regulated by non-integer powers of the integration
parameters, e.g. the @xmath of dimensional regularization, or some other
regulator. The general form of such integrals is

  -- -------- -- -------
     @xmath      (136)
  -- -------- -- -------

where @xmath are polynomial functions of the parameters @xmath , which
can also contain a set of symbolic constants @xmath . The user can leave
the parameters @xmath symbolic during the decomposition, assigning
values only for the numerical integration step. This way the
decomposition and subtraction steps do not have to be redone if the
values for the constants are changed. In Eq. ( 136 ), the indices @xmath
are of the form @xmath , with @xmath such that the integral is
convergent. Note that half integer powers are also possible.

#### 17.3 Implementation of Contour deformation

As explained in Sec. 15.1 , singularities on the real axis can be
avoided by a deformation of the integration contour to the complex
plane. The scale of the deformation is controlled by the parameter
@xmath defined in Eq. ( 125 ). The convergence of the numerical
integration can be improved significantly by choosing an “optimal” value
for @xmath . As analyzed in Sec. 15.2.1 , values of @xmath which are too
small lead to contours which are too close to the poles on the real axis
and therefore lead to bad convergence. Values of @xmath which are too
large can modify the real part of the function to an unacceptable
extent, and could even change the sign of the imaginary part if the
terms of order @xmath become larger than those terms linear in @xmath .
This would lead to a wrong result. Therefore a four-step procedure is
implemented in SecDec to optimize the value of @xmath . These are

-   Ratio check: To make sure that terms of order @xmath in Eq. ( 126 )
    do not spoil the sign of the imaginary part, the ratio of the terms
    linear and cubic in @xmath are evaluated for a quasi-randomly chosen
    set of samples to determine @xmath . The size of the set can be
    chosen by the user.

-   Modulus check: The imaginary part is vital at the points where the
    real part of @xmath is vanishing. In these regions, the deformation
    should be large enough to avoid large numerical fluctuations due to
    a highly peaked integrand. Therefore the modulus of each sub-sector
    function @xmath is checked at a number of sample points. At the
    points where the modulus is close to vanishing, the fraction of the
    value @xmath is picked which maximizes the modulus of @xmath .
    Hereby, the value of @xmath which keeps @xmath furthest from zero is
    chosen.

-   Individual @xmath adjustments: If the discrepancy of the values of
    @xmath for different @xmath is very large among one sub-sector
    @xmath , it can be convenient to have an individual parameter @xmath
    for each sub-sector function @xmath and each Feynman parameter
    @xmath . As was shown in Sec. 15.2.1 , it is beneficial to have
    small overall deformations of the integration contour. Therefore
    each individual parameter @xmath is divided by the largest value of
    @xmath for all @xmath in one sub-sector @xmath , decreasing the
    overall size of the deformation. If the largest deformations is
    smaller or equal to one, the @xmath are left unchanged.

-   Further optional @xmath adjustments:

    -   If the integrand is expected to be oscillatory and hence
        sensitive to small changes in the deformation parameter @xmath ,
        SecDec can minimize the argument of each sub-sector function
        @xmath by varying @xmath . The effect is shown in Sec. 15.2.1 .

    -   If the integrand is expected to have (integrable) singularities
        close to the endpoints of the integration ( @xmath ), the
        deformation should be as large as possible in order to move the
        contour away from the problematic region. To this end, each
        individual parameter @xmath is multiplied by the largest value
        of @xmath for all @xmath in one sub-sector @xmath .

-   Sign check: After the above adjustments to @xmath have been made,
    the sign of Im @xmath is again checked for a number of sample
    points. If the sign is ever positive, this value of @xmath is
    disallowed.

The contour deformation can be switched on or off, see App. B.3 . The
calculation takes longer if a deformation of the integration contour is
performed, so if the integrand is known to be positive definite, the
contour deformation option should be switched off. It must also be
emphasized that for integrands with a complicated singularity structure,
the success of the numerical integration can critically depend on the
parameters which tune the deformation and on the settings for the Monte
Carlo integration.

### 18 Additional capabilities

#### 18.1 Evaluation of user-defined functions with arbitrary kinematics

To calculate a “standard” loop integral, it is sufficient to specify the
numerator, the loop momenta and the propagators. The program will
construct the integrand in terms of Feynman parameters automatically. It
can also be desirable to take a mixed approach of computing an integral
numerically after having manipulated it analytically. An example
approach is given in Chap. id1 , where the numerical efficiency is shown
to be improved by a clever analytical preparation of the integrand for
the subsequent Monte Carlo integration. In this example, the preparation
includes the analytical integration of one sub-loop. This implies that
the constraint @xmath has been already used to achieve a convenient
parametrization, and therefore no primary sector decomposition is needed
anymore to eliminate the @xmath -constraint. In such a case, the user
can skip this step in SecDec and insert the functions to be factorized
directly into the Mathematica template file, using the favored
parametrization. More generally, this option offers more flexibility
regarding the functions to be integrated, such as expressions for loop
integrals which are not in the “standard form”. For example, analytic
manipulations which have already been performed on the integral can be
dealt with as well. This includes the possibility to perform a
deformation of the integration contour to the complex plane.
To better understand the types of function a user could insert, the
reader is invited to look back to Eq. ( 73 ). A general loop integral in
Feynman parametrization thus contains a numerator function @xmath
non-divergent by construction, two Symanzik polynomials @xmath and
@xmath , whose exponent can have either sign and therefore singular
points. Furthermore, it contains a fully analytical, but arbitrary
prefactor @xmath allowed to contain singularities in the regulator
@xmath , factorizing powers of Feynman parameters, and a @xmath
-distribution constraint. A user-defined function, may contain any of
the previously mentioned components or none, with the only exception
that the @xmath -constraint needs to either not exist or have been
integrated out already. In a more general form, such a user-defined
integral @xmath may have any of the following components

  -- -------- -- -------
     @xmath      (137)
  -- -------- -- -------

The functions @xmath , @xmath and @xmath can be polynomials or products
of polynomials with the only condition that they share a common
exponent, @xmath and @xmath . The factorizing Feynman parameters can
appear with exponents @xmath dependent or independent of @xmath . Any of
the exponents may in principle also contain fractional numbers. Details
about the usage of this option are given in the user manual, see App.
B.6 .

#### 18.2 Topology-based construction of the integrand

As already mentioned in Sec. 6 , the functions @xmath and @xmath can be
constructed from the topology of the corresponding Feynman graph,
without the need to assign the momenta for each propagator explicitly.
The implementation in SecDec is such that the user only has to label the
external momenta, the vertices and the masses of a graph. An example is
given in Sec. 20 and more examples can be found in SecDec . This feature
of constructing the graph topologically is only implemented for scalar
integrals so far. The syntax is explained in App. B.5 .

#### 18.3 Looping over ranges of parameters

In phenomenological applications usually not just one kinematical point
is of interest, but looping over ranges of parameters becomes necessary.
To this end, it is beneficial to decrease the computation time where
possible. The algebraic part of SecDec can deal with symbolic
expressions for the kinematic invariants, or other parameters contained
in the integrand. Consequently, the decomposition and subtraction need
only be done once, producing functions which contain general kinematics.
The generality of these functions allows for the computation of many
sets of different values for the invariants. SecDec allows for an
automated calculation of many numerical points, minimizing the effort
for the user. Scripts are provided for “standard” loop, user-defined and
more general parametric integrals, see App. B.7 .

#### 18.4 Integration-by-parts relations

After the iterated sector decomposition has been performed, poles of the
type

  -- -------- -- -------
     @xmath      (138)
  -- -------- -- -------

can arise in an integral @xmath of the sub-sector @xmath , where @xmath
is a sub-sector integrand. An exponent @xmath is associated with a
spurious linear pole, powers @xmath correspond to spurious poles of
higher order. These terms must be artificial because a renormalizable
gauge theory must be integrable. The function @xmath denotes the residue
integrand after subtraction, compare Eq. ( 106 ). Choosing @xmath , it
would read

  -- -------- -- -------
     @xmath      (139)
  -- -------- -- -------

Even in the limit of a vanishing @xmath , the integrand will remain
integrable and finite assuming the decomposition into plus-distributions
has already been performed. While approaching @xmath , both the
numerator and the denominator will become very small. This can introduce
numerical instabilities resulting, e.g., from rounding errors. To
mitigate this, the sub-sector integrand @xmath can be integrated by
parts

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (140)
     @xmath   @xmath      
              @xmath      (141)
  -- -------- -------- -- -------

thereby reducing the negative power in the Feynman parameter @xmath by
one and enhancing numerical stability, see Ref. [ 304 ] for a more
detailed description of the implementation. This procedure is automated
for arbitrary pole order.

#### 18.5 Leaving functions implicit during the algebraic part

When evaluating general parametric functions, the user may wish to
introduce a “dummy” function depending on (some of) the integration
parameters, specifying the actual form of the function later at the
numerical integration stage. There are a number of reasons why one might
want to leave functions implicit during the algebraic stage: for
example, squared matrix elements typically contain large but finite
functions of the phase space variables in the numerator, so the
algebraic part of the calculation will be quicker and produce much
smaller intermediate files if these functions are left implicit. Also,
one might like to use a number of measurement functions and be able to
specify or change them without having to perform the decomposition more
than once. Note that one may use more than one implicit function at a
time, and that these functions can have any number of arguments. The
syntax and usage are described in App. B.8 .

#### 18.6 Assessing the reliability of the numerical result

When dealing with numerical techniques, the knowledge of the reliability
of the result is of major importance. Although the integrands of all
sectors are stored analytically, it may, especially when dealing with
complicated integrals, be time consuming to analyze these, either due to
the abundance of Feynman parameters appearing in one function, or simply
due to a large number of functions. And even then, the numerical
integrator may still appear to be a black box. It is therefore appealing
to get an estimate for the correctness of the stated uncertainty. The
numerical integrators contained in the Cuba library [ 302 , 303 ] return
a probability for an estimated numerical uncertainty to be erroneous. A
maximal probability of @xmath therefore means that the stated
uncertainty of a result cannot be trusted. The program collects the
maximal probability for each computed order in the dimensional regulator
@xmath . The probability can be reduced by increasing the number of
sampling points used in an evaluation. More options tuning the numerical
integration parameters are given in the user manual, see App. B . To
prevent a suffering from underestimation of the true error given by the
numerical integrators, it is beneficial to check a result with different
integrators when dealing with complicated integrands.

#### 18.7 Automated remapping to one endpoint

The program is capable of remapping singularities in a Feynman parameter
@xmath appearing at the endpoint @xmath of the integration region to
zero. A remapping of the singularity to zero becomes necessary, if one
of the sub-sectors of @xmath or @xmath in the case of loop integrals, or
one of the @xmath of Eq. ( 136 ) in the case of more general parametric
functions, diverge in the limit of one or more @xmath . It works as
follows. The integration region is split into two parts at the point
@xmath , where @xmath is chosen rather arbitrarily

  -- -------- -------- -- -------
     @xmath   @xmath      (142)
     @xmath   @xmath      (143)
  -- -------- -------- -- -------

From Eq. ( 142 ) to Eq. ( 143 ) the substitutions @xmath and @xmath are
applied to the first and the second integral of the right-hand side,
respectively. Hereby, a remapping of the integration boundaries to the
unit hypercube is achieved. The resulting functions @xmath either vanish
for @xmath or do not vanish at all.

In SecDec , the integration region of those integrals over Feynman
parameters @xmath leading to a divergence at the upper integration
boundary, is split at @xmath and the resulting two integrals are
remapped to the unit interval.
This splitting of the integration region is performed before the
iterated sector decomposition. It increases the number of primary
sectors by @xmath , where @xmath is the number of split integration
variables, in favor of improved numerical convergence. After all
singularities at the endpoint are remapped to the lower integration
boundary, an iterated sector decomposition can be applied. The
occurrence of singularities at both endpoints is typically encountered
in massless diagrams. In Sec. 22.1 , the integration of one sub-loop
prior to the treatment of the full integral serves as an example.

#### 18.8 A word on the numerical integration

The numerical integration forms a crucial part in the calculation of any
type of integrand function resulting from a Laurent series expansion in
@xmath . SecDec contains interfaces to six different numerical
integrators, Bases [ 305 ] , Vegas, Suave, Divonne and Cuhre contained
in the Cuba library [ 302 , 303 ] , and NIntegrate contained in
Mathematica [ 306 ] . The user is offered to choose one of these in the
input files, see App. B.3 . It is crucial to have the parameters for the
numerical integrator under control. SecDec incorporates several options,
allowing for a good adjustment of these parameters. Two of them are the
desired relative and absolute accuracy, where the desired absolute
accuracy is necessary for integrals close to zero. If the real or
imaginary part of the integral tends to zero, the relative accuracy can
never be reached. The numerical integrators therefore attempt to find an
estimate @xmath of the integral @xmath that fulfills

  -- -------- -- -------
     @xmath      (144)
  -- -------- -- -------

see e.g. Ref. [ 302 ] . For vanishing values of the integral, the
integration time then heavily depends on the value chosen for the
desired absolute accuracy. When looping over ranges of kinematic
invariants, points below threshold have a zero imaginary part. This can
artificially increase the computation times, if the absolute accuracy
goal was set to a reasonable, but small value. This artifact can be
circumvented by setting the lowest threshold symbolically when
specifying the integrand. This new feature is incorporated in SecDec ,
switching off the contour deformation below the user-defined threshold.
Hence, unnecessarily long calculations are avoided in kinematic regions
where the imaginary part is known to be zero.

The other selectable parameters are described in the manuals of the
different numerical integrators, Bases [ 305 ] and the Cuba library [
302 , 303 ] , and in the user manual in App. B . Regarding the
advantages of these integrators, Bases is a Fortran compatible Monte
Carlo integrator that allows for a sequential evaluation only. In the
sequential mode it is fast, producing reliable results. The four
integrators included in the Cuba library can run in parallel mode and
are usable with Fortran and C/C @xmath . While Vegas gives very stable
results and tends to overestimate the numerical integration uncertainty,
Divonne is extremely adaptive but occasionally underestimates
integration uncertainties. The latter is very useful for very
complicated integrands and is especially good in regions close to a
threshold. Suave is heuristically found to converge slowly, but gives
very stable results. While Vegas, Suave and Divonne are mainly Monte
Carlo integrators which can sample pseudo and quasi-random numbers,
Cuhre is a fully deterministic integrator, able to reach high accuracy
if the integrand is comparatively simple.

In the integration phase, SecDec allows for different choices regarding
the number of integrands to be summed before integration. Setting
togetherflag=0 and grouping=0 at the same time leads to the separate
integration of each sector in all different pole structures @xmath and
orders @xmath in the regulator @xmath . Here @xmath denotes the number
of logarithmic poles, @xmath the number of linear poles and @xmath the
number of higher poles in @xmath . The summation of some integrand files
before integration is enabled by entering the allowed summed size of the
grouped files in bytes, e.g. grouping=2000000 corresponds to a grouping
of integrands with a maximal total file size of around 2 MB. Switching
on togetherflag=1 , all integrands leading to the full coefficient of a
certain order in the regulator are first summed up and then integrated
numerically.
The uncertainty resulting from the Monte Carlo integration is expected
to be bigger when each sector is integrated individually before summing
up all sectors. Yet, the difference turns out not to be large, as the
numerical integrator can tackle single functions much better and yield
more accurate results. Grouping files before integration can also lead
to a faster convergence if the integrand contains highly oscillating
terms which cancel each other out, but will in general slow down the
numerical calculation if the polynomial structure is rather smooth.

### 19 Operational sequence

The program is divided into two main directories, loop and general ,
respectively. They have the same global structure, only individual files
are specific to either loop integrals and integrals of loop structure or
to more general parametric functions. The directories contain a number
of Perl scripts steering Mathematica [ 306 ] modules (located in the
subdirectory src ), writing the files necessary for numerical
integration and executing them. The scripts use further Perl modules
contained in the subdirectory perlsrc .

To calculate a loop integral within the standard or user-defined setup,
the user needs to enter the loop directory in SecDec , while for the
computation of parameter integrals the user is referred to the general
directory. When operating SecDec in either of the two directories, the
program works through algebraic and numerical parts. The algebraic part
uses Mathematica code and performs the sector decomposition, the
subtraction of the singularities, the expansion in @xmath and the
generation of the files necessary for the numerical integration. In the
numerical part, Fortran or C @xmath functions forming the coefficient of
each Laurent series term in @xmath are integrated using the Monte Carlo
integrators contained in the Cuba library [ 302 , 303 ] , Bases [ 305 ]
or NIntegrate [ 306 ] .

The flowchart of SecDec shows the basic building blocks to calculate
multi-loop or more general parametric integrals, see Fig. 11 . The
Mathematica source files are located in the subdirectories src/deco
(files used for the decomposition), src/subexp (files used for the pole
subtraction and expansion in @xmath ) and src/util (miscellaneous useful
functions). The Robodoc [ 307 ] documentation is located in the
subdirectory doc . It contains an index to look up a documentation of
the source code in html format by loading masterindex.html into a
browser. The program comes with example input and template files in the
subdirectories loop/demos and general/demos , respectively. Further
details on the installation and operation are found in App. B.1 and App.
B.2 .

### 20 Selection of checks and examples

In the following, two loop topologies are looked at in more detail. The
first example is a two-loop three-point function which serves as the
default graph when running SecDec without an alteration of the
myparamfile.input, mytemplatefile.m in the loop directory. The second
example is the two-point two-loop function with five massive
propagators. Different cases of numerators and numbers of mass scales
are discussed. Further highly non-trivial two-loop examples are
discussed in Chap. id1 . For a detailed description of examples in the
general directory, see the directory SecDec/general/demos and Refs. [
301 , 304 , 1 ] .

#### 20.1 A two-loop three-point function

In this example, three of the new features of SecDec version 2 are
demonstrated: the construction of the two Symanzik polynomials @xmath
and @xmath from the topology of the graph and the evaluation of the
graph in the physical region. Finally, it is shown how results for a
whole set of different numerical values for the kinematic invariants can
be produced and plotted in an automated way. The diagram under
discussion is a two-loop three-point function, see Fig. 12 . It has been
studied extensively adopting either an analytical [ 308 ] , or a
numerical [ 309 , 310 ] approach. The name for the diagram, @xmath , was
introduced in Ref. [ 308 ] .

The template file templateP126.m in the loop/demos subdirectory contains
the following lines
proplist =
{{ms[1],{3,4}},{ms[1],{4,5}},{ms[1],{5,3}},{0,{1,2}},{0,{1,4}},{0,{2,5}}}
;
onshell = {ssp[1] @xmath 0, ssp[2] @xmath 0, ssp[3] @xmath sp[1,2]};
where each entry in proplist corresponds to a propagator of the diagram;
the first entry is the mass of the propagator and the second entry
contains the labels of the two vertices which the propagator connects.
The labels for the vertices are as shown in Fig. 12 . For vertices
containing only internal propagators the labeling is arbitrary. The o
nshell conditions in the above example state that @xmath . Results for
the finite @xmath part of graph @xmath agree very accurately with the
analytic prediction, compare Fig. 13 .

The calculation time for the numerical integration of the finite part is
around 5 secs, for a relative accuracy of about 1% and an absolute
accuracy of @xmath using the integrator Divonne of the Cuba library .
This is for a typical point far from the @xmath threshold on an Intel ®
Core™ i7-2600 CPU at 3.40GHz using two cores. For a point close to
threshold ( @xmath ), the timings are similar. To run this example, from
the loop directory, issue the command ‘ ./launch -d demos -p
paramP126.input -t templateP126.m ’. The diagram @xmath can also be
computed using the user-defined setup, see App. B.6 for a detailed
explanation.

##### 20.1.1 Producing data files for sets of numerical values

To loop over a set of numerical values for the invariants @xmath and
@xmath once the C++ files are created, issue the command
‘ perl multinumericsloop.pl -d demos -p multiparamP126.input ’.
This will run the numerical integrations for the values of @xmath and
@xmath specified in the file demos/multiparamP126.input . The files
containing the results will be found in demos/2loop/P126 , and the files
p-2.gpdat, p-1.gpdat and p0.gpdat will contain the data files for each
point, corresponding to the coefficients of @xmath and @xmath ,
respectively. These files can be used to plot the numerical results
using gnuplot, see Fig. 13 for an exemplary result.
The same steps can be performed using the user-defined option, where the
command reads ‘ perl multinumericsuserdefined.pl -p
multiparamuserdefined.input ’ and where the values for @xmath and @xmath
are given in the multiparamuserdefined.input file.

#### 20.2 Massive tensor two-loop two-point functions

In this subsection the option to evaluate integrals with a non-trivial
numerator is demonstrated by calculating two-loop two-point functions
involving different mass scales. This implies that a reduction to only
scalar (master) integrals is not necessarily needed.

The exemplary diagram considered here contains up to four different
scales, see Fig. 14 . As an example, a non-trivial numerator a
coefficient function resulting from a rank three integral Lorentz
decomposition is chosen

  -- -------- -- -------
     @xmath      (145)
     @xmath      (146)
     @xmath      (147)
  -- -------- -- -------

where the causal @xmath prescription is omitted. The fact that this
tensor integral is reducible does not play a role here, as the purpose
is to demonstrate that a reduction may become obsolete, when considering
the short integration times for the tensors.

Results for the scalar and tensor integrals with @xmath are shown in
Fig. 15 , while results for @xmath are shown in Fig. 16 .

The timings are expected to be higher for the rank three coefficient
function because its leading pole is of order @xmath , while the scalar
integral is finite. In the case of finite integrals, less functions need
to be integrated, leading to a faster result. A comparison of the
timings of the scalar massive two-point integrals with the rank three
two-loop two-point coefficient function shows, that for values of @xmath
above the mass threshold at @xmath , the timings for the rank three
integral coefficient function do not differ much from the ones for the
scalar integrals, compare Fig. 17 for the @xmath case and Fig. 17 for
@xmath . A relative accuracy of 0.1% and an absolute accuracy of @xmath
was required for the Monte Carlo integration in scalar and rank 3
integrals alike. With the new feature of symbolically defining a
threshold below which the contour deformation is switched off,
unnecessarily long calculations are avoided in kinematic regions where
the imaginary part is known to be zero. In the case of the rank three
two-loop bubble with two masses, no threshold appears in the @xmath
coefficient, a threshold at @xmath appears in the sub-leading pole and
the lowest threshold of the finite part is located at @xmath . As only
the lowest threshold is incorporated in the user-definition, the
computing times of this integral are largest, where the imaginary part
in the pole coefficients is zero, compare Fig. 17 and Sec. 18.8 . The
minimal numerical integration time for a kinematic point below threshold
is @xmath ms for the scalar two-loop bubble with @xmath . Above
threshold, the scalar two-loop bubble integrals minimally take 0.1 secs
for @xmath and 0.03 secs for @xmath . In Fig. 17 , a small peak in the
timings can be observed for @xmath . When solving the Landau equations
for the two-loop two-point function with two masses, a sub-threshold can
be detected at exactly @xmath . Given this observation, it may sometimes
be of interest to have a closer look at the timings and possibly learn
something about the singularity structure of the integrand.

### 21 Future developments

The upgraded version 2 of the program SecDec was presented. The main new
feature, an implementation of an automated deformation of the
integration contour to analytically continue the integrand, was
described along with other new capabilities. It allows for the
computation of multi-loop integrals with in principle no limitation on
the number of scales involved. Examples to show its full power are
discussed in Chap. id1 , where numerical results for diverse two-loop
four-point topologies are shown.

Although comparatively fast, the numerical evaluation of multi-loop
diagrams can, in general, still not compete with the evaluation time of
analytical results. Including more analytical calculations would
therefore be very beneficial. A future version of the program could
therefore integrate functions analytically where possible. Especially
for the pole parts, the integration of several Feynman parameters prior
to numerical integration is feasible. This can lead to a significant
decrease in the numerical integration times, as became apparent in the
discussion of the timings in Sec. 20.2 .

Furthermore, a decrease in the numerical integration times can be
achieved using the fastest Monte Carlo integrator available for
one-dimensional parameter representations, Quadpack which is included in
the GNU scientific library. Currently, the integrators Vegas or Suave
are used in such cases, although they only rise to their full power when
multiple integration parameters are involved.

Another missing piece, as mentioned in Sec. 12.3 , is the implementation
of an algorithm which is guaranteed to stop. It should set in
automatically (if not already chosen in the beginning) when the
heuristic algorithm runs into an infinite recursion.

Although some of the mentioned new features are already implemented in a
private version of the code, they will be made publicly available as a
whole in version 3 of SecDec .

Besides new features accelerating SecDec , further technological
developments are needed, e.g., for kinematic configurations very close
to pinch singularities. An extensive treatment of line singularities
within the integration region or singularities appearing close to the
endpoints of the integration region is still to be found.

Beyond improvements on the computation of the multi-loop integrals,
interfaces to other programs are highly desirable. For example, the
construction of interfaces to existing amplitude reduction programs
would mean another cornerstone towards the long-term goal of assembling
a highly automated program for the computation of processes beyond one
loop.

## Chapter \thechapter Non-planar two-loop four-point integrals with
external or internal masses

In this chapter, the program SecDec is used to make predictions for
two-loop four-point integrals, with massive external legs and/or massive
internal propagators, see Fig. 18 . The diagrams considered are
non-planar with a single planar case.

The focus lies on the non-planar diagrams as they are usually more
complicated. In this way the power of the program SecDec can be explored
and its limits extended. Contrary to analytical calculations, where
several mass scales lead to unaccessible elliptic integrals, the limits
for numerical integration are rather reached when spurious higher than
logarithmic divergences occur. Such spurious divergences lead to more
complicated subtraction terms which in turn may lead to numerical
instabilities if the divergence canceling terms are not fully resolved
by the numerical integrator.

Such spurious divergences appear, for example, in the case of the
non-planar seven-propagator two-loop topology with a massive
bracket/square bend and all other lines massless, see Fig. 18(a) . To
achieve a convenient representation for this integral (termed @xmath ),
analytical transformations are performed prior to starting the sector
decomposition algorithm; this way reducing the number of produced
sub-sectors, leading to improved numerical behavior.

The @xmath diagram is the most complicated master topology occurring in
the computation of the light fermionic two-loop corrections of @xmath
production in the @xmath channel. Analytic results are available, see
Ref. [ 69 , 81 ] . Conversely, the master topology containing a massive
sub-loop (termed @xmath , see Fig. 18(b) ) is not available in analytic
form. It enters the heavy fermionic corrections to top-quark pair
production in the gluon fusion channel. While the leading pole of @xmath
is of the order @xmath , and intermediate expressions during sector
decomposition show (spurious) pole structures with a
higher-than-logarithmic degree of divergence, the integral @xmath has
only finite contributions. The same is true for the all-massive two-loop
diagrams @xmath and @xmath , where "P" refers to planar and "NP" to
non-planar, see Figs. 18(c) and 18(d) . To show the predictive power of
the program, the @xmath diagram is shown in a toy mass configuration
with thirteen mass scales, a configuration which is not even close to
being accessible in an analytic calculation. To compare with other
numerical predictions, the two all-massive diagrams are also shown with
three different scales involved. Lastly, a prediction for the non-planar
six-propagator diagram of Fig. 18(e) is made, studied in the massless
case by Tausk, see Ref. [ 201 ] .

The diagrams @xmath , @xmath , @xmath and @xmath are evaluated with
SecDec 2.1 in a fully automated way. Apart from scalar master integrals,
results for an irreducible tensor integral of rank two for the @xmath
diagram are given. In the case of the @xmath diagram it is advantageous
to make some analytical manipulations beforehand. The results, presented
in Ref. [ 2 ] were useful as checks in the analytic result presented in
Ref. [ 81 ] .

The structure of this chapter is as follows: in Sec. 22 , an expression
serving as a starting point for evaluating the @xmath diagram is
derived, and a novel type of transformation is described which can be
used to reduce the number of sector decompositions and improve numerical
stability. To check consistency, the expected thresholds are derived
from the Landau equations, see Sec. 23 . In Sec. 24 , numerical results
are given for all four-point diagrams previously described, where the
dependence of the evaluation on the number of mass scales, the
singularity structure and integrals of higher rank are explored.

### 22 Analytical preparation of the non-planar seven propagator
integral @xmath

This section explores the possibilities arising from a mixed approach,
where reformulating the integral before sector decomposition algorithm
can lead to a large gain in numerical efficiency. The @xmath diagram has
four poles in the regulator @xmath and two spurious linear divergences
in the Feynman parameters of the type @xmath . The available numerical
integrators can deal with logarithmic divergences efficiently, while the
convergence of higher-than-logarithmic singularities depends heavily on
the kinematics and the number of Feynman parameters involved.
After integrating out the @xmath -constraint, the sub-sector functions
of the @xmath diagram contain six Feynman parameters and subtraction
terms from two spurious linear divergences. The numerical integrator
fails convergence in this case. Decreasing the number of Feynman
parameters or the degree of divergence would therefore be beneficial.
Methods for the latter are already implemented by means of integration
by parts, where the exponent of a factorized pole is increased. Though a
solution for many integrand topologies, the tradeoff in the case of
@xmath are long decomposition times (approximately one week on a
multi-core, 16 GB computer), resulting in an order of @xmath functions
to be integrated. Integrating so many expressions leads to computational
problems: either large cancellations when terms are integrated in
isolation, or memory/convergence issues when terms are grouped by
summation prior to integration.

Below, a different approach is presented, reducing the number of Feynman
parameters, the degree of divergence and the total number of functions
to be integrated numerically. It is based on Ref. [ 2 ] . Firstly, a
representation where one integration parameter of the @xmath diagram
factorizes naturally is derived, such that it can be integrated out
analytically. A subsequent remapping and the application of the recently
introduced backwards transformation [ 2 ] to single sectors allows for
an automated evaluation in arbitrary kinematics. The new feature
allowing for a treatment of user-defined functions in the program SecDec
was developed for this particular computation and is explained in Sec.
18.1 . It was tested, that an important byproduct of the backwards
transformation is the reduction in the total number of functions by two
thirds to be integrated numerically, compared to the initial sector
decomposition approach and without usage of integration by parts
relations.

#### 22.1 Integration in a sub-loop

The expression for the scalar integral @xmath in momentum space is given
by

  -- -------- -- -------
     @xmath      (148)
  -- -------- -- -------

where @xmath . The Feynman propagators @xmath corresponding to the
“rhombus" sub-loop in Fig. 18(a) are given by

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (149a)
     @xmath   @xmath      (149b)
  -- -------- -------- -- --------

where the @xmath are the external momenta with @xmath and @xmath , and
@xmath , @xmath are the loop momenta. All external momenta are assumed
to be ingoing. Integrating out the loop momentum @xmath first, we are
left with an expression containing only @xmath and external momenta, to
be combined with the propagators

  -- -- -- -------
           (150)
  -- -- -- -------

This procedure is not limited to our particular example, but requires an
analytical step of introducing a convenient parametrization which can
not be found when the sub-loop contains massive propagators. For the
rest of this chapter, the causal @xmath will be omitted and the
renormalization scale is set to @xmath for simplicity.

The introduction of Feynman parameters for the one-loop subgraph @xmath
constructed from all propagators containing the loop momentum @xmath
leads to

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (151)
  -- -------- -------- -- -------

with the second Symanzik polynomial reading

  -- -------- -- -------
     @xmath      (152)
  -- -------- -- -------

During integration of the @xmath -distribution in Eq. ( 151 ), the first
Symanzik polynomial reduces to unity. The substitutions

  -- -------- -- -------
     @xmath      (153)
  -- -------- -- -------

facilitate a factorization of the parameter @xmath which is integrated
out analytically. This yields

  -- -------- -------- -- -------
     @xmath   @xmath      (154)
  -- -------- -------- -- -------

with

  -- -------- -- -------
     @xmath      (155)
  -- -------- -- -------

where the shorthand notation @xmath is introduced. The expression for
the 1-loop rhombus @xmath is combined with the remaining @xmath
-dependent propagators, treating the expression of Eq. ( 155 ) as a
fourth propagator with power @xmath , to obtain,

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (156)
  -- -------- -------- -- -------

after integration of @xmath and where

  -- -------- -------- -- -------
     @xmath   @xmath      (157)
     @xmath   @xmath      (158)
  -- -------- -------- -- -------

with

  -- -------- -------- -- -------
     @xmath               
     @xmath   @xmath      (159)
  -- -------- -------- -- -------

The full integral @xmath is in total one Feynman parameter short. A
primary sector decomposition in the newly introduced Feynman parameters
@xmath is performed to obtain

  -- -------- -------- -- -------
     @xmath   @xmath      (160)
  -- -------- -------- -- -------

with

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (161a)
     @xmath   @xmath      (161b)
     @xmath   @xmath      (161c)
     @xmath   @xmath      (161d)
     @xmath   @xmath      (161e)
     @xmath   @xmath      (161f)
     @xmath   @xmath      (161g)
     @xmath   @xmath      (161h)
  -- -------- -------- -- --------

where the @xmath -distribution is naturally integrated out.
Observing the primary sectors, the first sector @xmath is of the form
@xmath , and does not need iterations of the decomposition into further
sectors. Primary sector three can be remapped to primary sector two by
exchanging @xmath and @xmath . The sectors two and four are therefore
the only ones needing further treatment. This is a benefit from the
prior integration of one sub-loop of the full integral. This treatment
has a small drawback though: with the introduction of the substitutions
in Eq. ( 153 ), singularities at the second endpoint are introduced. The
integrals @xmath can diverge both at zero and one in @xmath and @xmath .
With the sector decomposition algorithm, only singularities at zero are
factorized automatically. Consequently, the singularities located at the
upper integration limit are remapped to the origin of parameter space by
splitting the integration region at @xmath and transforming the
integration variables to remap the integration domain to the unit
hypercube, see Sec. 18.7 .
This procedure results in 12 integrals, some of which are already
finite, such that no subsequent sector decomposition is required. Other
integrals lead to linear divergences of the type @xmath in two Feynman
parameters after sector decomposition. These singularities are spurious
and can be subtracted by expanding the Taylor series in the subtraction
procedure up to the second term, see Sec. 11.3 . This procedure is prone
to introducing large cancellations between subtraction terms and a large
number of sectors and therefore challenges numerical stability. Avoiding
this type of singularity from scratch is therefore a highly desirable
goal.
The next subsection describes a strategy which can help to reduce the
number of higher than logarithmic divergences and functions to be
integrated numerically.

#### 22.2 Backwards transformation

The aim of the procedure described in this section is to achieve a
transformation of potential linear divergences into logarithmic
divergences as far as possible. A different procedure towards this goal
based on integration-by-parts identities, has been described in Ref. [
301 ] and Sec. 18.4 . The latter method however can increase the number
of functions to be integrated substantially, while the method described
below in general reduces the number of further iterations and therefore
the number of produced functions. Yet another method to reduce the
number of functions produced during factorization has been suggested in
Ref. [ 311 ] , where a non-linear transformation in the Feynman
parameters aims at a reduction of the exponent of the second Symanzik
polynomial. Although an attractive idea, it was not beneficial in the
case of the @xmath as further singularities at the upper integration
limit are introduced with the transformation. A subsequent remapping of
the divergences at the endpoint one to zero restores, or even
deteriorates the original singularity structure with spurious linear
divergences in two Feynman parameters.

Due to the fact that the iterated decomposition of the integral into
sub-sectors introduces higher powers in the Feynman parameters, the
desire to undo some of those decomposition steps before the final
integration is rather natural. Yet, in most cases this is not possible
without the introduction of unacceptable new divergences which have to
be subtracted before integration. Although it does not seem beneficial
to “undo” single sector decomposition steps, there may still be a
transformation that does a similar trick. Such a transformation was
introduced and utilized by the author and collaborators in Ref. [ 2 ] .
It relies on the possibility of blowing down an affine @xmath
-dimensional space as opposed to the blowing up used in the sector
decomposition approach. After performing the blowing down, the splitting
of the integration region as performed in the sector decomposition
approach can be applied backwards. The original function is thereby
split into two again. It may at first seem counter-intuitive to achieve
a reduction in the number of sectors to be integrated, when a splitting
of one primary sector using the backwards transformation doubles the
number of functions. However, in spite of these arguments, it turns out
that when applied in the right way, a reverse splitting of the
integration region can rearrange the Feynman parameters, such that the
double linear divergences of primary sectors two and three are
transformed into logarithmic ones. This leads to an overall reduction in
the number of functions describing @xmath by two thirds.

Below, the necessary preconditions for the application of such a
backwards transformation and the transformation itself are analyzed in
more detail.

##### 22.2.1 Preconditions

To explain the backwards transformation in more detail, it is convenient
to recall the sector decomposition and highlighting the important
aspects necessary for the transformation.

In the sector decomposition algorithm, the integration region is split
into at least as many parts as there are integration variables. This
splitting is done such that a clear and definite hierarchy can be
observed among the integration parameters. As recollection, the example
splitting of Sec. 11 reads

  -- -- -------- -- -------
        @xmath      
        @xmath      (162)
  -- -- -------- -- -------

where it is evident that @xmath in the first summand on the righthand
side must always be bigger than @xmath , otherwise the integral is zero.
In the second summand of Eq. ( 162 ) the hierarchy between @xmath and
@xmath is reversed. Both integrals in the second line of Eq. ( 162 ) are
of definite hierarchy. In the sector decomposition example of Eqs. ( 87
), a blowing up is applied to these two functions, leading to a
factorization of the previously overlapping singularities. The hierarchy
among the Feynman parameters is implicitly kept. Assuming one of the
resulting functions with non-overlapping singularities to be the
starting point of the backwards transformation, the sector decomposition
of Eqs. ( 87 ) can be reversed. It reads

  -- -- -------- -- --------
                    
        @xmath      (163a)
        @xmath      (163b)
        @xmath      
        @xmath      (163c)
        @xmath      (163d)
  -- -- -------- -- --------

Although the equations are just rearranged, the implications are
different. While the factorization of singularities works due to the
application of blowup sequences, here the prerequisite is the
possibility of applying a blowing down. Looking at the transition from
Eq. ( 163a ) to Eq. ( 163b ) more thoroughly, one finds

  -- -------- -- --------
                 
     @xmath      (164a)
     @xmath      (164b)
     @xmath      (164c)
     @xmath      (164d)
  -- -------- -- --------

From Eq. ( 164b ) to Eq. ( 164c ) the integration parameters @xmath and
@xmath are transformed as

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (165a)
     @xmath   @xmath      (165b)
  -- -------- -------- -- --------

Having examined a symmetric and simple example, it should be noted that
it is not trivial to a priori know that an equality of the type between
Eqs. ( 164a ) and ( 164b ) indeed holds. In general polynomial
integrals, the implicit hierarchies among the Feynman parameters must be
made explicit through the introduction of Heaviside @xmath functions.

In the following, a realistic example is presented, uncovering the
advantages of such a backwards transformation.

##### 22.2.2 Application to the @xmath

Returning to the non-planar double box integral @xmath , the following
structure can be identified for the function of Eq. ( 161d ) in sector
two (and three) after remapping

  -- -------- -- -------
     @xmath      (166)
  -- -------- -- -------

where @xmath , @xmath and @xmath and @xmath . @xmath , @xmath and @xmath
are polynomials of arbitrary degree in the Feynman parameters @xmath and
kinematic invariants. The carets denote those Feynman parameters which
are not part of the vector of Feynman parameters. This definition of a
vector of Feynman parameters with double index @xmath will be used
throughout this subsection.

In Eq. ( 166 ), all terms multiplied by the Feynman parameter @xmath are
also multiplied by the Feynman parameter @xmath . An explicit hierarchy
exists, allowing for a transformation of the type @xmath . The splitting
of the integration region can be performed backwards as

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (167a)
     @xmath   @xmath      (167b)
  -- -------- -------- -- --------

To explain this in more detail, a rearrangement of the terms leads to
the well-known sector decomposition

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (168a)
     @xmath   @xmath      (168b)
     @xmath   @xmath      (168c)
  -- -------- -------- -- --------

where @xmath was substituted in sector (1) and @xmath in sector (2).

The effect of the backwards transformation is twofold: The degree of the
polynomial in @xmath is reduced in Eq. ( 167a ), and in Eq. ( 167b ) the
degree of divergence in @xmath is reduced if @xmath . It can be
beneficial in the reduction of the number of functions as compared to
the custom sector decomposition procedure, and can be particularly
advantageous if the factor @xmath is much simpler than @xmath .

After all transformations of this type, the result is a total of 15
functions partly needing an iterated sector decomposition. Together with
the introduction of the new feature of user-defined functions in SecDec
2.1 described in Sec. 18.1 , the computation of the @xmath diagram is
now possible in a reasonable amount of time. The timings are discussed
in Sec. 24.1 .

### 23 Expected thresholds from the Landau equations

Before turning to the numerical evaluation of the @xmath diagram and
diverse other non-planar double box integrals, it should be analyzed
roughly where the thresholds of the @xmath diagram are expected, to have
a measure for the trustworthiness of the result. To this end, it is
useful to analyze the Landau equations as described in Sec. 14 ,
although the resulting singularities do not necessarily lead to a
divergence in the integral. The first set of equations resulting from
applying Eq. ( 121a ) to @xmath reads

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (169a)
     @xmath   @xmath      (169b)
     @xmath   @xmath      (169c)
     @xmath   @xmath      (169d)
     @xmath   @xmath      (169e)
     @xmath   @xmath      (169f)
     @xmath   @xmath      (169g)
  -- -------- -------- -- --------

where the @xmath are again the loop momenta, @xmath are the external
momenta and @xmath is the mass appearing in the propagators of the
@xmath diagram. The second set of equations resulting from Eq. ( 121b )
is

  -- -------- -------- -- --------
                          
     @xmath   @xmath      
              @xmath      (170a)
     @xmath   @xmath      
              @xmath      (170b)
  -- -------- -------- -- --------

Now, Eqs. ( 169a )-( 169g ) are either true when the @xmath are
vanishing or when the scalar products composed of the loop momenta
@xmath and external momenta @xmath are either equal to a mass, see Eq. (
169g ) or vanish altogether. Now, if one Feynman parameter @xmath is
zero, the graph to be considered is a subgraph of the original one
because the propagator connecting two vertices was removed. To analyze
the singularities of the original graph it is therefore sufficient to
assume @xmath . This means, Eqs. ( 169a )-( 169g ) all serve as
constraints on the scalar products to appear in Eq. ( 170a ) and Eq. (
170b ) when contracting them with the loop @xmath and respectively
external momentum vectors @xmath . Some of these are used to express the
missing scalar products in terms of other scalar products. The resulting
equations containing the Landau singularities for those sets of
kinematic invariants and Feynman parameters for which the equations
hold, then read

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (171a)
     @xmath   @xmath      (171b)
                          
     @xmath   @xmath      (171c)
     @xmath   @xmath      (171d)
     @xmath   @xmath      (171e)
     @xmath   @xmath      
              @xmath      
              @xmath      (171f)
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      (171g)
     @xmath   @xmath      
              @xmath      
              @xmath      (171h)
  -- -------- -------- -- --------

Still assuming all Feynman parameters @xmath , a leading Landau
singularity cannot be detected in the above equations. The graph is
regulated by its mass parameter. Yet, there are many sub-leading Landau
singularities. These are given when one or more of the above displayed
kinematic conditions are fulfilled. One such Landau singularity appears
at @xmath , when the Feynman parameters @xmath with @xmath vanish
simultaneously. Another one appears at @xmath when @xmath for @xmath .

### 24 Numerical evaluation

In this section, numerical results for several two-loop four-point
functions are presented, in the same order as is shown in Fig. 18 . The
results for the analytically prepared @xmath diagram are shown first.
All other results are obtained using the custom SecDec setup.

#### 24.1 The @xmath diagram

Analytic results for the pole coefficients of the @xmath and @xmath part
of the diagram @xmath shown in Fig. 18(a) have been provided in Ref. [
312 ] . Using the mixed analytic and numerical approach presented in the
work summarized in this thesis, the results of the purely analytical
pole predictions can be numerically confirmed, see Fig. 19 . The
numerical results for the remaining pole coefficients and the finite
part of @xmath are shown in Figs. 20 and 21 . For Figs. 19 - 21 , an
overall factor of @xmath is extracted.

Two Landau singularities can be observed, one is at @xmath and the other
at @xmath . All other Landau singularities do not appear in the plot, as
the values for @xmath and @xmath are kept fixed.

The numerical result presented here was compared to the fully analytical
result in two selected phase space points [ 313 ] for all Laurent
coefficients up to the finite part, finding agreement within the
numerical precision. The full prediction obtained from the numerical
result of Figs. 19 - 21 were useful as a cross-check in Ref. [ 81 ] .
The timings for the leading and next-to-leading pole coefficients of the
diagram @xmath range between fractions of a second and around 20
seconds. The coefficients of the @xmath pole take 13 - 300 seconds,
while the coefficients of the @xmath pole take between 75 seconds and 50
minutes, depending on their distance to thresholds. For the finite part,
the integration times range from 250 seconds to 67 minutes. For all
Laurent coefficients, a relative accuracy of @xmath has been stipulated,
which was not always reached for the @xmath and @xmath coefficients. It
should also be noted that the timings for points close to threshold are
rather sensitive to the Monte Carlo integration parameters.

#### 24.2 The @xmath diagram

Numerical results for the diagram @xmath with two massive external legs
are shown for the scalar integral and an irreducible rank two tensor
integral, compare Fig. 18(b) for the corresponding diagram and Fig. 22
for the numerical results.

The integral representation of the diagram @xmath is given by

  -------------------------------------------- -------- -- --------
                                                           
                                               @xmath      (172a)
  with the corresponding Feynman propagators               
                                                           
                                               @xmath      (172c)
                                               @xmath      (172d)
                                               @xmath      (172e)
  -------------------------------------------- -------- -- --------

where the infinitesimal @xmath is omitted for brevity, and where the
convention of all external momenta being ingoing was used. The dimension
is denoted by @xmath . Two external legs @xmath and @xmath are massive
and equal, @xmath . For the results shown in Fig. 22 , the numerical
values @xmath were used. In Fig. 22 , the two masses are set to @xmath ,
as this is the topology appearing in the process @xmath at NNLO if the
@xmath -quarks are assumed to be massless. While numerical results for
the scalar integral are shown in Fig. 22 , LABEL: corresponds to a rank
two tensor integral with the same propagators as LABEL: , with a scalar
product of loop momenta @xmath , in the numerator.

The timings for one kinematic point for the scalar integral in Fig. 22
range from 11-60 secs for points far from threshold to @xmath minutes
that are very close. In the vicinity of the threshold, the average is
around 500 secs. A relative accuracy of @xmath has been specified for
terminating the numerical integration, while the absolute accuracy has
been set to @xmath . For the tensor integral, the timings are better
than for the scalar case, as the numerator present in this case smoothes
out the integrand. A phase-space point far from threshold takes around
5-10 secs, while points very close to threshold do not exceed 1 hour for
the rank 2 tensor integral. The results were obtained on a single 8 core
Intel i7 machine.

Numerical results with @xmath are shown in Fig. 23 to demonstrate that
adding another mass scale is extremely straightforward with our
approach, whereas analytical calculations would suffer from enormous
additional complications.

#### 24.3 Planar seven-propagator all massive graph @xmath

To demonstrate the possibility to compute diagrams with arbitrarily many
scales, a planar seven-propagator toy graph involving the maximal amount
of 13 independent scales is computed, compare Fig. 18(c) . The diagram
has no poles in the regulator @xmath . All propagators are assumed to
have different masses, all external legs are chosen to be massive as
well. While keeping @xmath fixed, numerical values of the finite part of
the @xmath diagram are shown, see Fig. 24 , where @xmath and @xmath are
varied using the physical constraint @xmath .

The timings for the numerical integration range between 10 and 180 secs
with a relative accuracy of @xmath or an absolute accuracy of @xmath if
the imaginary part is zero, see Sec. 18.8 for a discussion on the
relative vs absolute accuracy.
These results show that there is in principle no constraint on the
number of scales involved.

#### 24.4 Non-planar seven-propagator all massive graph @xmath

In this example, a seven-propagator non-planar two-loop box integral is
considered, where all propagators are massive, using @xmath , @xmath ,
@xmath . The labeling is as shown in Fig. 25 .

Numerical results for this integral were obtained by Fujimoto et al. [
314 ] , where solutions are found by extrapolation in the @xmath
parameter. For comparison, results produced with SecDec are shown for
the same mass configuration using @xmath , see Fig. 26 . They are in
agreement with Ref. [ 314 ] . The computation time for the longest
sub-function (for both real and imaginary parts) for a relative accuracy
of one per mil vary between about 20 secs for a point far from and about
500 secs close to the threshold.

#### 24.5 Non-planar six-propagator diagram

First, the non-planar six-propagator two-loop four-point diagram is
considered, compare Fig. 18(e) . For light-like legs and massless
propagators, the analytic result has been calculated, see Ref. [ 201 ] .
The name of the graph @xmath , is adopted from this reference.

In the following, two different mass configurations of the graph @xmath
are studied

-   Two external legs, @xmath and @xmath , are massive, all propagators
    are massless. The leading pole of this topology is of order @xmath .

-   All external legs are light-like, four propagators are massive with
    @xmath , the other two are massless. This topology contains poles
    starting from order @xmath .

For the topology with light-like legs as considered in Ref. [ 201 ] ,
the leading pole is of the order @xmath . The difference in the pole
structure is due to cancellations related to the high symmetry of the
graph. See Figs. 28 and 29 for numerical results of the finite parts of
@xmath and @xmath and Fig. 27 for the corresponding diagrams.

In accordance with Ref. [ 201 ] , an overall prefactor of @xmath has
been extracted in all numerical results. Also, for all the values given,
@xmath is determined by the physical constraint @xmath .
For Fig. 28 , the numerical value @xmath was adopted while scanning over
@xmath . The massive external legs are set to @xmath . In Fig. 29 ,
numerical values for the finite part of the @xmath diagram are shown,
where the mass scale is set to @xmath . The kinematic invariant @xmath
is varied, choosing @xmath .

### 25 Summary

In this chapter the evaluation of diverse non-trivial two-loop diagrams,
including planar and non-planar topologies with six or seven
propagators, was shown. It was found that their evaluation with the
upgraded version of the program SecDec is not restricted by the number
of involved scales. Complicated diagrams hardly or not accessible with
analytical techniques can easily be computed, proving the program SecDec
a powerful tool for checks, comparisons and predictions. Yet, an
evaluation can become difficult if the singularity structure is very
complicated such that spurious linear divergences occur. In the example
of the massive non-planar two-loop box @xmath which exhibits such an
extremely complicated singularity structure, it has proven beneficial to
do an analytical preparation of the integral prior to numerical
integration. A simplification of the functions to be integrated
numerically can be achieved by a reduction in the number of Feynman
parameters to integrate over numerically or in the removal of spurious
divergences. The former was achieved by integration of one Feynman
parameter in a sub-loop. Towards the latter a new type of
transformation, introduced by the author and collaborators and
summarized in this thesis, facilitated a trading of linear divergences
for logarithmic ones, thereby achieving a reduction by two thirds in the
total number of functions to be integrated. As was shown, the analytical
preparation leads to an overall improved numerical convergence.
Exploiting the newly developed feature of including user-defined
functions into the SecDec setup, an automated evaluation of the @xmath
diagram was possible. The examples demonstrated in this chapter may
serve as a guideline for the evaluation of very complicated integrals,
to become of importance in future phenomenological applications.

## Chapter \thechapter Neutral MSSM Higgs-boson spectrum at the two-loop
level

The momentum-dependent two-loop contributions to the neutral @xmath
-even MSSM Higgs-boson masses are computed at order @xmath . This
requires the calculation of two-loop self-energies with a proper
renormalization at the two-loop level. The calculation is performed
using the Feynman-diagrammatic approach. An effective potential
approach, though leading to compact expressions, does not allow for the
incorporation of momentum dependence.

All relevant two-loop self-energy diagrams and those one- and two-loop
diagrams contributing to the renormalization are generated using
FeynArts [ 183 , 184 , 315 ] . From a diagrammatic point of view, the
diagrams involved in the calculation including the momentum-dependence
remain the same with respect to the calculation at zero momentum
transfer. This is due to the fact that the diagrams are selected by
coupling factors. The sole difference is in the dependence of the
self-energy diagrams on the external momentum. Yet, this difference is
non-trivial because analytical expressions involve the evaluation of
elliptic integrals which can presently not be performed yielding fully
analytical results. As numerical evaluations are generally more
time-consuming, the number of diagrams involved needs to be reduced
considerably, to a minimal set of master integrals.

A reduction of tensor integrals and the evaluation of traces is
performed with the packages TwoCalc [ 316 ] and FormCalc [ 186 , 317 ,
318 ] . While the package TwoCalc condenses the two-loop amplitudes to
only scalar master topologies, FormCalc reduces all one-loop
counter-terms and counter-term insertions to a basis of scalar integrals
and a small number of tensor coefficients. In the reduction, the method
of partial fractioning is used where applicable. Also the cancelation of
denominators after taking the derivative with respect to a kinematic
invariant is exploited, in addition to the application of symmetry
relations. An important feature of TwoCalc is benefitting from the
extension of the idea of a tensor decomposition introduced by Passarino
and Veltman [ 319 ] as a reduction technique at one-loop. In TwoCalc ,
the latter is applied to a sub-loop of an integral before the remains of
the integral can be further decomposed and simplified with the before
mentioned techniques.

After studying the one- and two-loop counter-terms renormalizing the
masses as well as the fields, and assuring a consistent cancellation of
all divergences, the resulting finite terms are evaluated. Where
possible, analytic results are used, all other integral topologies are
computed numerically using the program SecDec . The resulting
self-energy corrections are added to the inverse propagator matrix, as
discussed in Sec. 3 , and the resulting mass shifts to the neutral
@xmath -even Higgs-boson masses are computed. The two-loop calculation
is performed in the @xmath - @xmath basis. The rotation into the
physical @xmath - @xmath basis according to Eq. ( 5 ) is performed
afterwards.

### 26 Dominant momentum-dependent two-loop QCD corrections

To compute the dominant momentum-dependent two-loop contributions to the
neutral MSSM Higgs-boson spectrum, only those self-energy diagrams with
couplings strictly of the order @xmath are taken into account. While
electro-weak gauge contributions are incorporated up to the one-loop
level, they are assumed negligible at the two-loop level. The
corrections involving squares in the top Yukawa coupling, Eq. ( 40 ),
dominate the electro-weak higher-order contributions, as @xmath .
Contributions involving couplings to gauge bosons are therefore expected
to be relatively small.

Although the top Yukawa coupling is much larger than the bottom Yukawa
coupling, @xmath , it might be argued that the scalar bottom (sbottom)
quark mass could be significantly larger than the bottom mass, when
supersymmetry is broken. This region of parameter space could then lead
to corrections of similar size with respect to the couplings
proportional to @xmath . However, the couplings of the squarks to the
Higgs-bosons are all proportional to Yukawa couplings, see Sec. 2 . They
are composed of the fully supersymmetric F-term and the
non-supersymmetric soft-breaking contributions, where fully
supersymmetric refers to the relation @xmath . The relevant
soft-breaking terms are proportional to the trilinear couplings @xmath
and the Standard Model Yukawa couplings. Assuming no inverse hierarchy
among the trilinear couplings, the up-type Yukawa terms dominate over
the down-type ones. Therefore, all bottom and sbottom contributions can
be assumed to be negligible.

### 27 Self-energy diagrams

Twelve different two-loop topologies contribute to the MSSM Higgs-boson
self-energy corrections at the order @xmath , see Fig. 30 . They match
Eq. ( 62 ) choosing the scalar fields @xmath and @xmath for @xmath .
Every vertex of Yukawa type involving a Higgs-boson and quarks or
squarks contains a square root factor of @xmath . All quark or squark
interactions with a gluon or gluino and the 4-squark vertices contribute
with a factor of @xmath . Products of these coupling factors lead to an
overall order of @xmath for each diagram. The squared one-particle
irreducible diagrams and reducible two-loop diagrams @xmath do not
contribute to the corrections because their amplitudes are not of the
order @xmath .

After performing a tensor reduction with TwoCalc , the resulting
amplitudes are given in terms of a few scalar master integrals. Those
involve factorizing one-loop tadpole @xmath , one-loop bubble @xmath and
two-loop bubble @xmath topologies, compare Fig. 31 .

While the resulting one-loop integrals can only become UV divergent, the
two-loop integrals can be ultraviolet and infrared divergent. As the
gluons are the only massless particles in this calculation and appear
singly, no infrared singularities arise in any of the two-loop diagrams.
This remains valid throughout the whole calculation, even though some
sub-topologies contain more than one massless internal line. In these
cases, the integrand structure is such that an IR singularity does not
occur. Though IR divergences are absent, additional sub-ultraviolet
divergences can arise depending on the structure of the integrand. This
is true for the diagrams @xmath , @xmath and @xmath . The integral
@xmath is finite in all mass configurations appearing in this
calculation. To summarize, the previously shown two-loop self-energies
may contain single and double UV poles. The newly included
momentum-dependence gives rise to additional divergent terms. It is
desired to find the correct counter-terms for their renormalization.

It is beneficial to analyze which diagrams carry momentum dependence,
and which contribute with additional divergent parts. First of all, the
topologies 8, 9 and 10 of Figs. 30(h) - LABEL: do not contribute with an
additional momentum dependence at the two-loop level. Taking a
diagrammatic point of view, these diagrams appear as tadpoles with two
external legs pinched to one vertex, and all tadpoles are independent of
the external momentum. All other diagrams carry momentum dependence in
the loop and contain momentum-dependent single UV divergent pole
contributions. The diagram in Fig. 30(l) is the only one to contain a
single UV pole, all other diagrams have the double amount. Only three
diagrams contain momentum-dependent double UV divergent terms, namely
30(b) , LABEL: and LABEL: .

With the additional contributions stemming from momentum dependence
being exposed, the divergent terms can be eliminated, before the
momentum-dependent contributions in the finite self-energies can be
analyzed.

### 28 Renormalization

As mentioned in the previous section, up to two UV poles appear in the
unrenormalized self-energies. These can contain local divergences in a
sub-loop. To renormalize these, one-loop self-energies with counter-term
insertions have to cancel those divergences arising in a sub-loop of the
two-loop diagrams. In addition, two-loop counter terms need to cancel
the rest of the divergences. In this calculation, a mixed on-shell and
@xmath scheme is used. Within the @xmath renormalization scheme,
conventional dimensional reduction, see Sec. 5 , is imposed. The bar in
@xmath denotes that an additional factor of @xmath and @xmath stemming
from prefactors to loop integrals is absorbed into the renormalization
scale @xmath .

In a renormalized theory, all ultraviolet poles have to be canceled and
physical quantities need to be finite. The proof of the existence of a
set of counter-term vertices rendering any theory of superficial
divergence finite is summarized in the BPHZ theorem, see Refs. [ 143 ,
144 , 145 ] . While the renormalization of a sub-loop of two-loop
self-energies is constructed from vertex corrections, their two-loop
counter terms are determined from the Lagrangian. The latter can be
derived by looking at corrections to the fields and masses. To this end,
each scalar doublet gets a multiplicative field renormalization,

  -- -------- -- -------
     @xmath      (173)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (174)
  -- -------- -- -------

Similarly, the masses get renormalization constants. This is implemented
by adding counter-terms to the tree-level mass matrices

  -- -------- -- -------
     @xmath      (175)
  -- -------- -- -------

with @xmath . Recapitulating Sec. 1 , the Higgs-boson masses are
determined from both, the bilinear part of the Lagrangian, compare Eq. (
20 ), and the linear part, see Eqs. ( 16 )-( 18 ). They contain
parametric dependences on @xmath , @xmath , @xmath , the charge @xmath ,
the electroweak mixing angle @xmath , the angle relating the vacuum
expectation values @xmath and the tadpole coefficients @xmath and @xmath
, compare Eqs. ( 16 ), ( 23 ) and ( 25b ). Reformulating the kinetic
part of the Lagrangian including field renormalization constants yields

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (176a)
     @xmath   @xmath      
              @xmath      (176b)
  -- -------- -------- -- --------

Examining those parts of the potential part of the Lagrangian
contributing to the Higgs-boson masses, the following substitutions
appear in the formulation of UV finite quantities,

  -- -------- -- -------
     @xmath      (177)
  -- -------- -- -------

where the tree-level terms and the one, two and higher loop
counter-terms can be identified. An expansion of the square root becomes
necessary when the fields @xmath and @xmath mix,

  -- -------- -------- -- --------
                          
     @xmath   @xmath      
              @xmath      (178a)
     @xmath   @xmath      
              @xmath      (178b)
  -- -------- -------- -- --------

where a record of the expansion is only kept until second order. As
mentioned before, the terms linear in the field need to vanish at all
orders. Looking at these after the application of the equations of
motion Eq. ( 56 ), the dependence on the fields drops out, meaning that
there is no field renormalization involved here. This is in agreement
with the fact that tadpoles do not carry momentum dependence. Therefore,
the only counter-term correction needed in Eq. ( 57 ) arises from the
corrections to the tadpoles themselves

  -- -------- -- -------
     @xmath      (179)
  -- -------- -- -------

The solution to the equations of motion including counter-terms
therefore reads

  -- -------- -- -------
     @xmath      (180)
  -- -------- -- -------

#### 28.1 Two-loop counter terms for the renormalization at @xmath

Applying field and parameter renormalization yields the renormalized
self-energies in terms of the scalar fields @xmath and @xmath

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (181a)
     @xmath   @xmath      (181b)
     @xmath   @xmath      (181c)
  -- -------- -------- -- --------

The counter-terms @xmath arise from the kinetic Lagrangian, see Eq. (
176b ), and @xmath from the potential part of the Lagrangian, see Eq. (
177 ).

The counter-terms originating from perturbative expansions to the free
fields @xmath and @xmath read

  -- -------- -- -------
     @xmath      (182)
  -- -------- -- -------

as deducible from Eqs. ( 174 ) and ( 176b ). In the calculation of the
self-energies of the two orders @xmath and @xmath , the squared one-loop
field renormalization constants do not contribute. They would be needed,
e.g., in an order @xmath calculation. This reduces the field
renormalization counter-terms to

  -- -------- -------- -- -------
     @xmath   @xmath      (183)
     @xmath   @xmath      (184)
  -- -------- -------- -- -------

Based on the derivation from the beginning of Sec. 28 , the
counter-terms arising from the renormalization of the potential part of
the Lagrangian are given by

  -- -------- -------- -- -------
     @xmath   @xmath      (185)
     @xmath   @xmath      (186)
     @xmath   @xmath      (187)
  -- -------- -------- -- -------

where @xmath is computed from the Taylor series expansion of the mass
matrix @xmath in its parameters up to second order. Recalling the mass
matrix @xmath of Eq. ( 25b ), the vanishing of the tree-level tadpole
parameters due to the minimization of the MSSM Higgs-boson potential,
Eq. ( 15 ), was already incorporated. Tadpole contributions must be
taken into account at higher orders, as their coefficients do not
necessarily vanish, see Eq. ( 57 ) and Eq. ( 180 ). Yet, they only enter
in the renormalization of the momentum independent part. The mass matrix
is then parametrized by @xmath , @xmath , @xmath , the charge @xmath ,
the electroweak mixing angle @xmath , the angle relating the vacuum
expectation values @xmath and the tadpole coefficients @xmath and @xmath
. The computation of the Taylor series expansion of the mass matrix up
to second order corresponds to a perturbative expansion in these
parameters,

  -- -------- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath   @xmath      
     @xmath   @xmath                        (188)
  -- -------- -------- -------- -------- -- -------

With the computation performed in the “gaugeless” limit, all
counter-terms including electroweak gauge particles and their
contributions are discarded. Furthermore, squares of one-loop
renormalization constants appearing in @xmath do not contribute, as they
are proportional to electroweak coupling factors but they are not of the
order @xmath . For a full explicit presentation of @xmath , see e.g.
Ref. [ 86 ] . The final counter-terms for the potential in the quadratic
@xmath terms are

  -- -------- -------- -- --------
                          
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (189a)
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (189b)
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (189c)
  -- -------- -------- -- --------

With the derived counter-terms at hand, the renormalization constants
entering them need to be defined. As mentioned previously, these are
renormalization scheme dependent. To be consistent with previous
calculations incorporated in the public program FeynHiggs , all masses
and the tadpole parameters are renormalized using the on-shell scheme,
and all field contributions using the @xmath scheme [ 140 , 320 , 321 ,
322 , 323 ] . In the latter, a dependence on the renormalization scale
@xmath is exhibited. The two-loop renormalization constants entering the
mass renormalization counter-terms are known for vanishing external
momentum, see Refs. [ 117 , 118 , 119 , 109 ] . Taking the momentum
dependence into account, only the tadpole contributions remain
unchanged. The tadpole renormalization constants can be deduced from
Eq. ( 180 ). To meet the condition that the sum of all tadpole
contributions must vanish, it must follow

  -- -------- -- -------
     @xmath      (190)
  -- -------- -- -------

adopting an on-shell renormalization. To obtain the renormalization
constants @xmath , thus two-loop tadpole diagrams have to be computed,
matching the right order in the couplings, compare the diagrams in Fig.
32 .

For the renormalization constants @xmath , @xmath and @xmath several
choices are possible, see the discussion in [ 140 , 324 ] . As shown
there, the most convenient choice is a @xmath renormalization of @xmath
, @xmath and @xmath . The field renormalization constants can be
extracted by taking the derivative of the renormalized self-energies,
Eqs. ( 181 ), with respect to the external momentum squared, yielding in
@xmath renormalization

  -- -------- -------- -- -------
     @xmath   @xmath      (191)
     @xmath   @xmath      (192)
  -- -------- -------- -- -------

The formulation of the field renormalization constants in the @xmath
scheme has the advantage that all ultraviolet divergences of the
momentum-dependent integrals contained in the @xmath self-energies are
known analytically. The derivative with respect to @xmath can therefore
be performed in a fully analytical way. Adopting an on-shell field
renormalization, the derivatives of some integrals in the finite part
for which closed analytical expressions are not available, are expected
to be less straightforward. It could be argued that the calculation can
be simplified even further adopting a full @xmath scheme, compare Refs.
[ 136 , 137 , 138 ] . Yet, the dependence on the renormalization scale
is decreased adopting a hybrid renormalization scheme, physical effects
of higher order in the @xmath scheme already appear at the current order
in the on-shell scheme, and predictions are given with pole masses as
input parameters.

Due to the fact that @xmath is defined in terms of the two vacuum
expectation values @xmath and @xmath , Eq. ( 24 ), minimizing the MSSM
Higgs-boson potential according to Eq. ( 15 ), the field renormalization
also enters here. The renormalization of @xmath follows from

  -- -------- -- -------
     @xmath      (193)
  -- -------- -- -------

yielding in @xmath

  -- -------- -- -------
     @xmath      (194)
  -- -------- -- -------

The term in Eq. ( 194 ) is in general not the proper expression beyond
one-loop order even in the @xmath scheme. In the approximation with only
the top Yukawa coupling at the two-loop level, it is the correct @xmath
form, see Refs. [ 325 , 326 ] .

Finally, the two-loop renormalization constant of the @xmath -boson mass
is determined in the on-shell scheme as

  -- -------- -- -------
     @xmath      (195)
  -- -------- -- -------

in terms of the unrenormalized two-loop @xmath -boson self-energy @xmath
evaluated at the pole mass; see Fig. 30 for the generic @xmath -boson
self-energy diagrams at the two-loop level. The appearance of a non-zero
momentum in the self-energy goes beyond the @xmath corrections evaluated
in Refs. [ 117 , 118 , 119 , 109 ] . Fixing the external momentum of the
self-energy to the @xmath -boson mass is necessary to cancel additional
divergent terms arising from the two-loop counter-terms involving @xmath
, compare Eqs. ( 189a )-( 189c ). The latter contain a dependence on
@xmath , giving rise to divergences which do not cancel any divergence
arising in the neutral @xmath -even Higgs-bosons. The latter are
independent of the @xmath -boson mass. These additional divergences must
therefore be cancelled by @xmath .

#### 28.2 Renormalization at the sub-loop level

With the two-loop counter terms at hand, there is only one missing piece
left in the renormalization procedure. Filling this gap, those one-loop
diagrams with counter-term insertions are computed, which renormalize a
sub-loop of the two-loop self-energies and of the two-loop tadpole
diagrams. The latter are needed for the two-loop counter-terms, compare
Eq. ( 190 ). The one-loop amplitudes with counter-term insertions are
generated with the program FeynArts [ 183 , 184 , 315 ] , using the
model file including counter-terms from Ref. [ 327 ] .

One-loop corrections @xmath to the field @xmath with one-loop
counter-term insertions @xmath of a given parameter @xmath were already
shown in Eqs. ( 176b ) and ( 177 ). The contributions there, however,
are of the order @xmath and lack a dependence of @xmath . To find the
proper sub-loop renormalization terms, the particle interactions
proportional to @xmath , listed in Sec. 2 , need to be taken into
account. They amount to five different counter-term structures, see Fig.
33 , required for the insertions in the one-loop amplitudes of order
@xmath in Fig. 34 .

The latter involve the one-loop neutral @xmath -even and @xmath -odd
Higgs-boson amplitudes. The @xmath -odd amplitude is needed for the
sub-loop renormalization of the @xmath -boson self-energy entering the
two-loop counter-terms. Furthermore, the tadpole diagrams entering the
two-loop counter-terms need a proper sub-loop renormalization. Their
one-loop amplitudes must be computed as well, see the diagrams in Fig.
35 .

The one-loop diagrams are either of order @xmath or @xmath , see Figs.
34 and 35 , respectively. An additional factor @xmath comes in with the
prefactor to the tadpole renormalization constants, compare Eqs. ( 189a
)-( 189c ). The program FormCalc [ 186 ] is used to perform the
reduction of the one-loop diagrams to one-, two-, and three-point master
integrals @xmath , @xmath and @xmath , respectively. In addition, tensor
coefficients to the two- and three-point integrals can arise.

The counter-term insertions contain quark and squark
field-renormalization components as well as terms resulting from the
renormalization of the tree-level top-quark and squark mass matrices.
The renormalization of the top and stop sector at the one-loop level as
discussed below, has been analyzed in detail in Refs. [ 119 , 321 , 320
, 86 , 135 , 322 , 323 ] . One-loop counter-terms to the stop mass
matrix @xmath and the fields @xmath and @xmath read

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (196a)
     @xmath   @xmath      (196b)
     @xmath   @xmath      (196c)
  -- -------- -------- -- --------

The stop fields are transformed into the physical components @xmath and
@xmath via

  -- -------- -- -------
     @xmath      (197)
  -- -------- -- -------

where @xmath is the stop mixing matrix defined in Eq. ( 54 ). The
free-field kinetic terms of the stops are bilinear in the fields @xmath
and when computing the field renormalization counter-terms @xmath ;
these receive contributions from both the left-hand and right-hand
components. To disentangle them, it is beneficial to introduce the field
renormalization in the mass eigenstate basis

  -- -- -- -------
           (198)
  -- -- -- -------

Now, a transformation of coordinates of the left- and right-handed field
renormalization matrix can be performed

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (199a)
     @xmath   @xmath      (199b)
  -- -------- -------- -- --------

where a comparison of the coefficients with Eq. ( 198 ) yields

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (200a)
     @xmath   @xmath      (200b)
     @xmath   @xmath      (200c)
              @xmath      (200d)
              @xmath      (200e)
  -- -------- -------- -- --------

Now, the one-loop free field renormalization terms in the physical basis
read

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (201a)
     @xmath   @xmath      (201b)
     @xmath   @xmath      (201c)
  -- -------- -------- -- --------

In analogy to the previous sections, the counter-terms arising from the
squark potential get a field and a mass renormalization contribution.
The one-loop squark potential counter-terms result from

  -- -- -- -------
           (202)
  -- -- -- -------

where @xmath is defined in Eq. ( 53 ) and @xmath in Eq. ( 198 ).
Expanding them up to the first order they read

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (203a)
     @xmath   @xmath      (203b)
     @xmath   @xmath      (203c)
  -- -------- -------- -- --------

As the field renormalization matrices @xmath are not diagonal,
contributions from tree-level masses enter in the off-diagonal squark
potential counter-terms, even though the mass matrix @xmath is in
diagonal form. The mass renormalization counter-terms read

  -- -------- -- -------
     @xmath      (204)
  -- -------- -- -------

where the total differential of the matrix @xmath defines the one-loop
counter-terms. Written out in explicit form, each contribution reads as
follows,

  -- -------- -- -------
     @xmath      (205)
  -- -------- -- -------

Collecting all counter-terms, the renormalized one-loop squark
self-energies are given by

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (206a)
     @xmath   @xmath      (206b)
     @xmath   @xmath      (206c)
  -- -------- -------- -- --------

While the sum of all field renormalization contributions concerning
inner fields vanishes, the massive contributions do not. All
self-energies are renormalized on-shell where the following
renormalization conditions were used, following Refs. [ 119 , 321 , 320
, 322 , 323 ]

  -- -------- -------- -------- -- --------
                                   
     @xmath   @xmath   @xmath      (207a)
     @xmath   @xmath   @xmath      (207b)
     @xmath   @xmath   @xmath      (207c)
     @xmath   @xmath   @xmath      (207d)
     @xmath   @xmath   @xmath      (207e)
  -- -------- -------- -------- -- --------

These lead to the following determinations for the renormalization
constants entering the mass counter-terms,

  -- -------- -------- -- -------
     @xmath   @xmath      (208)
     @xmath   @xmath      (209)
  -- -------- -------- -- -------

and the field counter terms

  -- -------- -------- -- -------
     @xmath   @xmath      (210)
     @xmath   @xmath      (211)
  -- -------- -------- -- -------

@xmath was already defined in Eq. ( 200e ). In addition to the squarks,
also the quark sector has to be renormalized. The top-quark mass is
defined on-shell, yielding the one-loop counter-term

  -- -------- -- -------
     @xmath      (212)
  -- -------- -- -------

where @xmath are the left- and right-handed components of the quark
self-energy, respectively. The latter can be decomposed according to its
Lorentz structure,

  -- -------- -------- -- -------
     @xmath   @xmath      (213)
  -- -------- -------- -- -------

where it is split into the scalar part of the quark self energy @xmath ,
the vectorial @xmath and the axial vectorial part @xmath .

The renormalization of the stop-mixing soft breaking term @xmath ,
defined in Eq. ( 55 ), reads

  -- -------- -- -------
     @xmath      (214)
  -- -------- -- -------

where the fact that neither @xmath nor @xmath have couplings of the
order @xmath was already taken into account. The renormalization
constant @xmath enters in counter-term vertex insertions, e.g. in Fig.
34 . All renormalization constants are independent of an external
momentum, while the one-loop diagrams containing the counter-term
insertions carry momentum dependence.

### 29 Treatment of the integrals

In Sec. 27 , some relevant two-loop topologies have already been
introduced diagrammatically, compare Fig. 31 in Sec. 27 . Below, the
following notation

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (215)
  -- -------- -------- -- -------

for the scalar two-loop integrals is adopted, where @xmath is the
external momentum, the @xmath are the loop momenta, @xmath is the
renormalization scale and the @xmath denote the masses of the
propagators. To comply with @xmath renormalization, an additional factor
of @xmath and @xmath is absorbed into the renormalization scale @xmath
as

  -- -------- -- -------
     @xmath      (216)
  -- -------- -- -------

The momenta are continued to @xmath dimensions. The number @xmath of
indices of the two-loop integral @xmath corresponds to the number of
propagators involved. The indices’ numbers @xmath label which propagator
type appears in the integral. There are five different propagator types
@xmath , where the @xmath read

  -- -------- -- -------
     @xmath      (217)
  -- -------- -- -------

In accordance with Ref. [ 328 ] , the letters @xmath , @xmath and @xmath
are used to describe one-loop integrals with one, two and three external
legs, respectively. Hence, a one-loop tadpole integral @xmath can have
one momentum-independent propagator of type @xmath or @xmath . A
one-loop bubble integral @xmath is composed of two propagators of type
@xmath and @xmath or @xmath and @xmath . The three-point vertex
integrals @xmath can carry dependence of two different external momenta
@xmath and @xmath and have three propagators. In this calculation,
however, being part of a self-energy computation, they depend on one
external momentum @xmath only and are therefore always reducible to
two-point one-loop functions @xmath .

#### 29.1 Analytically known integrals

A series representation in the regulator @xmath of all one-loop
integrals entering the sub-loop renormalization is known in analytical
form and a variety of different representations is available. In this
calculation, the approaches of Refs. [ 328 , 329 , 330 , 331 , 332 , 333
] are used. At two loops, the resulting integrals from the reduction are
mostly factorizing one-loop diagrams. Also all @xmath -divergent parts
and a few finite parts of the two-loop integrals appearing in the
calculation are known from Refs. [ 330 , 331 , 332 , 334 , 335 ] .
Knowing the analytical representations of the divergent parts is vital
in finding exact cancellation of all divergences. The full analytical
cancellation of all divergences is achieved with the above mentioned
integral representations, taking into consideration all counter-terms of
Secs. 28.1 and 28.2 , with their proper renormalization constants. The
integral representations have all been cross-checked intensively with
SecDec and the Golem95 integral library [ 188 , 336 ] . Especially for
the imaginary parts these two tools were very useful.

Due to the occurrence of factorizing one-loop integrals, these must be
known to first order in @xmath as they can contribute to the finite part
through terms of the type

  -- -------- -- -------
     @xmath      (218)
  -- -------- -- -------

All necessary parts for the one-loop integrals are known analytically [
329 , 332 , 330 , 331 ] . As mentioned in the previous section,
three-point integrals @xmath result from the tensor reduction in the
sub-loop renormalization part. The @xmath and @xmath integrals are UV
divergent, the @xmath integrals appear as finite integrals only because
possible IR singularities are regulated by the massive propagators.
Additionally, the third external leg occurs in the context of counter
term insertions and therefore always has a vanishing external momentum,
compare Figs. 33 and 33 . It is due to this fact, that all three-point
functions can be reduced to derivatives of two-point functions. A
general three-point function reads

  -- -------- -------- -- -------
     @xmath   @xmath      (219)
  -- -------- -------- -- -------

Here, the notation of Ref. [ 328 ] is adopted. One example of a @xmath
integral occurring in the calculation, to be reduced to the derivative
of a two-point function, reads

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (220a)
              @xmath      (220b)
              @xmath      (220c)
  -- -------- -------- -- --------

From the derivative in the last line, the finiteness of the @xmath
integral becomes apparent; the divergent part of the two-point function
does not depend on the masses.

The reduction to purely @xmath and @xmath integrals is especially useful
as derivatives of all integrals occurring in the self-energies are
required in the calculation of the field renormalization constants.
Products of @xmath integrals with @xmath and @xmath integrals, entering
through the counter-term insertions, lead to finite three-point function
contributions to the derivatives. Now, the derivatives of @xmath
integrals can be easily computed for simpler cases, but for the very
general case of arbitrary masses in the loop and external momentum,
analytical results are much more involved. For cross checks, the
derivatives of the @xmath integrals for the simpler cases could be
computed using the algebraic output from SecDec and integrating the
expressions analytically.

Furthermore, the method of partial fractioning can be exploited, see
Ref. [ 337 ] . It is used for the reduction of @xmath integrals to
one-point integrals. The method works at arbitrary loop order. At two
loops, e.g., the vacuum integral @xmath function can be reduced as
follows

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      (221)
  -- -------- -------- -- -------

where the first entry in the integrals denotes a zero external momentum,
in accordance with the notation set up in Eq. ( 215 ).

The additional relation

  -- -------- -- -------
     @xmath      (222)
  -- -------- -- -------

is of use, where @xmath denotes the coefficient to the singly divergent
@xmath term in the Laurent expansion of the @xmath integral in the
regulator @xmath , and @xmath the finite part of the @xmath integral.
Relation Eq. ( 222 ) can be found as a direct consequence of the
formulas in the appendix of Ref. [ 330 ] . Furthermore, the relation

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (223a)
              @xmath      (223b)
  -- -------- -------- -- --------

deduced from

  -- -------- -- --------
                 
     @xmath      (224a)
     @xmath      (224b)
     @xmath      (224c)
  -- -------- -- --------

helps achieving a stable cancellation of the UV poles. A momentum shift
was performed in the first integral of Eq. ( 224b ).

The tensor coefficients @xmath and @xmath can be treated by expanding
their numerators. To give an example, the @xmath tensor coefficients
appearing in this calculation are of the type

  -- -- -------- -- --------
                    
        @xmath      (225a)
        @xmath      (225b)
        @xmath      (225c)
        @xmath      
        @xmath      (225d)
  -- -- -------- -- --------

Similarly, the @xmath coefficient can be reduced to @xmath and @xmath
integrals.

After the cancellation of all divergent parts, the finite parts must be
treated. These maximally include finite terms of two-loop integrals and
terms up to the order @xmath in the one-loop case. As is visible from
Eq. ( 222 ), the finite part of the one-loop bubble is part of a
sub-divergence of the two-loop integral @xmath . This fact suggests that
one-loop terms of order @xmath may contribute to the finite parts of
some of the two-loop integrals. As a matter of fact, @xmath parts of the
integral

  -- -------- -- -------
     @xmath      (226)
  -- -------- -- -------

appear in sums and subtractions with the finite part of the integral

  -- -------- -- -------
     @xmath      (227)
  -- -------- -- -------

An analytic result of the finite part of the latter is not available. As
the unknown integrals are computed numerically with version 2 of SecDec
, analytic and numeric results enter the self-energies and their
counter-terms. A stable evaluation of the analytically available and the
numerically computed functions is required. The analytically available
integrals appearing in the final result are the one-point functions
@xmath , the scalar two-point integral @xmath , the derivative of @xmath
by one mass, Eq. ( 226 ), the two-loop vacuum diagram with arbitrary
masses @xmath , and the two-loop bubble diagram @xmath with two massive
propagators of the same mass and one massless propagator. Explicit
expressions for the integrals are given in App. A . For the evaluation
of the analytical integrals beyond threshold, the proper analytical
continuation into the complex plane is essential. Square roots, as well
as logarithms, get an imaginary part as soon as their arguments are
negative. The correct prescription for the analytical continuation is
given by the causal @xmath appearing in the Feynman propagators. For the
kinematic invariants, they separately read

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (228a)
     @xmath   @xmath      (228b)
     @xmath   @xmath      (228c)
  -- -------- -------- -- --------

In practice, the infinitesimal quantity @xmath must be assigned a tiny,
but not infinitesimally small value. This can lead to numerical
instabilities in the evaluation of an analytical result. These can be
evaded by explicitly choosing a Riemann sheet of the square root

  -- -------- -- -------
     @xmath      (229)
  -- -------- -- -------

and the logarithm

  -- -------- -- -------
     @xmath      (230)
  -- -------- -- -------

as shown, e.g., in Ref. [ 332 ] . At the one-loop level, thresholds can
be parametrized by square roots of the fully symmetric Källén function

  -- -------- -- -------
     @xmath      (231)
  -- -------- -- -------

which frequently appears in the analytic expressions. Its sign in
different parameter regions is

  -- -------- -- -------
     @xmath      (232)
  -- -------- -- -------

With this knowledge at hand, all thresholds entering from the on-shell
renormalization of the sub-loop can be explained as the square root of
the Källén function becomes complex for a negative @xmath .

#### 29.2 Analytically unknown integrals

The evaluation of some of the integrals entering the calculation are not
known. This concerns different topologies of the integrals @xmath ,
@xmath , @xmath and @xmath , depicted in Fig. 31 . While the evaluation
is straightforward in the massless cases, the all-massive cases are very
hard to compute in a full analytical way. This is due to the fact that
neither the @xmath nor the @xmath can be expressed in terms of
polylogarithms, see Refs. [ 338 , 339 ] and references therein. An
analytical description of the diagram @xmath in terms of generalized
Lauricella functions was found in Ref. [ 338 ] . The Appell functions
appearing in their result are extremely hard to evaluate in a full
analytical way, compare, e.g. Ref. [ 340 ] . Up to date, only
one-dimensional integral representations are available for the
evaluation of the @xmath integral with arbitrary masses. Special cases
(large momentum expansion, equation of masses) of the @xmath integral
are available, see Refs. [ 341 , 330 , 331 , 342 ] . The general result
for different masses in the three propagators is still discussed today,
among physicists [ 343 , 344 , 345 ] and mathematicians [ 346 ] alike. A
description of the finite @xmath function in terms of a double integral
representation [ 347 ] , and in terms of a single integral
representation [ 335 ] , is also available. Furthermore, special cases
and asymptotic limits are known, see Refs. [ 348 , 349 , 350 , 330 ] .
The numerical evaluation of integrals not accessible with purely
analytical methods can be achieved, benefitting from the automated setup
of the program SecDec .

##### 29.2.1 Numerical computation of two-point two-loop integrals

The aforementioned four integral topologies @xmath , @xmath , @xmath and
@xmath appear in 34 different mass configurations. These involve up to
four different masses, in addition to the mass scale given by the
external momentum @xmath . They are all computed with SecDec . For the
whole evaluation, the integrator Divonne contained in the Cuba library [
302 , 303 ] is used. The integrator uses a deterministic algorithm and
can reach very accurate results in integrations of few, but more than
one, Feynman parameters. An additional acceleration was achieved by
introducing user-defined thresholds to the program SecDec , see Secs.
18.8 and B . This allows the user to define a lowest threshold
condition. Once it is met, SecDec switches to a deformation of the
integration contour into the complex plane. The kinematic values can
differ by up to 14 orders of magnitude. The evaluation of a single phase
space point for the most complicated topology, to reach a relative
accuracy of at least @xmath , ranges between 0.01 and 100 seconds on an
Intel i7 processor, where the larger timings are for points very close
to a kinematic threshold. The huge differences in the kinematic
invariants enter, for instance, when choosing a small value for the
squared external momentum, while testing large squared masses. These
configurations were of special interest in the performance of checks
against the results obtained for vanishing external momenta. For two
representative results, see Fig. 36 .

The configuration in Fig. 36 has three different mass scales where the
first two masses can be associated with the top mass @xmath GeV, the
third mass with one possible value for a stop mass @xmath GeV and the
fourth mass to a value for the gluino @xmath TeV. In Fig. 36 , the
numerical values @xmath GeV, @xmath GeV, @xmath TeV and @xmath GeV are
chosen. The relative accuracy of the plots is beyond @xmath . When
evaluating a specific scenario, some integrals needed to be evaluated up
to a numerical relative accuracy of at least @xmath to make up for
cancellations appearing between analytically evaluated integrals and
numerical ones. For the evaluation of an arbitrary scenario with
arbitrary rMSSM parameters, this accuracy was therefore demanded for
every SecDec integral.

### 30 Evaluation of the additional shifts to the Higgs-boson masses

The calculation is performed in the @xmath - @xmath basis. To be
consistent with all other higher-order contributions to the Higgs-boson
masses incorporated in the public program FeynHiggs , the renormalized
self-energies in the @xmath - @xmath basis can be rotated into the
physical @xmath - @xmath basis, where the tree-level propagator matrix
is diagonal, via

  -- -------- -------- -------- -- --------
                                   
     @xmath   @xmath   @xmath      (233a)
     @xmath   @xmath   @xmath      (233b)
     @xmath   @xmath   @xmath      (233c)
  -- -------- -------- -------- -- --------

where @xmath is the tree-level mixing angle and using Eq. ( 5 ). The
former is expressible in terms of the parameters @xmath , @xmath and
@xmath , see Eq. ( 30 ). The resulting new contributions to the neutral
@xmath -even Higgs-boson self-energies, containing all
momentum-dependent and additional constant terms, are assigned to the
differences

  -- -------- -- -------
     @xmath      (234)
  -- -------- -- -------

Note the tilde (not hat) on @xmath , which signifies that not only the
self-energies are evaluated at zero external momentum but also the
corresponding counter-terms, following Refs. [ 117 , 118 , 119 ] . A
finite shift @xmath therefore remains in the limit @xmath due to @xmath
being computed at @xmath in @xmath , but at @xmath in @xmath , as
discussed in Sec. 28.1 .

Subtracting the finite shift of @xmath , the @xmath in Eq. ( 234 ) must
vanish. This limit was tested numerically, see Sec. 29.2.1 . Moreover,
the zero momentum limit was checked analytically, deriving expressions
for the vacuum diagrams @xmath , @xmath and @xmath and using an
available expression for @xmath . The relations were computed from
@xmath using derivatives of the integral by the masses and partial
fractioning as mentioned in Ref. [ 337 ] . Expressions for the vacuum
diagram @xmath with different mass configurations can be found in Refs.
[ 351 , 349 ] . All deduced integrals were checked with SecDec ,
including the three-propagator vacuum bubble @xmath . For further
comparison, the expression for the @xmath integral of Ref. [ 352 ] could
be used.

The higher-order corrected @xmath -even Higgs-boson masses in the MSSM
are obtained from the corresponding propagators dressed by their
self-energies. Inserting the fields @xmath and @xmath in Eq. ( 64 ), the
inverse propagator matrix in the @xmath - @xmath basis is given by

  -- -------- -- -------
     @xmath      (235)
  -- -------- -- -------

The @xmath -even Higgs boson masses are determined by the poles of the
@xmath - @xmath -propagator matrix. This is equivalent to solving the
equation

  -- -------- -- -------
     @xmath      (236)
  -- -------- -- -------

yielding the loop-corrected pole masses, @xmath and @xmath .

#### 30.1 Phenomenological motivation for two different scenarios

Suitable scenarios to analyze the influence of the new self-energies on
the Higgs-boson mass shifts should cover a range of experimentally
allowed parameter space, in addition to maximizing the resulting
additional shifts. It should be noted, that a complete parameter scan
over the in principle more than one hundred free parameters of the MSSM
is not feasible and the experimental sensitivity to collectively
constrain many parameters is not sufficient with present experiments. In
practice, parameter scans are therefore done for a smaller set of
parameters with the highest phenomenological impact on the rMSSM Higgs
sector, see e.g. Ref. [ 353 ] . These reflections result in an @xmath
and a light stop scenario, motivated in and following analyses from
Refs. [ 353 , 354 ] .

The MSSM input parameters entering the calculation are

  -- -------- --
     @xmath   
  -- -------- --

The benchmark scenarios of Ref. [ 354 ] are given in terms of the
following set of MSSM input parameters

  -- -------- --
     @xmath   
  -- -------- --

In the conversion to the latter parameters, the left- and right-handed
soft SUSY breaking stop mass parameters are equated and set to the soft
SUSY breaking scale

  -- -------- -- -------
     @xmath      (237)
  -- -------- -- -------

The two stop masses @xmath , @xmath and the angle @xmath are expressed
in terms of @xmath , the soft SUSY breaking mixing parameter @xmath ,
the top-quark mass @xmath and @xmath using Eq. ( 50 ) with @xmath . The
value for the top mass @xmath GeV is used in both scenarios and taken
from the latest combination of all top-mass measurements undertaken by
the D0 and CDF collaborations [ 355 ] , in agreement with the top mass
resulting from the combination of the latest ATLAS, CMS results, see
Ref. [ 356 , 357 ] . For the computation of the Higgs-boson mass shifts,
a value of @xmath GeV [ 358 ] is used. ⁵ ⁵ 5 For simplicity, the @xmath
boson mass contribution is not taken into account in the Figs. 37 , 38 ,
39 and 40 .

Keeping the top and @xmath boson mass fixed, the stop masses can be
plotted with respect to the ratio @xmath . For the limit @xmath , the
squark mass eigenstates are equal. Following the analyses in Ref. [ 353
] , degenerate stop masses are excluded, if the observed new particle at
the LHC is associated with the light Higgs-boson at around @xmath GeV.
Therefore, a maximal mixing of @xmath is assumed in all scenarios,
varying only the SUSY breaking scale @xmath between @xmath GeV and
@xmath TeV.

The gluino mass parameter @xmath enters the MSSM Higgs-boson mass
predictions from two-loop order on. It is therefore of special interest
to analyze its impact on the MSSM Higgs-boson masses. The gluino mass
can be indirectly constrained from the hitherto non-observation of a
second neutral Higgs-boson.

The higgsino parameter @xmath enters the self-energies through the
trilinear coupling. In all scenarios considered, it is chosen @xmath
GeV, in accordance with Ref. [ 354 , 353 ] .

The @xmath -odd @xmath boson mass and @xmath are left as free parameters
and can be varied between @xmath and @xmath respectively, compare Ref. [
353 ] . When choosing @xmath to be rather light, the observed particle
@xmath GeV can be associated with the heavy Higgs-boson leaving room for
an additional lighter state.

The corresponding renormalization scale, @xmath , is set to @xmath in
all numerical evaluations. The scale uncertainties are expected to be
much smaller than the parametric uncertainties due to variations of
parameters like @xmath .

#### 30.2 Renormalized @xmath self-energies

##### 30.2.1 Scenario 1: @xmath scenario

Scenario 1 is oriented at the @xmath scenario described in Ref. Ref. [
354 ] . The following values are assigned to the MSSM parameters,

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (238)
  -- -------- -------- -- -------

leading to stop mass values of

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

With the introduction of the momentum dependence, thresholds occur in
the self-energy diagrams when the external momentum @xmath , in the
time-like region, is such that a cut of the diagram would correspond to
the on-shell production of the massive particles of the cut propagators,
compare the discussion of Sec. 13 . The resulting imaginary parts will
enter in the search for the complex poles of the inverse propagator
matrix of the Higgs-bosons. Therefore it is interesting to study the
behavior of the real and imaginary parts of the self-energies.

The momentum-dependent parts of the renormalized two-loop self-energies
are shown in the physical basis, Eq. ( 234 ), for two different values
of @xmath , @xmath and @xmath , at a fixed @xmath -boson mass @xmath
GeV, see Fig. 37 . The data points are not connected by a line in order
to show that each numerical point is obtained from a calculation of the
34 analytically unknown integrals with the program SecDec . The inlays
in Fig. 37 magnify the region @xmath , where it can be observed that for
@xmath , the subtracted self-energies are not exactly zero. As mentioned
at the beginning of this section, this is due to the fact that the
on-shell renormalization condition for the @xmath -boson self-energy is
defined differently with regard to the calculation without momentum
dependence. The resulting constant contributions are additionally
suppressed by factors @xmath , @xmath and @xmath appearing in the
counter-terms @xmath , @xmath and @xmath , respectively, see Eqs. ( 189
).

The imaginary part is independent of the @xmath -boson mass, as this
mass parameter solely appears in the counter-terms of @xmath
renormalized quantities and the @xmath counter-term, where only the real
part contributes. Therefore, the imaginary parts do not contain
additional constant terms, compare Fig. 37 . As to be expected, the
imaginary parts are zero below the @xmath production threshold at @xmath
, which results from the fact that the top mass is the smallest mass
appearing in the loop diagrams. Beyond this threshold, the imaginary
parts are nonzero but of the same order of magnitude as the real parts.
From these observations, the mass shifts in the region below the first
threshold at @xmath are expected not to be large.

Similar results, now including a variation of @xmath are shown in Fig.
38 . In the upper plot for @xmath and in the middle plot for @xmath the
solid lines depict @xmath , while the dashed lines are for @xmath . In
these plots the light shading covers the range for @xmath , while the
dark shading for @xmath . In the lower plot for @xmath results for
@xmath are shown as solid, dotted, dot-dashed, dashed lines,
respectively (and shading has been omitted). For @xmath at low values of
the momentum @xmath only a small variation with @xmath can be observed.
For @xmath and @xmath large, the contributions to the self-energy are
bigger. In @xmath larger effects are observed at smaller @xmath for
both, small and large @xmath values. For @xmath , on the other hand, at
low @xmath values, large effects can be observed for large @xmath due to
the aforementioned counter-term contribution @xmath . At large @xmath ,
as before, small @xmath values give a more sizable contribution.

##### 30.2.2 Scenario 2: Light stop scenario

Scenario 2 is oriented at the “light-stop scenario” of Ref. [ 354 ] ⁶ ⁶
6 While the original scenario in Ref. [ 354 ] is challenged by recent
scalar-top searches at ATLAS and CMS, a small modification in the
gaugino-mass parameters (which play no or only a very minor role here)
to @xmath , @xmath leads to a SUSY spectrum that is very difficult to
test at the LHC. . The following values are assigned to the MSSM
parameters

  -- -------- -------- -- -------
     @xmath   @xmath      
     @xmath   @xmath      (239)
  -- -------- -------- -- -------

leading to stop mass values of

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

Scenario 2 is analyzed with the same set of plots shown for scenario 1.
The effects of the new momentum-dependent two-loop contributions on the
renormalized Higgs-boson self-energies, @xmath , are shown in Fig. 39 .
As before, separate results are shown for the real and imaginary parts
of the self-energies. An additional threshold beyond the top-mass
threshold appears at @xmath . Analogously to scenario 1, the largest
contributions in the region below @xmath arise in the real part of
@xmath amounting to about @xmath at @xmath , where the dependence on the
value of @xmath is rather weak. The imaginary part equals the one of
scenario 1 up to the @xmath threshold. The discontinuity at the latter
enters through the integral involving the derivative of the @xmath
function with respect to @xmath , @xmath , see Sec. 29.1 and Eq. ( 223b
) therein.

The dependence of @xmath on @xmath is shown in Fig. 40 , using the same
line styles as in Fig. 38 . The curves show the same qualitative
behavior as in Fig. 39 , exhibiting again the new threshold at @xmath .
In general, outside the threshold region the effects in scenario 2 are
slightly smaller than in scenario 1.

#### 30.3 Neutral @xmath-even Higgs-boson mass corrections

The numerical evaluation to derive the physical masses for @xmath as the
poles (real parts) of the dressed propagators proceeds on the basis
of Eq. ( 236 ) in an iterative way.

-   In a first step, the squared masses @xmath are determined by solving
    Eq. ( 236 ) excluding the new terms @xmath from the self-energies.
    The masses @xmath are computed based on the higher-order
    contributions of Refs. [ 139 , 141 , 119 , 128 , 135 , 132 ] .

-   In a second step, the shifts @xmath and @xmath are calculated and
    added as constants to the self-energies in Eq. ( 236 ), @xmath .

-   In the third step, Eq. ( 236 ) is solved again, now including the
    constant shifts @xmath in the self-energies, to deliver the refined
    masses @xmath (with @xmath and @xmath (with @xmath ).

This procedure can be repeated for improving the accuracy; numerically
it turns out that going beyond the first iteration yields only marginal
changes. Below, results are shown for the mass shifts

  -- -------- -- -------
     @xmath      (240)
  -- -------- -- -------

The mass shifts, in particular @xmath for the light @xmath -even
Higgs-boson, can directly be compared with the current experimental
uncertainty as well as with the anticipated future ILC accuracy [ 359 ]
of

  -- -------- -- -------
     @xmath      (241)
  -- -------- -- -------

The results are obtained for two different scenarios, varying parameters
like @xmath , and illustrate the impact of these parameters via the new
two-loop corrections on the neutral @xmath -even Higgs-boson masses,
@xmath and @xmath .

##### 30.3.1 Implementation in the program FeynHiggs

The corrections of Eq. ( 234 ) are incorporated in FeynHiggs ⁷ ⁷ 7 The
embedding of these corrections into the FeynHiggs code was performed
with substantial support by Thomas Hahn. by the following recipe, which
is more general and in principle applicable also to the case of the
complex MSSM with @xmath -violation.

1.  Determine the Higgs-boson masses @xmath without the
    momentum-dependent terms of Eq. ( 234 ); the index @xmath enumerates
    the masses of @xmath in the real MSSM. This is done by invoking the
    FeynHiggs mass-finder.

2.  Compute the shifts @xmath with @xmath .

3.  Run FeynHiggs ’ mass-finder again including the @xmath as constant
    shifts in the self-energies to determine the refined Higgs masses
    @xmath and @xmath .

This procedure could conceivably be iterated until full self-consistency
is reached; yet the resulting mass improvements turn out to be too small
to justify extra CPU time.

On the technical side, an interface for an external program to FeynHiggs
was added, which exports relevant model parameters to the external
program’s environment, currently FHscalefactor ren. scale multiplicator,
FHTB @xmath , FHAlfasMT @xmath , FHGF @xmath , FHMHiggs2 @xmath @xmath ,
@xmath , FHMSt @xmath @xmath , @xmath , FH{Re,Im}USt1 @xmath @xmath ,
@xmath , FHMGl @xmath , FH{Re,Im}MUE @xmath , FHMA0 @xmath , where the
@xmath denote the elements of the stop mixing matrix, @xmath the running
strong coupling at the scale @xmath , and @xmath the Fermi constant. The
renormalization scale is defined within FeynHiggs as @xmath . Invocation
of the external program is switched on by providing its path in the
environment variable FHEXTSE . The program is executed from inside a
temporary directory which is afterwards removed.

The output (stdout) is scanned for lines of the form ‘ se @ @xmath
@xmath @xmath ’ which specify the correction @xmath [with @xmath ] to
self-energy se in the computation of mass @xmath , where @xmath is one
of Mh0 , MHH , MA0 , MHp , and se is one of h0h0 , HHHH , A0A0 , HmHp ,
h0HH , h0A0 , HHA0 , G0G0 , h0G0 , HHG0 , A0G0 , GmGp , HmGp , F1F1 ,
F2F2 , F1F2 . The latter three, if given, substitute

  -- -------- -------- -- --------
                          
     @xmath   @xmath      (242a)
     @xmath   @xmath      (242b)
     @xmath   @xmath      (242c)
  -- -------- -------- -- --------

in accordance with Eq. ( 233 ). Self-energies not given are assumed
zero.

The zero-momentum contributions @xmath with @xmath , defined in Eq. (
234 ) are subtracted if the output of the external program contains one
or more of ‘ sub asat ’, ‘ sub atat ’, ‘ sub asab ’, ‘ sub atab ’ for
the @xmath , @xmath , @xmath , and @xmath contributions, respectively.
All other lines in the output are ignored.

##### 30.3.2 Scenario 1: @xmath scenario

The effects of the newly computed momentum-dependent two-loop
corrections on the Higgs-boson masses @xmath via the mass shifts @xmath
and @xmath are now studied. In Fig. 41 , @xmath (upper plot) and @xmath
(lower plot) are shown as a function of @xmath for @xmath (blue) and
@xmath (red). For @xmath , the additional shifts from
momentum-dependence of up to @xmath are of the size of the expected
future experimental precision, see Eq. ( 241 ). The contribution to the
heavy @xmath -even Higgs-boson mass is of similar order of magnitude.
Around the threshold at @xmath , the heavy Higgs-boson mass is shifted
upwards.

Finally, the dependence of @xmath and @xmath on the gluino mass @xmath
is examined. The results are shown in Fig. 42 for @xmath (upper plot)
and @xmath (lower plot) for @xmath , with the same color coding as in
Fig. 41 . In the upper plot one can observe that the effects are
particularly small for the default value of @xmath in scenario 1. More
sizeable shifts occur for larger gluino masses, by more than @xmath for
@xmath , reaching thus the level of the current experimental accuracy in
the Higgs-boson mass determination. The corrections to @xmath , for the
given value of @xmath do not exceed @xmath in the considered @xmath
range.

##### 30.3.3 Scenario 2: Light stop scenario

The effects on the physical neutral @xmath -even Higgs-boson masses are
now analyzed within scenario 2. The results are shown for @xmath (upper
plot) and @xmath (lower plot) as a function of @xmath (with the same
line styles as in Fig. 41 ), see Fig. 43 . As can be expected from the
previous figures, below the @xmath threshold, the effects on @xmath and
@xmath are in general slightly smaller in scenario 2 than in scenario 1,
where @xmath still reaches the anticipated ILC accuracy, see Eq. ( 241
). Above the @xmath threshold, the variation of @xmath is larger than in
scenario 1 for @xmath . Around the threshold @xmath a shift in the GeV
range towards lower masses is found, reaching the level of about 0.2%.
Beyond this threshold, the heavy Higgs-boson mass can be shifted towards
higher masses. The latter shift is also roughly in the GeV range.

Finally the dependence on @xmath is examined, see Fig. 44 . In the upper
plot @xmath is shown for @xmath and @xmath , where both values yield
very similar results. As in scenario 1, “accidentally” small values of
@xmath are found around @xmath . For larger gluino mass values the
shifts induced by the new momentum-dependent two-loop corrections exceed
@xmath and are thus larger than the current experimental uncertainty.
The results for @xmath are shown in the lower plot. While they are
roughly twice as large as in scenario 1, they do not exceed @xmath .

### 31 Summary and Perspectives

The calculation and respective results for the leading
momentum-dependent @xmath contributions to the masses of neutral @xmath
-even Higgs-bosons in the MSSM were presented. They are obtained by
calculating the corresponding contributions to the dressed Higgs-boson
propagators in the Feynman-diagrammatic approach.

The required two-loop self-energy diagrams and one-loop counter-terms
with counter-term insertions are generated using FeynArts , and reduced
to a set of basic integrals with the help of TwoCalc and FormCalc .

The mass and field renormalization is performed adopting a mixed
on-shell/ @xmath renormalization scheme. More precisely, the field
renormalization part is renormalized in the @xmath scheme, the on-shell
scheme is used for the remaining mass renormalization. This scheme
choice has been beneficial in other contexts [ 140 , 320 , 321 , 322 ,
323 ] and proved constructive in this calculation as well. Furthermore,
the on-shell renormalization of the two-loop @xmath -boson mass
counter-term lead to an additional physical shift in the self-energies
which has not been found in the calculation at zero momentum. The scheme
choice is in contrast to Ref. [ 136 , 137 ] , where the same
contribution was computed using a full @xmath scheme.

The renormalized momentum-dependent two-loop Higgs-boson self-energies
contain analytically inaccessible two-loop integrals. These are computed
numerically using the program SecDec . The new momentum-dependent
contributions are incorporated in the public program FeynHiggs ,
including an interface to SecDec . The analysis of the mass shifts is
performed with the upgraded version of FeynHiggs .

The numerical analysis showed that the effects on the light @xmath -even
Higgs boson mass, @xmath , depend strongly on the value of the gluino
mass, @xmath . For values of @xmath corrections of about @xmath are
found, at the level of the anticipated future ILC accuracy. If the
gluino mass is assumed very large, @xmath , the corrections are
substantially larger and at the level of the current experimental
accuracy [ 360 ] . The shifts in the heavy @xmath -even Higgs-boson
mass, resulting from the incorporation of the momentum dependence are
mostly below current and future anticipated accuracies. Only close to
thresholds, e.g. around @xmath , the corrections are larger but do not
exceed 0.2%.

The evaluation times for the computation of the @xmath -even Higgs-boson
masses including the new momentum-dependent two-loop contributions
strongly depend on the performance of SecDec . They range between one
minute for a point far, and maximally one hour for a point very close to
a threshold. Being able to compute multi-loop multi-scale integrals, the
program SecDec reaches far beyond the applicability to two-loop
two-point functions which are required for the self-energy corrections
of the MSSM Higgs-boson masses. Yet, it is due to its universal
applicability that a program specifically tailored to the desired
two-loop two-point functions may perform better in terms of evaluation
times. One such alternative could be a package provided by Bauberger [
332 ] , which contains the desired loop integrals in terms of
one-parameter integral representations.

With the computation of the renormalized @xmath -odd Higgs-boson
self-energy at hand, it could be interesting to extend the analysis of
the mass shifts resulting from momentum dependence to the charged
Higgs-bosons. Furthermore, it could be interesting to compute the
effective couplings to gauge bosons.

As a further improvement on the precision of the prediction of the
Higgs-boson masses, the contributions involving couplings to the
electroweak gauge bosons can be included. Moreover, the computation of
the leading QCD corrections at the three-loop order to be incorporated
into FeynHiggs can be considered.

## Chapter \thechapter Conclusions

The upgrade of the program SecDec towards the automated computation of
multi-loop multi-scale Feynman graphs in the physical region including
thresholds was presented, thereby lifting the restriction to the
Euclidean region.

The program allows for an automated algebraic factorization of
dimensionally regulated singularities and a numerical evaluation of the
resulting pole coefficients. These can be multi-loop Feynman integrals
with up to several mass scales and with in principle no limitation on
the tensor rank, or more general parametric functions with singularities
only at the endpoints of the integration region. Additionally, the
program was enhanced by allowing for the automated evaluation of
user-defined functions in the physical region.

The extension to physical kinematics was achieved by the implementation
of an automated analytical continuation of the integrand. An algorithm
to find the optimal according deformation of the integration contour was
further developed, allowing for a stable evaluation of integrals over
large regions of values for the kinematic invariants.

Provided with the upgraded version of the program, a plethora of new
applications are feasible. Two applications were shown in this thesis.

In the first application, massive planar and non-planar six- and
seven-propagator integrals with four external legs were computed. It was
shown that the method is in principle independent of the number of
involved scales by providing numerical results for a planar four-point
seven-propagator diagram with internal and external lines all massive.

Furthermore, and in contrast to analytical evaluation techniques, adding
massive lines to a topology proved beneficial in terms of evaluation
times. This was shown for two of the most complicated massive non-planar
seven-propagator double box integrals, entering in the NNLO prediction
for top-quark pair production. Differing in the massive sub-loop
topology, the singularity structure of the diagram involved in the heavy
fermionic corrections is significantly simpler than the diagram entering
the light fermionic corrections. While the custom SecDec setup can be
used in the computation of the former, the latter diagram challenges the
automated setup of the program. Therefore, an analytical preparation of
this diagram prior to its treatment in SecDec was explored, leading to
an improved numerical behavior.

A systematic improvement of the numerical convergence can be achieved
through a reduction in the number of integration parameters involved and
the elimination of spurious divergences. The analytical preparation of
the massive non-planar box diagram entering the light fermionic
corrections follows these aims, reducing the number of involved
parameters by integration of one Feynman parameter in a sub-loop of the
integral. Hereby, singularities are mapped to both ends of the
integration region and spurious linear singularities appear in pairs.
While a remapping cures the occurrence of singularities at both
endpoints of the integration region, a newly introduced backwards
transformation serves in distributing the linear divergences more evenly
among the Feynman parameters, thereby achieving a total reduction by two
thirds in the number of functions to be integrated numerically.

In its second application, the new features of the program are
demonstrated in the computation of the the leading momentum-dependent
two-loop QCD corrections to the masses of neutral @xmath -even
Higgs-bosons in the MSSM. These are obtained by calculating the
corresponding contributions to the dressed Higgs-boson propagators using
the Feynman-diagrammatic approach and adopting a mixed on-shell/ @xmath
renormalization scheme.

A revised two-loop mass and field renormalization has to be carried out
for the mass of the neutral Higgs-bosons to cancel the additional
divergences arising from incorporating the momentum dependence. An
additional shift with respect to previous calculations of the order
@xmath at zero momentum transfer appears from evaluating the @xmath -odd
Higgs-boson mass counter-term at its pole mass, @xmath .

The effect of the new momentum-dependent two-loop corrections on the
predictions for the @xmath -even Higgs boson masses were analyzed
numerically. The program SecDec is used in the evaluation of the finite
parts of analytically unaccessible two-loop integrals with several mass
scales.

The obtained mass shifts of the light @xmath -even Higgs-boson mass
exhibit an overall strong dependence on the mass of the gluino. For
values of @xmath corrections of about @xmath are found, at the level of
the anticipated future ILC accuracy. For very large gluino masses,
@xmath , on the other hand, substantially larger corrections are found,
at the level of the current experimental accuracy at the LHC. Additional
shifts in the heavy @xmath -even Higgs-boson mass, are mostly below
current and future anticipated accuracies. Only close to thresholds,
e.g. around @xmath , the corrections are larger but do not exceed 0,2%.
The new momentum-dependent two-loop contributions have been incorporated
into the program FeynHiggs .

In conclusion, the leading momentum-dependent two-loop QCD corrections
to the neutral @xmath -even Higgs-boson masses should be taken into
account in precision analyses interpreting the discovered scalar
particle as a Higgs-boson in the MSSM.

Especially in this last application of the upgraded version of the
program SecDec within this thesis, it was shown that the program can be
used in the computation of phenomenological quantities. Yet, for an
application where Monte Carlo sampling of an amplitude at millions of
phase-space points is required, the speed in the evaluation has to be
improved.

Nonetheless, in its new version SecDec has proven an invaluable tool in
independent checks to analytical calculations of very complicated
two-loop topologies involving masses and multiple scales.

Based on the observation that the numerical evaluation in SecDec is
rather limited in the cases of diagrams with very complicated
singularity structures than limited by the number of mass scales
involved, this numerical method forms a vital counter part to purely
analytical approaches. Its extremely high potential needs to be
exploited further in the future. Considering the fact that the method is
very suitable for intense parallelization, the program has the potential
to be further applicable in a multitude of higher-order corrections in
quantum field theories.

## Chapter \thechapter Appendix

## Appendix A Analytical formulae

All following formulae are based on Refs. [ 328 , 337 , 329 , 330 , 331
, 332 , 334 , 335 ] and were either recalculated analytically or checked
numerically with SecDec or Golem95 [ 188 , 336 ] .

The prefactor at loop order @xmath reads

  -- -------- -- -------
     @xmath      (243)
  -- -------- -- -------

where the dimension @xmath contains the dimensional regulator @xmath .
The fully symmetric Källén function reads

  -- -------- -- -------
     @xmath      (244)
  -- -------- -- -------

The following definitions facilitate numerical stability of analytic
results. They are based on work of, e.g., Refs. [ 332 , 280 ] .

  -- -- -- -------
           (245)
  -- -- -- -------

where

  -- -------- -- -------
     @xmath      (246)
     @xmath      (247)
  -- -------- -- -------

Furthermore, the Riemann sheet of the logarithm can be chosen explicitly
as

  -- -------- -- -------
     @xmath      (248)
  -- -------- -- -------

or

  -- -------- -- -------
     @xmath      (249)
  -- -------- -- -------

Similarly, two choices for an analytical continuation of the dilogarithm
read

  -- -------- -- -------
     @xmath      (250)
     @xmath      (251)
  -- -------- -- -------

Furthermore, the following expressions can be abbreviated

  -- -------- -- -------
     @xmath      (252)
     @xmath      (253)
  -- -------- -- -------

#### a.1 One-loop representations

##### a.1.1 One-loop tadpole

The one-loop one-point function reads

  -- -------- -------- -- -------
     @xmath   @xmath      (254)
     @xmath   @xmath      (255)
     @xmath   @xmath      (256)
  -- -------- -------- -- -------

##### a.1.2 One-loop bubble

The one-loop two-point function is defined as follows

  -- -------- -------- -- -------
     @xmath   @xmath      (257)
     @xmath   @xmath      (258)
  -- -------- -------- -- -------

Special cases of the finite part read

  -- -------- -- -------
     @xmath      (259)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (260)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (261)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (262)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (263)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (264)
  -- -------- -- -------

  -- -------- -------- -- -------
     @xmath   @xmath      (265)
     @xmath   @xmath      (266)
  -- -------- -------- -- -------

For a general representation of the finite part @xmath , see Ref. [ 330
] . For the coefficient of order @xmath , see Refs. [ 332 , 329 , 330 ]
.

##### a.1.3 Derivative of the one-loop bubble

The derivative of the one-loop two-point function with respect to the
first mass reads

  -- -------- -------- -- -------
     @xmath   @xmath      (267)
     @xmath   @xmath      (268)
     @xmath   @xmath      (269)
  -- -------- -------- -- -------

It is finite as the divergent terms are independent of the masses.
Special cases read

  -- -------- -------- -- -------
     @xmath   @xmath      (270)
     @xmath   @xmath      (271)
     @xmath   @xmath      (272)
     @xmath   @xmath      (273)
  -- -------- -------- -- -------

For a representation of the general case with arbitrary masses, see Ref.
[ 332 , 329 ] .

#### a.2 Two-loop representations

##### a.2.1 Two-loop vacuum diagram

The two-loop three-propagator vacuum integral corresponding to the
diagram in Fig. 31(a) reads

  -- -------- -------- -- -------
     @xmath   @xmath      (274)
     @xmath   @xmath      
              @xmath      (275)
     @xmath   @xmath      
              @xmath      
              @xmath      (276)
  -- -------- -------- -- -------

A representation of the finite part @xmath is included in Ref. [ 331 ] .

##### a.2.2 Two-loop two-point three-propagator (sunrise) diagram

The two-loop two-point three-propagator function corresponding to the
diagram in Fig. 31(b) reads

  -- -------- -- -------
     @xmath      (277)
  -- -------- -- -------

The special case of one massless propagator reads

  -- -------- -------- -- -------
     @xmath   @xmath      (278)
              @xmath      (279)
  -- -------- -------- -- -------

A representation of the finite part @xmath is included in Ref. [ 331 ] .

##### a.2.3 Two-loop two-point four-propagator diagram

The two-loop two-point four-propagator function corresponding to the
diagram in Fig. 31(c) reads

  -- -- -------- -- -------
        @xmath      
        @xmath      (280)
  -- -- -------- -- -------

The divergent part can be expressed as

  -- -- -------- -- -------
        @xmath      
        @xmath      (281)
  -- -- -------- -- -------

A result for the finite part is not available in analytical form.

##### a.2.4 Two-loop two-point five-propagator diagram

The two-loop two-point five-propagator function corresponding to the
diagram in Fig. 31(d) reads

  -- -------- -- -------
     @xmath      (282)
     @xmath      (283)
     @xmath      (284)
  -- -------- -- -------

The divergent part can be expressed as

  -- -- -------- -- -------
        @xmath      
        @xmath      (285)
  -- -- -------- -- -------

A result for the finite part is not available in analytical form.

## Appendix B SecDec User Manual

#### b.1 Installation

The program can be downloaded from http://secdec.hepforge.org .
Unpacking the tar archive via ‘ tar xzvf SecDec-2.x.tar.gz ’ will create
a directory called SecDec-2.x with the subdirectories as described in
the previous section. Changing to the directory SecDec-2.x , the program
is installed by running ‘ ./install ’.
Prerequisites are Mathematica, version 6 or above, Perl (installed by
default on most Unix/Linux systems), a Fortran compiler (e.g. gfortran,
ifort) or a C @xmath compiler if the C @xmath option is used.
In order to use the program, the user only has to edit the two files
param*.input and template*.m . SecDec has three different setups, the
user might be interested in. This is the setup to compute standard loop
integrals, termed ‘Loop setup’ in the following, generalized parametric
functions, termed ‘General setup’ in what follows, and and functions
with a similar structure as loop integrals, referred to as ‘User-defined
setup’.

##### b.1.1 Loop setup

-    paramloop.input : (text file, Perl readable format)
    In this file the user needs to specify paths, the type of integrand,
    the desired order in @xmath , the output format, the parameters and
    kinematic values for numerical integration, the parameters for
    contour deformation and further options.

-    templateloop.m : (Mathematica syntax)
    Here enters the specification of the loop momenta and propagators,
    resp. of the topology; optionally a numerator different from 1,
    non-standard propagator powers, parameters to be split in the middle
    of the integration region and the space-time dimension.

##### b.1.2 General setup

-    param.input : (text file, Perl readable format)
    In this file the user needs to specify paths, the type of integrand,
    the symbols and dummy functions utilized in the template file, the
    desired order in @xmath , the output format, the parameters and
    values for numerical integration and further options.

-    Template.m : (Mathematica syntax)
    Here enters the specification of the integration variables, the
    factors of the integrand, variables to be split in the middle of the
    integration region and the space-time dimension.

##### b.1.3 User-defined setup

-    paramuserdefined.input : (text file, Perl readable format)
    In this file the user needs to specify paths, the type of integrand,
    the desired order in @xmath , the output format, the parameters and
    kinematic values for numerical integration, the parameters for
    contour deformation and further options.

-    templateuserdefined.m : (Mathematica syntax)
    Here enters the specification of the user-defined functions, the
    list of powers of the original propagators (optional), the rank of
    the integrand, the variables to be split in the middle of the
    integration region and the space-time dimension. As the file is read
    in by Mathematica, additional functions needed to evaluate the
    user-defined functions can be included without further specification
    in the parameter file.

#### b.2 Operation

1.  Change to the subdirectory loop for the calculation of a loop or a
    user-defined integral or to the subdirectory general to evaluate a
    more general parameter integral.

2.  Copy the files param*.input and template*.m to create your own
    parameter and template files myparamfile.input and mytemplatefile.m
    , respectively. These two files serve to define the integrand and
    the parameters for the numerical integration.

3.  Set the desired parameters in myparamfile.input and specify the
    integrand in mytemplatefile.m .

4.  Issue the command ‘ ./launch -p myparamfile.input -t
    mytemplatefile.m ’ in the shell. If you run the command with an
    additional ‘ -u ’ the user-defined setup is used.
    If you omit the option ‘ -p myparamfile.input ’, the file
    param.input ,
    paramloop.input or paramuserdefined.input will be taken as default,
    depending on the current directory (either general or loop ) and
    whether the user-defined setup was chosen by adding the ‘ -u ’.
    Likewise, if you omit the option ‘ -t mytemplatefile.m ’, the file
    Template.m , templateloop.m or templateuserdefined.m will be taken
    as default. If your files myparamfile.input, mytemplatefile.m are in
    a different directory, say, myworkingdir , use the option -d
    myworkingdir . The shell command then reads ‘ ./launch -d
    myworkingdir -p myparamfile.input -t mytemplatefile.m ’, executed
    from the directory SecDec/general or SecDec/loop .

5.  Collect the results. Depending on whether you have used a single
    machine or submitted the jobs to a cluster, the following actions
    will be performed:

    -   If the calculations are done sequentially on a single machine,
        the results will be collected automatically (via results.pl ,
        resultsloop.pl or
        resultsuserdefined.pl called by launch ). The output file will
        be displayed with the text editor specified in the
        myparamfile.input .

    -   If the jobs have been submitted to a cluster, execute the
        command ./results*.pl [-d myworkingdir -p myparamfile] when all
        jobs have finished. This will write the final results to files
        in the graph subdirectory specified in the input file.

6.  After the calculation and the collection of the results is
    completed, you can use the shell command ./launchclean[graph] to
    remove obsolete files.

It should be mentioned that the code starts working on the most
complicated pole structure, which also takes longest. When the jobs
expected to take longest are submitted to a cluster first, the time the
user has to wait for the results is minimized.

#### b.3 Program input parameters

The user manual is for loop diagrams; the input files in the
subdirectory general to compute more general parametric functions are
very similar.

For the computation of an arbitrary loop integral the user should switch
to the directory loop , copy the files paramloop.input and
templateloop.m and rename them arbitrarily to arrive at
myparamloop.input and mytemplateloop.m , respectively. In the file
myparamloop.input which is written in Perl readable format, the user
then needs to name the graph to be computed and specify its number of
propagators, external legs and loops. Furthermore, the kinematic
invariants @xmath , @xmath and @xmath need to be given numeric values.
Apart from these definitions only one further flag ( cutconstruct=0 or 1
) needs to be set which decides how the user chooses to define the
propagators of the diagram at hand in the mytemplateloop.m file, compare
Sec. 18.2 for further explanations. All other parameters do not need to
be specified, default values will be chosen.

The following parameters can be specified:

  subdir  

    specifies the name of the subdirectory to which the graph should be
    written to. If it does not exist yet, it will be created. The
    specified subdir contains the directory specified in outputdir .

  outputdir  

    The name for the desired output directory can be given here by
    specifying the full path to the desired output directory. If
    outputdir is not specified, the default directory for the output
    will have the graph name (see below) appended to the directory
    subdir .

    The output directory will contain all the files produced during the
    decomposition, subtraction, expansion and numerical integration, and
    the results. The output of the decomposition into sectors is found
    in the outputdir directly. The functions from subtraction and
    expansion and the respective files for numerical integration are
    found in subdirectories. The latter are named by the pole structure
    and contain subdirectories named after the order in @xmath to which
    the Laurent coefficients contained in these folders contribute.

  graph  

    The name of the diagram or parametric function to be computed is
    specified here. The graph name can contain underscores and numbers,
    but should not contain commas.

  propagators  

    Here, the number of propagators the diagram has is specified. This
    specification is mandatory in the computation of loop integrals
    using the automated setup. When utilizing the user-defined setup,
    the number of propagators only needs to be specified if the exponent
    of the two Symanzik polynomials should be computed in an automated
    way.

  legs  

    The number of external legs the diagram has is specified here
    (mandatory).

  loops  

    The number of loops the diagram has is specified here (mandatory).

  cutconstruct  

    If the graph to be computed corresponds to a scalar integral, the
    integrand ( @xmath and @xmath ) can be constructed via topological
    cuts. In this case set cutconstruct=1 , the default is =0. If
    cutconstruct is switched on, the input for the graph structure (*.m
    file) is just a list of labels connecting vertices, as explained in
    Secs. 18.2 and B.4 .

  epsord  

    The order to which the Laurent series in @xmath should be expanded,
    starting from @xmath , can be specified here. The default is
    epsord=0 where the Laurent series is cut after finite part @xmath .
    If epsord is set to a negative value, only the pole coefficients up
    to this order are computed.

  prefactorflag  

    Possible values for the prefactorflag are 0 (default), 1 and 2.

    -   0: The default prefactor @xmath is factored out of the numerical
        result.

    -   1: The default prefactor @xmath is included in the numerical
        result.

    -   2: Give the desired prefactor in prefactor= to be factored out
        in the final result.

  prefactor  

    If option 2 has been chosen in the prefactorflag , write down the
    desired prefactor in Mathematica syntax. In combination with options
    0 or 1 in the prefactorflag this entry will be ignored. Use Nn ,
    Nloops and Dim to denote the number of propagators, loops and
    dimension ( Dim=4-2*eps by default).

  IBPflag  

    Set IBPflag=0 if the integration by parts option should not be used
    and =1 if it should be used. IBPflag=2 is designed to use IBP
    relations when it is more efficient to do so.

    Using the integrations by parts method takes more time in the
    subtraction and expansion step and generally results in more
    functions for numerical integration. However, it can be useful if
    (spurious) linear poles of the type @xmath are found in the
    decomposition, as it reduces the power of @xmath in the denominator.

  compiler  

    Choose a Fortran compiler (tested with gfortran, ifort, g77) if
    language=Fortran . Left blank, the default is gfortran.

  exeflag  

    The exeflag can be used to execute the program in steps within one
    calculation.

    -   0: The iterated sector decomposition is done and the scripts to
        do the subtraction, the expansion in epsilon, the creation of
        the Fortran/C @xmath files and to launch the numerical
        integration are created (scripts batch* in the subdirectory
        graph ) but not run. This can be useful if a cluster is
        available to run each pole structure on a different node.

    -   1: In addition to the steps done in 0, the subtraction and
        epsilon expansion is performed and the resulting functions are
        written to Fortran/C @xmath files.

    -   2: In addition to the steps done in 1, all the files needed for
        the numerical integration are created.

    -   3: In addition to the steps done in 2, the compilation of the
        Fortran/C @xmath files is launched to make the executables.

    -   4: In addition to the steps done in 3, the executables are run,
        either by batch submission or locally.

  clusterflag  

    The clusterflag determines how jobs are submitted. Setting
    clusterflag=0 (default) the jobs will run on a single machine,
    setting it =1 the jobs will run on a cluster (a batch system to
    submit jobs).

  batchsystem  

    If a cluster is used ( clusterflag=1 ), this flag should be set to 0
    to use the setup for the PBS (Portable batch system). If the flag is
    set to 1 a user-defined setup is activated. Currently this is the
    submission via condor , but it can be adapted to other batch systems
    by editing perlsrc/makejob.pm .

  maxjobs  

    When using a cluster, specify the maximum number of jobs allowed in
    the queue here.

  maxcput  

    Specify the estimated maximal CPU time (in hours). This option is
    only used to send a job to a particular queue on a batch system.

  pointname  

    The name of the point to calculate is specified here. It should be
    either blank or a string and is useful to label the result files in
    case of different runs for different numerical values of the
    Mandelstam variables, masses etc.

  sij  

    The values for Mandelstam invariants @xmath in numbers are specified
    here (mandatory). The @xmath should be @xmath in the Euclidean
    region.

  pi2  

    Massive external legs @xmath , @xmath ,… are specified here
    (mandatory). @xmath should be @xmath in the Euclidean region.
    Light-like external legs must be specified in the onshell=
    conditions in the mytemplatefile.m , see Sec. B.4 .

  ms2  

    Specify the masses of propagators @xmath , @xmath ,… using the
    notation ms[i] for @xmath (mandatory). The masses should not be
    complex numbers.

  integrator  

    The program for numerical integration can be chosen here. BASES (
    integrator=0 ) can only be used in the Fortran version. Vegas (
    integrator=1 ), Suave ( integrator=2 ), Divonne ( integrator=3 ,
    default) and Cuhre ( integrator=4 ) are part of the Cuba library and
    can be used in both the Fortran and the C @xmath version. In
    practice, Divonne usually gives the fastest results when using the C
    @xmath version. In the following we therefore concentrate on the
    adjustment of the parameters needed for numerical integration using
    Divonne. For more details about the Cuba parameters, the reader is
    referred to Ref. [ 302 ] .

  cubapath  

    The path to the Cuba library can be specified here. The default
    directory is [your path to SecDec]/Cuba-3.2 . Cuba -3.2 uses
    parallel processing during the numerical evaluation of the integral.
    The older version ( Cuba -2.1) is still supported and can be used.

  maxeval  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the maximal number of evaluations to be used by
    the numerical integrator for each order in @xmath . If maxeval is
    not equal to mineval , the maximal number of evaluations does not
    have to be reached.

  mineval  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the number of evaluations which should at least
    be done before the numerical integrator returns a result. The
    default is 0.

  epsrel  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the desired relative accuracy for the numerical
    evaluation.

  epsabs  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the desired absolute accuracy for the numerical
    evaluation. These values are particularly important when either the
    real or the imaginary part of an integral is close to zero. Note,
    epsabs= must be chosen smaller than the resulting values for the
    integral.

  cubaflags  

    Set the cuba verbosity flags. The default is 2 which means, the Cuba
    input parameters and other useful information, e.g. about numerical
    convergence, are echoed during the numerical integration.

  key1  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify key1 which determines the sampling to be used
    for the partitioning phase in Divonne. With a positive key1 , a
    Korobov quasi-random sample of key1 points is used. A key1 of about
    1000 (default) usually is a good choice.

  key2  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify key2 which determines the sampling to be used
    for the final integration phase in Divonne. With a positive key2 , a
    Korobov quasi-random sample is used. The default is key2=1 which
    means, the number of points needed to reach the prescribed accuracy
    is estimated by Divonne.

  key3  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the key3 to be used for the refinement phase in
    Divonne. Setting key3=1 (default), each subregion is split once
    more.

  maxpass  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify how good the convergence has to be during the
    partitioning phase until the program passes on to the main
    integration phase. A maxpass of 3 (default) is usually sufficient to
    get a quick and good result.

  border  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the border for the numerical integration. The
    points in the interval @xmath and @xmath are not included in the
    integration but are extrapolated from a few points of the excluded
    range. This can be useful if the integrand is known to be peaked
    close to endpoints of the integration variables.

  maxchisq  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the maximally allowed @xmath at the end of the
    numerical integration.

  mindeviation  

    Separated by commas and starting with the lowest order coefficient
    in @xmath , specify the deviation two sample averages in one region
    can show without being treated any further.

These parameters are advanced options:

  primarysectors  

    Specify a list of primary sectors to be treated here. If left blank,
    primarysectors defaults to all, i.e. primarysectors=1,…,N will be
    assumed, where @xmath is the number of propagators. This option is
    useful if a diagram has symmetries such that some primary sectors
    yield the same result.

  multiplicities  

    Specify the multiplicities of the primary sectors listed above. List
    the multiplicities in same order as the corresponding sectors above.
    If left blank, a default multiplicity of @xmath is set for each
    primary sector.

  infinitesectors  

    The alternative heuristic sector decomposition strategy, compare
    Sec. 12.2 , is activated when specifying, separated by commas, those
    primary sectors which should get a pre-decomposition. Writing
    infinitesectors=2,3
    results in the application of the alternative sector decomposition
    strategy to primary sectors 2 and 3. infinitesectors must be left
    empty for the default strategy to be applied to all primary sectors.

  togetherflag  

    This flag defines whether to integrate subsets of functions for each
    pole order separately togetherflag=0 (default) or to sum all
    functions for a certain pole order prior to integration
    togetherflag=1 . The latter will allow cancellations between
    different functions and thus give a more realistic error, but should
    not be used for complicated diagrams where the individual functions
    are very large.

  grouping  

    Even though togetherflag=0 is chosen, it can be beneficial to first
    sum a few functions before integrating them. Choosing a value for
    the grouping which is unequal to zero, defines how many bytes a
    summed function may have. The number of bytes is set by
    grouping=#bytes . Setting grouping=0 , all functions f*.f resp.
    f*.cc are integrated separately. In practice, a grouping=0 has
    proven to lead to faster convergence and more accurate results. When
    considering integrals where large cancellations among the different
    functions occur, the grouping value should be chosen @xmath . The
    log files *results*.log in the results directory contain the results
    from the individual sub-sector integration. They can be viewed to
    spot cancellations between the individual functions.

  editor  

    Choose here which editor should be used to display the result. If
    editor=none is set, the full result will not be displayed in an
    editor window at the end of the calculation.

  language  

    The choice between Fortran and C @xmath can be made in the
    myparam.input file by choosing either language=Cpp (default) or
    language=Fortran . For diagrams with purely Euclidean kinematics,
    both languages can be chosen. In combination with contourdef=True ,
    language=Cpp must be used, as the inclusion of an imaginary part in
    the result is implemented in C @xmath only.

  rescale  

    If all invariants are very small or very large it is beneficial to
    rescale them to reach faster convergence during numerical
    integration. The rescaling (scaling out the largest invariant in the
    numerical integration part) can be switched on with rescale=1 and
    switched off when set to 0 (default). If switched on, it is not
    possible to set explicit values for any non-zero invariant in the
    onshell= conditions in the Mathematica template file mytemplate.m .

  contourdef  

    The contour deformation can be switched on or off, by choosing
    contourdef=True/False in the input file myparamloop.input . For
    multi-scale problems, respectively diagrams with non-Euclidean
    kinematics, set contourdef=True (default is False). In this case, a
    deformation of the integration contour in the form of Eq. ( 125 ) is
    done. In addition to the functions f*.cc to be integrated, auxiliary
    files g*.cc are produced which serve to optimize the deformation for
    each integrand function.

  lambda  

    The initial value for @xmath can be set for the deformation of Eq. (
    125 ) by assigning a value to lambda= . The program takes the @xmath
    value given by the user in the myparam.input file as a starting
    point. During the checks listed in Sec. 17.3 , the appropriate value
    for @xmath is found automatically by the program. The user should
    pick an initial value which is rather too large than too small. A
    too large initial value for @xmath can easily be accommodated to
    have the right size during the computation, while a too small
    initial value can not be increased anymore. lambda=3.0 usually
    serves as a good initial value. Without any knowledge about the
    characteristics of the integrand, lambda=1.0 is a good choice. If
    the diagram contains mostly massless propagators and light-like
    legs, it can be useful to choose the initial @xmath larger (e.g.
    lambda=5.0 ), in order to compensate for cases where the remainders
    of the IR subtraction lead to large cancellations for @xmath . For
    diagrams with mostly massive propagators the initial lambda can be
    chosen smaller, e.g. lambda=0.1 .

  smalldefs  

    If the integrand is expected to be oscillatory and hence sensitive
    to small changes in the deformation parameter @xmath , smalldefs
    should be set to 1 (default is 0). If switched on, the argument of
    each sub-sector function @xmath is minimized.

  largedefs  

    If the integrand is expected to have (integrable) endpoint
    singularities at @xmath or 1, the deformation should be maximized.
    If largedefs=1 , the program maximizes the deformation. The default
    is largedefs=0 .

  optlamevals  

    The number of pre-samples to determine the optimal contour
    deformation parameter @xmath can be chosen by assigning a number to
    optlamevals= in the myparam.input . The default value is 4000.

#### b.4 Input for the definition of the integrand

The Mathematica input file should be called *.m . The following
parameters can be specified in Mathematica readable format

  momlist  

    If cutconstruct=0 is set in the input file, specify the names of the
    loop momenta here.

  proplist  

    Specify the diagram topology here (mandatory). The syntax for
    cutconstruct=1 is described in Section B.5 . If cutconstruct=0 has
    been chosen, the propagators have to be given explicitly. An example
    propagator list could be
    proplist= { k^2-ms[1],(k+p1)^2-ms[1] }
    with the loop momentum @xmath , the propagator mass @xmath and
    external momentum @xmath .

  numerator  

    If present, specify the numerator of the integrand here. If not
    given, a numerator= { 1 } is assumed. Please note that the option
    cutconstruct=1 is not available in combination with numerator
    functions.

  powerlist  

    The propagator powers must be specified. If all propagators are
    raised to the power one, an example syntax reads powerlist=Table[1,
    { i,Length[proplist] } ]; .

  onshell  

    Specify replacements for kinematic invariants here. The
    specification of light-like external legs is of specific importance
    in the generation of the correct integrand topology. These can be
    assigned by writing
    onshell= { ssp[1] -> 0 },
    where ssp[1] denotes the first external momentum squared, @xmath .
    The kinematic invariants can be assigned other specific values, e.g.
    ssp[1] -> 0.25 . Furthermore, relations among the invariants can be
    set (e.g. ssp[1] @xmath sp[1,3] ). This option can not be used in
    combination with rescale=1 .

  Dim  

    Set the space-time dimension. The default is Dim=4-2*eps . The
    symbol for the regulator, eps , must be kept.

  threshold  

    This option works in combination with C @xmath and contourdef=True .
    Specify a kinematical threshold condition above which, an imaginary
    part is expected. For the calculation below this threshold, the
    imaginary part is set to zero and the contour deformation parameter
    @xmath is decreased to a very small value. Set threshold=none ,
    remove or comment the line out if the threshold option should not be
    used (default). Usage of constants and kinematic invariants ms[i],
    ssp[i] and sp[i,j] is allowed. An example syntax reads threshold =
    sp[1,2] > 0; .

  splitlist  

    The integration region of those integrals over Feynman parameters
    @xmath , specified in a splitlist= { @xmath , @xmath , …, @xmath }
    in the mytemplatefile.m is split at @xmath and the resulting two
    integrals are remapped to the unit interval. The procedure follows
    the explanations of Sec. 18.7 .

#### b.5 Topology based construction of an integrand

The implementation of the topology based construction of an integrand in
the program SecDec is such that the user only has to label the external
momenta @xmath , the vertices @xmath and the masses @xmath of a graph.
It is selected by choosing cutconstruct=1 in the input file. If an
external momentum @xmath is part of a vertex, this vertex needs to carry
the label @xmath . The labeling of vertices containing only internal
lines is arbitrary. In the mytemplate.m file, the user has to specify
the proplist as a list of entries of the form @xmath , where @xmath is
the mass squared of the propagator connecting vertex @xmath and vertex
@xmath . The mass label @xmath must correspond the the @xmath th entry
of the list of masses given in paramloop.input . While @xmath needs to
be the number labeling the masses, @xmath (with @xmath being an integer)
can be left symbolic until numerical integration. If the mass is zero,
the user has to put @xmath , because this changes the singularity
structure at the level of the decomposition into sectors. More examples
can be found in the Mathematica template files templateP126.m,
templateBnp6*.m, templateJapNP.m, templateggtt*.m in the subdirectory
loop/demos . The original form of specifying the propagators by their
momenta and including numerators different from one, as done in SecDec
1.0 [ 301 ] , is still operational. To use the latter option, the flag
cutconstruct= must be set to zero.

#### b.6 Utilization of the user-defined setup

When the functions @xmath and @xmath are already known and the @xmath
-distribution is integrated out, a user-defined setup can be considered.
The integrand could then be defined in the mytemplateuserdefined.m file.
First, the two known Symanzik polynomials, the numerator and the rank of
the integral are written to the template file. Taking the diagram @xmath
as an example, compare Sec. 20.1 , the input in the
mytemplateuserdefined.m file reads
U [z_] := z[1]*z[2] + z[2]*z[3] + z[1]*z[4] + z[2]*z[4] + z[3]*z[4] +
z[1]*z[5] + z[2]*z[5] + z[3]*z[5] + z[1]*z[6] + z[2]*z[6] + z[3]*z[6]
F [z_] := -(sp[1, 2]*z[1]*z[2]*z[3]) - sp[1, 2]*z[1]*z[3]*z[4] - sp[1,
2]*z[1]*z[3]*z[5] - sp[1, 2]*z[2]*z[3]*z[5] - sp[1, 2]*z[1]*z[2]*z[6] -
sp[1, 2]*z[1]*z[3]*z[6] - sp[1, 2]*z[1]*z[5]*z[6] - sp[1,
2]*z[2]*z[5]*z[6] - sp[1, 2]*z[3]*z[5]*z[6] + (ms[1]*z[1] + ms[1]*z[2] +
ms[1]*z[3])* (z[1]*z[2] + z[2]*z[3] + z[1]*z[4] + z[2]*z[4] + z[3]*z[4]
+ z[1]*z[5] + z[2]*z[5] + z[3]*z[5] + z[1]*z[6] + z[2]*z[6] + z[3]*z[6])
Num = 1;
rank = 0;
The naming of these functions is arbitrary and only needed for a clearer
presentation of the functions in the functionlist , to be explained now.
The diagram @xmath has six propagators and is therefore assumed to have
six primary sectors after having integrated out the @xmath
-distribution. The primary sectors are counted in the first entry of
each function. In a more general application, the first entry just lists
those functions of equal exponents in the Symanzik polynomials which
should be grouped together. The second entry of each function in the
functionlist gives the exponents of each Feynman parameter, starting
with the exponent of @xmath . The total number of Feynman parameters
occurring here (and to be integrated over afterwards) is 5 because the
@xmath -distribution was already integrated out. The third entry in the
list is the function, corresponding to the first Symanzik polynomial in
the first primary sector, followed by its exponent and a flag whether
the function needs further sector decomposition or not. Here, @xmath
denotes that further decomposition is necessary while @xmath means that
the function is already fully decomposed into sectors.
functionlist = {
{1, {0,0,0,0,0}, {{(U[t]/.t[1]->1)/.t[6]->t[1], @xmath , @xmath },
{(F[t]/.t[1]->1)/.t[6]->t[1], @xmath , @xmath }, Num},
{2, {0,0,0,0,0}, {{(U[t]/.t[2]->1)/.t[6]->t[2], @xmath , @xmath },
{(F[t]/.t[2]->1)/.t[6]->t[2], @xmath , @xmath }, Num},
{3, {0,0,0,0,0}, {{(U[t]/.t[3]->1)/.t[6]->t[3], @xmath , @xmath },
{(F[t]/.t[3]->1)/.t[6]->t[3], @xmath , @xmath }, Num},
{4, {0,0,0,0,0}, {{(U[t]/.t[4]->1)/.t[6]->t[4], @xmath , @xmath },
{(F[t]/.t[4]->1)/.t[6]->t[4], @xmath , @xmath }, Num},
{5, {0,0,0,0,0}, {{(U[t]/.t[5]->1)/.t[6]->t[5], @xmath , @xmath },
{(F[t]/.t[5]->1)/.t[6]->t[5], @xmath , @xmath }, Num},
{6, {0,0,0,0,0}, {{U[t]/.t[6]->1, @xmath , @xmath }, {F[t]/.t[6]->1,
@xmath , @xmath }, Num}};
Choosing @xmath and @xmath , the exponents of the functions @xmath and
@xmath , respectively, are computed automatically by the program using
the information about the number of loops, the number of propagators,
their powers, the rank and the space-time dimension. To run this
example, from the loop directory, issue the command ‘ ./launch -p
paramuserdefined.input -t templateuserdefined.m -u ’. The demo files
paramuserdefined.m and templateuserdefined.m in the loop directory come
with the code.

#### b.7 Looping over ranges of parameters

A looping over ranges of parameters in SecDec is put into practice using
the perl script multinumerics* , where the * stands either for loop.pl ,
userdefined.pl or just .pl , depending on whether a standard loop, a
user-defined integral or a more general parametric integral should be
computed for ranges of parameters. The sets of parameters to be
evaluated are specified in a text file mymultiparam.input in
myworkingdir , to be read by the multinumerics* -script.

The following information must be contained in this textfile:

-    paramfile = myparam.input : Specify the name of the parameter file
    containing the graph info.

-    pointname = my_prefix (optional): Specify a name which is used as
    prefix to each kinematical point. To distinguish different
    kinematical points, each gets a different name using the prefix and
    sequential number, e.g. my_prefix1 for the first point, my_prefix2
    for the second and so on.

-    lines = @xmath (optional): Specify the number @xmath of points you
    wish to calculate - if omitted all points (listed in separate lines)
    will be calculated.

-    xplot = @xmath (optional): If this option is set, a tab-separated
    data file is written with a variable of choice in the first column,
    the numeric result for the real part in the second column, the
    uncertainty in the third column, the numeric result for the
    imaginary part in the fourth column and its uncertainty in the
    fifth. The integer @xmath defines the number of the column in the
    multiparam.input file containing the values which should be stored
    in the first column of the data file (default is 1). The resulting
    data files can directly be used for producing plots with, e.g.,
    gnuplot.

The subsequent specifications are different for parametric integrals and
loop (or similar) integrals.
multinumericsloop.pl and multinumericsuserdefined.pl :
When computing either a standard loop or a user-defined integral, the
number of values given for @xmath , @xmath and @xmath need to be
specified by
numsij=
numpi2=
numms2= .
Numerical values of the parameters for each point to calculate need to
ensue these definitions. Examples come with the code of the program, one
is found in
loop/demos/multiparam.input . The perl script helpmulti.pl can be used
to generate multiparam.input files automatically, to avoid typing large
sets of numerical values.
multinumerics.pl :
When computing a more general parametric function, depending, for
instance, on the two symbols @xmath and @xmath (defined as such in the
param.input file), values for these can either be specified explicitly
in the multiparam.input file as
values1=0.1,0.2,0.4
values2=0.1,0.3,0.6 ,
where values1 is linked to the first symbol specified in the param.input
file, values2 to the second symbol and so on. Or, if the user desires to
calculate the integrand for values of parameters at incremental steps,
the following syntax applies
minvals=0.1,0.1
maxvals=0.3,0.7
stepvals=0.1,0.2 .
This input would calculate each combination of @xmath and @xmath , where
@xmath and @xmath are specified first and second in the list of symbols
in the param.input file. Further examples can be found in
general/demos/multiparam.input .

Before executing the script multinumerics* , the functions generated by
Mathematica must already be in place. The simplest way to guarantee this
is to run the launch script, choosing exeflag=1 in the myparamfile.input
and subsequently issue the command
‘./multinumerics* [-d myworkingdir -p multiparamfile]’ .
In the single-machine mode ( clusterflag=0 ) all integrations are
performed sequentially, in the batch mode, they are run in parallel.
Running the script again with the additional argument “1” as
‘./multinumerics* [-d myworkingdir -p multiparamfile] 1’ ,
all results are collated before writing the output as *.out files into
the graph directory specified in the myparamfile.input . If specified in
the multiparam.input file, an additional *.gpdat file is written,
containing the results of all computed points.
The script generates a parameter file for each numerical point
calculated. To remove such intermediate parameter files, issue the
command
‘./multinumerics* [-d myworkingdir -p multiparamfile] 2’ .
This should only be done after the results have been collected.

#### b.8 Leaving functions implicit during the algebraic part

To use this option, the Mathematica template file can contain a function
which is left undefined, but needs to be listed with the option dummys=
in the myparam.input file. If symbolic parameters are used in addition,
these do not need to be listed as arguments of the implicit function.
Once the template and parameter files are set up, the functions need to
be defined explicitly so that they can be used in the calculation. The
simplest way to do this is to prepare a Mathematica syntax file for each
implicit function, and place them in the output directory specified as
outputdir= in the myparam.input file. For a function named dum1 of two
variables, defined as @xmath , the following lines would need to be
inserted in a file dum1.m named after the dummy function
@xmath
where @xmath can be replaced by any variable name you wish, as long as
they are used consistently in dum1.m . Once these Mathematica files are
in place, issue the command
‘ createdummyfortran.pl [-d myworkingdir -p myparamfile] ’ .
This triggers the generation of the necessary Fortran files for the
user-defined dummy functions. The files are stored in the same
subdirectory as the originals.
It is equally possible to write these Fortran files oneself instead of
having them generated by the program, although the automated procedure
is recommended. An example of this can be found in the directory
general/demos , comprised in the files paramdummy.input ,
templatedummy.m and in the directory /testdummy .