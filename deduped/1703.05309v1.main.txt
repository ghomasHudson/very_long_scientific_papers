## Chapter 1 Introduction to Linear Optical Quantum Computing

### 1.1 Synopsis

In the 20th century the first quantum revolution occurred. Due to most
basic quantum effects, namely wave-particle duality and quantisation,
new technologies emerged that revolutionised the world as we know it.
Some of these technologies are lasers, transistors (which allow for
computers), magnetic resonance imaging, and the electron microscope. Due
to these quantum based technologies human civilisation has experienced
the most rapid cultural changes over the last century than ever before.
These technologies however only harness the most basic aspects of the
quantum world. There are much more subtle quantum effects like quantum
entanglement that promise a whole new era of human civilisation and
perhaps even larger changes as we advance through the 21st century. One
of the most promising of these new technologies is the quantum computer
as it promises significant advantages over ordinary classical computing.
There are many architectures for building a quantum computer but they
are all technically very challenging. A useful approach is to build a
quantum computer optically as it has many advantages and is the focus of
this thesis.

In section 1.2 we will motivate a particular type of quantum computer
called a linear optical quantum computer (LOQC) and provide a brief
history of the field of quantum computing. In section 1.3 we will review
quantum computing in the context of linear optics, discussing the
requirements of quantum computing. In section 1.4 we discuss what
classes of problems quantum computers are known to show advantages over
classical computers. In section 1.5 we review some basic computational
complexity classes in computer science that are important to this
thesis. Lastly, in section 1.6 , we explain why linear optical quantum
computing remains challenging.

### 1.2 Motivation

Quantum Computers are information processing devices that utilise the
laws of quantum mechanics to process information. It was the famous
physicist Richard Feynman who first postulated the idea of a quantum
computer. Since then quantum computing has inspired a huge field of
research commonly referred to as quantum information processing [ 1 ] .
For a quantum computer to utilise the laws of quantum mechanics the
quantum computer requires the use of various quantum objects like atoms
and photons. These objects are some of nature’s smallest and most
fundamental building blocks and so they are extremely sensitive to being
disturbed by their environment which is a greatly simplified reason of
why realising a quantum computer is extremely challenging.

Many different physical architectures for building a quantum computer
have been proposed. Some of these proposals use atom and ion traps [ 2 ,
3 ] , superconducting qubits [ 4 ] , nuclear magnetic resonance [ 5 ] ,
quantum dots [ 6 ] , nuclear spin [ 7 ] , and linear optical
interferometers [ 8 ] . Which of these models is likely to yield the
first quantum computer? It is likely not just one, but a composite of
these technologies that will be used. Whatever technology is used, we
will need something that is efficient, by which we mean that the
resources used by the quantum computer scale polynomially with the size
of the computation. Likewise, if the quantum computer is inefficient we
mean that the resources required scale exponentially with the size of
the computation. Inefficient devices by definition will not be scalable
and will be too costly to realise.

Optical quantum information processing is one promising candidate. It
has a particularly useful advantage over many other proposals since it
harnesses light which is highly resistant to certain forms of
decoherence. Looking into the history of linear optics for quantum
computing, we find that it was not always believed to be a viable method
for building a quantum computer. The physics community has investigated
linear optical interferometers to process quantum information for quite
some time. Before 2001 it was believed that a linear optical
interferometer alone could not be engineered to make a universal quantum
computer. For example, in 1993 C̆erný proposed that a linear
interferometer could be used to solve NP -complete problems in
polynomial time, but he found that the scheme suffered from an
exponential overhead in energy [ 9 ] , which is inefficient. In 1996
Clauser & Dowling showed that a linear optics Talbot interferometer
could factor integers in polynomial time, which is efficient, but with
either an exponential overhead in energy or physical size [ 10 ] , which
is inefficient. Again in 1996, Cerf, Adami & Kwiat demonstrated a
programmable linear optical interferometer that could implement any
universal logic gate (a requirement for universal quantum computing that
we talk about in section 1.3 ) with single photon inputs but this
suffered an exponential overhead in the spatial dimension which makes
the scheme inefficient. In 2002, Bartlett et al. showed that any
interferometer with Gaussian states at the input and with Gaussian
measurements at the output can be efficiently simulated classically even
in the presence of quadratic nonlinearities which created a continuous
variable analog of the Gottesman-Knill theorem for discrete variables in
the ordinary circuit quantum computation model [ 11 ] .

These examples, among others, led to the widespread belief that photonic
linear interferometry alone could not be used to build a universal
quantum computer. As a corollary it was believed that linear optical
interferometers were efficiently simulateable on an ordinary classical
computer. However, in 2001, Knill, Laflamme & Milburn (KLM) [ 8 ] showed
that efficient universal quantum computation can be implemented using
only photon sources, beam splitters, phase shifters, photo-detectors,
and feedback from the photo-detector’s proving that linear optical
interferometers are a viable architecture. This is known as linear
optical quantum computing (LOQC).

There are many models for processing quantum information. These models
include cluster (or graph) states [ 12 , 13 ] , topological [ 14 ] ,
adiabatic [ 15 ] , quantum random walks [ 16 ] , quantum Turing machines
[ 17 ] , permutational [ 18 ] , the one-clean qubit model [ 19 ] and the
gate model [ 1 ] . Some of these models are universal , meaning they can
efficiently implement any quantum algorithm, whereas others are
restricted , meaning they implement a specific subset of quantum
algorithms. There are always errors in these schemes, such as
decoherence, so the scheme needs to also be made fault-tolerant, which
means that a protocol for quantum error correction is implementable. The
gate model is perhaps the most intuitive and most familiar model as it
resembles classical circuit models so we will use this model to describe
LOQC. In chapters 3 , 4 , 5 , 6 , and 7 we discuss the results we have
on BosonSampling , which is a special purpose implementation of LOQC and
the main focus of this thesis.

### 1.3 Linear Optical Quantum Computing

In this section we introduce the LOQC approach to universal quantum
computing of KLM [ 8 , 20 ] . As mentioned above it is a promising route
forward for realising a universal quantum computer and many researchers
around the world are constantly improving the associated technologies of
linear optics. These technologies also promise a simple implementation
of BosonSampling , which is the primary focus of this work and
facilitates development of key technologies that will ultimately become
building blocks for universal LOQC.

There are three main results in the original work of KLM [ 8 ] .
Firstly, they showed that linear optical elements are sufficient for
efficient non-deterministic universal quantum computation with photons.
Secondly, the success probability of implementing the quantum gates may
be made asymptotically close to unity using a certain encoding
technique. Thirdly, the resources for obtaining accurately encoded
qubits scale efficiently using quantum coding. In addition they show
that by iterating their method LOQC can be made to be fault-tolerant [
21 , 22 , 23 , 24 ] .

In this section we review some of the fundamental requirements for
quantum computing including the photonic qubit, some of the optical
elements such as the beam splitter, phase shifter, and photo-detectors,
logic gates, the nonlinear sign-flip gate, and quatum gate
teleportation. In addition we discuss qubits on the Bloch sphere.

#### 1.3.1 Photonic Qubit

The fundamental unit of information of a quantum computer is the qubit
or quantum bit . This is analogous to the classical bit but is instead
quantum in nature and can be in a superposition of the logical zero
state @xmath and the logical one state @xmath as well as have arbitrary
phase relationships. A qubit can also be entangled with other qubits.
These properties of qubits, superposition and entanglement, give quantum
computers significant advantages over classical computers in solving
many algorithms. A popular encoding of a qubit in LOQC is one photon in
two optical modes, known as dual rail encoding. Another technique is
polarization encoding where the qubit is encoded in the horizontal and
vertical polarizations of light. Mathematically, a qubit is represented
as a unit vector in the complex two-dimensional vector space @xmath .
The vector space is spanned by the basis

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (1.1)
     @xmath   @xmath   @xmath      (1.2)
  -- -------- -------- -------- -- -------

A general quantum state may be then written as a normalised complex sum
of these basis states, i.e.

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

By setting @xmath and @xmath a particular qubit is obtained which has
the form

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

This superposition state has a 50% probability of being in logical state
zero @xmath and a 50% probability of being in logical state one @xmath
once a measurement is made on the system in the @xmath logical basis.
The probability of measuring a particular logical state is obtained by
taking the absolute square of the corresponding amplitude.

#### 1.3.2 Qubit Visualisation on a Bloch Sphere

A elegant way to visualize superposition states is to draw them on the
so called Bloch sphere as shown in Fig. 1.1 . The @xmath component of
the Bloch sphere represents the @xmath state while the @xmath component
represents the @xmath state. A qubit @xmath drawn on the Bloch sphere is
in a superposition of the @xmath and @xmath if it is pointing anywhere
except along the @xmath axes, where the state is a @xmath or @xmath
respectively. Pure states lie on the surface of the sphere while mixed
states are contained within the sphere. Once the qubit is measured in
the logical (i.e @xmath ) basis the quantum state will probabilistically
collapse to either the @xmath or @xmath .

#### 1.3.3 Optical Elements for Quantum Computing

There are certain optical elements necessary to implement linear optical
quantum computing including: beamsplitters, phase-shifters,
photodetectors, and feedforward from the photodetector outputs. With
these elements, it has been shown that [ 8 ] ,

1.  Universal quantum computation can be implemented
    non-deterministically.

2.  The probability of implementing non-deterministic quantum gates can
    be made asymptotically close to unity using an encoding technique
    with efficient overhead [ 25 ] .

3.  Error correcting codes can be implemented, enabling fault-tolerant
    quantum computation.

4.  Quantum computation can be efficiently implemented.

In general, to build a quantum computer three essential components are
required: a way to prepare quantum states, a way to implement any
operation from a universal gate set on the qubits, and to measure
quantum states. Using linear optical approaches we prepare a single
photon in the Fock basis. There are several technologies that exist such
as spontaneous parametric down conversion [ 26 ] and quantum dot sources
[ 27 ] to prepare single photons, which can be described by adding a
photon to the vacuum state @xmath . This is non-deterministic with all
existing methods of generating single photon states but even with this
non-determinism it may be used for quantum computing. As technologies
improve single-photon sources are becoming more and more deterministic.

The two core optical elements that are used to implement optical gates
on our prepared quantum states are phase-shifters and beamsplitters,
which are both unitary transformations on a qubit.

##### Phase-Shifter

The unitary for a phase-shifter acting on a single mode is

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

where @xmath is the number operator defined as @xmath .

##### Beamsplitter and Hong-Ou-Mandel effect

The unitary matrix for a beamsplitter may be represented by

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

in the basis of optical modes, where @xmath gives relative phase between
the modes and @xmath is related to the reflectivity of the beamsplitter,
which is @xmath . One of the most important quantum interference effects
in quantum optics is the Hong-Ou-Mandel (HOM) effect [ 28 ] . This
effect is can be seen when identical single photons are incident on both
of the input modes of a 50:50 beamsplitter at the same time (i.e. @xmath
). At the output you see the HOM effect where the photons always tend to
bunch. In other words both photons will always come out of the same
output mode but you do not know which one it will be. This effect can be
described mathematically as,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (1.7)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Note the quantum interference, where the middle terms from line four to
line five cancel, causing the HOM effect.

##### Photon Measurement

In order to measure the state, photodetectors are used. They
destructively determine the number of photons in a mode. A bucket
detector is the simplest kind of detector which only measures if a mode
contains zero photons or more than zero photons. For states with more
than one photon a photon counting detector may be used to distinguish
numbers but it is harder to realise than a bucket detector. An effective
photon-number-counting detector referred to as a multiplexed
photodetector [ 29 , 30 , 31 , 32 , 33 ] may be realised with bucket
detectors by using a series of beamsplitters to evenly spread out the
photons over @xmath modes and then use bucket detectors at the output of
those modes. @xmath should be large enough that the photons are
sufficiently spread out so the probability of more than one photon going
into the same bucket detector is negligible. The probability of
under-counting given that the photon number is @xmath is at most @xmath
. When measuring a photon-number state projective measurements may be
used with measurement operators given by [ 1 ]

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

where @xmath is the photon number being measured.

#### 1.3.4 Logic Gates

There is a set of logic gates that are needed to manipulate qubits. We
think of these gates as performing operations on the qubit [ 34 ] . Some
of the most common gates are:

  -- -------- -- -------- -- -------
                 @xmath      (1.9)
                 @xmath      
                 @xmath      
                 @xmath      
                 @xmath      
                 @xmath      
     @xmath      @xmath      
  -- -------- -- -------- -- -------

These gates form a universal gate set. To be universal requires that any
unitary operation can be expressed as a finite sequence of gates from
the set, thus there are many universal gate sets that can be chosen. A
convenient way to visualise how some of these gates act on our quantum
state @xmath is to use the Bloch sphere as shown in Fig. 1.1 .

The first in this list of logic gates, the CNOT gate, is a maximally
entangling gate, which is the quantum equivalent of the classical XOR
gate. The latter gates are single qubit gates, which implement rotations
on the Bloch sphere. The Hadamard gate @xmath will take a logical @xmath
or @xmath and rotate them to a 50:50 superposition of @xmath and @xmath
with a relative phase difference. Specifically, @xmath and @xmath . The
Pauli-X gate @xmath , Pauli-Y gate @xmath , and Pauli-Z gate @xmath will
rotate the state around the @xmath , @xmath , and @xmath axes by @xmath
radians of the Bloch sphere respectively. The phase gate leaves the
@xmath state alone and maps @xmath . Similar to the phase gate, the
@xmath gate applies a phase to the @xmath state. An advantage of using
linear optics is that waveplates easily implement all of these single
qubit gates with polarisation encoded qubits [ 35 ] but a disadvantage
is that implementing the CNOT gate requires an effective Kerr
non-linearity which is quite challenging in optics.

#### 1.3.5 Nonlinear Sign-Flip Gate

One way to realise a CNOT gate is to use a nonlinear sign-flip (NS) gate
[ 8 ] which implements the transformation

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

and is used as a building block to implement the CNOT gate. The required
universal gate set to perform quantum computing is given by the two
qubit CNOT gate along with the previously discussed single qubit gates.
With a set of one- and two-qubit universal gates multi-qubit gates may
be constructed. With photons, linear optical elements, and
photodetection, the NS gate may be implemented but is non-deterministic.

#### 1.3.6 Quantum Gate Teleportation

With multiple non-deterministic gates in a quantum computing circuit,
the success probability of performing the computation drops
exponentially with the number of gates. Overcoming this exponential drop
may be achieved by using quantum gate teleportation which increases the
probability of successfully implementing non-deterministic gates [ 8 ,
36 ] . To do this, two Bell pairs (which are maximally entangled
two-qubit states) act as a resource to teleport the action of a gate
onto two qubits. This teleportation trick is also non-deterministic but
still increases the success probability of the non-deterministic gate
close to unity which enables efficient quantum computation since it can
be concatenated [ 8 ] .

### 1.4 Classes of Solvable Problems on a Quantum Computer

The reason people are interested in building a quantum computer is
because it promises to solve certain problems much more efficiently than
a classical computer can. So far, there are three provable classes of
problems in which a quantum computer outperforms classical computers.
These are the quantum Fourier transform (which forms the basis for
various other quantum algorithms), quantum search algorithms, and
quantum simulation.

#### 1.4.1 Quantum Fourier Transform

The first class is the quantum Fourier transform which is employed in
Shor’s factoring algorithm and discrete logarithms [ 37 ] . As an
example of the speedup that quantum Fourier transforms give us versus
classical Fourier transforms we consider a problem with @xmath numbers.
A classical fast Fourier transform requires @xmath steps but a quantum
computer does this in @xmath steps [ 34 ] , which is exponentially
faster and thus makes fast Fourier transforms efficient. Shor’s
factoring algorithm is an interesting example in terms of the impact it
may have. Many security protocols that secure records such as financial
transactions and perhaps government databases around the world rely on
the conviction that factoring a large number cannot be efficiently
performed on a classical computer. Shor’s factoring algorithm however
can factor large numbers efficiently and may put many security protocols
at risk. Luckily, there are quantum security protocols that rely on the
laws of quantum mechanics to secure information that even quantum
computers can not hack. This is known as quantum cryptography [ 38 ] .

#### 1.4.2 Quantum Search Algorithms

The second class, quantum search algorithms, make use of superposition
to decrease the time it takes to search an unstructured database. The
most famous example of such an algorithm was discovered by Grover [ 39 ]
. In his work he stated the problem to be a search of an unstructured
database of @xmath elements with the goal of finding an element that
satisfies a specific property. On a classical computer this search would
require @xmath operations, whilst a quantum search could accomplish this
in @xmath operations using Grover’s algorithm.

#### 1.4.3 Quantum Simulation

The third class is quantum simulation, where one simulates complex
interacting quantum systems with a quantum computer. The ramifications
of a practical quantum simulator are huge and would have direct
applications in condensed-matter physics, high-energy physics, atomic
physics, quantum chemistry, and cosmology among others [ 40 ] . For
example, in condensed matter physics, where one could study quantum
phase transitions, quantum magnetism, and high temperature
superconductivity (one of the most sought after open problems in
condensed matter physics). It intuitively makes sense that quantum
computers would be required to simulate quantum phenomena since quantum
systems are described with an exponentially increasing Hilbert space and
the number of available states in a quantum computer also increases as
exponentially, specifically as @xmath , where @xmath is the number of
qubits. Classical computers are good at simulating most classical
phenomena such as how air plane wings should be optimised for certain
parameters such as lift and reducing drag and so too should quantum
computers be used to simulate quantum phenomena. In general a classical
computer requires @xmath resources to simulate a typical quantum system
that has @xmath distinct components while a quantum computer requires
@xmath qubits and time.

### 1.5 Computational Complexity

Computer scientists have devised a way to analyse how difficult a
problem is to solve. It is based on the fact that algorithms are used to
solve certain problems and depending on the resources required to solve
a problem it is classified in a particular way. There are three relevant
classes of problems important to this thesis: decision problems,
counting problems, and sampling problems. Decision problems ask what is
the answer to a particular problem. Counting problems ask how many
solutions there are to a problem. Sampling problems ask what are the
samples drawn from some distribution. More specifically the
classification that a problem falls into may be characterized by a
complexity class where similar problems will fall into the same
complexity class. Here we summarize just a few important complexity
classes important for this thesis:

-    P : Decision problems solvable on a deterministic Turing machine in
    time that scales polynomially with the size of the system.

-    NP : Decision problems with potentially multiple solutions where
    any particular solution is verifiable in time that scales
    polynomially with the size of the system.

-    NP-Hard : The set of problems that can be reduced to any NP problem
    with at most polynomial resource overhead.

-    NP-Complete : The set of all problems that are in NP and NP-Hard .
    These contain the most difficult problems in NP .

-   @xmath P : The set of problems that count the number of solutions to
    a deterministic, polynomial time problem.

-   @xmath P-Hard : The set of problems that can be reduced to any
    @xmath P problem with polynomial resource overhead.

-   @xmath P-Complete : The set of all problems that are in @xmath P and
    @xmath P-Hard . These contain the most difficult problems in @xmath
    P .

-    PostBPP : The class of problems that may be solved on a
    probabilisitic classical computer with postselection in time that
    scales polynomially with the size of the system and with a given
    bounded error of @xmath .

-    BQP : The class of problems that may be solved by a universal
    quantum computer in time that scales polynomially with the size of
    the system.

-    PostBQP : The class of problems that may be solved on a universal
    quantum computer with postselection in time that scales polynomially
    with the size of the system and with a given bounded error of @xmath
    .

-    BosonSampling P : The class of sampling problems that can be
    described by the formalism of BosonSampling , which is the main
    focus of this thesis, and is summarized in Ch. 3 .

### 1.6 Why is Linear Optical Quantum Computing Hard?

So far we have made this linear optical implementation of quantum
computing seem quite simple, so how come we have not built a universal
linear optical quantum computer yet? There are a myriad of
technicalities spread through all of the different components and stages
of implementing the linear optical quantum computer. Some of these
include generating indistinguishable single photons, synchronising the
photons, mode-matching, fast controllable delay lines, fast-feedforward,
tuneable beamsplitters and phase-shifters, and accurate, fast,
single-photon detectors. Much of these technicalities are realisable
with current technologies but the success probability of a quantum
computation reduces exponentially with the number of photons at the
output; therefore, efficiencies need to be very high. Another major
problem is inefficiency, such as loss, which can happen anywhere within
the circuit. Also, with current efficiencies of photodetectors the
implementation of teleportation and the more complex two qubit gate
operations are challenging.

Even though quantum computing remains challenging and seems like a
distant dream there remain many interesting problems to investigate with
more frugal resource requirements. One such problem is BosonSampling
which is the topic of Chapters 3 , 4 , 5 , 6 , 7 , and 8 . BosonSampling
does away with the requirement of fast-feedforward, teleportation, and
number-resolved photodetectors yet still implements an interesting
problem since no classical computer can efficiently simulate
BosonSampling .

In the next chapter, however, we will review our work on quantum random
walks which may be implemented with optical techniques. Quantum random
walks are a route towards implementing quantum information processing
tasks [ 16 , 41 , 42 , 43 ] and are also an important aspect of
understanding BosonSampling as the bosons in BosonSampling are
undergoing a quantum random walk as they evolve. In fact BosonSampling
is equivalent to a quantum random walk, albeit a very complex one [ 44 ]
.

{savequote}

[45mm] It is the mark of an educated mind to be able to entertain a
thought without accepting it. \qauthor Aristotle

## Chapter 2 Quantum Random Walks on Congested Lattices and the Effect
of Dephasing

### 2.1 Synopsis

Quantum random walks are an important aspect of quantum computing. They
form the basis of several quantum algorithms that give speedups over
their classical counterparts such as in some oracular problems [ 45 ] ,
Grover’s search algorithm [ 39 ] , and the element distinctness problem
[ 46 ] . In this work we consider quantum random walks on congested
lattices and contrast them to classical random walks. Congestion is
modelled on lattices that contain static defects which reverse the
walker’s direction. We implement a dephasing process after each step
which allows us to smoothly interpolate between classical and quantum
random walks as well as study the effect of dephasing on the quantum
walk. Our key results show that a quantum walker escapes a finite
boundary dramatically faster than a classical walker and that this
advantage remains in the presence of heavily congested lattices. It
follows that quantum walks on congested lattices remain advantageous
over classical random walks.

In section 2.2 we provide some motivation for our work and give an
overview of what we have done. In section 2.3 we introduce quantum
random walks, write out a mathematical formalism, explain their
evolution, and introduce the metrics we use to characterise our quantum
random walk simulations which are variance and escape probability. In
section 2.4 we introduce lattice congestion in the random walks and show
our metrics for the walker on a congested lattice in both the classical
and quantum case. In section 2.5 we introduce a model of dephasing into
the quantum random walk and show how, with full dephasing, the quantum
walker behaves like a classical walker. In section 2.6 we show how our
metrics behave in the presence of both congestion and dephasing for the
quantum random walk.

### 2.2 Motivation

One route to implementing quantum information processing tasks with
quantum computing is via quantum random walks [ 16 , 41 , 42 , 43 ]
whereby a particle, such as a photon, ‘hops’ between the vertices in a
lattice. The effects of a congested, or obstructed, lattice on a quantum
random walk (QRW) are studied and compared to a classical random walk
(CRW). Congestion may also be thought of as traffic and the walker is
like a car trying to avoid the traffic. The quantum walkers also suffer
a dephasing process as they propagate. This study provides insight into
how random errors in the lattice and dephasing affect the dynamics of
random walks and the robustness of certain quantum features. In our
model, congestion refers to where the lattice through which the walker
propagates has defects, which are like blocked streets that the walker
encounters and has to back out of during the next step. These defects
are stationary during the evolution of the random walk, though we
average over many such random lattices. Dephasing occurs when the state
decoheres and is implemented via a dephasing channel acting after each
step. In the limit of full dephasing the QRW becomes a CRW, so that
dephasing also allows us to interpolate between the classical and
quantum regimes. For an experimental demonstration of dephasing in a QRW
see Broome et al. [ 47 ] , and for related theoretical work on QRWs with
phase damping see Lockhart et al. [ 48 ] .

For characterising the resulting probability distributions for QRWs and
CRWs we use variance and ‘escape probability’, that is the probability
that the walker escapes a finite region of the lattice, or more
picturesquely, the probability that the walker ‘beats the traffic’.

### 2.3 Quantum Random Walks

A QRW describes the evolution of a quantum particle through a given
topological structure. A common choice of structure is a @xmath
dimensional lattice. In a CRW, the walker probabilistically follows
edges through a lattice to step to an adjacent vertex. In a QRW on the
other hand, the walker spreads as a superposition of different paths
through the graph. Physically, the walker can be a wide range of quantum
particles, though of particular interest is the photon as photons are
readily produced, manipulated and measured using off-the-shelf
components in the laboratory. Photons have found widespread use in
quantum information processing, most notably linear optical quantum
computing (LOQC) [ 8 ] . The technologies required in LOQC provide the
topological structure for implementing a QRW. They also allow for
multi-photon QRWs [ 44 ] , which increases the dimensionality of the
walk. For a further review on QRWs see Refs. [ 16 , 41 , 42 , 43 ] , and
see Refs. [ 49 , 50 , 47 , 51 , 52 , 53 , 54 , 55 , 56 ] for the
numerous optical demonstrations of elementary QRWs that have been
performed.

#### 2.3.1 Quantum Random Walk Formalism

To illustrate our QRW formalism we present the details for a
one-dimensional discrete QRW on an unbounded lattice without any
defects. The state of a one-dimensional QRW at any given time has the
form,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where @xmath represents the position of the particle; @xmath represents
the size of the lattice; @xmath is the coin value, unique for each time
step, that tells the walker whether to evolve to the left ( @xmath ) or
right ( @xmath ); and @xmath is the probability amplitude for a given
position and coin value. The dimension of the lattice is @xmath . Since
there are two coin values for each position, the probability that the
walker is at position @xmath is given by,

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

The one-dimensional walker begins at some specified input state @xmath
before it begins to evolve at time @xmath , where @xmath and @xmath are
the starting position and coin values respectively. The state then
evolves for a finite number of time steps. The evolution is described by
two operators: the coin @xmath and step @xmath operators,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (2.3)
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- -------

The coin operator takes a state and maps it to a superposition of new
states using the Hadamard coin,

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

exploiting both possible degrees of freedom in the coin while
maintaining the same position. Next, the step operator @xmath moves the
walker to an adjacent position according to the value of @xmath . @xmath
and @xmath act on the state at every time step and thus the evolution of
the system after @xmath steps is given by,

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

If the walker begins at the origin or on an even lattice position then,
as the walker evolves, it lies on odd positions for odd time steps and
on even positions for even time steps. Thus, as the walker evolves, the
allowed locations for the walker oscillate between even and odd sites.

It is straightforward to generalise Eq. ( 2.1 ) to multiple dimensions
by expanding the Hilbert space. For example, a two-dimensional walk
would have the form,

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

where @xmath and @xmath denote the two spatial dimensions, @xmath
indicates for the walker to move left or right, @xmath indicates for the
walker to move down or up, and the superscript represents the dimension.
The dimension of the two-dimensional system is @xmath , when the lattice
is both dimensions consist of the same number of sites in each direction
from the origin as total time steps @xmath , which is the case for this
work. The coin and step operator can be generalised by taking a tensor
product for each respective dimension, or alternately a coin could be
employed which entangles the two dimensions. In the case of a spatially
separable two-dimensional coin one obtains @xmath and @xmath . Likewise,
the Hadamard coin for two dimensions becomes @xmath .

After the system evolves, a measurement is made on either the position
or the coin degree of freedom yielding the output probability
distribution. With this probability distribution various metrics can be
defined to characterise the evolution of the system, which we define
next.

#### 2.3.2 Random Walk Metrics

The two common metrics that we use to quantify a QRW are the variance
@xmath and the escape probability @xmath . All simulations done in this
work have the initial condition that the walker begins at the origin
@xmath . Also, all statistics are averaged over one hundred simulations
unless the walk was deterministic (i.e. there was no congestion or
dephasing introduced) in which case only one simulation was needed. The
sample space is exponential in size, and so averaging over an
exponential number of simulations is not tractable; however, one hundred
simulations for the size of systems we consider is sufficient for our
work because it produces stable statistics that converge to fixed values
and it smooths out the oscillations between data points.

##### Variance

The variance @xmath is a measure of how much the walker has spread out
during its evolution. It is defined as,

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where @xmath is the position probability distribution of the walker in
the spatial degree of freedom, @xmath is the number of lattice sites,
and @xmath is the mean of the distribution. For calculating the variance
in two-dimensions we take the variance of the marginal probability
distribution where the probability distribution becomes @xmath and
@xmath is the two-dimensional probability distribution. Fig. 2.1 a
illustrates the variance versus time for both a QRW and a CRW on a
two-dimensional square lattice of size @xmath . The QRW demonstrates a
quadratic rate of spreading across the lattice while the CRW
demonstrates a linear rate of spreading. This quadratic spreading is one
of the distinguishing features of a QRW compared to the CRW. It forms
the basis of some QRW algorithms such as the QRW search algorithm, which
is quadratically faster than the best corresponding classical algorithm.
For simulations of the variance we do not impose boundary conditions
because the walker never reaches the boundary.

##### Escape Probability

The escape probability @xmath is a measure of how much of the walker’s
amplitude leaks outside of a certain region in the lattice. To calculate
@xmath a boundary must first be defined which depends on the size of the
lattice. For the square two-dimensional lattice we let the walker begin
in the state @xmath and let the escape boundary be two vertical lines at
@xmath , where @xmath is the distance the escape boundary is from the
origin ( @xmath ). To calculate the escape probability on this square
lattice we use,

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

where @xmath is the two-dimensional version of Eq. ( 2.2 ).

Fig. 2.1 b illustrates @xmath versus @xmath for both a QRW and a CRW on
a square lattice of size @xmath with a boundary given by @xmath . Here
the QRW exhibits a dramatic jump in escape probability compared to the
CRW. This is due to both the faster rate of spreading of the QRW, and to
the QRW having larger amplitudes at the tails of its distribution. This
dramatic jump is a key feature pointed out in this work that
demonstrates an advantage that QRWs have over CRWs.

For all escape probability simulations the walker is allowed to walk
back into the unescaped region which subtracts from the probability that
the walker has escaped. This, in conjunction with the fact that the
walker occupies alternating even and odd positions as the walker
evolves, explains the oscillatory nature of the escape probability.

The two metrics, @xmath and @xmath , are closely related. If the walker
has a large spread in its distribution then the walker also has a better
chance to fall outside of the escape boundary, since it is centered
around the origin. At any given time step @xmath during the evolution we
can determine the probability distribution over the lattice with Eq. (
2.2 ) and then calculate these various metrics to be used for
quantifying a random walk.

Any non-deterministic distribution obtained in this work was obtained
using a Monte-Carlo averaging technique. Since the sample space we are
averaging over grows quadratically we are limited to about @xmath time
steps. Next, we demonstrate how to add spatial defects, which cause
congestion, into the walkers’ lattice and explore how the variance and
escape probability are affected by this lattice congestion.

### 2.4 Lattice Congestion

Lattice congestion is a model of defects in a medium. For the QRW and
CRW the medium is the walkers’ lattice and the defects are modelled as
blocked pathways where the walker has to enter the pathway to realise it
is blocked and then reverse out on the next step. This model is closely
related to percolation theory [ 57 ] which models defects as missing
lattice nodes. For a detailed introduction on percolation theory see [
58 , 59 ] . Percolation is generally modeled on a @xmath dimensional
lattice with a given geometry such as a square, triangle or honeycomb.
Regardless of geometry, the lattice consists of two components: and . A
site is a point on the lattice and a bond is the connection between the
sites. These components give two strategies for introducing the random
fluctuations that define percolation theory: site percolations and bond
percolations . In site percolation the lattice sites exist with
probability @xmath and when a site does not exist it is a defect in the
lattice. In bond percolation the positions in a lattice are fixed while
the bonds between the positions exist with probability @xmath . The
model in this work is a variant of site percolation whereby the walker
can occupy any site, but with probability @xmath will find an
obstruction and reverse direction upon hitting the respective site.

Percolation theory has an associated scaling hypothesis that predicts
critical values, such as percolation thresholds [ 60 ] , which we do not
reproduce in this work due to our small lattice sizes. Instead we
observe the behavior of QRWs on congested lattices and compare them to
CRWs. However, we expect the same percolation characteristics such as
percolation thresholds to exist in the underlying lattice that the
walkers are exploring. For a two-dimensional square lattice with site
percolations that most closely resemble the lattice used in this work,
the percolation threshold is @xmath [ 61 ] . Values of @xmath higher
than this threshold produce long-range connectedness in the lattice. We
make the comparison to percolation in this work because our spatial
defects are equivalent to the defects in percolation theory; however, we
do not observe the critical values that percolation theory predicts so
we call it congestion to avoid confusion.

To generate a lattice with spatial defects a matrix of coin operators is
constructed. The matrix is the same size as the lattice and each
position in the matrix corresponds to a spatial position on the lattice.
The coin operator corresponding to a given position then determines the
behaviour of the walker. The coin operators are defined as either a
Hadamard coin, Eq. ( 2.4 ), if the site is present, or a bit-flip coin,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

if the site contains a defect. For the two-dimensional case the bit-flip
coin becomes @xmath .

As the quantum or classical walker evolves it will walk into these
defects that signify congested points on the lattice. Upon reaching a
defect the walker reverses direction, thus slowing the walker’s rate of
spread. In this work we define @xmath as the probability that the site
is not a defect; therefore, the probability that a site is a defect is
@xmath .

#### 2.4.1 Classical Random Walk on a Congested Lattice

The lattices we are considering contain randomly distributed defects, or
points of congestion that impede the walker’s progress. Questions such
as what is the probability that there is an open path from one side of
the lattice to the other, are answered by percolation theory . There are
many known applications for percolation theory [ 62 ] . A common example
is asking whether a liquid can flow through a porous material. If enough
pores (or sites) exist then the liquid can make it through. Another
example is whether or not an electric current can flow through some
medium where conductive sites are spread throughout some insulator. If
enough conductive sites are present then a path will exist through the
medium in the asymptotic limit.

Within the congested lattice we examine the spread of walkers. Defects
have the effect of reducing the rate of spread of the walker, or
stopping it entirely if the lattice is so congested that there is no
escape possible from the region the walker finds itself in. Fig. 2.2 a
shows the variance @xmath of a CRW versus time @xmath in the presence of
varying values of congestion @xmath on a lattice of size @xmath . As the
congestion increases the classical walker becomes trapped. In each case
the variance preserves the linear dependence as is expected in a CRW.

In Fig. 2.2 b the escape probability @xmath of a CRW is shown versus
time @xmath in the presence of varying values of congestion @xmath on a
lattice of size @xmath and escape boundary @xmath . @xmath decreases as
congestion is increased but remains linear modulo the oscillations being
averaged out. Again there is a threshold where in terms of @xmath the
walker stops escaping the boundary and the lattice becomes insulating.

#### 2.4.2 Quantum Random Walk on a Congested Lattice

Classically, the state can only move in one direction at a time while
quantum mechanically the state spreads in a superposition of every
direction simultaneously. As with a classical walker, the quantum walker
escapes the bounded region more often if there are less defects. The
significance of the quantum walker is both the quadratic spreading
behaviour and the resulting probability distribution having more weight
in the tails. For a review of work done on QRWs with percolation see [
63 ] for asymptotic results and analytic solutions. See [ 64 ] for
quantum tunneling effects on a one-dimensional QRW and, for a
two-dimensional lattice, average distance measures and the order of
quadratic scaling. This work is unique from these two for several
reasons. First, properties of a QRW on congested lattices with the
@xmath and @xmath metrics were not studied previously. Second, we
compare QRWs to CRWs and observe whether QRWs maintain their advantages
over CRWs on congested lattices. Third, we tune the random walks on
congested lattices between being fully quantum and fully classical using
a dephasing process, described later in this chapter, which acts as an
error model that describes coupling of the walker to the outside
environment.

Fig. 2.3 a shows the variance @xmath versus time @xmath for a QRW with
varying values of congestion @xmath for @xmath . As congestion increases
the variance of the walker decreases; however, it retains its quadratic
(i.e. ballistic) spreading albeit with a different quadratic
coefficient. This property shows that QRWs remain advantageous over
CRWs, since the quantum walker spreads faster, even in the presence of
lattice defects.

Fig. 2.3 b shows the escape probability @xmath versus time @xmath for
varying values of congestion probability @xmath on a lattice of size
@xmath and boundary @xmath . For @xmath there is no congestion present
and the @xmath metric experiences a sudden jump from @xmath to @xmath .
This is because the QRW has most of its amplitude in its tails as it
evolves. When @xmath decreases and the lattice becomes more and more
congested the sudden jump is still present at the same value of @xmath
but with a much smaller amplitude. This shows that QRWs retain their
advantage over CRWs in the presence of heavy congestion. Note that the
percolation threshold is around @xmath , below which we expect that on
average there is no clear route across the graph.

#### 2.4.3 Varying Escape Boundary

In the previous simulations involving escape probability the escape
boundary was set to be near the initialised position of the walker. The
next topic we consider on a congested lattice is how the escape
probability on a congested lattice changes as @xmath varies. Consider
Fig. 2.4 which shows @xmath as a function of @xmath with varying values
of congestion @xmath for the CRW (a) and the QRW (b). Both walkers
evolve for @xmath steps and @xmath is calculated at @xmath . In both the
CRW and QRW @xmath reduces with increased congestion and when @xmath is
farther from the walker’s initial position. What is interesting is that
the QRW maintains a significantly larger @xmath than the CRW as the
escape boundary moves away.

### 2.5 Dephasing

Next, we consider what happens to a QRW subject to dephasing. Dephasing
represents decoherence caused by unwanted interaction with the
environment which can be related to measurement errors caused by thermal
fluctuations, white noise, photons interfering with the quantum walker,
etc. To explore this we first introduce a model of dephasing and
characterise it with our two metrics: variance and escape probability.

Consider a QRW where after each step, each basis state has probability
@xmath of acquiring a @xmath phase flip. We can model this process as
choosing to apply one of a set @xmath of unitary matrices covering all
the combinations of @xmath on the diagonal. If @xmath has @xmath -1’s on
the diagonal we choose it with probability @xmath .

The probability of a particular sequence of @xmath ’s will be the
product of the probabilities of the @xmath appearing since they are
independently chosen at each step. If @xmath is the final pure density
matrix appearing with probability @xmath , then in general the final
state of the system is described by,

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

For any POVM element @xmath we have,

  -- -- -- --------
           (2.11)
  -- -- -- --------

We algorithmically implement dephasing by randomly flipping the signs of
individual basis states in the walker’s state with probability @xmath ,
and average measurement results over a large number of independent
trials. This in effect samples from the distribution represented by
@xmath and is automatically weighted by the probability of a given
sequence.

That this whole process represents dephasing is not immediately obvious.
To see it, we first rewrite @xmath as the vector @xmath using the vec
operation which simply stacks its columns on top of each other [ 65 ] .
Using the identity @xmath for any three square matrices @xmath , @xmath
, and @xmath ; then grouping the terms that turn up, we can write,

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

where @xmath , @xmath represents the step and coin operations, and
@xmath is the vectorised initial density matrix. This shows that after
each step we apply the process described by the dynamical matrix,

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

The matrices @xmath are diagonal so we write the diagonal as a vector
denoted by @xmath , so that the diagonal of @xmath is @xmath . Since
@xmath has only real entries we can rearrange it into the matrix @xmath
. We can do a similar arrangement with @xmath so that,

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

It is worthwhile pausing and noting what this matrix represents. From
Eq. ( 2.12 ) we can see that the diagonal of @xmath multiplies the
elements of the vectorised @xmath . Hence when we arrange the values
into a matrix, the entries of @xmath multiply the corresponding entries
in @xmath .

The first thing to note is that this matrix is symmetric. We will denote
the entries of @xmath by @xmath and drop the reference @xmath for
clarity. The diagonals of @xmath are of the form @xmath and since @xmath
the diagonal of @xmath is unity and the process does not change the
amplitudes of the states. The off-diagonals are of the form @xmath where
@xmath and their sum over @xmath has the value,

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

The terms on the left are the probabilities that both @xmath and @xmath
are positive, both negative, or one of each respectively. Each of these
terms is multiplied by the binomial sum of the probabilities of all the
combinations of @xmath on all the other elements of @xmath and not
@xmath or @xmath , which evaluates to 1. Note that this result holds for
any dimension. In summary, the map that is performed by @xmath
multiplies every off-diagonal element of @xmath by @xmath . This is a
dephasing map.

If @xmath none of the signs are flipped, and if @xmath all of the signs
are flipped. Since the QRW is invariant under a global phase flip, these
two extremes reproduce an ideal QRW. When @xmath dephasing is introduced
into the system. A value of @xmath corresponds to complete dephasing
which causes the walker to behave classically. The classical results in
this work were produced by using our QRW code with a value of @xmath .
This was checked with purely classical simulations to verify that we are
indeed obtaining a CRW.

If we imagine an weak measurement of the QRW at every step where it is
projectively measured with probability @xmath or otherwise left alone,
this map would describe dephasing by a dynamical matrix which multiplies
all the off diagonal elements of @xmath by @xmath . So our dephasing
process is equivalent to a measurement performed with a probability
@xmath .

In this work, dephasing is a method for introducing quantum decoherence
to the QRW. To illustrate the effect of dephasing in our model we plot
the probability distribution at the final time @xmath of various random
walks in Fig. 2.5 . In Fig. 2.5 a the walk has no dephasing @xmath and
is thus completely deterministic. We see that this probability
distribution has one main peak near the positive @xmath and positive
@xmath direction, which is in the initialised direction of the coins,
and is at the edge of the lattice. This strong peak is the result of
constructive interference with walkers moving in this direction and
destructive interference with walkers moving in other directions [ 66 ]
. This is in contrast to what occurs when dephasing is introduced. Fig.
2.5 b shows the same evolution again but with a dephasing probability of
@xmath . With this value of dephasing the distribution retains most of
its quantum behaviour. Fig. 2.5 c shows the same evolution again but
with a dephasing probability of @xmath . With this value of dephasing
the probability distribution loses much of its quantum behaviour and
begins behaving like a CRW. Finally in Fig. 2.5 d we show the same
evolution but with @xmath and obtain the probability distribution of a
CRW.

We notice that with sufficiently strong dephasing the probability
distribution becomes localized around the origin so that the QRW behaves
like a CRW distribution. Note that the corresponding value of @xmath
that collapses the QRW to a CRW depends on @xmath . As @xmath increases
the underlying lattice has more sites where dephasing can occur and thus
a smaller @xmath will cause the corresponding collapse. By incrementing
@xmath we can smoothly interpolate between QRWs and CRWs, which is a key
feature of this work.

### 2.6 Congestion and Dephasing Combined

Next we combine congestion and dephasing and examine the joint effects.
Fig. 2.6 a shows the variance obtained at the final time step of the QRW
as a function of the congestion probability @xmath for varying values of
the dephasing probability @xmath on a two-dimensional square lattice of
size given by @xmath . A monotonic decrease is observed in the variance
for a given @xmath as @xmath is increased and a quadratic rate of
spreading is maintained for small values of @xmath . Fig. 2.6 b shows
@xmath with boundary @xmath as a function of congestion probability
@xmath for varying values of dephasing probabilities @xmath on a
two-dimensional square lattice defined by @xmath . When @xmath the walk
is fully quantum so more of the probability distribution escapes the
boundary. When dephasing is increased process errors are introduced,
reducing @xmath for any given value of @xmath .

### 2.7 Summary

Quantum random walks are a promising route towards quantum information
processing, exhibiting many unique features compared to the classical
random walk as motivated in Sec. 2.2 . We review quantum random walks in
Sec. 2.3 . Then we introduced a model for adding congestion to the
underlying lattice via the introduction of bit-flip coins in Sec. 2.4 .
Congestions inhibits the spread of the classical and quantum walker,
reducing the escape probability and variance metrics. We found that as a
quantum random walk evolves it will suddenly and dramatically escape a
finite boundary. It maintains this property even in the presence of
congestion.

We also introduce a dephasing error model in Sec. 2.5 . Dephasing errors
are errors on the quantum walker caused by coupling to the environment
as it evolves. In the limit of large dephasing the quantum random walk
spatially localises and behaves like a classical random walk. The spread
of the walker is sensitive to small amounts of dephasing in our
dephasing model and becomes more sensitive as the size of the lattice
increases. Dephasing also allows for a mapping between quantum and
classical walks, via the coin operator, to allow for a direct comparison
of the two.

We studied the effects of spatial defects and dephasing together on the
propagation of the walker in Sec. 2.6 and found a monotonic decrease is
observed in the variance and escape probability for a given congestion
probability as the dephasing probability is increased. Our results
indicate that a quantum walker on a lattice with defects and dephasing
still exhibit a quadratic rate of spreading. Thus, as the quadratic
spread of quantum walks is one of the key features that make them
applicable to quantum information processing applications, such as the
quantum search algorithm, we expect that quantum walkers on congested
lattices may retain their advantage over CRWs despite the congestion.

In the coming chapters we will look at BosonSampling , which can be
regarded as a particular type of multi-walker quantum walk that
simulates the output statistics of a linear optics network fed with
multiple single-photon states.

{savequote}

[45mm] If I have seen further it is by standing on the shoulders of
giants. \qauthor Sir Isaac Newton

## Chapter 3 An Introduction to BosonSampling

### 3.1 Synopsis

BosonSampling is a problem that studies the interference via linear
optics of many bosonic particles. It can be thought of as a simplified
model for quantum computing and it may hold the key to implementing the
first ever post-classical quantum simulation. BosonSampling is
significantly more straightforward to implement than any universal
quantum computer proposed so far.

We begin this chapter in section 3.2 by motivating BosonSampling and
discussing some of its history. We then, in section 3.3 , summarize the
BosonSampling formalism giving a simplistic and detailed model, discuss
what the permanent represents physically, and explain errors in
BosonSampling . In section 3.4 we discuss BosonSampling and the
implications it has on the extended Church-Turing thesis. In section 3.5
we discuss the feasibility of building a BosonSampling device with a
particular focus on photon sources, linear optical networks, and
photodetection. In section 3.6 we go into some applications inspired by
BosonSampling one of which was our own work and, later, go into
extensive detail in chapter 7 .

### 3.2 Motivation for BosonSampling

Aaronson & Arkhipov (AA) surprised the quantum optics community when
they argued that a passive linear optical interferometer with Fock state
inputs cannot likely be efficiently simulated by a classical computer [
67 ] . This has become known as the BosonSampling problem. Since the
first appearance of this work in 2010, research into BosonSampling has
exploded. There have been a number of experimental implementations that
utilize three photons from spontaneous parametric down conversion (SPDC)
sources [ 68 , 69 , 70 , 71 , 72 , 73 , 74 ] (although the validity of
one of these experiments are under debate as not all three photons were
heralded single photons [ 75 ] ). There have also been many theoretical
developments in BosonSampling that consider the effects of loss, noise,
decoherence, non-Fock inputs, scalability of SPDC sources, ion-trap
implementations, and so forth [ 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 ,
84 ] . We will discuss and summarise some of these results in the
sections and chapters below.

Why is BosonSampling attracting so much hype? What is it good for?
BosonSampling can be implemented on passive linear optical
interferometers and thus it may be physically implemented with
significantly reduced experimental overhead as compared to building a
universal quantum computer. Yet, it still implements a computationally
complex problem that no classical computer can efficiently simulate. It
is the first interesting example of a realistically implementable
post-classical computing problem although the potential of BosonSampling
is yet to be fully understood.

To be more specific about what is computationally hard in BosonSampling
for a classical computer to simulate we must take a look at the output
distribution. The output distribution has an exponentially large number
of possible output configurations which are sampled using photon-number
detectors. In addition to there being an exponentially large number of
output configurations, the sample is computed by solving a matrix
permanent, which has no known efficient algorithm unlike matrix
determinants. In other words one cannot predict the outcome with a
classical computer unless they were to wait an exponential amount of
time or use an exponential amount of resources.

A logical question to ask is weather a passive linear optical
interferometer can be used for anything interesting other than
BosonSampling ? BosonSampling itself has no known applications other
than being able to efficiently sample the distribution of bosons that a
classical computer cannot efficiently simulate. However, it was recently
shown by MORDOR that passive linear optics, which captures the essence
of BosonSampling , can be applied to quantum metrology [ 85 ] . We go
into this result in chapter 7 . It was also shown almost simultaneously
by Huh et al. that a modification of BosonSampling can be used for
simulating molecular vibronic spectra [ 86 ] .

BosonSampling in this respect is similar to Feynman’s work in the 1980s
which had hypothesized that an ordinary quantum computer could be used
to carry out certain physics simulations without the exponential
overhead required on a classical computer. This hypothesis was not
proven until much later in Lloyd’s work in 1996 [ 87 , 88 ] . The
Deutsch-Jozsa algorithm of 1992 was the first exponential speedup
advantage for a quantum computer but it solved a problem that had no
practical applications [ 89 ] . BosonSampling is similar to the
Deutsch-Jozsa algorithm in that it solves no interesting problem but is
a first example demonstration of passive linear optics doing something
of interest from a computational complexity perspective. It shows that
passive linear optical interferometers have some sort of hidden
computational power. So the real question now is can BosonSampling be
shown to be good for more things besides the BosonSampling problem and
the two BosonSampling inspired applications above? This potential is
what has captured the imagination of many researchers in the field
including my close collaborators and myself.

Gard, et al. independently reached the same conclusion as AA. They did
this in the context of simulating multi-photon coincidence counts at the
output of a linear optical implementation of a quantum random walk with
multi-photon walkers [ 90 ] . In follow up work, Gard et al. [ 91 ]
argued using a physical instead of a computational complexity point of
view that the difficulty to simulate such interferometers arose from two
necessary requirements: (1) The photons interact at the beamsplitters
via a Hong-Ou-Mandel effect that leads to an exponentially large Hilbert
space in the number-path degrees of freedom, which rules out a brute
force simulation; and (2) That the simulation of the interferometer is
linked to computing the permanent of a large matrix with complex
entries, a problem known to be in the complexity class #P -hard. It is
believed that this complexity class is intractable for classical
computers as well as for universal quantum computers [ 92 ] . The first
of these requirements is a necessary condition but is not sufficient to
imply an intractable simulation. As a counterexample, the
Gottesman-Knill theorem gives examples of classically simulatable
quantum circuits where gates in the Clifford algebra generate
exponentially large amounts of qubit entanglement. In some problems it
is known that shortcuts through exponential Hilbert space exist.
Intuitively, we expect that since BosonSampling is tied to solving
matrix permanents the computation is expected to not have a shortcut. In
contrast, the equivalent sampling problem with fermions has an
exponentially large Hilbert space but is known to be classically easy to
simulate since the problem relates to matrix determinants rather than
permanents, which are known to be in the complexity class P and
classically simulatable [ 91 ] since a shortcut through Hilbert space
exists using Gaussian elimination.

An important point to note is that in almost all work on BosonSampling ,
the interferometer is described as a passive linear optical device with
non-interacting bosons. However, we know that the Hong-Ou-Mandel effect
followed by a projective measurement imparts an effective nonlinearity
and so there is effectively an interaction between the indistinguishable
bosons at each beamsplitter [ 8 ] . This exchange interaction between
indistinguishable bosons arises simply from the multi-particle
wavefunction needing to be properly symmetrized. This effect can give
rise to quite noticeable effects. As an example, the bound state of the
neutral hydrogen molecule, which is the most common molecule in our
Universe, arises from a similar exchange interaction. It is therefore
technically incorrect to describe these interferometers as linear
devices with non-interacting bosons. The exchange interaction is just as
real as tagging on an additional term in a Hamiltonian. If one adds
post-selection in BosonSampling -like schemes an effective Kerr-like
nonlinearity is imparted between the bosons [ 93 ] , but BosonSampling
itself remains linear as described by AA.

True BosonSampling certification may be done to distinguish it from
uniform sampling or random-state sampling [ 94 , 95 , 96 , 97 , 98 ] .
Various sources of error including mode-mismatch, spectra of the bosons,
and spectral sensitivities of detectors of BosonSampling have been
studied [ 83 , 99 , 76 ] . This has led to a theory of interference with
partially indistinguishable particles, where any realistic imperfections
in the source and detectors can be completely characterized [ 100 , 101
] . Scalable implementations of BosonSampling in optical systems, ion
traps, and microwave cavities have been proposed [ 79 , 102 , 103 , 104
, 105 ] .

Another interesting observation is that there is evidence that
BosonSampling may not be efficiently distinguished from a classical
device that efficiently produces samples from a suitable distribution.
This interesting question leads to the possibility of a classical
certification of BosonSampling , which is still largely open [ 106 , 67
] . More specifically, BosonSampling is not known to reside in the
sampling-equivalent complexity class NP , i.e the class of problems that
can be efficiently classically simulated. If that is indeed the case,
this effectively rules out verification algorithms that distinguish
BosonSampling from any classical distribution with certainty.

In the next section we formally introduce BosonSampling .

### 3.3 The BosonSampling Formalism

We begin by reviewing the BosonSampling model using single photons. For
an elementary introduction to BosonSampling , see [ 107 ] . Unlike
universal LOQC, which requires active elements (specifically
fast-feedforward), the BosonSampling model is strictly passive,
requiring only single-photon sources, passive linear optics (i.e
beamsplitters and phase-shifters), and photodetection. No quantum memory
or feedforward is required.

In section 3.3.1 we provide a very simplistic view of BosonSampling for
the casual reader while in section 3.3.2 we provide a detailed overview
of the BosonSampling formalism looking at the input state, the
evolution, the output state, measurement, and discussing why it is
inefficient to classically simulate. In section 3.3.3 we look into what
the permanent is representing physically, we compare the permanent
versus the determinant, we visualize what the unitary does physically,
and looking at how the permanent arises in the first place. Finally, in
section 3.3.4 we discuss an error threshold such that BosonSampling
remains classically intractable to simulate.

#### 3.3.1 Simplistic Model

BosonSampling can be thought of in a very simplistic way as illustrated
in Fig. 3.1 . BosonSampling is comprised of three particular elements.
First, there are @xmath input modes where the input state is inserted as
shown on the left. The first @xmath modes have single photon Fock states
@xmath while the remaining @xmath have vacuum states @xmath . Second
there is the evolution of the input state as shown as a box with the
@xmath , which is comprised of linear optical elements i.e.
beam-splitters and phase-shifters. Finally, there are the @xmath output
modes where the output state is detected using photo-detectors. In the
next section we go into this in more detail.

#### 3.3.2 Detailed Model

A full detailed model of BosonSampling is illustrated in Fig. 3.2 . In
this diagram the input state is inserted into @xmath modes at the top,
evolves via the photon creation operaters of the unitary evolution in
the box from input modes @xmath to output modes @xmath , and is
outputted at the bottom where the photo statistics are taken. In this
section we review this process in detail discussing the input state, the
evolution, the output state, measurement, and discussing the
inefficiency of simulating this classically.

##### Input State

First we prepare an @xmath -mode input state @xmath , where the first
@xmath modes are prepared with the single photon Fock state, and the
remaining @xmath modes are injected with the vacuum state @xmath ,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (3.1)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where @xmath is the photon creation operator on the @xmath th mode. It
is assumed that the number of modes scales quadratically with the number
of photons, @xmath . For a large @xmath , AA conjectured that the number
of modes @xmath sufficiently ensures that, to a high probability, no
more than a single photon arrives per output mode. This is sometimes
referred to as the “bosonic birthday paradox” [ 108 ] . This implies
that in this regime on/off (or ‘bucket’) detectors will suffice, and
photon-number resolution is not necessary, a further experimental
simplification compared to full-fledged LOQC. This does not imply that
no two-photon or higher-photon interference takes place since the output
statistics and complexity arguments rely on the fact that bosonic
interference occurs. In fact, many-photon interference is taking place,
particularly during the beginning stages of the beamsplitter array.

A variation to this input state is when @xmath photons are randomly
inputed into the @xmath modes. This has become colloquially known as
“scattershot BosonSampling .” This variation is advantageous in that the
input state is easier to prepare. Lund et al. showed that the sampling
problem remains classically difficult in the case of ‘scattershot
BosonSampling ’ [ 102 ] .

##### Evolution

Next we propagate this input state through a passive linear optics
network, which implements a unitary map on the photon creation
operators,

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

where @xmath is an @xmath unitary matrix characterizing the linear
optics network given by the Haar class. It was shown by Reck et al. [
109 ] that any @xmath may be efficiently constructed using @xmath linear
optics elements.

##### Output State

The output state is a superposition of the different configurations of
how the @xmath photons could have arrived at the @xmath output modes. In
a photon-number representation, the output state is a superposition of
every possible @xmath -photon-number configuration of the form,

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath is the set of all possible photon number configurations,
@xmath is a particular photon number configuration, @xmath is the number
of photons in the @xmath th mode associated with configuration @xmath ,
and @xmath is the amplitude associated with configuration @xmath . The
total photon-number is conserved, thus @xmath for all @xmath , which is
a way to account for any losses due to inefficiencies in the experiment
by post-selecting on all @xmath photons. Also, the probability of
measuring configuration @xmath is given by @xmath . Note that a large
class of distributions of photons at the output of a multi-mode
interferometer with Gaussian inputs was investigated by Kok and
Braunstein [ 110 ] , who gave an analytic form of the output state even
with post-selection.

##### Measurement

Finally, we perform number-resolved photodetection [ 111 ] , which are
described by projection operators @xmath , on the output distribution,
obtaining a sample from the distribution @xmath . Each time we obtain an
@xmath -fold coincidence measurement outcome with a total of @xmath
photons. The experiment is repeated many times, building up statistics
of the output distribution yielding the so-called sampling problem ,
whereby the goal is to sample a statistical distribution using a finite
number of measurements. This is in contrast to well-known decision
problems , such as Shor’s algorithm [ 37 ] , which provide a well
defined answer to a well posed question. Because BosonSampling is a
sampling problem, finding a computational application is further
complicated —if every time we run the device we obtain a different
outcome, how does the outcome answer a well-defined question, and how do
we map it to a problem of interest? This is one of the central
challenges of BosonSampling —what can we do with it?

##### Discussion of Inefficient Classical Simulation

The number of configurations @xmath grows exponentially with the number
of photons and modes,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (3.4)
  -- -------- -------- -------- -- -------

Thus, with an ‘efficient’ (i.e. polynomial) number of trials, we are
unlikely to sample from a given configuration more than once. This
implies that we are unable to determine any given @xmath with more than
binary accuracy. Thus, BosonSampling does not let us calculate matrix
permanents, as doing so would require determining amplitudes with a high
level of precision, which would require a super-exponentially large
number of measurements. This is evidence towards the hardness of
simulating BosonSampling as the Hilbert space is super-exponentially
large. Much stronger evidence arose when Aaronson & Arkhipov showed that
this sampling problem likely cannot be efficiently simulated classically
[ 67 ] . The intuitive explanation for this supposed classical hardness
is that each of the amplitudes @xmath is proportional to an @xmath
matrix permanentas shown by Scheel [ 112 ] ,

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

where @xmath is an @xmath sub-matrix of @xmath that depends on the
specific configuration @xmath , and @xmath is the permanent of @xmath .
Permanents are believed to be classically hard to calculate, residing in
the complexity class @xmath P -hard, the class of counting problems on
polynomial-time algorithms. The best known classical algorithm for
calculating matrix permanents is by Ryser [ 92 ] , requiring @xmath time
steps. Because this requires exponential time to evaluate and because
@xmath is proportional to a permanent, sampling from the output
distribution @xmath is a classically hard problem and thus so is
BosonSampling .

Exact BosonSampling by a polynomial-time classical probabilistic
algorithm would imply a collapse of the polynomial hierarchy, while
non-collapse of the polynomial hierarchy is generally believed to be a
reasonable conjecture [ 67 ] . Gard et al. gave an argument that
classical computers likely cannot efficiently simulate multimode
linear-optical interferometers with arbitrary Fock-state inputs [ 113 ]
. AA [ 67 ] gave a full complexity proof for the exact case where the
device is noiseless. For the noisy case, a partial proof was provided
which requires two conjectures that are believed to be true.

Importantly, BosonSampling is not believed to be capable of efficiently
simulating full quantum computation. Nonetheless, it is a relatively
simple scheme that is experimentally viable with currently available
technology that in the near future can sample bosonic statistics that
the worlds best classical super computer today cannot. It is as though
nature can easily do the simulation for us. Thus BosonSampling is an
attractive post-classical quantum computation scheme. It was shown by
Rohde & Ralph that BosonSampling may implement a computationally hard
algorithm even in the presence of high levels of loss [ 77 ] and
mode-mismatch [ 76 ] , although formal hardness proofs are still
lacking.

#### 3.3.3 What the Permanent Represents Physically

##### Permanent versus Determinant

Let us examine this relationship with the permanent more closely. The
permanent of a matrix @xmath is given by

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

where the sum is over all elements @xmath of the symmetric group @xmath
, or in other words it is the sum over all permutations of the numbers
ranging from one to @xmath . It can be seen that this form for the
permanent is closely related to the more familiar determinant

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where @xmath is the signature of the permutation @xmath and is @xmath if
the signature has an even number of permutations and @xmath if the
signature has an odd number of permutations. The main difference between
the permanent and the determinant is that the determinant has
alternating plus and minus signs along the entries while the permanent
has all plus signs. With the determinant a familiar trick called
Gaussian elimination may be used to simplify the problem into one that
can be solved in polynomial time making it efficiently solvable using a
classical computer. There is no known trick for simplifying the
permanent.

##### Convenient Visualization of the Unitary

Now let us look more closely at the unitary @xmath . It has the
following form in its matrix representation

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

A conveinent way to visualize this matrix is to think of the rows of
this matrix as the input modes and the columns of this matrix as the
output modes to the BosonSampling device. Then a particular entry @xmath
is the probability amplitude a photon went into the @xmath th mode and
exited the @xmath th mode. Then the classical probability of this
happening is @xmath .

Next we will take a look at the case of multiple photons and see how the
permanent arises for two and three photons.

##### How the Permanent Arises in BosonSampling

Let us first consider Fig. 3.3 . Here the first two modes have single
photons, with the remaining modes in the vacuum state. Let us consider
the case where one of the two input photons arrive at output mode 2
while the other arrives at output mode 3. There are two ways in which
this could occur. Either the first photon arrives at mode 2 and the
second at mode 3, or vice versa. This implies that there are @xmath ways
in which the photons could reach the outputs. We may write the amplitude
as

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (3.9)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

which is a @xmath matrix permanent.

As a slightly more complex example, consider the three photon case shown
in Fig. 3.4 . We are considering the case where we begin with a single
photon in each of the first three modes and consider the outcome where a
single photon arrives in each of the first three output modes. Now we
see that there are @xmath ways in which the three photons could reach
the outputs. The associated amplitude is given by a @xmath matrix
permanent,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (3.10)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

In general, with @xmath photons, there will be @xmath ways in which the
photons could reach the outputs, assuming they all arrive at distinct
outputs, and the associated amplitude will relate to an @xmath matrix
permanent. If multiple photons arrive in the same output mode then
multiple copies of the corresponding column of @xmath would be used to
solve the matrix permanent. In the case where multiple photons enter
into the same mode then multiple rows corresponding to the input row of
@xmath would be used to solve the matrix permanent. Calculating matrix
permanents, in general, is known to be #P -hard, even harder than NP
-complete, and the best known algorithm is by Ryser [ 92 ] , requiring
@xmath runtime. Thus, we can immediately see that if BosonSampling were
to be classically simulated by exactly calculating the matrix
permanents, it would require exponential classical resources.

#### 3.3.4 Errors in BosonSampling

In the original BosonSampling work AA provided a detailed analysis of
the robustness of their result in the presence of error. This is
important because a physical system such as BosonSampling is bound to
have some kind of error with all of the subtleties involving creating
single photons that all match perfectly, evolving them accurately with
optical elements, and detecting the distribution with imperfect
photodetectors. With all of this to account for one could never
experimentally achieve the true distribution.

In BosonSampling one may like to consider estimation of a distribution.
The exact permanent of a binary matrix is known to be #P -hard but one
can efficiently estimate the permanent of a matrix that has real,
non-negative entries [ 114 , 115 ] . Since BosonSampling has
complex-valued entries in the matrix it cannot be efficiently estimated
[ 115 ] . In fact if this complex case could be estimated then
BosonSampling would be an anomaly occuring only when trying to calculate
the exact value of the permanent. This does not seem to be the case
however so estimating the output distribution of a BosonSampling machine
is most likely computationally hard.

There is a limit to how much error can be tolerated so that we do not
deviate too far from the desired distribution or require so many samples
that the algorithm becomes inefficient. One method to remove errant
samples would be to post-select to remove statistics where photon-loss
occurred. However, if the post-selection probability scales as @xmath ,
then we would never be able to scale the BosonSampling problem to large
@xmath regimes.

What is an acceptable level of error? Let us apply an error threshold
@xmath , where @xmath is the maximum allowable variation distance from
the exact solution assuming some clever statistical distance metric of
which many are appropriate. Then we would need a success probably @xmath
where

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

to correctly and efficiently sample a BosonSampling device with
arbitrarily large photon number @xmath . If we would like to scale
@xmath smaller, then

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

[ 67 ] . This is an interesting point because it relates to the Extended
Church-Turing Thesis and cannot be experimentally verified for
asymptotic @xmath as discussed in the next section.

### 3.4 BosonSampling and the Extended Church-Turing Thesis

Any model for quantum computation is subject to errors of some type
including dephasing, photon loss, and mode-mismatch among others. Here
we consider a realistic generalised error model for BosonSampling such
that the desired single photon states are correct with probability
@xmath and incorrect in some erroneous state with probability @xmath [
116 ] . This error model may generally include incorrect photon number,
such as loss or higher order excitations, or mode-mismatch. We can then
write our input state as

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

where @xmath may be unique for each input mode @xmath . This error model
is independent such that each state is independently subjected to an
error channel. The fidelity of the single photon states are given by our
measure of @xmath . It is a dial that may be tuned such that when @xmath
, the input state is perfect, and when @xmath , the state contains terms
with error. We desire to sample from the distribution of Eq. 3.1 ,
whereby the input state is perfect. This occurs with probability @xmath
.

Now we let @xmath be the probability that we have sampled from the
correct distribution. By following the complexity proof for errors in
BosonSampling provided by AA where @xmath , we thus find that for
computational hardness our generalized error model requires that @xmath
. Clearly this bound can never be satisfied for any @xmath in the
asymptotic limit of large @xmath since we have an exponential dependance
on the left side and a polynomial dependance on the right side.
Therefore BosonSampling will always fail in the asymptotic limit with
this error model.

Various authors [ 69 , 104 , 94 , 117 , 97 ] have claimed that
large-scale demonstrations of BosonSampling could provide elucidation on
the validity of the Extended Church-Turing (ECT) thesis — the statement
that any physical system may be efficiently simulated on a Turing
machine. The ECT thesis though is an asymptotic statement about
arbitrarily large systems. We have shown that the required error bound
for BosonSampling is never satisfied for arbitrarily large systems,
therefore BosonSampling cannot elucidate the validity of the ECT thesis
since asymptotically large BosonSampling devices fail under any
realistic error model.

There seem to be two obvious ways that our claim about the ECT thesis
may be overcome: (1) It may be shown that the error bound can be
loosened to @xmath , or (2) fault-tolerance techniques for BosonSampling
may be developed that allow arbitrarily large scaling of BosonSampling .
No such developments have been made; therefore, based on current
understanding, BosonSampling will not illuminate whether the ECT thesis
is correct or not. However, BosonSampling may still yield an interesting
post-classical computation since this only requires a finite sized
device that can out perform the best classical computers.

### 3.5 How to Build a BosonSampling Device

In this section we explain the basic components required to build a
BosonSampling device. This device consists of three basic components:
(1) single-photon sources; (2) linear optical networks; and, (3)
photodetectors. Each of these present their own engineering challenges
and there are a range of technologies that could be employed for each of
these components. However, although BosonSampling is much easier to
implement than full-scale LOQC, it remains challenging to build a
post-classical BosonSampling device. While challenging, a realizable
post-classical BosonSampling device is foreseeable in the near future.

#### 3.5.1 Photon sources

The first engineering challenge is to prepare an input state of the form
of Eq. 3.1 . This state may be generated using various photon source
technologies. For a review of many of the photon sources see Ref. [ 118
] . Presently, the most commonly employed photon source technology is
spontaneous parametric down-conversion (SPDC). The topic of Ch. 4
focuses on BosonSampling with SPDC sources. Another viable source is
quantum dots [ 119 ] , which has been used to successfully implement a
four photon BosonSampling experiment in a temporal architecture [ 74 ]
proposed by Motes et al. [ 120 ] .

#### 3.5.2 Linear optics networks

After the input state has been prepared it is evolved via a linear
optics network, @xmath . @xmath transforms the input state as per Eq.
3.2 and may be completely characterized before the experiment using
coherent state inputs [ 109 ] . @xmath is composed of an array of
discrete elements, namely, beamsplitters and phase-shifters. A
beamsplitter with phase-shifters may be represented as a two-mode
unitary of the form [ 121 ] ,

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

where @xmath and @xmath are arbitrary phases.

For a @xmath that implements a classically hard problem, one would need
hundreds of discrete optical elements. Constructing an arbitrary @xmath
using the traditional linear optics approach of setting and aligning
each optical element would be extremely cumbersome. Thus, using discrete
optical elements is not a very promising route towards scalable
BosonSampling .

One method to simplify the construction of the linear optics network is
to use integrated waveguides. Quantum interference was first
demonstrated with this technology by Peruzzo et al. [ 122 ] . This
technology requires more frugal space requirements, is more optically
stable, and far easier to manufacture, allowing the entire linear optics
network to be integrated onto a small chip [ 123 , 124 , 125 ] . The
main issue with integrated waveguides is achieving sufficiently low loss
rates inside of the waveguide and in the coupling of the waveguide to
the photon sources and photodetectors. Presently, the loss rates in
these devices are extremely high and thus post-selection upon @xmath
photons at the output occurs with very low probability. It is
foreseeable that photon sources and photodetectors will eventually be
integrated into the waveguide which would eliminate coupling loss rates,
substantially improving scalability.

Another potential route to simplifying the linear optics network is to
use time-bin encoding in a loop architecture based on the work by Motes
et al. [ 103 ] . The major advantage of this architecture is that it
only requires two delay loops, two on/off switches, and one controllable
beam splitter. This possibility eliminates the problem of aligning
hundreds of optical elements and has fixed experimental complexity,
irrespective of the size of the BosonSampling device. A major problem
with this architecture however is that it remains difficult to control a
dynamic beamsplitter with high fidelity at a rate that is on the order
of the time-bin width @xmath . Nonetheless this architecture has since
been successfully experimentally implemented by He et al. [ 74 ]
performing four photon BosonSampling , which is the largest instance of
BosonSampling to be performed to date. Furthermore they claim that they
can do more than 20 photon BosonSampling with further refinement of
system efficiency. This temporal architecture is described in detail in
Chapter 5 .

#### 3.5.3 Photodetection

The final requirement in the BosonSampling device is sampling the output
distribution as shown in Eq. 3.3 . With linear optics this is done using
photodetectors. For a review on various types of photodetection see Ref.
[ 118 , 126 ] .

There are two general types of photodetectors —photon-number resolving
detectors and bucket detectors. The former counts the number of incident
photons. These are much more difficult to make and more expensive in
general than bucket detectors. Bucket detectors, on the other hand,
simply trigger if any non-zero number of photons are incident on the
detector. As discussed earlier, in the limit of large BosonSampling
devices, we are statistically guaranteed that we never measure more than
one photon per mode, since the number of modes scales as @xmath . Thus,
bucket detectors are sufficient for large BosonSampling devices, a
significant experimental simplification compared to universal LOQC
protocols.

The predominant mechanism for experimentally realising single-photon
counting is making use of beam-splitter cascades [ 126 ] , which uses
much the same technology as BosonSampling . More recently photodetector
designs use superconductivity to measure photons. Superconductivity is
an extreme state where electrical current flows with zero resistance. It
occurs in conductive materials when a certain critical temperature is
reached. This critical temperature is far from occurring naturally on
Earth and thus high-tech and expensive lab equipment is required. For
many materials this temperature is close to absolute zero.

While there are several variations of superconductive photodetectors,
many follow the same principles as the one shown in Fig. 3.5 . The idea
is that a superconductor is cooled to a point just below its critical
temperature. Current is then applied through the superconductor which
experiences zero resistance. If there is no resistance, then there is no
voltage drop across the superconductor and a conductance measurement
reads infinity. Then the photon or photons that are to be measured will
hit the superconductor and be absorbed. Each photon that is absorbed by
the superconductor imparts energy @xmath onto it, where @xmath is
Planck’s constant and @xmath is the frequency of the photon. This heats
the superconductor above its critical temperature. The conductance
measurement will then change according to the absorbed photon, thus
informing the measurer that a photon was detected. This scheme may be
able to count several photons since the conductance will change
proportionally to the number of absorbed photons. However, if too many
photons are absorbed all properties of superconductivity are lost and
thus number-resolution is lost. For a more extensive introduction to
this topic, see chapter four of Ref. [ 20 ] .

Photodetectors may be used to help overcome the problem of temporal
mismatch. Such detectors must have the ability to record the time at
which the photon arrived. If we post-select upon detecting all @xmath
output photons in the same time-window @xmath then we can assume that
their temporal distribution overlaps sufficiently well to yield a
classically hard sampling problem. This method however is not reliable
for scalable BosonSampling . If the temporal distributions are not
sufficiently overlapping, then the probability of post-selecting all
@xmath photons in the same time-window decreases exponentially with
@xmath . However, if the sources are producing nearly identical photons
in the time domain then this method would be a practical cross check.

As the distinguishability of photons varies the complexity of sampling
the output distribution also varies. A theoretical framework was
developed by Tillmann et al. [ 127 ] that describes the transition
probabilities of photons with arbitrary distinguishability through the
linear optical network. The output distribution of BosonSampling with
distinguishable photons is then given by matrix immanants, thus
affecting the computational complexity of the output distribution. They
also test this experimentally by tuning the temporal mismatch of their
input photons. This BosonSampling experiment is unique in that it is the
first to use distinguishable photons at the input. A similar situation,
of BosonSampling with photons of arbitrary spectral structure, was
considered by Rohde [ 99 ] .

### 3.6 Applications Inspired by BosonSampling

For the first four years of BosonSampling there was no application for
it. It was interesting simply because the statistics at the output of
the device could not be simulated efficiently with a classical computer
but the device itself could relatively easily be built where nature can
do the simulating for us. With a slight alteration to the protocol where
we harness the physics of the device as opposed to the computational
complexity we found an application inspired by BosonSampling , which is
in quantum metrology [ 85 ] . This work was released almost
simultaneously with another research group where they presented a
BosonSampling inspired application in generating molecular vibronic
spectra [ 86 ] . In Ch. 7 I present in detail how we developed one of
two world first applications inspired by BosonSampling .

### 3.7 Summary

In this chapter we have given an introduction to the rapidly evolving
field of BosonSampling — a scheme that has inspired many people in the
quantum information processing community in the last couple of years. It
is intriguing because it can easily be realised in the lab as compared
to building a universal quantum computer and at the same time actually
sample from a distribution that no computer on Earth can efficiently
simulate. It is the first known device that can do such a thing.

We began this chapter by motivating BosonSampling and discussing some of
its history in section 3.2 . We then went into detail discussing
BosonSampling by first giving a very simplistic model followed by a more
detailed model and discussing why it is inefficient to classically
simulate, what the permanent represents physically, and errors in
BosonSampling , all in section 3.3 . Next, in section 3.4 , we went into
the implications of BosonSampling to the extended Church-Turing thesis
and argued that BosonSampling can not be used to prove or disprove this.
In section 3.5 we discussed the technology required to build a
BosonSampling device including the photon sources, linear optical
networks, and photodetection. Finally, in section 3.6 we discussed two
BosonSampling inspired applications, but focus on our own metrology
result.

{savequote}

[45mm] Physics is like sex: sure, it may give some practical results,
but that’s not why we do it. \qauthor Richard Feynman

## Chapter 4 Spontaneous Parametric Down-Conversion Photon Sources for
BosonSampling

### 4.1 Synopsis

BosonSampling has emerged as a promising avenue towards post-classical
optical quantum computation, and numerous elementary demonstrations have
recently been performed. One of the challenges for realising
BosonSampling is the creation of single photons. Spontaneous parametric
down-conversion (SPDC) is the most common method for single-photon state
preparation and is employed in most optical quantum information
processing experiments.

We motivate this work in detail in section 4.2 . In section 4.3 we
describe what spontaneous parametric down conversion is. We present a
simple architecture for BosonSampling based on multiplexed SPDC sources,
as shown in section 4.4 , and demonstrate in section 4.5 that the
architecture is limited only by the post-selection detection efficiency
assuming that other errors, such as spectral impurity, dark counts, and
interferometric instability are negligible. For any given number of
input photons, there exists a minimum detector efficiency that allows
post selection. If this efficiency is achieved, photon-number errors in
the SPDC sources are sufficiently low as to guarantee correct
BosonSampling much of the time. In this scheme the required detector
efficiency must increase exponentially in the photon number. Thus, we
show that idealised SPDC sources will not present a bottleneck for
future BosonSampling implementations. Rather, photodetection efficiency
is the limiting factor and thus future implementations may continue to
employ SPDC sources.

### 4.2 Motivation

In this chapter we show that large scale BosonSampling can be
implemented provided that detection efficiencies, which must increase
exponentially with photon number, are sufficient to guarantee
post-selection with high probability. Increasing input photon number
will thus yield a larger required detection efficiency.

Spontaneous parametric down-conversion (SPDC) has become the main method
for single-photon state preparation, is widely used in optical quantum
information processing, and was employed in all of the recent
experimental BosonSampling implementations. A pressing question for
future larger-scale implementations is scalability. Scalability in this
context refers to increasing the input photon number into the
BosonSampling device provided that the error in the single photon
photo-detectors, which scales exponentially with input photon number, is
sufficiently low to ensure successful implementation of BosonSampling
most of the time. That is, what are the limitations and requirements on
physical resources to implement a scalable device? In particular, will
SPDC sources suffice, or will we have to transition to other photon
source technologies?

We consider a general architecture for the experimental implementation
of BosonSampling , where multiplexed SPDC sources are employed for state
preparation. We show that in such an architecture the device is limited
only by the post-selection probability. In other words, the architecture
is scalable provided that detector efficiencies are sufficiently high to
enable post-selected computation. In this regime, the quality of current
SPDC states is sufficient to enable large-scale BosonSampling . Thus, it
is photodetection, not SPDC sources, that provide the bottleneck to
larger-scale demonstrations.

### 4.3 Spontaneous Parametric Down-Conversion

The SPDC source works by first pumping a non-linear crystal with a
coherent state @xmath as shown in Fig. 4.1 . A coherent state is well
approximated by a laser source. This evolution is given by a Hamiltonian
of the form

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is the interaction strength, which depends on the
non-linear material and h.c. stands for the hermitian conjugate of the
first term (e.g. the hermitian conjugate of an operator @xmath is @xmath
). With some probability one of the laser photons interacts with the
crystal and emits an entangled superposition of photons across two
output modes, the signal and idler . The output of an SPDC source is of
the form [ 121 ] ,

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath is the number of photons, @xmath represents the signal
mode, and @xmath represents the idler mode. The photon-number
distribution is given by

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

where @xmath is the squeezing parameter and @xmath is the phase shift
between the two modes.

For BosonSampling , we are interested in the @xmath term of this
superposition since we require single photons at the input of the first
@xmath modes. The signal photons are measured by a photodetector and
because of the correlation in photon-number, we know that a photon is
also present in the idler mode. The idler photons are then routed into
one of the input ports of the BosonSampling device using a multiplexer [
128 , 32 , 33 , 126 ] .

There are several problems associated with SPDC sources, which limit the
scalability of BosonSampling . The major problem is higher order
photon-number terms. In the BosonSampling model we only want the @xmath
term, which is far from deterministic. The SPDC source is going to emit
the zero-photon term with highest probability and emit higher order
terms with exponentially decreasing probability. If the heralding
photodetector does not have unit efficiency, then the heralded mode may
contain higher order photon-number terms.

Another problem is that photons from SPDC sources have uncertainty in
their temporal distribution. If a BosonSampling device is built using
multiple SPDC sources it is difficult to temporally align each of the
@xmath photons entering the device. This is called temporal mismatch.
The error term associated with this scales exponentially with @xmath ,
yielding an error model consistent with Eq. ( 3.13 ), which undermines
operation in the asymptotic limit.

### 4.4 Spontaneous Parametric Down-Conversion Multiplexing Architecture
for BosonSampling

Given that SPDC is the most widely used and readily accessible source
for single-photon state preparation, we will present a simple
architecture for BosonSampling based on SPDC sources. In an ideal
BosonSampling implementation one would employ deterministic photon
sources that produce exactly one photon on demand. SPDC sources, on the
other hand, coherently prepare photon pairs in two modes with a
correlated Poisson probability distribution. By measuring one of the
modes and post-selecting upon detecting one photon in that mode, a
single photon is guaranteed to appear in the other mode. This method
provides us with a probabilistic, heralded single-photon source. It is
critical that each photon is heralded to ensure a pure set of Fock-state
inputs. The photon number probability distribution is given by [ 121 ] ,

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath is the photon number (per mode). Thus, The SPDC source most
often emits the vacuum state, and sometimes higher-order pairs with
exponentially decreasing probability. For small squeezing parameters the
higher-order terms can be made small, yielding a heralded source that
produces single-photon pairs with high confidence that the heralded
state has only a single photon. To herald a single photon, we detect one
arm of a single SPDC source using an inefficient number-resolving
photodetector. Such a detector can be characterised by the conditional
probability of detecting @xmath photons given that @xmath photons were
present. For a simple inefficient detector this is given by,

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

where @xmath is the detection efficiency. Thus, in the presence of loss,
the detector exhibits ambiguity in the measured photon number, sometimes
detecting fewer photons than were present. Dark counts, the other
dominant source of imperfection in photodetection, are measurement
events from extraneous photons from the environment and could also be
incorporated into the model, but this effect can be made very small with
time-gating.

We specifically consider heralded SPDC states, using just one mode of
the SPDC for the computation rather than both, to ensure that the state
entering @xmath closely approximates Eq. ( 3.1 ). Without the heralding,
the SPDC state is Gaussian, which is inconsistent with the BosonSampling
model and not known to implement a classically hard algorithm [ 129 ,
106 ] .

Combining Eqs. ( 4.4 ) & ( 4.5 ) we obtain the probability of detecting
@xmath photons in the heralding arm of a single SPDC source,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (4.6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Thus the probability of detecting a single photon in the heralding arm
is simply,

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

We will operate @xmath such heralded sources in parallel, where @xmath
and @xmath is the number of single photon Fock states required for
BosonSampling . The probability that at least @xmath of the SPDC sources
successfully herald is given by,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

In the limit of large @xmath this asymptotes to unity,

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

The asymptotic behaviour of @xmath is illustrated in Fig. 4.2 . Clearly,
with a sufficiently large number of SPDC sources operating in parallel,
we are guaranteed to successfully herald the required @xmath single
photons.

Having successfully heralded at least @xmath SPDC sources, we employ a
dynamic multiplexer [ 128 ] to route @xmath of the heralded states to
the first @xmath modes of the BosonSampling interferometer @xmath . We
will assume the multiplexer is ideal in our analysis, although losses
could be absorbed into the detector efficiency. Experimental progress
has been made recently in developing active multiplexers [ 33 , 32 , 126
] . These multiplexers rely on optical switches, which are the topic of
much experimental investigation [ 130 , 131 ] .

Following the unitary network, number-resolving photodetection is
applied. Because the photodetectors do not have unit efficiency we must
post-select on events where all @xmath photons are detected. The
post-selection probability scales as,

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

Thus, the required detection efficiency exponentially asymptotes to
unity for large @xmath . This necessitates that future large-scale
BosonSampling implementations will require extremely high efficiency
photodetectors.

The full architecture is illustrated in Fig. 4.3 . Note that the
multiplexer is critical to the operation of the device for the original
implementation of BosonSampling as presented by AA. Without the
multiplexer we still have high likelihood of sampling from at least an
@xmath -photon input distribution; however, every time the device is run
we are likely to sample from a different permutation of the vacuum and
single photon states at the input, making it impossible to perform
sampling on a consistent input. Thus, the multiplexing ensures that the
input state is consistently of the form of Eq. ( 3.1 ) if the
photodetectors have perfect efficiency. Results published after this
work by Lund et al. [ 102 ] showed that the sampling problem remains
classically difficult in the case of ‘scattershot BosonSampling ’,
whereby @xmath photons can be input into any random configuration to the
BosonSampling device as long as where they went into the device is
known, which is the case if you are using SPDC sources and heralding.
This means we can now further simplify our architecture by getting rid
of such a complicated multiplexer. The realistic case of inefficient
photodetectors is presented next.

### 4.5 Scalability of the Architecture

Having described a general architecture for BosonSampling based on SPDC
sources, the pressing question is its scalability. The obvious scaling
issue arises from Eq. ( 4.10 ), whereby the photodetection efficiency
must be exponentially close to unity. Unless error correction mechanisms
are introduced, this scaling is inevitable and post-selection is the
only avenue to guarantee successful operation of the device. However, no
error correction has been described in the context of BosonSampling .
Thus, we will focus on post-selected operation of the device, and
address the question as to whether the device acts correctly in that
context.

In the described architecture, the dominant error source is incorrect
heralding of the SPDC states. In the limit of perfect detectors we are
guaranteed to have prepared single-photon states. However, inefficient
detectors introduce ambiguity in the heralding, creating a situation
where higher-order photon number terms are perceived as single photon
terms. For example, if a single photon is lost via detection
inefficiency, the two photon state will be interpreted as a single
photon state. This will corrupt the input state to the interferometer,
yielding an input state different than Eq. ( 3.1 ).

For a single detector, the probability that we have prepared the @xmath
-photon Fock state, given that the detector has outcome @xmath , is
given by Bayes’ rule,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.11)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We are interested in the case where we herald a single photon. Thus,

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

@xmath can be interpreted as the conditional probability that we have
prepared the correct single photon state given that heralding was
successful. For small pump powers ( @xmath ) the unconditional
probability of detecting a single photon approaches zero, although the
conditional probability approaches unity since there are negligible
higher photon-number contributions.

The probability that a single photon is correctly heralded @xmath times
in parallel, thereby preparing the @xmath copies of a single photon Fock
state as per Eq. ( 4.10 ) is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.13)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We will require that, given @xmath heralded SPDC states, upon
post-selection we correctly detect exactly @xmath photons the majority
of the time. We will arbitrarily require that @xmath , where @xmath is
the lower bound on the probability that @xmath single photons are
successfully detected at the output of the BosonSampling device. This
puts a lower bound on the required photodetection efficiency of

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

Next we will assume that all photodetectors in the architecture have the
same efficiency. Thus, we obtain that the probability of correctly
preparing all @xmath photons via post-selected SPDC is,

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

In the limit of large @xmath (i.e. large instances of BosonSampling ),
this asymptotes to,

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

For small @xmath this approaches unity, and in the limit of large @xmath
to @xmath . Thus, for @xmath , in the worst case scenario, we are
sampling from the correct distribution in @xmath of the trials. This is
shown in Fig. 4.4 .

Equation ( 4.16 ) specifies the asymptotic probability of sampling from
the correct input distribution, given that post-selection was
successful. For small squeezing we sample from the correct input
distribution most of the time, due to the lower probability of
higher-order terms occurring. Thus, for experimentally realistic SPDC
sources, provided that detector efficiencies are sufficiently high to
enable post-selection, we have a high likelihood of correct
BosonSampling and SPDC photon-number errors are negligible.

Conversely, we could require that @xmath from Eq. ( 4.13 ), where @xmath
is the lower bound on the probability that a single photon is correctly
heralded @xmath times in parallel before entering the multiplexer.
Solving this for @xmath yields,

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

From Eq. ( 4.10 ) we obtain an expression for the post-selection
probability under the condition that we require a certain fidelity on
the SPDC heralding,

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

Fig. 4.5 illustrates @xmath as a function of the squeezing parameter
@xmath and the number of successfully routed photons @xmath . We observe
that for large @xmath , post-selection is highly likely to succeed if
the SPDC state preparation was successful to within error @xmath . We
observe that in the limit of large @xmath and experimentally realistic
values of @xmath , BosonSampling using @xmath SPDC sources is scalable.

### 4.6 Summary

We first explained why spontaneous parametric down conversion (SPDC) is
of interests for linear optical applications such as BosonSampling in
section 4.2 . Next, in section 4.3 , we explain spontaneous parametric
down conversion. We presented a simple architecture for BosonSampling
via multiplexed SPDC sources in section 4.4 . We demonstrated that the
SPDCs do not limit the scalability of the architecture. Rather, the
single-photon detectors, whose efficiencies must increase exponentially
with input photon number, limit the scalability. That is provided that
detection efficiencies are sufficiently high to enable post-selected
operation (section 4.5 ), the SPDCs will produce Fock states of
sufficient fidelity to implement correct BosonSampling with high
probability. Conversely, if detection efficiencies are sufficiently high
to guarantee SPDC heralding with high fidelity, post-selection will
succeed with high probability.

Thus, SPDC sources are a viable photon source technology for future
large-scale demonstrations of BosonSampling . Additionally, existing
SPDC sources will likely need significant improvement to increase
squeezing purity and mode-matching. While post-selection guarantees
correct operation of a BosonSampling device, the required detection
efficiencies scale unfavourably. Thus, future work should further
address the question as to whether lossy BosonSampling is
computationally hard [ 77 ] , as this could significantly reduce
physical resource requirements. Other error models, such as
mode-mismatch [ 76 ] , should also be investigated further.

The analysis presented could be applied to other post-selected linear
optics protocols employing SPDCs as heralded Fock state sources.

{savequote}

[45mm] Life is extraordinary don’t let it be ordinary! \qauthor Keenan
Crisp

## Chapter 5 Scalable BosonSampling with Time-Bin Encoding Using a
Loop-Based Architecture

### 5.1 Synopsis

It was recently shown by Motes, Gilchrist, Dowling & Rohde [ 103 ] that
a time-bin encoded fiber-loop architecture can implement an arbitrary
passive linear optics transformation and can perform arbitrarily
scalable BosonSampling . We being by motivating this work in section 5.2
. The architecture, as presented in section 5.3 , has fixed experimental
complexity, irrespective of the size of the desired interferometer,
whose scale is limited only by fiber and switch loss rates. The
architecture employs time-bin encoding, whereby the incident photons
form a pulse train, which enters the loops. Dynamically controlled loop
coupling ratios allow the construction of the arbitrary linear optics
interferometers required for BosonSampling . The architecture employs
only a single point of interference and may thus be easier to stabilize
than other approaches. The scheme has polynomial complexity and could be
realized using demonstrated present-day technologies. In section 5.4 we
discuss a simplification to the architecture whereby we allow for each
instance of the inner loop to have a fixed beam-splitter ratio. In
section 5.5 we discuss the advantages of our architecture over other
implementations.

The original work showed the case of an ideal scheme whereby the
architecture has no sources of error [ 103 ] . In any realistic
implementation, however, physical errors are present, which corrupt the
output of the transformation. We later investigated the dominant sources
of error in this architecture [ 132 ] — loss and mode-mismatch — which
are presented in section 5.6 and 5.7 respectively and consider how it
affects the BosonSampling protocol, a key application for passive linear
optics. For our loss analysis we consider two major components that
contribute to loss — fiber and switches — and calculate how this affects
the success probability and fidelity of the device. Interestingly, we
find that errors due to loss are not uniform (unique to time-bin
encoding), which asymmetrically biases the implemented unitary. Thus,
loss necessarily limits the class of unitaries that may be implemented,
and therefore future implementations must prioritise minimising loss
rates if arbitrary unitaries are to be implemented. Our formalism for
mode-mismatch is generlized to account for various phenomenon that may
cause mode-mismatch, but we focus on two — errors in fiber-loop lengths,
and time-jitter of the photon source. These results provide a guideline
for how well future experimental implementations might perform in light
of these error mechanisms. If any experimentalist would like to
implement this architecture I have code that can be reconfigured to
analyse more specific error models so feel free to contact me. In
section 5.8 we discuss some realistic parameters for losses and
mode-mismatch that are currently the best achieved experimentally.

### 5.2 Motivation

The remaining central challenge in BosonSampling is constructing linear
optics networks @xmath . It was shown by Reck et al. [ 109 ] that
arbitrary networks of this form can be decomposed into a sequence of
@xmath beamsplitters. In present-day experiments this type of
decomposition is implemented using waveguides or discrete optical
elements, but using these spatial techniques might require thousands of
optical etchings in the waveguide or thousands of discrete optical
elements, which must all be simultaneously aligned, so constructing the
required linear optical interferometer is challenging. Two demonstrated
ways to overcome the alignment problem are to use the time-bin encoded
scheme by Motes, Gilchrist, Dowling & Rohde (MGDR) [ 120 ] or
time-dependent dispersion techniques as presented by Pant & Englund [
133 ] . Both methods do away with the hundreds or perhaps thousands of
optical elements, requiring only a single pulsed photon-source and a
single time-resolved photo-detector. An attractive feature of the former
architecture is that there is only a single point of interference, and
may therefore be much easier to align than conventional approaches.
Additionally, the experimental complexity of these schemes are fixed,
irrespective of the size of the desired interferometer. Although the
fiber-loop scheme was initially presented for the purposes of
BosonSampling , Rohde recently demonstrated that with minor
modifications the scheme can be made universal for quantum computing [
134 ] . Here, however, we will focus on the application of this scheme
to BosonSampling , or purely passive linear optics applications more
generally.

MGDR originally showed that, using this fiber loop architecture,
arbitrary linear optics transformations can be implemented on a
pulse-train of photons [ 103 ] . However, the original work assumes that
the architecture has no sources of error. When errors are present the
scheme no longer implements an arbitrary unitary transformation, but is
constrained by the error model. In followup work we analyse in detail
various sources of error in the MGDR protocol [ 132 ] . Specifically, we
analyse the effects of lossy elements in the architecture and the
effects of mode-mismatch caused by imperfect fiber-loop lengths and
time-jitter in the source. These effects accommodate the main challenges
facing future experimental implementation BosonSampling .

### 5.3 Fiber-loop Architecture

In this architecture, shown in Fig. 5.1 , a pulse-train of photonic
modes consisting of, in general, Fock states and vacuum, are each
separated by time @xmath and sent into an embedded fiber-loop. The
@xmath th time-bin corresponds to the @xmath th mode in a conventional
spatially-encoded scheme.

This architecture assumes lossless components and perfect mode-matching
at the central beamsplitter. In any realistic implementation this will
not be the case, which we consider in sections 5.6 and 5.7 . Next, to
help better understand the architecture we break down the architecture
into its components by discussing what happens in a single inner loop
and how multiple iterations of the single loop can implement arbitrary
unitary transformation and thus BosonSampling .

#### 5.3.1 Single Inner Loop

We begin by triggering a single photon source at time intervals @xmath
(the source’s repetition rate), which prepares a pulse train of @xmath
single photons across a length of fiber. The first step in our
architecture is to propagate the pulse train through a fiber loop with
dynamically controlled coupling ratios, as shown in Fig. 5.2 (a). Each
pulse takes time @xmath to traverse the inner loop so that it will
interfere with the next time-bin at the central beamsplitter. Between
each pulse a dynamically controlled beamsplitter, @xmath of the form,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (5.1)
  -- -------- -------- -------- -- -------

where @xmath is an arbitrary, time-dependent SU(2) operation, is applied
at time @xmath , which splits the incident field into a component
entering the loop and a component exiting the loop. Here @xmath is the
amplitude of input mode @xmath reaching output mode @xmath . @xmath (
@xmath ) represents the mode entering from the source (inner loop), and
@xmath ( @xmath ) represents the mode exiting the loop (entering the
loop). When a mode enters the loop it progresses to the next time-bin.
The component entering the loop takes time @xmath to transverse the loop
such that it coincides with the subsequent pulse. In order for the first
photon to interfere with every photon pulse it will traverse this loop
@xmath times. The second photon will traverse the loop @xmath times and
so on. The dynamics of photons propagating through the loop architecture
may be ‘unravelled’ into an equivalent series of beamsplitters acting on
spatial modes, as shown in Fig. 5.2 (b). This elementary network is the
basic building block employed by our architecture.

The boundary conditions of the protocol are that the first time-bin is
coupled completely into the inner loop and the last time-bin coupled
completely out of the inner loop (after it traverses the inner loop
once), such that the implemented unitary is bounded as an @xmath matrix,
where @xmath is the length of the pulse-train. This can be obtained
with,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where @xmath is the unitary associated with the central beamsplitter at
time @xmath .

After the entire pulse-train exits the inner loop the unitary map @xmath
is implemented,

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath and @xmath represent input and output modes respectively.
We see that the probability of finding a photon in the @xmath th mode
decays exponentially with @xmath . In quite special cases, such as the
reflectivity going to zero in the beam-splitter, this probability would
not decay exponentially. When @xmath the @xmath th input mode does not
have access to the @xmath th output mode so this matrix element is zero.
When @xmath the modes do not enter the inner loop and travel strait
through to the detector picking up a factor of @xmath . When @xmath the
modes traverse the loop @xmath times. Note that we have employed a
slightly different, but equivalent, indexing convention to the original
MGDR proposal.

Evidently, the network shown in Fig. 5.2 (b) is not sufficient for
universal linear optics networks as it contains many zero elements. To
make the scheme universal we must show that the ingredients necessary to
perform a full Reck et al. type decomposition are available as we show
in the next section.

#### 5.3.2 Multiple Inner Loops

The inner loop alone cannot implement an arbitrary unitary
transformation so additional loops are required. The outer loop allows
for an arbitrary number of applications of the inner loop to be
implemented. The net unitary @xmath after @xmath consecutive inner loops
becomes,

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

where @xmath denotes the @xmath th iteration of the inner loop and
@xmath is given by Eq. ( 5.3 ). The pulse-train will traverse the inner
loop @xmath times and the outer loop @xmath times for an arbitrary
unitary transformation to be implemented. The outer loop must have round
trip time @xmath so that the pulse-train does not interfere with itself
for a particular instance of the inner loop @xmath . The pulse-train is
coupled in and out of the outer loop via on/off switches. Once the
desired transformation is performed, the pulse-train exits both loops
and is measured via time-resolved photo-detection, where the
time-resolution of the detector must be greater than @xmath . The @xmath
th time-bin at the output corresponds to the @xmath th spatial mode in
the standard BosonSampling model.

To understand the equivalent beamsplitter representation of a single
loop, consider Fig. 5.3 (a). The pulse train enters the loop, where the
numbers on the left represent the corresponding time-bin. The first
photon is deterministically coupled into the loop as depicted by an open
circle. After the first and second photons interact some of the
amplitude may escape the loop, which corresponds to the first output
time-bin. The pulse train continues to interact through the loop via
beamsplitter operations, which are represented as closed circles. After
the @xmath th photon transverses the loop any remaining amplitude
deterministically leaves the loop, which corresponds to the @xmath th
output time-bin.

Now consider Fig. 5.3 (b), which depicts how three consecutive loops in
series with three input photons produce an equivalent beamsplitter
network. The lengths of the black lines represent time in units of
@xmath . The three modes on the left represent the pulse train of
photons at the input of the device at the first round-trip. The first
photon reaches the first beamsplitter at @xmath , the second photon
reaches it at @xmath , and so on. The photons travel through the fiber
loop network interacting arbitrarily, which yields an arbitrary Reck et
al. style decomposition. Evidently, an @xmath -mode unitary can be built
using @xmath modes and @xmath loops. In Figs. 5.4 & 5.5 we show an
alternate proof based on an inductive argument.

We have shown that a series of consecutive fiber loops can implement an
arbitrary sequence of pairwise beamsplitter operations. Next, we
recognize that each of these fiber loops requires exactly the same
physical resources, only differing by the switch’s control sequence. We
need not physically build each of these identical loops. Rather, we will
embed the loop into a larger fiber loop of length @xmath , as shown in
Fig. 5.1 . The larger loop is controlled by another two switches, which
control the number of round trips in the larger loop. From the result of
Reck et al. we know that @xmath optical elements are required to
construct an arbitrary @xmath interferometer. Thus, the number of round
trips of the outer loop is @xmath .

### 5.4 Fixing Beam-Splitter Ratios for Simplicity

An experimental simplification is when we do not require full dynamic
control over the beamsplitter ratio. Although this scenario is not
universal, it may be possible to construct useful classes of unitaries.
We will consider the situation where the beamsplitter in each iteration
of the single loop can be toggled between two settings — completely
reflective, or some other arbitrary fixed ratio. The former is required
to allow that the time-bins be restricted to a finite time-window,
whilst the latter implements the ‘useful’ beamsplitter operations. We
may have an arbitrary number of such loops in series, each with a
potentially different fixed beamsplitter ratio.

Intuitively, we expect that a ‘maximally mixing’ unitary (i.e. one with
equal amplitudes between every input to output pair) would implement a
classically hard BosonSampling instance, as it maximizes the
combinatorics associated with calculating output amplitudes. If, for
example, a unitary is heavily biased towards certain output modes, or is
sparse, the combinatorics are reduced. Specifically, we define a
balanced unitary as, @xmath , such that, up to phase, all amplitudes are
equal.

In Fig. 5.6 we take the unitary implemented by a series of @xmath
fixed-ratio fiber loops, and compare it with the balanced unitary @xmath
. We characterize the uniformity of the obtained unitary using the
similarity metric,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where we maximize @xmath by performing a Monte-Carlo simulation over
different beamsplitter ratios, @xmath . That is, @xmath tells us how
close @xmath is to uniform, with @xmath being completely uniform up to
phase. With a sufficient number of loops in series, we obtain very high
similarities, suggesting that the simplified architecture may implement
hard instances of BosonSampling .

### 5.5 Discussion of Architecture

Because there is only a single point of interference, this architecture
may be significantly easier to stabilize and mode-match than
conventional approaches, where @xmath independent beamsplitters must be
simultaneously aligned and stabilized. At this point of interference,
the dominant source of error will be temporal mode-mismatch [ 135 ] ,
which is caused by errors in the lengths of the fiber loops, or
time-jitter in the photon sources. Temporal mismatch may be regarded as
a displacement in the temporal wavepacket of the photons [ 136 ] . Let
us assume that at each round trip the photon exiting the inner loop is
mismatched by time @xmath . Over short time scales this yields dephasing
[ 137 ] , and over longer time scales, ambiguity as to which time-bin
the photon resides in. The worst case is that a given photon undergoes
temporal mismatch of magnitude @xmath . Time-bin ambiguity occurs when
@xmath , which yields the requirement that @xmath . Over shorter
timescales, temporal mode-mismatch is equivalent to dephasing as
mismatched photons yield which-path information. This leads to the
constraint that @xmath , where @xmath is the width of the photons’ wave
packets. Thus, time-jitter or temporal mode-mismatch must be kept small
relative to the scale of the photons’ wavepackets. The current switching
rates of state-of-the-art dynamically controlled switches is on the
order of GHz [ 138 , 139 , 140 , 141 ] and the temporal spacing of
photons is on the order of nanoseconds. Whilst these switches are fast
enough, they require additional coupling that involves high loss. This
will encourage further development of these type of technologies which
is also required for LOQC architectures.

Integrated waveguides are gaining popularity in photonics as they are
inherently very stable. However, although interferometrically stable
after the fabrication process, there are nonetheless @xmath points of
interference, which must be carefully aligned. On the other hand, the
fiber-loop architecture has only a single point of interference that
needs to be aligned. Another advantage of our architecture is that only
one photon source (such as a quantum dot or SPDC source with high
repetition rate) could be employed, whereas bulk-optics or waveguide
implementations would require an array of sources operating in parallel,
further reducing experimental overhead.

The experimental viability of loop-based photonic architectures was
validated by recent quantum walk [ 16 ] experiments by Schreiber et al.
[ 55 , 50 ] , where quantum memories were implemented via delay lines in
free-space. It was also shown by Donohue et al. [ 142 ] that
transmitting time-bin encoded photons in optical fibers is a robust form
of optical quantum information given that the separation of time-bins is
larger than the time resolution of the detector.

In principle, the fiber loops could be replaced by any quantum memory or
delay line such as propagation in free-space, which would be
significantly less lossy. In this case, the dominant source of loss
would be in the dynamic switches, which, using present-day technology,
have high loss rates.

The presented universal architecture is in principle arbitrarily
scalable, provided the length of the larger loop is sufficiently high (
@xmath ). However, in practice, fiber is lossy with present-day
technology. If we let @xmath be the net efficiency of the inner loop
(i.e. the probability that an incident photon will reach the output),
and @xmath be the net efficiency of the outer loop, then the worst case
net efficiency of the device is, @xmath , which scales exponentially
with @xmath . Thus, to construct large interferometers using this
architecture will require exponentially low loss rates. This is also the
case for conventional spatially encoded implementations. However, it was
shown by Rohde & Ralph [ 143 ] that BosonSampling might remain a
computationally hard problem even in the presence of high loss rates.
Other error models, such as dephasing or mode-mismatch [ 76 ] , exhibit
similar scaling characteristics. Our architecture has the same
efficiency scaling as conventional bulk-optics or waveguide
implementations. If the worst-case photon efficiency (combining state
preparation, evolution and photodetection) is @xmath , then with @xmath
photons the net efficiency is lower bounded by @xmath .

Next, we discuss the major sources of error that would challenge an
experimental implementation of our architecture including an anaysis of
loss and mode-mismatch.

### 5.6 Loss Errors

In an implementation of a passive linear optics network, whereby the
loss between each input/output pair of modes is uniform, loss simply
amounts to a reduced success probability upon post-selecting on
detecting all photons. Loss in the unitary transformation is a problem
but there is evidence that even lossy systems or systems with
mode-mismatch are still likely hard to simulate given that the errors
are sufficiently small [ 77 , 76 ] . Recently, Aaronson and Brod
investigated the complexity of BosonSampling under photon losses [ 144 ]
. In the fiber-loop architecture, the different paths traverse the inner
loop a different number of times leading to non-uniform loss. This
biases the unitary transformation resulting in a unitary that is not the
desired one, even after post-selecting upon measuring all photons. That
is, the effects of loss cannot be simply factored out of the unitary. In
some architectures, asymmetric losses may be compensated for by
artificially adding losses that rebalance the circuit, at the expense of
overall success probability. In the fiber-loop architecture this turns
out not to be the case.

In Sec. 5.6.1 we introduce the metrics that we will use to analyse loss.
In Sec. 5.6.2 we determine the effect of loss due to the lossy switch
and lossy fiber in the inner loop. Then in Sec. 5.6.3 we analyse the net
loss combining the inner loop losses with the outer loop losses. We
denote quantities here that have loss with a prime.

#### 5.6.1 Loss Metrics

We consider two metrics: similarity and post-selection probability.

##### Similarity

An interesting question is how small does loss need to be such that a
particular unitary transformation is implemented with a particular error
bar. The answer to this question is highly dependent on which unitary we
wish to implement — some unitaries will suffer more asymmetric bias than
others, depending on the switching sequence that is required to
implement them. Thus, the first question to ask is which unitary to
consider. In the work of MGDR, a so-called ‘uniform’ unitary was
considered. This is a unitary where the amplitude (but not necessarily
phases) of each element of the unitary are equal. That is, the magnitude
of the amplitude between each input/output pair of modes is the same.
This class of unitaries was considered as an example of ‘non-trivial’
matrices, which uniformly mix every input mode with every output mode.
However, it is still an open question as to exactly what classes of
unitaries yield hard sampling problems in the context of BosonSampling .
We will here consider the same setting. We will explore this by using
the similarity metric, @xmath , which compares the implemented map with
the uniform map,

  -- -------- -------- -------- -------- -------
     @xmath   @xmath                     (5.6)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where @xmath is an @xmath uniform unitary given by @xmath . @xmath is
maximised by performing a Monte-Carlo simulation over different
beamsplitter ratios so as to find the optimal switching sequence to make
the map as uniform as possible. In this analysis we use @xmath instead
of @xmath to denote the similarity metric with loss present.

##### Post-selection Probability

Another interesting question is how the probability of post-selecting
upon all @xmath photons is affected by loss, i.e the total success
probability of the device. This is of especial importance
experimentally, as it directly translates to count rates. The
post-selection probability of detecting all @xmath photons at the output
is,

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath is an integer string of length @xmath that represents a
known input configuration of photons and @xmath is the number of photons
in mode @xmath . This equation is intuitively derived as follows. For a
single photon the probability of entering mode @xmath and exiting mode
@xmath is @xmath . Then the total probability that the @xmath th photon
exits the architecture is the sum of this over all @xmath possible
output ports, i.e. @xmath . Thus the probability of detecting all @xmath
photons at the output beginning in a particular configuration @xmath is
the product of this probability over all modes @xmath where @xmath , as
per Eq. ( 5.7 ). This generalisation, by allowing arbitrary strings
@xmath , allows for implementations such as randomised BosonSampling as
described by Lund et al. [ 102 ] .

With losses present, @xmath is in general no longer unitary. Rather, it
is a mapping of input-to-output amplitudes, and will not be normalised.
When there is no loss in the architecture @xmath , and with loss
strictly @xmath , dropping exponentially with the number of photons.
Implementing the required @xmath loops will have exponentially worse
loss than a single loop.

#### 5.6.2 Inner Loop Loss

We will model loss inside of the inner fiber-loop with a beamsplitter of
reflectivity @xmath and loss in the switch as @xmath as shown in Fig.
5.7 . When @xmath the device has perfect efficiency. Before and after
the inner loop the loss experienced by each mode in the fiber is
negligible, since it may be arbitrarily short. Taking these losses into
account, the implemented map of Eq. ( 5.3 ) becomes,

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

for a given loop, where @xmath . Note that this mapping is no longer a
unitary matrix when @xmath or @xmath . This uneven distribution of
losses in the input-to-output mapping causes a skew in the matrix which
prevents it from implementing the desired unitary transformation, even
after post-selection.

#### 5.6.3 Outer Loop Loss

In the full fiber-loop architecture @xmath inner loops are implemented
via @xmath round-trips of the outer loop, before being coupled out to
the detector. This architecture can implement an arbitrary unitary
transformation when @xmath if there are no errors present. The outer
loop and outer switches cause a uniform loss on the entire pulse-train,
since every path through the interferometer passes through these
elements the same number of times. Hence, these factor out of @xmath .
The full lossy transformation that occurs is then,

  -- -------- -- -------
     @xmath      (5.9)
  -- -------- -- -------

where @xmath if an arbitrary transformation is desired, and @xmath is
given by Eq. ( 5.8 ). The @xmath occurs because the pulse-train
traverses an @xmath length of fiber in the outer loop @xmath times (i.e
@xmath can be regarded as the efficiency per unit of fiber of length
@xmath ), and the @xmath occurs because the pulse-train passes through
the two outer switches @xmath times. Fig. 5.8 shows the entire
architecture with these loss errors. For an example of loop bias due to
loss see App. A.1 . Extending from this loop bias example we generalize
the loss matrix denoted as @xmath , which represents the accumulation of
losses in the fiber-loop architecture, as a function of the number of
loops @xmath for an arbitrarily sized @xmath transformation,

  -- -------- -- --------
     @xmath      (5.10)
  -- -------- -- --------

again where @xmath . Now the lossy map @xmath may be written as an
element wise product of the ideal unitary @xmath and the loss matrix
@xmath ,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.11)
  -- -------- -------- -------- -- --------

Elements of @xmath that have no losses in them due to input modes not
reaching output modes when @xmath will be accounted for appropriately
when @xmath is multiplied by @xmath by making the cooresponding matrix
element in @xmath go to zero.

In Fig. 5.9 (a) we show how the optimised similarity with the uniform
distribution varies with @xmath and @xmath for @xmath inner loops, one
photon in all @xmath modes, and @xmath . With low loss rates ( @xmath )
the implemented unitary remains highly uniform. However, with several
loops the success probability of detecting all @xmath photons at the
output decays exponentially as shown is Fig. 5.9 (b). For these plots
the randomly generated @xmath that maximises @xmath for each @xmath and
@xmath is used to calculate the corresponding @xmath .

Now we consider how @xmath and @xmath are affected in Fig. 5.10 with
both the fiber loss and switch loss. We show this for the case of @xmath
and one photon per input mode, which is in the regime of present-day
demonstrations.

### 5.7 Mode-mismatching Errors

In any interferometric experiment it is inevitable that mode-mismatch
will occur and is thus an essential source of error that we will
consider in this section. There are many factors that may contribute to
mode-mismatch in this architecture, such as incorrect fiber lengths,
time-jitter in the sources, beamsplitter misalignment, and dispersion of
the wave-packets. In this section we will focus on two major sources of
mode-mismatch: incorrect fiber lengths and source time-jitter. The
former results in reduced Hong-Ou-Mandel visibility at the central
beamsplitter, owing to mismatched arrival times of photons. The latter
effectively results in randomisation of the preparation times of the
photons.

We consider how mode-mismatch affects our protocol by calculating the
fidelity, @xmath , between the ideal output state @xmath that one
expects theoretically with no errors present, and the actual
experimentally obtained output state @xmath . Imperfect fiber lengths
and time-jitter both cause temporal shifts in the centre of the
wave-packet, which will affect the output by both introducing
uncertainty into the timing of the bins reaching the detector, and
undermining the Hong-Ou-Mandel visibility at the central beamsplitter.
To calculate @xmath then we need to calculate the temporal overlap
between @xmath and @xmath . Therefore, we need to consider the temporal
structure of the photons.

We will model the temporal structure of photons using the formalism of
Rohde et al. [ 136 ] . We only consider the inner loop in this analysis
because there is no interference at any point in the outer loop. We
obtain lower and upper bounds on @xmath by performing a Monte-Carlo
search over different randomly generated unitaries @xmath . We could
also instead consider @xmath in this formalism to also jointly include
losses. But we will treat losses separately from mode-mismatch for
simplicity.

#### 5.7.1 Temporal Structure of Photons

The temporal structure of a photon can be represented using a mode
operator,

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

where @xmath is the temporal density function centered at time @xmath ,
@xmath is a shift of the temporal centre of the photon, and @xmath is
the time-dependent photon creation operator. This operator @xmath acts
on the vacuum @xmath to create a photon with normalised Gaussian
spectral density function,

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

where @xmath is the standard deviation. We assume that @xmath , in which
case @xmath denotes a time-bin, and @xmath denotes a small mismatch
within the respective time-bin, not large enough to cause a photon to
‘jump’ from one time-bin to the next. Thus, both @xmath and @xmath
represent shifts in the centre of the photon’s wavepacket, but the
former is of the order of the time-bin separation, while the latter is
of much smaller order than the time-bin separation. The units of @xmath
, @xmath , @xmath , and @xmath are all units of time of which the
magnitude depends on the properties of the photon source used.

#### 5.7.2 Formalism for Analysis of Mode-Mismatch

To analyse mode-mismatch we will consider three regions of the
architecture we label as @xmath , @xmath , and @xmath as shown in Fig.
5.11 . Region @xmath corresponds to the modes that are input into the
architecture from the source, region @xmath corresponds to pulses inside
the inner loop, and region @xmath corresponds to pulses that exit the
dynamic beamsplitter towards the detector. We introduce mode operators
associated with each of these distinct regions — @xmath , @xmath , and
@xmath — each of the form of Eq. ( 5.12 ).

Since every pulse begins in region @xmath the input state is a tensor
product of pure states of the form,

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

where the tensor product is taken over all @xmath modes, @xmath is a
known string representing the input photon-number configuration, and
@xmath is number of photons in the @xmath th input mode.

Next, the input state is transformed by the dynamic beamsplitter, which
takes the mode-operators from region @xmath into superpositions of
regions @xmath and @xmath ,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.15)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and pulses from region @xmath to superpositions of regions @xmath and
@xmath ,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.16)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we have used Eq. ( 5.1 ) for the elements of the dynamic
beamsplitter at time @xmath . @xmath only acts on photons arriving at
the beamsplitter at time @xmath since @xmath . When a photon enters the
loop @xmath as it advances to the next time-bin and will interfere with
the next temporal mode. After this evolution, the entire pulse-train is
coupled out of the loop such that the entire output state is a
superposition of all possible output configurations.

Now we model the state of the pulse train after @xmath beam-splitters
have been implemented,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (5.17)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where the integer values of @xmath denote the distinct time-bins. We
note that there are @xmath total beam-splitters in a single
implementation of the inner loop since there are @xmath beamsplitters to
interfere the modes and another two beamsplitters to account for the
initial and final boundary conditions of the MGDR protocol. Given how we
modelled how the mode operators are transformed by @xmath in Eqs. ( 5.15
) and ( 5.16 ) the @xmath th beam-splitter acts on the mode operators
only in modal position @xmath . Since a pulse coming out of the inner
loop exits at beam-splitter @xmath its modal position is @xmath which
accounts for there being @xmath beam-splitters and @xmath modes.

In general the final evaluated form of @xmath may be expressed as a
superposition of all possible output photon-number configurations @xmath
, and their associated temporal configurations @xmath ,

  -- -- -- --------
           (5.18)
  -- -- -- --------

where @xmath is the probability amplitude associated with photon
time-bin configuration @xmath and temporal shift configuration @xmath ,
@xmath denotes the time-bin of the @xmath th photon, @xmath denotes a
configuration of temporal shifts associated with the configuration
@xmath , and @xmath is the temporal shift of the @xmath th photon
associated with configurations @xmath and @xmath . This is the most
general representation of a configuration of photons across time-bins
with associated shifts. The probability of measuring a particular
configuration is @xmath , and to evaluate these probabilities we must
fully characterise spectrum of time-bin and temporal shift
configurations, @xmath and @xmath . Finding analytic forms for these
expressions is largely prohibitive, and we calculate the @xmath via
brute-force simulation of the evolution of the mode-operators through
the network as described earlier.

#### 5.7.3 Fidelity Metric

We analyse the results of this section by calculating the fidelity
@xmath between the ideal output state and the actual output state, given
by,

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

where @xmath is the ideal output state with no mode-mismatch ( @xmath )
and @xmath is the actual output state obtained with mode-mismatch.
@xmath reduces to @xmath in the limit of no errors yielding @xmath .
Calculating this overlap but letting @xmath have general temporal mode
mismatch until the end of the calculation we obtain,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

To simplify this expression further we use the formalism of second
quantisation [ 145 ] , which describes how the indistinguishability of
particles in quantum mechanics undergo symmetrisation. Here we use the
exchange symmetry of the bosonic Fock states, which accounts for how
each temporal photon annihilation operator @xmath overlaps with each
temporal photon creation operator @xmath . Using bosonic exchange
symmetry we sum over all @xmath permutations of @xmath . Then Eq. (
5.7.3 ) becomes,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath are the permutations over @xmath elements.

Finally, to calculate @xmath we must find the wave packet simplification
for

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

which we perform in App. A.2 . Using this result we obtain,

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

Letting the ideal state @xmath have no temporal shifts, @xmath , this
reduces to,

  -- -------- -------- -- -- --------
     @xmath   @xmath         (5.24)
  -- -------- -------- -- -- --------

This derivation assumes the width of all wave-packets remain the same,
i.e the photons are identical up to a temporal displacement. The width
of the wave-packets may broaden due to dispersion but under the
relatively short lengths of fiber-loop required for small @xmath the
effect of dispersion may be neglected; however, as a very crude
approximation, this formalism may be easily modified to include
dispersion by creating an operator that broadens the wave-packet width
@xmath as a function of the length of the fiber the wave-packet has
traversed.

Next we consider two types of mode-mismatch: non-ideal lengths of the
inner loop, and time-jitter at the input source.

#### 5.7.4 Imperfect Inner Loop Length

Here we analyse errors in the MGDR fiber-loop architecture caused by a
non-ideal length of inner fiber-loop We let the length of the inner loop
have some length @xmath , where @xmath is the error in the intended
length @xmath and may be positive or negative. Thus every photon that
traverses the inner loop acquires a temporal shift of @xmath from its
expected centre. We ignore imperfect lengths of the outer loop because
every mode will traverse the outer loop an equal number of times
creating a global temporal shift with no impact on interference at the
central beamsplitter.

The input state is given by Eq. ( 5.14 ) where @xmath . This models an
ideal input state with no time-jitter or other errors in the source. To
account for the unwanted time-delay @xmath we introduce the time-delay
operator @xmath ,

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

which acts only in region @xmath – the region inside the inner loop.
This adds a small temporal displacement, not enough to confuse
time-bins. Thus it affects @xmath but not @xmath . It has no effect on
the mode-operators @xmath and @xmath . Using the boundary conditions
shown in the MGDR protocol, the first photon is coupled completely into
the loop so it picks up a time-delay of @xmath . Afterwards the
pulse-train interacts at the beamsplitter described in Eqs. ( 5.15 ) and
( 5.16 ), where it is sent into a superposition of regions @xmath and
@xmath . As the state evolves all amplitudes entering the inner loop
(region @xmath ) will acquire a time-shift of @xmath . After the last
mode traverses the inner loop the state is coupled completely out as per
the MGDR protocol. The output state is given by,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.26)
  -- -------- -------- -------- -- --------

where we have inserted the time-delay operator appropriately in Eq. (
5.17 ). Fig. (a)a shows how the fidelity @xmath scales with @xmath ,
@xmath , and @xmath and Fig. (b)b shows the worst- and best-case
fidelities, where we have searched over switching sequences.

#### 5.7.5 Time-Jitter from Input Source

A major source of error in the time-bin architecture is time-jitter of
the input source. Ideally each mode will be separated by time @xmath but
in reality non-ideal sources will randomly shift modes from their
desired centre of time @xmath in mode @xmath . To model time-jitter we
let the temporal shift of input mode @xmath be a Gaussian random
variable @xmath drawn from the normal distribution,

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

centered in mode @xmath at time @xmath and with a standard deviation of
@xmath . The input state of Eq. ( 5.14 ) becomes,

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

We assume that the shifts caused by time-jitter are much less than the
time-bin separation @xmath , such that the probability of time-bin
confusion remains negligible, i.e. @xmath . Fig. (a)a shows how the
fidelity @xmath scales with @xmath , @xmath , and @xmath . Fig. (b)b
shows the worst- and best-case @xmath , searching over many switching
sequences.

### 5.8 Discussion of Using Realistic Devices

With the two types of errors we have considered, loss and mode-mismatch,
the final wavefunction is affected in predictable ways. In the case of
loss the unitary transformation implemented on the initial input state
becomes biased. This causes the corresponding probabilities of obtaining
particular output wavefunctions to also become biased. There are two
quantities to consider in our work when considering loss, the loss in
the fiber @xmath and the loss from the switches @xmath . Today the best
known efficiencies for these are @xmath [ 146 ] and @xmath [ 147 ] .
With these efficiencies the post-selection probability @xmath is too
small to implement a BosonSampling experiment of interesting size as can
be seen in Fig. 5.10 . Current fiber technology is efficient enough to
implement our architecture but the switches are too lossy.

The second type of loss we have considered is mode-mismatch. Our model
can account for any error that can shift the temporal location of the
wave packets of the photons. We focused on two such errors: 1) an
imperfect length of the inner loop and 2) time-jitter from the input
source. In all cases this causes the experimentally obtained output
wavefunction to differ from the theoretically desired output
wavefunction. We used the Fidelity metric @xmath to characterize this
difference. We suspect that the error in the length of the inner loop
@xmath can be made extremely small by carefully characterizing the
length of the inner loop using coherent states of light so this should
not be a considerable issue. The second error source we considered,
time-jitter, however is a problem in any experiment that requires single
photons. Ideally the standard devitaion @xmath in Eq. ( 5.27 ) would
approach zero such that there is no time-jitter. Today some of the best
experimentally obtained values for this @xmath are found in references [
148 , 149 , 150 ] . In these works @xmath . Comparing this value to Fig.
5.13 we see that source jitter needs to improve by about an order of
magnitude for a time-bin architecture to be feasable.

It seems that in the near future a BosonSampling experiment with more
modes and more photons than has ever been implemented could be carried
out using this architecture due to the fast rate at which quantum
technologies are being researched and developed; however, it may be much
longer before this architecture has sufficiently low errors that it
could be used to implement a BosonSampling experiment that is in the
classically hard regime to simulate.

### 5.9 Summary

We have presented the original work of Motes, Gilchrist, Dowling & Rohde
[ 103 ] which is an arbitrarily scalable architecture for universal
BosonSampling based on two nested fiber loops as motivated in section
5.2 . The complexity of the architecture (which is described in detail
in section 5.3 ) is constant, independent of the size of the
interferometer being implemented. Scalability is limited only by fiber
and switch transmission efficiencies as well as source efficiencies.
There is only one point of interference in the architecture, which
suggests that it may be significantly easier to stabilize than
traditional approaches based on waveguides or discrete elements. We also
considered an experimental simplification in section 5.4 where full
dynamic control is not required and showed that, while not universal,
with sufficient loops the unitary approximates a maximally mixing
unitary. While we have specifically considered this architecture in the
context of BosonSampling , the same scheme, or variations on it, may
lend themselves to other linear optics applications, such as
interferometry, metrology, or full-fledged LOQC. In section 5.5 we
discuss many of the advantages of our architecture.

Following the work presented in [ 103 ] we analysed sources of error in
the fiber loop architecture. Specifically we have analysed loss and
mode-mismatch in section 5.6 and 5.7 respectively. In the loss analysis
we examined how lossy fibers and switches affect the operation of the
architecture in both the inner and outer loops. We found that loss
causes an asymmetric bias in the desired unitary, unique to a temporally
implemented unitary transformation. That is, even upon post-selection
the operation of the device is erroneous. Additionally, like all linear
optical architectures, our scheme has exponential dependence on loss,
thereby reducing the post-selection success probability of detecting all
@xmath photons. In the mode-mismatch analysis we analysed only the inner
loop since no interference occurs in the outer loop. We examined two
types of mode-mismatch including an imperfect length of fiber in the
inner loop, and time-jitter of the photon source. This analysis provides
a guideline for future experimental implementations, to provide insight
into how such a device might realistically behave in the presence of
loss and mode-mismatch, the two dominant error mechanisms affecting this
protocol. In section 5.8 we discuss our architecture using todays lowest
reported loss rates and amount of mode-mismatch.

{savequote}

[45mm] What we think, we become. \qauthor Buddha

## Chapter 6 BosonSampling with Other Quantum States of Light

### 6.1 Synopsis

It is known that the BosonSampling sampling problem likely cannot be
efficiently classically simulated. This raises the question as to
whether or not other similar systems implement a sampling problem that
is also computationally hard. One such variation to consider is what
happens when other quantum states of light are used at the input to the
BosonSampling device other than single-photon Fock states. Are there
other quantum states of light that are also computational hard to
simulate? We answer this in the affirmative.

In section 6.2 we provide further motivation for this question. We have
investigated the BosonSampling problem with three different quantum
states of light other than single-photon Fock states which are published
[ 81 , 151 , 80 ] and presented in this chapter:

1.  In section 6.3 we consider single-photon-added coherent states
    (SPACS). We show that the associated BosonSampling problem with
    displaced single-photon Fock states at the input and using a
    displaced photon-number detection scheme is in the same complexity
    class as BosonSampling for all values of displacement. We then show
    that the associated BosonSampling problem with SPACS and using a
    displaced photon-number detection scheme has an interesting
    computational complexity transition. The transition is from
    computationally hard when the coherent amplitudes are sufficiently
    small to computionally easy when the coherent amplitudes become
    large. The intuitive explanation is that with small coherent
    amplitudes we are approximating single photons while with large
    coherent amplitudes we are approximating a more classical state like
    a laser beam.

2.  In section 6.4 we show that the BosonSampling problem with
    photon-added or -subtracted squeezed vacuum (PASSV) states are in
    the same complexity class as BosonSampling when sampling at the
    output is performed via parity measurements. Here we found an exact
    proof that works for an arbitrary amount of squeezing.

3.  In section 6.5 we present the BosonSampling problem with a very
    broad class of quantum states of light — arbitrary superpositions of
    two or more coherent states — at the input. We were not able to find
    a full complexity proof for the hardness of this problem but have
    strong evidence that when this state is evolved via passive linear
    optics and sampled with number-resolved photodetection it is likely
    classically hard to simulate like BosonSampling .

### 6.2 Motivation

The linear optical community was surprised when they found out about the
computational complexity of BosonSampling . Simply having a linear
optical system with Fock states as input and sampling using a suitable
detection strategy at the output is computationally complex. This fact
opened inquiry into the complexity of other linear optical systems.
Understanding these systems could shed light in the field of
computational complexity increasing our understanding of complexity
classes and computation in general which would help lead us to a quantum
computer. It may also help us understand more systems that can be
simulated through experiment but are not efficient to simulate on a
classical computer.

A modification to consider is whether or not other states of light input
to a linear optical network using similar output detection strategies
are also of similar computational complexity as BosonSampling . It is
known that passive linear optics may be efficiently simulated with
Gaussian inputs and non-adaptive Guassian measurements [ 152 , 11 ] .
However, the more general question as to which quantum states of light
may be efficiently simulated with number-resolved measurements is an
open question. Recent results include that the case of sampling with
Gaussian states in the photon number basis can be just as hard as
BosonSampling [ 102 ] . Sampling with thermal states can be simulated
efficiently on a classical computer [ 153 ] . It has also been shown in
some special cases that sampling two-mode squeezed vacuum states is
likely hard to efficiently simulate classically [ 78 , 102 ] . Other
quantum states of light considered were single-photon-added coherent
states (SPACS) by Seshadreesan et al. [ 81 ] , photon-added or
-subtracted squeezed vacuum (PASSV) states by Olson et al. [ 151 ] , and
generalized cat states (i.e. arbitrary superpositions of coherent
states) by Rohde et al. [ 80 ] . These three states evolved using linear
optics and using photon number detection, have been analysed and are
presented in this chapter in sections 6.3 , 6.4 , and 6.5 respectively.
Although these input states are more difficult to prepare than the
single-photon Fock state, the analysis of their computational complexity
allows us to demonstrate interesting phenomenon. We provide clarity on
the theory of classifying the sampling complexity of various quantum
states and we demonstrate that Fock states are not unique in their
sampling complexity as there are a plethora of other quantum states of
light which yield sampling problems with similar complexity to
BosonSampling .

### 6.3 Single-Photon-Added Coherent State Sampling

In this section we look into the computational complexity of
BosonSampling with, instead of single-photon Fock states at the input,
displaced single-photon Fock states (DSPFS) and single-photon-added
coherent states (SPACS). We do this with a displaced photon number
detection. The displacement operator can be written as

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

where @xmath is a complex amplitude that indicates displacement in phase
space, and @xmath ( @xmath ) is the mode photon creation (annihilation)
operator. The DSPFS is the state @xmath , while the SPACS is @xmath . In
this section we show three things. Firstly we show in section 6.3.1 that
sampling with DSPFS is in the same complexity class as AA BosonSampling
for any displacement @xmath . In section 6.3.2 we show the modified
BosonSampling protocol with SPACS, which differ from DSPFS by the
ordering of the operators. The results here are interesting becuase the
sampling problem with SPACS is just as hard as AA’s BosonSampling when
the input coherent amplitudes are sufficiently small but when the input
coherent amplitudes become larger the problem transitions from hard to
simulate classically to easy to simulate classically. This interesting
transition is discussed in section 6.3.3 .

#### 6.3.1 Sampling Displaced Single-Photon Fock states (DSPFS)

In this section we use DSPFS instead of single-photon Fock states as per
Eq. ( 3.1 ) at the input to the linear-optical interferometer. This
input state has the form

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where @xmath is the displacement operator of the @xmath th mode, and
@xmath is the complex coherent amplitude for the displacement. The input
states reaches the linear optical interferometer @xmath where a unitary
operation transforms the state into

  -- -------- -------- -- -------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (6.3)
  -- -------- -------- -- -------

where @xmath is the new displacement amplitude in the @xmath th mode,
@xmath is the photon-creation operator of the @xmath th mode, and @xmath
is the number of photons in the @xmath th mode, associated with
configuration @xmath at the output such that @xmath for each @xmath . To
derive this expression we have used: @xmath , @xmath , Eq. ( 3.2 ) and
Eq. ( 3.3 ). We have also invoked that a unitary on a tensor product of
coherent states is another tensor product of coherent states as shown in
App. A.5 . The result is a displaced version of AA’s original
BosonSampling output state.

Now the question is does this output state have computational complexity
similar to that of BosonSampling ? The answer is yes. This is because
the the new complex displacement amplitudes @xmath can be efficiently
computed for any unitary operator @xmath . Also, a counter-displacement
with amplitudes @xmath could simply be applied to the @xmath output
modes since @xmath , which is classically efficient and can be performed
using unbalanced homodyning [ 154 , 155 ] . With this counter
displacement we are left with exact BosonSampling and thus sampling
DSPFS using a measurement scheme at the output that is comprised with an
inverse displacement followed by coincidence photon-number detection is
in the same complexity class as BosonSampling . This demonstrates that
an entire class of quantum states of light may be used to achieve a
sampling problem of equal complexity to BosonSampling .

#### 6.3.2 Sampling Single-Photon-Added Coherent States (SPACS)

SPACS differ from DSPFS in the ordering of the operators. Since the
displacement operator of Eq. ( 6.1 ) does not commute with the photon
creation operator @xmath , these states are sufficiently different. A
@xmath -photon-added coherent state may be written as

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

with normalization

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

where @xmath is the Laguerre polynomial of order @xmath . These states
were first described by Agarwal & Tara [ 156 ] . The state we are
interested in this work is the SPACS whereby we consider @xmath in Eq. (
6.4 ).

A SPACS may be created by mixing a single photon (perhaps prepared with
spontaneous parametric down-conversion) with a coherent state on a
highly reflective beam splitter as shown in Fig. 6.1 . When vacuum is
detected in one the top output mode we know that the single photon has
been added to the coherent state in the other output port, and thus a
SPACS has been heralded [ 157 , 158 , 159 , 160 ] .

The quantum-classical transition of SPACS have been studied since they
allow for a seamless interpolation between the highly nonclassical Fock
state @xmath ( @xmath ) and a highly classical coherent state @xmath (
@xmath ) [ 159 ] . The Wigner function of a SPACS can be expressed as [
156 ]

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

where @xmath is the phase-space complex variable, and @xmath the
coherent amplitude in the state. Fig. 6.2 shows the Wigner functions of
a SPACS and a coherent state. The former attains negative values at
points close to the origin in phase space, which clearly demonstrates
the nonclassical nature of the state. Fig. 6.3 shows 2-d slices along
position of the Wigner function of a SPACS taken at a fixed momentum of
zero as a function of the coherent amplitude @xmath . It can be seen
that the Wigner function loses its negativity as @xmath increases and
tends towards being a Gaussian state.

The SPACS-based input that we consider to a linear-optical sampling
device can be written as

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

with

  -- -------- -- -------
     @xmath      (6.8)
  -- -------- -- -------

where @xmath represents the complex coherent amplitude in the @xmath th
mode and @xmath is the overall normalization factor. In words this is
saying that the input to the first @xmath modes are SPACS and the
remaining @xmath modes are in the vacuum state. A unitary operation
@xmath then transforms the state as

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (6.9)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

This state can also be written as

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.10)
  -- -------- -------- -- --------

where we have used the commutation relation between the displacement
operator and the photon-creation operator

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

We can further simplify the state

  -- -------- -------- -- --------
     @xmath               
              @xmath      
              @xmath      
              @xmath      
              @xmath      (6.12)
  -- -------- -------- -- --------

where @xmath is the new displacement amplitude in the @xmath th mode. As
in the case of DSPFS sampling from subsection 6.3.1 , we can now apply a
counter-displacement operation of amplitude @xmath , which is
efficiently computed, so that the output state reduces to

  -- -------- -------- -- --------
     @xmath   @xmath      (6.13)
  -- -------- -------- -- --------

We will now denote the state @xmath , which corresponds to AA-type
BosonSampling as @xmath . Further, for simplicity, we choose all the
input coherent amplitudes to be equal to @xmath . Then, the output state
in Eq. ( 6.13 ) can be written as

  -- -------- -------- -- --------
     @xmath   @xmath      (6.14)
  -- -------- -------- -- --------

where @xmath is defined for @xmath as

  -- -------- -- --------
     @xmath      (6.15)
  -- -------- -- --------

with @xmath being the symmetric group of degree @xmath , @xmath being
the identity operator, and @xmath . Now we are left with performing
photon number detection at the output. Detection events consisting of
detecting all photons @xmath at the output correspond to sampling of the
@xmath term from the superposition. The probability of detecting a total
of @xmath photons at the output can be written as

  -- -------- -- --------
     @xmath      (6.16)
  -- -------- -- --------

since there are @xmath terms in @xmath , each with a weight of @xmath .

#### 6.3.3 The Quantum-Classical Divide and Computational-Complexity
Transitions

Now we would like to analyse the computational complexity of this
scheme. We know that the @xmath term in Eq. ( 6.14 ) is computationally
complex to sample from and is obtained when detecting @xmath photons at
the output. To show computational hardness we ask how should @xmath
scale in terms of @xmath (i.e. the total number of SPACS in the input)
so that the probability of detecting @xmath photons at the output may be
obtained in a polynomial number of measurements. In other words the
post-selection probability of the interferometer scales inverse
polynomially in @xmath since this scaling would guarantee that a
polynomial number of measurements would obtain samples from the desired
@xmath term at the output.

To show this we will let @xmath , where @xmath (the set of positive
integers). Using the state of a SPACS from Eq. ( 6.7 ) and solving for
@xmath that satisfies the above scaling requirement in the limit of a
large @xmath , we have

  -- -------- -------- -- --------
     @xmath   @xmath      
     @xmath   @xmath      
              @xmath      (6.17)
  -- -------- -------- -- --------

where the third inequality is due to the fact that for all @xmath ,

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.18)
  -- -------- -------- -- --------

From Eq. ( 6.17 ), we have

  -- -------- -- --------
     @xmath      (6.19)
  -- -------- -- --------

and the large- @xmath expansion

  -- -------- -- --------
     @xmath      (6.20)
  -- -------- -- --------

tells us that @xmath . The chain of inequalities

  -- -------- -- --------
     @xmath      (6.21)
  -- -------- -- --------

implies @xmath is a sufficient condition on @xmath to ensure that the
post-selection probability of the @xmath term scales inverse
polynomially in @xmath . For @xmath , in the limit of large @xmath , the
probability of the term @xmath being detected at the output is

  -- -------- -- --------
     @xmath      (6.22)
  -- -------- -- --------

Further, the probability @xmath converges to one when @xmath . In other
words the sampling problem with SPACS inputs reduces to AA BosonSampling
without the need for post selection. This result is consistent with AA’s
original result that BosonSampling is robust against small amounts of
noise [ 67 ] .

Another way analyse the computational complexity is by asking how should
@xmath scale so that the photon number sampling almost always yields the
@xmath -mode vacuum? For @xmath , we find that the probability of the
@xmath -mode vacuum term being detected at the output is

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      (6.23)
  -- -------- -------- -- --------

This shows that the sampling problem with SPACS inputs becomes
classically easy when @xmath scales as @xmath , or larger because it
always results in the detection of the @xmath -mode vacuum at the
output.

W found that that the computational complexity of sampling the SPACS
goes from being just as hard as AA’s BosonSampling for coherent
amplitudes when

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

to being classically simulatable when

  -- -------- -- --------
     @xmath      (6.25)
  -- -------- -- --------

where @xmath is the total number of SPACS inputs. There is an
intermediate regime between @xmath and @xmath where we were not able to
prove the computational complexity and so regime is left open as in
interesting phenomena to investigate. Interestingly, this problem
becomes classically easy to simulate when the number of photons at the
output exceeds the fluctuation of photon number in the coherent states.

### 6.4 Photon-Added or -Subtracted Squeezed Vacuum State Sampling

In this section we demonstrate that the BosonSampling problem using
photon-added or -subtracted squeezed vacuum (PASSV) states at the input
and parity measurements at the output is of equal computational
complexity to Fock state BosonSampling for an arbitrary amount of
squeezing. To do this we prove that this problem implements the same
logical problem as BosonSampling whereby the output statistics of the
device is given by the same matrix permanent sampling problem. This is
advantageous because we avoid doing the full complexity proof as done by
AA, which was about one hundred pages long, yet we can still use their
results. To do this we are careful to show that this problem is
equivalent to the BosonSampling problem. Also, since we are able to show
an exact mapping to BosonSampling , then the error threshold for
approximate BosonSampling holds. For consistency and simplicity, we
consider in this analysis for the case of photon-added squeezed vacuum
(PASV) states throughout this work and show that the subtracted (PSSV)
case arbitrarily follows.

In section 6.4.1 we discuss the PASSV input state and describe a
non-deterministic method for preparing this input state. In section
6.4.2 we discuss the evolution of this state, which is similar to the
evolution that is used in standard BosonSampling except that we choose a
Haar-random unitary with all real elements. In section 6.4.3 the output
state is calculated and in section 6.4.4 describe how parity measurement
is used to obtain the same computationally complex output statistics as
BosonSampling . Next we mention some complexity concerns in section
6.4.5 and wrap this section up with some discussion about this work in
6.4.6 .

#### 6.4.1 Input

PASV states may be prepared by mixing a SV state (obtained from a
degenerate parametric down-converter) with a single-photon state on a
low reflectivity beamsplitter and post-selecting upon detecting the
vacuum state in one of the output modes. The preparation scheme is shown
in Fig. 6.4 . When the post-selection is successful the PASV state is
heralded in the other output mode. This scheme is non-deterministic, as
are most quantum optical shemes, but may be performed in advance to the
sampling protocol. PSSV states may be prepared similarly by sending in a
squeezed state and a vacuum state to the inputs and post-selecting on
one photon in one of the modes.

For PASV BosonSampling we prepare the first @xmath modes with PASV
states and the remaining @xmath modes with squeezed vacuum (SV) states.
We let each mode have the same amount of squeezing given by the
squeezing parameter @xmath , which is of arbitrary value. The input
state is

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.26)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where we have abbreviated @xmath and again the subscript indicates mode
number. The input state in Eq. ( 6.26 ) is not normalized but can be
normalized with the state @xmath , where

  -- -------- -- --------
     @xmath      (6.27)
  -- -------- -- --------

This normalization does not affect our result so we leave it out for
simplicity. The squeezing operator is

  -- -------- -- --------
     @xmath      (6.28)
  -- -------- -- --------

where @xmath and @xmath are the photon creation and annihilation
operators respectively. In the Fock basis, if @xmath , then @xmath may
be expressed as [ 121 ]

  -- -------- -- --------
     @xmath      (6.29)
  -- -------- -- --------

It can be seen that the SV state contains only even photon-number terms.
When a creation or annihilation operator acts on a PASSV state then the
resulting state contains only odd photon-number terms. In the limit of
zero squeezing the SV state approaches the vacuum state

  -- -------- -- --------
     @xmath      (6.30)
  -- -------- -- --------

and the PASV state approaches the single-photon state

  -- -------- -- --------
     @xmath      (6.31)
  -- -------- -- --------

Thus, we see that in the limit of vanishing squeezing, PASV
BosonSampling reduces to ideal Fock state BosonSampling .

#### 6.4.2 Evolution

The input state is fed into a passive linear optics interferometer
consisting of beamsplitters and phaseshifters, like in the original
BosonSampling protocol, which transforms the creation operators
according to a linear map

  -- -------- -- --------
     @xmath      (6.32)
  -- -------- -- --------

where @xmath is an @xmath matrix. For PASSV BosonSampling we consider an
interferometer consisting of real beamsplitters that implements an
orthogonal matrix, which is also chosen to be Haar-random. So the
difference is that for Fock state BosonSampling @xmath , whereas for
PASSV BosonSampling @xmath . Reck et al. showed that any @xmath unitary
or orthogonal matrix can be implemented with at most @xmath optical
elements and that there is an efficient algorithm for finding the
decomposition [ 109 ] .

The complexity of choosing an orthogonal matrix instead of a unitary one
is a concern because there is the possibility of choosing a subset of
matrices from @xmath , whose permanent is efficiently simulatable by a
classical computer. If we were to sample from an efficiently simulatable
distribution, then the result would not be interesting since the whole
interesting aspect of BosonSampling is that it simulates a classically
intractable system. Later we prove that the associated complexities are
equivalent and so the problem remains interesting.

#### 6.4.3 Output

The output state for the original Fock state model of BosonSampling
after passing through the interferometer can be expressed as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.33)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where the last equality holds because @xmath (i.e. @xmath represents
passive optical elements which cannot generate new photons).

With PASSV BosonSampling we can use the same technique as in Eq. ( 6.33
)

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.34)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

It was shown by Jiang et al. [ 78 ] that a pure product state input to a
linear optical network is entangled at the output unless the input is
either a tensor product of coherent states or a tensor product of
squeezed states (with the same squeezing) provided that the network does
not mix the squeezed and anti-squeezed quadratures. Not mixing the
quadratures can be achieved by using a network comprised of real
beamsplitters. This condition is satisfied since we are using @xmath and
so the output state becomes

  -- -------- -- --------
     @xmath      (6.35)
  -- -------- -- --------

The leading operator corresponds to a configuration of @xmath creation
operators as in Eq. ( 6.33 ). The output can therefore be represented in
a form like BosonSampling where we distinguish all of the possible
output distributions

  -- -------- -- --------
     @xmath      (6.36)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (6.37)
  -- -------- -- --------

In the binary regime @xmath . For PSSV states the output is of the same
form. This can be seen by replacing @xmath with @xmath , but @xmath now
relates to @xmath instead of @xmath , which is also Haar-random. We
exclude the case of the PSSV states when @xmath since @xmath .

We know that from Eq. ( 6.29 ) that squeezed states represented in the
Fock basis have only even photon-number terms and so for a particular
configuration @xmath where mode @xmath does not have a
creation/annihilation operator acting on it, mode @xmath is a
superposition of only even photon number states, whereas if mode @xmath
has a creation/annihilation operator applied it contains only odd
photon-number terms.

#### 6.4.4 Measurement

The last step in BosonSampling is to measure the output distribution.
For PASSV BosonSampling we perform a parity measurement that
distinguishes between odd and even photon number. These measurements may
be characterised by the measurement operators

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (6.38)
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- --------

A photon-number-resolving detector could easily implement this
measurement scheme. Importantly this measurement scheme implies that
measuring an even photon-number at output mode @xmath means that there
was no creation/annihilation operator associated with that mode, whereas
measuring an odd photon-number implies that there was. This measurement
perfectly recovers the configuration @xmath since we are sampling the
same creation/annihilation operators as in BosonSampling . An
interesting observation is that the squeezing parameter @xmath has no
effect on the parity of the state and so the sampling statistics are
independent of the squeezing. In other words an arbitrary amount of
squeezing can be done and a computationally complex sampling problem is
still implemented.

#### 6.4.5 Complexity Concerns

We have shown that the PASSV model samples permanents of submatrices in
the same way as Fock state sampling. The only barrier to completing our
proof is that BosonSampling and PASSV sampling is in the same complexity
class is to show whether choosing an orthogonal matrix has any
implications for the complexity of PASSV sampling.

The first thing to consider is whether or not a Haar-random matrix in
@xmath might have an efficiently computable exact or approximate
permanent, where @xmath is the @xmath -dimensional rotation group. It is
known that the exact permanent case is # P -hard even for binary entries
of the matrix, @xmath [ 114 ] . If the matrix has entries consisting of
only non-negative real numbers then there is also a known algorithm for
efficiently approximating a permanent. Also, in the same work it was
shown that for a matrix with a single negative entry that an efficient
approximation algorithm would allow one to compute an exact @xmath
-permanent efficiently [ 115 ] . Although computing a difficult
permanent is a necessary but not sufficient condition for computational
hardness since @xmath is considered to be universal for linear optics [
161 ] , there is no such complexity condition between unitary and
orthogonal matrices.

More concretely, it is known that @xmath , where @xmath is the Lie group
of @xmath unitary matrices [ 162 ] . This statement means that for a
@xmath -mode interferometer, the set of all orthogonal transformations
includes all unitary @xmath -mode transformations as a subgroup. This
means that the complexity of sampling the output of a BosonSampling
device implementing an arbitrary matrix from @xmath is at least as hard
as sampling matrices from @xmath with only a linear increase of mode
number. The same complexity extends to an odd number of modes since
@xmath . This also implies that Fock state BosonSampling itself remains
hard under orthogonal transformations.

Now it is clear that PASSV BosonSampling is in the same complexity class
as Fock state AA BosonSampling . If we let @xmath be some complexity
class containing Fock state BosonSampling , then we have shown that
@xmath also contains PASSV BosonSampling . The output of PASSV
BosonSampling is completely independent of the squeezing parameter
@xmath and so we may assume without loss of generality that @xmath . In
this limit @xmath and thus this case of PASSV BosonSampling reduces to
an instance of Fock state BosonSampling since @xmath . Now suppose
@xmath is some complexity class containing PASSV BosonSampling . Again
choosing @xmath , the inclusion @xmath similarly implies that @xmath
also contains Fock state BosonSampling .

#### 6.4.6 Discussion of PASSV Sampling

This PASSV result can be thought of in terms of letting the ket in Eq. (
6.33 ) act as a ‘background’ signal which is invariant during the
evolution of @xmath . Since the leading operator in Eq. ( 6.35 ) takes
the same form as Eq. ( 6.33 ), we would like the ket to also be
independent of the choice of @xmath under some measurement, while still
being distinguishable from a state which has an added or subtracted
photon. The technique used in this work may be able to be used to
characterize other states which implement a logically equivalent
classically intractable sampling problem. A goal of ours is to prove an
even more experimentally friendly set of states and measurements that
implements the same computationally complex sampling problem.

PASSV BosonSampling is harder to implement than ordinary BosonSampling ,
which is already quite challenging. One particularly strong criticism of
PASSV BosonSampling is that it may require the use of photon-number
resolving detectors to implement parity measurements instead of bucket
detectors. Whilst this is true, one only needs to distinguish the parity
of the even and odd photon-number Fock states and not the value of the
Fock state.

For any given @xmath and error rate the maximum number of necessarily
distinguishable Fock states may be reduced. PASSV BosonSampling may be
regarded as a generalization of Fock state BosonSampling , since in the
limit of small squeezing ( @xmath ), the SV reduces to a vacuum state
and an on/off detector suffices. Additional experimental hurdles arise
because squeezed states become more susceptible to noise. We do not
address experimental errors such as this in our work. Our goal is to
theoretically demonstrate the non-uniqueness of Fock states for
computationally hard sampling problems and develop new techniques for
understanding the computational complexity of other sampling problems.

In this work we have shown that orthogonal matrices are sufficiently
hard for PASSV sampling. A natural question is whether or not choosing
another matrix, such as a unitary matrix, could change the complexity of
the PASSV sampling problem. This question is not so easily solved since
Eq. ( 6.35 ) no longer holds. Intuitively we expect that the problem
would not become easier. In the limit of zero squeezing, we know there
is no special computational complexity transition since PASSV sampling
reduces to Fock state sampling. If such a complexity transition did
exist, such as in SPACS sampling of section 6.3 , then we would expect a
complexity phase transition at @xmath . The same sampling probabilities
however may be constructed with a different measurement scheme but this
remains an open question.

### 6.5 Generalized Cat State Sampling

In this section we consider a BosonSampling device where the input
states are arbitrary superpositions of coherent states. This constitutes
a broad class of continuous-variable optical states. In this work we
focus on analysing cat sampling in three separate limits:

1.  First, in section 6.5.1 we review the coherent state, analyse even
    and odd cat states, and show that their Taylor expansions reduce to
    the vacuum and single-photon Fock state respectively as @xmath .
    Thus, in the zero amplitude limit, cat sampling exactly reduces to
    BosonSampling and therefore yields a computationally hard problem.

2.  Second, in section 6.5.2 we analyse small, but non-zero amplitude
    odd cat states. This is equivalent to Fock state sampling with some
    components that are treated as an error. This error is related to
    the AA proof for approximate BosonSampling , where it is required
    that the error rate satisfies a 1/poly(n) bound. Thus small, but
    non-zero, amplitude odd cat states are also computationally hard.

3.  Third, in section 6.5.3 we analyse general cat states which are
    arbitrary superpositions of two or more coherent states. We
    demonstrate that the output state is a highly entangled
    superposition of an exponential number of multi-mode coherent states
    [ 163 , 164 , 165 , 166 , 167 ] , where the amplitude of each term
    is related to a permanent-like combinatoric problem, which would
    require exponential resources to compute via a brute-force approach.
    This provides strong evidence that such generalized optical sampling
    problems might be implementing classically hard problems.
    Determining a complete characterization of the computational
    complexity of such problems is a notoriously difficult open problem,
    but based on the evidence we present here, it likely resides in a
    classically hard class comparable to ideal BosonSampling .

Next, in section 6.5.4 we present a complexity theoretic argument for
the hardness of cat state sampling to further support our evidence. We
show that unless the polynomial hierarchy collapses to the third level
there must not exist an efficient randomized classical algorithm which
can produce an output distribution approximating that of an arbitrary
interferometer with multiplicative error of @xmath or less.

While such states may be more challenging to prepare than Fock states,
addressing this question sheds light on what makes a quantum optical
system classically hard to simulate, and may provide motivation for
developing technologies for preparing quantum states of light beyond
Fock states. We end this work with section 6.5.5 by discussing the
prospects for experimentally preparing general cat states.

#### 6.5.1 Zero Amplitude Cat analysis

Cat state is a generic term for an arbitrary superposition of
macroscopic states and may be used for quantum information processing [
168 ] . In quantum optics, this is generally understood to mean a
superposition of two coherent states, potentially with large amplitudes.
This is the definition we will use in this work.

Firstly, a coherent state is state of the quantum harmonic oscillator [
121 ] that most closely resembles a state of a classical harmonic
oscillator. It may be defined in the Fock basis as,

  -- -------- -- --------
     @xmath      (6.39)
  -- -------- -- --------

where @xmath is a complex number, @xmath represents the amplitude of the
state, and @xmath represents the phase of the state.

Two illustrative examples of superpositions of these coherent states are
the even ( @xmath ) and odd ( @xmath ) cat states, so-called because
they contain only even or odd photon-number terms respectively,

  -- -------- -- --------
     @xmath      (6.40)
  -- -------- -- --------

The odd cat state has the property that all of the even photon number
terms vanish. In the limit of @xmath its amplitude identically
approaches the single-photon state as shown here,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.41)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

In the limit as @xmath we ignore all higher order @xmath terms.

Furthermore, the vacuum state is given by a trivial cat state containing
only a single term in the superposition with a respective amplitude of
@xmath . Alternately, the vacuum state can be regarded as the zero
amplitude limit of the even cat state,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.42)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Thus, it is immediately clear that in the @xmath amplitude limit, cat
state sampling reduces to ideal BosonSampling , using an appropriate
configuration of odd and even cat states, which is a provably hard
problem. We use the term ‘provably hard’ to mean computationally hard,
assuming that ideal and approximate BosonSampling are computationally
hard. Specifically, to implement exact BosonSampling with cat states, we
choose our input state to be,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.43)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

which is exactly the form of Eq. ( 3.1 ). This example is trivial but
the point is to show a simple example of cat states leading to a
computationally hard problem in a particular limit, which raises the
question as to whether it remains hard as we transition out of that
limit. In App. A.3 we present an example of this reduction in the case
of Hong-Ou-Mandel interference to explicitly demonstrate that small
amplitude cats behave as single photons. This demonstrates that in
certain regimes, cat state sampling reproduces single-photon statistics.

#### 6.5.2 Small Amplitude Cat Analysis

Having established that cat sampling reduces to BosonSampling in the
zero amplitude limit, the obvious next question is ‘what if the
amplitude is small but non-zero?’. It was shown by AA that BosonSampling
, when corrupted by erroneous samples, remains computationally hard
provided that the error rate scales as @xmath . If we consider a small,
but non-zero, amplitude odd cat state, we can treat the
non-single-photon terms, which scale as a function of @xmath , as
erroneous terms. The error that these erroneous terms induce must be
kept below the @xmath bound. Specifically,

  -- -------- -- --------
     @xmath      (6.44)
  -- -------- -- --------

where @xmath defines the odd photon-number distribution and follows from
Eq. ( 6.58 ). The left underbraced component represents the desired
single-photon term and the right underbraced component represents the
remaining photon-number terms, which are treated as errors.

In App. A.4 we show that the bound on the amplitude of the cat states
for a provably hard sampling problem to take place is,

  -- -------- -- --------
     @xmath      (6.45)
  -- -------- -- --------

where we input odd cat states in every mode requiring a @xmath and
vacuum in the remaining modes. Although this function is exponential in
@xmath the probability of successfully sampling from the correct
distribution will satisfy this bound for sufficiently small values of
@xmath and @xmath . The value of @xmath may still be large enough
however to implement a post-classical BosonSampling device. Thus, it
follows that for non-zero, but sufficiently small @xmath , cat sampling
remains computationally hard.

We have established that cat state BosonSampling is a provably
computationally hard problem in two regimes: (1) in the @xmath amplitude
limit, in which case we reproduce ideal BosonSampling , and (2) for
non-zero but sufficiently small amplitudes, in which case the
non-single-photon-number terms may be regarded as errors, which remains
a computationally hard problem, subject to the bound given in Eq. ( 6.45
). Having established this, the remainder of this work is dedicated to
the completely general case where the terms in the cat states may have
arbitrary amplitude, potentially at a macroscopic scale.

#### 6.5.3 Arbitrary Amplitude Cat Analysis

In this section we will consider generalized cat state sampling which
are arbitrary superpositions of an arbitrary number of coherent states
of the form

  -- -------- -- --------
     @xmath      (6.46)
  -- -------- -- --------

We let the input state to our more generalized BosonSampling model
comprise @xmath arbitrary superpositions of @xmath coherent states

  -- -------- -- --------
     @xmath      (6.47)
  -- -------- -- --------

where @xmath is the coherent state of amplitude @xmath of the @xmath th
superposition term in the @xmath th mode, and @xmath is the amplitude of
the @xmath th term of the superposition in the @xmath th mode ¹ ¹ 1
Continuous superpositions are a simple generalization of our formalism,
and with this generalization arbitrary states could be expressed as
continuous superpositions of coherent states. . In line with traditional
BosonSampling , we can choose a number of the modes to be the vacuum.
This is achieved by setting @xmath and @xmath .

Expanding this expression yields a superposition of multi-mode coherent
states of the form

  -- -------- -- --------
     @xmath      (6.48)
  -- -------- -- --------

where @xmath is shorthand for @xmath . We propagate this state through
the passive linear optics network @xmath as illustrated in Fig. 6.6 .
Such a unitary network has the property that a multi-mode coherent state
is mapped to another multi-mode coherent state,

  -- -------- -- --------
     @xmath      (6.49)
  -- -------- -- --------

where the relationship between the input and output amplitudes is given
by

  -- -------- -- --------
     @xmath      (6.50)
  -- -------- -- --------

as shown in App. A.5 . @xmath acts on each term in the superposition of
Eq. ( 6.48 ) independently yielding an output state of the form,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.51)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

The number of terms in the output superposition is @xmath , scaling
exponentially with the number of modes given that @xmath .

Our goal is to sample this distribution using number-resolved
photodetectors, which are described by the measurement projectors,

  -- -------- -- --------
     @xmath      (6.52)
  -- -------- -- --------

where @xmath is the photon-number measurement outcome on the @xmath th
mode. Multi-mode measurements are described by the projectors,

  -- -------- -- --------
     @xmath      (6.53)
  -- -------- -- --------

where @xmath is the multi-mode measurement signature, with @xmath
photons measured in the @xmath th mode. The sample probabilities are
given by,

  -- -------- -- --------
     @xmath      (6.54)
  -- -------- -- --------

In the case of continuous-variable states, the number of measurement
signatures, @xmath , is unbounded as the photon-number is undefined,
unlike Fock states where the total photon-number is conserved.

Now we argue, without presenting a rigorous complexity argument, that
this sampling problem is likely computationally hard if three criteria
are satisfied:

1.  There must be an exponential number of terms in the output
    distribution to rule out brute-force simulation.

2.  The terms in the superposition are entangled so that the
    distribution cannot be trivially obtained by sampling each mode
    independently.

3.  To ensure that the individual output amplitudes are not easy to
    simulate the output distribution must be related to a
    computationally hard problem.

These criteria are general properties that classically hard problems are
known to exhibit, but there is no proof that these criteria are
sufficient to establish whether a problem is classically hard. For
example, ideal BosonSampling is known to be computationally hard,
satisfying all 3 criteria but fermionic-sampling is known to be
classically efficient since it violates criteria (3) because its output
amplitudes relate to matrix determinants rather than permanents, which
reside in the computational complexity class P .

Criteria (1) is achieved due to our choice of input state — there are
@xmath terms in the output distribution. It is easily seen that criteria
(2) holds in general. As a simple example, consider the input state,

  -- -------- -- --------
     @xmath      (6.55)
  -- -------- -- --------

a tensor product of two even cat states. Passing this separable two-mode
state through a 50/50 beamsplitter gives rise to the output state,

  -- -------- -- --------
     @xmath      (6.56)
  -- -------- -- --------

where @xmath is a cat state. This is a path-entangled superposition of a
cat state across two modes. Thus, while Eq. ( 6.49 ) demonstrates that a
unitary network maps a tensor product of coherent states to a tensor
product of coherent states, such a network will generate
path-entanglement when the input state is a tensor product of
superpositions of coherent states. Note the structural similarity
between cat state interference and two-photon Hong-Ou-Mandel (HOM) [ 28
] type interference. In the case of HOM interference we have @xmath ,
whereas for cat states we have @xmath .

It was recently and independently reported by Jiang et al. [ 78 ] that
linear optics networks fed with nonclassical pure states of light almost
always generates modal entanglement, consistent with our observation
here. This ensures that the output state to our generalized
BosonSampling device is highly entangled, thus satisfying criteria (2).
However, Jiang et al. present no discussion about our hardness criteria
(3); they do not connect their states to a computationally hard problem.
Thus their work provides a necessary but not sufficient proof of
computational hardness. It is important, as in our work here, to examine
such non-classical input states individually and make the case for the
importance of criteria (3). For example it is well known from the
Gottesman-Knill theorem that some systems with exponentially large
Hilbert spaces that satisfy our criteria (1) and (2) can nevertheless be
efficiently simulated. An example is the circuit model for quantum
computation that deploys only gates from the Clifford algebra.

Finally let us consider criteria (3). Let the expansion for a coherent
state be,

  -- -------- -- --------
     @xmath      (6.57)
  -- -------- -- --------

in the photon-number basis, where,

  -- -------- -- --------
     @xmath      (6.58)
  -- -------- -- --------

is the amplitude of the @xmath -photon term. Then,

  -- -------- -- --------
     @xmath      (6.59)
  -- -------- -- --------

Thus, acting the measurement projector for configuration @xmath , Eq. (
6.53 ), on the output state, Eq. ( 6.51 ), we obtain,

  -- -------- -- --------
     @xmath      (6.60)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (6.61)
  -- -------- -- --------

and the sampling probability takes the form @xmath . We can group the
terms under the product and label them @xmath . Then the amplitudes are
given by,

  -- -------- -- --------
     @xmath      (6.62)
  -- -------- -- --------

which has the same analytic structure as the permanent when @xmath but
sums over additional terms that are not present in the permanent.
Evaluating this combinatoric problem requires exponential resources
using brute-force. Via brute force, evaluating this expression requires
summing @xmath terms. Given that Eq. ( 6.62 ) has the same analytic form
as the matrix permanent, which is known to be classically hard, this
implies a striking similarity between cat state sampling and Fock state
sampling, with the constraint that @xmath is of a form whose permanent
is not trivial. In fact, in the @xmath limit, evaluating this
combinatoric expression must be as hard as calculating an @xmath matrix
permanent, since we know that in this limit the problem reduces to ideal
BosonSampling . In the original proof by Aaronson & Arkhipov, it is
required that @xmath is Haar-random. It is an open question as to
whether @xmath can be made Haar-random in the presented generalized
BosonSampling model.

In the trivial case of @xmath this expression simplifies to,

  -- -------- -- --------
     @xmath      (6.63)
  -- -------- -- --------

which evaluates in polynomial time. In this case the input state is
simply a tensor product of coherent states, and the runtime is
consistent with the known result that simulating coherent states is
trivial as the tensor product structure allows sampling to proceed by
independently sampling each mode, each of which is an efficient sampling
problem. However, when @xmath the complexity of directly arithmetically
evaluating Eq. ( 6.61 ) grows exponentially.

#### 6.5.4 A Computational Complexity Argument

We found that unless the polynomial hierarchy collapses to the third
level (i.e. @xmath [ 67 ] ) there must not exist an efficient randomized
classical algorithm which can approximate generalized cat state sampling
with a multiplicative error of @xmath or less. The existence of this
algorithm would imply the collapse of computational complexity classes
believed to be distinct. The polynomial hierarchy PH is composed of an
infinite number of levels @xmath , which are themselves composed of
complexity classes @xmath , @xmath , and @xmath , where @xmath .

Bremner, Jozsa & Shepherd [ 169 ] developed a method for showing the
intractability of classical simulation of certain circuits composed of
commuting gates that we use. This technique has since been used in other
work [ 170 , 171 ] . Post-selection is important in the method as the
computational cost for certain computation is used only if that
computation results in a particular post-selection measurement.

We provide informal definitions used in our proof of two computational
complexity classes based on post-selection also used in [ 170 ] . For
formal definitions please see [ 169 ] . These classes are defined in
terms of either classical or quantum circuits with output and
post-selection registers @xmath and @xmath respectively. A language
@xmath is in the class @xmath if and only if there exists a uniform
family of classical circuits and a @xmath such that:

1.  if @xmath then @xmath ,

2.  if @xmath then @xmath .

Similarly, a language @xmath is in the class PostBQP if the above
criteria is satisfied for a uniform family of quantum circuits. We
define a third complexity class PostCAT to understand post-selection
applied to the interferometry experiments. We let a language @xmath be
in PostCAT if a uniform family of @xmath -port linear interferometers
acting on cat state inputs satisfying the preceding criteria exist.

It is known that PostBPP corresponds to @xmath , which is contained
within @xmath [ 172 ] . It was shown by Aaronson that @xmath [ 173 ] .
This is surprising, since @xmath , implying a difference between the
power of post-selected classical and quantum computation unless @xmath .
Following from the definitions of PostBPP and PostBQP given above, this
strengthens the argument that an efficient classical randomized
algorithm cannot produce the output of any quantum circuit. This is
because it would yield @xmath and collapse the polynomial hierarchy,
which is believed to be highly unlikely. More specifically: The
existence of a polynomial time randomized classical algorithm which
approximates the output distribution to an arbitrary quantum circuit to
within multiplicative error of @xmath would yield @xmath [ 169 , 170 ] .

To finish the proof we need to show that @xmath , from which it would
follow that the existence of an efficient randomized classical algorithm
which can approximate the output distribution of an arbitrary linear
interferometer applied to cat state inputs to within a multiplicative
error of @xmath would imply that @xmath and hence @xmath . It has
already been shown that @xmath in the work of Ralph et al. [ 174 ] .
They proved that arbitrary quantum circuits could be probabilistically
implemented exactly on qubits encoded as a superposition of even and odd
parity cat states and there is always some probability of obtaining a
measurement result which correctly implements the desired gate with unit
fidelity. This means that by post-selecting on these outcomes, the
system can be made to implement an arbitrary quantum circuit. Now
because post-selection could be applied to the output of the circuit and
remaining in PostCAT , it means that any computation in PostBQP is also
in PostCAT . Therefore, unless @xmath there does not exist an efficient
randomized classical algorithm which can produce an output distribution
approximating that of an arbitrary interferometer with multiplicative
error of @xmath or less.

#### 6.5.5 Preparing Cat States

Finally, we will discuss the prospects for experimentally preparing cat
states of the form used in our derivation. There exists a significant
number of schemes for generating a finite number of superpositions of
coherent states all of which are extremely difficult to scale to higher
order cat states. For example, superpositions of coherent states with
equal amplitudes but different phases can be produced with quantum
nondemolition (QND) measurements [ 175 ] via the interaction of a strong
Kerr nonlinearity [ 176 , 121 ] . Another approach is to use strong Kerr
nonlinearities together with coupled Mach-Zehnder interferometers [ 121
] but this is impractical as outside the cavity a strong Kerr would
require a coherent Electromagnetically Induced Transparency [ 177 , 178
] effect in an atomic gas cloud and even there in practice the
nonlinearities are too weak for our purposes.

In a similar way that measurements of photon number can produce discrete
coherent state superpositions in phase. Measurements of the phase can
produce discrete coherent state superpositions in amplitude. This can be
understood via the number-phase uncertainty relation. Any improved
knowledge of the phase of a state induces kicks in the number and vice
versa. In this way, by combining such different measurements, one can
produce discrete superpositions in both phase and amplitude, which
approaches the arbitrary superpositions of coherent states we require.
Exactly such a scheme was proposed by Jeong et al. in 2005 [ 179 , 180 ]
. By combining both types of detection schemes, even with detectors of
non-unit efficiency, they show that a large number of propagating
superpositions of coherent states may be produced. These states then
could be used in proof-of-principle experiments for our protocol
outlined here.

### 6.6 Summary

An open question is ‘what classes of quantum states of light yield hard
sampling problems using linear optics?’ In this chapter we have studied
this question in detail. We began by motivating this problem in section
6.2 . Specifically, we studied single-photon-added coherent states
(SPACS) in section 6.3 , photon-added or -subtracted squeezed vacuum
(PASSV) states in section 6.4 , and generalized superpositions of
coherent states called cat states in section 6.5 . Below we present a
summary of the conclusions we were able to draw from sampling these
three states:

-   SPACS: We first showed that displaced single-photon Fock state
    sampling remains hard to efficiently simulate for all values of the
    displacement with a coincidence photon number detection. We then
    considered a more interesting problem using SPACS sampling and found
    that this problem transitions from computationally hard to simulate
    too computationally easy as the amplitude of the coherent states
    increases from near zero into the limit of large coherent
    amplitudes.

-   PASSV: We have shown a direct mapping between Fock state
    BosonSampling and PASSV BosonSampling and that it operates in all
    squeezing parameter regimes. In other words there are no bounds on
    the amount of squeezing and no approximations need to be made. This
    is unique as compared to the case of SPACS sampling. In this
    protocol we use a Haar random matrix with all real elements for the
    evolution which is different than the original BosonSampling
    protocol where the Haar random matrix has complex values as well.
    This was so the squeezed and anti-squeezed quadratures of the
    squeezed vacuum do not mix with each other.

-   Generalized Cat State Sampling: We have presented evidence that a
    linear optics network, fed with arbitrary superpositions of coherent
    states, and sampled via number-resolved photodetection, is likely to
    be a classically hard problem. Our argument relies on three
    realistic criteria for computational hardness of a sampling problem
    of which these satisfy. We led the reader into the more difficult
    generalized case by first analysing what happens in the zero
    amplitude case and then the small amplitude limit case. Also,
    because coherent states form an over-complete basis, any pure
    optical state can be expressed in terms of coherent states,
    suggesting that most quantum states of light may yield hard sampling
    problems.

These results demonstrate that there are a large class of non-Fock
states that have associated sampling problems of equal computational
complexity to BosonSampling which means that there is nothing unique
about the computational complexity of single-photon Fock state
BosonSampling . In fact, it seems that there is a plethora of other
quantum states of light that exhibit similar sampling computational
complexity. These results support a conjecture presented in [ 113 ] that
computational complexity relates to the negativity of the Wigner
function as all of these analysed states have negative Wigner functions.
This work helps us to understand what constitutes a computationally hard
sampling problem and what it is about quantum states that are
computationally complex to sample from. The states presented here are
more experimentally challenging to create than single-photon Fock
states. This work will further motivate the need to develop quantum
state sources than can produce states other than Fock states.

{savequote}

[45mm] If you fail, never give up because F.A.I.L means “First Attempt
In Learning.” End is not the end, in fact E.N.D. means “Effort Never
Dies.” If you get no as an answer remember N.O. means “Next
Opportunity.” So Let us be positive. \qauthor A.P.J. Abdul Kalam

## Chapter 7 BosonSampling inspired Linear Optical Quantum Metrology —
An Application

### 7.1 Synopsis

It is known that quantum number-path entanglement is a resource for
super-sensitive quantum metrology as it allows for sub-shotnoise or even
Heisenberg-limited sensitivity. All known methods for generating such
number-path entanglement are extremely challenging because it requires
either very strong nonlinearities, or nondeterministic preparation
schemes with feed-forward, which are difficult to implement. We know
from studying quantum random walks with multi-photon walkers as well as
BosonSampling that passive linear optical devices generate a
superexponentially large amount of number-path entanglement.

In this work we show a method to use this resource of entanglement for
quantum metrology, which is motivated in section 7.2 . We show in
section 7.3 that a simple, passive, linear-optical interferometer — fed
with only uncorrelated, single-photon inputs, coupled with simple,
single-mode, disjoint photodetection — is capable of beating the
shotnoise limit. It is important to note that this protocol is an
alteration to the original BosonSampling protcol as presented by AA and
so it is an application inspired by BosonSampling and not exactly an
aplication of BosonSampling . Nonetheless, our result allows for
practical quantum metrology with readily available technology. Due to
the uniqueness of our architecture we use a new resource counting method
that we coined ordinal resource counting (ORC) as discussed in section
7.4 . ¹ ¹ 1 It is worth noting that this metrology work has three Lord
of the Rings [ 181 ] references in it. See if you can find them.

### 7.2 Motivation

Quantum number-path entanglement is a resource for super-sensitive
quantum metrology, as shown by Yurke & Yuen, allowing for sensors that
beat the shotnoise limit [ 182 , 183 ] . These sensors have applications
in super-sensitive gyroscopy [ 184 ] , gravimetry [ 185 ] , optical
coherence tomography [ 186 ] , ellipsometry [ 187 ] , magnetometry [ 188
] , protein concentration measurements [ 189 ] , and microscopy [ 190 ,
191 ] . This line of work culminated in the analysis of the bosonic NOON
state ( @xmath , where @xmath is the total number of photons). This was
shown to be optimal for local phase estimation with a fixed, finite
number of photons, and in fact allows one to hit the Heisenberg limit
and the Quantum Cramér-Rao Bound [ 192 , 193 , 194 , 195 ] .

The NOON state is used in a two-mode interferometer where all @xmath
particles are in a superposition of being in the first mode (with zero
in the second mode) or in the second mode (with zero in the first mode).
This state is known to be optimal as it reaches the Heisenberg limit but
its generation is known to be quite difficult. There are two main
methods for preparing NOON states: the first is to deploy very strong
optical nonlinearities [ 196 , 197 ] , and the second is to prepare them
using measurement and feed-forward [ 198 , 199 , 200 ] . These are
similar requirements to building a universal optical quantum computer
and thus NOON-states are just as difficult to build [ 201 ] . In
addition to being complicated to prepare the detection scheme is quite
challenging as parity measurements at each output port also likely need
to be performed [ 202 ] .

Recently two independent lines of research, the study of quantum random
walks with multi-photon walkers in passive linear-optical
interferometers [ 203 , 90 , 91 ] , as well as the computational
complexity analysis of the sampling problem using such devices [ 67 ,
107 ] , has led to a startling conclusion — passive, multi-mode,
linear-optical interferometers, fed with only uncorrelated single
photons in each mode, produce quantum mechanical states with path-number
entanglement that grows superexponentially fast in the two resources of
mode and photon-number. For another practical application inspired by
BosonSampling see [ 86 ] . It is remarkable is that such a large degree
of number-path entanglement is generated without the use of strong
optical nonlinearities or with complicated measurement and feed-forward
schemes. It is generated using the evolution of the single photons in a
passive linear optical device. It is commonly misunderstood that such
passive devices have ‘non-interacting’ photons in them. There is however
a type of photon-photon interaction due to the demand of bosonic state
symmetrization from multiple applications of the Hong-Ou-Mandel effect [
91 ] , which yields a superexponentially large amount of number-path
entanglement. It is known that the evolution of single photons in a
linear optical device, followed by projective measurements, can give
rise to ‘effective’ strong optical nonlinearities. We conjecture that
there is a hidden Kerr-like nonlinearity in these interferometers [ 93 ]
. Like BosonSampling [ 67 ] , and unlike universal quantum computing
schemes such as that by Knill, Laflamme, and Milburn [ 8 ] , this
protocol is deterministic and does not require any ancillary photons.

The advantage of our BosonSampling inspired method for quantum metrology
is that generating and detecting single photons is quite standardized
and relatively straightforward to implement in the lab [ 204 , 70 , 69 ,
73 , 68 , 79 , 95 ] . The linear optical community is moving towards
single photons, linear interferometers, and single-photon detectors all
on a single, integrated, photonic chip, which allows for the scalability
of linear optical devices to large numbers of modes and photons. This
implies that scalable quantum metrology using our technique is feasible
in the near future. We now show a method for using passive linear optics
for quantum metrology.

### 7.3 Metrological Device

The phase-sensitivity, @xmath , of a metrology device can be defined in
terms of the standard error propagation formula as,

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

where @xmath is the expectation of the observable being measured and
@xmath is the unknown phase we seek to estimate.

The photons evolve through a unitary network according to @xmath . In
our protocol, we construct the @xmath -mode interferometer @xmath to be,

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

which we call the quantum fourier transform interferometer (QuFTI)
because @xmath is the @xmath -mode quantum Fourier transform matrix,
with matrix elements given by,

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

@xmath and @xmath are both diagonal matrices with linearly increasing
phases along the diagonal represented by,

  -- -------- -- -------
     @xmath      
     @xmath      (7.4)
  -- -------- -- -------

where @xmath is the unknown phase one would like to measure and @xmath
is the control phase. @xmath is introduced as a reference, which can
calibrate the device by tuning @xmath appropriately. To see this tuning
we combine @xmath and @xmath into a single diagonal matrix with a
gradient given by,

  -- -------- -- -------
     @xmath      (7.5)
  -- -------- -- -------

The control phase @xmath can shift this gradient to the optimal
measurement regime, which can be found by minimizing @xmath with respect
to @xmath and @xmath . Since this is a shift according to a known phase,
we can for simplicity assume (and without loss of generality) that
@xmath is in the optimal regime for measurements and @xmath . Thus,
@xmath and is left out of our analysis for simplicity.

In order to understand how such a linearly increasing array of unknown
phase shifts may be arranged in a practical device, it is useful to
consider a specific example. Let us suppose that we are to use the QuFTI
as an optical magnetometer. We consider an interferometric magnetometer
of the type discussed in [ 205 ] where each of the sensing modes of the
QuFTI contains a gas cell of Rubidium prepared in a state of
electromagnetically induced transparency whereby a photon passing
through the cell at the point of zero absorption in the
electromagnetically induced transparency spectrum acquires a phase shift
that is proportional to the product of an applied uniform (but unknown)
magnetic field and the length of the cell. We assume that the field is
uniform across the QuFTI, as would be the case if the entire
interferometer was constructed on an all optical chip and the field
gradient across the chip were negligible. Since we are carrying out
local phase measurements (not global) we are not interested in the
magnitude of the magnetic field but wish to know if the field changes
and if so by how much. (Often we are interested in if the field is
oscillating and with what frequency.) Neglecting other sources of noise
then in an ordinary Mach-Zehnder interferometer this limit would be set
by the photon shotnoise limit. To construct a QuFTI with the linear
cascade of phase shifters, as shown in Fig. 7.1 , we simply increase the
length of the cell by integer amounts in each mode. The first cell has
length @xmath , the second length @xmath , and so forth. This will then
give us the linearly increasing configuration of unknown phase shifts
required for the QuFTI to beat the SNL.

One might question why one would employ a phase gradient rather than
just a single phase. Investigation into using a single phase in @xmath
indicates that this yields no benefit. We conjecture that this is
because the number of paths interrogating a phase in a single mode is
not superexponential as is the case when a phase gradient is employed.

The interferometer may always be constructed efficiently following the
protocol of Reck et al. [ 109 ] , who showed that an @xmath linear
optics interferometer may be constructed from @xmath linear optical
elements (beamsplitters and phase-shifters), and the algorithm for
determining the circuit has runtime polynomial in @xmath . Thus, an
experimental implementation of our protocol may always be efficiently
realized.

The input state to the device is @xmath , i.e. single photons inputed in
each mode. If @xmath then @xmath and thus @xmath . In this instance, the
output state is exactly equal to the input state, @xmath . Thus, if we
define @xmath as the coincidence probability of measuring one photon in
each mode at the output, then @xmath when @xmath . When @xmath , in
general @xmath . Thus, intuitively, we anticipate that @xmath will act
as a witness for @xmath .

In the protocol, assuming a lossless device, no measurement events are
discarded. Upon repeating the protocol many times, let @xmath be the
number of measurement outcomes with exactly one photon per mode, and
@xmath be the number of measurement outcomes without exactly one photon
per mode. Then @xmath is calculated as @xmath . Thus, all measurement
outcomes contribute to the signal and none are discarded. Note that, due
to preservation of photon-number and the fact that we are considering
the anti-bunched outcome, @xmath may be experimentally determined using
non-number-resolving detectors if the device is lossless. If the device
is assumed to be lossy, then number-resolving detectors would be
necessary to distinguish between an error outcome and one in which more
than one photon exits the same mode. The circuit for the architecture is
shown in Fig. 7.1 .

The state at the output to the device is a highly path-entangled
superposition of @xmath terms, which grows superexponentially with
@xmath . This corresponds to the number of ways to add @xmath
non-negative integers whose sum is @xmath , or equivalently, the number
of ways to put @xmath indistinguishable balls into @xmath
distinguishable boxes. We conjecture that this superexponential
path-entanglement yields improved phase-sensitivity as the paths query
the phases a superexponential number of times.

The observable being measured is the projection onto the state with
exactly one photon per output mode, @xmath . Thus, @xmath . And, the
phase-sensitivity estimator reduces to,

  -- -------- -- -------
     @xmath      (7.6)
  -- -------- -- -------

Following the result of [ 112 ] , @xmath is related to the permanent of
@xmath as,

  -- -------- -- -------
     @xmath      (7.7)
  -- -------- -- -------

Here the permanent of the full @xmath matrix is computed, since exactly
one photon is going into and out of every mode. This is unlike the
BosonSampling protocol [ 67 ] where permanents of sub-matrices are
computed.

We will now examine the structure of this permanent. The matrix form for
the @xmath -mode unitary @xmath is given by,

  -- -------- -- -------
     @xmath      (7.8)
  -- -------- -- -------

as derived in App. A.6 . Taking the permanent of this matrix is
challenging as calculating permanents are in general #P -hard. However,
based on calculating @xmath for small @xmath , we observe the empirical
pattern,

  -- -------- -- -------
     @xmath      (7.9)
  -- -------- -- -------

as conjectured in App. A.7 . This analytic pattern we observe is not a
proof of the permanent, but an empirical pattern — a conjecture — that
has been verified by brute force to be correct up to @xmath . Although
we do not have a proof beyond that point, @xmath is well beyond what
will be experimentally viable in the near future, and thus the pattern
we observe is sufficient for experimentally enabling super-sensitive
metrology with technology available in the foreseeable future.

Following as a corollary to the previous conjecture, the coincidence
probability of measuring one photon in each mode is,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (7.10)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

as shown in App. A.8 , where

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (7.11)
  -- -------- -------- -------- -- --------

The dependence of @xmath on @xmath and @xmath is shown in Fig. 7.2 .

It then follows that,

  -- -------- -- --------
     @xmath      (7.12)
  -- -------- -- --------

as shown in App. A.9 .

Finally, we wish to establish the scaling of @xmath . With a small
@xmath approximation ( @xmath , @xmath ) we find,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

as shown in App. A.10 . Thus, the phase sensitivity scales as @xmath as
shown in Fig. 7.3 .

### 7.4 Ordinal Resource Counting (ORC)

We would like to compare the performance of our QuFTI to an equivalent
multimode interferometer baseline for which we will construct the
shotnoise limit (SNL) and Heisenberg limit (HL). This is a subtle
comparison, due to the linearly increasing unknown phase-shifts, {
@xmath }, that the QuFTI requires to operate. The mathematical relation
is shown in Fig. 7.3 , where we have converted the number of resources,
@xmath , to the number of photons, @xmath . There is disagreement on how
such resources should be counted. This is the method, which we call
Ordinal Resource Counting (ORC), that we feel most fairly counts our
resources. A more detailed supporting discussion can be found in App.
A.11 .

While computing the sensitivity using the standard error propagation
formula of Eq. ( 7.1 ) provides clear evidence that our scheme does
indeed beat the SNL, it would be instructive to carry out a calculation
of the quantum Fisher information and thereby provide the quantum
Cramér-Rao bound, which would be a true measure of the best performance
of this scheme possible, according to the laws of quantum theory.
However, due to the need to compute the permanent of large matrices with
complex entries, this calculation currently remains intractable. We are
continuing to investigate such a computation for a future work. In
general, analytic solutions to matrix permanents are not possible. In
this instance, the analytic result is facilitated by the specific
structure of the QuFTI unitary. Other phase gradients may yield analytic
results, but we leave this for future work as well.

### 7.5 Efficiency

We consider how in the presence of inefficient photon sources and
photo-detectors the success probability of the protocol will drop
exponentially with the number of photons. Specifically, if @xmath and
@xmath are the source and detection efficiencies respectively, the
success probability of the protocol is @xmath . Current cutting edge
transition edge detectors operate at 98% efficiency, with negligible
dark count [ 206 ] . SPDC sources are the standard photon-source
technology but they are non-deterministic. However, there are techniques
that can greatly improve the heralding efficiency up to 42% at 2.1 MHz [
207 ] . Also, other source technologies, such as quantum dot sources are
becoming viable with efficiencies also up to 42% [ 208 ] . For @xmath ,
which is already well beyond current experiments, this yields @xmath ,
which is about 300 successful experimental runs per second when
operating with 2.1 MHz sources.

In App. A.12 we analyse dephasing, which is a source of decoherence, and
find that the QuFTI protocol is far more robust against dephasing than
the NOON state is.

### 7.6 Summary

We began this chapter with section 7.2 by discussing some of the history
of quantum metrology and why we need more simplistic implementations for
quantum metrology. Then, in section 7.3 , we showed that a passive
linear optics network fed with single-photon Fock states may implement
quantum metrology with phase-sensitivity that beats the shotnoise limit.
This scheme was inspired by BosonSampling . Unlike other schemes that
employ exotic states such as NOON states which are notoriously difficult
to prepare, single-photon states may be readily prepared in the
laboratory using present-day technology. This new approach to metrology
via easy-to-prepare single-photon states and disjoint photodetection
provides a road towards improved quantum metrology with frugal physical
resources. Importantly we use a new resource counting technique called
ordinal resource counting as discussed in section 7.4 . There remains
several open questions to be answered which are the subject of a soon to
be published followup manuscript [ 209 ] . Particularly, we will compare
and contrast different interferometric schemes, discuss resource
counting, calculate exact quantum Cramér-Rao bounds, and study details
of experimental errors.

{savequote}

[45mm] The creative principle resides in mathematics. In a certain
sense, therefore, I hold it true that pure thought can grasp reality, as
the ancients dreamed. \qauthor Albert Einstein

## Chapter 8 -hardness of Certain Multidimensional Integrals

### 8.1 Synopsis

Here we show that multi-dimensional integrals of a certain form can be
classified as #P -hard like in BosonSampling . In BosonSampling the
output statistics are given by matrix permanents. We map the
BosonSampling output statistics to an integral formalism using using
quantum optical characteristic functions, which represent the state of
the system in phase-space. The output statistics in the integral
formalism mapping is equivalent to matrix permanents yielding a
structure of integrals that can in general be categorised as #P -hard.
This yields new insight into the computational complexity of solving
certain classes of multidimensional integrals. Our work provides a new
approach for using methods from quantum physics to prove statements in
computer science.

In section 8.2 we motivate this work and present the main result of this
work. In section 8.3 we show a detailed proof and in section 8.4 we show
an example using permutation matrices, which yield the expected result
of unity.

### 8.2 Motivation

The BosonSampling work by Aaronson & Arkhipov [ 67 ] has led to much
interest in the physics community because it is a simple approach to
implementing a computationally hard problem. By mapping the
BosonSampling problem to an integral formalism we can obtain new
insights into integrals of a certain form. We do this by using quantum
optical characteristic functions to represent the output state of
BosonSampling as a multi-dimensional integral. This integral formalism
directly maps to matrix permanents which are known to be #P -hard in
general. Thus we have shown a class of integrals that are also #P -hard.
Our work shows broad applications for utilizing quantum optics tools, in
particular, and quantum physics paradigms, in general, to pose and to
answer questions about the computational complexity of certain
mathematical problems.

We show that integrals of the following form are #P -hard in general:

  -- -------- -------- -- -- -------
     @xmath   @xmath         (8.1)
  -- -------- -------- -- -- -------

This construction may be used as a new tool for examining open problems
regarding the complexity of BosonSampling like problems. As examples of
this formalism we show that the identity and permutation matrices are
trivial to solve. Recently Rahimi-Keshari et al. [ 210 ] showed the
conditions necessary for the efficient classical simulation of
quantum-optics experiments given particular input states in a quantum
process with measurements at the output, which is relevant to this work
because they use similar techniques.

### 8.3 Proof

To prove this we model the output photostatistics of a BosonSampling
system with characteristic functions [ 121 ] . This represents the state
in phase-space and allows other representations (e.g. Wigner function)
to be calculated. This formalism is identical but expresses the problem
in terms of multidimensional integrals. The integral equations that
arise then must also be #P -hard.

We begin with an @xmath -mode separable input state of the form

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

The single-mode characteristic @xmath function [ 121 ] is defined as

  -- -------- -- -------
     @xmath      (8.3)
  -- -------- -- -------

where @xmath is the displacement operator, given by

  -- -------- -- -------
     @xmath      (8.4)
  -- -------- -- -------

and @xmath is an arbitrary complex number representing the amplitude of
the displacement in phase-space. This generalizes to the multi-mode case
as

  -- -------- -- -------
     @xmath      (8.5)
  -- -------- -- -------

where @xmath is the displacement operator on the @xmath mode. Then, the
characteristic function for the state evolved via linear optics is

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (8.6)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where

  -- -------- -- -------
     @xmath      (8.7)
  -- -------- -- -------

as shown in App. A.13 . When the input state @xmath is separable, as per
Eq. ( 8.2 ), with @xmath so there are @xmath single photons in the first
@xmath modes, the multi-mode characteristic @xmath function reduces to

  -- -------- -------- -------- -------- -------
     @xmath   @xmath                     (8.8)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

We have used the identity shown in App. A.14 . Here

  -- -------- -- -------
     @xmath      (8.9)
  -- -------- -- -------

is the total energy of the system with amplitudes @xmath (or
equivalently @xmath due to energy conservation).

At this stage the characteristic @xmath function, @xmath , can always be
efficiently calculated with any separable input state, since it is
factorizable and there is no exponential growth in the number of terms.
The complexity arises when we wish to determine properties of the state,
such as reconstructing the Wigner function or determining individual
amplitudes within the state.

Next we consider the Wigner function, which may be computed as a type of
Fourier transform of @xmath [ 121 ] ,

  -- -------- -- --------
     @xmath      (8.10)
  -- -------- -- --------

in the single-mode case, which again logically generalizes to the
multi-mode case as

  -- -------- -- --------
     @xmath      (8.11)
  -- -------- -- --------

where all our integrals implicitly run over the range @xmath .

Let us denote

  -- -------- -- --------
     @xmath      (8.12)
  -- -------- -- --------

We can then evaluate the Wigner function as, for the @xmath -photon
input,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (8.13)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We have focused on the case where the input state is @xmath . Now we
consider a particular output amplitude where a single photon is measured
in the first @xmath output modes. This is determined by calculating the
expectation value of the projector @xmath , which is equal to the
expectation value of the @xmath -dimensional number operator, @xmath ,
where @xmath . In the usual permanent-based approach, this amplitude
corresponds to the permanent of a @xmath submatrix of @xmath .

Now we will consider a phase-space approach. For a single-mode state,
the expectation value of the number operator may be obtained from the
Wigner function as [ 121 ] ,

  -- -------- -------- -- --------
     @xmath   @xmath      (8.14)
  -- -------- -------- -- --------

where the @xmath term is obtained by expressing @xmath in symmetrically
ordered form, @xmath , and making the substitution @xmath , @xmath .

In the multimode case this generalizes to

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath is the probability of a single-photon output in each of the
first @xmath input modes ( @xmath ).

To further simplify this expression, we use the identity

  -- -------- -- --------
     @xmath      (8.16)
  -- -------- -- --------

This is true for all components of the vector @xmath . This leads us to
simplify Eq. ( 8.3 ) as such. We expand out the first product in the
last line to give a polynomial in @xmath and @xmath . We then eliminate
terms by multipling @xmath .

We note that if the terms contain @xmath and @xmath with a total odd
power, then the function is odd, and so the integral must yield zero.
Also, for any @xmath with @xmath the term does not contain @xmath or
@xmath and so the integral for that term must yield zero due to the
identity in Eq. ( 8.16 ). Using these two properties, we can see that
the remaining terms must contain each @xmath or @xmath a nonzero even
number of times for each @xmath . Since the first product in Eq. ( 8.3 )
goes to @xmath , the maximum total power of @xmath or @xmath in any term
is @xmath and so they must appear exactly twice.

With these properties all of the terms in the first product of the sum
in Eq. ( 8.3 ) for @xmath must yield zero. This means that the sum can
be truncated to @xmath . The expression then takes the form

  -- -------- -- --------
     @xmath      (8.17)
  -- -------- -- --------

It is now clear that @xmath only depends on the @xmath submatrix of
@xmath as expected because our knowledge from BosonSampling tells us
that the probability is given from the permanent of this submatrix.
Also, the @xmath in the first product of Eq. ( 8.3 ) only yields terms
that integrate to zero. We can then further simplify @xmath to

  -- -------- -- --------
     @xmath      (8.18)
  -- -------- -- --------

Next, using the identity

  -- -------- -- --------
     @xmath      (8.19)
  -- -------- -- --------

and integrating over all @xmath for @xmath gives

  -- -------- -- --------
     @xmath      (8.20)
  -- -------- -- --------

We can simplify further by expanding the first product. It is easy to
check that

  -- -------- -- --------
     @xmath      (8.21)
  -- -------- -- --------

and similarly for @xmath . That means that any terms with @xmath or
@xmath will integrate to zero. The terms that do not integrate to zero
are those with the product @xmath . We can also check that

  -- -------- -- --------
     @xmath      (8.22)
  -- -------- -- --------

and therefore

  -- -------- -- --------
     @xmath      (8.23)
  -- -------- -- --------

Therefore, the value of @xmath corresponds to the coefficient of @xmath
in the product @xmath . It is convenient to express this as

  -- -------- -- --------
     @xmath      (8.24)
  -- -------- -- --------

We can find the coefficient of @xmath by using the MacMahon Master
theorem on both expressions in brackets on the right-hand side [ 211 ] .
The coefficient of @xmath in the first brackets is @xmath , where @xmath
denotes the @xmath submatrix of @xmath . Similarly, the coefficient of
@xmath in the second brackets is @xmath . The coefficient of @xmath in
the product @xmath is then @xmath and so that means that we can evaluate
@xmath as @xmath .

We have found several forms of the integral for the probability. As
expected this integral can be evaluated to the square of the permanent.
Any of these forms of the integral will suffice but for simplicity we
will focus on Eq. ( 8.3 ). This equation gives an equivalence of two
different forms, that is a matrix permanent and a multidimensional
integral, for a particular output amplitude of a linear optics network.
It follows that since matrix permanents are known to be #P -hard in
general then integrals of this form are also #P -hard in general.

The integral formalism may be approximated with Monte Carlo sampling.
Since the quantity being sampled takes positive and negative values it
is difficult to accurately approximate. Nevertheless, there may be some
examples where Monte Carlo sampling is more efficient in the integral
formalism than for the permanent formalism but we have not found an
example.

### 8.4 Permutation Matrix Example

It is known that certain examples of matrix permanents can be calculated
efficiently due to certain symmetries or sparsities in the matrix. One
example is permutation matrices @xmath , where @xmath are elements of
the symmetric group. We show that example using our integral formalism
is as expected using matrix permanents.

When @xmath , we have the property

  -- -------- -- --------
     @xmath      (8.25)
  -- -------- -- --------

and we see that @xmath is separable across @xmath . In this case, the
@xmath -dimensional integral from Eq. ( 8.3 ) also becomes separable,
forming an @xmath -dimensional product of integrals,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (8.26)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where the integrals can easily be shown. Given the separable structure
this is clearly computationally efficient to evaluate. Thus, we have
confirmed that @xmath for all @xmath and @xmath , as expected.

### 8.5 Summary

We have shown that a particular class of integrals is #P -hard in
general. This is motivated in section 8.2 . We did this by representing
the known state of the output amplitudes of a BosonSampling device to a
integral formalism as shown in section 8.3 . We have shown that by
employing two alternate but equivalent approaches to expressing the
output amplitudes of linear optics networks fed with single-photon
inputs, we are able to provide a quantum optical equivalence between
matrix permanents and a particular class of multidimensional integrals.
This implies that this class of integrals is #P -hard in the worst case.
The equivalence provides two important insights with broad impact.
Firstly, it demonstrates the #P -hardness of these multi-dimensional
integrals. Secondly, by expressing the permanent in integral form,
existing knowledge of the structure of integrals provides further
insight into the computational complexity of permanents. Finally, we
have shown that tools from quantum optics can be used to prove results
in computational complexity theory. In addition we showed in section 8.4
that this integral-based formalism is consistent with our understanding
of permutation matrices.

{savequote}

[45mm] Any sufficiently advanced technology is indistinguishable from
magic. \qauthor Arthur C. Clark

## Chapter 9 Preparation Strategies of Large Fock States from Single
Photons

### 9.1 Synopsis

The photonic Fock state plays an important role in quantum technologies
such as quantum information processing and quantum metrology. While
single-photon Fock states are relatively easy to create and the
single-photon source technology is standard in labs, it remains
challenging to create large-photon Fock states. In this work we show how
to efficiently prepare large-photon Fock states using specific
strategies that require single-photon sources, beamsplitters,
number-resolved photo-detectors, fast-feedforward, and an optical
quantum memory.

In section 9.2 we motivate this work explaining why it is important. In
section 9.3 we show the most obvious way to prepare large-photon Fock
states using spontaneous parametric down conversion (SPDC) sources and
see that it is inefficient. In section 9.4 we show a method inspired by
BosonSampling that uses post-selected linear optics and find that it is
also inefficient. In section 9.5 we describe and derive the formalism
for our bootstrapped approach that leads us to efficient large-photon
Fock state preparation. In section 9.6 we describe how we fuse stored
Fock states so that we can grow them into larger Fock states, we give
some analytic approximations to some fusion strategies, we discuss more
complex fusion strategies, and we discuss a hybrid scheme. In section
9.7 we discuss how to reduce the photon Fock state in case the state is
made too large. In section 9.8 we summarise the results of this work and
numerically simulate the different Fock state preparation algorithms.

### 9.2 Motivation

Fock states are an essential resource for many quantum technologies [ 1
, 20 ] including quantum communication, quantum cryptography, quantum
metrology [ 197 , 182 , 183 , 184 , 196 ] , interferometric protocols [
212 ] , and quantum information processing [ 8 ] . There have been a lot
of advances made in single-photon source technology but technologies
that prepare large photon-number Fock states efficiently do not exist.
Naïve approaches for generating large photon-number Fock states may be
made from single-photon Fock states using non-deterministic linear
optics or by heralded spontaneous parametric down-conversion (SPDC).
Both of these methods are inefficient because the success probability is
exponentially low as we show in section 9.3 and 9.4 respectively.

It is known that quantum enhanced metrology is optimal (i.e. it reaches
the Heisenberg limit of phase sensitivity) when NOON states are used [
195 ] . Creating NOON states with large photon number can be as hard as
building a universal optical quantum computer, as it requires many of
the same technologies, such as a quantum memory, and feedforward.
Nonetheless, one needs Fock states with large photon number to first
generate NOON states with large photon number for quantum enhanced
metrology [ 197 ] . Thus, efficient schemes for generating Fock states
with large photon number, as presented in this work, are an important
stepping stone for realizing optimal quantum enhanced metrology.

In this work we show an iterative protocol for building up large
photon-number Fock states from readily available single photons. We show
that our technique exponentially improves scalability compared to naïve
methods giving efficient methods for preparing large photon-number Fock
states. The requirements for implementing our protocol are mostly the
same as for universal linear optical quantum computing (LOQC) [ 8 ] ,
which further motivates building LOQC related technologies.

### 9.3 Spontaneous Parametric Down-Conversion with Post-Selection

A very trivial approach to preparing large photon-number Fock states is
to use SPDC sources with post-selection. For a review of SPDC sources
see section 4.3 . We know that the photon number between the signal and
idler modes is correlated so we can prepare an arbitrarily large @xmath
photon-number Fock state by increasing the pump power. Experimentally
demonstration of this approach has been done for small Fock states up to
three photons [ 213 ] .

To prepare large photon-number Fock states from SPDC sources with at
least @xmath photons we add all possible probabilities as shown in Eq. (
4.3 ). The probability is then given by

  -- -------- -- -------
     @xmath      (9.1)
  -- -------- -- -------

We see that this decreases exponentially with @xmath . This means that
to obtain a desired Fock state one would have to wait a time that
depends exponentially with @xmath . In addition currently available mean
photon numbers are @xmath [ 214 ] , which means that this is unviable
for large @xmath . In Fig. 9.1 we show this preparation probability as a
function of @xmath and @xmath .

### 9.4 Single-Shot Post-Selected Linear Optics

One method to prepare a large photon Fock state would be to use a
multimode linear optical interferometer as illustrated in Fig. 9.2 . In
this method single photons are input into every input mode where the
number of input modes is equivalent to the photon-number Fock state
desired. These photons evolve through a linear optical interferometer
and then at the output all modes except one are post-selected upon to
have the vacuum state. The remaining non-measured mode would then obtain
the desired photon Fock state since photon number is conserved.

Now we want to determine the efficiency of doing this. First we consider
an @xmath -mode interferometer with a single photon at each input mode.
The input state is

  -- -------- -- -------
     @xmath      (9.2)
  -- -------- -- -------

The state then evolves through a linear optical network

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (9.3)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Now we post-select upon all photons exiting the first mode

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (9.4)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

where the probability of this event occurring is

  -- -------- -- -------
     @xmath      (9.5)
  -- -------- -- -------

It turns out that a balanced interferometer maximizes @xmath due to the
triangle inequality. A balanced interferometer has weightings

  -- -------- -- -------
     @xmath      (9.6)
  -- -------- -- -------

For these weightings @xmath becomes

  -- -------- -- -------
     @xmath      (9.7)
  -- -------- -- -------

which was obtained using Stirling’s approximation.

The probability of preparing an @xmath -photon Fock state using this
method decreases exponentially with @xmath and thus implementation is
limited. As an example let us consider two-photon Hong-Ou-Mandel (HOM) [
28 ] interference. In this instance @xmath , and

  -- -------- -- -------
     @xmath      (9.8)
  -- -------- -- -------

as expected, since the output state of a HOM interferometer is @xmath .

### 9.5 Bootstrapped Preparation with Post-Selected Linear Optics

We have devised a method to improve the inefficient scaling of previous
methods. We call this a bootstrapped approach where we iteratively fuse
two Fock states with photon numbers @xmath and @xmath attempting to
create a larger Fock state. The fusion is done with a beamsplitter with
some reflectivity @xmath and one of the output modes is post-selected
upon. At the other output mode a Fock state with known photon number is
created and is a usable resource. Specifically, when @xmath photons are
detected we have prepared a new state of size @xmath . This bootstrapped
method is shown in Fig. 9.3 . This is the basis of how our efficient
large photon-number Fock state generation protocol works. We expand on
this in the next section describing specific fusion strategies or
algorithms for improving efficiency further. Now we describe this
protocol in detail.

The input state to the fusion operation is

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (9.9)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

which is incident upon a beamsplitter of reflectivity @xmath , given by
a biased beamsplitter unitary

  -- -------- -- --------
     @xmath      (9.10)
  -- -------- -- --------

where local phases are irrelevant. Then the output state is

  -- -------- -------- -- --------
     @xmath   @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      
              @xmath      (9.11)
  -- -------- -------- -- --------

We are interested in the case where @xmath photons are measured in the
first mode, thereby producing an @xmath -photon-subtracted state in the
second mode. Thus, we let @xmath , and the unnormalized post-selected
state reduces to

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The probability of detecting @xmath photons is, therefore,

  -- -------- -------- -- --------
     @xmath   @xmath      (9.13)
  -- -------- -------- -- --------

The state will have grown if the @xmath -photon-subtracted state is
larger than both @xmath and @xmath so we require @xmath . The
probability of preparing a state at least as large as both the input
Fock states is

  -- -------- -- --------
     @xmath      (9.14)
  -- -------- -- --------

In the case of a protocol where we will not recycle resource states we
only keep the @xmath outcome.

For given configurations of input Fock states @xmath and @xmath we would
like to maximize this probability so that state growth is maximized, so
we optimize @xmath to maximize @xmath and obtain,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (9.15)
  -- -------- -------- -------- -- --------

In Fig. 9.4 we show the optimized beamsplitter reflectivities and growth
probabilities for @xmath and @xmath for the both cases where we accept
all possible outcomes and only the @xmath outcome.

### 9.6 Fusion

We can see from Fig. 9.4 that when using recycling the growth
probability is maximized when fusing Fock states of equal photon number,
@xmath and so we are led to believe that the best strategy is to always
fuse states of equal size. This is called the balanced strategy. This is
similar to cluster states [ 12 , 13 ] since Rohde & Barrett showed that
a balanced strategy is optimal [ 215 ] . This balanced fusion strategy
can be simplified because the only probabilities of interest are @xmath
with an optimized growth probability of @xmath . This is very favorable
when attempting to create large photon-number Fock states. When no
recycling is used the growth probability is maximized when fusing a
given Fock state with a single-photon state (i.e. @xmath or @xmath ).
Rohde & Barrett call this the modesty strategy since we are attempting
to fuse only single-photon Fock states. This strategy however decreases
exponentially with the number of fusion operations since each time we
require that @xmath . We can overcome this inefficiency by employing
ideas of recycling from cluster state protocols [ 34 , 216 , 215 ] ,
where we reuse any @xmath outcome as a resource to progressively build
up a large-photon Fock state.

#### 9.6.1 Generalized Fusion Protocol

In our fusion protocol we assume that we have an unlimited resource of
single-photon states. We also have quantum memories that store Fock
states of varying photon numbers. We let @xmath be the number of @xmath
-photon states that we have in the respective memory after the @xmath
fusion operation. Because we assume that we have an unlimited supply of
single-photon states, we let @xmath . All other buckets are initially
empty, @xmath .

The next part of fusion is to retrieve two Fock states from the memory
that depends on a specific fusion strategy, which we let be @xmath and
@xmath , and send them through the fusion as depicted in Fig. 9.3 . With
probability @xmath the @xmath -photon state is prepared. The
corresponding memories are then updated according to

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (9.16)
  -- -------- -------- -------- -- --------

A state from each of the @xmath - and @xmath -photon memory has been
removed while the new state is added to the @xmath photon memory. With
fusion then a random walk of populations between the memories occurs.

Now suppose that we would like to achieve a photon Fock state with
photon number of at least @xmath . Then the memories we are interested
in are

  -- -------- -- --------
     @xmath      (9.17)
  -- -------- -- --------

The rate at which these states are prepared, per fusion operation, is
then

  -- -------- -- --------
     @xmath      (9.18)
  -- -------- -- --------

where @xmath is the number of fusion operations. We establish the steady
state flow dynamics of states by considering the @xmath .

#### 9.6.2 Analytic Approximations

For some simplified schemes, we can establish analytic results that show
that the rate is improved exponentially over the single-shot case
discussed in Sec. 9.4 . First we consider a non-recycled scheme, where
we attempt to construct a Fock state with @xmath photons, where @xmath
is a power of @xmath . For values of @xmath that are not a power of
@xmath , we can simply construct a Fock state with a number that is the
next largest power of @xmath . The rate will be unchanged, and the
scaling will not be significantly affected.

As this is a non-recycled scheme we only retain successes. We therefore
fuse single-photon states until we obtain @xmath -photon states, fuse
@xmath -photon states until we obtain @xmath -photon states, and so
forth, which is why we consider powers of two. As was found above (see
Fig. 9.4 ), the success probability is maximized for 50/50 beam
splitters when using equal photon numbers, so we use 50/50 beam
splitters. To estimate the rate, we will estimate the average number of
single-photon states needed to produce one @xmath -photon state. Then
the preparation rate of @xmath -photon Fock states per fusion operation
will have scaling equal to the inverse of this number. That is because
there can be no more than a factor of @xmath between the number of
single-photon states used and the number of fusion operations.

To show that result, consider first the ideal case where every fusion
operation is successful. Then for @xmath single-photon states, there
would be @xmath fusion operations, and @xmath fusion operations on the
2-photon states, and so forth. Adding them together for @xmath a power
of two gives a total number of fusion operations equal to @xmath , or
one less than the number of single-photon states. As the success rate is
reduced, the number of fusion operations can only be reduced for a given
number of single photons. Hence the number of fusion operations cannot
be any larger than the number of single photons. For this scheme we
perform fusion operations on all pairs of single photons, so the number
of fusion operations must be at least half the number of single-photon
states.

Now we estimate the average number of single-photon states required to
produce one @xmath -photon state. The expected number of attempts to
fuse two @xmath -photon states to produce the @xmath -photon state will
be @xmath . This corresponds to an expected number of @xmath -photon
states of @xmath , as there are two in each attempt. Then, the expected
number of @xmath -photon states required to produce each @xmath -photon
state is @xmath . As a result, the expected number of @xmath -photon
states required to produce one @xmath -photon state is @xmath .
Continuing this reasoning, the expected number of single photons
required to produce one @xmath -photon state is

  -- -------- -- --------
     @xmath      (9.19)
  -- -------- -- --------

To estimate this expected number of photons, we can use

  -- -------- -- --------
     @xmath      (9.20)
  -- -------- -- --------

where the approximation is via Stirling’s formula. Using this
approximation with Eq. ( 9.19 ) gives the expected number of
single-photon states scaling as

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (9.21)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Testing this expression numerically, we find that the expected number of
single photons is about @xmath times this value.

As discussed above, the preparation rate of @xmath -photon Fock states
per fusion operation will scale as the inverse of this expression, and
is therefore

  -- -------- -- --------
     @xmath      (9.22)
  -- -------- -- --------

There is an exponential improvement over the case with just a single
interferometer and single-shot preparation. The scaling is not
exponential in @xmath , but it is also not polynomial in @xmath ,
because the power is logarithmic in @xmath .

For a further improvement, we can add limited recycling. Rather than
just requiring zero photons to be lost at each stage, we require that
the number of photons lost is no more than @xmath when fusing two @xmath
-photon states so that the probability of success is given by

  -- -------- -- --------
     @xmath      (9.23)
  -- -------- -- --------

with @xmath . Then we find numerically that the probability of success
approaches @xmath in the limit @xmath as can be seen in Fig. 9.5 . An
analytic solution for this result currently eludes us so we have
provided numerical evidence. It is smaller for smaller values of @xmath
, but because we are calculating the scaling for large @xmath we will
take the probability of success to be @xmath . Without loss of
generality we can require that on success the photon number is @xmath .
If the photon number is larger than that, we can reduce the photon
number with the state reduction scheme described in Sec. 9.7 . Now, in
order to obtain photon number @xmath , we need a number of levels
scaling as @xmath , rather than @xmath . However, the multiplying factor
for the number of repetitions at each stage is smaller.

Therefore, the number of single photons required to obtain a single
@xmath -photon Fock state scales as

  -- -------- -- --------
     @xmath      (9.24)
  -- -------- -- --------

Testing this expression numerically, the number of single photons
required is about @xmath times less. The corresponding rate for this
scheme then scales as

  -- -------- -- --------
     @xmath      (9.25)
  -- -------- -- --------

This scaling is again an exponential improvement, and is now strictly
polynomial.

#### 9.6.3 Fusion Strategies

An analytic bound for more advanced recycling strategies is nontrivial
and we instead simulate these protocols as classical Markov processes
between the buckets, with transition probabilities given by the
parameters @xmath , and transition rules from Eq. ( 9.6.1 ).

We see that Fig. 9.4 intuitively hints that the balanced strategy may be
optimal we consider several other strategies as a comparison to provide
further insight into the importance of fusion strategy. We will consider
four recycling strategies in total:

-   Balanced: Fuse the largest two available states of equal size,
    @xmath . This strategy is based on the observation of Fig. 9.4 that
    fusing states of equal size maximizes the growth probability, @xmath
    .

-   Modesty: Always attempt to grow our state by a single photon, by
    fusing the state @xmath with a single-photon state, @xmath .

-   Random: Randomly choose any two available states, irrespective of
    their relative sizes.

-   Frugal: Same as balanced, except that it does not attempt to fuse
    two equally sized states if @xmath , where @xmath . For states of
    size @xmath , it instead attempts to fuse available states such that
    @xmath . This is because larger number states are costly to prepare,
    so it is wasteful to fuse two states with a total photon number well
    in excess of the target @xmath .

The optimization technique for the frugal strategy is different than for
the other strategies, which use @xmath . If the total input photon
number @xmath , then for each configuration of input Fock states @xmath
and @xmath , we optimize @xmath to maximize the probability of getting
at least @xmath photons. Specifically, we maximize

  -- -------- -- --------
     @xmath      (9.26)
  -- -------- -- --------

If @xmath , then for each configuration of input Fock states @xmath and
@xmath , we optimize @xmath to maximize the probability of increasing
the maximum photon number with increasing weightings for growing the
photon number larger. Specifically, we maximize

  -- -------- --
     @xmath   
  -- -------- --

#### 9.6.4 Hybrid Schemes

We have considered the scenario where single-photon states are freely
available. This is an appropriate choice as single-photon sources are
becoming mainstream. Improved photon-source technologies, such as
quantum dot sources, may have the ability to prepare small-photon-number
Fock states higher than @xmath on demand. A simple modification then to
our protocol is to begin with a resource of infinitely available
resource states that are larger than one and we find that this further
improves preparation rates.

Our framework can account for this by letting @xmath rather than @xmath
, where @xmath is the photon number that can be prepared on demand. The
rest of the protocol then proceeds as usual using a given fusion
strategy.

### 9.7 State Reduction

In our analysis so far we calculate the rate of preparing at least
@xmath photons but for some applications we may want exactly @xmath
photons. For this we may need to then reduce the photon number of a Fock
state if we were to prepare one too large. Luckily, this may also be
done with post-selected linear optics by inputting the Fock state on a
low-reflectivity beamsplitter with vacuum at the other input ( @xmath ,
@xmath ). Most of the time no photons will be detected in the reflected
mode since the beamsplitter reflectivity is low. With low probability a
single photon will be detected and with even lower probability more than
one photon will be detected. When photons are detected we have reduced
the photon number by the number of photons we detected. We can do this
repeatedly until we obtain the desired Fock state with photon number
@xmath . This protocol is efficient since it requires @xmath
beamsplitter operations on average when attempting to subtract @xmath
photons. Also, this is experimentally easier to implement than state
growth since the vacuum state is used in one of the modes meaning that
there are no mode-matching requirements.

### 9.8 Results

First we show the rate of @xmath -photon state preparation, @xmath , for
both the recycled and non-recycled bootstrapped protocols (Sec. 9.5 ),
the single-shot protocol (Sec. 9.4 ), and heralded SPDC (Sec. 9.3 ) in
Fig. 9.6 . For the bootstrapped protocols we implemented the balanced
fusion strategy and let the resource requirements be the number of
beamsplitter operations. For the single-shot protocol, Eq. ( 9.7 ), the
number of trials of the entire interferometer is the resource that we
consider and we convert this to the number of implemented beamsplitters
by realising that the interferometer shown in Fig. 9.2 can be built from
@xmath beamsplitters in a linear array such that the photons are
progressively routed to the top mode. And so the measured number of
beamsplitter operations is given by Eq. ( 9.7 ) but with an additional
factor of @xmath , yielding @xmath . For heralded SPDC we cannot convert
the resource requirements because the SPDC protocol does not use
beamsplitters or single photons as a resource. We decided to use as a
resource the number of repetitions of the SPDC source. So that the SPDC
case has the same 20-photon preparation rate as the balanced
bootstrapped protocol with recycling, we chose the mean photon-number to
be @xmath . This is a threshold by which the SPDC is more efficient than
the recycled bootstrapped protocol. Below it is less efficient. A mean
photon number of @xmath is well beyond the capabilities of typical
experiments today. This plot clearly shows that all cases, other than
the balanced recycled protocol, the rate of state preparation decreases
exponentially with @xmath . Also, recycling improves the preparation
rate as can be seen in the example for 20-photon state preparation where
the preparation rate improves by a factor of @xmath over the single-shot
protocol.

Now in Fig. 9.7 we show the recycled protocol for some of the fusion
strategies from Sec. 9.6.3 . We show a log log plot and find that the
preparation rate exhibits polynomial scaling against @xmath for the
frugal and balanced recycled strategies and exponential scaling for the
the random and modesty recycled strategies. This means that the frugal
and balanced recycled protocols exhibit an exponential efficiency
improvement. The scaling for the Frugal and Balanced strategies is
@xmath and @xmath respectively, which is a significant improvement to
the doubling approach that led to the rate in Eq. ( 9.25 ).

In Fig. 9.8 , we show the hybrid schemes as discussed in section 9.6.4 .
Here we show the recycled frugal fusion strategy since this exhibits the
best scaling.

### 9.9 Summary

We have devised a protocol based on the idea of cluster states for
efficiently preparing large photon-number Fock states as motivated in
section 9.2 . We began by describing some obvious ways to inefficiently
prepare large photon-number Fock states in section 9.3 and section 9.4 .
Then in section 9.5 we explain the basic formalism for our efficient
approach. This protocol is non-deterministic, assumes that a resource of
single-photon states is readily available, requires quantum memory,
requires fast-feedforward, and uses linear optics with post-selection.
Large photon-number Fock states are generated by iteratively fusing
together smaller Fock states into larger ones that undergo a random walk
in the quantum memory and shown in section 9.6 . The quantum memory
allows for us to use state recycling where we can use all post-selected
events as a resource. We found that our method was able to exponentially
improve the state preparation rate by orders of magnitude compared to
other naïve brute-force and single-shot methods. In section 9.7 we
discuss how to reduce the photon number of a Fock state in case the
resource state obtained is larger than desired. In section 9.8 we show
the results of this work and plot the various strategies showing the
exponential improvement in scaling for certain fusion strategies. The
requirements of this protocol like quantum memory and fast-feedforward
are extremely challenging and are essentially the same requirements as
LOQC. When LOQC is available so will the efficient preparation of large
photon-number Fock states. Thus, the technology for generating these
states will also help lead to realising LOQC.

{savequote}

[45mm] Son: Dad, why is cannabis illegal?
Dad: Well, son, legalizing the cannabis plant could free us from oil
dependence, stop deforestation, become a saf alternative to many
pharmaceuticals, and would cause prisons to shut down.
Now do you understand? \qauthor Anonymous

## Chapter 10 Preparing Continuous Variable Optical States for Quantum
Error Correction by Coupling Atomic Ensembles to Squeezed States of
Light

### 10.1 Synopsis

In 2001 Gottesman, Kitaev, and Preskill (GKP) showed theoretically how
to encode information fault-tolerantly on a continuous variable system
using linear optical techniques; however, GKP did not show how to
physically prepare these continuous variable states. In this work we
present a non-deterministic scheme for generating optical continuous
variable states by coupling an atomic ensemble with a squeezed state of
light. The coupling creates a comb of Gaussian squeezed states of light
in one channel. Upon particular measurement events of angular momentum
in the other channel a desired resource state for encoding logical
information is prepared.

This work is first motivated in Sec. 10.2 . We then show how to prepare
the resource state in Sec. 10.3 . Once we have the resource state we may
encode logical states onto certain outcomes of the resource state as
discussed in Sec. 10.4 . In Sec. 10.4.1 we discuss the relationship
between the total angular momentum required of the atomic ensemble for a
given amount of squeezing in the squeezed state of light to prepare a
resource state that is sufficient for resolving encoded states. In Sec.
10.4.2 we discuss the success probability of post-selecting upon a
measurement that yields a useful resource state. Specifically, for a
given total angular momentum @xmath of the atomic ensemble there are
@xmath possible outcomes. We find that the events that yield desirable
resource states for encoding logical information also have the highest
probability of being measured, which is a significant advantage of this
scheme over others. In Sec. 10.5 we propose a physical implementation
based on the Faraday interaction between an atomic ensemble and a
squeezed optical field.

### 10.2 Motivation

Realising a quantum computer is laden with many difficult problems to
overcome. Whatever architecture may be used for this realisation will
require quantum error correction because the quantum state that is
processed within the quantum computer is extremely susceptible to being
destroyed by its environment and so it is a requirement that the state
of the quantum system within the quantum computer remains coherent in
the presence of such environmental noise. One way to overcome this noise
is to encode logical information on either discrete or continuous
variable (CV) systems. In general these systems use an ensemble of
quantum particles to encode a logical state. There have been several
proposals for encoding information in a larger system that is resilient
against various types of errors such as the now famous toric code [ 217
] among others [ 218 , 219 ] .

A promising CV proposal by Gottesman, Kitaev, and Preskill (GKP) in 2001
[ 220 ] showed how to encode logical states in an infinite-dimensional
Hilbert space such that in the ideal encodings a universal set of
fault-tolerant quantum gates can be implemented with linear optical
operations, squeezing, homodyne detection, and photon counting. They
described how a comb of delta functions can perfectly encode logical
states; however, these delta functions are unphysical as they would
require infinite energy, so they approximated the delta functions with
Gaussians, which can encode the logical states with an associated error
probability. These Gaussians approach the ideal delta functions as
squeezing goes to infinity or equivalently their variance tends to zero.
It remained unknown weather or not these error prone continuous variable
encodings could enable fault-tolerant measurement-based quantum
computation until Menicucci showed in 2014 that they can [ 221 ] .

GKP did not propose how to actually make these states and just presented
them as a model for encoding. Since then proposals have been made to
generate these GKP states optically [ 222 ] and by a variety of other
methods [ 223 , 224 , 225 , 226 , 227 ] , but none are particularly easy
to implement. In this work we propose a new way of generating optical
GKP states by coupling an atomic ensemble to a squeezed state of light.

### 10.3 Preparing the Resource State

In this section we show how the optical resource state is conditionally
prepared using a controlled interaction with an angular momentum. Where
appropriate, the state of the angular momentum and of the optical field
will be labelled with @xmath and @xmath , respectively. The calculations
follow the progress of the joint state through the circuit in Fig. 10.1
, which begins with an unentangled spin-optical state @xmath at (a) and
ends with a conditional optical state prepared upon a measurement of the
spin at (d). A succinct derivation of the protocol for more general
initial angular momentum and optical states is given in Appendix A.15 .

Initially the atomic ensemble with total collective angular momentum
@xmath is prepared in a spin-polarized state along the @xmath -axis
which we will denote as @xmath . Expressed in terms of eigenstates along
the @xmath -axis, @xmath , the initial spin state is

  -- -------- -------- -- --------
     @xmath   @xmath      (10.1)
  -- -------- -------- -- --------

where the coefficients, @xmath , are the matrix elements of the operator
@xmath that rotates the state around the @xmath -axis by an angle @xmath
. For our purposes @xmath is fixed and @xmath ; henceforth, we drop
unnecessary notation and define

  -- -------- -- --------
     @xmath      (10.2)
  -- -------- -- --------

where we have used the explicit form for @xmath that appears, @xmath ,
in Sakurai [ 228 ] Sec. 3.8.

When @xmath only a single term in the summation in Eq. ( 10.2 ) has
non-negative factorials in the denominator, and the matrix element
simplifies to

  -- -------- -- --------
     @xmath      (10.3)
  -- -------- -- --------

Specifically, the factorials in the denominator are non-negative only
when @xmath for @xmath ; and for @xmath for @xmath . The values of
@xmath are significant as these yield desirable resource states, as we
will see in Sec. 10.4 .

The initial optical state is the bosonic vacuum @xmath with creation and
annihilation operators @xmath and @xmath obeying the usual commutation
relations @xmath . The squeezing operator [ 20 ]

  -- -------- -- --------
     @xmath      (10.4)
  -- -------- -- --------

acts on this initial state to produce a squeezed vacuum,

  -- -------- -- --------
     @xmath      (10.5)
  -- -------- -- --------

Hence the joint state at (b) in Fig. 10.1 is

  -- -------- -- --------
     @xmath      (10.6)
  -- -------- -- --------

The angular momentum and optical field are entangled with a controlled
interaction that applies a displacement to the field proportional to the
angular momentum in the @xmath -direction given by

  -- -------- -- --------
     @xmath      (10.7)
  -- -------- -- --------

where @xmath is the momentum-quadrature operator. The key feature is
that the controlled displacement has the following effect:

  -- -------- -- --------
     @xmath      (10.8)
  -- -------- -- --------

where @xmath is a displaced squeezed state [ 20 ] . The scaling of the
displacement has been chosen to match the work of GKP [ 220 ] for the
encoded logical states. After the controlled displacement given by Eq. (
10.7 ), the joint state at (c) in Fig. 10.1 is

  -- -------- -- --------
     @xmath      (10.9)
  -- -------- -- --------

Finally, the angular momentum is projectively measured along the @xmath
-direction as shown in (d) in Fig. 10.1 . We express @xmath in the basis
of @xmath -eigenstates,

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.10)
  -- -------- -------- -- ---------

where we used the fact that the matrix elements in Eq. ( 10.2 ) satisfy
@xmath . Then, measurement trivially collapses the @xmath summation in
Eq. ( 10.10 ) to a single term. That is, given measurement outcome
@xmath , the optical state is projected to

  -- -------- -- ---------
     @xmath      (10.11)
  -- -------- -- ---------

The state is normalized by the probability of obtaining @xmath ,

  -- -------- -- ---------
     @xmath      (10.12)
  -- -------- -- ---------

as shown in Appendix A.16 , noting that the squeezing parameter @xmath
is taken to be real.

The conditional optical state can be written as a wavefunction in the
position and momentum quadratures, @xmath and @xmath respectively,

  -- -------- -------- -------- -- ---------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (10.13)
  -- -------- -------- -------- -- ---------

For spins of size @xmath and @xmath , the position- and momentum-space
wavefunctions are plotted in Fig 10.2 . Each measurement outcome @xmath
prepares a conditional position-space wavefunction, Eq. ( 10.3 ),
composed of a superposition of displaced squeezed states separated by
@xmath (for integer @xmath ) with amplitudes governed by the product of
matrix elements, @xmath .

### 10.4 Encoding

Logical information is encoded into the continuous variable degrees of
freedom of the optical states using the scheme proposed by GKP. We seek
maximally distinguishable encoded states with logical operations
corresponding to translations in position and momentum. Of the
conditional optical states after the angular momentum measurement, those
corresponding to @xmath or @xmath are suitable for GKP encoding.
However, the state produced by @xmath has a much smaller probability
than @xmath (see Sec. 10.4.2 ), is not available for half-integer @xmath
, and cannot be used together with the states generated by the other
outcomes. Consequently, we concentrate exclusively on the resource
states generated by the outcomes @xmath . The outcomes @xmath
respectively generate the logical states @xmath and @xmath . The
corresponding wavefunctions, which appear in the far right and far left
panels of Fig. 10.2 , follow from Eqs. ( 10.3 ),

  -- -------- -------- -- ----------
                          
     @xmath   @xmath      (10.14a)
     @xmath   @xmath      (10.14b)
  -- -------- -------- -- ----------

where we have used the @xmath matrix elements in Eq. ( 10.3 ). The
position-space wavefunction, @xmath , can be described as a product of a
comb of Gaussians each with variance

  -- -------- -- ---------
     @xmath      (10.15)
  -- -------- -- ---------

separated by @xmath ; and an envelope arising from the binomial
distribution that appears in Eq. ( 10.14a ), namely

  -- -------- -- ---------
     @xmath      (10.16)
  -- -------- -- ---------

Similarly, the momentum-space wavefunction, @xmath , can be described as
a product of a Gaussian envelope with variance

  -- -------- -- ---------
     @xmath      (10.17)
  -- -------- -- ---------

and a comb of approximately Gaussian peaks generated by @xmath , hence
separated by @xmath . The variance of the individual peaks is given by

  -- -------- -------- -------- -------- ---------
     @xmath   @xmath   @xmath            (10.18)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ---------

where @xmath is the Hurwitz zeta function, as shown in Appendix A.17 .

In the @xmath -quadrature, the peaks in the logical @xmath and @xmath
states are interleaved such that the spacing between peaks of one state
and the other is a minimum of @xmath . With this encoding @xmath -shifts
smaller than @xmath can be corrected, and Pauli- @xmath errors
correspond to interchanging @xmath and @xmath . These relationships are
most clearly seen in the left-most panels of Fig. 10.2 (a,c) for the
logical state @xmath . In the @xmath -quadrature the peak locations are
the same for the @xmath and @xmath logical states, so a @xmath
-measurement is unable to distinguish them. In the @xmath -quadrature,
however, the @xmath and @xmath states are distinguishable while the
@xmath and @xmath that are not. In this quadrature the minimum
separation between peaks of @xmath and of @xmath is @xmath , so @xmath
-shifts smaller than @xmath can be corrected, and Pauli- @xmath errors
correspond to interchanging @xmath and @xmath . Depending on the error
model for a particular physical implementation, the interaction strength
in the controlled displacement [Eq. ( 10.7 )], can be modified to
produce encodings more robust to errors in one quadrature than the
other.

#### 10.4.1 Symmetric encoding

A specific encoding, laid out in the original GKP paper [ 220 ] ,
assumes that @xmath - and @xmath -quadrature errors are comparable in
size. This symmetric encoding is a resource for fault-tolerant quantum
computing using continuous-variable cluster states. For the ideal
(infinite squeezing) code, the size of a correctable shift error in the
@xmath -representation is half the minimum separation between a peak in
the logical @xmath state and a peak in the logical @xmath state and
similarly for @xmath and @xmath in the @xmath -representation. Equating
the two yields @xmath in the code states, Eqs. ( 10.14 ).

The prepared logical states are finite-squeezing approximations to the
ideal GKP states. Thus, irrespective of shift errors, there is an
embedded probability of mistaking @xmath and @xmath , or @xmath and
@xmath , due to finite overlap of neighboring Gaussians peaks. To
balance the embedded errors, the symmetric encoding is completed by
equating the peak variances in the @xmath - and @xmath -quadratures as
follows. For input squeezing @xmath , the peak variance in @xmath is
given by Eq. ( 10.15 ) and in @xmath follows from Eq. ( 10.18 ) using
@xmath . For fixed @xmath and @xmath , the size of the angular momentum
@xmath determines the peak variance in @xmath . Setting @xmath gives

  -- -------- -- ---------
     @xmath      (10.19)
  -- -------- -- ---------

which carries the additional effect of symmetrizing the envelopes,
@xmath . Table 10.1 gives the required @xmath for various values of
squeezing (in dB). A symmetric encoding prepared with a total angular
momentum @xmath with @xmath dB of squeezing is sufficient for
fault-tolerant measurement-based quantum computing with
continuous-variable cluster states [ 221 ] . Examples of symmetric
resource states prepared by the conditional procedure presented here are
shown in Fig. 10.3 along with the corresponding logical states.

#### 10.4.2 Success Probability

The probabilities of obtaining the resource states for symmetric
encoding follow from Eq. ( 10.12 ). Given total angular momentum @xmath
, which arises from Eq. ( 10.19 ), the probabilities for all spin
measurement outcomes are shown in Fig. 10.4 . A significant advantage of
the protocol proposed here is the relatively large success
probabilities.

The outcomes we care most about (i.e. @xmath ) are also the most
probable ones to measure. This significantly boosts the success
probability of obtaining a resource state that is usable for encoding
logical states onto and is thus an advantage in our scheme.

  -- -------- -- ---------
     @xmath      (10.20)
  -- -------- -- ---------

Since both resource states are compatible with the same encoding, and
the probabilities are equal, the total success probability is @xmath .

As shown in Appendix A.16 , in the limit that the squeezing @xmath and
associated @xmath are large, the embedded error vanishes and the
probabilities become,

  -- -------- -- ---------
     @xmath      (10.21)
  -- -------- -- ---------

Now the success probability of obtaining the desired resource state
@xmath is given by

  -- -------- -- ---------
     @xmath      (10.22)
  -- -------- -- ---------

which is shown in App. A.16 and the approximation holds for large @xmath
. Taking for instance @xmath , which corresponds to a squeezing of 20 dB
as shown in table 10.1 , we find @xmath . This is a highly favourable
success probability for such a large value of total angular momentum
@xmath . Fig. 10.5 shows how the probability of successfully
post-selecting on a desired resource state scales with @xmath . Whereas
the success probability of the iterated scheme scales exponentially
poorly in the number of steps @xmath (equivalent to an angular momentum
@xmath here), the probability here falls off only as @xmath . The reason
comes from the nature of the measurements. Here, the measurement is
collective and the outcomes arise solely from the highest irreducible
representation of angular momentum for @xmath spins, where the @xmath
spins are measured individually, yielding outcomes spread over the full
@xmath Hilbert space, the vast majority of them fail to produce the
desired resource states.

### 10.5 Physical Implementation

In this section we show three important aspects for experimental
implementation: (1) We present a method for experimental implementation
using a Faraday-based quantum non-demolition (QND) interaction. (2) We
show a method for measuring the angular momentum @xmath in the top
channel of Fig. 10.3 .

##### Faraday-based QND interaction

To implement the protocol, we couple the squeezed field to the
collective spin formed by an ensemble of polarizable neutral atoms.
Consider @xmath such atoms, each with effective spin @xmath defined on
metastable ground states @xmath . The atoms couple to a common mode of
light possessing two orthogonal linear polarizations, horizontal (
@xmath ) and vertical ( @xmath ), with respective annihilation operators
@xmath and @xmath . For an off-resonant field the atoms and light become
entangled via the dispersive Faraday interaction, @xmath , which
describes a coupling of the collective atomic spin, @xmath , to the
3-component of the field’s Stokes vector [ 229 ] ,

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.23)
  -- -------- -------- -- ---------

The Faraday interaction generates a rotation of the Stokes vector around
the 3-axis proportional to the atomic spin projection along @xmath with
a strength characterized by the dimensionless, single-photon rotation
angle @xmath .

The controlled displacement required for the GKP resource-state protocol
arises by preparing the @xmath -mode of light in a coherent state with
@xmath photons. Making the linearization @xmath , the Stokes operator in
Eq. ( 10.23 ) becomes @xmath where @xmath is the momentum quadrature of
the @xmath -polarized mode. This linearized Faraday interaction
generates the requisite controlled translations of @xmath -mode photons,
Eq. ( 10.7 ), with effective coupling strength,

  -- -------- -- ---------
     @xmath      (10.24)
  -- -------- -- ---------

##### Projective spin measurement

Once the optical field and atoms have become entangled, the collective
spin state is projectively measured. A single atomic spin may be
measured by driving a cycling transition and detecting the resulting
fluorescence [ 230 ] . Concatenating with unitary transformations, a
projective measurement can be realized. However, since our
resource-state protocol benefits from spins larger than can be achieved
using a single atom, we focus instead on a quantum nondemolition (QND)
measurement of the collective spin of many atoms. The collective spin is
coupled via the same Faraday interaction to a second field that serves
as a meter . The meter experiences a spin-dependent polarization
rotation that is measured via homodyne polarimetry. When the spin-meter
coupling is strong, relative rotations from different projective @xmath
-values become distinguishable over the meter’s shot noise, and the
collective spin measurement is projective. This is indeed the same
strong-coupling requirement for the GKP-state peaks to be sufficiently
separated. It become more challenging to distinguish angular momentums
from each other as the total angular momentum increases (e.g. @xmath
versus @xmath as opposed to @xmath versus @xmath ).

To implement the measurement, the collective spin is first rotated into
the @xmath -basis with a @xmath -pulse. The meter is initialized with
@xmath photons in the @xmath -mode and squeezed vacuum in the quantum
mechanical @xmath -mode, @xmath . Polarimetry in the diagonal
polarization basis implements an effective homodyne measurement of the
position quadrature for @xmath -mode photons, with the @xmath -mode
serving as the local oscillator [ 231 ] . The degree to which the
measurement is projective is determined by the distinguishability of the
meter states, @xmath , given by Eq. ( A.64 ),

  -- -------- -------- -- ---------
     @xmath   @xmath      (10.25)
  -- -------- -------- -- ---------

In the limit that @xmath , the meter states become orthogonal. Thus,
distinguishing neighboring eigenstates of @xmath requires @xmath , with
limits set by the characteristic coupling strength @xmath and the
squeezed shot noise in the polarimeter. Note that in contrast to the GKP
encoding as discussed in Sec. 10.4 , the spin-meter coupling is not
constrained by a specific value of @xmath for a given @xmath , since the
goal is only to produce distinguishable meter states.

Practical limitations on @xmath in both the GKP mode and the meter arise
for two related reasons. First, the Faraday interaction is a valid
description of the light-matter coupling when the quantum emitters
remain far below saturation. Second, increased @xmath precipitates more
spontaneous photon scattering that spoils the QND interaction and
measurement. Indeed, this has restricted QND spin squeezing in free
space to the Gaussian regime, far from a projective measurement. The
requirement to overcome the effects of decoherence is that the coupling
to the collective optical mode is large relative to all other modes.
This can be characterized by the optical density per atom, @xmath , the
ratio of the resonant atomic scattering cross section @xmath to the
transverse mode area @xmath . While typical optical densities per atom
in free space, @xmath , are far too weak for our purposes [ 231 ] ,
those in engineered photonic environments, such as photonic crystal
waveguides, can be much larger; @xmath . Future improvements are
expected by operating near a band edge where “slow light” can enhance
the interaction by nearly two orders of magnitude [ 232 ] .

A detailed study of optical pumping for atoms very near and strongly
coupled to a waveguide is beyond the scope of our work; nevertheless, an
estimate of the required coupling can be found from a free-space model
for alkali atoms. Here, the Faraday rotation angle per photon per unit
angular momentum given above is @xmath , for detuning @xmath and
spontaneous emission rate @xmath [ 229 ] . To realize the coupling
strength in Eq. ( 10.24 ) required for @xmath -symmetric codewords and
an approximately projective measurement while limiting the number of
free-space scattered photons, we find that for @xmath and @xmath the
required optical density per atom is @xmath , within the reach of
near-term technology. It may be possible to augment the atom-light
coupling with an optical cavity [ 233 ] and to suppress the deleterious
effects of optical pumping by judiciously selecting the effective spin
within each atom [ 234 ] . Alternative fruitful avenues have opened in
other physical architectures, where demonstrated strong coupling of
“artificial atoms” to photonic environments could provide the necessary
interaction strength [ 235 , 236 ] . In such systems, Purcell
enhancement of the total coupling rate has the potential to reduce the
collective spin’s susceptibility to other sources of noise.

### 10.6 Summary

In this work we show how to create a state that may be used for quantum
error correction in quantum information processing. Quantum error
correction is a critical component of realising any quantum computer.
Specifically, we show a non-deterministic scheme for generating optical
continuous variable states by coupling an atomic ensemble with a
squeezed state of light, which can be used to encode and correct quantum
information. This work is further motivated in Sec. 10.2 . The coupling
creates a comb of Gaussian squeezed states of light, which upon
particular measurement events of angular momentum, a desired resource
state for encoding logical information is prepared. The details of
preparing the resource state are presented in Sec. 10.3 and how to
encode information on these states is discussed in Sec. 10.4 . In Sec.
10.4.1 bounds between the total angular momentum required of the atomic
ensemble for a given amount of squeezing in the squeezed state of light
to sufficiently prepare a useable resource state is derived. In Sec.
10.4.2 the success probability of our scheme to yield desirable resource
states for encoding logical information is shown. We find that the
desired post-selection events also have the highest probability of
occuring, which is a significant advantage of this scheme over others.
Finally, we propose a physical implementation for realising our protocol
based on the Faraday interaction in Sec. 10.5 .

{savequote}

[45mm] You can tell more about a person by what they say about others
than you can by what others say about them. \qauthor Leo Aikman

## Chapter 11 Summary

In this thesis we developed research areas that will lead to novel
quantum technologies such as a universal quantum computer. Such
technologies promise significant breakthroughs in digital security, our
ability to measure various phenomena in nature, condensed-matter
physics, high-energy physics, atomic physics, quantum chemistry,
cosmology, and medicine among others. Although the technologies required
to build a universal quantum computer remain daunting, these
breakthroughs are motivation enough for much of the world to focus
resources into funding these futuristic technologies.

To summarize this thesis we begin by introducing a specific type of
quantum computing called linear optical quantum computing in Ch. 1 . We
then looked at quantum random walks in Ch. 2 , which are used in various
quantum algorithms, and found that quantum walkers retain quantum
advantages over classical walkers even in the presence of lattice
congestion and dephasing. In Ch. 3 we introduce BosonSampling , which is
a very simplified model of a linear optical quantum computer that
simulates the interference of bosons and which contains many of the core
technologies required to build a linear optical quantum computer.
BosonSampling is the topic of most of this thesis including this
introduction and chapters 4 , 5 , 6 , 7 , and 8 . In Ch. 4 we found that
BosonSampling using the most readily available photon source, called a
spontaneous parametric down conversion source, may be successfully
implemented with a multiplexed device that can reroute photons. In Ch. 5
we invent how to implement BosonSampling in a temporal implementation
instead of spatial one and show that the temporal architecture
simplifies the number of optical elements required from thousands or
millions to just three. In addition we give a detailed error analysis of
this new architecture. This was done in theory but has since been
successfully experimentally demonstrated implementing the worlds largest
ever BosonSampling experiment. In Ch. 6 we show that the original
protocol for BosonSampling is not unique and that there is a large class
of quantum states of light that may be used to implement a
computationally hard to simulate instance of BosonSampling . In Ch. 7 we
show another key discovery we made where we invented a world’s first
application inspired by BosonSampling in the field of quantum metrology.
This is one of only two BosonSampling inspired applications that
currently exist. In Ch. 8 we use a quantum optics approach to map the
BosonSampling formalism into a multidimensional integral formalism using
characteristic functions showing that they are equivalent and thus
finding multidimensional integrals of a certain form that are also
computationally complex. Finally, we switched away from BosonSampling
and focused on creating certain quantum states of light that are useful
for quantum computing. In Ch. 9 we give a protocol that can efficiently
generate large-photon Fock states, which are a critical quantum state
used in quantum information processing, and in Ch. 10 we show how to
create a continuous variable optical state by coupling light to atoms
that can be used to encode and correct quantum information in a quantum
computer.
