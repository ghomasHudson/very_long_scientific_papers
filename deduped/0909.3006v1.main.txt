# Chapter 1 Heavy Ion Physics

### 1.1 Introduction

Colliding heavy ions in particle accelerators offers a unique
opportunity to study the strong interaction of matter in the regime of
extremely high densities and temperatures. It is believed that in such
collisions temperatures and densities are reached that prevailed in the
universe the first few microseconds after the Big Bang.

In the Standard Model of particle physics, the strong interactions
between the fundamental quark constituents of matter are described by a
field theory called Quantum Chromo Dynamics (QCD) [ 1 ] . In this
theory, the quarks carry a strong charge, called color , and the strong
force is mediated between the colored quarks by the exchange of gluons,
which are the quanta of the strong field. A very important feature of
QCD is that the gluons also carry color charge, so that they do not only
act as mediators but also themselves couple to the strong force. It
turns out that, as a consequence, the potential increases with
increasing distance between the color charges. This is in sharp contrast
with the field theory of Quantum Electro Dynamics (QED) [ 2 ] , where
the force between electrically charged particles is mediated by the
electrically neutral photon. Here the potential vanishes for large
distances.

The behavior of the strong coupling with varying distance, which is
related to the behavior of the potential as discussed above, has
profound phenomenological consequences. First, the coupling between the
colored quarks becomes weak at short distances, a property called
asymptotic freedom . Such short distances are probed in hard scattering
processes, where the momentum exchange between the participating quarks
is large. Since the strong coupling is weak in the hard regime, the
interaction cross sections can be calculated in a framework called
perturbative QCD (pQCD). Because they are calculable, hard scattering
processes form a unique probe of the constituents of matter, while
being, at the same time, a testing ground for the validity of QCD. In
this way, QCD has been firmly established as the correct theory of the
strong interaction in the last four decades by performing a large
variety of experiments on deep inelastic scattering of electrons and
muons on protons and neutrons and by the study of electron-positron and
proton-(anti)proton collisions at large centre of mass energies in
storage rings.

The strong coupling increases with the distance between the quarks, and
the interaction becomes, in fact, so strong, that in ordinary matter the
quarks are permanently confined to colorless hadrons. In this regime of
large distances or, equivalently, small momentum transfers, pQCD breaks
down, so that it cannot be used to calculate soft scattering cross
sections from first principles. However, recently much progress has been
made in the understanding of the non-perturbative domain by so @xmath
-called lattice QCD calculations, where the QCD field equations are
numerically solved on a discrete space @xmath -time lattice [ 3 ] .

One of the remarkable results of lattice QCD is the prediction that
hadronic matter at sufficiently high temperatures and densities will
undergo a phase transition to a state of quasi-free quarks and gluons.
This deconfined dense state of matter is called a Quark Gluon Plasma
(QGP). In Figure 1.1

are shown lattice QCD calculations of the energy density ( @xmath )
divided by the fourth power of the temperature ( @xmath ) [ 4 ] . This
dimensionless quantity is proportional to the effective number of
degrees of freedom available in the medium. Below the critical
temperature ( @xmath ) the medium consists mainly of confined hadrons,
while above @xmath the quarks and gluons become deconfined, causing a
rapid increase in the number of degrees of freedom. Figure 1.1 shows
that the phase transition occurs when nuclear matter is heated to a
temperature @xmath of about 175 @xmath , corresponding to an energy
density of 0.7 @xmath .

In the limit of an ideal Stefan- @xmath Boltzmann gas, the equation of
state (EoS) of a QGP is given by

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where @xmath is the pressure, @xmath the energy density, @xmath the
effective number of partonic degrees of freedom, and @xmath is the
temperature [ 5 ] . The effective number of partonic degrees of freedom
is given by

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

where @xmath and @xmath are the degeneracies of, respectively, the quark
and gluon states. Each quark flavor has a quark/antiquark state, two
spin states, and three color states, whereas each gluon has two spin
states and eight color states. The total degeneracy is, therefore, given
by

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

which yields the value @xmath for an @xmath flavor QGP. This is an order
of magnitude larger than for a hadron gas, where @xmath .

The horizontal arrow in Figure 1.1 indicates the Stephan- @xmath
Boltzmann limit for a QGP with @xmath light flavors. The lattice QCD
calculation shows that @xmath above @xmath remains far below this limit,
indicating that a QGP, according to these calculations, does not behave
as an ideal gas of quarks and gluons.

The possible existence of a QGP was conjectured before the advent of
lattice QCD calculations, and already in the 1980s experiments started
to look for signatures of this plasma in heavy ion collisions. This
initiated the rapidly developing field of heavy ion physics, and led to
a large series of experiments performed at the AGS in Brookhaven, the
ISR and the SPS at CERN, and, since the year 2000, at the Relativistic
Heavy Ion Collider (RHIC) at the Brookhaven National Laboratory (BNL) in
the USA.

### 1.2 Heavy ion collisions

To describe a particle collision, we denote by @xmath the 4 @xmath
-momentum of particle @xmath moving along the beam ( @xmath axis), and
by @xmath the 4 @xmath -momentum of particle @xmath moving in the
opposite direction. The Lorentz @xmath -invariant measure of the square
of the center-of-mass energy available in the collision is

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

The Lorentz @xmath -invariant inclusive cross section of the scattering
process

  -- -------- --
     @xmath   
  -- -------- --

is defined by

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

where @xmath is the final state particle being measured and @xmath
denotes all other particles produced in the collison [ 6 ] . Because of
azimuthal symmetry, it is convenient to separate longitudinal and
transverse momentum components. In Eq. ( 1.5 ), @xmath and p are the
energy and 3 @xmath -momentum, @xmath is the transverse component of the
momentum, @xmath is the azimuthal angle, and @xmath is the rapidity of
particle @xmath in the center-of-mass frame. The rapidity is a measure
of the longitudinal momentum component ( @xmath ) and is defined by

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

The rapidity variable has the advantage of being additive under Lorentz
boosts along the @xmath axis. Another commonly used variable is the
pseudorapidity ( @xmath ) defined by

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

which is simply a measure of the polar angle ( @xmath ) and does not
depend on the particle mass. This is, therefore, a convenient variable,
since it can be calculated without knowing the particle identity. In the
limit @xmath of very energetic particles, the pseudorapidity @xmath
approaches the rapidity @xmath , because particle masses can then be
neglected.

Because atomic nuclei are spatially extended objects, a characteristic
of nucleus @xmath -nucleus collisions is the impact parameter ( @xmath
), which is the transverse distance between the centers of the two
colliding nuclei, as shown in Figure 1.2 .

Other measures of the collision centrality are the number of
participants ( @xmath ) and the number of binary collisions ( @xmath ).
The number of binary collisions @xmath is defined as the number of
individual inelastic nucleon-nucleon collisions that happened during the
nucleus @xmath -nucleus collision. The number of participants @xmath is
defined as the number of nucleons that suffered at least one inelastic
collision with another nucleon. The relation between the impact
parameter @xmath and the number of collisions @xmath or @xmath is
calculable in the framework of the Glauber model [ 7 ] .

Experimentally, the centrality of a heavy ion collision is estimated
from a measurement of one or more quantities that vary monotonically
with the impact parameter. Such quantities are the charged particle
multiplicity ( @xmath ), the transverse energy ( @xmath ) of all charged
particles emitted near midrapidity, or the forward energy ( @xmath )
measured close to the beam line. The relation between the observables
and the impact parameter is established by Monte Carlo event generators
that model nuclear collisions at relativistic energies [ 8 ] .

The range of impact parameters can be represented as a fraction of the
total geometric cross section. It is customary to define centrality
classes as adjacent intervals in @xmath that contain a certain
percentile of the differential cross section @xmath . For instance, a
@xmath – @xmath centrality class contains events with five percent of
the smallest impact parameters, such that it corresponds to five percent
of the total geometric cross section.

### 1.3 Heavy ion physics at RHIC

RHIC is a multipurpose colliding beam facility [ 9 , 10 ] , capable of
accelerating protons, deuterons, and heavy ions over a broad energy
range. At present, RHIC has delivered colliding beams of protons,
deuterons, copper, and gold ions with beam energies of up to 100 @xmath
per nucleon [ 11 , 12 ] .

An estimate of the energy density in the created medium is obtained
using the Bjorken formula [ 13 ]

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

where @xmath is the formation time and @xmath is the initial radius of
the expanding system. Using the value @xmath measured in central @xmath
collisions [ 14 ] and taking @xmath , together with reasonable guess for
the parameter value @xmath , an initial energy density of about @xmath
is calculated. This is well above the critical energy density of about 1
@xmath predicted by lattice QCD for a phase transition to the
quark-gluon plasma, as shown in Figure 1.1 . A major part of the physics
program at RHIC is, therefore, to measure particle production in high
energy nuclear collisions with the goal to study the properties of the
state of matter (presumably a QGP) produced in such collisions.

Particles emitted with large transverse momentum are important probes of
the medium produced in the collision, because they most likely originate
from high energetic partons that propagate through and couple to the
created medium and thus carry information about its properties. A
convenient way to observe medium-induced modification of particle
production is to compare a nucleus @xmath -nucleus collision ( @xmath )
with an incoherent superposition of the corresponding number of
individual nucleon-nucleon collisions ( @xmath ). This is done via the
nuclear modification factor ( @xmath ), defined as the ratio of the
particle yield in nucleus @xmath -nucleus collisions and the yield in
nucleon-nucleon collisions scaled with the number of binary collisions
@xmath ,

  -- -- -- -------
           (1.9)
  -- -- -- -------

Here @xmath @xmath @xmath is the nuclear overlap function that is
related to the number of inelastic nucleon-nucleon collisions in one
@xmath collision through

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

In the absense of medium effects, the nuclear modification factor is
unity, while @xmath indicates a suppression of particle production in
heavy ion collisions, compared to an expectation based on an incoherent
sum of nucleon-nucleon collisions.

In Figure 1.3

we show the ratio @xmath of charged hadron production, as a function of
@xmath , measured by the STAR Collaboration in central @xmath collisions
at @xmath [ 15 ] (the quantity @xmath is the center-of-mass energy of an
individual nucleon-nucleon collision). It is evident that charged
particle production in @xmath collisions is significantly suppressed,
compared to that in @xmath collisions at the same center-of-mass energy,
in particular at large @xmath , where @xmath reaches a value of about
0.2.

Also shown in Figure 1.3 is the nuclear modification factor measured in
minimum bias (no centrality selection) and central @xmath collisions.
This measurement is important to distinguish between initial and final
state effects. Since we can safely assume that in @xmath collisions no
hot and dense medium is created, the presence of a suppression would
indicate initial state effects, such as nuclear modification of the
parton densities in the gold nucleus. It is seen from Figure 1.3 that
such suppression is absent in @xmath collisions, indicating that the
suppression observed in @xmath collisions is a final state effect caused
by the dense medium created in such collisions.

A significant enhancement @xmath seen in @xmath collisions in the region
@xmath in Figure 1.3 can be explained by the so @xmath -called Cronin
effect [ 16 ] . This effect is likely caused by multiple scattering of
the projectile parton inside the target nucleus, which acts as an
additional transverse momentum kick of the parton, overpopulating the
@xmath region. Since there is an indication in Figure 1.3 of a possible
suppression in @xmath collisions at @xmath , it is interesting to
measure the @xmath factor at even higher @xmath . This thesis presents
such a measurement.

For peripheral collisions, the number of participant nucleons is small
and the creation of a dense medium is not expected. This is illustrated
in Figure 1.4 ,

which shows the centrality dependence of @xmath for charged hadrons as
measured by STAR in @xmath collisions. Indeed, the large suppression
observed in central collisions gradually vanishes with decreasing
centrality. This suggests that, instead of @xmath interactions,
peripheral collisions can be used as a reference. This is done through
the ratio of particle production in central ( @xmath ) and peripheral (
@xmath ) events:

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

The advantage of this measure is that no @xmath reference data are
needed. The disadvantage is that a stronger model dependence is
introduced, because the uncertainties in @xmath @xmath @xmath are much
larger for peripheral collisions. In Figure 1.5

is shown @xmath for charged hadrons measured in @xmath collisions by
STAR [ 17 ] .

To provide a useful reference, it is important to measure particle
production in nucleus @xmath -nucleus interactions, as well as in the
@xmath collisions, under the same experimental conditions. For instance,
prior to having the first @xmath collisions delivered by RHIC, both STAR
and PHENIX collaborations have published the measurements of @xmath [ 18
, 19 ] based on @xmath and @xmath reference spectra obtained from a
large body of world data, extrapolated to RHIC energies. These
extrapolations yielded significant systematic uncertainties and more
precise measurements of @xmath [ 17 , 20 ] only became available when
@xmath reference data were taken at RHIC in 2001–2002.

A detailed study of the intermediate- and high- @xmath production of
various hadron species shows that there is a systematic difference
between meson and baryon production in @xmath collisions, as illustrated
in Figure 1.6 [ 21 ] .

The @xmath ratio for identified hadrons is shown separately for mesons
(a) and baryons (b), and the clear difference between them suggests that
the particle production in this @xmath range depends not on the mass of
the hadron but rather on the number of valence quarks contained within
it. This can be explained naturally in the quark recombination model for
hadron formation, rather than fragmentation. We do not discuss this
model here and refer to [ 22 , 23 , 24 , 25 , 26 , 27 ] for details. The
measurement of @xmath for neutral pions and eta mesons would also be
interesting in context of this observation.

This thesis presents a baseline measurement with the STAR detector of
neutral pion and eta meson production in @xmath and @xmath
collisions at a center-of- mass energy of @xmath . The neutral pion
spectrum complements that of the charged pions measured in STAR in the
range @xmath [ 28 ] and extends up to @xmath . Preliminary results of
this analysis have been published in [ 29 , 30 ] . Also presented in
this thesis are the first measurements by STAR of @xmath meson
production.

### 1.4 Proton-proton collisions

In QCD, the hadronic interactions are described in terms of the
interactions of their constituent partons. The inclusive cross section
of the reaction

  -- -------- --
     @xmath   
  -- -------- --

is calculated as the weighted sum of differential cross sections of all
possible parton scatterings that can contribute [ 6 ] :

  -- -- -- --------
           (1.12)
  -- -- -- --------

Here @xmath is the parton density function (PDF) that gives the
probability that hadron @xmath contains a parton @xmath which carries
the fraction @xmath of its momentum. A similar definition applies to the
density @xmath . The cross section @xmath of the hard partonic
scattering

  -- -------- --
     @xmath   
  -- -------- --

is calculated in pQCD. The invariant kinematic variables for the
partonic sub @xmath -process are

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the partonic center-of-mass energy and @xmath is the
momentum transfer from @xmath to @xmath . The fragmentation function
@xmath in Eq. ( 1.12 ) describes the probability that a given parton
@xmath produces a final state hadron @xmath carrying a momentum fraction
@xmath .

It follows from the above that the cross section calculations rely on
two inputs — parton densities @xmath and fragmentation functions @xmath
. These functions are non-perturbative, so that they cannot be
calculated in QCD from first principles. However, they represent a
properties of individual hadrons independent of the process in which
they participate. Parton densities and fragmentation functions can,
therefore, be obtained from an analysis of a large variety of scattering
data.

A widely used set of parton densities is obtained by the CTEQ
Collaboration from a global QCD analysis of a large body of experimental
data [ 31 ] . The global fit, together with a detailed treatment of
published experimental uncertainties, resulted in an excellent agreement
with all available data. An alternative popular parametrization is MRST
[ 32 ] .

The fragmentation functions @xmath can be obtained directly from the
process @xmath , in which the initial state has no hadrons. Such
annihilation processes have been measured at many @xmath colliders over
a wide range of center-of-mass energies. The most recent
parametrizations of fragmentation functions are KKP [ 33 ] , BKK [ 34 ]
, BFGW [ 35 ] , and Kretzer [ 36 ] .

The cross sections for the individual partonic sub @xmath -processes are
calculated in pQCD with no additional input, except for the strong
coupling constant @xmath . These calculations are usually performed at
next-to @xmath -leading order (NLO), or even at next-to @xmath -next-to
@xmath -leading order (NNLO).

An important initial state effect in the heavy ion collisions is the
modification of parton distribution functions inside nuclei. It is well
known, that the quark structure functions at low fractional momentum are
depleted in a nucleus relative to a free nucleon. This depletion is
commonly referred to as nuclear shadowing. In Figure 1.7

we show the shadowing effects in @xmath collisions on the @xmath ratio [
40 ] , calculated with various parametrizations — EKS98 [ 37 ] , nDS [
38 ] , and HIJING [ 39 ] . It is also a motivation for the present
analysis to observe the nuclear shadowing and differentiate between
models, although the required experimental precision may be
prohibitively high.

## Chapter 2 The experiment

### 2.1 RHIC accelerator complex

The STAR experiment is located at the Brookhaven National Laboratory
(BNL) on Long Island, USA. An important part of the physics program of
the Laboratory is carried out at the Relativistic Heavy Ion Collider
(RHIC). This is a multipurpose colliding beam facility [ 9 , 10 ] ,
capable of accelerating protons, deuterons, and heavy ions over a broad
energy range from the injection energy per nucleon of @xmath up to the
top energy of @xmath for heavy ions and @xmath for protons.

The layout of the accelerator complex is shown in Figure 2.1 .

Heavy ions are accelerated in the Tandem Van de Graaff accelerator, the
Booster, the Alternating Gradient Synchrotron (AGS), and in the RHIC
accelerator itself. The Linac serves to accelerate protons, which are
then injected into the Booster. Below we will give a short description
of each component of the accelerator complex.

##### Tandem Van de Graaff generator

Gold ions with unit negative charge are generated in the Pulsed Sputter
Ion Source which delivers @xmath pulses of @xmath duration each. The
ions are then accelerated in the Tandem Van de Graaff generator from the
ground to @xmath potential. They pass a set of stripping foils where
they acquire a unit positive charge, and are subsequently accelerated
again to the ground potential. The @xmath ions leaving the Tandem are
stripped further to a charge of @xmath . There are two identical Tandems
available to provide two different ion species simultaneously (presently
deuterium and copper in addition to gold).

##### Linac

The LINAC serves to accelerate protons to an energy of @xmath , which
are injected directly into the Booster.

##### Booster synchrotron

The @xmath long Tandem pulse is injected into the Booster, after which
the particles are captured into six bunches and accelerated to an energy
of @xmath . Gold ions, when they are extracted from the Booster, are
stripped to the charge @xmath , leaving only two tightly bound @xmath
@xmath -shell electrons to be stripped at a later stage in the
acceleration chain.

##### Ags

From the Booster, @xmath bunches are injected into the AGS and
rearranged into four final bunches containing @xmath ions each. Those
bunches are accelerated to an energy of about @xmath . When transferred
to the RHIC accelerator, the ions are fully stripped to the charge
@xmath in case of copper and to @xmath in case of gold.

##### RHIC accelerator

The final stage of acceleration takes place in the RHIC synchrotron,
where beams are circulating in two rings in opposite directions. The
rings have a circumference of @xmath and are equipped with independent
bending and focusing magnets and RF cavities. This provides the
capability of operating the accelerator with two beams of unequal
species. The bending magnets are superconductive and cooled by liquid
helium. The complete cooling of the rings from room temperature to the
operating temperature of @xmath takes about ten days.

Up to @xmath bunches can be injected in each ring and accelerated to an
energy between @xmath and @xmath . After acceleration, the bunches are
transferred to the storage RF system, which maintains the bunch length
at @xmath or @xmath . The lifetime of a stored beam is about @xmath
hours, whereafter the beam is dumped and a new fill begins. A chosen
pattern of empty buckets provides a sample of unpaired bunches crossing
each interaction region for beam- @xmath background studies.

Beams are made to cross at six points along the ring, four of which are
used by the experiments STAR [ 41 ] , PHENIX [ 42 ] , PHOBOS [ 43 ] ,
and BRAHMS [ 44 ] . Of the remaining two crossing points, one is
occupied by the RF system, while the other is not used at present.

##### RHIC performance

To date, RHIC has delivered a variety of colliding beams of protons,
deuterons, copper ( @xmath ), and gold ions ( @xmath ) [ 11 , 12 ] .
In Table 2.1

we list the RHIC runs from the beginning of operations in the
year 2000 up to the year 2007. The @xmath runs provide reference data
for the heavy ion physics program, as well as data to measure the proton
spin structure at RHIC. For the latter purpose, the proton beams are
polarized, reaching degrees of up to @xmath in 2006. The data used in
this thesis were taken in the @xmath run in 2002/03 and @xmath in 2005,
both at center-of-mass energies of @xmath .

### 2.2 STAR detector

The STAR detector (Solenoidal Tracker At RHIC) [ 41 ] was designed
primarily for measurements of hadron production in heavy ion and
proton-proton collisions over a large solid angle. For this purpose,
large acceptance high granularity tracking detectors are placed inside a
large volume magnetic field. A perspective view of the detector is shown
in Figure 2.2 ,

and a cutaway side view in Figure 2.3 .

The barrel tracking detectors in STAR are a Silicon Vertex Tracker,
surrounding the beam pipe, (SVT, not used in this analysis) and a large
volume Time Projection Chamber (TPC), with an inner radius of @xmath ,
an outer radius of @xmath , and a length of @xmath . The TPC covers a
pseudorapidity range of @xmath and is designed to reconstruct the very
high multiplicity events produced in heavy ion collisions. These
multiplicities can reach up to @xmath charged tracks per unit rapidity
in a central @xmath collision at the largest beam energies. High
granularity tracking in the forward and backward regions is achieved by
two Forward TPCs (FTPC), each covering a range of @xmath in
pseudorapidity.

For trigger purposes, the TPC is surrounded by a layer of scintillator
tiles (Central Trigger Barrel, CTB, not used in this analysis) .

To trigger on the energy deposited by high transverse momentum photons,
electrons, and electromagnetically decaying hadrons, a Barrel
Electromagnetic Calorimeter (BEMC) [ 45 ] was incrementally added to the
STAR setup from the year 2001 to 2005.

The calorimeter surrounds and covers the full acceptance of the TPC and
CTB. An Endcap Electromagnetic Calorimeter (EEMC) [ 46 ] was installed
in 2002–2003  to cover the pseudorapidity range @xmath . In the data
taking period covered by this thesis, only the West half of the BEMC was
fully operational ( @xmath ).

The STAR barrel detectors are placed inside a room temperature
solenoidal magnet with maximum field of @xmath . The inner dimensions of
the magnet are @xmath in length and @xmath in diameter.

To provide a minimum bias trigger and to measure centralities in heavy
ion collisions, two sampling calorimeters (ZDC) are placed in the RHIC
tunnel at @xmath from the interaction point. Tiled arrays of
scintillator counters (Beam-Beam Counter, BBC) are mounted around the
beam pipe at a distance of @xmath from the interaction point, to provide
a minimum bias trigger in @xmath collisions. The detector subsystems
relevant for the present analysis are briefly described in the following
sections. We refer to Chapter 3 for a detailed description of the BEMC,
which plays a central role in the analysis.

Throughout this thesis we will use a Cartesian coordinate system defined
as follows: @xmath pointing along the beam in the West direction (see
Figure 2.1 ), @xmath pointing upward, right @xmath -handed.

#### 2.2.1 Time Projection Chamber

The Time Projection Chamber [ 47 ] is the central tracking device in
STAR. It allows one to track charged particles, measure their momenta,
and identify the particle species by measuring the ionization energy
loss @xmath .

A schematic layout of the TPC is shown in Figure 2.4 .

The TPC barrel measures @xmath in length and has an inner radius of
@xmath and an outer radius of @xmath . The TPC acceptance covers @xmath
units in pseudorapidity and full azimuth. Particles are identified over
a momentum range from @xmath to @xmath , and their momentum is measured
in the range from @xmath to @xmath .

The TPC is a gas filled cylindrical volume with a well defined uniform
electric field gradient of about @xmath . The secondary electrons
released by ionizing particles along their path drift in the electric
field towards the readout endcaps. The electric field is generated
between a central membrane held at @xmath potential and the endcaps,
which are held at ground potential. A uniform field gradient is
maintained by concentric equi-potential field cage cylinders biased via
@xmath resistors. The drift volume is filled with a gas mixture of
@xmath methane and @xmath argon, which is held slightly above
atmospheric pressure. The drift velocity is @xmath , and the maximum
drift time from the central membrane to endcap is @xmath .

The endcaps are instrumented with Multi @xmath - @xmath Wire
Proportional Chambers (MWPC) with pad readout. The transverse
coordinates of a track are reconstructed from the hits in the MWPCs,
while the @xmath coordinate is reconstructed from a measurement of the
drift time. The total drift time of @xmath is sampled by the readout
electronics in @xmath time buckets.

In each endcap, the MWPCs are arranged in @xmath sectors, each
consisting of inner and outer sub @xmath -sector. The inner sub @xmath
-sectors are in the region of highest track density and are, therefore,
optimized for better two @xmath -track resolution, while the outer sub
@xmath -sectors are optimized for better performance in the measurement
of @xmath .

In the analysis presented in this thesis, the TPC is used as a charged
particle veto in the identification of photons in the BEMC. Samples of
electrons reconstructed in the TPC serve to calibrate the energy
response of the BEMC.

#### 2.2.2 Forward TPC modules

Two Forward Time Projection Chambers (FTPC) [ 48 ]
extend the STAR tracking capability to the pseudorapidity range @xmath .
The layout of the FTPC is shown in Figure 2.5 .

Each FTPC is a cylindrical volume with a diameter of @xmath and a length
of @xmath , with radial drift field and pad readout chambers mounted on
the outer cylindrical surface. Two such detectors are installed
partially inside the main TPC on both sides of the interaction point.
The FTPC is capable of reconstructing all charged tracks (typically
@xmath ) traversing the detector in a central @xmath event.

In this thesis, the forward charged track multiplicity recorded in the
FTPCs is used as a measure of the centrality in @xmath collisions.

#### 2.2.3 Zero Degree Calorimeter

In addition to the STAR barrel detectors, a sampling calorimeter is
placed at a distance of @xmath from the interaction point in the RHIC
tunnel on both sides of the experimental hall, as shown in Figure 2.6 .

These Zero Degree Calorimeters (ZDC) [ 50 , 49 ] are used to provide the
minimum bias trigger and to measure centralities in heavy ion
collisions. Furthermore, identical ZDC detectors are installed at each
of the four RHIC experiments, providing comparable collision rate
measurements to monitor the RHIC luminosity.

The ZDC detector measures the total energy of the unbound neutrons
emitted from the nuclear fragments after a collision. The charged
fragments of the collision are bent away by the RHIC dipole magnets DX.
In the upper plot of Figure 2.6 is shown a transverse view at the front
face of the ZDC, indicating the position of the two beam pipes, the
neutron spot inside the ZDC acceptance, and the spot of deflected
fragments with @xmath .

The mechanical layout of the ZDC is shown in Figure 2.7 .

It consists of alternating layers of tungsten absorber and Cherenkov
fibers with a total length of about @xmath . The transverse dimension of
@xmath corresponds to an angular acceptance of about @xmath around the
forward direction.

In this thesis, we do not use the ZDC for centrality measurement, and
refer to [ 49 ] for details on such a measurement. For the @xmath data
used in the present analysis, the ZDC provided a minimum bias trigger by
requiring the detection of at least one neutron in the @xmath beam
direction. The acceptance of this trigger corresponds to @xmath of the
total @xmath geometrical cross section, as determined from detailed
simulations of the ZDC acceptance [ 15 ] .

#### 2.2.4 Beam-Beam Counter

To provide a minimum bias trigger in @xmath collisions, Beam-Beam
Counters (BBC) [ 51 , 52 ] are mounted around the beam pipe beyond both
poletips of the STAR magnet at a distance of @xmath from the interaction
point. The BBC also serves to reject beam-gas events at the trigger
level and to measure the beam luminosity in @xmath runs.

The detector

consists of two sets of hexagonal scintillator tiles, see Figure 2.8 . A
ring with radius between @xmath and @xmath is formed by @xmath small
tiles, while @xmath large tiles on the outside cover a radius between
@xmath and @xmath . The small and large tile arrangements cover the
pseudorapidities @xmath and @xmath , respectively.

In @xmath runs, a minimum bias trigger is provided by a coincidence of
signals in at least one of the @xmath small BBC tiles on each side of
the interaction region.

The two BBC counters also record the time of flight, which provides a
measurement of the @xmath position of the interaction vertex to an
accuracy of about @xmath . Large values of the time of flight difference
between the two BBC counters indicate the passage of beam halo, which is
rejected at the trigger level.

A measurement of the counting rate in the BBCs allows for a
determination of the absolute luminosity to an accuracy of about @xmath
, the relative luminosities per run are determined to a precision of
better than @xmath [ 51 , 52 ] .

## Chapter 3 STAR Electromagnetic Calorimeter

The Barrel Electromagnetic Calorimeter (BEMC) [ 45 ] is a
lead-scintillator sampling calorimeter, surrounding the STAR TPC as
shown in Figure 3.1 .

The BEMC was installed in several stages during the period of 2001–2005.
Only the West half of the BEMC was fully operational during the 2003 and
2005 runs which provided the data presented in this thesis. The Endcap
Calorimeter [ 46 ] , which is not used in the present analysis, was
installed in the years 2002–2003.

The BEMC is used to trigger on and to measure jets, leading hadrons,
direct photons, and electrons from heavy quarks produced at large
transverse momentum. For this purpose, the BEMC provides large
acceptance for photons, electrons, @xmath , and @xmath mesons in all
colliding systems ranging from @xmath up to @xmath . In the next
sections we will describe the BEMC in more detail.

### 3.1 Mechanical layout

The calorimeter is located inside the magnet coil and surrounds the TPC.
It covers a pseudorapidity range of @xmath and full azimuth, matching
the TPC acceptance. The calorimeter is divided in two adjacent barrels,
one positioned at the West half of the STAR detector ( @xmath ) and the
other one at the East half ( @xmath ). Each half-barrel has a length of
@xmath , an inner radius of @xmath , and an outer radius of @xmath .

The half-barrel is azimuthally segmented into @xmath modules. Each
module is approximately @xmath wide and covers @xmath degrees ( @xmath )
in azimuth and one unit in pseudorapidity. The active depth is @xmath ,
to which is added @xmath of structural elements at the outer radius. The
longitudinal and transverse segmentation of a module is shown in Figure
3.2 ,

and the radial structure in Figure 3.3 .

The modules are segmented into @xmath projective towers of
lead-scintillator stacks, @xmath in @xmath and @xmath in @xmath . A
tower covers @xmath in @xmath and @xmath in @xmath . Each calorimeter
half is thus segmented into a total of @xmath towers.

Each tower consists of an inner stack of @xmath layers of lead and
@xmath layers of scintillator, and an outer stack of @xmath layers of
lead and @xmath layers of scintillator. All these layers are @xmath
thick, except the innermost two scintillator layers, which are @xmath
thick. A separate readout of these latter two layers provides the
calorimeter preshower signal. A Shower Maximum Detector ( @xmath ) is
positioned between the inner and outer stacks, at a depth of
appoximately @xmath radiation lengths. The whole stack is held together
by mechanical compression and friction between layers.

### 3.2 Optical structure

The plastic scintillator layers are machined as “megatiles”, covering
the full length and width of a module. These megatiles are segmented
into @xmath optically isolated tiles, as shown in the top diagram of
Figure 3.2 . The optical separation between the individual tiles is
achieved by @xmath deep cuts in the scintillator filled with opaque
epoxy. The optical crosstalk between adjacent tiles is reduced to a
level of @xmath by painting a black line on the surface opposite to the
isolation groove.

The optical readout scheme is shown in Figure 3.4 .

The signal from each tile is collected by a wavelength shifting (WLS)
fiber embedded in a @xmath -groove in the tile. The WLS fibers run along
the outer surface of the stack and terminate in an optical connector
mounted at the back-plate of the module. From the back-plate, @xmath
long fibers run through the STAR magnet structure to the readout boxes
mounted on the outer side of the magnet. In these boxes, the 21 fibers
from the tiles of one tower are connected to a single photomultiplier
tube (PMT). The PMTs are powered by Cockroft- @xmath Walton bases, which
are remotely controlled over a serial communication line by the slow
control software.

From layer by layer tests of the BEMC optical system, together with an
analysis of cosmic ray and test beam data, the nominal energy resolution
of the calorimeter is estimated to be @xmath [ 53 ] .

### 3.3 Shower Maximum Detector

The Shower Maximum Detector ( @xmath ) is a multi- @xmath wire
proportional counter with strip readout. It is located at a depth of
approximately @xmath radiation lengths at @xmath increasing to @xmath
radiation lengths at @xmath , including all material immediately in
front of the calorimeter.

The purpose of the @xmath is to improve the spatial resolution of the
calorimeter. This is necessary because the transverse dimension of each
tower (about @xmath ) is much larger than the lateral spread of an
electromagnetic shower. The improved resolution is essential to separate
the two photon showers originating from the decay of high momentum
@xmath and @xmath mesons.

The layout of the @xmath is shown in Figure 3.5 .

Independent cathode planes with strips along @xmath and @xmath
directions allow the reconstruction of a two-dimensional image of a
shower. The coverage in @xmath is @xmath for the @xmath strips and
@xmath for the @xmath strips. There are a total of @xmath strips in the
full detector.

Beam test results at the AGS have shown that the @xmath has an
approximately linear response versus energy. The energy resolution in
the @xmath coordinate (front plane) is approximately @xmath , whereas
that in the @xmath coordinate (back plane) is worse by about @xmath –
@xmath . The position resolution is @xmath and @xmath .

### 3.4 Preshower Detector

The first and second scintillating layers of each calorimeter module are
used as a preshower detector (PSD). To achieve a separate readout of
these layers, two WLS fibers are embedded instead of one in the @xmath
-groove of each tile. This additional pair of fibers from the two layers
illuminate a single pixel of a multi-anode PMT. A total of @xmath @xmath
@xmath -pixel multi-anode PMTs are used to provide the @xmath tower
preshower signals.

The preshower detector was fully instrumented and read out only in 2006,
so that it could not be used in the present analysis.

### 3.5 BEMC electronics

The calorimeter is a “fast” detector in STAR, so that its ADCs can be
read out on each RHIC bunch crossing. The calorimeter data is also used
in the STAR level-0 trigger, in the form of the “High Tower” and “Patch
Sum” trigger primitives.

The level-0 HighTower trigger used in this analysis is a requirement
that the energy deposited in any single calorimeter cell in the event
exceeds a given threshold. This allows one to enhance the statistics at
the high energy part of the spectrum.

The complete description of the BEMC electronics operation is given in
Appendix A .

## Chapter 4 Event reconstruction in STAR

### 4.1 Data aquisition and trigger

The STAR data aquisition system (DAQ) [ 54 ] receives the input from
multiple detectors at various readout rates. The typical recorded event
rate of @xmath is limited by the drift time in the TPC (the slowest
detector in STAR). The total event size can reach up to @xmath in @xmath
collisions. STAR takes data in runs of about half an hour duration, each
having @xmath @xmath – @xmath events.

The STAR trigger [ 55 ] is a pipelined system, capable to cope with the
RHIC beam crossing frequency of @xmath . The trigger processes
information from fast detectors, such as the ZDC, BBC, CTB, or BEMC, and
decides if the event should be read out and saved to tape. Each event is
categorized by multiple trigger criteria, and the events selected by
various branches of the decision tree are written to tape, sharing the
available @xmath DAQ bandwidth.

The datasets used in the present analysis were taken in the @xmath run
of 2003 and the @xmath run of 2005, see also Table 2.1 . The following
trigger conditions had to be satisfied:

##### Minimum bias (MinBias) trigger in @xmath collisions

This condition required the presence of at least one neutron signal in
the ZDC in the gold beam direction. As given in [ 15 ] , this trigger
condition captured @xmath of the total @xmath geometric cross section of
@xmath .

##### MinBias trigger in @xmath collisions

This condition required the coincidence of signals from two BBC tiles on
the opposite sides of the interaction point. Due to the dual-arm
configuration, this trigger is sensitive to the non-singly diffractive
(NSD) cross section, which is a sum of the non-diffractive and doubly
diffractive cross section. The total inelastic cross section is a sum of
the NSD and singly diffractive cross section.

A minimum bias cross section of @xmath was independently measured via
Vernier scans in dedicated accelerator runs [ 56 ] . This trigger
captured @xmath of the @xmath non-singly diffractive (NSD) cross
section, as was determined from the detailed simulation of the BBC
acceptance [ 17 ] . Correcting the BBC cross section for the acceptance,
we obtain a value for the NSD cross section of @xmath .

##### HighTower trigger

This condition required, in addition to the MinBias, an energy deposit
above a predefined threshold in at least one calorimeter tower. The
purpose of this trigger is to enrich the sample with events that have a
large transverse energy deposit. Two different thresholds were applied,
giving the HighTower- @xmath 1 and HighTower- @xmath 2 datasets. The
values of these thresholds for the various runs are shown in Table 4.1 .

### 4.2 STAR reconstruction chain

The events recorded on tape are passed through the standard STAR
reconstruction chain. This reconstruction is performed routinely on the
RHIC Computing Facility (RCF), which is a large computing farm located
at BNL.

The most important part of the data reconstruction at this stage is
tracking in the TPC and FTPCs. Charged tracks are reconstructed in the
main TPC using a Kalman filter [ 57 ] , and in the FTPCs using a
conformal mapping method [ 58 ] . The primary vertex is found by
extrapolating and intersecting all reconstructed tracks. The vertex
resolution in @xmath is between @xmath and @xmath depending on the track
multiplicity, whereas in the transverse plane it is about @xmath . Once
the vertex has been found, all tracks that approach to it closer than
@xmath are re @xmath -fitted to include the vertex position as the
origin. Although the wire chambers are sensitive to almost @xmath of the
drifting electrons, the overall tracking efficiency is only @xmath –
@xmath due to fiducial cuts, track merging, bad pads, and dead channels.
The momentum resolution of tracks worsens linearly with @xmath from
@xmath for @xmath pions to @xmath for @xmath pions.

Because the BEMC reconstruction is not yet performed in the standard
STAR reconstruction, the raw BEMC data are passed to the physics
analysis. The tower ADC data can be directly passed because they only
take a small fraction of the event size. The @xmath strip ADCs are
zero-suppressed and then also passed to the analysis. This scheme
implies that removal of malfunctioning elements and a full calibration
of the BEMC is performed as a part of the physics analysis. This has the
advantages that the reconstructed electron tracks in the TPC can be used
to calibrate the energy response of the BEMC, and that the successive
improvements in the BEMC calibration do not require re-generating the
full dataset from the raw events on tape.

All data reconstruction and analysis in STAR is performed using the ROOT
framework [ 59 ] . The processing of a full dataset, such as @xmath or
@xmath , takes about three months.

### 4.3 BEMC status tables

A quality assurance (QA) procedure for the BEMC is routinely performed
before the physics analysis, in order to remove malfunctioning detector
components from the data and to correctly reproduce the time dependence
of the detector acceptance in the Monte Carlo simulation. This QA
procedure results in timestamped status tables, which are used as an
input to the physics analysis. Below we describe the QA procedure
performed for the BEMC towers, a similar procedure is applied to the
@xmath strips.

For each run, the raw ADC spectra of all towers were accumulated and a
number of criteria were applied to recognize common failure modes, such
as the malfunctioning of entire readout boards and crates. A typical ADC
spectrum of a tower is displayed in Figure 4.1

and shows the signal distribution and the accumulation of ADC counts in
absense of a signal (pedestal). The position of these pedestals provide
the zero offset of the ADC measurement and are, together with the width,
stored in time dependent tables for each tower. Channels with anomalous
pedestal positions and widths are flagged as bad. The signal fraction
was defined as the number of ADC counts that are more than six standard
deviations above the pedestal. Towers with a signal fraction smaller
than @xmath are flagged as “cold” or “dead”, while those with a fraction
above @xmath are marked as “hot” or “noisy” (the exact numbers are
multiplicity dependent and are adjusted for each collision system).
Saved are, as function of run number, the position of the pedestals,
their widths, and flags indicating the status of each tower. The average
fraction of good towers was found to be about @xmath in the 2003 @xmath
run, with run-to-run fluctuations of about @xmath – @xmath . In the 2005
@xmath data the fraction of good towers was found to be about @xmath .

In the BEMC reconstruction performed in this analysis, the status tables
were read in and used for pedestal subtraction of the ADC signals and
for removal of towers which were flagged as bad.

### 4.4 BEMC energy calibration

The purpose of the energy calibration is to establish the relation
between ADC counts and the energy scale in @xmath . The calibration
proceeds in two stages. First, a relative calibration matches the gains
of individual towers to achieve an overall uniform response of the
detector. A common scale between ADC counts and energy is then
determined in a second absolute calibration step. The relative
tower-by-tower calibration is done using minimum ionizing particles
(MIP), while the absolute energy scale is determined from energy
measurements of identified electrons in the TPC.

#### 4.4.1 MIP calibration

A significant fraction ( @xmath – @xmath ) of high energy charged
hadrons traversing the BEMC only deposit a small amount of energy in the
towers, equivalent to a @xmath – @xmath electron, largely due to
ionization energy loss (minimum ionizing particles). The signal from
these particles is usually well separated from the tower pedestals.

To identify MIP particles, TPC tracks of sufficiently large momentum
above @xmath are extrapolated to the BEMC and the response spectra are
accumulated, provided that the track extrapolation is contained within
one tower and that there are no other tracks found in a @xmath patch
around this tower. In Figure 4.2

is shown a tower ADC spectrum collected from the @xmath dataset, which
clearly shows the position of the MIP peak superimposed on a broad
background [ 60 ] . The position of the fitted Gaussian is calculated
for each tower and used to calculate the tower-by-tower gain corrections
needed to equalize the detector.

The disadvantage of this method is that the calibration is performed at
the low end of the scale, where the signal is more susceptible to noise
and where the lack of lever arm does not allow to detect possible
non-linearities in the detector response.

#### 4.4.2 Electron calibration

Because the electron momentum can be independently measured in the TPC,
it is possible to calibrate the absolute energy scale of the calorimeter
using the simple relation for the ultra-relativistic electrons, @xmath .

Figure 4.3

shows the electron energy measured in the calorimeter versus its
momentum measured in the TPC [ 60 ] . The calorimeter response is quite
linear up to @xmath , and the global gain correction obtained from the
linear fit is applied to all towers.

This method takes advantage of the well understood TPC detector for the
precise measurement of the electron track momentum in a wide range.
However, it requires high statistics to calibrate the high energy part
of the spectrum, so that only one global calibration constant for the
calorimeter is obtained at present. The systematic year-to @xmath -
@xmath year uncertainty on the electron calibration was estimated to be
@xmath [ 61 ] .

It has been found that the current calibration is less reliable at the
edges of the calorimeter half-barrel, therefore, the tower signals from
the two @xmath -rings at each side are later removed from this analysis
(see Section 5.2 ).

This combination of the MIP-based equalization and electron-based
absolute calibration is applied to the data after each running period,
starting from 2003 @xmath run. The run dependent calibration constants
are saved in the STAR database and automatically applied to the @xmath
readout in the software.

### 4.5 Event selection

The event selection starts with rejecting events where subdetectors
needed for this analysis were not operational or malfunctioning. In the
following sub @xmath -sections we will describe several additional
selection criteria in detail.

#### 4.5.1 Beam background rejection

In @xmath events, interactions of gold beam particles with material
approximately @xmath upstream from the interaction region give rise to
charged tracks that traverse the detector almost parallel to the beam
direction. To identify events containing such background tracks, the
ratio

  -- -------- --
     @xmath   
  -- -------- --

is calculated, where @xmath is the total energy recorded in the BEMC and
@xmath is the energy of all charged tracks reconstructed in TPC. In
events containing background tracks, the ratio @xmath tends to become
large because the background tracks give a large energy deposit in a
calorimeter without being reconstructed in the TPC, since they do not
point to the vertex. This is shown in Figure 4.4 ,

where the distribution of @xmath is plotted for the @xmath and @xmath
datasets. The peak near unity in the left @xmath -hand plot indicates
the presence of beam halo in @xmath collisions, and events with @xmath
were removed from the @xmath analysis. This cut rejected @xmath of
MinBias and @xmath of HighTower- @xmath 2 triggered events. From a
polynomial fit to the @xmath distribution in the region @xmath – @xmath
(curve in Figure 4.4 ), the false rejection rate was estimated to be
@xmath in the @xmath HighTower- @xmath 2 data and less than @xmath in
the other datasets.

The cut was not applied to the @xmath data since here the beam
background is almost absent, as can be seen in the right @xmath -hand
plot of Figure 4.4 .

During the summer in 2006, additional shielding walls were installed in
STAR to reduce this beam background to a negligible level.

#### 4.5.2 Vertex reconstruction

The event vertex is reconstructed to an accuracy of better than a
millimiter in the @xmath direction, from the tracks reconstructed in the
TPC. The distribution of the vertex @xmath coordinate in the @xmath
MinBias data is shown in Figure 4.5 .

Events with @xmath were rejected in the analysis, as indicated by the
vertical lines in Figure 4.5 . This cut is applied because the amount of
material traversed by a particle increases dramatically at large values
of @xmath . As a consequence, the TPC tracking efficiency drops for
vertices located far from the center of the detector.

In the HighTower trigger data, the track multiplicity is almost always
sufficient for a TPC vertex reconstruction, but this is not so in the
@xmath and @xmath minimum bias data. Since the @xmath minimum bias
trigger is based on coincidences in the BBC, we can use the timing
information of the BBC to reconstruct a vertex for every event, even
when the TPC vertex reconstruction failed (about @xmath of the minimum
bias events). The timing information from the BBC was calibrated against
the @xmath vertex coordinate reconstructed in the TPC, as illustrated in
Figure 4.6 (top),

where we show the correlation between the BBC time difference @xmath and
@xmath in the TPC. The straight line in the plot corresponds to a linear
fit

  -- -------- --
     @xmath   
  -- -------- --

yielding @xmath per ADC count and @xmath . In the bottom plot of Figure
4.6 we show the distribution of @xmath , together with a Gaussian fit.
From this fit we obtain the BBC vertex resolution of @xmath .

Whereas @xmath events without a TPC vertex can be recovered by using the
BBC timing information, this cannot be done for @xmath events because
the BBC is not in the trigger and timing information may be absent.
Since the @xmath reconstruction requires the presence of vertex, the
@xmath events without a TPC vertex are removed from the analysis. The
vertex finding efficiency was determined from detailed Monte Carlo
simulation of the full @xmath events and was found to be @xmath in the
@xmath window [ 15 ] . This result is used to correct the @xmath data
for vertex inefficiencies, as will be explained in Section 7.1 .

#### 4.5.3 HighTower trigger condition

The HighTower-triggered data are filtered using a software
implementation of the HighTower trigger. In this filter, the highest
tower ADC value found in the event is required to exceed the same
HighTower- @xmath 1 (HighTower- @xmath 2) threshold as the one that was
used during the run. This filter is needed to remove events that were
falsely triggered due to the presence of noisy channels (hot towers).
Such channels are identified offline in a separate analysis and recorded
in a database as described Section 4.3 . This software filter also
serves to make the trigger efficiency for Monte Carlo and real data as
close as possible.

### 4.6 Centrality selection in d@xmathAu data

To measure the centrality in @xmath collisions, we use the correlation
between the impact parameter of the collision and the charged track
multiplicity in the forward direction. This correlation was established
from a Monte Carlo Glauber simulation [ 19 , 62 ] using, as an input,
the Woods-Saxon nuclear matter density for the gold ion [ 63 ] and the
Hulthén wave function of the deuteron [ 64 ] . In this simulation, the
inelastic cross section of an individual nucleon-nucleon collision was
taken to be @xmath . The produced particles were then propagated through
a full GEANT simulation of the STAR detector and the charged track
multiplicity was recorded, together with the number of nucleon-nucleon
collisions simulated by the event generator.

For the event-by-event centrality determination, we measured the
multiplicity ( @xmath ) of tracks reconstructed in the FTPC @xmath -East
acceptance (in the @xmath beam direction), following the centrality
binning scheme used in other STAR publications [ 15 , 65 ] . The
following quality cuts were applied to the reconstructed tracks: @xmath
(i) at least @xmath hits are required on the track; (ii) @xmath , to
guarantee that the track is fully contained in the FTPC acceptance, and
(iii) distance of closest approach (DCA) to the vertex should be less
than @xmath . The multiplicity distributions obtained from the @xmath
dataset are shown in Figure 4.7

for the MinBias, HighTower- @xmath 1, and HighTower- @xmath 2 triggers.

Based on the measured multiplicity, the events were separated into three
centrality classes: @xmath – @xmath @xmath most central, @xmath – @xmath
mid central, and @xmath – @xmath @xmath most peripheral, as illustrated
by the vertical lines in Figure 4.7 .

Table 4.2

lists the @xmath ranges that defined the centrality classes, and the
corresponding mean number of binary collisions @xmath @xmath @xmath in
each class, obtained from the Glauber model. In the table are also
listed the systematic uncertainties on @xmath @xmath @xmath , which are
estimated by varying the Glauber model parameters.

## Chapter 5 Neutral meson reconstruction

The goal of this analysis is to measure @xmath and @xmath production in
@xmath and @xmath collisions. The @xmath and @xmath are identified by
their decay

  -- -------- --
     @xmath   
  -- -------- --

These decay modes have branching ratios of @xmath and @xmath ,
respectively [ 66 ] . The BEMC is used to detect the decay photons, as
will be described in the next sections. The lifetime of the @xmath is
@xmath , which corresponds to a decay length @xmath . The lifetime of
the @xmath is even shorter ( @xmath ). Therefore, we can assume that the
decay photons originate from the primary vertex. For each event, the
invariant mass

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

is calculated for all pairs of photons detected in the BEMC. Here @xmath
and @xmath are the energies of the decay photons and @xmath is the
opening angle between them, as measured in the laboratory system.

The reconstructed masses are accumulated in invariant mass spectra,
where the @xmath and the @xmath show up as peaks around their nominal
masses. These peaks are superimposed on a broad distribution of
combinatorial background, which originates from photon pairs that are
not produced by the decay of a single parent particle.

In Table 5.1

we list the number of events in all datasets used in the analysis after
the event selection procedures described in section 4.5 were applied.

### 5.1 BEMC clustering

The first step in the invariant mass reconstruction is to find clusters
of energy deposits in the calorimeter. The purpose of the cluster
finding algorithm is to group adjacent hits that are likely to have
originated from a single incident photon. The algorithm is applied to
the BEMC tower and preshower signals, as well as to the signals from
each of the two @xmath layers.

The clustering algorithm starts by accumulating a list of cluster seeds
that contains all hits in a module with an energy deposit above a
certain threshold ( @xmath ). Starting from the most energetic seed in
the list, an energy ordered list of module hits is searched for those
adjacent to the present cluster. When such a hit is found, then,
provided that it is above a second threshold ( @xmath ), it is added to
the cluster and removed from the list. The clustering stops when either
a pre-defined maximum cluster size ( @xmath ) is reached or no more
adjacent hits are found. The clustering algorithm then proceeds to
process the next most energetic seed. At the end, clusters with total
energy below the third threshold ( @xmath ) are discarded. Note, that,
by construction, the clusters are confined within a module and cannot be
shared by adjacent modules. However, the likelihood of cluster sharing
between modules is considered to be low since the modules are physically
separated by about @xmath air gaps. In Table 5.2

we list the threshold values used in the clustering algorithm for all
four detectors.

In Figure 5.1

we show the assignments made by the algorithm on several possible one
@xmath -dimensional cluster topologies. Note, that the rightmost hit
pattern in this figure shows a double @xmath -peak structure, which is
splitted into two adjacent clusters by the algorithm. However,
statistical fluctuations in single photon signals may also be the cause
of a double @xmath -peak structure. In such a case, the cluster
splitting by the algorithm becomes a source of background, as will be
discussed in Section 5.5 .

The readout of the @xmath @xmath and @xmath planes is one @xmath
-dimensional, so that there is no ambiguity in what is considered to be
an adjacent hit. The calorimeter tower readout is two-dimensional, and
two hits are considered to be adjacent when they share a side and not
when they share only a corner.

The cluster position in the @xmath and @xmath coordinates is calculated
as the energy weighted mean position of the participating hits. In this
calculation, the geometrical center of the detector element is taken as
the hit position.

After the tower, preshower, and @xmath clusters are found, the next step
is to combine them into so @xmath -called BEMC points that should
correspond as closely as possible to the impact point and energy deposit
of a photon that traversed the calorimeter. This procedure treats @xmath
tower patches corresponding to the @xmath - @xmath segmentation, as
shown by the top diagram in Figure 5.2 .

It is required that every reconstructed BEMC point contains a tower
cluster, since the energy deposit of the incident particle is measured
in the BEMC towers. Adding information from the @xmath leads to a
variety of combinations, as shown schematically by the diagrams (a)–(c)
in Figure 5.2 . In the following paragraphs we describe how each case
leads to the reconstruction of a BEMC point.

##### Tower, SMD-@xmath and SMD-@xmath clusters

The algorithm calculates for all combinations of @xmath - @xmath and
@xmath - @xmath clusters in a patch the energy asymmetry

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are, respectively, the energy deposits measured
in the @xmath - @xmath and @xmath - @xmath planes.

The cluster assignment constitutes a well known problem in combinatorics
(Assignment problem [ 67 ] ) which we solve by a call to the CERN
library routine ASSNDX [ 68 ] that combines objects into pairs in a way
that minimizes the total cost. In the present algorithm the cost
function is defined as energy asymmetry @xmath between clusters.

Each associated @xmath pair is matched to the tower cluster closest in
@xmath and @xmath . The total tower energy in a patch (including
unassociated) is shared between points, weighted by their average @xmath
energy, that is, each @xmath -th pair will produce a point with energy

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . The @xmath and @xmath coordinates are that of the @xmath
clusters.

This procedure works well, provided that the occupancies of the @xmath
tower patches are low. Indeed, the number of tower or @xmath clusters
reconstructed even in the most central @xmath events is below @xmath in
the complete half-barrel, corresponding to a mean number of @xmath
clusters per event and an average occupancy of @xmath per patch.

##### Tower and SMD-@xmath clusters

In this case, the tower and @xmath - @xmath clusters are associated by
the same algorithm as used above, except that here the cost function is
defined by the energy asymmetry

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the energy deposit in a tower and @xmath is the energy
deposit measured in the @xmath - @xmath plane. The total energy of tower
clusters in a patch is shared between associated pairs, weighted by
their tower energy:

  -- -------- --
     @xmath   
  -- -------- --

The @xmath coordinate associated to the BEMC point is taken directly
from the @xmath - @xmath cluster, while the @xmath coordinate is taken
from the tower cluster.

##### Tower and SMD-@xmath clusters

This case is treated as described above. The resulting BEMC points will
have the @xmath coordinate from the @xmath - @xmath clusters and the
@xmath coordinate from the tower clusters.

##### Tower clusters only

If there are no @xmath clusters in a patch that contains the tower
cluster position, the energy and coordinates of the BEMC point are taken
to be those of the tower cluster.

The relative occurances of these four cases are approximately in
proportion of @xmath for clusters with energy above @xmath , and @xmath
at the lower energies.

The information about the shower shape in the @xmath is in principle
available but not used in the present clustering algorithm.

### 5.2 BEMC cluster cuts

After clustering, only the BEMC points containing tower and both @xmath
- @xmath and @xmath - @xmath clusters were kept to be used in the
further analysis of the HighTower-triggered data. In the analysis of
MinBias data all reconstructed BEMC points were used, even when they do
not contain SMD clusters. From the decay kinematics in the laboratory it
follows that the opening angle between the photons is smallest when
these photons equally share the energy of the parent. In Figure 5.3

is shown this minimal opening angle versus the energy of the parent
@xmath or @xmath and compared to the tower and @xmath strip size. It is
seen that the spatial resolution of better than a calorimeter tower is
needed to resolve the decay photons of neutral pions with momenta larger
than @xmath . For this reason, the @xmath information is essential in
this analysis.

It is seen from beam tests [ 53 ] that the @xmath efficiency decreases
rapidly with energy of the traversing particle and is smaller than
@xmath at @xmath . The energy resolution @xmath is also poor at low
energy, so that significant fluctuations in the strip readout are
expected. Therefore, an @xmath cluster is required to contain signals
from at least two strips in order to be accepted in the HighTower-
@xmath 1 data. This cut rejects a large fraction of the distorted and
falsely split @xmath clusters, and reduces a possible effect of poor
@xmath response simulation at low energies.

It has been found that the tower calibration is less reliable at the
edges of the calorimeter acceptance. For this reason, we only keep the
reconstructed clusters in the range @xmath for the further analysis,
excluding two tower @xmath -rings at each side of the calorimeter
half-barrel.

A charged particle veto (CPV) cut is applied to reject the charged
hadrons that are detected in the calorimeter. These charged hadrons can
be recognized as BEMC clusters with a pointing TPC track. The cluster
was rejected if the distance between the BEMC point and the closest TPC
track ( @xmath ) was smaller than @xmath in the @xmath – @xmath
coordinates,

  -- -------- --
     @xmath   
  -- -------- --

The BEMC points remaining after this cut are considered to be photon
candidates, which are combined into pairs, defining the set of @xmath
candidates.

The asymmetry of the two @xmath -body decay of neutral mesons is defined
as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are the energies of the decay photons. From the
decay kinematics it follows that this energy asymmetry is uniformly
distributed between @xmath and @xmath [ 69 ] . In Figure 5.4

we show the distribution of the asymmetry of photon pairs reconstructed
in @xmath data. In the MinBias data the distribution is not flat because
of the acceptance effects — photons from the asymmetric decay have a
large opening angle and there is a large probability that one of them
escapes the barrel. It is also seen that the HighTower energy threshold
biases the asymmetry to the higher values, because it is easier for an
asymmetric decay to pass the trigger. In this analysis, the @xmath
candidates were only accepted if the asymmetry was less than @xmath , in
order to reject very asymmetric decays, where one of the BEMC points has
low energy, and to reject a significant part of the low mass background
(this background will be described in the following sections). It turns
out that the asymmetry cut improves the signal to background ratio by
approximately a factor of @xmath .

Finally, for the HighTower-triggered data the requirement is made that
at least one of the reconstructed decay photons alone satisfies this
trigger. This requirement is made to guarantee that the trigger
efficiency is the same in both real and simulated data, as was already
mentioned in section 4.5.3 .

### 5.3 Invariant mass distribution

After cuts, the pairs of BEMC points are turned into 4 @xmath -vectors
by assuming that the decay photons originate from the reconstructed main
vertex. For each @xmath candidate, the pseudorapidity @xmath , the
azimuth @xmath , the transverse momentum @xmath , and the invariant mass
@xmath (Eq. 5.1 ) are calculated. In Figure 5.5

we show the @xmath , @xmath , @xmath , and @xmath distributions of the
@xmath candidates in the @xmath dataset. For the @xmath data these
distributions look similar as those shown for @xmath .

The @xmath distribution shows the decrease of the calorimeter acceptance
at the edges, because there it is likely than one of the decay photons
escapes the calorimeter. The asymmetry is due to the fact that the
calorimeter half-barrel is positioned asymetrically with respect to the
interaction point. The structure seen in the @xmath distribution
reflects the azimuthal dependence of the calorimeter acceptance caused
by failing @xmath modules.

In Figure 5.5 (c) is shown the @xmath distribution of the photon pairs
separately for the MinBias and HighTower datasets. It is seen that the
HighTower triggers significantly increase the rate of pion candidates at
large @xmath . The @xmath -integrated invariant mass distribution in
Figure 5.5 (d) clearly shows the @xmath and @xmath peaks superimposed on
a broad background distribution. This background has a combinatorial and
a low mass component. In the next two sections we will discuss each
background component in detail.

### 5.4 Combinatorial background

The combinatorial background in the invariant mass distribution
originates from pairs of photon clusters that are not produced in a
single @xmath decay. To describe the shape of the combinatorial
background, we use the event mixing technique, where photon clusters
from two different events are combined. To mix only similar event
topologies, the data were subdivided into the mixing classes based on
the vertex position, BEMC multiplicity, and trigger type (MinBias,
HighTower- @xmath 1, and HighTower- @xmath 2). In Figure 5.6

we show the @xmath vertex and multiplicity distributions, and the bins
defining the mixing classes.

Figure 5.7

shows an example of an invariant mass distribution in the @xmath bin,
obtained from the HighTower- @xmath 1 @xmath data, together with the
combinatorial background obtained from the event mixing. The mixed event
background distribution is normalized to the same-event distribution in
the invariant mass region @xmath . In the bottom panel of this figure
the background subtracted distribution is shown.

It can be seen that there is still some residual background in the
interval @xmath , which could be caused by the fact that the mixing
procedure does not fully take into account the correlation structure of
the event. For example, an important source of particle correlations is
the jet structure, which is not present in the sample of mixed events.
In order to preserve jet-induced correlations, the jet axes in both
events are aligned before mixing, as described below.

To determine the @xmath position of the most energetic jet in every
event, the standard STAR jet finding algorithm [ 70 ] was used. The
mixed pion candidates were constructed by taking two photons from
different events, where one of the events was displaced in @xmath and
@xmath by @xmath and @xmath , respectively. Here @xmath and @xmath are
the jet orientations in the two events.

In Figure 5.8

we show a schematic view of two superimposed events, where the jet axes
are aligned. In order to minimize acceptance distortions, the events
were divided into mixing classes in the jet @xmath coordinate. By mixing
only events in the same class, the shift @xmath was kept smaller than
@xmath . Because the calorimeter has a cylindrical shape, the shift in
@xmath does not induce any significant acceptance distortion.

However, a side effect of this procedure is that correlations are
induced if there is no real jet structure, because the jet finding
algorithm will then simply pick the most energetic track in the event.
To reduce possible bias introduced by such correlations, we assume that
a jet structure is associated with large @xmath pions but not with low
@xmath pions. The combinatorial background is then taken as a @xmath
-dependent linear combination of the distributions obtained by random
mixing and jet-aligned mixing,

  -- -------- --
     @xmath   
  -- -------- --

Here @xmath and @xmath are the background spectra from, respectively,
the jet-aligned and random event mixing in a given @xmath bin. The
interpolation coefficient @xmath is given by

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

where the coefficients are @xmath and @xmath . We assign a systematic
uncertainty of @xmath to @xmath , which propagates into a systematic
uncertainty of @xmath on the @xmath and @xmath on the @xmath yields.

In Figure 5.9

we plot the same invariant mass spectrum as that shown in Figure 5.7 ,
with the background estimated by the combined random and jet-aligned
event mixing. The mixed event background is normalized to same-event
distribution in the ranges @xmath and @xmath . By changing the
subtracted background within the normalization uncertainty we obtained a
systematic error on the @xmath and @xmath yields. This error was found
to increase with @xmath from @xmath to @xmath for the @xmath and from
@xmath to @xmath for the @xmath yield.

In the bottom panel of Figure 5.9 the background subtracted spectrum is
plotted, which still shows a residual background component at low
invariant mass. The origin of this background is described in the next
section.

### 5.5 Low-mass background

In Figure 5.1 we have shown a double peaked hit pattern, which will be
reconstructed by the clustering algorithm as two separate adjacent
clusters. However, it is possible that random fluctuations will
accidentally generate such a two peak structure, so that the clustering
algorithm will incorrectly split the cluster. These random fluctuations
enhance the yield of pairs with minimal angular separation and thus
contribute to the lowest di- @xmath photon invariant mass region, as can
be seen in Figure 5.9 . However, at a given small opening angle the
invariant mass increases with increasing energy of the photons, so that
the low mass background spectrum will extend to larger values of @xmath
with increasing @xmath of the parent particle.

The shape of the low mass background was obtained from a simulation as
follows. Single photons were generated with flat distributions in @xmath
, @xmath and @xmath . These photons were tracked through a detailed
description of the STAR geometry with the GEANT program [ 71 ] . A
detailed simulation of the electromagnetic shower development in the
calorimeter was used to generate realistic signals in the towers and the
@xmath . The simulated signals were processed by the same reconstruction
chain as the real data. Photons with more than one reconstructed cluster
were observed, and the invariant mass and @xmath of such cluster pairs
were calculated. The invariant masses were histogrammed with each entry
weighted by the @xmath spectrum of photons in the real data, corrected
for the photon detection efficiency.

In the top plot of Figure 5.10

we show the low mass background distributions in three bins of the
reconstructed pair @xmath . It is seen that the distributions indeed
move to larger invariant masses with increasing @xmath and extend far
into the pion window at large @xmath . For this reason, it is not
possible to estimate this background from a phenomenological fit to the
data, so that we have to rely on the Monte Carlo simulation to subtract
the low mass background.

The second significant source of neutral clusters in the calorimeter are
the neutral hadrons produced in the collision, mostly antineutrons. As a
first attempt to account for the additional low mass background from
these hadrons, simulations of antineutrons were performed in the same
way as photons, and the reconstructed invariant mass distribution was
added according to the realistic proportion @xmath . The ratio @xmath
was taken to be equal to the average value of @xmath from the STAR
measurement [ 28 ] in the @xmath range covered by each of MinBias and
HighTower datasets. In the bottom plot of Figure 5.10 we compare the
simulated low mass background (histogram) to the data.

In Figure 5.11

we show the invariant mass spectra and the low mass background component
(top), together with the final background subtracted spectrum (bottom).

### 5.6 Yield extraction

The complete set of invariant mass spectra for all @xmath bins,
triggers, and datasets are shown in Figures 5.12 – 5.17 . For display
purposes, the spectra are normalized to the bin content in the @xmath
peak. The shaded areas in the figures indicate the @xmath and @xmath
peak regions where the yields are calculated simply by adding up the bin
contents.

The left border of the @xmath peak region was taken to be a linear
function of @xmath , common for all datasets and triggers. It was
adjusted in a way that most of the yield is captured, while the low mass
background and its associated uncertainty is avoided as much as
possible. The right border also linearly increases with @xmath , in
order to cover the asymmetric right tail of the peak. Similarly, the
@xmath peak region is a @xmath -dependent window that captures most of
the signal. For completeness, we give below the parametrization of the
@xmath and @xmath windows:

  -- -------- --
     @xmath   
  -- -------- --

The stability of the yields was determined by varying the vertex
position cut, the energy asymmetry cut, and the yield integration
window. From the observed variations, a point @xmath -to @xmath -point
systematic error of @xmath was assigned to the yields.

## Chapter 6 Invariant yield calculation

The invariant yield of the neutral pions and @xmath mesons per one
minimum bias collision, as a function of the transverse momentum @xmath
, is given by

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

where in the last equation isotropic production in azimuth is assumed.
Using the experimentally measured quantities, the invariant yield is
calculated as

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where:

-   @xmath is the raw yield measured in the bin @xmath ;

-   @xmath is the number of triggers recorded;

-   @xmath is the trigger prescale factor that is unity for the MinBias
    events and larger than unity for the HighTower data. The product
    @xmath then gives the equivalent number of minimum bias events that
    produced the yield @xmath ;

-   @xmath is the vertex finding efficiency in minimum bias events;

-   @xmath is the beam background contamination in minimum bias events;

-   @xmath is the width of the @xmath bin for which the yield is
    calculated;

-   @xmath is the rapidity range of the measurements, in this analysis
    @xmath ;

-   @xmath is the BEMC acceptance and efficiency correction factor;

-   @xmath is a correction for random vetoes;

-   @xmath is the branching ratio of the di- @xmath photon decay
    channel, equal to @xmath for @xmath and @xmath for @xmath [ 66 ] .

Each of these corrections is described in detail in one the following
sections.

### 6.1 Acceptance and efficiency correction

To calculate the acceptance and efficiency correction factor @xmath , a
Monte Carlo simulation of the detector was used, where neutral pions and
their decay photons were tracked through the STAR detector geometry
using GEANT [ 71 ] . The simulated signals were passed through the same
analysis chain as the real data.

The pions were generated in the pseudorapidity region @xmath , which is
sufficiently large to account for edge effects caused by the calorimeter
acceptance limits of @xmath . The azimuth was generated flat in @xmath
.The @xmath distribution was taken to be flat between zero and @xmath ,
which amply covers the measured pion @xmath range of up to @xmath . The
vertex distribution of the generated pions was taken to be Gaussian in
@xmath , with a spread of @xmath and centered at @xmath .

The generated pions were allowed to decay into @xmath . The GEANT
simulation accounts for all interaction of the decay photons with the
detector, such as pair conversion into @xmath and showering in the
calorimeter or in the material in front.

To reproduce a realistic energy resolution of the calorimeter, an
additional smearing has to be applied to the energy deposit generated by
GEANT in the towers. The effect of this can be seen in Figure 6.1 ,

where the simulated @xmath invariant mass peak is shown in comparison to
the @xmath data with and without smearing. An additional spread of
@xmath was used to reproduce the @xmath data and @xmath for the @xmath
data.

To reproduce the @xmath spectrum of pions in the data, each Monte Carlo
event was weighted by a @xmath -dependent function. Such weighting
technique allows to sample the whole @xmath range with good statistical
power, while, at the same time, the bin migration effect caused by the
finite detector energy resolution is reproduced. A next-to @xmath
-leading order QCD calculation [ 72 ] provided the initial weight
function, parametrized as described in Section 7.1 , which was
subsequently adjusted in an iterative procedure.

As mentioned in Section 4.3 , the time dependence of the calorimeter
acceptance is stored in data tables, which are fed into the analysis. In
order to reproduce this time dependence in the Monte Carlo, the
simulated events were assigned time stamps that follow the timeline of
the real data taking. In Figure 6.2

is shown, separately for MinBias and HighTower data, the accumulated
real data statistics per day (histogram), together with the time
distribution of the simulated events (full circles). In this way, the
geometrical calorimeter acceptance (fraction of good towers) was
reproduced in the Monte Carlo with a precision of better than @xmath .

In the real data analysis, we use vertices reconstructed from the TPC
tracks with a sub @xmath -millimiter resolution, as well as vertices
derived from the BBC time of flight measurement with a precision of
about @xmath . To account for this poor resolution, a fraction of the
simulated pions had their point of origin artificially smeared in the
@xmath direction. This fraction was taken to be @xmath of the generated
pions in case of the @xmath MinBias analysis, and taken to be zero for
all the other datasets since no BBC vertex was used in these sets (see
Chapter 4 ).

In Figure 6.3

we show the @xmath and @xmath distributions of the reconstructed Monte
Carlo pions in comparison to the @xmath data. The agreement is
satisfactory, indicating that the calorimeter acceptance is well
reproduced in the simulation. In Figure 6.4

the reconstructed @xmath of simulated pions is compared to that of pion
candidates from the @xmath data. It is seen that the HighTower trigger
threshold effects are reasonably well reproduced.

In Figure 6.5 (a)

the background subtracted invariant mass distribution is shown in the
region @xmath obtained from the @xmath HighTower- @xmath 1 data,
together with the corresponding distribution from the Monte Carlo. In
order to compare the real and simulated invariant mass distributions for
all bins in @xmath and for all datasets, we have estimated the position
and width of the peaks by Gaussian fits in the peak region. In Figure
6.5 (b) are shown the peak positions obtained from the fit to the @xmath
data. It is seen that the peak position shifts towards higher masses
with increasing @xmath . This shift is a manifestation of bin migration
effects that originate from statistical fluctuations in the calorimeter
response. Due to the steeply falling @xmath spectrum the energy
resolution will cause a net migration towards larger @xmath . Since
larger values of @xmath imply larger values of @xmath , the migration
effect will bias the invariant mass peak towards larger values. The good
agreement between the data and Monte Carlo indicates that such
resolution and migration effects are well reproduced.

In Figure 6.5 (c) is shown the comparison of the @xmath peak width in
data and simulation. The peak width is well reproduced in simulation,
which is not surprising since additional smearing was introduced to
improve the comparison between data and Monte Carlo, see Figure 6.1 .

The acceptance and efficiency correction factor was calculated from the
Monte Carlo simulation as the ratio of the raw yield of neutral pions
reconstructed in a @xmath bin, to the number of simulated pions with the
true @xmath in that bin. This was done separately for each trigger,
using the same pion reconstruction cuts as was done in the real data
analysis. In particular, the reconstructed value of pseudorapidity was
required to fall in the range @xmath in both the data and the Monte
Carlo, while in the latter the generated value of @xmath was also
required to fall in this range.

In Figure 6.6 are shown the @xmath and @xmath correction factors for all
datasets and triggers used in this analysis.

The large difference between the MinBias and HighTower correction
factors is caused by the SMD requirement in the HighTower data, while in
the MinBias data we accept all reconstructed BEMC points. The absense of
the SMD information also reduces the @xmath reconstruction efficiency at
@xmath , where the decay photons are separated by less than two towers.
The @xmath reconstruction starts being affected at larger values of
@xmath .

We have checked the effect of the @xmath quality requirement (at least
two adjacent strips in a cluster) on the correction factor for HighTower
triggered data. In Figure 6.7 is shown the correction factor calculated
for the @xmath HighTower- @xmath 1 dataset with (full squares) and
without (crosses) the @xmath quality requirement. It is seen that this
requirement reduces the number of accepted @xmath candidates by about
@xmath . This explains the difference between the HighTower- @xmath
1 and HighTower- @xmath 2 (no @xmath quality cut) correction factors at
large @xmath seen in Figure 6.6 .

To verify a possible dependence of the acceptance correction on the
track multiplicity and thus on the centrality, we have analyzed a sample
of generated neutral pions embedded in real @xmath data. These embedded
data are centrally produced by the STAR offline group and are used by
several analyses in STAR [ 73 ] . No significant centrality dependence
was found, so that same correction factors were applied to the various
centrality classes in the @xmath data.

### 6.2 Corrections for random vetoes

This analysis uses the TPC as a veto detector to reject charged
particles, which introduces false rejection of photon clusters if an
unrelated charged particle happens to hit the calorimeter nearby the
cluster. In Figure 6.8

we plot the distribution of distances between the BEMC point and the
closest charged track in the event. In this plot one easily
distinguishes the peak of real charged particles at small distances,
superimposed on a random component, which shows up as a shoulder at
larger distances. Assuming that the charged tracks are uniformly
distributed in @xmath and @xmath around the BEMC point, it follows that
the radial distribution is given by

  -- -------- -- -------
     @xmath      (6.3)
  -- -------- -- -------

where the parameter @xmath has the meaning of the local track density in
the region where the photon probes it. This parameter is obtained from a
simultaneous fit to the data in all bins of the event multiplicity
@xmath , assuming its linear dependence on the multiplicity @xmath . The
parametrization ( 6.3 ) well describes the random component, as shown by
the full curve in Figure 6.8 . The relative amount of random
coincidences is then obtained by integrating the fitted curve up to the
distance cut and weighting with the multiplicity distribution observed
in each @xmath bin. Separate sets of correction factors were calculated
for the various triggers in the @xmath and @xmath data. The results are
shown in Figure 6.9

as a function of @xmath . We have applied a correction factor of @xmath
to the @xmath datasets and of @xmath to the @xmath datasets. The errors
assigned to these corrections contribute to a @xmath independent
systematic error on the corrected @xmath and @xmath yields.

### 6.3 HighTower trigger scale factors

We have shown in Figure 5.5 (c) the @xmath distribution of @xmath
candidates for the @xmath MinBias, HighTower- @xmath 1, and HighTower-
@xmath 2 data. To match the HighTower spectra to those of the MinBias, a
@xmath -independent scale factor ( @xmath ) was applied. These scale
factors were estimated as the ratio of observed MinBias to HighTower
event rates

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

Here @xmath and @xmath are the numbers of MinBias and HighTower triggers
that pass the event selection cuts described in Chapter 4 . The factors
@xmath and @xmath are the hardware prescale factors adjusted on a
run-by-run basis to accomodate the DAQ bandwidth. In Eq. ( 6.4 ), the
sums are taken over all runs in which both the MinBias and HighTower
triggers were active.

To check the results, the scale factors were also estimated using
another method. Here the HighTower software filter (see section 4.5.3 )
was applied to the minimum bias data. The scale factors were then
obtained as the ratio of the total number of MinBias events to the
number of those that passed the filter. To obtain a more precise
HighTower- @xmath 1/ @xmath HighTower- @xmath 2 relative scale factor,
the software filter was applied to the HighTower- @xmath 1 dataset.

The results from the two methods agree within @xmath for HighTower-
@xmath 1 data and within @xmath for HighTower- @xmath 2 data. This is
taken as the systematic uncertainties on the trigger scale factors.

### 6.4 Vertex finding efficiency

In the @xmath reconstruction it is assumed that the decay photons
originate from the vertex. It is therefore required that each event
entering the analysis has a reconstructed vertex. In the @xmath dataset
this requirement is always fulfilled, because we use the BBC timing
information in case the TPC vertex reconstruction fails (this happens in
about @xmath of the minimum bias events).

In the @xmath HighTower data, the charged track multiplicities are large
enough to always have a reconstructed TPC vertex. However, a TPC vertex
is missing in about @xmath of the minimum bias events and cannot be
recovered from BBC information because the BBC is not included in the
@xmath minimum bias trigger. Minimum bias events without vertex have low
charged track multiplicity, and the contribution from these very soft
events to the @xmath yield above @xmath is assumed to be negligible [ 74
] . Therefore, the correction for vertex inefficiency is applied as a
constant normalization factor to the yield and its uncertainty
contributes to the total normalization uncertainty of the measured cross
sections.

The vertex efficiencies were determined to be @xmath from a full
simulation of @xmath minimum bias events as described in [ 74 ] .
However, this efficiency depends on the centrality, and we assume that
central events are @xmath efficient. Scaling the above efficiency by the
ratio of peripheral to total number of @xmath events, we obtain an
efficiency correction factor of @xmath for the sample of peripheral
events.

Note, that the difference between vertex finding efficiencies in MinBias
and HighTower data is effectively absorbed in the scale factor @xmath
defined in the previous section. The vertex finding efficiency
correction is, therefore, applied to the minimum bias data, as well as
to the scaled HighTower trigger data.

### 6.5 Residual beam background contamination

The beam background contamination in the @xmath minimum bias trigger has
been estimated from an analysis of the RHIC empty bunches to be @xmath [
75 ] . In our analysis, the beam background in @xmath events is rejected
when the energy deposit in the calorimeter is much larger than the total
energy of all charged tracks reconstructed in the TPC, see Section 4.5 .
To estimate the residual beam background in our data, we have analysed a
sample of @xmath minimum bias triggers from unpaired RHIC bunches. These
events were passed through the same analysis cuts and reconstruction
procedure as the real data. We observed that about @xmath of the fake
triggers passed all cuts, and that none of these contained a
reconstructed @xmath . The residual beam background contamination is
thus estimated to be @xmath , which is considered to be negligible.

In the @xmath data the beam background contamination to the minimum bias
trigger rate is also estimated to be negligible due to the BBC
coincidence requirement in the trigger and the cut on the BBC vertex
position.

### 6.6 Bin centering scale factors

To assign a value of @xmath to the yield measured in a @xmath bin, the
procedure from [ 76 ] was applied. Here the measured yield, initially
plotted at the bin centers, is approximated by a power law function of
the form

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

To each bin a momentum @xmath was assigned as calculated from the
equation

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

The function ( 6.5 ) is then re @xmath -fitted taking @xmath as the
abscissa. This procedure was re @xmath -iterated until the values of
@xmath were stable (typically after three iterations). Final fitted
curves are shown in Figures 6.11 and 6.12 .

For convenience of comparing results from the various datasets, the
yields were scaled to the @xmath bin centers by the ratio

  -- -------- -- -------
     @xmath      (6.7)
  -- -------- -- -------

where @xmath is the center of the bin. The statistical and systematic
errors were also scaled by the same factor.

### 6.7 Jacobian correction

All calculations in this analysis were performed in the defined
pesudorapidity region @xmath that corresponds to the rapidity region
@xmath , where the rapidity limit @xmath is well approximated by
pseudorapidity for a particle with momentum much larger than its mass.

The correction was applied to account for the rapidity limit @xmath
being not equal to pseudorapidity @xmath , as shown in Figure 6.10 .

This correction is smaller than @xmath for the @xmath data points at
@xmath , and is negligible for the other data points.

### 6.8 Fully corrected yields

The fully corrected @xmath invariant yields per minimum bias event in
@xmath and @xmath collisions were calculated from Eq. ( 6.2 ) and are
shown in the top plots of Figure 6.11 and Figure 6.12 .

The curves in these figures represent a fit of Eq. ( 6.5 ) to the data.
In the bottom plots are shown the ratios between the data and the fit.
From these plots it is seen that the agreement between the datasets
taken with the three triggers is satisfactory.

For the calculation of the final cross section results and cross section
ratios, the data from three triggers were merged together and only one
data point was chosen in each overlapping @xmath bin. The HighTower-
@xmath 1 points were preferred over MinBias, and HighTower- @xmath
2 over HighTower- @xmath 1, because at high @xmath data samples are
highly correlated, while HighTower datasets typically have smaller
statistical error.

The systematic uncertainty due to the calorimeter calibration was
estimated from

  -- -------- --
     @xmath   
  -- -------- --

where @xmath was taken to be @xmath in the @xmath and @xmath data (see
Section 4.4 ), and where the derivative was calculated from the fitted
function, Eq. ( 6.5 ). This @xmath -dependent systematic uncertainty is,
on average, @xmath in the @xmath data and @xmath in the @xmath data.

All systematic error contributions mentioned in this and in the previous
sections are summarized in Table 6.1 ,

classified into the following categories:

-   A point-by-point systematic uncertainty;

-   B point-by-point @xmath -correlated systematic uncertainty, but
    uncorrelated between datasets;

-   C point-by-point @xmath -correlated systematic uncertainty, also
    correlated between datasets;

-   N normalization uncertainty, uncorrelated between datasets.

## Chapter 7 Results and discussion

### 7.1 Cross section

The invariant differential cross section for @xmath and @xmath
production in inelastic @xmath interactions is given by

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

It has been shown that the singly diffractive (SD) contribution to the
inelastic cross section is negligible at @xmath [ 77 ] , so that we can
assume that the differential inelastic cross section is equal to the
differential NSD cross section in our @xmath range. The total NSD cross
section in @xmath collisions was taken to be @xmath , as described in
Section 4.1 . The total hadronic cross section in @xmath collisions was
taken to be @xmath [ 15 ] .

In tables 7.1 and 7.2 we list the cross sections calculated from Eq. (
7.1 ) for the @xmath and @xmath datasets. In the third column of these
tables are given the statistical errors, while in the remaining columns
the quadratic sum of the systematic errors are given separately for each
group defined in Table 6.1 . In addition to these @xmath -dependent
systematic errors, the quadratic sum of the normalization uncertainties
is found to be @xmath for the @xmath and @xmath for the @xmath data.

To parametrize the @xmath dependence, the measured @xmath cross sections
were fitted to the power law function from Eq. ( 6.5 ), resulting in the
following parameters:

  -- -- --
        
  -- -- --

The measured cross sections for @xmath production in @xmath collisions
are shown in Figure 7.1 ,

compared to the NLO pQCD calculation from Ref. [ 78 ] . Input to this
calculation are the CTEQ6M parton densities [ 31 ] and the KKP
fragmentation functions [ 33 ] . The factorization scale @xmath was set
equal to @xmath and was varied by a factor of two to estimate the scale
uncertainty, as indicated by the shaded band in the bottom plot of
Figure 7.1 that shows the ratio of the measured cross sections to the
QCD prediction.

The errors shown in the plot are the statistical and point @xmath -to
@xmath -point systematic uncertainties added in quadrature, excluding
the uncertainty due to the energy calibration of the calorimeter. This
additional uncertainty is shown by the outer lines around the data
points on the lower plot. The normalization uncertainty is indicated by
shaded band around unity on the right hand side of the plot.

The @xmath cross section measured in @xmath collisions is shown in
Figure 7.2

and compared to the NLO pQCD calculations of [ 72 ] . Here were used the
KKP fragmentation functions, the CTEQ6M parton distributions for
deuterium and the nuclear parton distributions for @xmath [ 79 , 80 , 81
] . The errors shown in the plot are defined in the same way as in
Figure 7.1 for @xmath .

It is seen that the measured @xmath cross section in both the @xmath and
@xmath collisions is well described by the pQCD calculations in the full
@xmath range. The possible excess relative to the theory seen in the
@xmath data at low @xmath may be an indication of the Cronin effect,
which was not included in the pQCD calculations.

In Figure 7.3

we compare the @xmath measurement in @xmath and @xmath with previous
measurements of charged pions by STAR [ 28 ] . For ease of comparison,
the @xmath data points are divided by the @xmath pQCD curves. Note, that
the normalization uncertainty shown by the grey bands in the figure are
largely correlated between the @xmath and the @xmath data points. It is
seen that the neutral and charged pion spectra agree with each other
very well in both @xmath and @xmath datasets.

In Figure 7.4

we compare the present @xmath measurements with the neutral pion results
from PHENIX [ 78 , 20 ] . This comparison indicates a good agreement,
within errors, between the results of the two experiments.

### 7.2 Eta to pion ratio

The @xmath measurement is presented here as the ratio of @xmath to
@xmath invariant yields, which allows many systematic uncertainties to
cancel, see Table 6.1 in Section 6.8 . The @xmath ratios measured in
@xmath and @xmath collisions are listed in Tables 7.3

and 7.4

and are shown in Figure 7.5 .

The error definitions in the tables and in the plot are the same as
described above for the differential cross sections. The present
measurement agrees very well with previous PHENIX results [ 82 ] , as
shown by the open squares in the plot. The full curves in Figure 7.5
show the asymptotic ratio @xmath consistent with the world @xmath
measurements. The constant fit to our data at @xmath gives @xmath and
@xmath . The dotted curves in Figure 7.5 show the prediction based on
empirical @xmath -scaling observation [ 83 ] that the hadron production
cross sections have the same shape as a function of the transverse mass
of the produced particle @xmath . It is seen that the data are
consistent with such scaling behaviour.

### 7.3 Nuclear modification factor

We calculate the @xmath ratio, defined by Eq. ( 1.9 ) and ( 1.10 ), as

  -- -- -- -------
           (7.2)
  -- -- -- -------

where the nucleon-nucleon inelastic cross section @xmath is taken to be
@xmath and @xmath is calculated from the Glauber model, as described in
Section 4.6 .

The nuclear modification factors for @xmath and @xmath are listed in
Tables 7.5

and 7.6

and shown in Figure 7.6 .

Again, the definition of the errors is as given for the differential
cross sections in Section 6.8 . Also shown in Figure 7.6 are the results
of @xmath for charged pions measured by STAR [ 28 ] . A good agreement
between STAR neutral and charged pions is observed.

In Figure 7.7

we compare the @xmath ratio for @xmath (top panel) and @xmath (bottom
panel) to the corresponding PHENIX measurements [ 82 , 84 ] . Our data
agree reasonably well with PHENIX, except at @xmath , where the present
results seem to be systematically higher by about @xmath .

The @xmath ratio for @xmath is listed in Table 7.7 and shown in Figure
7.8 compared to the STAR charged pions [ 28 ] . It is seen that the
agreement between the neutral and charged pion measurements in STAR is
very good. The ratio stays constant at a value consistent with unity
beyond @xmath . The indication of a decrease from the charged pion data
is not supported by this measurement.

### 7.4 Conclusions and outlook

There is a good agreement between the @xmath cross sections in @xmath
and @xmath collisions and nuclear modification factors measured in the
present analysis, and the charged pions previousely measured in STAR at
@xmath . This demonstrates a consistency between the charged and neutral
pion results, in spite of very different analysis methods and detectors
(BEMC versus TPC) used for the measurements. This analysis extends the
@xmath range of identified hadron measurements in STAR up to @xmath .
There is also a good agreement with the corresponding @xmath cross
sections measured by the PHENIX experiment and with those calculated in
NLO pQCD.

From the measurement of the nuclear modification factor @xmath
, no suppression of the @xmath production is seen in the @xmath
collisions compared to @xmath collisions. This is in line with the
observation made elsewhere [ 15 ] that the large suppression seen in the
central @xmath collisions is due to the final state effects.

This analysis presents the first @xmath meson measurement in STAR. The
cross section, presented as an @xmath ratio, is in agreement with the
PHENIX measurement and with the @xmath -scaling assumption.

There are several important and unique features in the present analysis.
First, the technique of estimating the low invariant mass background
using the single photon simulation allows to remove the @xmath
contamination at high @xmath , where this type of background is
indistinguishable from the signal. A possible further improvement would
require a better handle on the @xmath response simulation. Second, the
jet-aligned event mixing method very well reproduces the combinatorial
background in the @xmath and @xmath peak region, so that no residual
background subtraction is necessary. This eliminates the systematic
uncertainty usually related to a residual background parametrization.

It is seen that the experimental uncertainties can be significantly
reduced by improving the calorimeter energy calibration. Better
measurement in the low @xmath region would also require improvements in
the simulation of the @xmath response. Furthermore, calorimeter-based
measurements using data taken since 2006 will benefit from the full
@xmath BEMC acceptance coverage.

In summary, these @xmath and @xmath results provide a baseline
measurement for the future @xmath measurements. These measurements are
interesting to shed light on quark number scaling in particle production
at intermediate @xmath and to study the origin of suppression phenomena
at large @xmath .

## Appendix A BEMC electronics operation

The tower phototubes are powered by Cockroft- @xmath Walton (CW) bases
that are able to keep the high voltage up to a high precision. The bases
are programmed through the serial line from a dedicated computer in the
Control Room. The analog signals from the phototubes are routed to the
tower digitizer crates mounted on the outer side of the magnet.

The tower digitizer crate contains five boards that take @xmath analog
PMT inputs each and digitize it to @xmath bit on each RHIC bunch
crossing, storing in the digital pipeline until a level-0 trigger
arrives. The crate controller board then sends the data packets to the
Tower Data Collector on the platform that feeds it to the DAQ. The crate
controller is also responsible for the slow control communication.

The STAR level-0 trigger uses the BEMC data in the form of trigger
primitives calculated by the tower digitizer boards, instead of the full
tower data. Two trigger primitives are calculated for each tower patch
of @xmath in @xmath ( @xmath towers) using pedestal subtracted tower
@xmath :

-   High Tower - single largest tower signal in a patch

-   Patch Sum - sum of all @xmath towers signal in a patch

In the process of calculating those primitives, the on-board FPGA
algorithm performs the following operations, as illustrated on Figure
A.1 :

1.  Drop the last @xmath bits of the tower @xmath , which becomes a
    @xmath bit signal.

2.  Subtract the stored pedestal @xmath from the @xmath , mask the
    channel out if necessary. The pedestals are calculated in a special
    way as described below.

3.  For the High Tower: convert @xmath bits into @xmath , using one of
    four methods ( @xmath selector), then select the largest value of
    all @xmath towers as output.

4.  For the Patch Sum: drop the last @xmath bits to make it an @xmath
    bit value, then sum those from all @xmath channels into a @xmath bit
    value and transform it into the @xmath bit output value. The
    transformation function has a special shape which is described in
    details later. Internally, it uses a lookup table ( @xmath ); the
    @xmath bit number stored in the @xmath is the output. The PatchSum
    trigger sensitivity is, therefore, fixed:

      -- -- --
            
      -- -- --

Finally, two @xmath bit numbers are sent to the trigger Data Storage and
Manipulation (DSM) boards upon recceiving a trigger signal.

The following @xmath methods are available to select @xmath of @xmath
bits for the High Tower output with varying degree of sensitivity,
almost equivalent to selecting a constant attenuation factor:

1.  Select @xmath lowest bits, combine five highest bits by logical
    “and” into the highest bit of the result. This is the most sensitive
    trigger setting, one HighTower trigger @xmath count is equal to
    @xmath raw tower @xmath counts.

2.  Select @xmath lower bits starting from @xmath , combine four highest
    bits by logical “and” into the highest bit of the result: @xmath

3.  Select @xmath lower bits starting from @xmath , combine three
    highest bits by logical “and” into the highest bit of the result:
    @xmath

4.  Select @xmath lower bits starting from @xmath , combine two highest
    bits by logical “and” into the highest bit of the result. This is
    the least sensitive trigger setting, one HighTower trigger @xmath
    count is equal to @xmath raw tower @xmath counts.

The tower pedestals and masks, the @xmath selectors, and the @xmath
arrays are prepared and uploaded into the on-board registers via the
slow control program.

The tower pedestals are being specially prepared in a way that puts the
@xmath bit High Tower and Patch Sum pedestals at @xmath (not zero) to be
observed during the run. For each tower, the calculation starts from the
exact value of the pedestal, which is measured by issuing the software
triggers to FEE via slow control in a periods between data taking when
there is no beam in the machine. The global “pedestal shift” variable
@xmath gets subtracted from the tower pedestal, in order to center the
pedestal-subtracted tower signal at @xmath . Finally, the pedestal gets
rounded to the nearest multiple of four and two last bits are removed.
The last four bits of the result are used, together with the sign, to
fill the @xmath bit pedestal register @xmath in the FEE.

During 2003 data taking run, the pedestal subtraction scheme was not yet
implemented in FEEs, and the HighTower sensitivity was chosen to be
@xmath . For the 2005 data, the settings were @xmath and @xmath , which
defined @xmath and thus aligned the HighTower and PatchSum readings at
the center of bin @xmath in the absense of tower physics signal.

The @xmath arrays are prepared in a way that gives a linear response to
the patch sum in the range from @xmath to @xmath , to allow diagnosing
the broken cables by observing “all ones” bit pattern @xmath during the
run. With the nominal setting of @xmath , each tower contributes a
pedestal value @xmath to the sum, so the @xmath is constructed from the
following three pieces:

-   Zero , if the sum of @xmath towers is below @xmath

    @xmath

-   Linear rise in response to the sum in the @xmath bit range,
    excluding @xmath

    @xmath

-   Saturation at @xmath

    @xmath

If a tower is masked out of the PatchSum trigger, the @xmath is modified
to accomodate the loss of its pedestal, in this case it starts rising
one count earlier than the nominal @xmath . Therefore, the so @xmath
-called @xmath pedestal ( @xmath ) is equal to the number of masked
towers in a patch.

The @xmath electronics (FEE) board is mounted on the @xmath side of each
module. At the FEE board, the amplified cathode strip signals are
buffered in a switched capacitor array (SCA) before being delivered to
external digitizer boards outside of the STAR magnet.

The signals from the pads of the @xmath are amplified and stored in an
analog pipeline, composed of switched capacitor arrays, to await the
level-0 trigger. Upon level-0 trigger, the @xmath analog signals are
queued with multiplexing ratio of @xmath to the @xmath -bit @xmath
digitizers. @xmath digitized signals are first available in STAR
level-2 trigger processors in @xmath , still well ahead of digital
information from the TPC.

The digitizing electronic boards and crates for preshower detector are
identical to the ones used in the @xmath .