# Acknowledgements

To Stefan Wrobel, for giving me the opportunity to pursue doctoral
studies at Fraunhofer IAIS, and for reading drafts of this manuscript
and suggesting improvements.
To Michael May, for giving me the opportunity to pursue doctoral studies
at the Knowledge Discovery lab.
To Michael Clausen, Peter Köpke, and Andreas Weber, for serving on my
thesis committee.
To Thomas Gärtner, for continual advice, for introducing me to graph
theory and kernel methods, for teaching me how to write technical
papers, for numerous discussions on topics related to machine learning,
and for carefully reading drafts of this manuscript and suggesting
improvements.
To Tamas Horváth and Kristian Kersting, for numerous discussions on
research in general.
To Mario Boley, for discussions on topics related to theoretical
computer science, and for being a wonderful colleague.
To Jörg Kindermann, for providing me with computing resources needed to
run the experiments described in Chapter 5.
To Daniela Börner, Renate Henkeler, and Myriam Jourdan, for helping me
with administrative issues.
To Jens Humrich and Katrin Ullrich, for being wonderful colleagues.
To all the members of the IAIS.KD lab, for creating a conducive
atmosphere that allowed me to work towards a doctorate in machine
learning.
To family and friends.
And lastly, to David Baldacci’s novels, for inspiring me to acknowledge
in this way!

###### Contents

-    Abstract
-    Acknowledgements
-    Notational Conventions
-    1 Introduction
    -    1.1 Structured Prediction
    -    1.2 Why Predict Combinatorial Structures?
    -    1.3 Goals and Contributions
    -    1.4 Thesis Outline
    -    1.5 Bibliographical Notes
-    2 Structured Prediction
    -    2.1 Loss Functions
    -    2.2 Algorithms
    -    2.3 Summary
-    3 Predicting Permutations
    -    3.1 Preliminaries
        -    3.1.1 Related Problems
    -    3.2 Learning Reductions
    -    3.3 Boosting Methods
    -    3.4 Label Ranking SVM
    -    3.5 Structured Prediction
    -    3.6 Online Methods
    -    3.7 Instance-based Learning
    -    3.8 Summary
-    4 Complexity of Learning
    -    4.1 Efficient Learning
    -    4.2 Hardness Results
    -    4.3 Two New Assumptions
        -    4.3.1 The Counting Assumption
        -    4.3.2 The Sampling Assumption
    -    4.4 Summary
-    5 Structured Ridge Regression
    -    5.1 Ridge Regression
    -    5.2 Training Combinatorial Structures
    -    5.3 Scalability Issues
        -    5.3.1 Linear models
        -    5.3.2 Online Optimisation
    -    5.4 Approximate Inference
        -    5.4.1 Approximate Decoding
        -    5.4.2 Approximate Enumeration
    -    5.5 Empirical Results
    -    5.6 Summary
-    6 Probabilistic Structured Prediction
    -    6.1 Probabilistic Models and Exponential Families
    -    6.2 Hardness of Computing the Partition Function
    -    6.3 Approximating the Partition Function Using Uniform Samplers
    -    6.4 Approximating the Partition Function Using Counting
        Formulae
    -    6.5 Approximating the Gradient of the Log Partition Function
    -    6.6 Sampling Techniques
        -    6.6.1 Basics of Markov Chains
        -    6.6.2 A Meta Markov chain
    -    6.7 Summary
-    7 Conclusions
    -    7.1 Learning with Minimal Supervision
    -    7.2 Large-scale Learning
-    A Concentration Inequalities
-    B Kernels and Low-dimensional Mappings
-    C Counting Dicyclic Permutations
-    D Appendix for Chapter 6
    -    D.1 Approximating the Partition Function using Approximate
        Samples
    -    D.2 Approximating the Gradient of the Log Partition Function
        using a Reduction from Counting to Sampling
    -    D.3 Mixing Time Analysis of @xmath using Path Coupling
    -    D.4 Vertices of a hypercube / Path Coupling

] ]

## Notational Conventions

We will follow the notational conventions given below wherever possible.

-   Calligraphic letters ( @xmath ) denote sets (or particular spaces):

    -   @xmath an instance space.

    -   @xmath a label set.

    -   @xmath a Hilbert space.

-   Capital letters ( @xmath ) denote matrices or subsets of some set.

-   Bold letters or numbers denote special matrices or vectors:

    -   @xmath the identity matrix, i.e., a diagonal matrix (of
        appropriate dimension) with all components on the diagonal equal
        to @xmath .

    -   @xmath the zero element of a vector space or a matrix with all
        components equal to @xmath . For the vector spaces @xmath the
        zero element is the vector (of appropriate dimension) with all
        components equal to @xmath . \hide In function spaces we will
        sometimes use @xmath to make explicit that @xmath is a function

    -   @xmath the matrix (in @xmath ) or the vector (in @xmath ) with
        all elements equal to @xmath .

-   Lowercase letters ( @xmath ) denote vectors, numbers, elements of
    some set, or functions:

    -   @xmath the number of training instances.

    -   @xmath a single instance.

    -   @xmath a single label.

-   Lowercase Greek letters @xmath denote real numbers.

-   Bold lowercase Greek letters @xmath denote vectors of real numbers.

-   Symbols:

    -   @xmath : if @xmath then @xmath .

    -   @xmath : @xmath if @xmath .

    -   @xmath : @xmath if and only if @xmath .

    -   @xmath denotes a function from @xmath to @xmath .

    -   @xmath : to clearly distinguish a function @xmath from a
        function value @xmath , we use @xmath for the function and
        @xmath only for the value of the function @xmath applied to
        @xmath . This is somewhat clearer than using @xmath for the
        function as (out of context) @xmath could be read as a number.
        \hide In @xmath -notation we could denote @xmath as @xmath .
        \hide

    -   @xmath denotes the set of elements of @xmath for which the
        function @xmath evaluates to true

    -   @xmath : the set of functions from @xmath to @xmath which are
        defined pointwise, that is, @xmath Alternative notations are
        @xmath and @xmath but we prefer @xmath for clarity

    -   @xmath : As a shorthand for a function that maps every element
        of @xmath to function from @xmath to @xmath that could be
        denoted by @xmath we use the notation @xmath . If arguments
        @xmath and @xmath are given at the same time, the notation
        @xmath could denote the same function

    -   @xmath denotes the transpose of the matrix @xmath .

    -   @xmath denotes the function returning the @xmath norm of a
        vector.

    -   @xmath denotes the function returning the @xmath norm of a
        vector.

    -   @xmath denotes the set @xmath .

-   Other notational conventions and exceptions:

    -   @xmath denotes the component in the @xmath -th row and @xmath
        -th column of matrix @xmath .

    -   @xmath denotes the @xmath -th row vector of matrix @xmath .

    -   @xmath denotes the @xmath -th column vector of matrix @xmath .

    -   @xmath denotes the regularisation parameter.

    -   @xmath a probability distribution.

    -   @xmath the set of all real numbers.

    -   @xmath the set of all natural numbers @xmath .

] ]

## Chapter 1 Introduction

The discipline of machine learning (Mitchell, 2006 ) has come a long way
since Frank Rosenblatt invented the perceptron in 1957. The perceptron
is a linear classifier: it takes a sequence of features as its input,
computes a linear combination of the features and a sequence of weights,
feeds the result into a step function (also known as activation
function), and outputs a binary value ( @xmath or @xmath ). A non-linear
classifier can be designed using multiple layers of computational units
(neurons) with non-linear activation function such as the sigmoid
function resulting in what is called a multi-layered perceptron (MLP) or
a feed-forward network (see Figure 1.1 ). An MLP is a universal function
approximator (Cybenko, 1989 ) , i.e, any feed-forward network with a
single hidden layer comprising a finite number of units can be used to
approximate any function. The weights of an MLP are learned from a given
set of training examples using a procedure called backpropagation
(Rumelhart et al., 1986 ) which typically modifies the weights
iteratively based on the error incurred on the current training example,
with training examples being fed into the network in a sequential
manner. We thus have a machinery that is able to learn from experience
and can be used for prediction thereby mimicking human behaviour (to a
certain extent).

This thesis is concerned with structured prediction — the problem of
predicting multiple outputs with complex internal structure and
dependencies among them . One of the classical structured prediction
problems is temporal pattern recognition with applications in speech and
handwriting recognition. The MLP as described above can be used to
approximate a multi-valued function using multiple units in the output
layer. The downside, however, is that it cannot model dependencies
between the outputs since there are no connections between units within
a layer. Recurrent neural networks or feedback networks (Jordan, 1986 ;
Elman, 1990 ; Hochreiter and Schmidhuber, 1997 ) address this problem by
introducing feedback connections between units (see Figure 1.2 ). The
feedback mechanism can be seen as introducing memory into the network
which makes the network particularly suitable to solve temporal pattern
recognition problems ¹ ¹ 1 Temporal structure in data can also be
modeled using a feed-forward network known as time delay neural network
(Waibel et al., 1989 ) . . Despite the fact that artificial neural
networks can be used to handle temporal sequences, statistical models
like hidden Markov models (HMM) (Rabiner, 1989 ) have enjoyed
significant success in adoption (including commercial use) in speech
recognition problems as they can be trained efficiently using procedures
like the expectation-maximisation algorithm (Dempster et al., 1977 ) .
An HMM is a dynamic Bayesian network that models the joint distribution
of inputs and outputs by placing a Markovian assumption on the input
sequence. \hide The reader is referred to the works of Bilmes99 ; Sha07
; Keshet07 that address some of the limitations of HMMs.

As evidenced above, structured prediction and algorithms pertaining to
it have existed since the mid-80s. Later, with the introduction of
support vector machines (SVM) in the 90s (Boser et al., 1992 ; Cortes
and Vapnik, 1995 ) , there has been a flurry of activity in formulating
and solving every conceivable machine learning problem using tools from
convex optimisation. Unsurprisingly, this has also had an effect on
research in structured prediction resulting in several algorithmic
developments including models/algorithms ² ² 2 Typically, a machine
learning algorithm is a mechanism to estimate the parameters of an
underlying model of the given data. We use the terms model and algorithm
interchangeably. like conditional random fields (Lafferty et al., 2001 )
, max-margin Markov networks (Taskar et al., 2003 , 2005 ) , and
structured SVMs (Tsochantaridis et al., 2005 ) . The contributions of
this thesis follow this line of research in the following sense:

  we address some of the limitations of recent structured prediction
  algorithms when dealing with the specific problem of predicting
  combinatorial structures by proposing new techniques that will aid in
  the design and analysis of novel algorithms for structured prediction.

### 1.1 Structured Prediction

We begin with a non-technical introduction to structured prediction
focusing particularly on applications. A formal algorithmic treatment is
deferred until the next chapter.

We restrict ourselves to supervised learning problems where training
examples are provided in (input, output) pairs. This is in contrast to
other machine learning problems like density estimation and
dimensionality reduction that fall under the category of unsupervised
learning . More formally, let @xmath be the input space. For example, in
OCR applications such as handwriting recognition, this space could
represent the set of all possible digits and letters including
transformations like scaling and rotation. Each element @xmath is
represented as a sequence of features (e.g., image pixels) in @xmath .
Let @xmath be the discrete space ³ ³ 3 If the space is continuous, then
it is a regression problem. We are only concerned with discrete output
spaces. of all possible outcomes. In handwriting recognition, this space
could be the set of possible outcomes, i.e., digits ‘0’–‘9’ and letters
‘a’–‘z’. The goal of a supervised learning algorithm is to learn a
hypothesis (function) @xmath that maps all elements of the input space
to all possible outcomes, i.e., @xmath . Typically, we fix the
hypothesis space (decision trees, neural networks, SVMs), parameterise
it, and learn or estimate these parameters from a given set of training
examples @xmath , which are all drawn independently from an identical
distribution (i.i.d.) @xmath over @xmath . The parameters are estimated
by minimising a pre-defined loss function @xmath — such as the @xmath
loss or the squared loss — on the training examples with the hope of
obtaining a small loss when predicting the outputs of unseen instances.
Often, the hypothesis space is further restricted using a mechanism
known as regularisation so that the learned hypothesis performs well
(w.r.t. the loss function) not only on training but also on unseen
examples. This property of a learning algorithm is called generalisation
.

In structured prediction, the output space is complex in the following
sense: (i) there are dependencies between and internal structure among
the outputs, and (ii) the size of the output space is exponential in the
problem’s input. As a simple example, consider multi-label
classification where the goal is to predict, for a given input example,
a subset of labels from among a set of pre-defined labels of size @xmath
. Clearly, the size of the output space @xmath is @xmath . A reduction
from multi-label to multi-class prediction may not yield good results as
it does not take the correlations between labels into account (McCallum,
1999 ; Schapire and Singer, 2000 ) .

##### Applications

Natural language processing (NLP) has always been a driving force behind
research in structured prediction. Some of the early algorithmic
developments in (discriminative) structured prediction (Collins, 2002 )
were motivated by NLP applications. Part-of-speech tagging is a
classical example where the goal is to mark (tag) the words in a
sentence with their corresponding parts-of-speech. An attempt to perform
this tagging operation by treating the words independently would discard
important contextual information. Linguistic parsing is the process of
inferring the grammatical structure and syntax of a sentence. The output
of this process is a parse tree, i.e, given a sentence of words, the
goal is to output its most likely parse tree (see Figure 1.3 ).

Machine translation, which is the problem of translating text in one
natural language into another, is another application where structured
predition algorithms have been successfully applied (Liang et al., 2006
) . The reader is referred to the works of Taskar ( 2004 ) and Daumé III
( 2006 ) for more details on structured prediction applications in NLP.

Another group of applications is based on graph matching. A matching in
a graph is a set of edges without common vertices. Finding a matching
that contains the largest possible number of edges in bipartite graphs,
also known as maximum bipartite matching , is a fundamental problem in
combinatorial optimisation with applications in computer vision. For
example, finding a correspondence between two images is a graph matching
problem and was recently cast as a structured prediction problem
(Caetano et al., 2009 ) . Segmenting three-dimensional images obtained
from robot range scanners into object categories is an important task
for scene understanding, and was recently solved using a structured
prediction algorithm (Anguelov et al., 2005 ) . Imitation learning is a
learning paradigm where the learner tries to mimic the behaviour of an
expert. In robotics, this type of learning is useful in planning and
structured prediction has been successfully used to solve such problems
(Ratliff et al., 2006 ) . The reader is referred to the works of Taskar
( 2004 ) and Ratliff ( 2009 ) for more details on structured prediction
applications in robotics.

Further applications in bioinformatics and computational biology can be
found in the works of Sonnenburg ( 2008 ) and Frasconi and Passerini (
2008 ) .

### 1.2 Why Predict Combinatorial Structures?

We have already seen that some of the applications described in the
previous section involve predicting combinatorial structures such as
permutations in maximum bipartite matching and trees in linguistic
parsing. Furthermore, several existing, well-studied machine learning
problems can be formulated as predicting combinatorial structures.
Multi-label classification (Schapire and Singer, 1999 , 2000 ; Elisseeff
and Weston, 2001 ; Fürnkranz et al., 2008 ) is a generalisation of
multi-class prediction where the goal is to predict a set of labels that
are relevant for a given input. The combinatorial structure
corresponding to this problem is the set of vertices of a hypercube.
Multi-category hierarchical classification (Cesa-Bianchi et al., 2006 ;
Rousu et al., 2006 ) is the problem of classifying data in a given
taxonomy when predictions associated with multiple and/or partial paths
are allowed. A typical application is taxonomical document
classification where document categories form a taxonomy. The
combinatorial structure corresponding to this problem is the set of
subtrees of a directed, rooted tree.
Label ranking (Dekel et al., 2003 ) is an example of a complex
prediction problem where the goal is to not only predict labels from
among a finite set of pre-defined labels, but also to rank them
according to the nature of the input. A motivating application is again
document categorisation where categories are topics (e.g., sports,
entertainment, politics) within a document collection (e.g., news
articles). It is very likely that a document may belong to multiple
topics, and the goal of the learning algorithm is to order (rank) the
relevant topics above the irrelevant ones for the document in question.
The combinatorial structure corresponding to this problem is the set of
permutations.

##### Real-world Applications

In this thesis, we focus particularly on the problem of predicting
combinatorial structures such as cycles, partially ordered sets,
permutations, and other graph classes. There are several applications
where predicting such structures is important. Consider route prediction
for hybrid vehicles — the more precise the prediction of routes, the
better the optimisation of the charge/discharge schedule, resulting in
significant reduction of energy consumption (Froehlich and Krumm, 2008 )
. Route prediction corresponds to prediction of cycles in a street
network. The input space @xmath would be a set of properties of people,
situations, etc.; the output space @xmath would be the set of cycles
over the places of interest; and @xmath are the known tours of people
@xmath . Route prediction could also find interesting applications in
the design of intelligent personal digital assistants that are smart
enough to recommend alternative routes or additional places to visit.

As another application, consider de novo construction of (personalised)
drugs from the huge space of synthesisable drugs (e.g., a fragment
space) (Mauser and Stahl, 2007 ) — better predictions lead to more
efficient entry of new drugs into the market. The task here is to
predict graphs (molecules) on a fixed set of vertices. State-of-the-art
software systems to support drug design are \hide in silico virtual
screening methods predicting the properties of database compounds. The
set of molecules that can be synthesised is, however, orders of
magnitude larger than what can be processed by these methods. In this
application, @xmath would be some set of properties; @xmath would be a
particular set of graphs over, say, functional groups; and @xmath are
the compounds known to have properties @xmath .

\hide \todo

graph cuts, partitions, maximum weight perfect matching, bipartite
matchings, spanning trees (Taskar, 2004 )

### 1.3 Goals and Contributions

The main goal of this thesis is to design and analyse machine learning
algorithms for predicting combinatorial structures. This problem is not
new and there exists algorithms (Collins, 2002 ; Taskar et al., 2003 ;
Taskar, 2004 ; Taskar et al., 2005 ; Tsochantaridis et al., 2005 ) to
predict structures such as matchings, trees, and graph partitions.
Therefore, as a starting point, we investigate the applicability of
these algorithms for predicting combinatorial structures that are of
interest to us. It turns out that the assumptions made by these
algorithms to ensure efficient learning do not hold for the structures
and applications we have in mind. We elucidate the limitations of
existing structured prediction algorithms by presenting a complexity
theoretic analysis of them. We then introduce two novel assumptions
based on counting and sampling combinatorial structures, and show that
these assumptions hold for several combinatorial structures and complex
prediction problems in machine learning. The consequences of introducing
these two assumptions occupy a major portion of this work and are
briefly described below.

#### A New Algorithm for Structured Prediction

We present an algorithm that can be trained by solving an unconstrained,
polynomially-sized quadratic program. The resulting algorithmic
framework is a generalisation of the classical regularised least squares
regression, also known as ridge regression, for structured prediction.
The framework can be instantiated to solve several machine learning
problems, including multi-label classification, ordinal regression,
hierarchical classification, and label ranking. We then design
approximation algorithms for predicting combinatorial structures. We
also present empirical results on multi-label classification,
hierarchical classification and prediction of directed cycles. Finally,
we address the scalability issues of our algorithm using online
optimisation techniques such as stochastic gradient descent.

#### Analysis of Probabilistic Models for Structured Prediction

Maximum a posteriori estimation with exponential family models is a
cornerstone technique used in the design of discriminative probabilistic
classifiers. One of the main difficulties in using this technique for
structured prediction is the computation of the partition function. The
difficulty again arises from the exponential size of the output space.
We design an algorithm for approximating the partition function and the
gradient of the log partition function with provable guarantees using
classical results from Markov chain Monte Carlo theory (Jerrum et al.,
1986 ; Jerrum and Sinclair, 1996 ; Randall, 2003 ) . We also design a
Markov chain that can be used to sample combinatorial structures from
exponential family distributions, and perform a non-asymptotic analysis
of its mixing time. These results can be applied to solve several
learning problems, including but not limited to multi-label
classification, label ranking, and multi-category hierarchical
classification.

### 1.4 Thesis Outline

The thesis is organised as follows:

    Chapter 2 serves as an introduction to structured prediction. We
    begin with a description of generative and discriminative learning
    paradigms followed by a detailed exposition of several machine
    learning algorithms that exist in the literature for structured
    prediction.

    Chapter 3 is a survey, including original contributions, on
    algorithms for predicting a specific combinatorial structure —
    permutations. \hide This problem has recently attracted a lot of
    attention resulting in several algorithms.

    Chapter 4 analyses existing discriminative structured prediction
    algorithms through the lens of computational complexity theory. We
    study the assumptions made by existing algorithms and identify their
    shortcomings in the context of predicting combinatorial structures.
    The study will consequently motivate the need to introduce two new
    assumptions — the counting and the sampling assumption. We provide
    several examples of combinatorial structures where these assumptions
    hold and also describe their applications in machine learning.

    Chapter 5 proposes a new learning algorithm for predicting
    combinatorial structures using the counting assumption. The
    algorithm is a generalisation of the classical ridge regression for
    structured prediction.

    Chapter 6 analyses probabilistic discriminative models for
    structured prediction using the sampling assumption and some
    classical results from Markov chain Monte Carlo theory.

    Chapter 7 summarises the contributions of this thesis and points to
    directions for future research.

### 1.5 Bibliographical Notes

Parts of the work described in this thesis appear in the following
publications:

-   Thomas Gärtner and Shankar Vembu. On Structured Output Training:
    Hard Cases and an Efficient Alternative. Machine Learning Journal ,
    76(2):227–242, 2009. Special Issue of the European Conference on
    Machine Learning and Principles and Practice of Knowledge Discovery
    in Databases 2009.

-   Shankar Vembu, Thomas Gärtner, and Mario Boley. Probabilistic
    Structured Predictors. In Proceedings of the 25th Conference on
    Uncertainty in Artificial Intelligence , 2009.

-   Shankar Vembu and Thomas Gärtner. Label Ranking Algorithms: A
    Survey. To appear in a book chapter on Preference Learning ,
    Johannes Fürnkranz and Eyke Hüllermeier (Editors),
    Springer-Verlag, 2010.

-   Thomas Gärtner and Shankar Vembu. Learning to Predict Combinatorial
    Structures. In Proceedings of the Workshop on Structured Inputs and
    Structured Outputs at the 22nd Annual Conference on Neural
    Information Processing Systems , 2008.

-   Shankar Vembu and Thomas Gärtner. Training Non-linear Structured
    Prediction Models with Stochastic Gradient Descent. In Proceedings
    of the 6th International Workshop on Mining and Learning with Graphs
    , 2008.

## Chapter 2 Structured Prediction

In supervised learning, the goal is to approximate an unknown target
function @xmath or to estimate the conditional distribution @xmath .
\hide We assume that there is an underlying joint distribution on @xmath
from which a set of @xmath iid training examples @xmath is drawn.There
are essentially two ways to define a learning model and estimate its
parameters. In what is known as generative learning, a model of the
joint distribution @xmath of inputs and outputs is learned, and then
Bayes rule is used to predict outputs by computing the mode of

  -- -------- --
     @xmath   
  -- -------- --

The above is also known as Bayes classifier. In discriminative learning,
the goal is to directly estimate the parameters of the conditional
distribution @xmath . According to Vapnik ( 1995 ) , one should always
solve a problem directly instead of solving a more general problem as an
intermediate step, and herein lies the common justification to model the
conditional instead of the joint distribution. This has led to an
upsurge of interest in the design of discriminative learning algorithms
for structured prediction. Prominent examples include conditional random
fields (Lafferty et al., 2001 ) , structured perceptron (Collins, 2002 )
, max-margin Markov networks (Taskar et al., 2003 , 2005 ) , and support
vector machines (Tsochantaridis et al., 2005 ) . It has now become folk
wisdom to attack a learning problem using discriminative as opposed to
generative methods. An interesting theoretical and empirical study was
performed by Ng and Jordan ( 2001 ) who showed that while discriminative
methods have lower asymptotic error, generative methods approach their
higher asymptotic error much more faster. This means that generative
classifiers outperform their discriminative counterparts when labeled
data is scarce.

A plethora of algorithms have been proposed in recent years for
predicting structured data. The reader is referred to Bakir et al. (
2007 ) for an overview. In this chapter, we discuss several of these
algorithms, in particular those that are relevant to and motivated the
contributions of this thesis. The main goal of a learning algorithm is
to minimise an appropriately chosen risk (loss) function depending on
the problem or application. We describe several loss functions for
binary classification and their extensions to structured prediction. The
algorithmic contributions of this thesis fall under the category of
discriminative learning. We therefore review classical approaches to
discriminatively learning a classifier using perceptron, logistic
regression and support vector machine, and show that many of the
recently proposed structured prediction algorithms are natural
extensions of them.

\hide

We now highlight the differences between generative and discrminative
classifiers using naive Bayes classifier and logistic regression as
examples.

##### Naive Bayes Classifier

We follow Mitchell05 . For the sake of simplicity, consider boolean
inputs and outputs, i.e., @xmath and @xmath . Let @xmath , where @xmath
is an index on the input dimension and @xmath is the number of outputs (
@xmath ), be the set of parameters that are to be estimated. Clearly,
the number of parameters is exponential in the input dimension @xmath
and therefore we need at least as many training examples to obtain
reliable maximum-lilelihood parameter estimates. Therefore, we need to
make further assumptions on @xmath in order to use a Bayes classifier in
practice, and the commonly used assumption is that of conditional
independence of features which is to say that the distribution @xmath
factories into @xmath . Consequently, the number of parameters to be
estimated when modeling @xmath has been reduced dramatically from @xmath
to @xmath . The two sets of parameters are given as

  -- -------- --
     @xmath   
  -- -------- --

These parameters are estimated using maximum likelihood estimation (or
by using maximum a posteriori estimation by defining a prior over the
parameters). For count data, The MLE estimate of the parameters are
given as

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

where @xmath returns the number of elements in @xmath that satisfy
property @xmath .

##### Logistic Regression

Logistic regression is a probabilistic binary classifier. The
probability of class label @xmath given an input @xmath is modeled using
exponential families

  -- -------- --
     @xmath   
  -- -------- --

Given a set of observationstraining sequence @xmath , @xmath , the
parameters @xmath can be estimated using the maximum (log) likelihood
principle

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

### 2.1 Loss Functions

Given an input space @xmath , an output space @xmath , a probability
distribution @xmath over @xmath , a loss function @xmath maps pairs of
outputs to a quantity that is a measure of “discrepancy” between these
pairs, i.e., @xmath . The goal of a machine learning algorithm is to
minimise the true risk

  -- -------- --
     @xmath   
  -- -------- --

Since we do not have any knowledge of the distribution @xmath , it is
not possible to minimise this risk. But given a set of training examples
@xmath drawn independently from @xmath according to @xmath , we can
minimise an approximation to the true risk known as the empirical risk

  -- -------- --
     @xmath   
  -- -------- --

In structured prediction, a joint scoring function on input-output pairs
is considered, i.e., @xmath (with an overload of notation), where the
score is a measure of “affinity” between inputs and outputs.
Analogously, a joint feature representation of inputs and outputs @xmath
is considered. A linear scoring function parameterised by a weight
vector @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

The goal of a structured prediction algorithm is to learn the parameters
@xmath by minimising an appropriate structured loss function. Given a
test example @xmath , the output is predicted as

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

One of the major challenges in designing a structured prediction
algorithm is to solve the above “argmax problem”. The difficulty arises
in non-trivial structured prediction applications due to the exponential
size of the output space. \hide The solution to this problem depends
very much on the loss function used and also on the structure of the
output space.

In the following, we describe commonly used loss functions in
classification and regression and extend them to structured prediction.

##### Zero-One Loss

The zero-one loss is defined as

  -- -------- --
     @xmath   
  -- -------- --

It is non-convex, non-differentiable, and optimising it is a hard
problem in general. Therefore, it is typical to consider approximations
(surrogate losses) to it, for instance, by upper-bounding it with a
convex loss such as the hinge loss ¹ ¹ 1 The hinge loss is
non-differentiable, but learning algorithms like support vector machines
introduce slack variables to mitigate this problem. Support vector
machines will be described in the next section. .

##### Squared Loss

The squared loss is commonly used in regression problems with @xmath and
is defined as

  -- -------- --
     @xmath   
  -- -------- --

The extension of this loss function for structured prediction is
non-trivial due to inter-dependencies among the multiple output
variables. However, these dependencies can be removed by performing
(kernel) principal component analysis (Schölkopf et al., 1998 ) on the
output space and by subsequently learning separate regression models on
each of the independent outputs. The final output can be predicted by
solving a pre-image problem that maps the output in the transformed
space back to the original output space. Thus, a structured prediciton
problem can be reduced to regression. This technique is called kernel
dependency estimation (Weston et al., 2002 ) .

##### Hinge Loss

The hinge loss became popular with the introduction of support vector
machines (Cortes and Vapnik, 1995 ) . For binary classification, it is
defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath is the output of the classifier. The
generalisation of hinge loss for structured prediction (Taskar et al.,
2003 ; Tsochantaridis et al., 2005 ) is defined with respect to a
hypothesis @xmath and a training example @xmath . Let @xmath be a
discrete (possibly non-convex) loss function, such as the Hamming loss
or the zero-one loss, defined on the output space. Consider the loss
function

  -- -------- --
     @xmath   
  -- -------- --

To ensure convexity, the above loss function is upper-bounded by the
hinge loss as

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

##### Logistic Loss

The logistic loss is used in probabilistic models and is a measure of
the negative conditional log-likelihood, i.e., @xmath . For binary
classification (cf. logistic regression) it is defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

For structured prediction, it is defined (again w.r.t. to a hypothesis
@xmath and a training example @xmath ) as

  -- -------- --
     @xmath   
  -- -------- --

##### Exponential Loss

The exponential loss for binary classification is defined as

  -- -------- --
     @xmath   
  -- -------- --

As shown in Figure 2.1 , the exponential loss imposes a heavier penalty
on incorrect predictions than the logistic loss. However, this also
means that the exponential loss is sensitive to label noise. The
exponential loss for structured prediction (Altun et al., 2003 ) is
defined as

  -- -------- --
     @xmath   
  -- -------- --

We will revisit this loss in Chapter 5 .

### 2.2 Algorithms

#### Perceptron

The perceptron (Rosenblatt, 1958 ) , briefly described in the
introductory chapter, is a simple online learning algorithm. It learns a
linear classifier parameterised by a weight vector @xmath and makes
predictions according to @xmath . The algorithm operates in rounds
(iterations). In any given round, the learner makes a prediction @xmath
for the current instance @xmath using the current weight vector. If the
prediction differs from the true label @xmath (revealed to the algorithm
after it has made the prediction), then the weights are updated
according to @xmath . The weights remain unchanged if the learner
predicts correctly. If the data are linearly separable, then the
perceptron makes a finite number of mistakes (Block, 1962 ; Novikoff,
1962 ; Minsky and Papert, 1969 ) and therefore if the algorithm is
presented with the training examples iteratively, it will eventually
converge to the true solution, which is the weight vector that
classifies all the training examples correctly. {theorem} (Block, 1962 ;
Novikoff, 1962 ) Let @xmath be a sequence of training examples with
@xmath for all @xmath . Suppose there exists a unit norm vector @xmath
such that @xmath for all the examples. Then the number of mistakes made
by the perceptron algorithm on this sequence is at most @xmath .

If the date is not separable, then we have the following result due to
Freund and Schapire ( 1999 ) . {theorem} (Freund and Schapire, 1999 )
Let @xmath be a sequence of training examples with @xmath for all @xmath
. Let @xmath be any unit norm weight vector and let @xmath . Define the
deviation of each example as @xmath and define @xmath . Then the number
of mistakes made by the perceptron algorithm on this sequence of
examples is at most

  -- -------- --
     @xmath   
  -- -------- --

The perceptron can be extended to learn non-linear functions using the
kernel trick (Schölkopf and Smola, 2002 ) . The resulting algorithm
called kernel perceptron (Freund and Schapire, 1999 ) learns functions
of the form

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a reproducing kernel function ² ² 2 A reproducing kernel
@xmath is a function with the following two properties: (i) for every
@xmath , the function @xmath is an element of a Hilbert space @xmath ,
(ii) for every @xmath and every function @xmath , the reproducing
property, @xmath , holds. on the inputs and @xmath are the kernel
expansion coefficients. For every reproducing kernel, there exists a
function @xmath (the high-dimensional, possibly infinite, feature space)
such that @xmath . Thus any learning algorithm whose function can be
represented as a linear combination of inner products can use the kernel
trick to avoid explicit computations of these inner products in the
high-dimensional feature space. The kernel perceptron starts by setting
all coefficients @xmath to @xmath . It then operates in rounds, similar
to the perceptron, by repeatedly cycling through the data. If the
algorithm makes a mistake on a particular instance, then the
corresponding coefficient is updated according to @xmath . The algorithm
stops when it classifies all training instances correctly. The
convergence results of the perceptron can be extended to the kernelised
case (Gärtner, 2005 ) . {theorem} (Gärtner, 2005 ) Let @xmath be a
sequence of training examples. Let @xmath be a kernel function such that
@xmath for all @xmath . Let @xmath be a function that classifies all the
training instances correctly. Suppose there exists a margin @xmath such
that @xmath for all @xmath . Then the number of mistakes made by the
kernel perceptron algorithm on this sequence of examples is at most

  -- -------- --
     @xmath   
  -- -------- --

The perceptron algorithm can be extended to predict structured data
(Collins, 2002 ) . Consider linear scoring functions on input-output
pairs @xmath ³ ³ 3 Note the use of joint feature representaion of inputs
and outputs (with a slight abuse of notation). . In every iteration, the
output structure of an instance @xmath is determined by solving the
argmax problem, @xmath , using the current weight vector @xmath . If
this output is different from the true output, i.e., if @xmath , then
the weight vector is updated as follows:

  -- -------- --
     @xmath   
  -- -------- --

The algorithm stops when all the training instances have been predicted
with their correct output structures. Similar to the perceptron,
convergence results can be established for structured prediction in the
separable and inseparable cases (Collins, 2002 ) . Let @xmath be a
function which generates a set of candidate output structures for input
@xmath and let @xmath for a training example @xmath . A training
sequence @xmath is said to be separable with a margin @xmath if there
exists a unit norm vector @xmath such that for all @xmath and for all
@xmath , the following condition holds: @xmath . {theorem} (Collins,
2002 ) Let @xmath be a sequence of training examples which is separable
with margin @xmath . Let @xmath denote a constant that satisfies @xmath
, for all @xmath , and for all @xmath . Then the number of mistakes made
by the perceptron algorithm on this sequence of examples is at most
@xmath . For the inseparable case, we need a few more definitions. For
an @xmath pair, define @xmath and @xmath and define @xmath . Then the
number of mistakes made by the structured perceptron was shown by
Collins ( 2002 ) to be at most

  -- -------- --
     @xmath   
  -- -------- --

#### Logistic Regression

Logistic regression is a probabilistic binary classifier. The
probability distribution of class label @xmath for an input @xmath is
modeled using exponential families

  -- -------- --
     @xmath   
  -- -------- --

Given a set of training examples @xmath , @xmath , the parameters @xmath
can be estimated using the maximum (log) likelihood principle by solving
the following optimisation problem:

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

Observe that the loss function minimised by this model is the logistic
loss. Often, a Bayesian approach is taken to estimate the distribution
of parameters

  -- -------- --
     @xmath   
  -- -------- --

and the mode of this distribution is used as a point estimate of the
parameter vector. By imposing a Gaussian prior on @xmath , @xmath ,
which acts as a regulariser with @xmath being the regularisation
parameter, a point estimate can be computed by maximising the joint
likelihood in @xmath and @xmath :

  -- -------- -- --
     @xmath      
                 
  -- -------- -- --

This technique of parameter estimation is known as maximum a posterior
(MAP) estimation. The optimisation problem is convex in the parameters
@xmath and differentiable, and therefore gradient descent techniques can
be applied to find the global optimum solution.

Logistic regression can be extended for structured prediction with the
class conditional distribution

  -- -------- --
     @xmath   
  -- -------- --

The denominator of the above expresion is known as the partition
function @xmath . Computation of this function is usually intractable
for non-trivial structured output spaces, and depends very much on the
features @xmath and the structure of @xmath .

We now look at a particular structure — sequences — that motivated the
development of one of the popular conditional probabilistic models for
structured prediction — conditional random fields (CRF) (Lafferty
et al., 2001 ) . CRFs offer a viable alternative to HMMs for segmenting
and labeleling sequences. Whereas HMMs model the joint distribution of
input and outputs @xmath , CRFs model the conditional @xmath . A CRF is
defined as follows (Lafferty et al., 2001 ) : {definition} Let @xmath be
a random variable over data sequences, of finite length @xmath , to be
labeled. Let @xmath be a random variable over corresponding label
sequences, where the components @xmath can take values from a finite
alphabet @xmath . Let @xmath be a graph such that the vertex set @xmath
indexes @xmath , i.e., @xmath . Then @xmath is a conditional random
field if, when conditioned on @xmath , the random variables @xmath
satisfy the Markov property

  -- -------- --
     @xmath   
  -- -------- --

wher @xmath means that @xmath is a neighbour of @xmath in G. In the case
of sequences, @xmath is a simple chain (see Figure 2.2 ). The advantage
of CRFs over HMMs is that the probability of transition between labels
can depend on past and future observations (if available) and not only
on the current observation.

The partition function for sequences is computationally tractable using
dynamic programming techniques similar to the forward-backward algorithm
of HMMs (Lafferty et al., 2001 ; Sha and Pereira, 2003 ) . Furthermore,
CRFs are guaranteed to converge to the optimal solution due to the
optimisation problem being convex, whereas HMMs can only guarantee a
locally optimum solution using expectation-maximisation for parameter
estimation.

#### Support Vector Machines

A learning algorithm for binary classification that has attracted
considerable interest during the past decade is the support vector
machine (Boser et al., 1992 ; Cortes and Vapnik, 1995 ) . An SVM learns
a hyperplane that separates positive and negative examples with a large
margin thereby exhibiting good generalisation abilities (Vapnik, 1995 )
. It learns a linear function @xmath and minimises the hinge loss by
optimising

  -- -------- --
     @xmath   
  -- -------- --

The above optimisation problem can be rewritten as

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

where @xmath are the slack variables which correspond to the degree of
misclassification of the training instances. Non-linear functions in the
original feature space can be learned by mapping the features to a
high-dimensional space and by using the kernel trick. It is
computationally more convenient to optimise the Lagrangian dual rather
than the primal optimisation problem \eq eq:svm due to the presence of
box constraints as shown below:

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where @xmath is the kernel matrix with entries @xmath and @xmath is a
diagonal matrix with entries @xmath . The non-linear predictor can be
expressed in the following form due to the representer theorem
(Schölkopf et al., 2001 ) :

  -- -------- --
     @xmath   
  -- -------- --

SVMs can be extended for structured prediction by minimising the
structured hinge loss \eq eq:struct_hinge which was first proposed by
Taskar et al. ( 2003 ) . In structured SVMs (Tsochantaridis et al., 2005
) , the following optimisation problem is considered:

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

Observe that the slack variables have been rescaled. In another
formulation, proposed by Taskar et al. ( 2003 ) , the margin is rescaled
and the resulting optimisation problem is

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

While there are exponential number of constraints in the above
optimisation problems, it is possible to employ the cutting plane method
(Tsochantaridis et al., 2005 ) by designing an algorithm that returns
the most violated constraint in polynomial time. The most violated
constraint w.r.t. a training example @xmath can be computed by solving
the following loss-augmented inference (argmax) problems in the slack
re-scaling \eq eq:StructSVM1 and the margin re-scaling \eq eq:StructSVM2
settings respectively:

  -- -- -- -------
           (2.7)
  -- -- -- -------

and

  -- -- -- -------
           (2.8)
  -- -- -- -------

Tsochantaridis et al. ( 2005 ) showed that a polynomial number of
constraints suffices to solve the optimisation problem \eq eq:StructSVM1
accurately to a desired precision @xmath assuming that the algorithm
that returns the most violated constraint runs in polynomial time. Note
that even if the argmax problem is tractable, solving the loss-augmented
argmax problem requires further assumptions on the loss function such as
it being decomposable over the output variables. An example of such a
loss function on the outputs is the Hamming loss.

The optimisation problem \eq eq:StructSVM2 can be rewritten using a
single max constraint for each training example instead of the
exponentially many in the following way:

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

The above formulation, known as min-max formulation, of the optimisation
problem \eq eq:StructSVM2 was proposed by Taskar et al. ( 2005 ) who
showed that if it is possible to reformulate the loss-augmented
inference problem as a convex optimisation problem in a concise way,
i.e., with a polynomial number of variables and constraints, then this
would result in a joint and concise convex optimisation problem for the
original problem \eq eq:minmax. In cases where it is not possible to
express the inference problem as a concise convex program, Taskar et al.
( 2005 ) showed that it suffices to find a concise certificate of
optimality that guarantees that @xmath . Intuitively, verifying that a
given output is optimal can be easier than finding one.

Structured SVMs can be kernelised by defining a joint kernel function on
inputs and outputs @xmath and by considering the Lagrangian dual of the
optimisation problem \eq eq:StructSVM2:

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

The non-linear scoring function can be expressed as

  -- -------- --
     @xmath   
  -- -------- --

using the representer theorem (Schölkopf et al., 2001 ) .

### 2.3 Summary

The main purpose of this chapter was to review classical discriminative
machine learning algorithms, including perceptron, support vector
machine and logistic regression, and describe how they can be extended
to predict structured data. These extensions resulted in several
recently proposed structured prediction algorithms such as conditional
random fields (Lafferty et al., 2001 ) , max-margin Markov networks
(Taskar et al., 2003 , 2005 ) , and structured SVMs (Tsochantaridis
et al., 2005 ) . In Chapter 4 , we will discuss the assumptions made by
these algorithms in order to ensure efficient learning, point to their
limitations in the context of predicting combinatorial structures, and
propose solutions to circumvent these problems.

## Chapter 3 Predicting Permutations

Binary classification is a well-studied problem in supervised machine
learning. Often, in real-world applications such as object recognition,
document classification etc., we are faced with problems where there is
a need to predict multiple labels. Label ranking is an example of such a
complex prediction problem where the goal is to not only predict labels
from among a finite set of predefined labels, but also to rank them
according to the nature of the input. A motivating application is
document categorisation where categories are topics (e.g.: sports,
entertainment, politics) within a document collection (e.g.: news
articles). It is very likely that a document may belong to multiple
topics, and the goal of the learning algorithm is to order (rank) the
relevant topics above the irrelevant ones for the document in question.

Label ranking is also the problem of predicting a specific combinatorial
structure — permutations. It is an interesting problem as it subsumes
several supervised learning problems such as multi-class, multi-label,
and hierarchical classification (Dekel et al., 2003 ) . This chapter is
a survey of label ranking algorithms. \hide

### 3.1 Preliminaries

We begin with some definitions from order theory, and describe distance
metrics and kernels that will be used in this survey.

A binary relation @xmath on a (finite) set @xmath is a partial order if
@xmath is asymmetric ( @xmath ) and transitive ( @xmath ). The pair
@xmath is then called a partially ordered set (or poset ).

We denote the set @xmath by @xmath and the set of all partial orders
over @xmath by @xmath . Note that every partially ordered set @xmath
defines a directed acyclic graph @xmath . This graph is also called as
preference graph in the label ranking literature.

A partially ordered set @xmath such that @xmath is a totally ordered set
and @xmath is called a total order , a linear order , a strict ranking
(or simply ranking ), or a permutation . \hide We denote the set of all
total orders over @xmath by @xmath . A partial ranking is a total order
with ties.

A partial order @xmath extends a partial order @xmath on the same @xmath
if @xmath . An extension @xmath of a partial order @xmath is a linear
extension if it is totally ordered (i.e., a total order @xmath is a
linear extension of a partial order @xmath if @xmath , @xmath ). A
collection of linear orders @xmath realises a partial order @xmath if
@xmath . We denote this set by @xmath . The dual of a partial order
@xmath is the partial order @xmath with @xmath .

#### Distance Metrics

Spearman’s rank correlation coefficient ( @xmath ) (Spearman, 1904 ) is
a non-parametric measure of correlation between two variables. For a
pair of rankings @xmath and @xmath of length @xmath , it is defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the sum of squared rank distances. The sum of absolute
differences @xmath defines the Spearman’s footrule distance metric .

Kendall tau correlation coefficient ( @xmath ) (Kendall, 1938 ) is a
non-parametric statistic used to measure the degree of correspondence
between two rankings. For a pair of rankings @xmath and @xmath , it is
defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the number of concordant pairs, and @xmath is the number
of discordant pairs in @xmath and @xmath . The number of discordant
pairs defines the Kendall tau distance metric .

#### Kernels

We now define kernels on partial orders and describe their properties.
\hide We now assume a fixed domain @xmath . We have some obvious choises
for kernel functions. All could be made more fancy by changing the
measure but for simplicity we stick to the cardinality for now. For an
application, consider the following example {example} Suppose users rent
movies and later compare them (instead of rating them). If we assume a
consistent rating, we will observe a partial order for each user. To
cluster the users (or to classify them according to, say, their
favourite genre) we would like to have a kernel on the posets.
Position kernel: Define

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a kernel on natural numbers.

-   This function is a kernel and can be computed in time polynomial in
    @xmath .

-   It is injective, in the sense that @xmath , for linear orders but
    not for partial orders.

Edge Kernel: Define

  -- -------- --
     @xmath   
  -- -------- --

-   This function is a kernel and can be computed in time polynomial in
    @xmath . \hide It would be interesting to see if we can compute it
    in time polynomial in @xmath .

-   This kernel is injective in the sense that @xmath .

A downside of this kernel is that @xmath is as similar to @xmath as it
is to @xmath . However, we can overcome this problem easily. Let @xmath
be the dual partial order of @xmath . Define

  -- -------- --
     @xmath   
  -- -------- --

-   This function is a kernel (the feature space has one feature for
    every pair of elements and the value of feature @xmath is @xmath iff
    @xmath , @xmath iff @xmath , and @xmath otherwise).

-   It can be computed in time polynomial in @xmath .

Extension Kernel: Define

  -- -------- --
     @xmath   
  -- -------- --

-   This function is a kernel. \hide It probably works reasonable on
    (sparse) partial orders.

-   It is injective in the sense that @xmath .

-   The kernel cannot be computed in polynomial time as counting linear
    extensions (or, equivalently, computing @xmath ) is @xmath
    P-complete (Brightwell and Winkler, 1992 ) . However, it can
    possibly be approximated as ( @xmath ) the number of linear
    extensions can be approximated (Huber, 2006 ) , and ( @xmath ) the
    set of linear extensions can be enumerated almost uniformly.

-   We have @xmath . We call such partial orders contradicting .

-   For non-contradicting partial orders @xmath define the partial order
    @xmath such that @xmath .

\hide

##### Chain Kernel

-   Consider the labelled graph @xmath obtained from a poset @xmath as
    @xmath

-   Apply the walk kernel (can be computed much faster in this case).

#### Label Ranking — Problem Definition

Let @xmath be the input (instance) space, @xmath be a set of labels, and
@xmath be the output space of all possible partial orders over @xmath .
Let @xmath be a set of training examples. Let @xmath denote the
preference graph corresponding to @xmath , for all @xmath . The goal of
a label ranking algorithm is to learn a mapping @xmath , where @xmath is
chosen from a hypothesis class @xmath , such that a predefined loss
function @xmath is minimised. In general, the mapping @xmath is required
to output a total order, but it is also possible to envisage settings
where the desired output is a partial order. Let @xmath and @xmath
denote positive definite kernels on @xmath and @xmath , respectively.
\hide

#### 3.1.1 Related Problems

We now describe some problems that are related to label ranking. A
comprehensive survey of literature for these problems is beyond the
scope of this chapter. Nevertheless, we refer to, what we believe, are
important (and classical), and possibly also recent contributions.

##### Multi-Label Classification

Multi-label classification (Schapire and Singer, 1999 , 2000 ; Elisseeff
and Weston, 2001 ; Fürnkranz et al., 2008 ) was introduced in Chapter 1
. It is a generalisation of multi-class prediction where the goal is to
predict a set of labels that are relevant for a given input. It is a
special case of multi-label ranking (Brinker and Hüllermeier, 2007 )
where the preference graph is bipartite with directed edges between
relevant and irrelevant labels.

##### Object Ranking

In this setting, the preference information is given on a subset of the
input space and not on the labels. The goal is to learn a ranking
function @xmath such that for any @xmath , @xmath iff @xmath . Thus, a
total order is induced on the input space. This setting has attracted a
lot of attention recently, in particular, in information retrieval
applications. Various approaches to solve this problem have been
proposed in the literature ( Cohen/Schapire/Singer/99 ;
Herbrich/et/al/00 ; Crammer/Singer/01 ; Joachims/02 ; Freund/et/al/03 ;
Burges/et/al/05 ; Fung/etal/05 ; Burges/etal/06 ; Rudin/06 ) . Learning
object ranking functions on structured inputs (graphs) was proposed
recently in ( Agarwal/06 ; Agarwal/Chakrabarti/07 ; Vembu/etal/07 ) . A
survey on object ranking algorithms also appears as a chapter in this
book.

##### Ordinal Regression

Ordinal regression ( Herbrich/et/al/99 ; Herbrich/et/al/00 ;
Chu/Keerthi/05 ; Chu/Ghahramani/05 ) is a form of multi-class prediction
where the labels are defined on an ordinal scale and therefore cannot be
treated independently. It is closely related to the object ranking
problem where the preference information on (a subset of) the input
space is a directed @xmath -partite graph where @xmath is the number of
ordinal values (labels).

### 3.2 Learning Reductions

Learning reductions are an efficient way to solve complex prediction
problems using simple models like (binary) classifiers as primitives.
Such techniques have been applied to solve problems like ranking (Balcan
et al., 2008 ) , regression (Langford and Zadrozny, 2005 ) , and
structured prediction (Daumé III, 2006 ) , just to name a few.

Label ranking can be reduced to binary classification using Kesler’s
construction (Nilsson, 1965 ) . This approach was proposed by Har-Peled
et al. ( 2002a , b ) under the name of constraint classification. The
idea is to construct an expanded example sequence @xmath in which every
example @xmath with its corresponding preference graph @xmath is
embedded in @xmath , with each preference @xmath contributing a single
positive and a single negative example. The Kesler mapping @xmath is
defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a @xmath -dimensional vector whose @xmath th component
is one and the rest are zeros. Let @xmath . The expanded set of examples
is then given by

  -- -------- --
     @xmath   
  -- -------- --

A binary classifier (linear separating hyperplane) trained on this
expanded sequence can be viewed as a sorting function over @xmath linear
functions, each in @xmath . The sorting function is given as @xmath ,
where @xmath is the @xmath -th chunk of the weight vector @xmath , i.e.,
@xmath .

A reduction technique proposed by Fürnkranz ( 2002 ) known as pairwise
classification can be used to reduce the problem of multi-class
prediction to learning binary classifiers. An extension of this
technique known as ranking by pairwise comparison (RPC) was proposed in
(Fürnkranz and Hüllermeier, 2003 ; Hüllermeier et al., 2008 ) to solve
the label ranking problem. The central idea is to learn a binary
classifier for each pair of labels in @xmath resulting in @xmath models.
Every individual model @xmath with @xmath learns a mapping that outputs
1 if @xmath and 0 if @xmath for an example @xmath . Alternatively, one
may also learn a model that maps into the unit interval @xmath instead
of @xmath . The resulting model assigns a valued preference relation
@xmath to every example @xmath :

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

The final ranking is obtained by using a ranking procedure that
basically tries to combine the results of these individual models to
induce a total order on the set of labels. A simple ranking procedure is
to assign a score @xmath to each label @xmath and obtain a final
ordering by sorting these scores. This strategy exhibits desirable
properties like transitivity of pairwise preferences. Furthermore, the
RPC algorithm minimises the sum of squared rank distances and an
approximation of the Kendall tau distance metric under the condition
that the binary models @xmath provide correct probability estimates,
i.e., @xmath .

### 3.3 Boosting Methods

A boosting (Freund and Schapire, 1997 ) algorithm for label ranking was
proposed by Dekel et al. ( 2003 ) . A label ranking function @xmath is
learned such that for any given @xmath , a total order is induced on the
label set by @xmath . The label ranking function is represented as a
linear combination of a set of @xmath base ranking functions, i.e,
@xmath , where @xmath are parameters that are estimated by the boosting
algorithm. We denote the label ranking induced by @xmath for @xmath by
@xmath (with a slight abuse of notation). A graph decomposition
procedure @xmath , which takes a preference graph @xmath for any @xmath
as its input and outputs a set of @xmath subgraphs @xmath , has to be
specified as an input to the learning algorithm. A simple example of a
graph decomposition procedure is to consider every edge @xmath as a
subgraph. Other examples include decomposing the graph into bipartite
directed graph @xmath such that @xmath or @xmath (see Figure 2 in Dekel
et al. ( 2003 ) for an illustration). The generalised loss due to @xmath
w.r.t. @xmath is the fraction of subgraphs in @xmath with which @xmath
disagrees. The generalised loss over all the training instances is
defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a loss function defined on the subgraphs such as the 0-1
loss or the ranking loss (Schapire and Singer, 2000 ) . While minimising
such a discrete, non-convex loss function is NP-hard, it is possible to
minimise an upper bound given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath (resp. @xmath ) is the label corresponding to the initial
(resp. terminal) vertex of any directed edge @xmath . To minimise this
upper bound, Dekel et al. ( 2003 ) proposed to use a boosting-style
algorithm for exponential models (Lebanon and Lafferty, 2001 ; Collins
et al., 2002 ) to estimate the model parameters @xmath and also proved a
bound on the decrease in loss in every iteration of the algorithm.

### 3.4 Label Ranking SVM

Elisseeff and Weston ( 2001 ) proposed a kernel method for multi-label
classification. A straightforward generalisation of this approach
results in a label ranking algorithm. Define a scoring function for
label @xmath and input @xmath as @xmath , where @xmath is a weight
vector corresponding to label @xmath . These scoring functions together
will define the mapping @xmath by a sorting operation, i.e., @xmath .
The ranking loss (Schapire and Singer, 2000 ) w.r.t. to a preference
graph @xmath is defined as @xmath . The following optimisation problem
minimises the ranking loss:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the regularisation parameter that trades-off the balance
of the loss term against the regulariser. \hide

     Primal:

      -- -------- -- -------
         @xmath      (3.1)
      -- -------- -- -------

     Dual: The dual formulation for multi-label classification is given
    in the technical report of (Elisseeff and Weston, 2001 ) including
    an efficient technique to solve the dual optimisation problem.

The algorithm induces a total order on the labels using the individual
scoring functions @xmath . Let us try to reformulate the optimisation
problem using a different approach. Let @xmath be a joint feature map on
the input-output space. We assume that a label @xmath can be decomposed
into parts @xmath and that the joint feature map on these parts @xmath
is well-defined. Consider a simple decomposition of a label @xmath into
its individual edges (parts) @xmath of the corresponding DAG. An
alternative optimisation problem minimising the ranking loss could be
formulated as follows:

     Primal:

      -- -------- -- -------
         @xmath      (3.2)
      -- -------- -- -------

     Dual: todo

For any given instance @xmath , label @xmath is ranked higher than label
@xmath if @xmath . A total order on the labels is thus obtained.

A shortcoming of this approaches is that it is not possible to train the
algorithms using arbitrary loss functions or similarity measures
(kernels) on the output space @xmath . The algorithms also assume that
the preferences (directed edges) are independent of each other thereby
discarding any structural information of the label. The structural
information could be nicely captured by defining an appropriate kernel
on the label space. We then would like to design a label ranking
algorithm that is able to directly operate on these kernels. Later
sections will deal with algorithms of this kind.

Shalev-Shwartz and Singer ( 2006 ) considered the setting where the
training labels take the form of a feedback vector @xmath . The
interpretation is that label @xmath is ranked higher than label @xmath
iff @xmath . The difference @xmath encodes the importance of label
@xmath over label @xmath and this information is also used in the
optimisation problem. The loss function considered in this work is a
generalisation of the hinge-loss for label ranking. For a pair of labels
@xmath , the loss with respect to @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . At the heart of the algorithm lies a decomposition
framework, similar to the one mentioned in the previous section, that
decomposes any given feedback vector into complete bipartite subgraphs,
and losses are defined and aggregated over these subgraphs. This
decomposition framework makes the approach very general, albeit at the
cost of solving a complex optimisation problem. Interestingly, the
quadratic programming formulation for multi-label classification as
proposed by Elisseeff and Weston (Elisseeff and Weston, 2001 ) can be
recovered as a special case of this approach.

\hide

Before proceeding to the next section, we would like to refer to the
label ranking algorithm described in (Shalev-Shwartz and Singer, 2006 )
. The authors formulate the task of ranking labels as a QP optimisation
problem. In their approach, the training label takes the form of a
feedback vector @xmath and the interpretation is that label @xmath is
ranked higher than label @xmath iff @xmath . The difference @xmath is
interpreted as encoding the importance of label @xmath over label @xmath
and this information is also used in the optimisation problem by
defining a loss function which generalises the hinge-loss used in binary
classification. The algorithm learns a mapping of the form @xmath and
outputs a total order on the label set @xmath . At the heart of the
algorithm lies a decomposition framework that decomposes any given
feedback vector into complete bipartite subgraphs, and losses are
defined and aggregated over these subgraphs. This decomposition
framework makes their approach very general, albeit at the cost of
solving a complex optimisation problem, and indeed the QP formulation
for multi-label classification as proposed in (Elisseeff and Weston,
2001 ) can be recovered as a special case. What is unclear though is the
construction of the feedback vector @xmath given an arbitrary partial
order or a weighted directed acyclic graph.

### 3.5 Structured Prediction

The motivation behind using a structured prediction framework to solve
the label ranking problem stems from the added flexibility to use
arbitrary loss functions and kernels, in principle, on the output space.
In this section, we let @xmath to be the space of all total orders of
the label set @xmath .

Recall the optimisation problem of structured SVMs (Tsochantaridis
et al., 2005 ) (cf. Chapter 2 ):

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

Here, @xmath is a loss function on total orders. To handle the
exponential number of constraints in the above optimisation problem, we
need to design a separation oracle (Tsochantaridis et al., 2005 ) that
returns the most violated constraint in polynomial time. The most
violated constraint with respect to a training example @xmath can be
computed using the following optimisation problem:

  -- -- --
        
  -- -- --

The above loss-augmented inference problem and the decoding problem can
be solved using techniques described by Le and Smola ( 2007 ) . Here,
the scoring function @xmath takes a slightly different form. Let @xmath
( @xmath is feature map of inputs) denote the scoring function for an
individual label @xmath parameterised by weight vector @xmath . Now
define the scoring function @xmath for the pair @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

parameterised by the set @xmath of weight vectors, where @xmath is a
decreasing sequence of reals and @xmath denotes the permutation of
@xmath according to @xmath , i.e., @xmath for all @xmath . \hide Note
that the joint scoring function on input-output pairs can be written as
@xmath .The final prediction @xmath is obtained by sorting the scores
@xmath of the individual labels. This is possible due to the
Polya-Littlewood-Hardy inequality (Le and Smola, 2007 ) . The decoding
problem is thus solved. We now turn our attention to designing a
separation oracle. The goal is to find

  -- -- -- -------
           (3.4)
  -- -- -- -------

For certain loss functions that are relevant in information retrieval
applications, Le and Smola ( 2007 ) showed that the above optimisation
problem can be formulated as a linear assignment problem and can be
solved using the Hungarian marriage method (Kuhn-Mungres algorithm) in
@xmath time. For arbitrary loss functions, it may not be feasible to
solve the optimisation problem \eq eq:LinAs efficiently. Note that the
term @xmath in the separation oracle and also in the constraint set of
structured SVM specifies an output dependent margin. Replacing it with a
fixed margin @xmath would greatly simplify the design of separation
oracle since it reduces to a sorting operation as in the decoding
problem. \hide The optimisation problem \eq eq:StructSVMLRank can be
kernelised in its dual form. Let @xmath denote the set of dual
parameters. Let the joint scoring function on input-output pairs be an
element of @xmath with @xmath where @xmath are the RKHS of @xmath
respectively and @xmath denotes the tensor product. Note that the
reproducing kernel of @xmath is then @xmath . The dual optimisation
problem is then given as

  -- -------- --
     @xmath   
  -- -------- --

Since the optimisation problem \eq eq:StructSVMLRank can be kernelised
in its dual form (cf. problem \eq eq:NLStructSVM), it allows us to use
arbitrary kernel functions on the output space such as those described
in Section 3.1 .

\hide

Let us consider a loss function that minimises the disagreement between
permutations, i.e., the ranking loss defined in an earlier section. Let
@xmath be a matrix with elements @xmath . Then, maximising ( 3.4 )
amounts to solving the following linear assignment problem

  -- -- --
        
  -- -- --

and can be done using the Hungarian marriage method (Kuhn-Mungres
algorithm) in @xmath time.

### 3.6 Online Methods

Online classification and regression algorithms like perceptron
(Rosenblatt, 1958 ) typically learn a linear model @xmath parameterised
by a weight vector @xmath . The algorithms operate in rounds
(iterations). In round @xmath , nature provides an instance to the
learner; the learner makes a prediction using the current weight vector
@xmath ; nature reveals the true label @xmath of @xmath ; learner incurs
a loss @xmath and updates its weight vector accordingly. Central to any
online algorithm is the update rule that is designed in such a way so as
to minimise the cumulative loss over all the iterations. In label
ranking scenarios, online algorithms (Crammer and Singer, 2003 , 2005 ;
Shalev-Shwartz and Singer, 2007b ) maintain a set of weight vectors
@xmath , one for every label in @xmath , and the update rule is applied
to each of these vectors.

Online algorithms for label ranking have been analysed using two
different frameworks: passive-aggressive (Crammer et al., 2006 ) and
primal-dual (Shalev-Shwartz and Singer, 2007a ) . Passive-aggressive
algorithms for label ranking (Crammer and Singer, 2005 ) are based on
Bregman divergences and result in multiplicative and additive update
rules (Kivinen and Warmuth, 1997 ) . A Bregman divergence (Bregman, 1967
) is similar to a distance metric, but does not satisfy the triangle
inequality and the symmetry properties. In every iteration @xmath , the
algorithm updates its weights in such a way that it stays close to the
previous iteration’s weight vector w.r.t. the Bregman divergence, and
also minimises the loss on the current input-output @xmath pair. Let
@xmath denote the set of weight vectors in matrix form. The following
optimisation problem is considered:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the Bregman divergence defined via a strictly convex
function @xmath . The choice of the Bregman divergence and the loss
function result in different update rules. Additive and multiplicative
update rules can be derived respectively by considering the following
optimisation problems (Crammer and Singer, 2005 ) :

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the Kullback-Liebler divergence. The loss functions
considered by Crammer and Singer ( 2005 ) is similar to the ones defined
by Dekel et al. ( 2003 ) (see also Section 3.3 ), where a preference
graph is decomposed into subgraphs using a graph decomposition
procedure, and a loss function such as the 0-1 loss or the ranking loss
is defined on every subgraph. The loss incurred by a ranker @xmath for a
graph decomposition procedure @xmath is given as

  -- -------- --
     @xmath   
  -- -------- --

The primal-dual framework (Shalev-Shwartz and Singer, 2007a ) was used
by Shalev-Shwartz and Singer ( 2007b ) resuting in a unifying
algorithmic approach for online label ranking. The loss function
considered in this work is a generalisation of the hinge-loss for label
ranking. The training labels are assumed to be a set of relevant and
irrelevant labels (as in multi-label classification). For a given
instance @xmath , let @xmath denote the set of relevant labels. The
hinge-loss for label ranking w.r.t. an example @xmath at iteration
@xmath is defined as:

  -- -------- --
     @xmath   
  -- -------- --

The central idea behind the analysis is to cast online learning as an
optimisation (minimisation) problem consisting of two terms: the
complexity of the ranking function and the empirical label-ranking loss.
The notion of duality in optimisation theory (Boyd and Vandenberghe,
2004 ) is used to obtain lower bounds on the optimisation problem, which
in turn yields upper bounds on the number of prediction mistakes made by
the algorithm. The reader is referred to (Shalev-Shwartz and Singer,
2007b ) that presents several update rules for label ranking, and these
are also shown to generalise other update rules such as the ones defined
by Crammer and Singer ( 2003 ) .

### 3.7 Instance-based Learning

In instance-based learning, the idea is to predict a label for a given
instance based on local information, i.e., labels of neighbouring
examples. In label ranking, these labels are rankings (partial orders,
total orders, partial rankings) and one has to use aggregation
algorithms (Dwork et al., 2001 ; Fagin et al., 2004 ; Ailon et al., 2008
; Ailon, 2007 ; van Zuylen and Williamson, 2007 ) to combine rankings
from neighbouring examples. Instance-based learning algoritms for label
ranking were proposed recently by Brinker and Hüllermeier ( 2006 , 2007
); Cheng and Hüllermeier ( 2008 ); Cheng et al. ( 2009 ) . Let @xmath
denote a set of @xmath neighbouring rankings for any given instance
@xmath . The goal is to compute a ranking @xmath that is optimal w.r.t.
a loss function @xmath defined on pairs of rankings. More formally, the
following optimisation problem needs to be solved:

  -- -- -- -------
           (3.5)
  -- -- -- -------

This is a very general statement of the problem. Various aggregation
algorithms, which we survey in the sequel, can be used to solve this
optimisation problem depending on the nature of the loss function and
also on the inputs (of the optimisation problem).

##### Aggregating Total Orders

The problem of finding an optimal ranking when the inputs in the
optimisation problem \eq eqn:rankaggr are total orders can be fomulated
as a feedback arc set problem in digraphs (specifically in tournaments)
(Ailon et al., 2008 ) . A tournament is a directed graph @xmath such
that for each pair of vertices @xmath , either @xmath or @xmath . The
minimum feedback arc set (FAS) is the smallest set @xmath such that
@xmath is acyclic. The rank aggregation problem can be seen as special
case of weighted FAS-tournaments; the weight @xmath of an edge @xmath is
the fraction of rankings that rank @xmath before @xmath .

Optimising the Spearman footrule distance metric in the minimisation
problem \eq eqn:rankaggr is equivalent to finding the minimum cost
maximum matching in a bipartite graph with @xmath nodes (Dwork et al.,
2001 ) . A @xmath -factor approximation algorithm with time complexity
@xmath was proposed by Fagin et al. ( 2004 ) . Optimising the Kendall
tau distance metric in \eq eqn:rankaggr is NP-hard (Bartholdi III
et al., 1989 ) and therefore one has to use approximation algorithms
(Ailon et al., 2008 ; van Zuylen and Williamson, 2007 ) to output a
Kemeny optimal ranking. There exists a deterministic, combinatorial
@xmath -approximation algorithm for aggregating total orders (van Zuylen
and Williamson, 2007 ) . The approximation ratio can be improved to
@xmath by using a randomised algorithm (Ailon et al., 2008 ) and to
@xmath by using a derterministic linear programming based algorithm (van
Zuylen and Williamson, 2007 ) . A polynomial time approximation scheme
was proposed by Kenyon-Mathieu and Schudy ( 2007 ) .

\hide

All of these algorithms assume that the weights satisfy the probability
constraints ( @xmath for all pairs @xmath ) and triangle inequality
constraints ( @xmath for all triples @xmath ). The reader is referred to
(van Zuylen and Williamson, 2007 ) for an overview of approximation
algorithms in cases where one of these assumptions fail.

##### Aggregating Partial Rankings

A typical application of this setting is multi-label ranking (Brinker
and Hüllermeier, 2007 ) where the preference graph is bipartite with
directed edges between relevant and irrelevant labels. There exists a
deterministic, combinatorial @xmath -approximation algorithm for
aggregating partial rankings (van Zuylen and Williamson, 2007 ) . The
running time of this algorithm is @xmath . A slightly better
approximation guarantee of @xmath can be obtained by using a
deterministic, linear programming based algorithm (van Zuylen and
Williamson, 2007 ) . These algorithms minimise the Kemeny distance
between the desired output and the individual partial rankings. An exact
method for aggregating partial rankings using (generalised) sum of
squared rank distance metric was proposed by Brinker and Hüllermeier (
2007 ) .

##### Aggregating Partial Orders

In this setting, we allow inupt labels to be partial orders and the
desired output is a total order. To the best of our knowledge, there are
no approximation algorithms to aggregate partial orders, but it is
possible to reduce the problem to that of aggregating total orders as
follows: given a partial order, sample a set (of some fixed cardinality)
of linear extensions (Huber, 2006 ) and use existing approximation
algorithms for aggregating total orders. If the desired output is a
partial order and not a total order, one can consider the following
optimisation problem:

  -- -- --
        
  -- -- --

Under the assumption that @xmath and @xmath , and if the edge kernel
(cf. Section 3.1 ) on partial orders is used, the above optimisation
problem can be approximately solved using the maximum acyclic subgraph
algorithm (Hassin and Rubinstein, 1994 ; McDonald et al., 2005 ) .

### 3.8 Summary

Label ranking is a specific example of the learning problem of
predicting combinatorial structures. The problem has attracted a lot of
interest in recent years as evidenced by the increasing number of
algorithms attempting to solve it. The main purpose of this chapter was
to give an overview of existing literature on label ranking algorithms.
While most of these are specialised algorithms, we have seen in Section
3.5 that the problem can also be solved within the structured prediction
framework using structured SVMs. We will revisit the problem of
predicting permutations — as an example — in the next chapter.

## Chapter 4 Complexity of Learning

In Chapter 2 , we discussed several discriminative structured prediction
algorithms. We will now revisit some of these algorithms, try to get a
deeper understanding of the assumptions they make to ensure efficient
learning, and identify their shortcomings in the context of predicting
combinatorial structures. We will then introduce two new assumptions and
show that they hold for several combinatorial structures. These
assumptions will be used in the design and analysis of structured
prediction algorithms in subsequent chapters.

### 4.1 Efficient Learning

Recall that the structured loss with respect to a hypothesis @xmath and
a training example @xmath is defined as @xmath , where @xmath is a
discrete, non-convex loss function defined on the output space. To
ensure convexity, it is typical to upper-bound this loss by the
structured hinge loss @xmath . Regularised risk minimisation based
approaches (Tsochantaridis et al., 2005 ; Taskar et al., 2003 , 2005 )
aim at solving the optimisation problem @xmath

  -- -- -- -------
           (4.1)
  -- -- -- -------

where @xmath is a regularisation parameter, @xmath is a reproduding
kernel Hilbert space with a corresponding kernel @xmath , and @xmath is
a convex regularisation function such as the squared @xmath norm, @xmath
.

The major issue in solving this optimisation problem is that the number
of constraints grows proportional to @xmath . If the set @xmath is
parameterised by a finite alphabet @xmath , then the number of
constraints is usually exponential in @xmath . To ensure polynomial time
complexity different assumptions need to be made, and depending on the
nature of @xmath different methods are used that iteratively optimise
and add violated constraints. We now describe these assumptions in
decreasing order of strength.

##### Decoding

The strongest assumption is the existence of a polynomial time algorithm
for exact decoding (Tsochantaridis et al., 2005 ) . Decoding refers to
the problem of computing @xmath for a given @xmath .

##### Separation

A weaker assumption made by structured perceptron (Collins, 2002 ) is
the existence of a polynomial time algorithm for separation ¹ ¹ 1
Algorithms for decoding and separation are also referred to as inference
algorithms in the literature. . Separation refers to the problem of
finding for a given scoring function @xmath , @xmath and @xmath , any
@xmath such that @xmath if one exists or prove that none exists
otherwise. A polynomial time algorithm for exact decoding implies a
polynomial time algorithm for separation.

##### Optimality

An even weaker assumption is that optimality is in NP (Taskar et al.,
2005 ) . Optimality refers to the problem of deciding if for a given
scoring function @xmath , @xmath and @xmath , it holds that @xmath . A
polynomial time algorithm for separation implies a polynomial time
algorithm for optimality.

For several combinatorial structures considered in this work, there
exists a short certificate of non-optimality (i.e., non-optimality is in
NP), but there is no short certificate of optimality unless coNP=NP
(complexity classes are illustrated in Figure 4.1 ).

This implies that polynomial time algorithms for exact decoding and
separation do not exist. In other words, none of the existing structured
prediction algorithms (Collins, 2002 ; Taskar et al., 2003 ;
Tsochantaridis et al., 2005 ; Taskar et al., 2005 ) can be trained
efficiently to predict the combinatorial structures that are of interest
to us.

Recently, there have been some attempts to use approximate inference
algorithms for learning structured prediction models. Kulesza and
Pereira ( 2007 ) performed a theoretical analysis of the relationship
between approximate inference and efficient learning. They showed that
learning can fail even when there exists an approximate inference
algorithm with strong approximation guarantees and argued that, to
ensure efficient learning under approximate inferece, it is crucial to
choose compatible inference and learning algorithms. As an example, they
showed that a linear programming based approximate inference algorithm
is compatible with the structured perceptron. Martins et al. ( 2009 )
provided risk bounds for learning with relaxations of integer linear
programming based inference that is common in natural language
applications. Training structured SVMs with approximate inference was
considered in the works of Finley and Joachims ( 2008 ) and Klein et al.
( 2008 ) with mixed (negative and positive) results. The conclusion of
Klein et al. ( 2008 ) ’s empirical study was that structured SVMs
trained with exact inference resulted in improved performance when
compared to those trained with approximate inference. Finley and
Joachims ( 2008 ) considered two classes of approximate inference
algorithms — undergenerating (e.g., greedy methods) and overgenerating
(e.g., relaxation methods like linear programming and graph cuts)
algorithms — and showed that models trained with overgenerating methods
have theoretical and empirical advantages over undergenerating methods.
The aforementioned mixed results motivated us to consider efficient
learning methods for structured prediction that tries to avoid using any
inference algorithm, be it exact or approximate, during training.

### 4.2 Hardness Results

In the following, we will also be interested in the set of hypotheses
potentially occurring as solutions to the optimisation problem \eq
eq:cvx and denote it as @xmath .

First, we show that the assumptions described in the previous section do
not hold for several relevant output sets. In particular, we show that
they do not hold if the non-optimality decision problem for a given
@xmath is NP-hard. This decision problem is the complement of the
optimality problem and is defined as deciding if for a given @xmath ,
@xmath and @xmath , a @xmath exists such that @xmath . Second, we show
that for the specific case of undirected cycles (route prediction),
non-optimality is indeed NP-complete. Our hardness result gains further
significance as we can also show that this case can indeed occur for a
specific set of observations. Third, we turn to a class of problems for
which the output forms particular set systems, show that in this case
the assumptions decoding , separation , and optimality are not contained
in P, and note that decoding is often hard as it corresponds to edge
deletion problems (Yannakakis, 1978 ) .

#### Representation in Output Space

As described in the previous section, state-of-the-art structured output
learning algorithms assume (at least) that deciding if an output
structure with higher score than a given one exists is in NP. With the
definitions given above, we formally define @xmath Optimality as:

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the kernel function on the input space @xmath . We assume
that a polynomial time computable map @xmath is defined and will refer
to the inner product under this map by @xmath . For the joint kernel of
inputs and outputs, we consider the tensor product kernel @xmath , which
has found applications in many important domains (Jacob and Vert, 2008 ;
Erhan et al., 2006 ) . We refer to the Hilbert spaces corresponding to
@xmath as @xmath , respectively.

The strong representer theorem (Schölkopf et al., 2001 ) holds for all
minimisers @xmath of the optimisation problem \eq eq:cvx. It shows that
@xmath . That is,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

This motivates us to first consider @xmath Optimality , defined as:

  -- -------- --
     @xmath   
  -- -------- --

To show that @xmath Optimality is not in NP, it suffices to show that
@xmath Non-Optimality is NP-hard. Formally, we define @xmath
Non-Optimality as:

  -- -------- --
     @xmath   
  -- -------- --

and correspondingly @xmath Non-Optimality .

#### Route Prediction — Hardness of Finding Cyclic Permutations

We now consider the route prediction problem introduced in Section 1.2
as an instance for which @xmath Non-Optimality is NP-complete and for
which thus the assumptions made by state-of-the-art structured
prediction algorithms do not hold.

In the route prediction problem, each @xmath represents a sequence of
features such as an individual person, a day, and a time of the day;
@xmath is a set containing points of interest; @xmath is the set of
cyclic permutations of subsets of @xmath (we are interested in cyclic
permutations as we assume that each individual starts and ends each
route in the same place, e.g., his/her home); and @xmath . We represent
a cyclic permutation by the set of all neighbours. For instance, the
sequences @xmath are equivalent and we use @xmath to represent them.
Furthermore, we define @xmath if @xmath , i.e., @xmath and @xmath are
neighbours in @xmath , and @xmath otherwise.

\hide {proposition}

With all constants and functions defined as above, @xmath Non-Optimality
is not in P unless P=NP. {proof} Repeated calls to a polynomial time
@xmath Non-Optimality subroutine could be used to solve the Hamiltonian
cycle problem by the following algorithm:

0: Graph @xmath

0: G has a Hamiltonian cycle

1: Let @xmath be any cyclic permutation of @xmath

2: Let @xmath be a function mapping a graph to its adjacency matrix

3: while @xmath Non-Optimality @xmath do

4: Let @xmath

5: for @xmath do

6: if @xmath Non-Optimality @xmath then

7: @xmath

8: end if

9: end for

10: Let @xmath be any cyclic permutation of @xmath containing @xmath

11: end while

Here, for a graph @xmath we denote @xmath . To see that this algorithm
provides a polynomial time Turing reduction, it is sufficient to observe
that @xmath .

We will now prove the stronger lemma: {lemma} With all constants and
functions defined as above, @xmath Non-Optimality is NP-complete.
{proof} The proof is given by a Karp reduction of the Hamiltonian path
problem. Let @xmath be an arbitrary graph. Wlog, assume that @xmath .
Construct a weighted graph @xmath on the vertices of @xmath with
adjacency matrix

  -- -------- --
     @xmath   
  -- -------- --

Now, @xmath Non-Optimality ( @xmath ) holds iff there is a cyclic
permutation on @xmath that has an intersection with @xmath of size
@xmath . The intersection of any graph with a graph is a set of paths in
@xmath . As the total number of edges in the intersection is @xmath ,
the intersection is a path in @xmath . A path in @xmath with @xmath
edges is a Hamiltonian path.

{theorem}

With all constants and functions defined as above, @xmath Non-Optimality
is NP-complete. {proof} We consider @xmath for @xmath and construct
training data and a function that satisfies all but one constraint if
and only if the graph has a Hamiltonian cycle.

For an arbitrary graph @xmath let @xmath be defined as above and let
@xmath with @xmath . With @xmath , @xmath , and @xmath we choose the
training data @xmath as

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , let @xmath for @xmath and @xmath otherwise. For @xmath ,
let @xmath for @xmath and @xmath otherwise. For @xmath , we then have

  -- -------- --
     @xmath   
  -- -------- --

and @xmath . Thus there are no violated constraints for @xmath and
@xmath is indeed optimal on this part of the data. Furthermore, for all
@xmath :

  -- -------- --
     @xmath   
  -- -------- --

Together with Lemma 4.2 this gives the desired result.

\hide

#### Partially Ordered Sets \todo

check Consider @xmath ; @xmath such that all @xmath form a poset (an
acyclic directed graph closed under transitivity) @xmath ; as well as
@xmath with @xmath if @xmath , @xmath if @xmath , and @xmath otherwise.

{theorem}

The optimality problem for posets is coNP-complete. {proof} We will show
that the problem of finding the minimum feedback arc set in tournaments
(FAST) ( Charbit/etal/07 ) , which is known to be NP-hard, is reducible
to the non-optimality problem for posets. We know that the
non-optimality problem is in NP due to the existence of a short
certificate. This would then imply that the problem is NP-complete.

Consider directed graphs @xmath with unit edge weights. Suppose the
non-optimality problem is in P. The weight of the optimal (maximal)
poset can be found by testing for non-optimality for all weights @xmath
and counting the number ( @xmath ) of ’yes’ instances. The optimal poset
can then be found by deleting edges in succession from the given
directed graph and testing every resulting graph for non-optimality with
weight @xmath . This procedure can also be used to solve FAST in
polynomial time. This completes the reduction.

#### Connections to other Assumptions

Recall the assumptions made by existing structured prediction
algorithms, namely, decoding, separation, and optimality (cf. Section
4.1 ). Define @xmath Decoding as

  -- -------- --
     @xmath   
  -- -------- --

@xmath Separation as

  -- -------- --
     @xmath   
  -- -------- --

and @xmath Optimality as

  -- -------- --
     @xmath   
  -- -------- --

{proposition}

1.  @xmath Non-Optimality is NP-complete.

2.  @xmath Optimality is coNP-complete.

3.  @xmath Separation is coNP-hard.

4.  @xmath Decoding is coNP-hard.

{proof}

The result follows immediately by observing that (1) optimality is the
complement of non-optimality, (2) any separation oracle can be used to
decide optimality, and (3) any algorithm for decoding can be used as a
separation oracle.

{corollary}

Unless coNP=NP, @xmath Optimality is not contained in NP. Unless P=coNP,
there is no polynomial time algorithm for

1.  @xmath Separation , and

2.  @xmath Decoding .

{proof}

The result follows immediately from Theorem 4.2 and Proposition 4.2 .

Lastly, the decision problem @xmath Optimal-Value

  -- -------- --
     @xmath   
  -- -------- --

is also of interest. Following the proof that Exact-TSP ² ² 2 Exact-TSP
is the problem of finding a Hamiltonian cycle of a given lenth. is
@xmath -complete (Papadimitriou, 1994 ) ³ ³ 3 @xmath , introduced by
Papadimitriou and Yannakakis ( 1984 ) , is the class of languages that
are the intersection of a language in NP and a language in coNP, i.e.,
@xmath . Note that this is not the same as @xmath , and that @xmath . ,
it can be shown that @xmath Optimal-Value is @xmath -complete. The
significance of this result is that optimal-value can be reduced to
separation and decoding, providing even stronger evidence that neither
of these problems can be solved in polynomial time.

#### Set Systems

We now consider set systems @xmath with @xmath where @xmath , and let
@xmath be the indicator function @xmath if @xmath and @xmath otherwise.
An independence system is a set system for which @xmath implies @xmath ,
i.e., for which @xmath is hereditary. A particularly interesting family
of independence systems corresponds to hereditary properties on the
edges of a graph @xmath where @xmath . \hide A property is called
hereditary if @xmath implies @xmath , for all @xmath , i.e., hereditary
properties correspond to independence systems on the edges of the graph.
In this case, decoding corresponds to minimum edge deletion problems,
which are NP-hard for several important hereditary graph properties like
planar, outerplanar, bipartite graph, and many more (Yannakakis, 1978 )
. \hide As closure under subsets implies closure under intersection, the
algorithm given in the proof of Proposition 4.2 is sufficient to show
that optimality, separation, and decoding are not contained in P (unless
P=NP) for all NP-hard hereditary properties on the edges of a graph.

{proposition}

With all constants and functions defined as above, @xmath Non-Optimality
is in @xmath if and only if the minimum edge deletion problem
corresponding to the hereditary property @xmath is in @xmath .

0: Graph @xmath

0: A maximum subgraph of @xmath that satisfies @xmath

1: Let @xmath

2: while @xmath Non-Optimality @xmath do

3: Let @xmath ;

4: for @xmath do

5: if @xmath Non-Optimality @xmath then

6: @xmath

7: end if

8: end for

9: for @xmath do

10: if @xmath Non-Optimality @xmath then

11: @xmath

12: end if

13: end for

14: Let @xmath

15: end while

Algorithm 1 An algorithm for solving the minimum edge deletion problem
for some hereditary property @xmath given a (non-)optimality oracle.

{proof}

( @xmath ) For the graph @xmath , let @xmath be a function mapping a set
@xmath to the adjacency matrix of @xmath . We give a Turing reduction
from the minimum edge deletion problem to non-optimality in Algorithm 1
. To see this, it is sufficient to observe that: (i) the while loop
continues as long as @xmath contains a subset that is larger than @xmath
and satisfies @xmath ; as @xmath increases in each iteration by one, the
number of iterations is bounded from above by @xmath , (ii) the first
for loop (lines 4–8) removes edges not contained in @xmath from @xmath
while ensuring that @xmath contains a subset that is larger than @xmath
and satisfies @xmath ; therefore @xmath ; and because of hereditary
@xmath , and (iii) the second for loop (lines 9–13) removes edges of
@xmath from @xmath while ensuring that @xmath still contains a subset
that is larger than @xmath and satisfies @xmath . As the removal of
these edges from the adjacency matrix @xmath will shrink @xmath , the
weight of the edges in @xmath is reduced accordingly.

( @xmath ) It remains to observe that to decide whether @xmath is
non-optimal, it suffices to find a maximum structure and compare its
size with @xmath .

For properties @xmath that are non-hereditary, two modifications of the
algorithm arise: (a) The constant in line @xmath may be larger than 1;
its precise value can be found by first increasing it from @xmath until
@xmath Non-Optimality @xmath . (b) The structure @xmath itself is not
necessarily contained in @xmath ; a polynomial time subroutine for
finding a @xmath such that @xmath is needed additionally.

### 4.3 Two New Assumptions

We have thus far discussed the assumptions mabe by existing structured
prediction algorithms to ensure efficient learning. We have also seen,
in the form of hardness results, that these assumptions do not hold for
several combinatorial structures thereby exposing the limitations of
existing algorithms to learn efficiently to predict combinatorial
structures. We are now ready to introduce two new assumptions, and
provide several examples of combinatorial structures and applications in
machine learning where these assumptions hold. These assumptions are
based on counting and sampling combinatorial structures and will be
elucidated in the following sections.

#### 4.3.1 The Counting Assumption

The major difficulty in structured output learning is to handle the
exponentially many constraints in the optimisation problem \eq eq:cvx.
While successively adding violated constraints is feasible under several
assumptions, in the previous section we discussed cases like route
prediction where none of these assumptions hold.

In the following, we will first show, considering again cyclic
permutations as a concrete example, that counting the number of
super-structures can be feasible even if there are exponentially many of
them and even if the assumptions of decoding, separation, and optimality
do not hold. The counting assumption is stated as follows: {assumption}
Denote by @xmath the finite dimensional embedding of the output space
@xmath . It is possible to efficiently compute the quantities

  -- -------- --
     @xmath   
  -- -------- --

\hide

Following this insight, we aim at a structured output learning algorithm
that is efficient whenever super-structure counting is feasible. Indeed,
whenever we are minimising a quadratic upper bound on the loss, we can
derive an unconstrained optimisation problem accessing the output set
only by means of @xmath , @xmath and @xmath . To solve this optimisation
problem, we give the gradient as well as the Hessian-vector
multiplication. \hide We now present several combinatorial structures
for which the above assumption holds.

##### Route Prediction — Counting Cyclic Permutations

For a given alphabet @xmath , we are now interested in computing @xmath
, the number of cyclic permutations of subsets of @xmath . For a subset
of size @xmath there are @xmath permutations of which @xmath represent
the same cyclic permutation. That is, there are @xmath cyclic
permutations of each subset of size @xmath , and for an alphabet of size
@xmath there are

  -- -------- --
     @xmath   
  -- -------- --

different cyclic permutations of subsets of @xmath .

Computing @xmath is simple. For each pair of neighbours, there are
@xmath remaining vertices, and for each subset of these of size @xmath ,
there are @xmath permutations, of which @xmath represent the same cyclic
permutation:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the vector of all ones.

It remains to compute @xmath . Each element of this matrix is computed
as

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , we have

  -- -- --
        
  -- -- --

and for @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

\hide

Having introduced an algorithm that can be trained whenever
super-structure counting is feasible, we now show that this assumption
not only holds for route prediction but also for a large class of other
output sets. For some of these output sets efficient decoding is
trivial, whereas for others decoding is infeasible, showing that our
super-structure counting assumption is orthogonal to the assumptions
made in the structured prediction literature.

##### Simple Set System Cases

We first consider the general @xmath -label prediction problem where
@xmath with @xmath defined as @xmath if @xmath and @xmath otherwise.
This setting generalises the usual multi-class ( @xmath ) and
multi-label (by summing over all @xmath ) settings. For general @xmath
we have (with @xmath ),

  -- -------- --
     @xmath   
  -- -------- --

As special cases we have for multi-class ( @xmath ) that @xmath , and
for multi-label ( @xmath ) that @xmath .

For both of these simple cases, exact decoding is very simple. For a
given (test) instance @xmath , let @xmath with @xmath . For the
multi-class case decoding is @xmath . For the multi-label case it is
@xmath . Hence, we could in this case also apply separation based
learning algorithms.

\hide

Note, however, that this simple treatment of multi-label prediction has
several disadvantages. To see this, consider the simple example of
predicting what (mixed) drinks a certain person likes. Say, one likes
good scotch and good wine. In reality this hardly implies that the same
person likes mixing good wine with good scotch. This, more sophisticated
approach is tackled later in Section 4.3.1 as clique prediction .

##### Simple Non-Set System Cases

We now consider poset regression. Let @xmath , @xmath and let @xmath be
a poset. With @xmath if @xmath and @xmath otherwise, we have @xmath and
@xmath . As a special case, we have the ordinal regression problem where
@xmath (with @xmath ordinal values), @xmath with @xmath if @xmath and
@xmath otherwise. In this case @xmath and @xmath . Note that
hierarchical classification is also a special case of poset regression
where @xmath forms a directed tree. In both cases, decoding can be done
by exhaustively testing only @xmath alternatives.

##### Permutations

Let @xmath be the set of permutations of @xmath and let @xmath . Then
@xmath and with @xmath if @xmath , @xmath if @xmath , and @xmath
otherwise, we have @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

The assumptions made in the literature are unlikely to hold in this case
as the ‘without any cycle of length @xmath ’ edge deletion problem is
NP-complete (Yannakakis, 1978 ) for any fixed @xmath .

##### Posets and Partial Tournaments

Consider @xmath ; @xmath such that all @xmath form a poset (an acyclic
directed graph closed under transitivity) @xmath ; as well as @xmath
with @xmath if @xmath , @xmath if @xmath , and @xmath otherwise. To the
best of our knowledge, no exact way to compute @xmath is known \hide but
it is strongly related to so called ‘correlation inequalities’ that
could be used to bound and hence approximate @xmath . However, we can
relax @xmath to @xmath such that all @xmath form a partial tournament (a
directed graph with no cycles of length two). With @xmath we have @xmath
, @xmath , and

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

The assumptions made in the literature are unlikely to hold in this case
as the ‘transitive digraph’ edge deletion problem is NP-complete
(Yannakakis, 1978 ) .

##### Graph Prediction

Consider graphs on a fixed set of vertices, that is @xmath and a
property @xmath such as acyclicity, treewidth bounded by a given
constant, planarity, outerplanarity bounded by a constant, clique etc.
Let @xmath and @xmath be defined as in Section 4.2 . For properties like
clique, we can compute @xmath :

  -- -------- --
     @xmath   
  -- -------- --

For other properties, no way to compute @xmath might be known but we can
always relax @xmath to @xmath . We then have @xmath and @xmath .

#### 4.3.2 The Sampling Assumption

The sampling assumption pertains to discriminative probabilistic models
for structured prediction. The sampling assumption is stated as follows:
{assumption} It is possible to sample efficiently from the output space
@xmath exactly and uniformly at random. We now describe three
combinatorial structures with their corresponding application settings
in machine learning. For each of these structures, we show how to obtain
exact samples uniformly at random. \hide We describe three combinatorial
structures with their corresponding application settings in machine
learning. For each of these structures, we show how to obtain exact
samples uniformly at random. Together with the ‘meta’ approach presented
in the previous section, it is then possible to obtain exact samples of
these structures from exponential family distributions considered in
this work. Therefore, we have all the necessary ingredients to
approximate the partition function.

##### Vertices of a hypercube

The set of vertices of a hypercube is used as the output space in
multi-label classification problems (see, for example, Elisseeff and
Weston ( 2001 ) ). An exact sample can be obtained uniformly at random
by generating a sequence (of length @xmath , the number of labels) of
bits where each bit is determined by tossing an unbiased coin.

##### Permutations

The set of permutations is used as the output space in label ranking
problems (see, for example, Dekel et al. ( 2003 ) ). An exact sample can
be obtained uniformly at random by generating a sequence (of length
@xmath , the number of labels) of integers where each integer is sampled
uniformly from the set @xmath without replacement.

##### Subtrees of a tree

Let @xmath denote a directed, rooted tree with root @xmath . Let @xmath
denote a subtree of @xmath rooted at @xmath . Sampling such rooted
subtrees from a rooted tree finds applications in multi-category
hierarchical classification problems as considered by Cesa-Bianchi
et al. ( 2006 ) and Rousu et al. ( 2006 ) . We now present a technique
to generate exact samples of subtrees uniformly at random. The technique
comprises two steps. First, we show how to count the number of subtrees
in a tree. Next, we show how to use this counting procedure to sample
subtrees uniformly at random. The second step is accomplished along the
lines of a well-known reduction from uniform sampling to
exact/approximate counting (Jerrum et al., 1986 ) .

First, we consider the counting problem. Let @xmath be a vertex of
@xmath and denote its set of children by @xmath . Let @xmath denote the
number of subtrees rooted at @xmath . Now, @xmath can be computed by
using the following recursion:

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

Next, we consider the sampling problem. Note that any subtree can be
represented by a @xmath -dimensional vector in @xmath , where @xmath . A
naïve approach to generate samples uniformly at random would be the
following: generate a sequence of @xmath bits where each bit is
determined by tossing an unbiased coin; accept this sequence if it is a
subtree (which can be tested in polynomial time). Clearly, this sample
has been generated uniformly at random from the set of all subtrees.
Unfortunately, this naïve approach will fail if the number of
acceptances (subtrees) form only a small fraction of the total number of
sequences which is @xmath , because the probability that we encounter a
subtree may be very small. This problem can be rectified by a reduction
from sampling to counting, which we describe in the sequel.

We will use the term prefix to denote a subtree @xmath included by
another subtree @xmath , both rooted at @xmath . Let @xmath denote the
set of leaves of @xmath . We will reuse the term prefix to also denote
the corresponding bit representation of the induced subtree @xmath . The
number of subtrees with @xmath as prefix can be computed using the
recursive formula \eq eq:tree_count and is given (with a slight abuse of
notation) by @xmath . Now, we can generate a sequence of @xmath bits
where each bit @xmath with a prefix @xmath is determined by tossing a
biased coin with success probability @xmath and is accepted only if it
forms a tree with its prefix. The resulting sequence is an exact sample
drawn uniformly at random from the set of all subtrees.

### 4.4 Summary

The purpose of this chapter was to highlight the limitations of existing
structured prediction algorithms in the context of predicting
combinatorial structures. We have shown how even the weakest assumption
made by these algorithms — Optimality — is coNP-complete for several
classes of combinatorial structures. We then introduced two new
assumptions based on counting and sampling combinatorial structures. We
have also seen how these assumptions hold for several combinatorial
structures and applications in machine learning. As we will see in the
next chapters, these two assumptions will result in (i) the design of a
new learning algorithm and (ii) a new analysis technique for
discriminative probabilistic models for structured prediction.

## Chapter 5 Structured Ridge Regression

In this chapter, we design a new learning algorithm for structured
prediction using the counting assumption introduced in the previous
chapter. The algorithm, as we will see in the following sections, is a
generalisation of ridge regression for structured prediction. The
algorithm can be trained by solving an unconstrained, polynomially-sized
quadratic program, and does not assume the existence of polynomial time
algorithms for decoding, separation, or optimality. The crux of our
approach lies in the polynomial time computation of the vector @xmath
and the matrix @xmath leading to a tractable optimisation problem for
training structured prediction models.

### 5.1 Ridge Regression

Given a set of training examples @xmath , the goal of regularised least
squares regression (RLRS) (Rifkin and Lippert, 2007 ) or ridge
regression is to find a solution to the regression problem ¹ ¹ 1 The
same technique can be used for binary classification. The term
regularised least squares classfication (RLSC) was coined by Rifkin (
2002 ) . via Tikhonov regularisation in a reproduding kernel Hilbert
space. The following optimisation problem is considered:

  -- -- -- -------
           (5.1)
  -- -- -- -------

where @xmath is the regularisation parameter. According to the
representer theorem (Schölkopf et al., 2001 ) , the solution to this
optimisation problem can be written as

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath and a positive definite kernel @xmath on the input
space. Using this fact, the optimisation problem \eq eq:rlsr can be
rewritten as

  -- -- --
        
  -- -- --

\hide

Setting the derivative w.r.t. @xmath to @xmath ,The optimal solution
@xmath can be found by solving a system of linear equations

  -- -------- --
     @xmath   
  -- -------- --

Rifkin ( 2002 ) discusses the pros and cons of regularised least squares
regression in comparison with SVMs. Training an SVM requires solving a
convex quadratic optimisation problem, whereas training an RLSR requires
the solution of a single system of linear equations. However, the
downside of training a non-linear RLSR is that it requires storing (
@xmath space) and also inverting ( @xmath time) the entire kernel
matrix. Also, the solution of an RLSR is not sparse unlike the solution
of an SVM thereby demanding huge computations at test time. In the case
of linear RLSR, the optimisation problem \eq eq:rlsr_sol can be written
as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the input matrix of size @xmath . If @xmath , it is
possible to solve this system in @xmath operations using the
Sherman-Morrison-Woodbury formula (Rifkin, 2002 ) . Empirically, RLSC
was shown to perform as well as SVMs (Rifkin, 2002 ) . \hide \todo
expand - Nyström apx, etc?

### 5.2 Training Combinatorial Structures

We now present a learning algorithm for predicting combinatorial
structures. Interestingly, the connection to ridge regression is
incidental that is established due to manipulating a particular
structured loss function in such a way that the resulting optimisation
problem remains tractable under the counting assumption.

##### Problem Setting

Let @xmath be an input space and @xmath be the output space of a
combinatorial structure. Given a set of training examples @xmath , the
goal is to learn a scoring function @xmath that, for each @xmath ,
orders (ranks) @xmath in such a way that it assigns a higher score to
all @xmath than to all @xmath . Let @xmath be a finite dimensional
embedding of @xmath with the dot-product kernel @xmath . Let @xmath be a
kernel on @xmath . Denote the joint scoring function on input-output
pairs by @xmath where @xmath denotes the tensor product and @xmath are
the reproducing kernel Hilbert spaces (RKHS) of @xmath , respectively.
Note that the reproducing kernel of @xmath is then @xmath . We aim at
solving the optimisation problem

  -- -- -- -------
           (5.2)
  -- -- -- -------

where @xmath is the empirical risk on a training instance and @xmath is
the regularisation parameter.

##### Loss Functions \hide

We propose to directly minimise the number of misordered pairs as
minimising ranking measures is often more robust, i.e.,For each @xmath
we aim at ordering all elements of @xmath before all elements of @xmath
. Note that traditional (label) ranking methods cannot be applied due to
the huge (exponential) size of @xmath . We use AUC-loss as the empirical
error of the optimisation problem \eq eq:basicopt:

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath is the modified step function: @xmath if @xmath , @xmath if
@xmath , and @xmath if @xmath . Our definition of @xmath differs from
the ‘area under the ROC curve’ measure only in the sense that it is not
normalised. To obtain a convex function we bound it from above by the
exponential loss

  -- -- -- -------
           (5.4)
  -- -- -- -------

Despite being convex the exponential loss does not allow compact
formulations in our setting, but using its second order Taylor expansion
at @xmath , i.e., @xmath , does. Ignoring constants that can be
accommodated by the regularisation parameter, we get

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

The above loss function can be seen as a generalisation of the square
loss for structured prediction, and hence the connection to ridge
regression is established. Similar loss functions were considered in
previous work on structured prediction problems. Altun et al. ( 2002 )
introduced the ranking loss \eq eq:aucloss for structured prediction and
minimised an upper bound given by the exponential loss \eq eq:exploss
for discriminative sequence labeling. For this specific problem, dynamic
programming was used to explicity compute the sums over all possibls
sequences efficiently (Altun et al., 2002 ) . A closely related loss
function to our approximation \eq eq:texploss is the one minimised by
least-squares SVM (Suykens and Vandewalle, 1999 ) and also its
multi-class extension (Suykens, 1999 ) . Our approach can therefore be
seen as an extension of least-squares SVM for structured prediction
problems. The main reason behind deviating from the standard max-margin
hinge loss is to make the problem tractable. As will become clear in the
following sections, using the loss function \eq eq:texploss results in a
polynomially-sized unconstrained optimisation problem.

##### Representer

The standard representer theorem (Schölkopf et al., 2001 ) states that
there is a minimiser @xmath of the optimisation problem \eq eq:basicopt
with

  -- -------- --
     @xmath   
  -- -------- --

It also holds in our case: Without loss of generality we consider @xmath
where @xmath and @xmath with @xmath . Now @xmath as well as @xmath .
This shows that for each @xmath there is an @xmath for which the
objective function of \eq eq:basicopt is no larger.

As usually @xmath grows exponentially with the input of our learning
algorithm, it is intractable to optimise over functions in @xmath
directly. Let @xmath be the canonical orthonormal bases of @xmath . We
then have that @xmath spans @xmath . Therefore it is sufficient to
optimise over only @xmath variables. We hence consider

  -- -- -- -------
           (5.6)
  -- -- -- -------

with

  -- -------- --
     @xmath   
  -- -------- --

##### Optimisation \hide

We now consider cases in which we have a finite dimensional output
embedding @xmath with @xmath and the vector @xmath as well as the matrix
@xmath can be computed in polynomial time for all of @xmath . We present
several examples in Section LABEL:sc:examples . If @xmath can be
computed in polynomial time but computing @xmath is hard, we may resort
to approximations to @xmath and @xmath . Another option would be to
relax the set @xmath to a superset @xmath such that @xmath can be
computed in polynomial time for @xmath . For set systems we can (in the
worst case) relax to @xmath such that @xmath with @xmath . For
approximate decoding it might however sometimes be necessary to choose a
different kernel. We also give several examples for this case in Section
LABEL:sc:examples . We now exploit the fact that the vector @xmath and
the matrix @xmath can be computed in polynomial time for a wide range of
combinatorial structures as shown in Section 4.3 . If @xmath can be
computed in polynomial time but computing @xmath is hard, we may resort
to approximations to @xmath and @xmath . Another option would be to
relax the set @xmath to a superset @xmath such that @xmath can be
computed in polynomial time for @xmath . For set systems we can (in the
worst case) relax to @xmath such that @xmath with @xmath .

Denote @xmath by @xmath . Let @xmath be the matrix @xmath such that
@xmath and @xmath be the kernel matrix such that @xmath . We then have
@xmath and @xmath . We can now express @xmath using @xmath , @xmath ,
and

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

For the simple setting where @xmath , for all @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

We have thus expressed @xmath explicity in terms of @xmath and @xmath ,
and computing these quantities will depend on the specific combinatorial
structure (cf. Section 4.3 ).

Let @xmath denote the Hadamard product, let @xmath denote the trace
operator, and let @xmath be the operator that maps a square matrix to
the column vector corresponding to its diagonal as well as a column
vector to the corresponding diagonal matrix. Using the canonical
orthonormal basis of @xmath , we can write the optimisation problem \eq
eq:texpoptform as

  -- -------- -------- -------- -------
     @xmath   @xmath            (5.7)
                       @xmath   
  -- -------- -------- -------- -------

We can use iterative methods like Newton conjugate gradient for training
with the gradient

  -- -------- --
     @xmath   
     @xmath   
  -- -------- --

and the product of the Hessian with vector @xmath

  -- -------- --
     @xmath   
  -- -------- --

##### Computing @xmath with Infinite Dimensional Output Embedding

In the case of infinite dimensional output embeddings, we assume that
@xmath has a basis @xmath with @xmath polynomial in the input of our
learning problem. Therefore it is sufficient to optimise over @xmath
variables as @xmath resuting in the following optimisation problem:

  -- -- --
        
  -- -- --

with

  -- -------- --
     @xmath   
  -- -------- --

We now introduce bra-ket notation for convenience and denote @xmath by
@xmath and its dual by @xmath . General elements of @xmath and its dual
will be denoted by kets and bras with letters other than @xmath and
@xmath . Note that hence @xmath does not imply that there exists @xmath
with @xmath . In particular, we denote @xmath by @xmath . The product
@xmath is a linear operator @xmath , the product @xmath is just the
inner product @xmath . Note that for @xmath with \hide @xmath and @xmath
it holds that @xmath and @xmath . With @xmath , @xmath , and @xmath we
obtain

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

and hence

  -- -------- --
     @xmath   
  -- -------- --

### 5.3 Scalability Issues

The optimisation problem \eq eq:finiteopt suffers from scalability
issues similar to those that arise in ridge regression. Also, the
gradient and Hessian computations involve dense matrix-matrix and
matrix-vector computations that may prove to be detrimental for use in
large-scale structured prediction problems. We now present a couple of
techniques to address these issues. First, we reformulate the problem
using a linear scoring function. This is not a serious restriction as we
will see in the following section that it is indeed possible to solve an
equivalent problem to \eq eq:finiteopt using linear models and
techniques from low-dimensional embeddings. Second, we show how to solve
the problem \eq eq:finiteopt using online optimisation and RKHS updates
very much similar in spirit to the kernel perceptron (Freund and
Schapire, 1999 ) .

#### 5.3.1 Linear models

Consider a linear model with scoring function @xmath where @xmath , and
the following problem:

  -- -- -- -------
           (5.8)
  -- -- -- -------

where the loss function is the same as \eq eq:texploss. If we let @xmath
, we can again express @xmath using @xmath , @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Under the assumption that @xmath , the optimisation problem \eq
eqn:linopt is equivalent to \eq eq:finiteopt with @xmath . Given any
arbitrary kernel on the input space @xmath and a set of training
examples, we can extract a corresponding low-dimensional embedding of
the inputs (see Appendix B for an overview on kernel functions and
low-dimensional mappings) and still be able to solve the problem \eq
eqn:linopt. The advantage of such a formulation is that it is
straightforward to apply online optimisation techniques like stochastic
gradient descent and its extensions (Zinkevich, 2003 ; Shalev-Shwartz
et al., 2007 ; Hazan et al., 2007 ) resulting in scalable algorithms for
structured prediction.

#### 5.3.2 Online Optimisation

Online methods like stochastic gradient descent (SGD) and stochastic
meta descent (SMD) (Schraudolph, 1999 ) incrementally update their
hypothesis using an approximation of the true gradient computed from a
single training example (or a set of them). While this approximation
leads to slower convergence rates (measured in number of iterations)
when compared to batch methods, a common justification to using these
methods is that the computational cost of each iteration is low as there
is no need to go through the entire data set in order to take a descent
step. Consequently, stochastic methods outperform their batch
counterparts on redundant, non-stationary data sets (Vishwanathan
et al., 2006 ) . The central issue in using stochastic methods is
choosing an appropriate step size of the gradient descent, and
techniques like stochastic meta descent (Schraudolph, 1999 ;
Vishwanathan et al., 2006 ) have emerged as powerful means to address
this issue.

##### Stochastic Gradient Descent in Feature Space

It is rather straightforward to use stochastic gradient descent for
linear models with a parameter vector @xmath . The update rule at any
iteration is @xmath , where @xmath and @xmath are the instantaneous
gradient and step size ² ² 2 The step size is often chosen to be
time-dependent. respectively. However, for non-linear models such as
kernel methods, we have to perform gradient descent in RKHS and update
the dual parameters. We illustrate this with an example following the
online SVM algorithm of Kivinen et al. ( 2001 ) . Consider regularised
risk minimisation with loss function @xmath :

  -- -------- --
     @xmath   
  -- -------- --

The stochastic approximation of the above functional is given as

  -- -------- --
     @xmath   
  -- -------- --

The gradient of @xmath w.r.t. @xmath is

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The second summand follows by using the reproducing property of @xmath
to compute the derivate of @xmath , i.e., @xmath , and therefore for the
loss function which is differentiable in its third argument we obtain
@xmath . The update rule is then given as @xmath . For the commonly used
regulariser @xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

Expressing @xmath as a kernel expansion @xmath , where the expansion is
over the examples seen until the current iteration, we get

  -- -------- --
     @xmath   
  -- -------- --

We are now ready to derive SGD updates for the optimisation problem \eq
eq:finiteopt.

##### Online Structured Ridge Regression

At iteration t, let @xmath , @xmath , @xmath , and let @xmath be the
parameter matrix. In the following, we omit the subscript @xmath . The
instantaneous objective of the optimisation problem \eq eq:finiteopt can
be written as \hide Let @xmath such that @xmath if @xmath or @xmath
otherwise. At iteration @xmath , define @xmath , @xmath , @xmath , and
let @xmath be the parameter vector. In the following, we omit the
subscript @xmath . The instantaneous objective of \eq eq:finiteopt can
be written as

  -- -------- -------- -------
     @xmath            (5.9)
              @xmath   
  -- -------- -------- -------

with gradient @xmath ,

  -- -------- -------- --------
     @xmath            (5.10)
              @xmath   
  -- -------- -------- --------

product of Hessian @xmath with vector @xmath ,

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

With step size @xmath , we obtain the following update rule: @xmath . We
now discuss a couple of implementation aspects pertaining to the
optimisation problem \eq eq:insfiniteopt.

##### Truncating Kernel Expansion Coefficients

As the function \eq eq:insfiniteopt is minimised in RKHS, the parameter
matrix @xmath grows incrementally with time by adding a single row in
every iteration. In order to speed up computations, we truncate all
parameters that were updated before time @xmath . This is justified for
regularised risk minimisation problems because at every iteration,
@xmath with @xmath is shrunk by a factor @xmath and therefore the
contribution of old parameters in the computation of the kernel
expansion \hide eq:form decreases with time (Kivinen et al., 2001 ;
Vishwanathan et al., 2006 ) . We use this simple technique to speed up
computations in our experiments. It is also possible to apply other
techniques (see, for example, (Dekel et al., 2008 ) ), to discard less
important coefficients. Note that the learning rate is set to ensure
@xmath .

##### Step Size Adaptation

The step size plays an important role in the convergence of stochastic
approximation techniques and has been the focus of recent research
(Vishwanathan et al., 2006 ) . We set the step size @xmath where @xmath
is a parameter that has to be fine tuned to obtain good performance. A
better way to set the step size would be to consider SMD updates
(Schraudolph, 1999 ; Vishwanathan et al., 2006 ) . \hide or perform SGD
followed by the greedy projection method where the step size is cleary
defined with theoretical guarantees

### 5.4 Approximate Inference

Thus far we have described a training algorithm for structured
prediction, but have not discussed how to predict structures using the
learned model. We now design (inference) algorithms for predicting
combinatorial structures. As exact inference is in general hard (cf.
Chapter 4 ), we have to resort to approximations. We therefore design
approximation algorithms using the notion of z-approximation (Hassin and
Khuller, 2001 ; Ausiello and Marchetti-Spaccamela, 1980 ) with provable
guarantees.

#### 5.4.1 Approximate Decoding

In order to construct (predict) structures for a given input using the
model described in Section 5.2 , we have to solve the decoding problem

  -- -------- --
     @xmath   
  -- -------- --

In cases where exact decoding is not possible, we resort to approximate
decoding methods and aim at finding @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

\hide

The general idea is to first construct a complete directed graph and
assign a weight to each edge using the trained model. Given a directed
edge @xmath , its weight @xmath is given by @xmath . The next step is to
apply an appropriate approximation algorithm on this graph. This idea
was first introduced by McDonald et al. ( 2005 ) to predict maximum
spanning trees for dependency parsing in natural language processing
applications. We illustrate with an example that this idea does not work
for the kind of combinatorial structures considered in this work and
thereby motivate the use of z-approximation algorithms.

Consider the prediction of posets. At first glance, it seems plausible
to use existing approximation algorithms for the maximum acyclic
subgraph problem (Hassin and Rubinstein, 1994 ) or the complementary
minimum feedback arc set problem ( Even/etal/95 ) . We now give a
counter-example where this is not possible. Consider the digraph @xmath
with @xmath and @xmath for all @xmath . The maximum acyclic subgraph is
@xmath with total weight @xmath . However, if we also consider edges
included by transitivity (as needed for the decoding problem for
posets), the weight reduces to @xmath . On the other hand, the subgraph
@xmath has weight only @xmath , but if we also consider the edges
included by transitivity, the weight is still 2!. The problem is due to
negative weights on edges which makes these algorithms inapplicable in
our setting because we cannot guarantee that the weights returned by our
model are non-negative.

As another example, consider the min-cut problem on a graph with
non-negative edge weights. Now, if we change the sign of every edge
weight, we see that one has to solve the max-cut problem which is a hard
problem. Changing the sign of the weights has dramatically affected the
optimisation problem (min-cut to max-cut)! In the following section, we
describe the notion of @xmath -measure for approximation algorithms and
prove approximation guarantees for the decoding problem of various
classes of combinatorial structures.

##### z-approximation

@xmath -approximation algorithms are particularly suitable for
optimisation problems involving negative weights. The reader is referred
to Hassin and Khuller ( 2001 ) for examples. \hide For prediction, we
resort to approximate decoding methods for @xmath such that @xmath . For
output kernels @xmath we have @xmath for some @xmath determined \hide
for finite dimensional output spaces by the above described optimisation
\eq eq:finiteopt and the representation of the function \eq eq:form as
@xmath .

We now give two cases that are as simple as they are general and in
which a @xmath -approximation for decoding can be guaranteed. @xmath
-approximation was proposed by Zemel ( 1981 ) instead of the more common
“percentage-error” @xmath . @xmath has @xmath -approximation factor
@xmath for a maximisation problem if

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

An algorithm with @xmath is optimal whereas an algorithm with @xmath is
trivial. @xmath -approximation has several advantages (Hassin and
Khuller, 2001 ; Ausiello and Marchetti-Spaccamela, 1980 ) including (
@xmath ) @xmath has the same @xmath -approximation as @xmath where
@xmath is a constant; ( @xmath ) @xmath has the same @xmath
-approximation as @xmath ( @xmath -approximation for minimisation is
defined by exchanging @xmath and @xmath in \eq eq:z); and ( @xmath ) the
@xmath -approximation factor is invariant under replacing binary
variables by their complement.

##### Decoding Sibling Systems

A sibling system is an output space @xmath with a sibling function
@xmath and an output map @xmath such that @xmath as well as @xmath with
fixed @xmath . In other words, it is an output space in which each
structure can be complemented by its sibling. {proposition} There is a
@xmath -factor @xmath -approximation algorithm for decoding sibling
systems. {proof} Choose some @xmath at random. If @xmath , then @xmath ;
otherwise @xmath . This completes the description of the algorithm.

We know that @xmath for some @xmath . Now @xmath and as @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

thereby satisfying \eq eq:z with @xmath .

Notice that if @xmath is bijective and @xmath , then also @xmath , thus
significantly simplifying the optimisation problem \eq eq:finiteopt. For
@xmath an example of a bijective sibling function is @xmath .

##### Decoding Independence Systems

An independence system @xmath is an output space @xmath such that @xmath
and membership in @xmath can be tested in polynomial time. \hide If
@xmath is defined by a property @xmath , this property is called
hereditary.Consider an output map @xmath with @xmath if @xmath and
@xmath otherwise, for some measure @xmath . Here we find a polynomial
time @xmath factor @xmath -approximation following (Halldórsson, 2000 )
. {proposition} There is a @xmath factor @xmath -approximation algorithm
for decoding independence systems. {proof} Partition @xmath into @xmath
many subsets @xmath of size @xmath . Choose @xmath where @xmath . We
again choose @xmath such that @xmath . Now we can find @xmath in
polynomial time by exhaustively testing @xmath alternatives in each of
the @xmath subsets. This completes the description of the algorithm.

For the @xmath -approximation, suppose there was @xmath with @xmath . As
@xmath partitions @xmath , we have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

contradicting the assumption. Together with @xmath this proves the
stated approximation guarantee.

#### 5.4.2 Approximate Enumeration

If the non-optimality decision problem @xmath is NP-hard then there is
no algorithm for enumerating any set @xmath of cardinality polynomial in
@xmath in output polynomial time (unless P=NP). To see this, suppose
there was an output polynomial algorithm for listing @xmath . This
algorithm can be used to obtain an output polynomial algorithm for
listing @xmath with additional complexity only for testing @xmath for
all @xmath . Now if the algorithm terminates in input polynomial time
then it is sufficient to check the cardinality of @xmath to decide
non-optimality. If on the other hand the algorithm does not terminate in
input polynomial time, then @xmath can not be empty.

Hence, we consider enumerating approximate solutions, i.e., we want to
list

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

If @xmath is such that listing the whole of @xmath is hard, this
precludes a general algorithm for the trivial case ( @xmath ) and makes
it rather unlikely that an algorithm for @xmath exists. We now assume
that we know how to list (sample uniformly from) @xmath and that the
situation is similar to sibling systems: We have a bijective function
@xmath and a map @xmath such that @xmath as well as @xmath for fixed
@xmath . Then we can list the set \eq eq:zset in incremental polynomial
time by a simple algorithm that internally lists @xmath from @xmath but
instead of outputting @xmath directly, it only outputs @xmath if @xmath
and otherwise outputs @xmath . \hide Note that the case @xmath does not
deserve special attention as the subroutine for listing @xmath will also
list @xmath .

### 5.5 Empirical Results

In all experiments, we fixed the regularisation parameter of our
algorithm @xmath .

##### Multi-label Classification

We compared the performance of our algorithm with the kernel method
proposed by Elisseeff and Weston ( 2001 ) that directly minimises the
ranking loss (number of pairwise disagreements). We followed the same
experimental set up (dataset and kernels) as described in (Elisseeff and
Weston, 2001 ) . We used the Yeast dataset consisting of 1500 (training)
and 917 (test) genes with 14 labels, and trained our algorithm with
polynomial kernel of varying degree (2-9). Figure 5.1 shows the results
for Hamming loss and ranking loss. \hide Our algorithm consistently
outperforms the kernel method for multi-label classification.

##### Hierarchical Classification

We trained our algorithm on the WIPO-alpha patent dataset ³ ³ 3
Available at one of the authors (Rousu et al., 2006 ) webpage:
http://users.ecs.soton.ac.uk/cjs/downloads/ consisting of 1352 training
and 358 test documents. The number of nodes in the hierarchy is 188 with
maximum depth of 3. Each document belongs to exactly one leaf category
and hence contains no multiple paths. The performance of several
algorithms (results are taken from (Rousu et al., 2005 ) ) is shown in
Table 5.1 .

@xmath denotes the zero-one loss in percentage, @xmath is the average
Hamming loss per instance, and @xmath is the hierarchical loss (average
per instance). The hierarchical loss is based on the intuition that if a
mistake is made at node @xmath , then further mistakes made in the
subtree rooted at @xmath are unimportant (Cesa-Bianchi et al., 2006 ) .
Formally, for any pair of hierarchical labels @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the set of ancestors of @xmath , and @xmath is 1 if
@xmath is true and @xmath otherwise. In this way the hierarchy or
taxonomy of the problem domain is taking into account. SVM denotes an
SVM trained for each microlabel independently, H-SVM denotes an SVM
trained for each microlabel independently and using only those samples
for which the ancestor labels are positive, H-RLS is the hierarchical
least squares algorithm described in (Cesa-Bianchi et al., 2006 ) and H-
@xmath is the kernel-based algorithm proposed by Rousu et al. ( 2006 )
which uses the maximum margin Markov network framework. The two
different versions of this algorithm correspond to using the Hamming
loss and the hierarcical loss during training. While the performance on
the Hamming loss was comparable to the baseline SVM, our algorithm
resulted in best performance on the @xmath loss and the hierarchical
loss. \hide We note that on this particular dataset there is no positive
correlation between the Hamming loss and the other two losses. Our
algorithm was able to output the true label for around half of the test
instances. Among the rest, it was making a lot of mistakes in the
subtrees of incorrectly labeled nodes that are anyway considered to be
unimportant and therefore not penalised by the hierarchical loss.

##### Dicycle policy estimation

We experimented with an artificial setting due to lack of real world
data sets on dicycle prediction ⁴ ⁴ 4 Note that we considered directed
cyclic permutations as opposed to undirected ones described in Section
4.2 due to the fact that @xmath for dicycles, thereby reducing the
number of computations involved in optimising \eq eq:finiteopt. See
Appendix C for more details on counting dicyclic permutations. . We
simulate the problem of predicting the cyclic tour of different people.
We assume that there is a hidden policy for each person and he/she takes
the route that (approximately) maximises the reward of the route. In the
setting of dicycle prediction, the learned function is linear in the
output space ( @xmath ) and for testing we can check how well the
estimated policy @xmath approximates the hidden policy in the test (
@xmath ) set. The data is constructed as follows: ( @xmath ) generate
@xmath matrices @xmath uniformly at random with entries in the interval
@xmath and @xmath ; ( @xmath ) generate @xmath random numbers uniformly
between @xmath and @xmath to form the inputs @xmath ; ( @xmath ) create
the output structures @xmath for training, that is @xmath . On the test
set, we evaluated our algorithm by cosine similarity of the learned
policy and the true policy:

  -- -------- --
     @xmath   
  -- -------- --

Figure 5.2 shows a plot of the cosine measure on an experiment ( @xmath
) for varying number of training instances.

\hide

We generated 5 cycles per training instance, i.e. @xmath . As expected,
we see that our algorithm is able to estimate the true policy with
increasing accuracy as the number of training instances increases. The
plot also shows the performance of structured SVM using approximate
decoding during training. Approximate decoding is performed by randomly
sampling a couple of cyclic permutations and using the best scoring one
(the number of repetitions used in our experiments was @xmath ). The
results were unstable due to approximate decoding and the results shown
on the plot are averages from 5 trials. For structured SVM, we
experimented with several values of the regularisation parameter and
report the best results obtained on the test set — though this procedure
gives an advantage to structured SVM, our algorithm still resulted in
better performance.

##### Stochastic Gradient Descent

We performed a couple of experiments using artificially generated data
to compare the performances of online and batch learning. In the first
experiment, we trained a simple multi-label classification model to
learn the identity function @xmath . The goal was to compare batch and
online learning, using Newton conjugate gradient (NCG) and stochastic
gradient descent (SGD) respectively, in terms of the final objective
value of the optimisation problem \eq eq:finiteopt and training time. We
also studied the effects of the truncation parameter @xmath on speed and
final objective. We trained SGD on a single pass of the data set.
Figures 5.3 and 5.4 summarises the results for multi-label
classification on an artificial data set with 5 features and labels.

We set the truncation parameter @xmath to @xmath , where @xmath is the
number of training instances. We see that the final solution of SGD is
comparable to that of NCG, and the speed up achieved by SGD is apparent
for large data sets. The effect of @xmath on training time and objective
is shown in Figure 5.4 . The training time of SGD increases with @xmath
and attains the training time of NCG at around @xmath of @xmath . Beyond
this value, we found that SGD was taking longer time than NCG. This
underlines the effect of @xmath when performing SGD updates in RKHS.

In our second experiment, we considered dicycle policy estimation as
before. We trained SGD and NCG on data sets of varying size from @xmath
to @xmath with @xmath and @xmath . We fixed @xmath to @xmath kernel
expansion coefficients. Figure 5.5 shows a plot of final objective
versus training time of SGD and NCG on the different data sets.

The plot shows that NCG takes a much longer time to attain the same
final objective value as SGD. Note that as we perform single pass
training, with fixed amounts of training instances NCG attains a smaller
value of the objective function than SGD. However, as SGD can deal with
much more training instances in the same time, after a fixed amount of
time, SGD attains a smaller value of the objective function than NCG.

### 5.6 Summary

We presented a learning algorithm for predicting combinatorial
structures under the counting assumption introduced in Chapter 4 . Our
approach subsumes several machine learning problems including
multi-class, multi-label and hierarchical classification, and can be
used for training complex combinatorial structures. As for most
combinatorial structures considered in this paper the inference problem
is hard, an important aspect of our approach is that it obviates the
need to use inference algorithms, be it exact or approximate, for
training.

We have also seen how to train a linear model using the counting
assumption. Under some reasonable assumptions, a non-linear model can be
approximated using a linear model using techniques from metric embedding
theory. Furthermore, we addressed the scalability issues of our approach
by presenting an online learning algorithm using stochastic gradient
descent that can update model parameters (kernel expansion coefficients)
in RKHS.

For prediction, inference can naturally not be avoided. Therefore, we
have to rely on approximation algorithms described in Section 5.4 . We
note that it is non-trivial to design approximation algorithms for the
decoding problem of the combinatorial structures considered in this
work. Indeed, there are hardness of approximation results for the
maximum acyclic subgraph problem (Guruswami et al., 2008 ) and the
problem of finding longest directed cycles (Björklund et al., 2004 ) .

While we have seen how to minimise a squared loss function, it would be
interesting to train a probabilistic model by minimising the negative
log-likelihood @xmath la conditional random fields (cf. Section 2.2 ).
This will be the focus of the next chapter.

## Chapter 6 Probabilistic Structured Prediction

Maximum a posteriori (MAP) estimation with exponential family models is
a fundamental statistical technique for designing probabilistic
classifiers (cf. logistic regression). In this chapter, we consider MAP
estimators for structured prediction using the sampling assumption
introduced in Chapter 4 , i.e., we concentrate on the case that
efficient algorithms for uniform sampling from the output space exist.
We show that under this assumption (i) exact computation of the
partition function remains a hard problem, and (ii) the partition
function and the gradient of the log partition function can be
approximated efficiently. The main result of this chapter is an
approximation scheme for the partition function based on Markov chain
Monte Carlo theory. We also design a Markov chain that can be used to
sample combinatorial structures from exponential family distributions
given that there exists an exact uniform sampler, and also perform a
non-asymptotic analysis of its mixing time.

### 6.1 Probabilistic Models and Exponential Families

Let @xmath be the domain of observations and labels, and @xmath , @xmath
be the set of observations. Our goal is to estimate @xmath using
exponential families via

  -- -------- --
     @xmath   
  -- -------- --

where @xmath are the joint sufficient statistics of @xmath and @xmath ,
and @xmath is the partition function. We perform MAP parameter
estimation by imposing a Gaussian prior on @xmath . This leads to
optimising the negative joint likelihood in @xmath and @xmath :

  -- -------- -- -- -------
     @xmath         (6.1)
                    
  -- -------- -- -- -------

where @xmath is the regularisation parameter. We assume that the @xmath
norm of the sufficient statistics and the parameters are bounded, i.e.,
@xmath and @xmath , where @xmath and @xmath are constants. \hide This is
not a serious restriction as @xmath can be bounded from above as is
proved in Appendix LABEL:sec:norm_bound .Note that it is possible to
upper bound the norm of the parameter vector @xmath as shown below.
{proposition} The norm of the optimal parameter vector @xmath is bounded
from above as follows:

  -- -------- --
     @xmath   
  -- -------- --

{proof}

Consider any @xmath . Denote by @xmath the loss function, where @xmath ,
and note that @xmath . Let @xmath . The true regularised risk w.r.t.
@xmath and an underlying joint distribution @xmath on @xmath is

  -- -------- --
     @xmath   
  -- -------- --

This implies that the optimal solution @xmath of the above optimisation
problem lies in the set @xmath .

The difficulty in solving the optimisation problem \eq eqn:opt lies in
the computation of the partition function. The optimisation is typically
performed using gradient descent techniques and advancements thereof. We
therefore also need to compute the gradient of the log partition
function, which is the first order moment of the sufficient statistics,
i.e., @xmath .

Computing the log partition function and its gradient are in general
NP-hard. In Section 6.2 , we will show that computing the partition
function still remains NP-hard given a uniform sampler for @xmath . We
therefore need to resort to approximation techniques to compute these
quantities. Unfortunately, application of concentration inequalities do
not yield approximation guarantees with polynomial sample size. We
present Markov chain Monte Carlo (MCMC) based approximations for
computing the partition function and the gradient of the log partition
function with provable guarantees. There has been a lot of work in
applying Monte Carlo algorithms using Markov chain simulations to solve
@xmath P-complete counting and NP-hard optimisation problems. Recent
developments include a set of mathematical tools for analysing the rates
of convergence of Markov chains to equilibrium (see (Randall, 2003 ;
Jerrum and Sinclair, 1996 ) for surveys). To the best of our knowledge,
these tools have not been applied in the design and analysis of
structured prediction problems, but have been referred to as an
important research frontier (Andrieu et al., 2003 ) for MCMC based
machine learning problems in general.

### 6.2 Hardness of Computing the Partition Function

We begin with a hardness result for computing the partition function.
Consider the following problem: {definition} Partition : For a class of
output structures @xmath over an alphabet @xmath , an input structure
@xmath , a polynomial time computable map @xmath , and a parameter
@xmath , compute the partition function @xmath . We now show that no
algorithm can efficiently solve Partition on the class of problems for
which an efficient uniform sampling algorithm exists. To show this, we
suppose such an algorithm existed, consider a particular class of
structures, and show that the algorithm could then be used to solve an
NP-hard decision problem. We use that (a) cyclic permutations of subsets
of the alphabet @xmath can be sampled uniformly at random in time
polynomial in @xmath ; and (b) there is no efficient algorithm for
Partition for the set of cyclic permutations of subsets of the alphabet
@xmath with @xmath if @xmath and @xmath otherwise. Here (a) follows from
Sattolo’s algorithm (Sattolo, 1986 ) to generate a random cyclic
permutation. To prove (b), we show that by applying such an algorithm to
a multiple of the adjacency matrix of an arbitrary graph and comparing
the result with @xmath we could decide if the graph has a Hamiltonian
cycle or not.

{theorem}

Unless P=NP, there is no efficient algorithm for Partition on the class
of problems for which we can efficiently sample output structures
uniformly at random. {proof} Consider the output space of undirected
cycles over a fixed set of vertices @xmath , i.e., @xmath , for an input
@xmath . Let @xmath with @xmath if @xmath and @xmath otherwise.

Suppose we can compute @xmath efficiently. Given an arbitrary graph
@xmath with adjacency matrix @xmath , let @xmath and @xmath . We will
show that @xmath has a Hamiltonian cycle if and only if @xmath .

Necessity: As the exponential function is positive and @xmath is
monotone increasing, it follows that @xmath .

Sufficiency: First, observe that @xmath . Suppose G has no Hamiltonian
cycle. Then

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

This completes the proof.

We are interested in the class of problems for which sampling uniformly
at random is easy, and cyclic permutations is one example of these. The
above result shows that computing the partition function is hard even if
we restrict the problem to this class. Essentially, it transfers the
general NP-hardness result of computing the partition function to the
restricted class of problems that we are interested in. In the following
section, we show how to approximate the partition function given that
there exist efficient algorithms for uniform sampling.

### 6.3 Approximating the Partition Function Using Uniform Samplers

As a first step towards approximating the partition function, let us
consider using concentration inequalities. If we can sample uniformly at
random from @xmath , then we can apply Hoeffding’s inequality to bound
the deviation of the partition function @xmath from its finite sample
expectation @xmath . Let @xmath denote the sample size. Then

  -- -------- --
     @xmath   
  -- -------- --

\hide

By Cauchy-Schwarz’s inequality, we have @xmath .A direct application of
Hoeffding’s inequality gives us the following result:

  -- -------- --
     @xmath   
  -- -------- --

Unfortunately, this bound is not useful due to its dependence on the
size of the output space @xmath . Unfortunately, the bound obtained from
using Hoeffding’s inequality is not useful due to its dependence on the
size of the output space @xmath . We now present an algorithm that is a
fully-polynomial randomised approximation scheme for computing the
partition function.

{definition}

Suppose @xmath is a function that maps problem instances @xmath to
positive real numbers. A randomised approximation scheme for @xmath is a
randomised algorithm that takes as input an instance @xmath and an error
parameter @xmath , and produces as output a number @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

A randomised approximation scheme is said to be fully polynomial (FPRAS)
if it runs in time polynomial in the length of @xmath and @xmath .

We exploit the intimate connection between counting and sampling
problems (Jerrum et al., 1986 ) to approximately compute the partition
function using sampling algorithms. The technique is based on a
reduction from counting to sampling. \hide Computing the partition
function is basically a counting problem and if we are able to sample
from the corresponding output space @xmath , then it is also possible to
solve the counting problem with approximation guarantees.The standard
approach (Jerrum and Sinclair, 1996 ) is to express the quantity of
interest, i.e., the partition function @xmath , as a telescoping product
of ratios of parameterised variants of the partition function. Let
@xmath denote a sequence of parameters also called as cooling schedule
and express @xmath as a telescoping product

  -- -------- --
     @xmath   
  -- -------- --

\hide

  -- -------- --
     @xmath   
  -- -------- --

Define the random variable @xmath (we omit the dependence on @xmath to
keep the notation clear), for all @xmath , where @xmath is chosen
according to the distribution @xmath . We then have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

which means that @xmath is an unbiased estimator for the ratio

  -- -------- --
     @xmath   
  -- -------- --

This ratio can now be estimated by sampling using a Markov chain
according to the distribution @xmath and computing the sample mean of
@xmath . The desideratum is an upper bound on the variance of this
estimator. Having a low variance implies a small number of samples
@xmath suffices to approximate each ratio well. The final estimator is
then the product of the reciprocal of the individual ratios in the
telescoping product.

We now proceed with the derivation of an upper bound on the variance of
the random variable @xmath , or more precisely on the quantity @xmath .
We first assume that @xmath can be computed in polynomial time. This
assumption is true for all combinatorial structures consider in this
chapter. If it is not possible to compute @xmath in polynomial time,
then we can approximate it using the same machinery described in this
section. We use the following cooling schedule (Stefankovic et al., 2009
) :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is a constant integer @xmath , i.e., we let the cooling
schedule to be of the following form:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath (w.l.o.g. we assume that @xmath is non-integer). Given this
cooling schedule, observe that @xmath , which follows from the
definition of the random variable @xmath , and also that

  -- -------- --
     @xmath   
  -- -------- --

We are now ready to prove the bound on the quantity @xmath .

{proposition}

@xmath .

We first need to prove the following lemma. {lemma} @xmath . {proof}
@xmath as the exponential function is monotone increasing. Thus @xmath .
\hide Now, @xmath . Therefore, we have @xmath . Setting @xmath with
@xmath and using the fact that @xmath for all @xmath proves the lemma.
{proof} (of Proposition 6.3 ) Consider @xmath . This implies @xmath .
Next, consider @xmath . This implies @xmath . Combining these, we get
@xmath , which implies @xmath , and therefore @xmath .

Equipped with this bound, we are ready to design an FPRAS for
approximating the partition function. We need to specify the sample size
@xmath in each of the Markov chain simulations needed to compute the
ratios. {theorem} Suppose the sample size @xmath and suppose it is
possible to sample exactly according to the distributions @xmath , for
all @xmath , with polynomially bounded time. Then, there exists an FPRAS
with @xmath as the error parameter for computing the partition function.
{proof} The proof uses standard techniques described in (Jerrum and
Sinclair, 1996 ) . Let @xmath be a sequence of @xmath independent copies
of the random variable @xmath obtained by sampling from the distribution
@xmath , and let @xmath be the sample mean. We have @xmath , and @xmath
. The final estimator @xmath is the random variable @xmath with @xmath .
Now, consider

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

where the last inequality follows by choosing @xmath and using the
inequality @xmath for @xmath . By applying Chebyshev’s inequality to
@xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

and therefore, with probability at least @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Thus, with probability at least @xmath , the partition function @xmath
lies within the ratio @xmath of @xmath . Polynomial run time immediately
follows from the assumption that we can sample exactly according to the
distributions @xmath in polynomial time.

We have shown how to approximate the partition function under the
assumption that there exists an exact sampler for @xmath from the
distribution @xmath for all @xmath . A similar result can be derived by
relaxing the exact sampling assumption and is proved in Appendix D.1 .
In fact, it suffices to have only an exact uniform sampler. As we will
see in Section 6.6 , it is possible to obtain exact samples from
distributions of interest other than uniform if there exists an exact
uniform sampler.

### 6.4 Approximating the Partition Function Using Counting Formulae

In this section, we describe how to approximate the partition function
using counting formulae (cf. Section 4.3 ), which obviates the need to
use the sophisticated machinery described in the previous section.
However, the downside of this technique is that it is not an FPRAS and
works only for a specific feature representation.

Consider output spaces @xmath with a finite dimensional embedding @xmath
, and an input space @xmath . Define the scoring function @xmath , where
@xmath . Suppose that the size of the output space @xmath , the vector
@xmath , and the matrix @xmath can be computed efficiently in polynomial
time. We first observe that given these quantities, it is possible to
compute @xmath and @xmath (as in the linear model described in Section
5.3 ), and these are given as

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

We then consider the second order Taylor expansion of the @xmath
function at @xmath , i.e., @xmath and write the partition function as

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

At first glance, one may argue that using the second order Taylor
expansion is a crude way to approximate the partition function. But
observe that in a restricted domain around @xmath , the second order
Taylor expansion approximates the @xmath function very well as
illustrated in Figure 6.1 .

In order to exploit this property, we have to constrain the scoring
function @xmath to lie in the range @xmath . But from Proposition 6.1 ,
we know that @xmath . A direct application of Cauchy-Schwarz’s
inequality gives us the following bound on the scoring function: @xmath
. An immediate consequence is a bound that relates the regularisation
parameter and the scoring function. {proposition} @xmath .

### 6.5 Approximating the Gradient of the Log Partition Function

The optimisation problem \eq eqn:opt is typically solved using gradient
descent methods which involves gradient-vector multiplications. We now
describe how to approximate the gradient-vector multiplication with
provable guarantees using concentration inequalities. Let @xmath be a
vector in @xmath (where @xmath is the dimension of the feature space
@xmath ) with bounded @xmath norm, i.e., @xmath , where @xmath is a
constant. The gradient-vector multiplication is given as

  -- -------- --
     @xmath   
  -- -------- --

We use Hoeffding’s inequality to bound the deviation of @xmath from its
estimate @xmath on a finite sample of size @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

and the sample is drawn according to @xmath .

Note that by Cauchy-Schwarz’s inequality, we have @xmath , for all
@xmath . Applying Hoeffding’s inequality, we then obtain the following
exponential tail bound:

  -- -------- --
     @xmath   
  -- -------- --

For online optimisation methods like stochastic gradient descent and
advancements thereof (Hazan et al., 2007 ; Shalev-Shwartz et al., 2007 ;
Ratliff et al., 2007 ) , the optimisation problem is solved using plain
gradient descent, and therefore it might be desirable to approximate the
gradient and analyse the effects of the approximation guarantee on
factors such as rates of convergence and regret bounds (Ratliff et al.,
2007 ) . In Appendix D.2 , we show how to approximate the gradient of
the log partition function using the machinery described in Section 6.3
, i.e., using the reduction from counting to sampling.

### 6.6 Sampling Techniques

We now focus on designing sampling algorithms. These algorithms are
needed (i) to compute the partition function using the machinery
described in Section 6.3 , and (ii) to do inference, i.e., predict
structures, using the learned model by solving the optimisation problem
@xmath for any @xmath . Sampling algorithms can be used for optimisation
using the Metropolis process (Jerrum and Sinclair, 1996 ) and possibly
other methods like simulated annealing for convex optimisation (Kalai
and Vempala, 2006 ) . \hide We will revisit these techniques in Chapter
7 .Note that these methods come with provable guarantees and are not
heuristics.

#### 6.6.1 Basics of Markov Chains

We start with some preliminaries on Markov chains. The exposition mostly
follows the articles by Jerrum and Sinclair ( 1996 ); Jerrum ( 1998 );
Randall ( 2003 ) , and the lecture notes of Vigoda ( Fall 2006 ) .

Let @xmath denote the state space of a Markov chain @xmath with
transition probability matrix @xmath . Let @xmath denote the
distribution of the state space at time @xmath given that @xmath is the
initial state. Let @xmath denote the stationary distribution of @xmath .
A Markov chain is said to be ergodic if the probability distribution
over @xmath converges asymptotically to @xmath , regardless of the
intial state. A Markov chain is said to be (a) irreducible if for all
@xmath , there exists a @xmath such that @xmath , and (b) aperiodic if
@xmath , for all @xmath . Any finite, irreducible, aperiodic Markov
chain is ergodic. A Markov chain is said to be time-reversible with
respect to @xmath if it satisfies the following condition also known as
detailed balanced equations :

  -- -------- --
     @xmath   
  -- -------- --

The mixing time of a Markov chain is a measure of the time taken by the
chain to converge to its stationary distribution. It is measured by the
total variation distance between the distribution at time @xmath and the
stationary distribution. The total variation distance at time @xmath is

  -- -------- --
     @xmath   
  -- -------- --

For any @xmath , the mixing time @xmath is given by

  -- -------- --
     @xmath   
  -- -------- --

A Markov chain is said to be rapidly mixing if the mixing time is
bounded by a polynomial in the input and @xmath . We now describe
techniques to bound the mixing time of Markov chains.

##### Canonical Paths

Let @xmath be an ergodic, time-reversible Markov chain with state space
@xmath , transition probabilties @xmath and stationary distribution
@xmath . Given that @xmath satisfies the detailed balanced condition, we
may view @xmath as an undirected graph @xmath with vertex set @xmath and
edge set @xmath , where @xmath .

For every ordered pair @xmath , a canonical path @xmath from @xmath to
@xmath in the graph corresponds to a sequence of legal transitions in
@xmath that leads from the initial state @xmath to the final state
@xmath . Let @xmath be the set of all canonical paths. In order to
obtain good bounds on the mixing time, it is important to choose a set
of paths @xmath that avoids the creation of “hot spots:” edges that
carry a heavy burden of canonical paths. The degree to which an even
loading has been achieved is measured by a quantity known as congestion
, which is defined as follows:

  -- -------- --
     @xmath   
  -- -------- --

where the maximum is over oriented edges @xmath of @xmath and @xmath
denotes the length of the path @xmath . Intuitively, we expect a Markov
chain to be rapidly mixing if it admits a choice of paths @xmath for
which the congestion @xmath is not too large. This intuition is
formalised in the following result due to Sinclair ( 1992 ) . {theorem}
(Sinclair, 1992 ) Let @xmath be an ergodic, time-reversible Markov chain
with stationary distribution @xmath and self-loop probabilities @xmath
for all states @xmath . Let @xmath be a set of canonical paths with
maximum edge loading @xmath . Then the mixing time of @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

for any choice of initial state @xmath .

##### Coupling

A coupling is a Markov process @xmath on @xmath such that each of the
processes @xmath and @xmath , considered in isolation, is a faithful
copy of @xmath , i.e.,

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

and also

  -- -------- --
     @xmath   
  -- -------- --

If it can be arranged that the processes @xmath and @xmath coalesce
rapidly independent of the initial states @xmath and @xmath , we may
deduce that the Markov chain @xmath is rapidly mixing. The key result is
what is called the “Coupling Lemma” due to Aldous ( 1983 ) . {lemma}
(Aldous, 1983 ) (Coupling lemma) Suppose @xmath is a countable, ergodic
Markov chain with transition probabilities @xmath and let @xmath be a
coupling. Suppose @xmath is a function such that @xmath , for all @xmath
, uniformly over the choice of initial state @xmath . Then the mixing
time @xmath of @xmath is bounded from above by @xmath .

##### Path Coupling \hide {definition}

A coupling is a Markov chain on @xmath defining a stochastic process
@xmath such that each of the processes @xmath and @xmath is a faithful
copy of @xmath and @xmath . {definition} For initial states @xmath and
@xmath , the coupling time @xmath . The coupling time @xmath provides a
good bound on the mixing time of @xmath . {theorem} (version from
(Randall, 2003 ) ) The mixing time of @xmath is bounded from above by
@xmath , where @xmath is the base of the natural logarithm. Although
coupling is a powerful technique to bound the mixing time, it may not be
easy to measure the expected change in distance between two arbitrary
configurations. The idea of path coupling introduced by Bubley and Dyer
( 1997 ) is to consider only a small set of pairs of configurations
@xmath that are close to each other w.r.t. a distance metric @xmath .
Suppose we are able to measure the expected change in distance for all
pairs in @xmath . Now, for any pair @xmath , we define a shortest path
@xmath of length @xmath from @xmath to @xmath (sequence of transitions
of minimal weight from @xmath to @xmath ), where @xmath for all @xmath .
If we define @xmath appropriately, then @xmath , and by linearity of
expectation, the expected change in distance between the pair @xmath is
just the sum of the expected change between the pairs @xmath . We now
state the “path coupling lemma” of Bubley and Dyer ( 1997 ) . \hide
{lemma} (Bubley and Dyer, 1997 ) (Coupling lemma) Suppose @xmath and
@xmath be a random process (the coupling) such that marginally, @xmath
and @xmath are both copies of @xmath . Moreover, suppose @xmath is
chosen from @xmath , and @xmath is the distribution of @xmath , then
@xmath , where @xmath is the total variation distance metric on
measures. {lemma} (Bubley and Dyer, 1997 ) (Path Coupling lemma) Let
@xmath be an integer valued metric defined on @xmath , which takes
values in @xmath . Let @xmath be a subset of @xmath such that for all
@xmath there exists a path @xmath between @xmath and @xmath where @xmath
for @xmath and @xmath . Suppose a coupling @xmath of the Markov chain
@xmath is defined on all pairs of @xmath such that there exists a @xmath
such that @xmath for all @xmath . Then the mixing time @xmath is bounded
from above as follows:

  -- -------- --
     @xmath   
  -- -------- --

##### Coupling from the Past

Coupling from the past (CFTP) (Propp and Wilson, 1996 ; Huber, 1998 ) is
a technique used to obtain an exact sample from the stationary
distribution of a Markov chain. The idea is to simulate Markov chains
forward from times in the past, starting in all possible states, as a
coupling process. If all the chains coalesce at time @xmath , then Propp
and Wilson ( 1996 ) showed that the current sample has the stationary
distribution.

Suppose @xmath is an ergodic (irreducible, aperiodic) Markov chain with
(finite) state space @xmath , transition probabilties @xmath and
stationary distribution @xmath . Suppose @xmath is a probability
distribution on functions @xmath with the property that for every @xmath
, its image @xmath is distributed according to the transition
probability of @xmath from state @xmath , i.e.,

  -- -------- --
     @xmath   
  -- -------- --

To sample @xmath , we perform the following steps: (i) sample,
independently for each @xmath , a state @xmath from the distribution
@xmath , and (ii) let @xmath be the function mapping from @xmath to
@xmath for all @xmath . Now, let @xmath with @xmath be an indexed
sequence of functions, and denote by @xmath the iterated function
composition

  -- -------- --
     @xmath   
  -- -------- --

A @xmath -step simulation of @xmath can be performed as follows starting
from some initial state @xmath : (i) select @xmath independently from
@xmath , (ii) compute the composition @xmath , and (iii) return @xmath .
Of course, this is very inefficient requiring about @xmath times the
work of a direct simulation. However, this view will be convenient to
explain the conceptual ideas behind CFTP as given below.

For fixed transition probabilities @xmath , there is a considerable
flexibility in the choice of distributions @xmath , allowing us to
encode uniform couplings over the entire state space. The Coupling Lemma
can be stated in this setting. Suppose @xmath are sampled independently
from @xmath . If there exists a function @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

then the mixing time @xmath of @xmath is bounded by @xmath . Thus, in
principle, it is possible to estimate the mixing time of @xmath by
observing the coalesence time of the coupling defined by @xmath , and
thereby obtain samples from an approximation to the stationary
distribution of @xmath by simulating the Markov chain for a number of
steps comparable with the empirically observed mixing time. However, in
practice, the explicit evaluation of @xmath is computationally
infeasible.

The first idea underlying Propp and Wilson’s proposal was to work with
@xmath instead of @xmath , i.e., by “coupling from the past”, it is
possible to obtain samples from the exact stationary distribution.

{theorem}

Suppose that @xmath is a sequence of independent samples from @xmath .
Let the stopping time @xmath be defined as the smallest number @xmath
for which @xmath is a constant function, and assume that @xmath . Denote
by @xmath the unique value of @xmath (which is defined with probability
1). Then @xmath is distributed according to the stationary distribution
of @xmath .

The second idea underlying Propp and Wilson’s proposal is that in
certain cases, specifically when the coupling @xmath is “monotone”, it
is possible to evaluate @xmath without explicity computing the function
composition @xmath . Suppose that the state space @xmath is partially
ordered @xmath with maximal and minimal elements @xmath and @xmath
respectively. A coupling @xmath is monotone if it satisfies the
following property:

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is montone, then @xmath implies @xmath is a constant function
and that @xmath . Therefore, it suffices to track the two trajectories
starting at @xmath and @xmath instead of tracking @xmath trajectories.
Since only an upper bound on @xmath is needed for computing @xmath , a
doubling scheme @xmath is typically used rather than iteratively
computing @xmath for @xmath .

#### 6.6.2 A Meta Markov chain

The main contribution of this section is a generic, ‘meta’ approach that
can be used to sample structures from distributions of interest given
that there exists an exact uniform sampler. We start with the design of
a Markov chain based on the Metropolis process (Metropolis et al., 1953
) to sample according to exponential family distributions @xmath under
the assumption that there exists an exact uniform sampler for @xmath .
Consider the following chain Meta : If the current state is @xmath ,
then

1.  select the next state @xmath uniformly at random, and

2.  move to @xmath with probability @xmath .

##### Exact Sampling Using Coupling from the Past

As described in the previous section, CFTP is a technique to obtain an
exact sample from the stationary distribution of a Markov chain. To
apply CFTP for Meta , we need to bound the expected number of steps
@xmath until all Markov chains are in the same state. For the chain Meta
, this occurs as soon as we update all the states, i.e., if we run all
the parallel chains with the same random bits, once they are in the same
state, they will remain coalesced. This happens as soon as they all
accept an update (to the same state @xmath ) in the same step. First
observe that, using Cauchy-Schwarz and triangle inequalities, we have

  -- -------- --
     @xmath   
  -- -------- --

The probability of an update is given by

  -- -------- --
     @xmath   
  -- -------- --

We then have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

By using the identity @xmath with @xmath , we get @xmath . We now state
the main result of this section. {theorem} The Markov chain Meta can be
used to obtain an exact sample according to the distribution @xmath with
expected running time that satisfies @xmath . Note that the running time
of this algorithm is random. To ensure that the algorithm terminates
with a probability at least @xmath , it is required to run it for an
additional factor of @xmath time (Huber, 1998 ) . In this way, we can
use this algorithm in conjunction with the approximation algorithm for
computing the partition function resulting in an FPRAS. The implication
of this result is that we only need to have an exact uniform sampler in
order to obtain exact samples from other distributions of interest.
\hide As we will see in the next section, designing an exact uniform
sampler is possible for several combinatorial structures that are of
importance in machine learning problems.

We conclude this section with a few remarks on the bound in Theorem
6.6.2 and its practical implications. At first glance, we may question
the usefulness of this bound because the constants @xmath and @xmath
appear in the exponent. But note that we can always set @xmath by
normalising the features. Also, the bound on @xmath (cf. Proposition 6.1
) could be loose in practice as observed recently by Do et al. ( 2009 )
, and thus the value of @xmath could be way below its upper bound @xmath
. We could then employ techniques similar to those described by Do
et al. ( 2009 ) to design optimisation strategies that work well in
practice. Also, note that the problem is mitigated to a large extent by
setting @xmath and @xmath .

While in this section we focused on designing a ‘meta’ approach for
sampling, we would like to emphasise that it is possible to derive
improved mixing time bounds by considering each combinatorial structure
individually. As an example, we design a Markov chain to sample from the
vertices of a hypercube and analyse its mixing time using path coupling.
Details are delegated to Appendix D.3 .

##### Mixing Time Analysis using Coupling \hide

We first consider Meta with an exact uniform sampler. We later
generalise it to the approximate uniform sampling case. We will use the
coupling lemma (cf. Lemma 6.6.1 ) in our analysis.We now analyse the
Markov chain Meta using coupling and the coupling lemma (cf. Lemma 6.6.1
). {theorem} The mixing time of Meta with an exact uniform sampler is
bounded from above as follows:

  -- -------- --
     @xmath   
  -- -------- --

{proof}

Using Cauchy-Schwarz and triangle inequalities, we have

  -- -------- --
     @xmath   
  -- -------- --

The probability of an update is

  -- -------- --
     @xmath   
  -- -------- --

The probability of not updating for @xmath steps is therefore less than
@xmath . Let

  -- -------- --
     @xmath   
  -- -------- --

We now only need to show that @xmath . Consider

  -- -- -------- --
        @xmath   
        @xmath   
        @xmath   
        @xmath   
  -- -- -------- --

The bound follows immediately from the Coupling Lemma. \hide We now
consider the case that there exists only an approximate uniform sampler
for the output space @xmath . Let @xmath be the stationary distribution
of a Markov chain that can be used to obtain approximate samples
uniformly at random, and let the total variation distance between @xmath
and uniform distribution @xmath be at most @xmath , i.e., @xmath .
Consider the following modification of the Markov chain Meta : If the
current state is @xmath , then

1.  select the next state @xmath uniformly at random, and

2.  move to @xmath with probability

      -- -------- --
         @xmath   
      -- -------- --

We are essentially running Metropolis-Hastings algorithm with @xmath as
the proposal density. We may not be able to compute @xmath , but we can
use the following lower bound in the second step of Meta :

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath denote the r.h.s. of the above inequality. The probability of
an update is now given as

  -- -------- --
     @xmath   
  -- -------- --

Note that the above expression is bounded from below by @xmath . Using
an analysis similar to the one described in the proof of Theorem 6.6.2
for the exact uniform sampler, we arrive at the following result:
{theorem} The mixing time of Meta with an approximate uniform sampler is
bounded from above as follows:

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then @xmath , and we recover Meta with an exact uniform
sampler. Note that in order to have a non-trivial bound, we need to
ensure that @xmath .

### 6.7 Summary

The primary focus of this chapter was to rigorously analyse
probabilistic structured prediction models using tools from MCMC theory.
We designed algorithms for approximating the partition function and the
gradient of the log partition function with provable guarantees. We also
presented a simple Markov chain based on Metropolis process that can be
used to sample according to exponential family distributions given that
there exists an exact uniform sampler. \hide While we were able to
design an exact uniform sampler for combinatorial structures like
vertices of a hypercube, permutations and subtrees of a tree (cf.
Section 4.3 ), we note that this may not be feasible in general for all
machine learning applications. In such cases, we can design a Markov
chain to obtain approximate samples from distributions of interest and
also bound its mixing time. This is possible using coupling technique.
Indeed, we show how to obtain approximate samples given that there
exists an exact uniform sampler and the analysis is given in \eq
sec:meta_coupling. We note that the coupling technique is much more
amenable than the coupling from the past technique to the problem of
obtaining approximate samples from a non-uniform distribution given only
approximate uniform samples.

If we were to solve the optimisation problem \eq eqn:opt using iterative
techniques like gradient descent, then we have to run Markov chain
simulations for every training example in order to compute gradients in
any iteration of the optimisation routine. We therefore argue for using
online convex optimisation techniques (Hazan et al., 2007 ;
Shalev-Shwartz et al., 2007 ) as these would result in fast, scalable
algorithms for structured prediction.

## Chapter 7 Conclusions

State-of-the-art approaches for structured prediction like structured
SVMs (Tsochantaridis et al., 2005 ) have the flavour of an algorithmic
template in the sense that if, for a given output structure, a certain
assumption holds, then it is possible to train the learning model and
perform inference efficiently. The standard assumption is that the
argmax problem

  -- -------- --
     @xmath   
  -- -------- --

is tractable for the output set @xmath . Thus, all that is required to
learn and predict a new structure is to design an algorithm to solve the
argmax problem efficiently in polynomial time. The primary focus of this
thesis was on predicting combinatorial structures such as cycles,
partially ordered sets, permutations, and several other graph classes.
We studied the limitations of existing structured prediction models
(Collins, 2002 ; Tsochantaridis et al., 2005 ; Taskar et al., 2005 ) for
predicting these structures. The limitations are mostly due to the
argmax problem being intractable. In order to overcome these
limitations, we introduced new assumptions resulting in two novel
algorithmic templates for structured prediction.

Our first assumption is based on counting combinatorial structures. We
proposed a kernel method that can be trained efficiently in polynomial
time and also designed approximation algorithms to predict combinatorial
structures. The resulting algorithmic template is a generalisation of
the classical regularised least squares regression for structured
prediction. It can be used to solve several learning problems including
multi-label classification, ordinal regression, hierarchical
classification, and label ranking in addition to the aforementioned
combinatorial structures.

Our second assumption is based on sampling combinatorial structures.
Based on this assumption, we designed approximation algorithms with
provable guarantees to compute the partition function of probabilistic
models for structured prediciton where the class conditional
distribution is modeled using exponential families. Our approach uses
classical results from Markov chain Monte Carlo theory (Jerrum and
Sinclair, 1996 ; Randall, 2003 ) . It can be used to solve several
learning problems including but not limited to multi-label
classification, label ranking, and multi-category hierarchical
classification.

We now point to some directions for future research. In Section 5.4 , we
designed approximation algorithms for solving the argmax problem for
combinatorial output sets, but could do so with only weak approximation
guarantees. While the approximation factor has no effect in training
models such as structured ridge regression (cf. Chapter 5 ), we believe
there is a need to further investigate the possibilities of designing
approximation algorithms with improved guarantees. We describe two
promising directions for future work below.

##### Approximate Inference using Simulated Annealing

If we restrict ourselves to linear models, then the argmax problem
reduces to a linear programming problem:

  -- -------- --
     @xmath   
  -- -------- --

Recently, Kalai and Vempala ( 2006 ) considered using simulated
annealing (Kirkpatrick et al., 1983 ) to minimise a linear function over
an arbitrary convex set. More precisely, they considered the following
linear minimisation problem: for a unit vector @xmath and a convex set
@xmath :

  -- -------- --
     @xmath   
  -- -------- --

under the assumption that there exists a membership oracle for the set
@xmath . The algorithm, described below, goes through a series of
decreasing temperatures as in simulated annealing, and at each
temperature, it uses a random walk based sampling algorithm (Lovász and
Vempala, 2006 ) to sample a point from the stationary distribution whose
density is proportional to @xmath , where @xmath is the current
temperature. Let @xmath be an an upper bound on the radius of a ball
that contains the convex set @xmath , and @xmath be a lower bound on the
radius of a ball around the starting point contained in @xmath . At
every temperature (indexed by @xmath ), the algorithm performs the
following steps:

1.  Set the temperature @xmath .

2.  Move the current point to a sample from a distribution whose density
    is proportional to @xmath , obtained by executing @xmath steps of a
    biased random walk (refer (Lovász and Vempala, 2006 ) for details).

3.  Using @xmath ¹ ¹ 1 The @xmath notation hides logarithmic factors.
    samples observed during the above walk and estimate the covariance
    matrix of the above distribution.

{theorem}

(Kalai and Vempala, 2006 ) For any convex set @xmath , with probability
@xmath , the above procedure given a membership oracle for the convex
set @xmath , starting point @xmath , @xmath , @xmath , @xmath , @xmath ,
and @xmath , outputs a point @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

The total number of calls to the membership oracle is @xmath .

While the combinatorial structures considered in this work do not form a
convex set, it would be interesting to consider the convex hull of such
sets, for instance, the convex hull of the vertices of a hypercube and
derive results similar in spirit to Theorem 7 . For sampling, we can use
the algorithms described in Section 6.6 to sample from exponential
family distributions.

##### Approximate Inference using Metropolis Process

The Metropolis process (Metropolis et al., 1953 ) can be seen as a
special case of simulated annealing with fixed temperature, i.e., the
Markov chain on the state space @xmath is designed to be
time-homogeneous . This chain can be used to maximise (or minimise)
objective functions @xmath defined on the combinatorial set @xmath
(Jerrum and Sinclair, 1996 ) . Consider the following chain @xmath which
is similar to the ‘meta’ Markov chain described in Section 6.6 . If the
current state is @xmath , then

1.  select the next state @xmath uniformly at random, and

2.  move to @xmath with probability @xmath ,

where @xmath is a fixed parameter. The stationary distribution of this
chain is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the partition function.

Note that @xmath is a monotonically increasing function of @xmath as
desired. It is now crucial to choose the parameter @xmath appropriately.
Having a small @xmath would make the distribution @xmath well
concentrated whereas having a large @xmath would make the chain less
mobile and susceptible to locally optimal solutions. Note than we can
analyse the mixing time of this chain using the techniques described in
Section 6.6 . The probability of finding the optimal solution is at
least the weight of such solutions in the stationary distribution @xmath
. Therefore, if we can derive non-trivial upper bounds on the weights of
optimal solutions, then we can be certain of hitting such solutions
using the chain with high probability. The success probability can be
boosted by running the chain multiple times. Choosing an appropriate
@xmath and deriving non-trivial upper bounds on the the weights of
optimal solutions is an open question. Note that if we set @xmath , we
are essentially sampling from exponential family distributions.

\hide

### 7.1 Learning with Minimal Supervision

In supervised machine learning, if labeling training data is expensive,
then it becomes imperative to exploit information present in auxiliary
data sources. To this end, several new paradigms of learning like
semi-supervised learning ( Blum/Mitchell/98 ) , transfer learning (
Thrun95 ) , multi-task learning ( Caruana97 ) , and active learning (
Angluin87 ) have produced a plethora of learning algorithms that use
unlabeled data and data from multiple tasks. A natural extension of our
results described in Chapter 6 would be to design a semi-supervised
learning algorithm using hybrids of generative and discriminative models
( Lasserre/etal/06 ; Agarwal/Daume/09 ) . Also, having a probabilistic
model for structured prediction makes it appealing to design
semi-supervised and multi-task learning algorithms using the machinery
of Bayesian statistics (see, for example, ( Xue/etal/07 ) ).

### 7.2 Large-scale Learning

While there has been a considerable amount of work in scaling up binary
classifiers for large data sets ( Langford/etal/07 ; Shalev-Shwartz
et al., 2007 ) , research on large-scale structured prediction is still
in its infancy. Investigating online convex optimisation techniques
(Hazan et al., 2007 ; Shalev-Shwartz et al., 2007 ) for training
probabilistic models for structured prediction (cf. Chapter 6 ) is an
interesting direction for future research.

\hide

## Appendix A Concentration Inequalities

We state the concentration inequalities used in Chapter 6 . \hide
{theorem} (Cauchy-Schwarz’s Inequality) For vectors @xmath in an inner
product space, @xmath . {theorem} (Chebyshev’s Inequality) Let @xmath be
a random variable with expected value @xmath and variance @xmath . Then
for any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

{theorem}

(Hoeffding’s Inequality ( Hoeffding63 ) ) Let @xmath be @xmath
independent random variables such that @xmath with probability @xmath
for all @xmath , where @xmath and @xmath are real constants. Then, the
sum of these random variables @xmath satisfies the following
inequalities for any @xmath :

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

## Appendix B Kernels and Low-dimensional Mappings

The Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984 ;
Dasgupta and Gupta, 1999 ) states that any set of @xmath points in high
dimensional Euclidean space can be mapped to an @xmath Euclidean space
such that the distance between any pair of points are preserved within a
( @xmath ) factor, for any @xmath . Such an embedding is constructed by
projecting the @xmath points onto a random @xmath -dimensional
hypersphere. A similar result was proved by Arriaga and Vempala ( 1999 )
and Balcan et al. ( 2006 ) in the context of kernels. Given a kernel
matrix @xmath in the (possibly) infinite dimensional @xmath -space, it
is possible to compute a low-dimensional mapping of the data points that
approximately preserves linear separability. Specifically, if the data
points are separated with a margin @xmath in the @xmath -space, then a
random linear projection of this space down to a space of dimension
@xmath will have a linear separator of error at most @xmath with
probability at least @xmath .

One of the drawbacks of using this random projection method for
low-dimensional mapping of kernels is the need to store a dense random
matrix of size @xmath which is prohibitively expensive for large-scale
applications. To circumvent this problem, one could resort to Nyström
approximation of the eigenvectors and eigenvalues of the kernel matrix
using random sampling. This technique is a fast, scalable version of the
classical multi-dimensional scaling (MDS) (see (Platt, 2005 ) and
references therein), and is referred to as NMDS in the following.

Let @xmath be a positive definite matrix. Choose @xmath samples (or data
points) at random, re-order the matrix such that these samples appear at
the beginning, and partition it as

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath , @xmath be the dimension of the embedding, @xmath and @xmath
be the submatrices corresponding to the @xmath largest eigenvectors and
eigenvalues respectively. Then the low-dimensional embedding (matrix) is

  -- -------- --
     @xmath   
  -- -------- --

The computational complexity of this approach is @xmath .

The NMDS approach cannot be applied if the input matrix is indefinite.
For example, in learning on graphs, the input is a sparse adjacency
matrix but not a kernel. In such cases, we would still like to find a
low-dimensional embedding of this graph without the need to compute a
kernel as an intermediate step. Note that if we use the (normalised)
Laplacian as the input, then the algorithm would output the leading
eigenvectors, but we need to compute the trailing eigenvectors of the
Laplacian. Fowlkes et al. ( 2004 ) proposed a two-step approach to solve
the problem even when the input matrix is indefinite. First, NMDS is
applied to the input matrix. Let @xmath , @xmath and let @xmath denote
the orthogonalisation of @xmath . Then the matrix @xmath contains the
leading orthonormalised approximate eigenvectors of @xmath and the
low-dimensional embedding is given by @xmath . The computational
complexity is still in the order of @xmath , but with an additional
@xmath orthogonalisation step.

## Appendix C Counting Dicyclic Permutations

We represent a directed cyclic permutation by the set of (predecessor,
successor)-pairs. For instance, the sequences @xmath are equivalent and
we use @xmath to represent them. Furthermore, we define @xmath if @xmath
, i.e., @xmath follows directly after @xmath in @xmath ; @xmath if
@xmath , i.e., @xmath follows directly after @xmath in @xmath ; and
@xmath otherwise.

For a given alphabet @xmath , we are now interested in computing @xmath
, the number of cyclic permutations of subsets of @xmath . For a subset
of size @xmath there are @xmath permutations of which @xmath represent
the same cyclic permutation. That is, there are @xmath cyclic
permutations of each subset of size @xmath , and for an alphabet of size
@xmath there are

  -- -------- --
     @xmath   
  -- -------- --

different cyclic permutations of subsets of @xmath .

Computing @xmath is simple. Observe that, for each cyclic permutation
containing a (predecessor, successor)-pair @xmath , there is also
exactly one cyclic permutation containing @xmath . Hence the sum over
each feature is zero and @xmath (where @xmath is the vector of all
zeros).

It remains to compute @xmath . Each element of this matrix is computed
as

  -- -------- --
     @xmath   
  -- -------- --

As seen above, for @xmath and @xmath there are as many cyclic
permutations for which @xmath as there are cyclic permutations for which
@xmath . In both cases, @xmath , and to compute @xmath it suffices to
compute the number of cyclic permutations containing @xmath or @xmath .
There are @xmath different cyclic permutations of each subset of size
@xmath that contain @xmath . We thus have

  -- -------- --
     @xmath   
  -- -------- --

Similarly, for @xmath and @xmath , we have @xmath and

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , we observe that there are as many cyclic permutations
containing @xmath and @xmath as there are cyclic permutations containing
@xmath and @xmath , and hence in this case @xmath . Finally, we need to
consider @xmath , @xmath , @xmath , and @xmath . Here we have

  -- -------- --
     @xmath   
  -- -------- --

## Appendix D Appendix for Chapter 6

### d.1 Approximating the Partition Function using Approximate Samples

In Section 6.3 , we designed an FPRAS for approximating the partition
function under the assumption that there exists an exact sampler. We now
consider the case where we only have approximate samples resulting from
a truncated Markov chain. Let @xmath denote the simulation length of the
Markov chains, for all @xmath . The main result of this section is as
follows: {theorem} Suppose the sample size @xmath and suppose the
simulation length @xmath is large enough that the variation distance of
the Markov chain from its stationary distribution @xmath is at most
@xmath . Under the assumption that the chain is rapidly mixing, there
exists an FPRAS with @xmath as the error parameter for computing the
partition function. {proof} The proof again uses techniques described in
(Jerrum and Sinclair, 1996 ) . The bound @xmath (from Proposition 6.3 )
w.r.t. the random variable @xmath will play a central role in the proof.
We cannot use this bound per se to prove approximation guarantees for
the partition function @xmath . This is due to the fact that the random
variable @xmath is defined w.r.t. the distribution @xmath , but our
samples are drawn from a distribution @xmath resulting from a truncated
Markov chain, whose variation distance satisfies @xmath . \hide

  -- -------- --
     @xmath   
  -- -------- --

Therefore, we need to obtain a bound on @xmath w.r.t. the random
variable @xmath defined analogously to @xmath with samples drawn from
the distribution @xmath . An interesting observation is the fact that
Lemma 6.3 still holds for @xmath , i.e., @xmath \hide

  -- -------- --
     @xmath   
  -- -------- --

for all integers @xmath , and using similar analysis that followed Lemma
6.3 , we get

  -- -------- --
     @xmath   
  -- -------- --

Also, note that @xmath implies @xmath (using the fact that @xmath ).
Therefore,

  -- -------- -- -------
     @xmath      (D.1)
  -- -------- -- -------

Equipped with these results, we are ready to compute the sample size
@xmath needed to obtain the desired approximation guarantee in the
FPRAS. Let @xmath be a sequence of @xmath independent copies of the
random variable @xmath obtained by sampling from the distribution @xmath
, and let @xmath be the sample mean. We have @xmath , and @xmath . The
final estimator @xmath is the random variable @xmath with @xmath . From
\eq eqn:rho_bounds, we have

  -- -------- -- -------
     @xmath      (D.2)
  -- -------- -- -------

Now, consider

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

if we choose @xmath (because @xmath for @xmath ). By applying
Chebyshev’s inequality to @xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

and therefore, with probability at least @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Combining the above result with \eq eqn:rhohat_bounds, we see that with
probability at least @xmath , the partition function @xmath lies within
the ratio @xmath of @xmath . Polynomial run time follows from the
assumption that the Markov chain is rapidy mixing.

### d.2 Approximating the Gradient of the Log Partition Function using a
Reduction from Counting to Sampling

In this section, we show how to approximate the gradient of the log
partition function using the reduction from counting to sampling
described in Section 6.3 . Recall that the gradient of the log partition
function generates the first order moment (mean) of @xmath , i.e.,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

The numerator in the above expression is the quantity of interest and it
can be seen as a weighted variant of the partition function @xmath where
the weights are the features @xmath . We will use @xmath to denote the
@xmath th component of @xmath and let @xmath . Consider again the random
variable @xmath , where @xmath is now chosen according to the
distribution

  -- -------- --
     @xmath   
  -- -------- --

i.e, we use the same random variable as defined in Section 6.3 , but
sample according to a slightly different distribution as given above. It
is easy to verify that @xmath is an unbiased estimator for the ratios in
the telescoping product of the quantity of interest, i.e,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the length of the cooling schedule (cf. Section 6.3 ).
It remains to analyse the mixing time of the Markov chain with
stationary distribution @xmath . Since features can take the value of
zero, Theorems 6.6.2 and 6.6.2 cannot be applied. One solution to this
problem would be to modify the state space of the Markov chain in such a
way that we sample (uniformly) only those structures satisfying @xmath ,
where @xmath is a parameter, and then run the Metropolis process. It
would be interesting to further analyse the approximation that is
introduced by discarding all those structures satisfying @xmath , but we
do not focus on this aspect of the problem here.

A note on computational issues follows. At first glance, it may seem
computationally inefficient to run the machinery for every feature, but
note that it is possible to reuse the computations of individual ratios
of the telescoping product by designing an appropriate cooling schedule.
First, consider the following expression:

  -- -------- -------- --
              @xmath   
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

where @xmath . Let @xmath . The cooling schedule is then given as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath .

### d.3 Mixing Time Analysis of @xmath using Path Coupling

The state space @xmath is the vertices of the @xmath -dimensional
hypercube @xmath . Consider the following Markov chain @xmath on @xmath
. Initialise at @xmath and repeat the following steps: (i) pick @xmath ,
and (ii) move to the next state, formed by changing @xmath th bit to
@xmath , with probability @xmath . Let @xmath denote the Hamming
distance. The transition probabilities of this chain are given by

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

We now analyse the mixing time of @xmath using path coupling (cf.
Section 6.6.1 ). Recall that the first step in using path coupling is to
define a coupling on pairs of states that are close to each other
according to some distance metric @xmath on the state space. {theorem}
The Markov chain @xmath on the vertices of a @xmath -dimensional
hypercube has mixing time @xmath . {proof} We will first prove the bound
on the mixing time for uniform stationary distribution and later
generalise it to arbitrary distributions.

We choose Hamming distance as the metric, and consider pairs of states
@xmath that differ by a distance of @xmath . To couple, we choose @xmath
uniformly at random, and then update to @xmath formed by changing @xmath
th bit to @xmath if possible. We now need to show that the expected
change in distance due to this update never increases. More precisely,
we need to prove that

  -- -------- --
     @xmath   
  -- -------- --

and invoke the path coupling lemma (cf. Lemma 6.6.1 ) to obtain the
final result. Let @xmath and @xmath differ at the @xmath -th bit. We
have the following two cases.

-   Case 1: @xmath . In this case, if @xmath , then there is no update
    and therefore no change in distance. If @xmath , then both @xmath
    and @xmath update their @xmath -th bit and therefore, again, there
    is no change in distance.

-   Case 2: @xmath . In this case, there is definitely a decrease in
    distance with probability @xmath as one of @xmath or @xmath (but not
    both) will update their @xmath -th bit.

We therefore have

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

with @xmath as desired. Invoking the path coupling lemma gives us the
following bound on the mixing time

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath be the new state formed in cases 1 and 2 above. For
non-uniform distributions, we have the following:

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

Note that @xmath differs from @xmath and @xmath by a single bit, and
therefore under the assumptions that

  -- -------- -------- --
     @xmath   @xmath   
     @xmath   @xmath   
  -- -------- -------- --

we have, once again,

  -- -------- --
     @xmath   
  -- -------- --

with @xmath .

\hide

### d.4 Vertices of a hypercube / Path Coupling

The state space @xmath is the vertices of the @xmath -dimensional
hypercube @xmath . Consider the following Markov chain @xmath on @xmath
. Initialise at @xmath and repeat the following steps: (i) Pick @xmath ,
(ii) Move to the next state, formed by changing @xmath th bit to @xmath
, with probability @xmath . Let @xmath denote the Hamming distance. The
transition probabilities of this chain are given by

  -- -------- -------- --
     @xmath   @xmath   
  -- -------- -------- --

A bound on the mixing time for @xmath with uniform stationary
distribution was given by Randall ( 2003 ) using canonical paths.
{lemma} (Randall, 2003 ) The mixing time @xmath of @xmath is bounded
from above as follows:

  -- -------- --
     @xmath   
  -- -------- --

We now generalise this result for arbitrary distributions. {theorem} The
mixing time @xmath of @xmath is bounded from above as follows:

  -- -------- --
     @xmath   
  -- -------- --

{proof}

Let @xmath be an arbitrary edge. The capacity of this edge

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
              @xmath   
              @xmath   
  -- -------- -------- --

Now consider @xmath . The number of terms in this summation is equal to
the number paths passing through @xmath which is @xmath . Therefore,

  -- -------- -------- --
     @xmath   @xmath   
              @xmath   
  -- -------- -------- --

By noting that the length of any path is at most @xmath , we have @xmath
. We now apply Theorem 6.6.1 to get

  -- -------- --
     @xmath   
  -- -------- --