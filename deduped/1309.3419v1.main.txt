##### Contents

-    0 Introduction
    -    0.1 The model
    -    0.2 Informal description of the results
    -    0.3 Discussion of this work
    -    0.4 A brief history
    -    0.5 Open problems for our model and ongoing work
-    1 Basic notation and main results
    -    1.1 Basic notation
    -    1.2 Main results on exit laws
    -    1.3 Main results on mean sojourn times
    -    1.4 Perturbation expansion
    -    1.5 A short reading guide
-    2 Coarse graining schemes and notion of badness
    -    2.1 Coarse graining schemes in the ball
    -    2.2 Good and bad points
    -    2.3 Bad regions in the case @xmath
    -    2.4 Bad regions when @xmath is a constant
-    3 Some important estimates
    -    3.1 Hitting probabilities
    -    3.2 Smoothed exit measures
-    4 Green’s functions for the ball
    -    4.1 A local central limit theorem
    -    4.2 Estimates on coarse grained Green’s functions
    -    4.3 Difference estimates
    -    4.4 Modified transitions on environments bad on level @xmath
-    5 Globally smoothed exits
    -    5.1 Estimates on “goodified” environments
    -    5.2 Estimates in the presence of bad points
-    6 Non-smoothed and locally smoothed exits
-    7 Proofs of the main results on exit laws
-    8 Mean sojourn times in the ball
    -    8.1 Preliminaries
    -    8.2 Good and bad points
    -    8.3 Estimates on mean times
-    9 Proofs of the main results on sojourn times
-    10 Appendix
    -    10.1 Some difference estimates
    -    10.2 Proof of Lemma 3.5
    -    10.3 Proof of Lemma 3.2
    -    10.4 Proofs of Propositions 4.1 and 4.2

## 0 Introduction

### 0.1 The model

#### General description

Consider the integer lattice @xmath with unit vectors @xmath , whose
@xmath th component equals @xmath . We let @xmath be the set of
probability distributions on @xmath . Given a probability measure @xmath
on @xmath , we equip @xmath with its natural product @xmath -field
@xmath and the product measure @xmath . Each element @xmath yields
transition probabilities of a nearest neighbor Markov chain on @xmath ,
the random walk in random environment (RWRE for short), via

  -- -------- --
     @xmath   
  -- -------- --

We write @xmath for the “quenched” law of the canonical Markov chain
@xmath with these transition probabilities, starting at @xmath . The
probability measure

  -- -------- --
     @xmath   
  -- -------- --

is commonly referred to as averaged or “annealed” law of the RWRE
started at the origin.

#### Additional requirements

We study asymptotic properties of the RWRE in dimension @xmath when the
underlying environments are small perturbations of the fixed environment
@xmath corresponding to simple or standard random walk. In order to fix
a perturbative regime, we introduce the following condition.

-   Let @xmath . We say that @xmath holds if @xmath , where

      -- -------- --
         @xmath   
      -- -------- --

The perturbative behavior concerns the behavior of the RWRE when @xmath
holds for small @xmath . However, even for arbitrarily small @xmath ,
such walks can behave very differently compared to simple random walk.
This motivates a further “centering” restriction on @xmath .

-   We say that @xmath holds if @xmath is invariant under all orthogonal
    transformations fixing the lattice @xmath , i.e. if @xmath is any
    orthogonal matrix that maps @xmath onto itself, then the laws of
    @xmath and @xmath coincide.

If @xmath holds, @xmath is called isotropic .

### 0.2 Informal description of the results

In the following, we write @xmath instead of @xmath and denote by @xmath
the corresponding expectation.

#### Exit laws from balls

In the main part of this work, we investigate the RWRE exit distribution
from the ball @xmath when the radius @xmath is large. Assuming @xmath
and @xmath for small @xmath , we show that the exit law of the walk,
started from a point @xmath with @xmath , is close to that of simple
random walk. More precisely, using the multiscale analysis introduced in
Bolthausen and Zeitouni [ 6 ] , we prove that if the radius @xmath tends
to infinity, then

1.  The difference of the two exit laws measured in total variation
    stays small as @xmath increases (but does not tend to zero, due to
    boundary effects) (Theorem 1.1 (i)).

2.  The distance between the two exit laws converges to zero if they are
    convolved with an additional smoothing kernel on a scale increasing
    arbitrarily slowly with @xmath (Theorem 1.1 (ii)).

3.  The RWRE exit measure of boundary portions of size @xmath can be
    bounded from above by that of simple random walk. Evaluated on
    segments of size @xmath , the two measures agree up to a
    multiplicative error, which tends to one as @xmath increases
    (Theorem 1.2 ).

The first two parts already appeared in [ 6 ] , which serves as the
basis for our work. However, for reasons explained below, it was of
great interest to find a somewhat different approach.

#### Mean sojourn times

The results on exit laws can be used to prove transience of the RWRE
(Corollary 1.1 ), and they provide an invariance principle up to time
transformation. Getting complete control over time is a major open
problem, and in that direction, we look in Section 8 at mean holding or
sojourn times in balls. Our basic insight is that exceptionally small or
large times can only be produced by spatially atypical regions.
Consequently, the philosophy behind our approach is to derive statements
on sojourn times from estimates on exit laws. However, our results on
exit distributions seem not quite sufficient to handle the presence of
strong traps, i.e. regions where the RWRE cannot escape for a long time
with high probability. We therefore make an additional assumption which
guarantees that the mass of environments producing very large times is
sufficiently small. Let @xmath be the first exit time of the RWRE from
the ball @xmath , and denote by @xmath the expectation with respect to
@xmath .

-   We say that A2 holds if for large @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

Assuming this additional condition, we prove

1.  For almost all environments, the normalized quenched mean time
    @xmath is finally contained in a small interval around one, where
    the size of the interval converges to zero if the disorder @xmath
    tends to zero (Proposition 1.2 and Theorem 1.3 ).

We believe that A2 follows from @xmath and @xmath , even with a faster
decay of the probability. It remains an open (and possibly challenging)
problem to prove this. An example where A2 trivially holds true is given
in Remark 8.1 .

### 0.3 Discussion of this work

The part on exit measures should be seen as a corrected and extended
version of Bolthausen and Zeitouni [ 6 ] . Most of the ideas can already
be found there, and also our proofs sometimes follow those of [ 6 ] .
However, our focus lies more on Green’s function estimates on
“goodified” environments, which are developed in Section 4 . Partly
based on (unpublished) notes of Bolthausen, this section is entirely
new, and the results obtained make the proofs of the main statements
more transparent. The core statement is Lemma 4.1 , which gives a bound
on the (coarse grained) RWRE Green’s function, for a large class of
environments. As such estimates were only partially present in [ 6 ] ,
the authors had to repeatedly consider higher order expansions in terms
of Green’s functions coming from simple random walk, which led to
serious problems, for example in Sections 4.3 and 4.4 in [ 6 ] .

The reason for developing a new approach was twofold: On the one hand,
it seemed difficult to fix these problems ad hoc. On the other hand, we
aimed at establishing a solid basis for future work on this topic, in
particular in the direction of a central limit theorem. Further new
points of this work can be summarized as follows.

-   We give either new proofs of the statements in [ 6 ] or we revise
    the old ones. For example, the proofs leading to the main results on
    the exit measures in Sections 5 and 6 are based on our new
    techniques. These include the bounds on Green’s functions, the use
    of parametrized coarse graining schemes and the concept of goodified
    environments, which goes back to [ 6 ] and is further elaborated
    here.

-   The appendix is completely rewritten. In this part, the main
    corrections concern the proof of the key Lemma 3.2 (Lemma 3.4 in [ 6
    ] ), where different case had to be considered. Also, we provide a
    lower bound on exit probabilities (Lemma 3.2 (iii)), which was
    already implicitly used in [ 6 ] , but not proved.

-   We obtain local estimates for the exit measures (Theorem 1.2 ). The
    global estimates in total variation distance are extended to
    starting points @xmath .

-   The results on the exit distributions are used to control the mean
    sojourn time of the RWRE in balls, under an extra assumption on
    @xmath .

To improve readability, we overview the main steps of this work in
Section 1.5 .

### 0.4 A brief history

The literature on random walks in random environment is vast, and we do
by no means intend to give a full overview here. Instead, we point at
some cornerstones and focus on results which are relevant for our
particular model. For a more detailed survey, the reader is invited to
consult the lecture notes of Sznitman [ 30 ] , [ 32 ] and Zeitouni [ 38
] , [ 39 ] , and also the overview article of Bogachev [ 7 ] .

Recall the general model defined at the very beginning under “General
description”. We additionally assume that the environment is uniformly
elliptic , i.e. there exists @xmath such that @xmath -almost surely,
@xmath for all @xmath , @xmath . Note that in the perturbative regime,
this is automatically true.

The natural condition of uniform ellipticity can sometimes be relaxed to
mere ellipticity @xmath for @xmath , @xmath . Also, it often suffices to
require @xmath to be stationary and ergodic instead of being “i.i.d.”.

#### Dimension @xmath

Early interest in models of RWRE can be traced back to the 60’s in the
context of biochemistry, where they were used as a toy model for DNA
replication, cf. Chernov [ 9 ] and Temkin [ 35 ] . Solomon [ 27 ]
started a rigorous mathematical analysis in dimension @xmath . He proved
that if

  -- -------- --
     @xmath   
  -- -------- --

then the RWRE is @xmath -almost surely transient, whereas in the case
@xmath , the walk is @xmath -a.s. recurrent. Further, he obtained almost
sure existence of the limit speed @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

His results already reveal some surprising features of the model. For
example, it can happen that @xmath , but nonetheless the RWRE is
transient (note that this is impossible for a Markov chain with
stationary increments, according to Kesten [ 18 ] ). Also, if @xmath
denotes the mean local drift, it is possible that @xmath . Such slowdown
effects, caused by traps reflecting impurities in the medium, come again
to light in limit theorems for the RWRE under both the quenched and the
annealed measure. In [ 20 ] , Kesten, Kozlov and Spitzer proved that in
the transient case under the annealed law, both diffusive and
sub-diffusive behavior can occur, depending on a critical exponent
connected to hitting times. However, the strongest form of
sub-diffusivity appears in the recurrent case with non-degenerate site
distribution @xmath , for which Sinai [ 26 ] proved that after @xmath
steps, the RWRE is typically at distance of order only @xmath away from
the starting point. His analysis shows that the walk spends most of the
time at the bottom of certain valleys. The limit law of @xmath is given
by the distribution of a functional of Brownian motion, cf. Kesten [ 19
] and Golosov [ 14 ] . Let us finally mention that slowdown phenomena
also show up when studying probabilities of atypical events like large
deviations, see e.g. [ 15 ] , [ 11 ] , [ 13 ] .

#### Dimensions @xmath

While the one-dimensional picture is quite complete, many questions
remain open in higher dimensions, including a classification into
recurrent/transient behavior, existence of a limit speed and invariance
principles. The main difficulties come from the non-Markovian character
under the annealed measure and the fact that the RWRE is irreversible
under the quenched measure as soon as @xmath .

Let us illustrate one prominent open problem, the directional zero-one
law. For an element @xmath from the unit sphere @xmath , denote the
event that the RWRE is transient in direction @xmath by

  -- -------- --
     @xmath   
  -- -------- --

Kalikow proved in [ 17 ] that @xmath . Is it also true that @xmath ? The
answer is affirmative in dimension @xmath ( [ 27 ] for @xmath , Merkl
and Zerner [ 25 ] for @xmath ), but unknown for higher dimensions. It is
known that a limit speed @xmath (possibly zero) exists if @xmath for
every @xmath , cf. Sznitman and Zerner [ 34 ] .

Much progress has been made in characterizing models which exhibit
ballistic behavior, that is when the limit velocity @xmath is an almost
sure constant vector different from zero. Here Sznitman’s conditions
@xmath , @xmath , give a criterion for ballisticity and lead to an
invariance principle under the annealed measure @xmath , see Sznitman [
28 ] , [ 29 ] and also his lecture notes [ 32 ] . When @xmath and the
disorder is small, a quenched invariance principle has been shown by
Bolthausen and Sznitman [ 4 ] . A stronger ballisticity condition was
given earlier by Kalikow [ 17 ] . However, as examples in Sznitman [ 31
] demonstrate, Kalikow’s condition does not completely describe
ballistic behavior in dimensions @xmath . A handy and complete
characterization of ballisticity has still to be found. For recent
developments, see the work of Berger [ 1 ] and Berger, Drewitz, Ramírez
[ 2 ] . In [ 2 ] , it is conjectured that in dimensions @xmath , a RWRE
which is transient in all directions @xmath out of an open subset @xmath
is ballistic (for an i.i.d uniformly elliptic environment).

Turning to ballistic behavior in the perturbative regime, Sznitman shows
in [ 31 ] that for @xmath in dimension @xmath or for @xmath in
dimensions @xmath , there exists @xmath such that if @xmath is fulfilled
for some @xmath and the mean local drift under the static measure
satisfies

  -- -------- --
     @xmath   
  -- -------- --

then the RWRE is ballistic in direction @xmath , i.e. @xmath . Moreover,
a functional limit theorem holds under @xmath . In [ 5 ] , Bolthausen,
Sznitman and Zeitouni consider RWRE in dimensions @xmath where the
projection onto at least five components behaves as simple random walk.
Among other things, examples are constructed under @xmath for which
@xmath , but @xmath @xmath , and a quenched invariance principle is
proved when @xmath . On the other hand, it can happen that @xmath but
@xmath . As a further remarkable result of [ 5 ] , it can even happen
that @xmath for some @xmath , which exemplifies that the environment
acts on the path of the walk in a highly nontrivial way. Large
deviations of @xmath are studied in Varadhan [ 36 ] .

Concerning non-ballistic behavior, much is known for the class of
balanced RWRE when @xmath for all @xmath . One first notices that the
walk is a martingale, which readily leads to limit speed zero. Employing
the method of environment viewed from the particle, Lawler proves in [
22 ] that for @xmath -almost all @xmath , @xmath converges in @xmath
-distribution to a non-degenerate Brownian motion with diagonal
covariance matrix. Moreover, the RWRE is recurrent in dimension @xmath
and transient when @xmath , see [ 38 ] . Recently, within the i.i.d.
setting, diffusive behavior has been shown in the mere elliptic case by
Guo and Zeitouni [ 16 ] and in the non-elliptic case by Berger and
Deuschel [ 3 ] .

Our study of random walks in random environment in the perturbative
regime under the isotropy condition @xmath aims at a quenched central
limit theorem, showing that in dimensions @xmath , the RWRE is
asymptotically Gaussian, on @xmath -almost all environments @xmath .
Such an invariance principle has already been shown by Bricmont and
Kupiainen [ 8 ] , who introduced condition @xmath . However, it is of
interest to find a self-contained new proof. A continuous counterpart of
this model, isotropic diffusions in a random environment which are small
perturbations of Brownian motion, has been investigated by Sznitman and
Zeitouni in [ 33 ] . They prove transience and a full quenched
invariance principle in dimensions @xmath .

### 0.5 Open problems for our model and ongoing work

As we already pointed out above, with respect to a central limit theorem
one still needs to find ways to handle large times, which are in a
certain sense excluded by Assumption A2 . In this direction, a more
complete picture of exit laws could prove helpful, including sharper
estimates for the appearance of balls with an atypical exit measure. A
further task is to combine space and time estimates in the right way.

In the direction of a fully perturbative theory it would be desirable to
replace the isometry condition @xmath by the requirement that @xmath is
just invariant under reflections mapping a unit vector to its inverse.
Then the RWRE exit law from a ball should be close to that of some
@xmath -dimensional symmetric random walk. The relaxed condition on
@xmath would, for example, include the class of walks that are balanced
in one coordinate direction, where time can be controlled much easier.
This is work in progress.

Another open problem is the case of small isotropic perturbations in
dimension @xmath . One expects diffusive behavior as in dimensions
@xmath , but there is no rigorous result yet. In principle, one might
try to follow a similar multiscale approach for the exit measures as it
is presented below. But the same perturbation argument shows that unlike
dimensions @xmath , the disorder does not contract in leading order.
Therefore, one has to look closer at higher order terms. While for
@xmath , the nonlinear terms in the perturbation expansion for the
Green’s function can be estimated in a somewhat crude way once the right
scales are found, it seems that in dimension @xmath , at least terms up
to order three have to be carefully taken into account.

## 1 Basic notation and main results

### 1.1 Basic notation

Our purpose here is to cover the most relevant notation which will be
used throughout this text. Further notation will be introduced later on
when needed.

#### Sets and distances

We have @xmath and @xmath . For a set @xmath , its complement is denoted
by @xmath . If @xmath is measurable and non-discrete, we write @xmath
for its @xmath -dimensional Lebesgue measure. Sometimes, @xmath denotes
the surface measure instead, but this will be clear from the context. If
@xmath , then @xmath denotes its cardinality.

For @xmath , @xmath is the Euclidean norm. If @xmath , we set @xmath and
@xmath . Given @xmath , let @xmath , and for @xmath , @xmath . For
Euclidean balls in @xmath we write @xmath and for @xmath , @xmath .

If @xmath , then @xmath is the outer boundary, while in the case of a
non-discrete set @xmath , @xmath stands for the usual topological
boundary of @xmath and @xmath for its closure. For @xmath , we set
@xmath . Finally, for @xmath , the “shell” is defined by

  -- -------- --
     @xmath   
  -- -------- --

#### Functions

If @xmath are two real numbers, we set @xmath , @xmath . The largest
integer not greater than @xmath is denoted by @xmath . As usual, set
@xmath . For us, @xmath is the logarithm to the base @xmath . For @xmath
, the Delta function @xmath is defined to be equals one for @xmath and
zero otherwise. If @xmath is a set, then @xmath is the probability
distribution on the subsets of @xmath satisfying @xmath if @xmath and
zero otherwise.

Given two functions @xmath , we write @xmath for the (matrix) product
@xmath , provided the right hand side is absolutely summable. @xmath is
the @xmath th power defined in this way, and @xmath . @xmath can also
operate on functions @xmath from the left via @xmath .

We use the symbol @xmath for the indicator function of the set @xmath .
By an abuse of notation, @xmath will also denote the kernel @xmath . If
@xmath , @xmath is its @xmath -norm. When @xmath is a (signed) measure,
@xmath is its total variation norm.

Let @xmath be a bounded open set, and let @xmath . For a @xmath -times
continuously differentiable function @xmath , that is @xmath , we define
for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where the first supremum is over all multi-indices @xmath , @xmath ,
with @xmath . Let @xmath . Putting @xmath , we define

  -- -------- --
     @xmath   
  -- -------- --

We will mostly interpret functions @xmath as maps from @xmath . A
typical function we have in mind is the constant function @xmath .

#### Transition probabilities and exit distributions

Given (not necessarily nearest neighbor) transition probabilities @xmath
, we write @xmath for the law of the canonical Markov chain @xmath on
@xmath , @xmath the @xmath -algebra generated by cylinder functions,
with transition probabilities @xmath and starting point @xmath @xmath
-a.s. The expectation with respect to @xmath is denoted by @xmath . We
will often consider the simple random walk kernel @xmath .

If @xmath , we denote by @xmath the first exit time from @xmath , with
@xmath , whereas @xmath is the first hitting time of @xmath . Given
@xmath and @xmath as above, we define

  -- -------- --
     @xmath   
  -- -------- --

Notice that for @xmath , @xmath . For simple random walk, we write

  -- -------- --
     @xmath   
  -- -------- --

Given @xmath , we set

  -- -------- --
     @xmath   
  -- -------- --

Here, @xmath should be understood as a random exit distribution, but we
suppress @xmath in the notation.

#### Coarse grained random walks

In order to transfer information about both exit measures and sojourn
times from one scale to the next, we work with coarse graining schemes.

Fix once for all a probability density @xmath with compact support in
@xmath . Given a nonempty subset @xmath , @xmath and @xmath , the image
measure of the rescaled density @xmath under the mapping @xmath defines
a probability distribution on (finite) sets containing @xmath . If
@xmath is a field of positive numbers, we obtain in this way a
collection of probability distributions indexed by @xmath , a coarse
graining scheme on @xmath .

Now if @xmath is a collection of transition probabilities on @xmath , we
define the coarse grained transitions belonging to @xmath by

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

If @xmath , we write @xmath instead of @xmath . Note that for every
choice of @xmath and @xmath , @xmath defines a probability kernel.

For the motion in the ball @xmath , we use a particular field @xmath ,
which we describe in Section 2.1 . There, we will also introduce a
coarse grained RWRE transition kernel.

#### Further notation and abbreviations

For simplicity, we set @xmath , @xmath . Given transition probabilities
@xmath coming from an environment @xmath , we use the notation @xmath ,
@xmath . In order to avoid double indices, we usually write @xmath
instead of @xmath , @xmath for @xmath and @xmath for @xmath if @xmath is
the ball of radius @xmath around zero.

Many of our quantities will be indexed by both @xmath and @xmath , where
@xmath is an additional parameter. While we always keep the indices in
the statements, we normally drop both of them in the proofs. We will
often use the abbreviations @xmath for @xmath , @xmath for @xmath and
@xmath for @xmath .

#### Some words about constants, @xmath-notation and large @xmath
behavior

All our constants are positive. They only depend on the dimension @xmath
unless stated otherwise. In particular, they do not depend on @xmath ,
on @xmath or on any point @xmath , and they are also independent of the
parameter @xmath which will be introduced in Section 2 .

We use @xmath and @xmath for generic positive constants whose values can
change in different expressions, even in the same line. If we use other
constants like @xmath , their values are fixed throughout the proofs.
Lower-case constants usually indicate small (positive) values.

Given two functions @xmath defined on some subset of @xmath , we write
@xmath if there exists a positive @xmath and a real number @xmath such
that @xmath

If a statement holds for “ @xmath large (enough)”, this means that there
exists @xmath depending only on the dimension such that the statement is
true for all @xmath . This applies analogously to the expressions “
@xmath (or @xmath ) small (enough)”.

The reader should always keep in mind that we are interested in
asymptotics when @xmath and @xmath is a (arbitrarily) small positive
constant. Even though some of our statements are valid only for large
@xmath and @xmath sufficiently small, we do not mention this every time.

### 1.2 Main results on exit laws

We still need some notation. For @xmath , @xmath and @xmath define

  -- -------- --
     @xmath   
  -- -------- --

  -- -- --
        
  -- -- --

If @xmath is constant, we write @xmath instead of @xmath . We usually
drop @xmath from the notation if @xmath . Further, let

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

With @xmath , we set for @xmath

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

The following technical condition will play a key role.

Let @xmath and @xmath . We say that @xmath holds if for all @xmath , all
@xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Notice that if @xmath is satisfied, then for any @xmath and any @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We can now formulate our results. Proposition 1.1 , Theorem 1.1 and
Corollary 1.1 are in a similar form already present in [ 6 ] . See also
our discussion in the introduction.

The main technical statement is

###### Proposition 1.1.

Assume @xmath . There exists @xmath such that for any @xmath , there
exists @xmath with the following property: If @xmath and @xmath holds,
then

1.   There exists @xmath such that for @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   There exist sequences @xmath , @xmath such that if @xmath and
    @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

As a direct consequence, we get

###### Theorem 1.1 (@xmath).

Assume @xmath .

1.   There exists @xmath such that for any @xmath , there exists @xmath
    with the following property: If @xmath and @xmath holds, then for
    all @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   There exists @xmath such that if @xmath is satisfied for some
    @xmath , then for any @xmath , we can find @xmath and a smoothing
    radius @xmath such that for @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

###### Remark 1.1.

(i) In particular, part (i) of Proposition 1.1 tells us that if @xmath ,
then @xmath holds for all large @xmath , provided @xmath is fulfilled
for @xmath small enough, depending only on @xmath (and the dimension).
This follows immediately from the fact that given any @xmath and any
@xmath , we can always find @xmath such that @xmath implies @xmath .
(ii) As an easy consequence of part (ii) of the theorem, if one
increases the smoothing scale with @xmath , i.e. if @xmath (arbitrary
slowly) as @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

(iii) One could define the smoothing kernel @xmath differently. However,
our particular form is useful for the induction procedure.

Our methods allow us to compare the exit measures in a more local way.
For positive @xmath and @xmath , let @xmath . Then @xmath contains on
the order of @xmath points. The center @xmath will play no particular
role, so we drop it from the notation. If we choose our parameters
according to Theorem 1.1 (i), we have good control over @xmath in terms
of @xmath , provided @xmath has a distance of order @xmath from the
boundary and @xmath is sufficiently large. For the statement of the
following theorem, we pick @xmath and @xmath according to Proposition
1.1 , and choose the perturbation @xmath small enough such that @xmath
implies @xmath (and then @xmath for all @xmath , according to the
proposition).

###### Theorem 1.2.

Assume @xmath . In the setting just described, if @xmath is fulfilled,
then for @xmath , there exists an event @xmath with @xmath such that on
@xmath , the following holds true. If @xmath and @xmath , then

1.   For @xmath and every set @xmath as above, there exists @xmath with

      -- -------- --
         @xmath   
      -- -------- --

2.   For @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    Here, the constant in the @xmath -notation depends only on @xmath
    and @xmath .

From Proposition 1.1 , we also obtain transience of the RWRE.

###### Corollary 1.1 (Transience).

Assume @xmath . There exist @xmath , @xmath such that if @xmath is
satisfied for some @xmath , then for @xmath -almost all @xmath , there
exists @xmath with the following property: For integers @xmath and
@xmath ,

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

In particular, the RWRE @xmath is transient.

### 1.3 Main results on mean sojourn times

For the times, we propagate a condition similar to @xmath . In this
regard, we first introduce a monotone increasing function which will
limit the normalized mean sojourn time in the ball. Let @xmath , and
define @xmath by setting

  -- -------- --
     @xmath   
  -- -------- --

Note that @xmath and therefore @xmath .

Recall that @xmath is the expectation with respect to simple random walk
starting at the origin. We say that @xmath holds, if for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Our main technical result is

###### Proposition 1.2.

Assume @xmath and A2 , and let @xmath . There exists @xmath with the
following property: If @xmath and @xmath holds, then

1.   There exists @xmath such that for @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.  -- -------- --
         @xmath   
      -- -------- --

By Borel-Cantelli and the Markov property, we immediately have

###### Corollary 1.2 (Quenched moments).

In the framework of Proposition 1.2 , for @xmath and @xmath -almost all
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

The bounds on the quenched moments for @xmath are useful to prove

###### Theorem 1.3.

Assume @xmath and A2 . Given @xmath , one can find @xmath such that if
@xmath is satisfied for some @xmath , then the following holds: There
exist @xmath , @xmath @xmath such that for @xmath -almost all @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Remark 1.2.

(i) Given @xmath and @xmath , we can always guarantee (by making @xmath
smaller if necessary) that @xmath implies @xmath .
(ii) The factor @xmath in the definition of condition @xmath , the
factor @xmath and also the choice of @xmath inside the probability in
the statement of Proposition 1.2 (ii) are connected to Assumption A2 .
If, for instance, one could prove that for some @xmath and large @xmath
,

  -- -------- --
     @xmath   
  -- -------- --

then Proposition 1.2 (ii) would hold with @xmath replaced by @xmath for
every @xmath .
(iii) In the last theorem, we strongly believe that @xmath .

### 1.4 Perturbation expansion

Our approach of comparing the behavior of the RWRE in space and time
with that of simple random walk is based on a perturbation argument.
Namely, the resolvent equation allows us to express Green’s functions of
the RWRE in terms of ordinary Green’s functions. More generally, let
@xmath be a family of finite range transition probabilities on @xmath ,
and let @xmath be a finite set. The corresponding Green’s kernel or
Green’s function for @xmath is defined by

  -- -------- --
     @xmath   
  -- -------- --

The connection with the exit measure is given by the fact that for
@xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Now write @xmath for @xmath and let @xmath be another transition kernel
with corresponding Green’s function @xmath for @xmath . With @xmath , we
have by the resolvent equation

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

In order to get rid of @xmath on the right hand side, we iterate ( 3 )
and obtain

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

provided the infinite series converges, which will always be the case in
our setting, due to @xmath and @xmath being finite. A modification of (
4 ) turns out to be particularly useful. Note that by ( 4 ),

  -- -------- --
     @xmath   
  -- -------- --

Replacing the rightmost @xmath by @xmath and reordering terms, we arrive
at

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

where @xmath .

### 1.5 A short reading guide

The key idea behind our results on exit laws from @xmath is to compare
the RWRE exit measure with that of simple random walk by means of the
perturbation expansion

  -- -------- --
     @xmath   
  -- -------- --

Here, @xmath is a coarse grained RWRE transition kernel inside @xmath ,
@xmath is a coarse grained simple random walk kernel, and @xmath is the
Green’s function associated to @xmath .

Our coarse grained transition kernels are given by exit distributions
from smaller balls inside @xmath , and we obtain our results by
transfering inductively estimates on smaller scales to scale @xmath .
The coarse graining schemes defined in Section 2 determine the radii of
the smaller balls. In the bulk of @xmath , we choose the radius @xmath ,
but we refine the radii when approaching the boundary. Our schemes are
parametrized by a real number @xmath , which determines the distance to
the boundary @xmath at which the refinement stops. We choose @xmath
equal to @xmath for the estimates involving a global smoothing, and
equals a large constant for the non- or locally smoothed estimates.

Besides the coarse graining schemes, Section 2 introduces the concept of
“good” and “bad” points and so-called goodified Green’s functions.
Roughly speaking, we call a point @xmath good if the exit measure on
such a smaller ball around @xmath is close to the exit measure of simple
random walk, in both a smoothed and non-smoothed way. If inside @xmath
all points are good, then the estimates on smaller balls can be
transferred to a (globally smoothed) estimate on the larger ball @xmath
(Lemma 5.2 ), using some averaging argument and an exponential
inequality.

But bad points can appear, and in fact we have to distinguish four
different levels of badness (Section 2.3 ). When bad points are present,
it is convenient to “goodify” the environment, that is to replace bad
points by good ones. This important concept is first explained in
Section 2 and then further developed in Section 4 . However, for the
globally smoothed estimate, due to the additional smoothing step we only
have to deal with the case where all bad points are enclosed in a
comparably small region - two or more such regions are too unlikely
(Lemma 2.1 ). Some special care is required for the worst class of bad
points in the interior of the ball. For environments containing such
points, we slightly modify the coarse graining scheme inside @xmath , as
described in Section 4.4 . In Lemma 5.3 , we prove the smoothed
estimates on environments with bad points and show that the degree of
badness decreases by one from one scale to the next.

Concerning exit measures where no or only a local last smoothing step is
added (Section 6 , Lemmata 6.1 and 6.2 , respectively), bad points near
the boundary of @xmath are much more delicate to handle, since we have
to take into account several possibly bad regions. However, they do not
occur too frequently (Lemma 2.2 ) and can be controlled by capacity
arguments.

All these estimates require precise bounds on coarse grained Green’s
functions, which are developed in Section 4 . Basically, we show that on
environments with no bad points, the coarse grained RWRE Green’s
function for the ball can be estimated from above by the analogous
quantity coming from simple random walk.

Section 3 is devoted to technical bounds on hitting probabilities of
both simple random walk and Brownian motion, and to difference estimates
of smoothed exit measures. The reason for working sometimes with
Brownian motion instead of a random walk is of technical nature - some
estimates are easier to prove for the former, as for example Lemma 3.5
(iii). They can then be transferred to random walks via coupling
arguments provided in the appendix.

The statements from Sections 5 and 5 are finally used in Section 7 to
prove the main results on exit measures, including the proof of
transience of the RWRE.

The object of interest in Section 8 is the mean sojourn time of the RWRE
in the ball @xmath . Employing the Markov property, we represent this
quantity as a convolution of a coarse grained RWRE Green’s function
@xmath and mean sojourn times in smaller balls @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Again, multiscale analysis is used to transport time estimates on a
smaller to a bigger scale. It turns out that we need control over space
and time on the next two lower levels. This requires a stronger notion
of good and bad points concerning both spatial and temporal behavior. In
Section 9 , we prove our main results on mean sojourn times.

Finally, in the appendix we prove the main statements of Section 3 , as
well as a local central limit theorem for the coarse grained simple
random walk.

## 2 Coarse graining schemes and notion of badness

The purpose of this section is to introduce coarse graining schemes in
the ball as well as the concept of “good” and “bad” points. Also, we
prove two estimates ensuring that we do not have to consider
environments with bad points that are widely spread out in the ball or
densely packed in the boundary region.

### 2.1 Coarse graining schemes in the ball

Once for all, define

  -- -------- --
     @xmath   
  -- -------- --

We will use particular coarse graining schemes indexed by a parameter
@xmath , which can either be a constant @xmath , but much smaller than
@xmath , or, in most of the cases, @xmath . We fix a smooth function
@xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

such that @xmath is strictly monotone and concave on @xmath , with first
derivative bounded uniformly by @xmath . Define @xmath by

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

Since we mostly work with @xmath , we use the abbreviation @xmath . We
write @xmath for the coarse grained RWRE transition kernel associated to
@xmath ,

  -- -- --
        
  -- -- --

and @xmath for that coming from simple random walk, where @xmath is
replaced by @xmath . For convenience, we set @xmath for @xmath . Notice
that by the strong Markov property, the exit measures from the ball
@xmath remain unchanged under these transition kernels, i.e.

  -- -------- --
     @xmath   
  -- -------- --

We denote by @xmath the (coarse grained) RWRE Green’s function coming
from @xmath , and by @xmath the Green’s function from @xmath ,
everything in @xmath .

###### Remark 2.1.

(i) Later on, we will also work with slightly modified transition
kernels @xmath and @xmath , which depend on the environment. We
elaborate on this in Section 4.4 .
(ii) Due to the lack of the last smoothing step outside @xmath , we need
to zoom in near the boundary in order to handle non-smoothed exit
distributions in Section 6 . The parameter @xmath allows us to adjust
the step size in the boundary region.
(ii) Note that for every choice of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

### 2.2 Good and bad points

We shall partition the grid points inside @xmath according to their
influence on the exit behavior. We say that a point @xmath is good (with
respect to @xmath , @xmath and @xmath , @xmath ) if

-   For all @xmath , @xmath .

-   If @xmath , then additionally

      -- -------- --
         @xmath   
      -- -------- --

A point @xmath which is not good is called bad . We denote by @xmath the
set of all bad points inside @xmath and write @xmath for short.
Furthermore, set @xmath and @xmath . Of course, the set of bad points
depends also on @xmath , but we do not indicate this.

###### Remark 2.2.

(i) For the coarse graining scheme associated to @xmath , we have by
definition @xmath . When performing the estimates in Section 6 , we work
with constant @xmath . In this case, @xmath can contain more points than
@xmath .
(ii) Assume @xmath large. If @xmath with @xmath , then the function
@xmath , defined in ( 6 ), lies in @xmath for each @xmath . Thus, for
all @xmath , we can use @xmath to control the event @xmath , provided
@xmath . We make use of this in Lemma 2.1 .

#### Goodified transition kernels

It is difficult to obtain estimates for the RWRE in the presence of bad
points. For all environments, we therefore introduce “goodified”
transition kernels @xmath ,

  -- -- -- -----
           (7)
  -- -- -- -----

Furthermore, we write @xmath for the corresponding (random) Green’s
function.

### 2.3 Bad regions in the case @xmath

The following lemma shows that with high probability, all bad points
with respect to @xmath are contained in a ball of radius @xmath . Let

  -- -------- --
     @xmath   
  -- -------- --

We will look at the events @xmath and @xmath . It is also useful to
define the set of good environments, @xmath .

###### Lemma 2.1.

For large @xmath , @xmath implies that for @xmath with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: Set @xmath , @xmath . For all @xmath with @xmath , using @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath            
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

and a similar estimate holds in the case @xmath . On the event @xmath ,
there exist @xmath with @xmath . But for such @xmath , the events @xmath
and @xmath are independent, whence for @xmath large

  -- -------- --
     @xmath   
  -- -------- --

@xmath

The estimate is good enough for our inductive procedure, so we only have
to deal with the case where all bad points are enclosed in a ball @xmath
. However, inside @xmath we need to look closer at the degree of
badness. We say that @xmath is bad on level @xmath , @xmath , if the
following holds:

-   For all @xmath , for all @xmath , @xmath .

-   For all @xmath with @xmath , additionally

      -- -------- --
         @xmath   
      -- -------- --

-   There exists @xmath with @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

If @xmath is neither bad on level @xmath nor good, we call @xmath bad on
level @xmath . In this case, @xmath contains “really bad” points. We
write @xmath for the subset of all those @xmath which are bad on level
@xmath . Observe that

  -- -------- --
     @xmath   
  -- -------- --

On @xmath , @xmath and therefore @xmath .

### 2.4 Bad regions when @xmath is a constant

When estimating the non-smoothed quantity @xmath , we cannot stop the
refinement of the coarse graining in the boundary region @xmath .
Instead, we will choose @xmath as a (large) constant. However, now it is
no longer true that essentially all bad points are contained in one
single region @xmath . For example, if @xmath such that @xmath is of
order @xmath , we only have a bound of the form

  -- -------- --
     @xmath   
  -- -------- --

which is clearly not enough to get an estimate as in Lemma 2.1 . We
therefore choose a different strategy to handle bad points within @xmath
. We split the boundary region into layers of an appropriate size and
use independence to show that with high probability, bad regions are
rather sparse within those layers. Then the Green’s function estimates
of Corollary 4.1 will ensure that on such environments, there is a high
chance to never hit points in @xmath before leaving the ball.

To begin with the first part, fix @xmath with @xmath , where @xmath is a
constant that will be chosen below. Let @xmath be large enough such that
@xmath , and set @xmath . We define layers @xmath and @xmath , @xmath .
Then,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath . For @xmath , consider the interval @xmath . We divide
@xmath into subsets by setting @xmath , where @xmath . Denote by @xmath
the set of these subsets which are not empty. Setting @xmath , it
follows that

  -- -------- --
     @xmath   
  -- -------- --

We say that a set @xmath is bad if @xmath . As we want to make use of
independence, we partition @xmath into disjoint sets @xmath , such that
for each @xmath we have

-   @xmath for all @xmath ,

-   @xmath .

Notice that @xmath can be chosen to depend on the dimension only. Then
the events @xmath is bad @xmath , @xmath , are independent. Further, if
@xmath , it follows that under @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath and @xmath , if @xmath is big enough. Let @xmath and
@xmath be the number of bad sets in @xmath and @xmath , respectively.
For @xmath , we have @xmath . A standard large deviation estimate for
Bernoulli random variables yields

  -- -------- --
     @xmath   
  -- -------- --

with @xmath . By enlarging @xmath if necessary, we get @xmath for @xmath
, whence

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

for @xmath , @xmath and @xmath large enough. In particular,

  -- -------- --
     @xmath   
  -- -------- --

Therefore, setting

  -- -------- --
     @xmath   
  -- -------- --

we have proved the following

###### Lemma 2.2.

There exists a constant @xmath such that if @xmath and @xmath is large
enough, then @xmath implies that for @xmath with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

## 3 Some important estimates

In this section, we present various results on exit and hitting
proababilities for both simple random walk and Brownian motion.

### 3.1 Hitting probabilities

The first two lemmata concern simple random walk.

###### Lemma 3.1.

Let @xmath .

1.   There exists @xmath such that for all @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   There exists @xmath such that for all @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

3.   Let @xmath and @xmath with @xmath . Then

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) @xmath is harmonic inside @xmath . Applying a discrete
Harnack inequality, as, for example, provided by Theorem 6.3.9 in the
book of Lawler and Limic [ 24 ] , we see that @xmath on @xmath , for
some @xmath . Part (i) then follows from Lemma 6.3.7 in the same book.
(ii) By the triangle inequality,

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , the function @xmath is harmonic inside @xmath . The claim
now follows from [ 24 ] Theorem 6.3.8, (6.19), together with (i).
(iii) This is Proposition 1.5.10 of [ 23 ] . @xmath

A good control over hitting probabilities is given by

###### Lemma 3.2.

Let @xmath and @xmath with @xmath . Then

1.  -- -------- --
         @xmath   
      -- -------- --

2.   There exists @xmath , independent of @xmath , such that when @xmath
    ,

      -- -------- --
         @xmath   
      -- -------- --

3.   There exists @xmath such that for all @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

This lemma will be proved in the appendix.

We need analogous results for Brownian motion in @xmath . Denote by
@xmath the exit measure of @xmath -dimensional Brownian motion from
@xmath , started at @xmath . By a small abuse of notation, we also write
@xmath for the (continuous version of the) density with respect to
surface measure on @xmath , which is given by the Poisson kernel

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

where @xmath is the volume of the unit ball. From this explicit form, we
can directly read off the analogous statements of Lemma 3.1 (i), (ii)
and Lemma 3.2 (iii), with @xmath replaced by @xmath . Let us now
formulate and prove the analog of parts (i) and (ii) from the last
lemma. Denote by @xmath the law of standard @xmath -dimensional Brownian
motion, started at @xmath . For the following statement, @xmath and
@xmath are defined in the obvious way in terms of Brownian motion.

###### Lemma 3.3.

Let @xmath and @xmath with @xmath . Then

1.  -- -------- --
         @xmath   
      -- -------- --

2.   Assume @xmath . There exists @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

Proof of Lemma 3.3 : (i) See for example the book of Durrett [ 12 ] ,
@xmath .
(ii) Recall that the Green’s function of Brownian motion for @xmath is
given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is an explicit constant, and for @xmath , @xmath is the
inversion of @xmath with respect to @xmath . Now, for @xmath and @xmath
, we have

  -- -------- --
     @xmath   
  -- -------- --

By Proposition @xmath of [ 10 ] , the infimum on the right-hand side can
be bounded from below by @xmath . Using the second upper bound on @xmath
from the same proposition, we get

  -- -------- --
     @xmath   
  -- -------- --

@xmath

###### Remark 3.1.

Lemma 3.2 (ii) can be proved in the same way if @xmath for some @xmath ,
for example by using Proposition 8.4.1 of [ 24 ] , which is based on a
coupling argument. Since we need an estimate including the case when
@xmath or @xmath are near the boundary, we give a self-contained proof
in the appendix.

Probabilities of the above type will often be estimated by the following

###### Lemma 3.4.

Let @xmath , @xmath and @xmath . Set @xmath , @xmath . Then for some
constant @xmath

  -- -------- --
     @xmath   
  -- -------- --

Proof: If @xmath , then the left-hand side is bounded by

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , we set @xmath . Then, for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Since for @xmath we have @xmath , the claim then follows from

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

@xmath

### 3.2 Smoothed exit measures

We will compare exit laws of simple random walk with exit laws of
Brownian motion. Given a field of positive real numbers @xmath , we
define the smoothed exit law from @xmath of simple random walk as

  -- -------- --
     @xmath   
  -- -------- --

Denoting by @xmath the exit measure of @xmath -dimensional Brownian
motion from @xmath , started at @xmath , we let analogous to ( 1 ),

  -- -------- --
     @xmath   
  -- -------- --

Then define the smoothed Brownian exit measure from @xmath as

  -- -------- --
     @xmath   
  -- -------- --

By @xmath we denote the density of @xmath with respect to @xmath
-dimensional Lebesgue measure.

###### Lemma 3.5.

Let @xmath . There exists a constant @xmath such that

1.  -- -------- --
         @xmath   
      -- -------- --

2.  -- -------- --
         @xmath   
      -- -------- --

3.  -- -------- --
         @xmath   
      -- -------- --

For the proof, we refer to the appendix. The next proposition will be
applied at the end of the proof of Lemma 5.2 . At this point, the
invariance condition @xmath comes into play. We give a general
formulation in terms of a signed measure @xmath . Let us introduce the
following notation. For @xmath , put

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

###### Proposition 3.1.

Let @xmath . Consider a measure @xmath on @xmath with total mass zero
satisfying

1.  @xmath for all @xmath and all @xmath .

2.  @xmath for all @xmath and all @xmath , @xmath .

Then there exists @xmath such that for @xmath with @xmath and all @xmath
, @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: Since the proof is the same for all @xmath with @xmath , we can
assume @xmath . By Lemma 3.5 (i),

  -- -------- --
     @xmath   
  -- -------- --

Taylor’s expansion gives

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath is the gradient, @xmath the Hessian of @xmath with respect
to the first variable, and @xmath is the remainder term, which can be
bounded by Lemma 3.5 (ii), namely

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath satisfies property (i), the first summand on the right side
of ( 3.2 ) vanishes. Due to the same reason, the second summand equals

  -- -------- --
     @xmath   
  -- -------- --

By property (ii), the sum over @xmath does not depend on @xmath , so a
multiple of the Laplacian of @xmath remains. But for each @xmath ,
@xmath is harmonic in @xmath , thus also the Laplacian vanishes. This
proves the proposition. @xmath

## 4 Green’s functions for the ball

One principal task of our approach aims at developing good estimates on
Green’s functions for the ball of both coarse grained (goodified) RWRE
as well as coarse grained simple random walk. The main result is Lemma
4.1 . For the coarse grained simple random walk, the estimates on
hitting probabilities of the last section together with Proposition 4.2
yield the right control.

On a certain class of environments, we need to modify the transition
kernels in order to ensure that bad points are not visited too often by
the coarse grained random walks. This modification will be described in
Section 4.4 .

### 4.1 A local central limit theorem

Let @xmath . Denote by @xmath the coarse grained transition
probabilities on @xmath belonging to the field @xmath , where @xmath is
chosen constant in @xmath . Notice that @xmath is centered, and the
covariances satisfy

  -- -------- --
     @xmath   
  -- -------- --

where for large @xmath (recall the coarse graining scheme) @xmath .

###### Proposition 4.1 (Local central limit theorem).

Let @xmath . For @xmath and all integers @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For the corresponding Green’s function @xmath we obtain

###### Proposition 4.2.

Let @xmath . There exists @xmath such that if @xmath , then

1.   For @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   For @xmath , there exists a constant @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

Here, the constants in the @xmath -notation are independent of @xmath
and @xmath .

In our applications, @xmath will be a function of @xmath . Although
these results look rather standard, we cannot directly refer to the
literature because we have to keep track of the @xmath -dependency. We
give a proof of both statements in the appendix.

We will use the last proposition to estimate the Green’s function for
the ball @xmath , @xmath . Clearly, @xmath is bounded from above by
@xmath , and more precisely, the strong Markov property shows

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

### 4.2 Estimates on coarse grained Green’s functions

As we will show, the perturbation expansion enables us to control the
goodified Green’s function @xmath essentially in terms of @xmath . The
boundary region @xmath turns out to be problematic, since even for good
@xmath , we cannot estimate the variational distance between the
transition kernels by @xmath . We therefore work in this (and only in
this) section with slightly modified transition kernels @xmath , @xmath
, @xmath in the enlarged ball @xmath , taking the exit measure in @xmath
from uncut balls @xmath , @xmath . More precisely, setting @xmath for
@xmath , we let @xmath be the coarse grained simple random walk kernel
under @xmath , that is

  -- -------- --
     @xmath   
  -- -------- --

For the corresponding RWRE kernel, we forget about the environment on
@xmath and set

  -- -------- --
     @xmath   
  -- -------- --

For all good @xmath we now have @xmath , while for @xmath , the
difference vanishes anyway. The goodified version of @xmath is then
obtained in an analogous way to ( 7 ),

  -- -------- --
     @xmath   
  -- -------- --

We write @xmath and @xmath for the corresponding Green’s functions on
@xmath . Note that

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

Since we do not have exact expressions for @xmath or @xmath , we will
construct a (deterministic) kernel @xmath that bounds the Green’s
functions from above. For @xmath , set

  -- -------- --
     @xmath   
  -- -------- --

Further, let

  -- -------- --
     @xmath   
  -- -------- --

The kernel @xmath is defined as the pointwise minimum

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

We cannot derive pointwise estimates on the Green’s functions in terms
of @xmath , but we can use this kernel to obtain upper bounds on
neighborhoods @xmath . Call a function @xmath a positive kernel . Given
two positive kernels @xmath and @xmath , we write @xmath if for all
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath stands for @xmath . Further, we write @xmath , if there is
a constant @xmath such that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We adopt this notation to positive functions of one argument: For @xmath
, @xmath means that for some @xmath , @xmath on any @xmath . Finally,
given @xmath , we say that a positive kernel @xmath on @xmath is @xmath
- smoothing , if for all @xmath , @xmath , and @xmath whenever @xmath .

Now we are in the position to formulate our main statement of this
section. Recall our convention concerning constants: They only depend on
the dimension unless stated otherwise.

###### Lemma 4.1.

1.   There exists a constant @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

2.   There exists a constant @xmath such that for small @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

###### Remark 4.1.

Thanks to ( 11 ), it suffices to prove the bounds for @xmath and @xmath
. For later use, we keep track of the constant in part (i) of the lemma.

We first prove part (i), which is a straightforward consequence of the
estimates on hitting probabilities in Section 3 and the next lemma.

###### Lemma 4.2.

There exists @xmath such that for all @xmath and @xmath with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: If @xmath , then the claim follows from transience of simple
random walk. Now assume @xmath , and always @xmath . Consider first the
case @xmath . Let @xmath be defined as in the beginning of Section 4.1 .
Recall our coarse graining scheme. With @xmath we have

  -- -------- --
     @xmath   
  -- -------- --

Since

  -- -------- --
     @xmath   
  -- -------- --

uniformly in @xmath , it follows from Proposition 4.2 that

  -- -------- --
     @xmath   
  -- -------- --

If @xmath we use Lemma 3.2 (i) and the first case to get

  -- -------- --
     @xmath   
  -- -------- --

@xmath

Proof of Lemma 4.1 (i): It suffices to prove the bound for @xmath .
First we show that there exists a constant @xmath such that for all
@xmath ,

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

At first let @xmath . Then @xmath . We claim that

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

for some @xmath . Indeed, if @xmath , then @xmath is an (averaging) exit
distribution from balls @xmath , where @xmath . Using Lemma 3.1 (i), we
find a constant @xmath such that starting at any @xmath , @xmath is left
after @xmath steps with probability @xmath , uniformly in @xmath . This
together with the strong Markov property implies ( 14 ). Next assume
@xmath . Then @xmath . We claim that

  -- -------- -- ------
     @xmath      (15)
  -- -------- -- ------

For @xmath , @xmath is an averaging exit distribution from balls @xmath
, where @xmath . By Lemma 3.1 (i), we find some small @xmath and a
constant @xmath such that after @xmath steps, the walk has probability
@xmath to be in @xmath , uniformly in @xmath and @xmath . But starting
in @xmath , Lemma 3.1 (iii) shows that with probability @xmath , the
ball @xmath is left before @xmath is visited again. Therefore ( 15 ) and
hence ( 13 ) hold in this case. At last, let @xmath . Then @xmath for
@xmath . Estimating

  -- -------- --
     @xmath   
  -- -------- --

we get with part (i) that

  -- -------- --
     @xmath   
  -- -------- --

Summing over @xmath , ( 13 ) follows. Finally, note that for any @xmath
,

  -- -------- --
     @xmath   
  -- -------- --

Now @xmath follows from ( 13 ) and the hitting estimates of Lemma 3.2 .
@xmath

Let us now explain our strategy for proving part (ii). By version ( 5 )
of the perturbation expansion, we can express @xmath in a series
involving @xmath and differences of exit measures. The Green’s function
@xmath is already controlled by means of @xmath . Looking at ( 5 ), we
thus have to understand what happens if @xmath is concatenated with
certain smoothing kernels. This will be the content of Proposition 4.3 .

We start with collecting some important properties of @xmath , which
will be used throughout this text. Define for @xmath

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 4.3 (Properties of @xmath).

1.   Both @xmath and @xmath are Lipschitz with constant @xmath .
    Moreover, for @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.  -- -------- --
         @xmath   
      -- -------- --

3.   For @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

4.   For @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    and for @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

5.   For @xmath , in the case of constant @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    In the case @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) The second statement is a direct consequence of the Lipschitz
property, which in turn follows immediately from the definitions of
@xmath and @xmath .
(ii) As for @xmath , @xmath and similarly with @xmath replaced by @xmath
, it suffices to show that for @xmath , @xmath ,

  -- -------- -- ------
     @xmath      (16)
  -- -------- -- ------

First consider the case @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

If @xmath then

  -- -------- --
     @xmath   
  -- -------- --

while for @xmath , using part (i) in the first inequality,

  -- -------- --
     @xmath   
  -- -------- --

This proves the first inequality in ( 16 ). The second one follows from

  -- -------- --
     @xmath   
  -- -------- --

(iii) If @xmath and @xmath , then @xmath is of order @xmath . By Lemma
3.4 we have

  -- -------- --
     @xmath   
  -- -------- --

It remains to show that

  -- -- -- ------
           (17)
  -- -- -- ------

If @xmath , this is clear. If @xmath ,  ( 17 ) follows from @xmath .
(iv) If @xmath , then @xmath . Estimating @xmath by @xmath , we get

  -- -------- --
     @xmath   
  -- -------- --

Now the first assertion of (iv) follows from (iii). The second is proved
similarly, so we omit the details.
(v) Set @xmath . For @xmath , it holds that @xmath and @xmath .
Therefore,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore,

  -- -------- --
     @xmath   
  -- -------- --

Lemma 3.4 bounds the second term by @xmath . For the first term, we use
twice part (iii) and once Lemma 3.4 to get

  -- -------- --
     @xmath   
  -- -------- --

This proves (v). @xmath

###### Proposition 4.3 (Concatenating).

Let @xmath be positive kernels with @xmath .

1.   If @xmath is @xmath -smoothing and @xmath , then for some constant
    @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   If @xmath is a positive function on @xmath with @xmath , then for
    some @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) As @xmath is Lipschitz with constant @xmath , we can choose
@xmath points @xmath out of the set @xmath such that @xmath is covered
by the union of the @xmath Since @xmath implies @xmath , we then have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Using @xmath , we get @xmath . Clearly @xmath , so that

  -- -------- --
     @xmath   
  -- -------- --

A second application of @xmath yields the claim.
(ii) We can find a constant @xmath and a covering of @xmath by
neighborhoods @xmath , @xmath , such that every @xmath is contained in
at most @xmath many of the sets @xmath . Using @xmath , it follows that
for @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

@xmath

In terms of our specific kernel @xmath , we obtain

###### Proposition 4.4.

Let @xmath be @xmath -smoothing, and let @xmath be a positive kernel
satisfying @xmath .

1.   There exists a constant @xmath not depending on @xmath and @xmath
    such that

      -- -------- --
         @xmath   
      -- -------- --

2.   If additionally @xmath for @xmath and @xmath for @xmath , then
    there exists a constant @xmath not depending on @xmath and @xmath
    such that for all @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) This is Proposition 4.3 (i) with @xmath .
(ii) We set @xmath and split into

  -- -------- -- ------
     @xmath      (18)
  -- -------- -- ------

Let @xmath be fixed, and consider first @xmath . Using @xmath , @xmath .
As @xmath and @xmath , we get by Proposition 4.3 ii)

  -- -------- --
     @xmath   
  -- -------- --

Setting @xmath , @xmath , we split further into

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then @xmath . By Lemma 4.3 (iv), @xmath . Together we obtain

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then @xmath and @xmath , whence

  -- -------- --
     @xmath   
  -- -------- --

We therefore have shown that

  -- -------- --
     @xmath   
  -- -------- --

To handle the second summand of ( 18 ), set @xmath , @xmath . Clearly,
@xmath and @xmath . Furthermore, @xmath , so that by Proposition 4.3 ii)

  -- -------- --
     @xmath   
  -- -------- --

Consider @xmath , @xmath and split into

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then @xmath , implying @xmath if we prove

  -- -------- -- ------
     @xmath      (19)
  -- -------- -- ------

To this end, we treat the summation over @xmath and @xmath separately.
If @xmath , then @xmath . Estimating @xmath by @xmath and @xmath ,
@xmath simply by @xmath , we get

  -- -------- -- ------
     @xmath      (20)
  -- -------- -- ------

If @xmath , we estimate @xmath again by @xmath and split the summation
into the layers @xmath , @xmath . On @xmath , @xmath . Thus, by Lemma
4.3 (iii),

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Together with ( 20 ), we have proved ( 19 ). It remains to bound the
term @xmath . But if @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

whence @xmath . Using Lemma 4.3 (i), we have

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath follows again from ( 19 ). @xmath

Now we have collected all ingredients to finally prove part (ii) of our
main Lemma 4.1 . Proof of Lemma 4.1 (ii): As already remarked, we only
have to prove the statement involving @xmath . The perturbation
expansion ( 5 ) yields

  -- -------- --
     @xmath   
  -- -------- --

where @xmath , @xmath . With the constants @xmath of Lemma 4.1 (i) and
@xmath of Proposition 4.4 we choose

  -- -------- --
     @xmath   
  -- -------- --

From Lemma 4.1 (i) and Proposition 4.4 (i) with @xmath , @xmath we then
deduce that @xmath , and, by iterating,

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, by part (ii) of Proposition 4.4 with @xmath and Lemma 4.1
(i),

  -- -------- --
     @xmath   
  -- -------- --

Repeating this procedure shows that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Finally, by a further application of Proposition 4.4 (i),

  -- -------- --
     @xmath   
  -- -------- --

This proves the lemma. @xmath

### 4.3 Difference estimates

The results from the preceding section enable us to prove some
difference estimates on the coarse grained Green’s functions, which will
be used in the part on mean sojourn times. The reader who is only
interested in the exit measures may skip this section.

###### Lemma 4.4.

There exists a constant @xmath such that

1.  -- -------- --
         @xmath   
      -- -------- --

2.   For @xmath small,

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) Set @xmath . Recall the definitions of @xmath and @xmath from
Section 4.1 . We write

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath , we have @xmath . Clearly, @xmath . Thus, with @xmath ,
expansion ( 3 ) and Lemma 4.3 yield (remember @xmath )

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

It remains to handle the middle term of ( 4.3 ). By ( 10 ),

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

Using Proposition 4.2 , it follows that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

At last, we claim that

  -- -------- -- ------
     @xmath      (22)
  -- -------- -- ------

Since @xmath , we can define on the same probability space, whose
probability measure we denote by @xmath , a random walk @xmath starting
at @xmath and a random walk @xmath starting at @xmath , both moving
according to @xmath on @xmath , such that for all times @xmath , @xmath
. However, with @xmath , @xmath the same for @xmath , we cannot deduce
that @xmath , since it is possible that one of the walks, say @xmath ,
exits @xmath and then moves far away from the exit point, while staying
close to both @xmath and the walk @xmath , which might still be inside
@xmath . In order to show that such an event has a small probability, we
argue in a similar way to [ 24 ] , Proposition 7.7.1. Define

  -- -------- --
     @xmath   
  -- -------- --

and analogously @xmath . Let @xmath . Since @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , we introduce the events

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

By the strong Markov property and the gambler’s ruin estimate of [ 24 ]
, p. 223 (7.26),

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath independent of @xmath . Applying the triangle inequality
to

  -- -------- --
     @xmath   
  -- -------- --

we deduce, for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , it follows that

  -- -------- --
     @xmath   
  -- -------- --

Also, for @xmath outside and @xmath inside @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

By Proposition 4.2 , ( 22 ) now follows from summing over @xmath .
(ii) Let @xmath with @xmath and set @xmath . With @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Replacing successively @xmath in the first summand on the right-hand
side,

  -- -------- --
     @xmath   
  -- -------- --

where we have set @xmath . With @xmath , expansion ( 5 ) gives

  -- -------- -- ------
     @xmath      (23)
  -- -------- -- ------

Following the proof of Lemma 4.1 (ii), one deduces @xmath . By Lemma 4.3
(iv) and (v), we see that for large @xmath , uniformly in @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Therefore,

  -- -------- --
     @xmath   
  -- -------- --

Using ( 23 ) and twice part (i),

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

The first expression on the right is estimated by

  -- -------- --
     @xmath   
  -- -------- --

where we have used part (i) and the fact that @xmath . The second factor
of ( 4.3 ) is again bounded by (i) and the fact that for @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath            
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Altogether, this proves part (ii). @xmath

### 4.4 Modified transitions on environments bad on level @xmath

We shall now describe an environment-depending second version of the
coarse graining scheme, which leads to modified transition kernels
@xmath , @xmath , @xmath on “really bad” environments.

Assume @xmath is bad on level @xmath , with @xmath . Then there exists
@xmath with @xmath , @xmath . On @xmath , @xmath . By Lemma 4.1 and the
definition of @xmath , it follows easily that we can find a constant
@xmath , depending only on @xmath , such that whenever @xmath for some
@xmath , we have

  -- -------- -- ------
     @xmath      (25)
  -- -------- -- ------

On such @xmath , we let @xmath and define on @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

By replacing @xmath by @xmath on the right side, we define @xmath in an
analogous way. Note that @xmath depends on the environment. We work
again with a goodified version of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For all other environments falling not into the above class, we change
nothing and put @xmath , @xmath , @xmath . This defines @xmath , @xmath
and @xmath on all environments. We write @xmath , @xmath , @xmath for
the Green’s functions corresponding to @xmath , @xmath and @xmath .

#### Some properties of the new transition kernels

The following observations can be read off the definition and will be
tacitly used below.

-   On environments which are good or bad on level at most @xmath , the
    new kernels agree with the old ones, and so do their Green’s
    functions, i.e. @xmath and @xmath . On @xmath with the choice @xmath
    , we have equality of all four Green’s functions.

-   If @xmath is not bad on level @xmath with @xmath , then

      -- -------- --
         @xmath   
      -- -------- --

    This will be used in Sections 5.2 and 6 .

-   In contrast to @xmath , the kernel @xmath depends on the
    environment, too. However, @xmath , @xmath and @xmath do not change
    the exit measure from @xmath , i.e. for example,

      -- -------- --
         @xmath   
      -- -------- --

-   The old transition kernels are finer in the sense that the (new)
    Green’s functions @xmath , @xmath , @xmath are pointwise bounded
    from above by @xmath , @xmath and @xmath , respectively. In
    particular, we obtain with the same constants as in Lemma 4.1 ,

###### Lemma 4.5.

1.  -- -------- --
         @xmath   
      -- -------- --

2.   For @xmath small,

      -- -------- --
         @xmath   
      -- -------- --

For the new goodified Green’s function, we have

###### Corollary 4.1.

There exists a constant @xmath such that for @xmath small,

1.   On @xmath , if @xmath or for general @xmath in the case @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

    On @xmath , if @xmath , then, with @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

2.   On @xmath , @xmath .

3.   For @xmath bad on level at most @xmath with @xmath , or for @xmath
    bad on level @xmath with @xmath , putting @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) The set @xmath is contained in a neighborhood @xmath . As
@xmath , we have

  -- -------- -- ------
     @xmath      (26)
  -- -------- -- ------

From this, the first statement of (i) follows. Now let @xmath be inside
@xmath , and @xmath . If the midpoint @xmath of @xmath can be chosen to
lie inside @xmath , @xmath and @xmath are bounded on @xmath . Then, the
second statement of (i) is again a consequence of ( 26 ). If @xmath , we
have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

(ii) Recall the notation of Section 2.4 . In order to bound @xmath , we
look at the different bad sets @xmath of layer @xmath , @xmath .
Estimating @xmath by @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

On @xmath , the number of bad sets in layer @xmath is bounded by

  -- -------- --
     @xmath   
  -- -------- --

Therefore,

  -- -------- --
     @xmath   
  -- -------- --

Summing over @xmath , this shows

  -- -------- --
     @xmath   
  -- -------- --

(iii) Assume @xmath or @xmath is bad on level @xmath . Then @xmath .
Further, if @xmath , we have @xmath . By choosing @xmath small enough,
the claim follows. If @xmath is bad on level @xmath and @xmath , we do
not gain a factor @xmath from @xmath . However, thanks to our modified
transition kernels, using ( 25 ), @xmath (recall that @xmath pointwise),
so that (ii) follows in this case, too. @xmath

###### Remark 4.2.

All @xmath and @xmath appearing in the next sections are understood to
be chosen in such a way that if we take @xmath and @xmath , then the
conclusions of Lemmata 4.1 , 4.4 , 4.5 and Corollary 4.1 are valid.

## 5 Globally smoothed exits

The aim here is to establish the estimates for the smoothed difference
@xmath which are required to propagate condition @xmath . For the entire
section, we choose @xmath . We start with an auxiliary statement which
will be of constant use.

###### Lemma 5.1.

Let @xmath and set @xmath . Then, for some @xmath ,

  -- -- --
        
  -- -- --

Proof: Using @xmath and the fact that @xmath sums up to zero,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

For @xmath , we have by definition @xmath . Further, notice that @xmath
implies @xmath . Bounding @xmath by Lemma 3.5 (iii), the statement
follows for those @xmath . If @xmath , we simply bound @xmath by @xmath
. Now we can restrict the supremum to those @xmath with @xmath , so the
claim follows again from Lemma 3.5 (iii). @xmath

### 5.1 Estimates on “goodified” environments

The following Lemma 5.2 compares the “goodified” smoothed exit
distribution with that of simple random walk. In particular, it provides
an estimate for @xmath on @xmath . Here we will work with the transition
kernels @xmath , @xmath and @xmath . For the goodified exit measure from
@xmath we write

  -- -------- --
     @xmath   
  -- -------- --

###### Lemma 5.2.

Assume @xmath . There exist @xmath and @xmath such that if @xmath and
@xmath , then for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: Clearly, the claim follows if we show

  -- -------- -- ------
     @xmath      (27)
  -- -------- -- ------

Using the abbreviations @xmath , @xmath , we start with the perturbation
expansion

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath and write

  -- -------- -- ------
     @xmath      (28)
  -- -------- -- ------

Using @xmath , Lemma 4.3 (iv) (with @xmath ) and Lemma 5.1 yield the
estimate

  -- -------- --
     @xmath   
  -- -------- --

for @xmath large. It remains to bound @xmath . With @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

By replacing successively @xmath in the first summand on the right-hand
side,

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (29)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

where @xmath . With @xmath , expansion ( 5 ) shows

  -- -------- --
     @xmath   
  -- -------- --

From the proof of Lemma 4.1 (ii) we learn that @xmath . By Lemma 4.3
(iv), (v) and again Lemma 5.1 , we see that for large @xmath , uniformly
in @xmath and @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Thus, the second summand of ( 29 ) is harmless. However, with the first
summand one has to be more careful. With @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

Clearly, @xmath , so it remains to estimate @xmath , uniformly in @xmath
and @xmath . Set @xmath . For small @xmath , the summands of @xmath with
@xmath are readily bounded by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Now we look at the summands with @xmath . Since the coarse grained walk
cannot bridge a gap of length @xmath in less than @xmath steps, we can
drop the kernel @xmath . Defining @xmath , we thus have

  -- -------- --
     @xmath   
  -- -------- --

The first summand is bounded in the same way as @xmath from ( 28 ).
Further, we can drop the kernel @xmath in the second summand.
Therefore, ( 27 ) follows if we show

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , consider the interval @xmath . We divide @xmath into
subsets @xmath , where @xmath . Let @xmath be the set of those @xmath
for which @xmath . Then we can find a constant @xmath depending only on
the dimension and a disjoint partition of @xmath into sets @xmath , such
that for any @xmath ,

  -- -------- -- ------
     @xmath      (30)
  -- -------- -- ------

For @xmath , @xmath , we set

  -- -------- --
     @xmath   
  -- -------- --

and further @xmath . Assume that we can prove

  -- -------- -- ------
     @xmath      (31)
  -- -------- -- ------

Then

  -- -------- --
     @xmath   
  -- -------- --

Due to ( 30 ), the random variables @xmath , @xmath , are independent
and centered. Hoeffding’s inequality yields, with @xmath ,

  -- -------- -- ------
     @xmath      (32)
  -- -------- -- ------

for some constant @xmath . In order to control the @xmath -norm of the
@xmath , we use the estimates

  -- -------- --
     @xmath   
  -- -------- --

and, by Lemma 5.1 for @xmath , @xmath . Altogether we arrive at

  -- -------- --
     @xmath   
  -- -------- --

uniformly in @xmath . If we put the last display into ( 32 ), we get,
using @xmath in the last line,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

It follows that for @xmath large enough, uniformly in @xmath and @xmath
,

  -- -------- --
     @xmath   
  -- -------- --

It remains to prove ( 31 ). We have

  -- -------- --
     @xmath   
  -- -------- --

Now ( 31 ) follows from the estimates @xmath and

  -- -------- --
     @xmath   
  -- -------- --

which in turn follows from Proposition 3.1 applied to @xmath . @xmath

###### Remark 5.1.

The reader should notice that for @xmath , the signed measure @xmath
fulfills the requirements (i) and (ii) of Proposition 3.1 . Indeed,
after @xmath steps away from @xmath , the coarse grained walks are still
in the interior part of @xmath , where the coarse graining radius did
not start to shrink. Due to @xmath , we thus deduce that (i) and (ii)
hold true for the signed measure @xmath . Replacing @xmath by @xmath
does not destroy the symmetries of this measure, so that Proposition 3.1
can be applied to @xmath .

### 5.2 Estimates in the presence of bad points

In the following lemma, we estimate @xmath on environments with bad
points. We work with the modified kernels @xmath , @xmath , @xmath from
Section 4.4 . Recall that the exit measures under these kernels do not
change, e.g. @xmath . Again, we make the choice @xmath for the coarse
graining scheme.

###### Lemma 5.3.

In the setting of Lemma 5.2 , for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: By the triangle inequality,

  -- -------- -- ------
     @xmath      (33)
  -- -------- -- ------

The second summand on the right is estimated by Lemma 5.2 . For the
first term we have, with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Note that since we are on @xmath , the set @xmath is contained in a
small region. First assume that @xmath . Then @xmath by Corollary 4.1 ,
which bounds the first summand of ( 33 ). Next assume @xmath bad on
level @xmath and @xmath . Then @xmath by the same corollary, which is
good enough for this case.

It remains to consider the cases @xmath bad on level at most @xmath with
@xmath , or @xmath bad on level @xmath with @xmath . We put @xmath and
expand

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

By Corollary 4.1 ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Proceeding as in Lemma 5.1 ,

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is not bad on level @xmath , we have on @xmath the equality
@xmath . Since @xmath , this gives @xmath for every @xmath . For the
second factor in the last display, Lemma 3.5 (iii) yields the bound
@xmath . We arrive at @xmath . For @xmath , we obtain once more with
Corollary 4.1 ,

  -- -------- --
     @xmath   
  -- -------- --

This term is again estimated by Lemma 5.2 , and the lemma is proved.
@xmath

## 6 Non-smoothed and locally smoothed exits

Here, we aim at bounding the total variation distance of the exit
measures without additional smoothing (Lemma 6.1 ), as well as in the
case where a kernel of constant smoothing radius @xmath is added (Lemma
6.2 ). We use the transition kernels @xmath and @xmath .

Throughout this section, we work with constant parameter @xmath . We
always assume @xmath large enough such that @xmath . The right choice of
@xmath depends on the deviations @xmath and @xmath we are shooting for
and will become clear from the proofs. In either case, we choose @xmath
, where @xmath is the constant from Section 2.4 . The value of @xmath
will then also influence the choice of the perturbation @xmath in Lemma
6.1 and the smoothing radius @xmath in Lemma 6.2 , respectively.

We recall the partition of bad points into the sets @xmath , @xmath ,
@xmath , @xmath and the classification of environments into @xmath ,
@xmath and @xmath from Section 2 .

The bounds for @xmath (Lemma 2.1 ) and for @xmath (Lemma 2.2 ) ensure
that we may restrict ourselves to environments @xmath . For such
environments, we introduce two disjoint random sets @xmath , @xmath as
follows:

-   If @xmath , set @xmath and @xmath .

-   If @xmath , set @xmath and @xmath .

Of course, on @xmath , we have @xmath and @xmath .

###### Lemma 6.1.

There exists @xmath such that if @xmath , there exist @xmath and @xmath
with the following property: If @xmath and @xmath , then @xmath , @xmath
imply that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: We choose @xmath according to Remark 4.2 and take @xmath . The
right choice of @xmath and @xmath will be clear from the course of the
proof. From Lemmata 2.1 and 2.2 we learn that if we take @xmath large
enough and @xmath with @xmath , then under @xmath

  -- -- --
        
  -- -- --

Therefore, the claim follows if we show that on @xmath , we have for all
sufficiently small @xmath and all large @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath . We use the partition of @xmath into the sets @xmath ,
@xmath described above. With @xmath , we have inside @xmath

  -- -------- --
     @xmath   
  -- -------- --

By replacing successively @xmath in the first summand on the right-hand
side, we arrive at

  -- -------- --
     @xmath   
  -- -------- --

Since with @xmath , @xmath , we obtain

  -- -------- -- -------- -------- ------
     @xmath                        (34)
                                   
                 @xmath   @xmath   
                 @xmath   @xmath   
  -- -------- -- -------- -------- ------

We will now prove that each of the three parts @xmath is bounded by
@xmath . If @xmath , then @xmath and @xmath . Using Corollary 4.1 in the
second and Lemma 3.1 (ii) in the third inequality,

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (35)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

for @xmath large enough, @xmath . Regarding @xmath , we have in the case
@xmath by Corollary 4.1 (ii)

  -- -------- --
     @xmath   
  -- -------- --

On the other hand, if @xmath , then @xmath is outside @xmath , so that
by Corollary 4.1 (i), (ii)

  -- -------- --
     @xmath   
  -- -------- --

Altogether, for all @xmath , by choosing @xmath and @xmath large enough,

  -- -------- -- ------
     @xmath      (36)
  -- -------- -- ------

It remains to handle @xmath . Once again with Corollary 4.1 (iii) for
some @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We have by definition of @xmath ,

  -- -------- -- ------
     @xmath      (37)
  -- -------- -- ------

and @xmath vanishes on @xmath except for the case @xmath bad on level
@xmath with @xmath . In this case, we use Corollary 4.1 (i) and Lemma
3.1 (ii) to obtain

  -- -------- -- ------
     @xmath      (38)
  -- -------- -- ------

for @xmath large enough, @xmath . Concerning the first term of ( 37 ),
we note that on @xmath , @xmath . Therefore, if @xmath , we obtain
@xmath . Since @xmath , it follows that

  -- -------- -- ------
     @xmath      (39)
  -- -------- -- ------

for @xmath large. Recall the definition of the layers @xmath from
Section 2.4 . For @xmath , @xmath , we have @xmath . By Lemma 4.3 (iii),
@xmath for some constant @xmath , independent of @xmath and @xmath .
Therefore,

  -- -- -- ------
           (40)
  -- -- -- ------

if @xmath is chosen large enough. Finally, for the first layer @xmath ,
there is a constant @xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

Now we take @xmath small enough such that for @xmath , @xmath . We have
shown that @xmath , and the lemma is proven. @xmath

###### Remark 6.1.

As the proof shows, we do not have to assume @xmath for the desired
deviation @xmath . We could instead assume @xmath for some @xmath .
However, @xmath has to be larger than @xmath , which depends on @xmath .
This observation will be useful in the next lemma.

###### Lemma 6.2.

There exists @xmath with the following property: For each @xmath , there
exist a smoothing radius @xmath and @xmath such that if @xmath , @xmath
and @xmath holds for some @xmath , then for @xmath and @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: The proof is based on a modification of the computations in the
foregoing lemma. Let @xmath be as in Lemma 6.1 . We fix an arbitrary
@xmath and assume @xmath for some @xmath , where @xmath will be chosen
later. In the following, “good” and “bad” is always to be understood
with respect to @xmath . Again, for @xmath ,

  -- -- --
        
  -- -- --

For @xmath , we use the splitting ( 34 ) of @xmath into the parts @xmath
. For the summands @xmath and @xmath , we do not need the additional
smoothing by @xmath , since by ( 35 )

  -- -------- --
     @xmath   
  -- -------- --

and by ( 36 )

  -- -------- --
     @xmath   
  -- -------- --

if @xmath and @xmath , @xmath are chosen large enough, depending on
@xmath and @xmath . We turn to @xmath . With ( 38 ), ( 39 ) and ( 40 )
we have (recall that @xmath )

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (41)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

if @xmath and @xmath , @xmath are sufficiently large. Regarding the
second summand of ( 41 ), set @xmath and define for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes simple random walk with start in @xmath . By the
invariance principle for simple random walk, we can choose @xmath so
large such that

  -- -------- --
     @xmath   
  -- -------- --

uniformly in @xmath , where @xmath is the constant from ( 41 ). If
@xmath , @xmath with @xmath , we have @xmath and @xmath . Thus, using
Lemma 10.2 (iii) of the appendix with @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

if we choose @xmath large enough. This proves the lemma. @xmath

## 7 Proofs of the main results on exit laws

Proof of Proposition 1.1 : We take @xmath small enough and, for @xmath ,
we choose @xmath large enough according to Remark 4.2 and the statements
of Sections 5 , 6 .
(ii) is a consequence of Lemma 6.2 , so we have to prove (i). Let @xmath
, and assume that @xmath holds. Then, for @xmath and @xmath , @xmath ,
using Lemma 2.1 ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

For the last summand, we have by Lemmata 5.2 , 5.3 , under @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Therefore, by enlarging @xmath if necessary,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

For the case @xmath , notice that

  -- -------- --
     @xmath   
  -- -------- --

The first summand can be estimated as the corresponding terms in the
case @xmath , while for the last term we use Lemma 6.1 . @xmath

As Theorem 1.1 now follows immediately, we turn to the proof of the
local estimates. Here, the results from Section 4 play again a key role.

Proof of Theorem 1.2 : As usual, we mostly drop @xmath as index, so
always @xmath , @xmath and so on. For the whole proof, we let @xmath .
Choose @xmath and @xmath as in Proposition 1.1 . Recall the definition
of @xmath from Section 2 . By Proposition 1.1 , we find @xmath , @xmath
and @xmath such that under @xmath and @xmath , condition @xmath holds
true for all @xmath . We put @xmath and note that similar to Lemma 2.1 ,
if @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For the rest of the proof, take @xmath . On such environments, @xmath
equals @xmath by our choice @xmath . Now let us prove part (i). Observe
that @xmath can be covered by @xmath many neighborhoods @xmath , @xmath
, as defined in Section 4.2 , where @xmath depends on the dimension
only. In particular, @xmath . Applying Lemma 4.1 (ii), we deduce that

  -- -------- --
     @xmath   
  -- -------- --

From Lemma 3.1 (i) we know that if @xmath , then for some constant
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Together with the preceding equation, this shows (i).

(ii) Set @xmath and consider the smoothing kernel @xmath with @xmath .
Let

  -- -------- --
     @xmath   
  -- -------- --

We claim that

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      (42)
     @xmath   @xmath   @xmath      (43)
  -- -------- -------- -------- -- ------

Concerning the first inequality,

  -- -------- --
     @xmath   
  -- -------- --

since @xmath for @xmath . Also,

  -- -------- --
     @xmath   
  -- -------- --

since @xmath for @xmath . This proves ( 42 ), while ( 43 ) is entirely
similar. In the remainder of this proof, we often write @xmath for
@xmath . If we show

  -- -------- -- ------
     @xmath      (44)
  -- -------- -- ------

then by ( 42 ),

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

On the other hand, by ( 43 ) and still assuming ( 44 ),

  -- -------- --
     @xmath   
  -- -------- --

so that indeed

  -- -------- --
     @xmath   
  -- -------- --

provided we prove ( 44 ). In that direction, set @xmath and write, with
@xmath ,

  -- -------- -- ------
     @xmath      (45)
  -- -------- -- ------

Looking at the first summand we have

  -- -------- --
     @xmath   
  -- -------- --

Following the proof of Proposition 4.4 (ii), we deduce

  -- -------- --
     @xmath   
  -- -------- --

Together with @xmath and @xmath this yields the bound

  -- -------- --
     @xmath   
  -- -------- --

To obtain ( 44 ), it remains to handle the second summand of ( 45 ),
i.e. we have to bound

  -- -------- --
     @xmath   
  -- -------- --

We abbreviate @xmath and split into

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

First note that since @xmath if @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

For @xmath , we apply Lemma 3.2 (iii) together with Lemma 3.4 and obtain

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath , @xmath , and thus

  -- -------- --
     @xmath   
  -- -------- --

It remains to bound

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath . If @xmath , then @xmath . Using Lemma 10.2 (iii) for the
difference of the smoothing steps and the usual estimate for @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

The region @xmath we split into @xmath , and

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, let

  -- -------- --
     @xmath   
  -- -------- --

Then

  -- -------- --
     @xmath   
  -- -------- --

If @xmath and @xmath , then by Lemma 3.2 (iii)

  -- -------- --
     @xmath   
  -- -------- --

while in the case @xmath , using the same lemma and additionally Lemma
3.4 ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

For the Green’s function, we use the estimates

  -- -------- --
     @xmath   
  -- -------- --

while for @xmath , it holds that @xmath , whence

  -- -------- --
     @xmath   
  -- -------- --

Altogether, we obtain

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This finishes the proof of part (ii). @xmath

Let us finally show how to obtain transience of the RWRE.
Proof of Corollary 1.1 : Fix numbers @xmath , @xmath to be specified
below. With these parameters and @xmath , we set

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is chosen constant in @xmath , namely @xmath . Define

  -- -------- --
     @xmath   
  -- -------- --

By Proposition 1.1 (i), there exists @xmath such that given @xmath ,
@xmath implies that for @xmath large enough, we have

  -- -------- --
     @xmath   
  -- -------- --

Therefore, for any choice of @xmath it holds that

  -- -------- --
     @xmath   
  -- -------- --

whence by Borel-Cantelli

  -- -------- -- ------
     @xmath      (46)
  -- -------- -- ------

We denote the coarse grained RWRE transition kernel by

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is large enough and @xmath , we have on @xmath

  -- -------- --
     @xmath   
  -- -------- --

Now assume @xmath . For @xmath fixed, @xmath large and @xmath , it
follows that for @xmath

  -- -------- -- ------
     @xmath      (47)
  -- -------- -- ------

For fixed @xmath , let @xmath be the Markov chain running with
transition kernel @xmath . Clearly, @xmath can be obtained by observing
the basic RWRE @xmath at randomized stopping times. Then

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Using Proposition 4.1 , we can find @xmath , depending not on @xmath ,
such that for any @xmath with @xmath , it holds that @xmath . With ( 47
), we conclude that for such @xmath , @xmath large enough and @xmath ,

  -- -------- -- ------
     @xmath      (48)
  -- -------- -- ------

On the other hand, if @xmath is outside @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath , then @xmath as long as @xmath . By first choosing @xmath
large enough, then @xmath small enough and estimating the higher
summands again with Proposition 4.1 , we deduce that for such @xmath and
all large @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Together with ( 47 ), we have for large @xmath , @xmath and @xmath ,

  -- -------- -- ------
     @xmath      (49)
  -- -------- -- ------

Let @xmath be the event that the walk @xmath leaves @xmath before
reaching @xmath . From ( 48 ) and ( 49 ) we deduce that @xmath ,
provided @xmath is large enough, @xmath and @xmath . But on @xmath , the
underlying basic RWRE @xmath clearly leaves @xmath before reaching
@xmath . Hence if @xmath , there exists @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

for all @xmath , @xmath with @xmath (of course, we may now drop the
constraint @xmath ). From this property, transience easily follows.

Indeed, for @xmath satisfying @xmath and @xmath , set

  -- -------- --
     @xmath   
  -- -------- --

Then @xmath solves the difference inequality

  -- -------- --
     @xmath   
  -- -------- --

with boundary conditions @xmath , @xmath . Further, by either applying a
discrete maximum principle or by a direct computation, we see that
@xmath , where @xmath is the solution of the difference equality

  -- -------- -- ------
     @xmath      (50)
  -- -------- -- ------

with boundary conditions @xmath , @xmath . Solving ( 50 ), we get

  -- -------- --
     @xmath   
  -- -------- --

Letting @xmath , we deduce that for @xmath ,

  -- -------- -- ------
     @xmath      (51)
  -- -------- -- ------

Together with ( 46 ), this proves that for almost all @xmath , the
random walk is transient under @xmath . @xmath

## 8 Mean sojourn times in the ball

Using the results about the variational difference of the exit measures
and the estimates of Section 4 , we provide in this section the basis
for the proof of Proposition 1.2 , which then leads to Theorem 1.3 .
Recall that we work under Assumption A2 .

### 8.1 Preliminaries

Given three real numbers @xmath and @xmath , we write @xmath for the
interval @xmath . Recall the definition of @xmath and the corresponding
coarse graining scheme on @xmath from Section 2.1 . In this part, we
take a closer look at movements in balls @xmath inside @xmath , where
@xmath is large. As in Section 2.1 , we let

  -- -------- --
     @xmath   
  -- -------- --

We transfer the coarse graining schemes on @xmath in the obvious way to
@xmath . We write @xmath for the transition probabilities in @xmath
belonging to @xmath , where @xmath stands for @xmath , which is defined
in ( 6 ). The kernel @xmath is defined similarly, with @xmath replaced
by @xmath .

For the corresponding Green’s functions we use the expressions @xmath
and @xmath . If we do not keep @xmath as an index, we always mean @xmath
as before. Notice that for @xmath , we have @xmath and @xmath . Plainly,
this is in general not true for @xmath and @xmath .

We will readily use the fact that for simple random walk starting in
@xmath (cf. [ 24 ] , Proposition 6.2.6),

  -- -------- -- ------
     @xmath      (52)
  -- -------- -- ------

Define the “coarse grained” RWRE sojourn times

  -- -------- --
     @xmath   
  -- -------- --

and the analog for simple random walk,

  -- -------- --
     @xmath   
  -- -------- --

We will also consider the corresponding quantities @xmath , @xmath for
balls @xmath . For example,

  -- -- --
        
  -- -- --

We often let kernels operate on mean sojourn times from the left. As an
example,

  -- -------- --
     @xmath   
  -- -------- --

The basis for our inductive scheme is established by

###### Lemma 8.1.

For environments @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

In particular,

  -- -------- --
     @xmath   
  -- -------- --

Proof: We take a probability space @xmath carrying independently for
each @xmath a family of independent real-valued random variables @xmath
, distributed according to @xmath . For the sake of convenience set
@xmath for all @xmath and all @xmath . Define the filtration @xmath .
Here, @xmath is the projection on the @xmath th component of the first
factor of @xmath . Then @xmath is a Markov chain on @xmath with
transition kernel @xmath and starting point @xmath . With @xmath , and
iteratively

  -- -------- --
     @xmath   
  -- -------- --

one shows by induction that @xmath is a stopping time with respect to
@xmath . Moreover, in @xmath , the coarse grained chain running with
transition kernel @xmath can be obtained from @xmath by looking at times
@xmath , that is by considering @xmath . Denote by @xmath the
expectation with respect to @xmath . Then, using the strong Markov
property in the next to last equality,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

@xmath

Note that the proof of the statement does not depend on the particular
form of the coarse graining scheme.

### 8.2 Good and bad points

As in our study of exit laws, we introduce the terminology of good and
bad points, but now with respect to both space and time. It turns out
that we need simultaneous control over two levels, which is reflected in
a stronger notion of “goodness”.

#### Space-good and space-bad points

We say that @xmath is space-good , if

-   @xmath , that is @xmath is good in the sense of Section 2.2 .

-   If @xmath , then additionally for all @xmath and for all @xmath ,

    -   For all @xmath , @xmath .

    -   If @xmath , then additionally

          -- -------- --
             @xmath   
          -- -------- --

A point @xmath which is not space-good is called space-bad . The set of
all space-bad points inside @xmath is denoted by @xmath . We classify
the environments into @xmath and @xmath . Notice that @xmath and @xmath
. As an immediate consequence of the definition,

###### Lemma 8.2.

There exists @xmath such that if @xmath is small, then on @xmath ,

1.  @xmath .

2.   If @xmath with @xmath , then for all @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

Proof: (i) Since @xmath , we have @xmath on @xmath , and Lemma 4.1 can
be applied.
(ii) Take @xmath and @xmath as in the statement. On @xmath , the kernel
@xmath coincides with its goodified version, since within @xmath , there
are no bad points. The claim now follows again from Lemma 4.1 . @xmath

###### Lemma 8.3.

If @xmath is large enough, then @xmath implies that for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: One can proceed as in the proof of Lemma 2.1 . We omit the
details. @xmath

#### Time-good and time-bad points

We will also judge points inside @xmath according to their influence on
the time the RWRE spends in the ball. Remember the definitions of @xmath
and condition @xmath from Section 1.3 . We fix @xmath . For points in
the bulk, we again shall control two levels. We say that a point @xmath
is time-good if the following holds:

-   For all @xmath , @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

-   If @xmath , then additionally for all @xmath , @xmath and for all
    @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

A point @xmath which is not time-good is called time-bad . We denote by
@xmath the set of all time-bad points inside @xmath . Recall the
definition @xmath from Section 2 . We let @xmath , @xmath , and @xmath .

#### Important remark

The second point in the definition of time-good provides control over
coarse grained mean times on the preceding level, which will be crucial
for the proof of Lemma 8.6 . Let us look at the first point. If @xmath
is time-good and @xmath , then by definition of the coarse-graining,

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is time-good and @xmath , then at least

  -- -------- --
     @xmath   
  -- -------- --

Due to ( 52 ), this implies

  -- -------- --
     @xmath   
  -- -------- --

However, time-bad points could possibly be very bad and give rise to a
sojourn time which is visible on many subsequent larger scales. For
example, assume that all transition probabilities inside a ball of
radius @xmath have the tendency to push the walker towards the center of
the ball (see Figure 7). Then the mean sojourn time will be of order
@xmath for some @xmath . The probability of such an event should however
be exponentially small in the volume @xmath . Of course, between this
extreme case and a well-behaved environment, there are many intermediate
configurations. One needs to show that “very (time-)bad” environments do
not occur too often, which seems to be a challenging problem. This is
the point where Assumption A2 helps out. It allows us to concentrate on
the event

  -- -------- -- ------
     @xmath      (53)
  -- -------- -- ------

###### Lemma 8.4.

If A2 holds, then for @xmath sufficiently large,

  -- -------- --
     @xmath   
  -- -------- --

Proof: First notice that with

  -- -------- --
     @xmath   
  -- -------- --

we have @xmath . As @xmath , the complement of @xmath is bounded under
A2 by @xmath .

@xmath

###### Remark 8.1.

(i) On a certain class of environments, we can easily bound the mean
time the RWRE spends the a ball. Fix a unit vector @xmath from the
canonical basis of @xmath . We consider an environment @xmath such that
for each @xmath , @xmath , i.e. the environment is balanced in direction
@xmath . In such a case,

  -- -------- --
     @xmath   
  -- -------- --

is a @xmath -martingale with respect to the filtration generated by the
walk @xmath . By the stopping theorem, @xmath . Since @xmath , it
follows that

  -- -------- --
     @xmath   
  -- -------- --

Therefore,

  -- -------- --
     @xmath   
  -- -------- --

and A2 is trivially satisfied.

However, for measures @xmath which are invariant under rotations, the
class of such environments has positive measure under @xmath only if
@xmath is supported on the subset of symmetric transition probabilities
@xmath , implying @xmath for all unit vectors @xmath and @xmath almost
surely. In this case, @xmath is a quenched martingale, and @xmath for
almost all environments.
(ii) Before proceeding, let us mention that Assumption @xmath can be
expressed in terms of hitting probabilities. For example, if there
exists @xmath such that for @xmath large,

  -- -------- --
     @xmath   
  -- -------- --

then @xmath holds. Indeed, on the event @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

By the Markov property it follows that on this event,

  -- -------- --
     @xmath   
  -- -------- --

whence for large @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Let us continue by showing that we can forget about environments with
space-bad points or widely spread time-bad points.

###### Lemma 8.5.

If @xmath is large, then @xmath , @xmath imply that for @xmath with
@xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: We have @xmath . The first summand is bounded by Lemma 8.3 . For
the second, it follows from the definition of time-badness, @xmath and (
52 ) that if @xmath and @xmath is large, then either

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath , or, if @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

for some @xmath , @xmath .

Now notice that for all @xmath , we have @xmath . Moreover, if @xmath ,
then @xmath , whence for all @xmath , @xmath , it follows that @xmath .
We conclude that under @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and therefore

  -- -------- --
     @xmath   
  -- -------- --

@xmath

### 8.3 Estimates on mean times

It remains to deal with environments @xmath . In contrast to the
estimates on exit measures, we treat all these environments at once. The
main statement of this section, Lemma 8.8 , can therefore be seen as the
analog for sojourn times of both Lemmata 5.2 and 5.3 . In the following,
we will always assume that @xmath and @xmath are such that Lemma 8.2 can
be applied. We start with two auxiliary statements. Here, the difference
estimates on the coarse grained Green’s functions from Section 4.3 play
a crucial role.

###### Lemma 8.6.

Let @xmath and @xmath with @xmath . On @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: The claim follows if we show that for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath . Then @xmath . Further, let @xmath . By Lemma 8.1 ,

  -- -------- -- ------
     @xmath      (54)
  -- -------- -- ------

Since @xmath , it follows that @xmath , for all @xmath . Moreover, since
@xmath , we have by Lemma 8.2 @xmath . Thus, Lemma 4.3 (iv) yields

  -- -------- --
     @xmath   
  -- -------- --

for @xmath (and therefore also @xmath ) sufficiently large. Concerning
@xmath , we split again into

  -- -------- --
     @xmath   
  -- -------- --

As above, the second summand is bounded by @xmath . For @xmath , we have
@xmath . In particular, @xmath , and also @xmath . Since both @xmath and
@xmath are contained in @xmath , the strong Markov property gives

  -- -------- --
     @xmath   
  -- -------- --

where

  -- -------- --
     @xmath   
  -- -------- --

Therefore,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

The quantity @xmath is estimated as above. For the sum over @xmath , we
notice that if @xmath , then @xmath . We can use twice Lemma 4.3 (v) to
get

  -- -------- --
     @xmath   
  -- -------- --

Finally, for the sum over the Green’s function difference, we recall
that @xmath coincides with its goodified version, so we may apply Lemma
4.4 . Doing so @xmath times gives

  -- -------- --
     @xmath   
  -- -------- --

This proves the statement. @xmath

###### Lemma 8.7.

Set @xmath . On @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: We have

  -- -------- --
     @xmath   
  -- -------- --

By Lemma 8.2 , @xmath . Therefore, with @xmath , we bound @xmath by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where in the next to last inequality we have used the bound on @xmath
coming from ( 53 ), Lemma 4.3 (iv), (v) and in the last additionally
Lemma 4.4 . For the term @xmath , we let

  -- -------- --
     @xmath   
  -- -------- --

and define @xmath . We split into

  -- -------- --
     @xmath   
  -- -------- --

Lemma 4.3 (iv) and an analogous application of Corollary 4.1 with @xmath
instead of @xmath yield

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , this estimates the second summand of @xmath . For the
first one,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath , the claim follows we show that for @xmath ,

  -- -------- -- ------
     @xmath      (55)
  -- -------- -- ------

which, by definition of @xmath , in turn follows if for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Notice that on @xmath , @xmath . We now fix @xmath and @xmath . Set
@xmath and @xmath . By expansion ( 3 ),

  -- -------- -- ------
     @xmath      (56)
  -- -------- -- ------

Since @xmath , we get

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Here, in the next to last inequality we have used the fact that @xmath
is space-good, all @xmath are time-good, and Lemma 4.3 (v). The last
inequality follows from the bound @xmath . For the second summand of (
56 ), Lemma 4.3 (iv) gives @xmath , whence

  -- -- --
        
  -- -- --

Fix @xmath . Set @xmath and choose @xmath such that @xmath . With

  -- -------- --
     @xmath   
  -- -------- --

we write

  -- -------- -- -------- -------- ------
     @xmath                        (57)
                                   
                 @xmath   @xmath   
                 @xmath   @xmath   
                          @xmath   
  -- -------- -- -------- -------- ------

For @xmath , Lemma 8.6 yields @xmath . Therefore,

  -- -------- --
     @xmath   
  -- -------- --

It remains to handle the second term of ( 57 ). To this end, let @xmath
. Using for @xmath the simple bound @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath and @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

For such @xmath , we get by Lemma 3.2 (ii) and Lemma 3.4

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This bounds the second term of ( 57 ). We have proved ( 55 ) and hence
the lemma. @xmath

Now it is easy to prove

###### Lemma 8.8.

There exists @xmath such that for @xmath and environments @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Proof: By Lemma 8.1 and perturbation expansion ( 3 ), with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath . The term @xmath we split into

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath , the second summand of @xmath can be bounded by
@xmath . The main contribution comes from the first summand. First
notice that

  -- -------- --
     @xmath   
  -- -------- --

Further, we have for @xmath ,

  -- -- --
        
  -- -- --

Collecting all terms, we conclude that

  -- -------- --
     @xmath   
  -- -------- --

Lemma 8.7 bounds @xmath by @xmath . Since for @xmath sufficiently large,

  -- -------- --
     @xmath   
  -- -------- --

we arrive at

  -- -------- --
     @xmath   
  -- -------- --

@xmath

## 9 Proofs of the main results on sojourn times

Proof of Proposition 1.2 : (i) From Lemmata 8.4 , 8.5 and 8.8 we deduce
that for large @xmath , if @xmath and @xmath , we have under @xmath and
@xmath

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
                       @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

By Proposition 1.1 , if @xmath is small, @xmath holds under @xmath for
all large @xmath , provided @xmath . This proves part (i) of the
proposition.
(ii) We take @xmath from part (i). By choosing @xmath small enough, we
can guarantee that @xmath holds. Then, by what we just proved, @xmath
holds for all @xmath . Recalling ( 52 ), we therefore have for large
@xmath

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

@xmath

Proof of Corollary 1.2 : First let @xmath . Using Proposition 1.2 (ii)
and Borel-Cantelli, we obtain for @xmath -almost all @xmath

  -- -------- -- ------
     @xmath      (58)
  -- -------- -- ------

For the rest of the proof, take an environment @xmath satisfying ( 58 ).
Assume @xmath . Then

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

By the Markov property, using the case @xmath and induction in the last
step,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

if @xmath is sufficiently large. @xmath

Proof of Theorem 1.3 : Both statements are proved in the same way, so we
restrict ourselves to the @xmath . Set @xmath and

  -- -------- --
     @xmath   
  -- -------- --

By Proposition 1.2 and Borel-Cantelli it follows that @xmath if @xmath .
Moreover, on @xmath the conclusion of Corollary 1.2 holds true.
Corollary 1.1 tells us that for small enough @xmath , on a set @xmath of
full measure the RWRE satisfies ( 2 ) and is therefore transient. Let
@xmath and define

  -- -------- --
     @xmath   
  -- -------- --

Choose an bijective enumeration function @xmath with @xmath and @xmath
whenever @xmath . Let @xmath denote the collection of all @xmath -null
sets in @xmath and set @xmath , where @xmath , @xmath , is the
projection on the @xmath -th component. Let @xmath be the (completed)
tail @xmath -field. We show that @xmath is measurable with respect to
@xmath , implying that @xmath is @xmath -almost surely constant. Take
@xmath . We claim that for each fixed ball @xmath around the origin,
@xmath its hitting time,

  -- -------- -- ------
     @xmath      (59)
  -- -------- -- ------

But then also

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath depends only on the random variables @xmath with @xmath ,
@xmath is in fact measurable with respect to @xmath , provided the above
representation holds true. Therefore, we only have to prove ( 59 ).
Obviously, @xmath . For the other direction, by the Markov property in
the first inequality,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

Clearly,

  -- -------- --
     @xmath   
  -- -------- --

so @xmath will follow if we show that

  -- -------- -- ------
     @xmath      (60)
  -- -------- -- ------

By Cauchy-Schwarz in the first and Corollary 1.2 in the last inequality,
for large @xmath ,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

For the probability inside the expectation, note that as a consequence
of ( 2 ), for each @xmath we can choose @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

Therefore, replacing the probability by @xmath on @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Using again ( 2 ), there exists @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

On the other hand, for each fixed @xmath and @xmath , transience also
implies

  -- -------- --
     @xmath   
  -- -------- --

if @xmath is large enough. Altogether, we have shown that for @xmath
sufficiently large,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath can be chosen arbitrarily small, this shows ( 60 ) from
which we deduce ( 59 ). @xmath

## 10 Appendix

### 10.1 Some difference estimates

In this section we collect some difference estimates of (non)-smoothed
exit distributions needed to prove Lemma 3.5 (i) and (iii). The first
technical lemma compares the exit measure on @xmath of simple random
walk to that on @xmath of standard Brownian motion.

###### Lemma 10.1.

Let @xmath with @xmath . For large @xmath , there exists a constant
@xmath such that for @xmath , @xmath and @xmath with @xmath , the
following holds.

1.  @xmath

2.  @xmath

Proof: (i) Set @xmath , @xmath and denote by @xmath the image of @xmath
on @xmath under the map @xmath . With @xmath , using the Poisson kernel
representation ( 8 ) in the second equality,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath , an evaluation of the integrand shows

  -- -------- -- ------
     @xmath      (61)
  -- -------- -- ------

for some positive constant @xmath . By [ 37 ] , Corollary 1, for each
@xmath there exists a constant @xmath such that for each integer @xmath
, one can construct on the same probability space a Brownian motion
@xmath with covariance matrix @xmath as well as simple random walk
@xmath , both starting in @xmath and satisfying (with @xmath denoting
the probability measure on that space)

  -- -- -- ------
           (62)
  -- -- -- ------

Choose @xmath and let @xmath be the corresponding constant. The
following arguments hold for sufficiently large @xmath . By standard
results on the oscillation of Brownian paths,

  -- -------- -- ------
     @xmath      (63)
  -- -------- -- ------

With

  -- -------- --
     @xmath   
  -- -------- --

we deduce from ( 62 ) and ( 63 ) that

  -- -------- --
     @xmath   
  -- -------- --

Let @xmath and @xmath . We claim that

  -- -------- -- ------
     @xmath      (64)
  -- -------- -- ------

By the central limit theorem, one finds a constant @xmath with @xmath
for @xmath large. By the Markov property, we obtain @xmath . A similar
bound holds for the probability @xmath , and ( 64 ) follows. Since
@xmath is unchanged if the Brownian motion is replaced by a Brownian
motion with covariance @xmath , we have

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Let @xmath . Then

  -- -------- --
     @xmath   
  -- -------- --

By the strong Markov property,

  -- -------- --
     @xmath   
  -- -------- --

Further, there exists a constant @xmath such that for @xmath and @xmath
, we have @xmath and @xmath . Therefore, an application of first Lemma
3.2 (ii) and then Lemma 3.4 yields

  -- -------- --
     @xmath   
  -- -------- --

uniformly in @xmath . Going back to ( 10.1 ), we arrive at

  -- -------- --
     @xmath   
  -- -------- --

Together with ( 61 ), this shows (i).
(ii) The ideas are the same as in (i), so we only sketch the proof. Set
@xmath , @xmath . Denote by @xmath the image of @xmath on @xmath under
@xmath . Similar to ( 61 ), one finds

  -- -------- --
     @xmath   
  -- -------- --

With @xmath , @xmath , @xmath and @xmath , @xmath defined as above,
@xmath the law of @xmath conditioned on @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Then, with @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Using the hitting estimates for Brownian motion from Lemma 3.3 , one
obtains for @xmath

  -- -------- --
     @xmath   
  -- -------- --

Altogether, (ii) follows. @xmath

We write @xmath for the density of @xmath with respect to @xmath
-dimensional Lebesgue measure, i.e. for @xmath ,

  -- -------- -- ------
     @xmath      (66)
  -- -------- -- ------

###### Lemma 10.2.

There exists a constant @xmath such that for large @xmath , @xmath ,
@xmath and any @xmath ,

1.  @xmath .

2.  @xmath .

3.  @xmath .

4.  @xmath .

5.  @xmath .

6.  @xmath .

7.  @xmath .

###### Corollary 10.1.

In the situation of the preceding lemma,

1.  -- -------- -------- -------- --
         @xmath                     
                                    
                  @xmath   @xmath   
      -- -------- -------- -------- --

2.  -- -------- -------- -------- --
         @xmath                     
                                    
                  @xmath   @xmath   
      -- -------- -------- -------- --

###### Proof.

Combine (iii)-(vii). ∎

###### Remark 10.1.

The condition on @xmath and @xmath in the lemma is only to ensure that
both points lie in the domain of @xmath .

Proof of Lemma 10.2 : (i), (ii) This follows from the definition of
@xmath , @xmath together with Lemma 3.1 (i) and the explicit form of the
Poisson kernel ( 8 ), respectively.
(iii), (iv) We can restrict ourselves to the case @xmath as otherwise we
take a shortest path connecting @xmath with @xmath inside @xmath and
apply the result for distance @xmath @xmath times. We have

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Using the fact that @xmath and part (i) for @xmath , it follows that
@xmath . Using additionally the smoothness of @xmath and, by Lemma 3.1
(i), @xmath , we also have @xmath . It remains to handle @xmath . By
translation invariance of simple random walk, @xmath . In particular,
both (iii) and (iv) will follow if we prove that

  -- -------- -- ------
     @xmath      (67)
  -- -------- -- ------

for @xmath with @xmath . By definition of @xmath , @xmath . We may
therefore assume that @xmath for @xmath . Due to the smoothness of
@xmath and the fact that the integral is over an interval of length at
most @xmath , ( 67 ) will follow if we show

  -- -------- --
     @xmath   
  -- -------- --

We set @xmath and @xmath , where

  -- -------- --
     @xmath   
  -- -------- --

@xmath is an interval of length at most @xmath , and @xmath has the same
length up to order @xmath . Furthermore, @xmath is of order @xmath , and
@xmath . Using that both @xmath and @xmath are of order @xmath , it
therefore suffices to prove

  -- -------- -- ------
     @xmath      (68)
  -- -------- -- ------

Write @xmath for @xmath and @xmath for @xmath . By a first exit
decomposition,

  -- -------- --
     @xmath   
  -- -------- --

By Lemma 3.1 (ii), we can replace @xmath by @xmath . For @xmath we have
by Lemma 3.2 (ii) @xmath and @xmath , uniformly in @xmath . Further,
using @xmath , we have with @xmath

  -- -------- --
     @xmath   
  -- -------- --

and for any @xmath , it follows by a geometric consideration that

  -- -------- --
     @xmath   
  -- -------- --

Altogether, applying Lemma 3.4 in the last step,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The reverse inequality, proved in the same way, then implies ( 68 ).
(v) We can assume @xmath . Then the claim follows from

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

(vi) This is proved in the same way as (v).
(vii) Fix @xmath , @xmath , and let @xmath . Set @xmath and @xmath . By
part (iv), we have

  -- -------- -- ------
     @xmath      (69)
  -- -------- -- ------

Further,

  -- -------- -- ------
     @xmath      (70)
  -- -------- -- ------

By Lemma 10.1 (i), it follows that for @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and the constant @xmath is uniform in @xmath . If we plug
the last line into ( 70 ) and use part (ii) and (vi), we arrive at

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Notice that in our notation, @xmath is the volume of @xmath , while
@xmath is the cardinality of @xmath . From Gauss we have learned that
@xmath . Going back to ( 69 ), this implies

  -- -------- --
     @xmath   
  -- -------- --

as claimed. To prove the reverse inequality, we can follow the same
steps, replacing the random walk estimates by those of Brownian motion
and vice versa. @xmath

### 10.2 Proof of Lemma 3.5

Proof of Lemma 3.5 : (i) Set @xmath , @xmath and @xmath . Choose @xmath
such that @xmath . First assume @xmath . The following estimates are
valid for @xmath large. Write

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , notice that @xmath implies @xmath . Using Lemmata 10.2 (i),
3.2 (iii) in the first and Lemma 3.4 in the second inequality, we have

  -- -------- -- ------
     @xmath      (71)
  -- -------- -- ------

For @xmath , we first use Lemma 10.2 part (iii) to deduce

  -- -------- --
     @xmath   
  -- -------- --

Therefore by part (vii),

  -- -------- --
     @xmath   
  -- -------- --

From the Poisson formula ( 8 ) we deduce much as in ( 71 ) that

  -- -------- --
     @xmath   
  -- -------- --

Using Lemma 10.2 (ii) in the first and (v) in the second inequality, we
conclude that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Now we look at the case @xmath . We take a cube @xmath of radius @xmath
, centered at @xmath , and set @xmath . Then we can find a partition of
@xmath into disjoint sets @xmath , @xmath , where @xmath is a cube such
that for some @xmath depending only on @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

For @xmath , we fix an arbitrary @xmath . Let @xmath . Applying first
Lemma 10.2 (iii) and then Lemma 10.1 (i) gives

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (72)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

As the @xmath overlap, we refine them as follows: Set @xmath , and split
@xmath into a collection of disjoint measurable sets @xmath , @xmath ,
such that @xmath and @xmath for some @xmath . By construction we can
find constants @xmath such that @xmath and, for @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

which implies by ( 8 ) that

  -- -------- --
     @xmath   
  -- -------- --

For @xmath we then have

  -- -------- --
     @xmath   
  -- -------- --

Plugging the last line into ( 72 ),

  -- -------- --
     @xmath   
  -- -------- --

A reapplication of Lemma 10.2 (iii), (vii) and then (ii) yields

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The reverse inequality in both the cases @xmath and @xmath is obtained
similarly.
(ii) Let @xmath and @xmath . For @xmath with @xmath we set

  -- -------- -- ------
     @xmath      (73)
  -- -------- -- ------

Then

  -- -------- --
     @xmath   
  -- -------- --

Choose a cutoff function @xmath with compact support in @xmath such that
@xmath on @xmath . Setting @xmath for @xmath , we define

  -- -------- --
     @xmath   
  -- -------- --

By ( 8 ) we have the representation

  -- -------- --
     @xmath   
  -- -------- --

Notice that @xmath , with @xmath if @xmath or @xmath . The Poisson
integral @xmath , solves the Dirichlet problem

  -- -------- -- ------
     @xmath      (74)
  -- -------- -- ------

where @xmath is the Laplace operator with respect to @xmath . Moreover,
by Corollary 6.5.4 of Krylov [ 21 ] , @xmath is smooth on @xmath . Write

  -- -------- --
     @xmath   
  -- -------- --

Theorem 6.3.2 in the same book shows that for some @xmath independent of
@xmath

  -- -------- --
     @xmath   
  -- -------- --

A direct calculation shows that @xmath . Now the claim follows from

  -- -------- --
     @xmath   
  -- -------- --

(iii) Let @xmath . Choose @xmath next to @xmath and @xmath next to
@xmath . Then @xmath if @xmath and @xmath otherwise. By the triangle
inequality,

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
  -- -------- -------- -------- --

By parts (i) and (ii) combined with the mean value theorem, we get for
the middle term

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath , then @xmath , so that we can write the first term of ( 10.2
) as

  -- -------- --
     @xmath   
  -- -------- --

Set @xmath . Then by Lemmata 3.2 (iii) and 3.4 ,

  -- -------- --
     @xmath   
  -- -------- --

For all @xmath , we have by Lemma 10.2 (i) that @xmath . If @xmath ,
then part (iii) gives @xmath . Altogether,

  -- -------- --
     @xmath   
  -- -------- --

The third term of ( 10.2 ) is treated in exactly the same way. @xmath

### 10.3 Proof of Lemma 3.2

We start with an auxiliary lemma, which already includes the upper bound
of part (iii).

###### Lemma 10.3.

Let @xmath , @xmath , and set @xmath .

1.  -- -------- --
         @xmath   
      -- -------- --

2.  -- -------- --
         @xmath   
      -- -------- --

3.  -- -------- --
         @xmath   
      -- -------- --

Proof: (i) We can assume that @xmath . If @xmath , then @xmath . Using
Lemma 3.1 (iii), we compute for any @xmath with @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

By Lemma 3.2 (i) it follows that uniformly in @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Thus, by the strong Markov property at @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Since by time reversibility of simple random walk

  -- -------- --
     @xmath   
  -- -------- --

the claim is proved.
(ii) We may assume that @xmath and @xmath . Choose a point @xmath
outside @xmath such that @xmath and @xmath . Then @xmath . Furthermore,
since @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We apply twice the strong Markov property and obtain

  -- -------- --
     @xmath   
  -- -------- --

Evaluating the expression in Lemma 3.1 (iii) shows

  -- -------- --
     @xmath   
  -- -------- --

which concludes the proof of part (ii).
(iii) By (ii) it suffices to prove that for some constant @xmath and for
all @xmath

  -- -------- -- ------
     @xmath      (76)
  -- -------- -- ------

Let @xmath and @xmath be the constants from (i) and (ii), respectively.
Define @xmath and @xmath . For @xmath there is nothing to prove since
@xmath . Thus let @xmath , and choose @xmath with @xmath . Assume that (
76 ) is proved for all @xmath . We show that ( 76 ) also holds for
@xmath . For @xmath with @xmath , it follows from (i) that

  -- -------- --
     @xmath   
  -- -------- --

If @xmath , then by (ii) and the fact that @xmath

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

If @xmath , then again by (i)

  -- -------- --
     @xmath   
  -- -------- --

This proves the claim. @xmath

Proof of Lemma 3.2 : (i) follows from Proposition 6.4.2 of [ 24 ] .
(ii) We consider different cases. If @xmath , then @xmath and thus by
Lemma 3.2 (i)

  -- -------- --
     @xmath   
  -- -------- --

For the rest of the proof we assume that @xmath . Set @xmath . First we
argue that in the case @xmath , we only have to prove the bound for
@xmath . Indeed, if @xmath , we get an upper bound by replacing @xmath
by @xmath . For @xmath , the strong Markov property together with Lemma
3.2 (i) yields

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Now we prove the claim for @xmath . We take a point @xmath closest to
@xmath . If @xmath for all @xmath , then by Lemma 10.3 (iii)

  -- -------- --
     @xmath   
  -- -------- --

As a subset of @xmath , @xmath contains on the order of @xmath points.
Therefore, by Lemma 3.1 (i), we deduce that there exists some @xmath
such that

  -- -------- --
     @xmath   
  -- -------- --

We conclude that

  -- -------- -------- -------- -------- ------
     @xmath   @xmath   @xmath            (77)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- ------

On the other hand, if @xmath for some @xmath , then

  -- -------- --
     @xmath   
  -- -------- --

and thus

  -- -------- -- ------
     @xmath      (78)
  -- -------- -- ------

If @xmath , we use Lemma 3.2 (i) again. For @xmath , we get by Lemma 3.1
(iii)

  -- -------- --
     @xmath   
  -- -------- --

Together with ( 78 ), this proves the claim in this case. Altogether, we
have proved the bound for @xmath . It remains to handle the case @xmath
. If @xmath , we have that

  -- -------- --
     @xmath   
  -- -------- --

and thus, using @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Therefore Lemma 10.3 (iii) yields

  -- -------- --
     @xmath   
  -- -------- --

Again by Lemma 3.1 (i), we find some @xmath such that

  -- -------- --
     @xmath   
  -- -------- --

A similar argument to ( 77 ), with @xmath replaced by @xmath , finishes
the proof of (ii).
(iii) It only remains to prove the lower bound. Let @xmath . First
assume @xmath . Then Lemma 3.1 (iii) gives

  -- -------- --
     @xmath   
  -- -------- --

and the claim follows from the strong Markov property and Lemma 3.1 (i).
Now assume @xmath . Let @xmath such that @xmath and @xmath . If @xmath ,
there is by Lemma 3.1 (i) a strictly positive probability to exit the
ball @xmath within @xmath . Since by the same lemma,

  -- -------- -- ------
     @xmath      (79)
  -- -------- -- ------

we obtain the claim in this case again by applying the strong Markov
property. Finally, assume @xmath . Then a careful evaluation of the
expression in Lemma 3.1 (iii) shows

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

By a simple geometric consideration and again Lemma 3.1 (i), the second
probability on the right side is bounded from below by some @xmath , and
the first probability has already been estimated in ( 79 ). @xmath

### 10.4 Proofs of Propositions 4.1 and 4.2

Since @xmath , it suffices to look at @xmath and @xmath . Recall the
definition of @xmath from Section 4.1 .

Proof of Proposition 4.1 : For bounded @xmath , that is @xmath for some
@xmath , the result is a special case of [ 24 ] , Theorem 2.1.1. Also,
for @xmath and all @xmath , the statement follows from Lemma 10.2 (i).
We therefore have to prove the proposition only for large @xmath and
@xmath . To this end, let @xmath , and for @xmath set

  -- -------- --
     @xmath   
  -- -------- --

The Fourier inversion formula gives

  -- -------- --
     @xmath   
  -- -------- --

We decompose the integral into

  -- -------- --
     @xmath   
  -- -------- --

where, with @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

By completing the square in the exponential, we get

  -- -------- --
     @xmath   
  -- -------- --

For @xmath and @xmath , we expand @xmath in a series around the origin,

  -- -------- -------- -------- -- ------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (80)
  -- -------- -------- -------- -- ------

Therefore,

  -- -------- --
     @xmath   
  -- -------- --

so that

  -- -------- --
     @xmath   
  -- -------- --

Similarly, @xmath is bounded by

  -- -------- --
     @xmath   
  -- -------- --

Concerning @xmath , we follow closely [ 6 ] , proof of Proposition B1,
and split the integral further into

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath and @xmath are constants that will be chosen in a moment,
independently of @xmath and @xmath . By ( 10.4 ), we can find @xmath
such that for @xmath , @xmath (recall that @xmath ). Then

  -- -------- --
     @xmath   
  -- -------- --

As a consequence of Lemma 3.1 (i) and of our coarse graining, it follows
that for any @xmath , one has for some @xmath , uniformly in @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Using this fact,

  -- -------- --
     @xmath   
  -- -------- --

To deal with the last two integrals is more delicate since we have to
take into account the @xmath -dependency. First,

  -- -------- --
     @xmath   
  -- -------- --

We bound the integrand pointwise. Since @xmath is invariant under
rotations preserving @xmath , it suffices to look at @xmath with all
components positive. Assume @xmath . Set @xmath and @xmath . Notice that
@xmath implies @xmath . By taking @xmath large enough, we can assume
that on the domain of integration, @xmath . First,

  -- -------- --
     @xmath   
  -- -------- --

Inside the @xmath -summation, we write for each @xmath separately

  -- -------- --
     @xmath   
  -- -------- --

where @xmath . By Corollary 10.1 ,

  -- -------- --
     @xmath   
  -- -------- --

Thus,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

On our domain of integration, @xmath for large @xmath . Therefore,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

and altogether for sufficiently large @xmath , @xmath and @xmath ,

  -- -- --
        
  -- -- --

For @xmath we again assume all components of @xmath positive and @xmath
. Since

  -- -------- --
     @xmath   
  -- -------- --

we have

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The sum over the exponentials is estimated by @xmath , so that again
with Corollary 10.1 ,

  -- -------- --
     @xmath   
  -- -------- --

Hence, for @xmath close to @xmath and large @xmath , @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

@xmath

For Proposition 4.2 , we still need a large deviation estimate.

###### Lemma 10.4 (Large deviation estimate).

There exist constants @xmath such that for @xmath

  -- -------- --
     @xmath   
  -- -------- --

Proof: Write @xmath for @xmath and @xmath for the expectation with
respect to @xmath , and denote by @xmath the @xmath th component of the
random walk @xmath under @xmath . For @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

We claim that

  -- -------- --
     @xmath   
  -- -------- --

By the martingale maximal inequality for all @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath and @xmath is convex, it follows that

  -- -------- --
     @xmath   
  -- -------- --

Therefore, using the symmetry of @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Putting @xmath we get

  -- -------- --
     @xmath   
  -- -------- --

From this it follows that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

@xmath

Proof of Proposition 4.2 : (i) follows from Proposition 4.1 . For (ii),
we set

  -- -------- --
     @xmath   
  -- -------- --

We split @xmath into

  -- -------- --
     @xmath   
  -- -------- --

For the first sum on the right, we use the large deviation estimate from
Lemma 10.4 ,

  -- -------- --
     @xmath   
  -- -------- --

In the second sum, we replace the transition probabilities by the
expressions obtained in Proposition 4.1 . The error terms are estimated
by

  -- -------- --
     @xmath   
  -- -------- --

Putting @xmath , we obtain for the main part

  -- -------- -------- -------- --
     @xmath                     
                                
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

This proves the statement for @xmath with

  -- -------- --
     @xmath   
  -- -------- --

@xmath

### Acknowledgments

I would like to thank Erwin Bolthausen and Ofer Zeitouni for many
helpful discussions and constant support.