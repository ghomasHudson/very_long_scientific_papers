## Chapter 1 Introduction

  --------------------------------------------------------------------------------------------------------------------------------------------------------------
  … to divide each of the difficulties under examination into as many parts as possible, and as might be necessary for its adequate solution. - René Descartes
  --------------------------------------------------------------------------------------------------------------------------------------------------------------

### 1 Motivation

Structural (or syntactic) disambiguation in sentence analysis is still a
central problem in natural language processing. To resolve ambiguities
completely, we would need to construct a human language ‘understanding’
system [ \citename Johnson-Laird1983 , \citename Tsujii1987 , \citename
Altmann and Steedman1988 ] . The construction of such a system would be
extremely difficult, however, if not impossible. For example, when
analyzing the sentence

  -- -- -- -----
           (1)
  -- -- -- -----

a natural language processing system may obtain two interpretations: “I
ate ice cream using a spoon” and “I ate ice cream and a spoon.” i.e., a
pp-attachment ambiguity may arise, because the prepositional phrase
‘with a spoon’ can syntactically be attached to both ‘eat’ and ‘ice
cream.’ If a human speaker reads the same sentence, common sense will
certainly lead him to assume the former interpretation over the latter,
because he understands that: “a spoon is a tool for eating food,” “a
spoon is not edible,” etc. Incorporating such ‘world knowledge’ into a
natural language processing system is highly difficult, however, because
of its sheer enormity.

An alternative approach is to make use of only lexical semantic
knowledge, specifically case frame patterns [ \citename Fillmore1968 ]
(or their near equivalents: selectional patterns [ \citename Katz and
Fodor1963 ] , and subcategorization patterns [ \citename Pollard and
Sag1987 ] ). That is, to represent the content of a sentence or a phrase
with a ‘case frame’ having a ‘head’ ¹ ¹ 1 I slightly abuse terminology
here, as ‘head’ is usually used for subcategorization patterns in the
discipline of HPSG, but not in case frame theory. and multiple ‘slots,’
and to incorporate into a natural language processing system the
knowledge of which words can fill into which slot of a case frame.

For example, we can represent the sentence “I ate ice cream” as

  -- -- --
        
  -- -- --

where the head is ‘eat,’ the arg1 slot represents the subject and the
arg2 slot represents the direct object. The values of the arg1 slot and
the arg2 slot are ‘I’ and ‘ice cream,’ respectively. Furthermore, we can
incorporate as the case frame patterns for the verb ‘eat’ the knowledge
that a member of the word class @xmath animal @xmath can be the value of
the arg1 slot and a member of the word class @xmath food @xmath can be
the value of the arg2 slot, etc.

The case frames of the two interpretations obtained in the analysis of
the above sentence ( 1 ), then, become

  -- -------- --
     @xmath   
  -- -------- --

Referring to the case frame patterns indicating that ‘spoon’ can be the
value of the ‘with’ slot when the head is ‘eat,’ and ‘spoon’ cannot be
the value of the ‘with’ slot when the head is ‘ice cream,’ a natural
language processing system naturally selects the former interpretation
and thus resolves the ambiguity.

Previous data analyses have indeed indicated that using lexical semantic
knowledge can, to a quite large extent, cope with the structural
disambiguation problem [ \citename Hobbs and Bear1990 , \citename
Whittemore, Ferrara, and Brunner1990 ] . The advantage of the use of
lexical knowledge over that of world knowledge is the relative smallness
of its amount. By restricting knowledge to that of relations between
words , the construction of a natural language processing system becomes
much easier. (Although the lexical knowledge is still unable to resolve
the problem completely, past research suggests that it might be the most
realistic path we can take right now.)

As is made clear in the above example, case frame patterns mainly
include ‘generalized information,’ e.g., that a member of the word class
@xmath animal @xmath can be the value of the arg2 slot for the verb
‘eat.’

Classically, case frame patterns are represented by ‘selectional
restrictions’ [ \citename Katz and Fodor1963 ] , i.e., discretely
represented by semantic features, but it is better to represent them
continuously, because a word can be the value of a slot to a certain
probabilistic degree , as is suggested by the following list [ \citename
Resnik1993b ] :

  -- -------- --
     @xmath   
  -- -------- --

Furthermore, case frame patterns are not limited to reference to
individual case slots. Dependencies between case slots need also be
considered. The term ‘dependency’ here refers to the relationship that
may exist between case slots and that indicates strong co-occurrence
between the values of those case slots. For example, consider the
following sentences: ² ² 2 ‘*’ indicates an unacceptable natural
language expression.

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

We see that an ‘airline company’ can be the value of the arg1 slot, when
the value of the arg2 slot is an ‘airplane’ but not when it is an
‘airline company.’ These sentences indicate that the possible values of
case slots depend in general on those of others: dependencies between
case slots exist. ³ ³ 3 One may argue that ‘fly’ has different word
senses in these sentences and for each of these word senses there is no
dependency between the case slots. Word senses are in general difficult
to define precisely, however. I think that it is preferable not to
resolve them until doing so is necessary in a particular application.
That is to say that, in general, case dependencies do exist and the
development of a method for learning them is needed.

Another consensus on lexical semantic knowledge in recent studies is
that it is preferable to learn lexical knowledge automatically from
corpus data. Automatic acquisition of lexical knowledge has the merits
of (1) saving the cost of defining knowledge by hand, (2) doing away
with the subjectivity inherent in human-defined knowledge, and (3)
making it easier to adapt a natural language processing system to a new
domain.

Although there have been many studies conducted in the past (described
here in Chapter 2) to address the lexical knowledge acquisition problem,
further investigation, especially that based on a principled methodology
is still needed, and this is, in fact, the problem I address in this
thesis.

The search for a mathematical formalism for lexical knowledge
acquisition is not only motivated by concern for logical niceties; I
believe that it can help to better cope with practical problems (for
example, the disambiguation problem). The ultimate outcome of the
investigations in this thesis, therefore, should be a formalism of
lexical knowledge acquisition and at the same time a high-performance
disambiguation method.

### 2 Problem Setting

The problem of acquiring and using lexical semantic knowledge,
especially that of case frame patterns, can be formalized as follows. A
learning module acquires case frame patterns on the basis of some case
frame instances extracted from corpus data. A processing
(disambiguation) module then refers to the acquired knowledge and judges
the degrees of acceptability of some new case frames, including
previously unseen ones. The goals of learning are to represent more
compactly the given case frames, and to judge more correctly the degrees
of acceptability of new case frames.

In this thesis, I propose a probabilistic approach to lexical knowledge
acquisition and structural disambiguation.

### 3 Approach

In general, a machine learning process consists of three elements:
model, strategy (criterion), and algorithm. That is, when we conduct
machine learning, we need consider (1) what kind of model we are to use
to represent the problem, (2) what kind of strategy we should adopt to
control the learning process, and (3) what kind of algorithm we should
employ to perform the learning task. We need to consider each of these
elements here.

##### Division into subproblems

The lexical semantic knowledge acquisition problem is a quite
complicated task, and there are too many relevant factors
(generalization of case slot values, dependencies between case slots,
etc.) to simply incorporate all of them into a single model. As a first
step, I divide the problem into three subproblems: case slot
generalization, case dependency learning, and word clustering (thesaurus
construction).

I define probability models (probability distributions) for each
subproblem and view the learning task of each subproblem as that of
estimating its corresponding probability models based on corpus data.

##### Probability models

We can assume that case slot data for a case slot for a verb are
generated on the basis of a conditional probability distribution that
specifies the conditional probability of a noun given the verb and the
case slot. I call such a distribution a ‘case slot model.’ When the
conditional probability of a noun is defined as the conditional
probability of the noun class to which the noun belongs, divided by the
size of the noun class, I call the case slot model a ‘hard case slot
model.’ When the case slot model is defined as a finite mixture model,
namely a linear combination of the word probability distributions within
individual noun classes, I call it a ‘soft case slot model.’

Here the term ‘hard’ means that the model is characterized by a type of
word clustering in which a word can only belong to a single class alone,
while ‘soft’ means that the model is characterized by a type of word
clustering in which a word can belong to several different classes.

I formalize the problem of generalizing the values of a case slot as
that of estimating a hard (or soft) case slot model. The generalization
problem, then, turns out to be that of selecting a model, from a class
of hard (or soft) case slot models, which is most likely to have given
rise to the case slot data.

We can assume that case frame data for a verb are generated according to
a multi-dimensional joint probability distribution over random variables
that represent the case slots. I call the distribution a ‘case frame
model.’ I further classify this case frame model into three types of
probability models each reflecting the type of its random variables: the
‘word-based case frame model,’ the ‘class-based case frame model,’ and
the ‘slot-based case frame model.’

I formalize the problem of learning dependencies between case slots as
that of estimating a case frame model. The dependencies between case
slots are represented as probabilistic dependencies between random
variables.

We can assume that co-occurrence data for nouns and verbs with respect
to a slot are generated based on a joint probability distribution that
specifies the co-occurrence probabilities of noun verb pairs. I call
such a distribution a ‘co-occurrence model.’ I call this co-occurrence
model a ‘hard co-occurrence model,’ when the joint probability of a noun
verb pair is defined as the product of the following three elements: (1)
the joint probability of the noun class and the verb class to which the
noun and the verb respectively belong, (2) the conditional probability
of the noun given its noun class, and (3) the conditional probability of
the verb given its verb class. When the co-occurrence model is defined
as a double mixture model, namely, a double linear combination of the
word probability distributions within individual noun classes and those
within individual verb classes, I call it a ‘soft co-occurrence model.’

I formalize the problem of clustering words as that of estimating a hard
(or soft) co-occurrence model. The clustering problem, then, turns out
to be that of selecting a model from a class of hard (or soft)
co-occurrence models, which is most likely to have given rise to the
co-occurrence data.

##### MDL as strategy

For all subproblems, the learning task turns out to be that of selecting
the best model from among a class of models. The question now is what
the learning strategy (or criterion) is to be. I employ here the Minimum
Description Length (MDL) principle. The MDL principle is a principle for
both data compression and statistical estimation (described in Chapter
2).

MDL provides a theoretically way to deal with the ‘data sparseness
problem,’ the main difficulty in a statistical approach to language
processing. At the same time, MDL leads us to an information-theoretic
solution to the lexical knowledge acquisition problem, in which case
frames are viewed as structured data , and the learning process turns
out to be that of data compression .

##### Efficient algorithms

In general, there is a trade-off between model classes and algorithms. A
complicated model class would be precise enough for representing a
problem, but it might be difficult to learn in terms of learning
accuracy and computation time. In contrast, a simple model class might
be easy to learn, but it would be too simplistic for representing a
problem.

In this thesis, I place emphasis on efficiency and restrict a model
class when doing so is still reasonable for representing the problem at
hand.

For the case slot generalization problem, I make use of an existing
thesaurus and restrict the class of hard case slot models to that of
‘tree cut models.’ I also employ an efficient algorithm, which provably
obtains the optimal tree cut model in terms of MDL.

For the case dependency learning problem, I restrict the class of case
frame models to that of ‘dependency forest models,’ and employ another
efficient algorithm to learn the optimal dependency forest model in
terms of MDL.

For the word clustering problem, I address the issue of estimating the
hard co-occurrence model, and employ an efficient algorithm to
repeatedly estimate a suboptimal MDL model from a class of hard
co-occurrence models.

##### Disambiguation methods

I then view the structural disambiguation problem as that of statistical
prediction . Specifically, the likelihood value of each interpretation
(case frame) is calculated on the basis of the above models, and the
interpretation with the largest likelihood value is output as the
analysis result.

I have devised several disambiguation methods along this line.

One of them is especially useful when the data size for training is at
the level of that currently available. In implementation of this method,
the learning module combines with the hard co-occurrence model to
cluster words with respect to each case slot, and it combines with the
tree cut model to generalize the values of each case slot by means of a
hand-made thesaurus. The disambiguation module first calculates a
likelihood value for each interpretation on the basis of hard
co-occurrence models and outputs the interpretation with the largest
likelihood value; if the likelihood values are equal (most particularly,
if all of them are 0), it uses likelihood values calculated on the basis
of tree cut models; if the likelihood values are still equal, it makes a
default decision.

The accuracy achieved by this method is @xmath , which is higher than
that of state-of-the-art methods.

### 4 Organization of the Thesis

This thesis is organized as follows. In Chapter 2, I review previous
work on lexical semantic knowledge acquisition and structural
disambiguation. I also introduce the MDL principle. In Chapter 3, I
define probability models for each subproblem of lexical semantic
knowledge acquisition. In Chapter 4, I describe the method of using the
tree cut model to generalize case slots. In Chapter 5, I describe the
method of using the dependency forest model to learn dependencies
between case slots. In Chapter 6, I describe the method of using the
hard co-occurrence model to conduct word clustering. In Chapter 7, I
describe the practical disambiguation method. In Chapter 8, I conclude
the thesis with some remarks (see Figure 1 ).

## Chapter 2 Related Work

  ----------------------------------------------------------------------------------
  Continue to cherish old knowledge so as to continue to discover new. - Confucius
  ----------------------------------------------------------------------------------

In this chapter, I review previous work on lexical knowledge acquisition
and disambiguation. I also introduce the MDL principle.

### 1 Extraction of Case Frames

Extracting case frame instances automatically from corpus data is a
difficult task, because when conducting extraction, ambiguities may
arise, and we need to exploit lexical semantic knowledge to resolve
them. Since our goal of extraction is indeed to acquire such knowledge,
we are faced with the problem of which is to come first, the chicken or
the egg.

Although there have been many methods proposed to automatically extract
case frames from corpus data, their accuracies do not seem completely
satisfactory, and the problem still needs investigation.

\namecite

Manning92, for example, proposes extracting case frames by using a
finite state parser. His method first uses a statistical tagger (cf., [
\citename Church1988 , \citename Kupiec1992 , \citename Charniak et
al.1993 , \citename Merialdo1994 , \citename Nagata1994 , \citename
Schütze and Singer1994 , \citename Brill1995 , \citename Samuelsson1995
, \citename Ratnaparkhi1996 , \citename Haruno and Matsumoto1997 ] ) to
assign a part of speech to each word in the sentences of a corpus. It
then uses the finite state parser to parse the sentences and note case
frames following verbs. Finally, it filters out statistically unreliable
extracted results on the basis of hypothesis testing (see also [
\citename Brent1991 , \citename Brent1993 , \citename Smadja1993 ,
\citename Chen and Chen1994 , \citename Grefenstette1994 ] ).

\namecite

Briscoe97 extracted case frames by using a probabilistic LR parser. This
parser first parses sentences to obtain analyses with ‘shallow’ phrase
structures, and assigns a likelihood value to each analysis. An
extractor then extracts case frames from the most likely analyses (see
also [ \citename Hindle and Rooth1991 , \citename Grishman and
Sterling1992 ] ).

\namecite

Utsuro92 propose extracting case frames from a parallel corpus in two
different languages. Exploiting the fact that a syntactic ambiguity
found in one language may not exist at all in another language, they
conduct pattern matching between case frames of translation pairs given
in the corpus and choose the best matched case frames as extraction
results (see also [ \citename Matsumoto, Ishimoto, and Utsuro1993 ] ).

An alternative to the automatic approach is to employ a semi-automatic
method, which can provide much more reliable results. The disadvantage,
however, is its requirement of having disambiguation decisions made by a
human, and how to reduce the cost of human intervention becomes an
important issue.

\namecite

Carter97 developed an interaction system for effectively collecting case
frames semi-automatically. This system first presents a user with a
range of properties that may help resolve ambiguities in a sentence. The
user then designates the value of one of the properties, the system
discards those interpretations which are inconsistent with the
designation, and it re-displays only the properties which remain. After
several such interactions, the system obtains a most likely correct case
frame of a sentence (see also [ \citename Marcus, Santorini, and
Marcinkiewicz1993 ] ).

Using any one of the methods, we can extract case frame instances for a
verb, to obtain data like that shown in Table 1 , although no method
guarantees that the extracted results are completely correct. In this
thesis, I refer to this type of data as ‘case frame data.’ If we
restrict our attention on a specific slot, we obtain data like that
shown in Table 2 . I refer to this type of data as ‘case slot data.’

### 2 Case Slot Generalization

One case-frame-pattern acquisition problem is that of generalization of
(values of) case slots; this has been intensively investigated in the
past.

#### 2.1 Word-based approach and the data sparseness problem

Table 3 shows some example cast slot data for the arg1 slot for the verb
‘fly.’ By counting occurrences of each noun at the slot, we can obtain
frequency data shown in Figure 1 .

The problem of learning ‘case slot patterns’ for a slot for a verb can
be viewed as the problem of estimating the underlying conditional
probability distribution which gives rise to the corresponding case slot
data. The conditional distribution is defined as

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where random variable @xmath represents a value in the set of nouns
@xmath , random variable @xmath a value in the set of verbs @xmath , and
random variable @xmath a value in the set of slot names @xmath @xmath .
Since random variables take on words as their values, this type of
probability distribution is often referred to as a ‘word-based model.’
The degree of noun @xmath ’s being the value of slot @xmath for verb
@xmath ¹ ¹ 1 Hereafter, I will sometimes use the same symbol to denote
both a random variable and one of its values; it should be clear from
the context, which it is denoting at any given time. is represented by a
conditional probability.

Another way of learning case slot patterns for a slot for a verb is to
calculate the ‘association ratio’ measure, as proposed in [ \citename
Church et al.1989 , \citename Church and Hanks1989 , \citename Church et
al.1991 ] . The association ratio is defined as

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

where @xmath assumes a value from the set of nouns, @xmath from the set
of verbs and @xmath from the set of slot names. The degree of noun
@xmath being the value of slot @xmath for verb @xmath is represented as
the ratio between a conditional probability and a marginal probability.

The two measures in fact represent two different aspects of case slot
patterns. The former indicates the relative frequency of a noun’s being
the slot value, while the latter indicates the strength of
associativeness between a noun and the verb with respect to the slot.
The advantage of the latter may be that it takes into account of the
influence of the marginal probability @xmath on the conditional
probability @xmath . The advantage of the former may be its ease of use
in disambiguation as a likelihood value.

Both the use of the conditional probability and that of the association
ratio may suffer from the ‘data sparseness problem,’ i.e., the number of
parameters in the conditional distribution defined in ( 1 ) is very
large, and accurately estimating them is difficult with the amount of
data typically available.

When we employ Maximum Likelihood Estimation (MLE) to estimate the
parameters, i.e., when we estimate the conditional probability @xmath as
² ² 2 Throughout this thesis, @xmath denotes an estimator (or an
estimate) of @xmath .

  -- -------- --
     @xmath   
  -- -------- --

where @xmath stands for the frequency of noun @xmath being the value of
slot @xmath for verb @xmath , @xmath the total frequency of @xmath for
@xmath (Figure 2 shows the results for the data in Figure 1 ), we may
obtain quite poor results. Most of the probabilities might be estimated
as 0, for example, just because a possible value of the slot in question
happens not to appear.

To overcome this problem, we can smooth the probabilities by resorting
to statistical techniques [ \citename Jelinek and Mercer1980 , \citename
Katz1987 , \citename Gale and Church1990 , \citename Ristad and
Thomas1995 ] . We can, for example, employ an extended version of the
Laplace’s Law of Succession (cf., [ \citename Jeffreys1961 , \citename
Krichevskii and Trofimov1981 ] ) to estimate @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the size of the set of nouns. ³ ³ 3 This smoothing
method can be justified from the viewpoint of Bayesian Estimation. The
estimate is in fact the Bayesian estimate with Jeffrey’s Prior being the
prior probability.

The results may still not be satisfactory, however. One possible way to
cope better with the data sparseness problem is to exploit additional
knowledge or data rather than make use of only related case slot data.
Two such approaches have been proposed previously: one is called the
‘similarity-based approach,’ the other the ‘class-based approach.’

#### 2.2 Similarity-based approach \namecite

Grishman94 propose to estimate conditional probabilities by using other
conditional probabilities under contexts of similar words, where the
similar words themselves are collected on the basis of corpus data.
Their method estimates the conditional probability @xmath as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath represents a verb similar to verb @xmath , and @xmath the
similarity between @xmath and @xmath . That is, it smoothes a
conditional probability by taking the weighted average of other
conditional probabilities under contexts of similar words using
similarities as the weights. Note that the equation

  -- -------- --
     @xmath   
  -- -------- --

must hold. The advantage of this approach is that it relies only on
corpus data. (Cf., [ \citename Dagan, Marcus, and Makovitch1992 ,
\citename Dagan, Pereira, and Lee1994 , \citename Dagan, Pereira, and
Lee1997 ] .)

#### 2.3 Class-based approach

A number of researchers have proposed to employ ‘class-based models,’
which use classes of words rather than individual words.

An example of a class-based approach is Resnik’s method of learning case
slot patterns by calculating the ‘selectional association’ measure [
\citename Resnik1993a , \citename Resnik1993b ] . The selectional
association is defined as:

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

where @xmath represents a value in the set of nouns, @xmath a value in
the set of verbs and @xmath a value in the set of slot names, and @xmath
denotes a class of nouns present in a given thesaurus. (See also [
\citename Framis1994 , \citename Ribas1995 ] .) This measure, however,
is based on heuristics, and thus can be difficult to justify
theoretically.

Other class-based methods for case slot generalization are also proposed
[ \citename Almuallim et al.1994 , \citename Tanaka1994 , \citename
Tanaka1996 , \citename Utsuro and Matsumoto1997 , \citename Miyata,
Utsuro, and Matsumoto1997 ] .

### 3 Word Clustering

Automatically clustering words or constructing a thesaurus can also be
considered to be a class-based approach, and it helps cope with the data
sparseness problem not only in case frame pattern acquisition but also
in other natural language learning tasks.

If we focus our attention on one case slot, we can obtain ‘co-occurrence
data’ for verbs and nouns with respect to that slot. Figure 3 , for
example, shows such data, in this case, counts of co-occurrences of
verbs and their arg2 slot values (direct objects). We can classify words
by using such co-occurrence data on the assumption that semantically
similar words have similar co-occurrence patterns.

A number of methods have been proposed for clustering words on the basis
of co-occurrence data. \namecite Brown92, for example, propose a method
of clustering words on the basis of MLE in the context of n-gram
estimation. They first define an n-gram class model as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath represents a word class. They then view the clustering
problem as that of partitioning the vocabulary (a set of words) into a
designated number of word classes whose resulting 2-gram class model has
the maximum likelihood value with respect to a given word sequence
(i.e., co-occurrence data). Brown et al have also devised an efficient
algorithm for performing this task, which turns out to iteratively merge
the word class pair having the least reduction in empirical mutual
information until the number of classes created equals the designated
number. The disadvantage of this method is that one has to designate in
advance the number of classes to be created, with no guarantee at all
that this number will be optimal.

\namecite

Pereira93 propose a method of clustering words based on co-occurrence
data over two sets of words. Without loss of generality, suppose that
the two sets are a set of nouns @xmath and a set of verbs @xmath , and
that a sample of co-occurrence data is given as @xmath . They define

  -- -------- --
     @xmath   
  -- -------- --

as a model which can give rise to the co-occurrence data, where @xmath
represents a class of nouns. They then view the problem of clustering
nouns as that of estimating such a model. The classes obtained in this
way, which they call ‘soft clustering,’ have the following properties:
(1) a noun can belong to several different classes, and (2) each class
is characterized by a membership distribution. They devised an efficient
clustering algorithm based on ‘deterministic annealing technique.’ ⁴ ⁴ 4
Deterministic annealing is a computation technique for finding the
global optimum (minimum) value of a cost function [ \citename Rose,
Gurewitz, and Fox1990 , \citename Ueda and Nakano1998 ] . The basic idea
is to conduct minimization by using a number of ‘free energy’ functions
parameterized by ‘temperatures’ for which free energy functions with
high temperatures loosely approximate a target function, while free
energy functions with low temperatures precisely approximate it. A
deterministic-annealing-based algorithm manages to find the global
minimum value of the target function by continuously finding the minimum
values of the free energy functions while incrementally decreasing the
temperatures. (Note that deterministic annealing is different from the
classical ‘simulated annealing’ technique [ \citename Kirkpatrick,
Gelatt, and Vecchi1983 ] .) In Pereira et al’s case, deterministic
annealing is used to find the minimum of average distortion. They have
proved that, in their problem setting, minimizing average distortion is
equivalent to maximizing likelihood with respect to the given data
(i.e., MLE). Conducting soft clustering makes it possible to cope with
structural and word sense ambiguity at the same time, but it also
requires more training data and makes the learning process more
computationally demanding.

\namecite

Tokunaga95 point out that, for disambiguation purposes, it is necessary
to construct one thesaurus for each case slot on the basis of
co-occurrence data concerning to that slot. Their experimental results
indicate that, for disambiguation, the use of thesauruses constructed
from data specific to the target slot is preferable to the use of
thesauruses constructed from data non-specific to the slot.

Other methods for automatic word clustering have also been proposed [
\citename Hindle1990 , \citename Pereira and Tishby1992 , \citename
McKeown and Hatzivassiloglou1993 , \citename Grefenstette1994 ,
\citename Stolcke and Omohundro1994 , \citename Abe, Li, and
Nakamura1995 , \citename McMahon and Smith1996 , \citename Ushioda1996 ,
\citename Hogenhout and Matsumoto1997 ] .

### 4 Case Dependency Learning

There has been no method proposed to date, however, that learns
dependencies between case slots. In past research, methods of resolving
ambiguities have been based, for example, on the assumption that case
slots are mutually independent [ \citename Hindle and Rooth1991 ,
\citename Sekine et al.1992 , \citename Resnik1993a , \citename Grishman
and Sterling1994 , \citename Alshawi and Carter1994 ] , or at most two
case slots are dependent [ \citename Brill and Resnik1994 , \citename
Ratnaparkhi, Reynar, and Roukos1994 , \citename Collins and Brooks1995 ]
.

### 5 Structural Disambiguation

#### 5.1 The lexical approach

There have been many probabilistic methods proposed in the literature to
address the structural disambiguation problem. Some methods tackle the
basic problem of resolving ambiguities in quadruples @xmath (e.g., (eat,
ice-cream, with, spoon)) by mainly using lexical knowledge. Such methods
can be classified into the following three types: the double approach,
the triple approach, and the quadruple approach.

The first two approaches employ what I call a ‘generation model’ and the
third approach employs what I call a ‘decision model’ (cf., Chapter 3).

##### The double approach

This approach takes doubles of the form @xmath and @xmath , like those
in Table 4 , as training data to acquire lexical knowledge and judges
the attachment sites of @xmath in quadruples based on the acquired
knowledge.

\namecite

Hindle91 propose the use of the so-called ‘lexical association’ measure
calculated based on such doubles:

  -- -------- --
     @xmath   
  -- -------- --

where random variable @xmath represents a verb (in general a head), and
random variable @xmath a slot (preposition). They further propose
viewing the disambiguation problem as that of hypothesis testing. More
specifically, they calculate the ‘t-score,’ which is a statistic on the
difference between the two estimated probabilities @xmath and @xmath :

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath denote the standard deviations of @xmath and
@xmath , respectively, and @xmath and @xmath denote the data sizes used
to estimate these probabilities. If, for example, @xmath , then @xmath
is attached to @xmath , @xmath , @xmath is attached to @xmath , and
otherwise no decision is made. (See also [ \citename Hindle and
Rooth1993 ] .)

##### The triple approach

This approach takes triples @xmath and @xmath , i.e., case slot data,
like those in Table 5 , as training data for acquiring lexical
knowledge, and performs pp-attachment disambiguation on quadruples.

For example, \namecite Resnik93a proposes the use of the selectional
association measure (described in Section 2) calculated on the basis of
such triples. The basic idea of his method is to compare @xmath and
@xmath defined in ( 3 ), and make a disambiguation decision.

\namecite

Sekine92 propose the use of joint probabilities @xmath and @xmath in
pp-attachment disambiguation. They devised a heuristic method for
estimating the probabilities. (See also [ \citename Alshawi and
Carter1994 ] .)

##### The quadruple approach

This approach receives quadruples @xmath , as well as labels that
indicate which way the pp-attachment goes, such as those in Table 6 ;
and it learns disambiguation rules.

It in fact employs the conditional probability distribution – a
‘decision model’

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

where random variable @xmath takes on attv and attn as its values, and
random variables @xmath take on quadruples as their values. Since the
number of parameters in the distribution is very large, accurate
estimation of the distribution would be impossible.

In order to address this problem, \namecite Collins95 devised a back-off
method. It first calculates the conditional probability @xmath by using
the relative frequency

  -- -------- --
     @xmath   
  -- -------- --

if the denominator is larger than 0; otherwise it successively uses
lower order frequencies to heuristically calculate the probability.

\namecite

Ratnaparkhi94 propose to learn the conditional probability distribution
( 4 ) with Maximum Entropy Estimation. They adopt the Maximum Entropy
Principle (MEP) as the learning strategy, which advocates selecting the
model having the maximum entropy from among the class of models that
satisfies certain constraints (see Section 7.4 for a discussion on the
relation between MDL and MEP). The fact that a model must be one such
that the expected value of a feature with respect to it equals that with
respect to the empirical distribution is usually used as a constraint.
Ratnaparkhi et al’s method defines, for example, a feature as follows

  -- -------- --
     @xmath   
  -- -------- --

It then incrementally selects features, and efficiently estimates the
conditional distribution by using the Maximum Entropy Estimation
technique (see [ \citename Jaynes1978 , \citename Darroch and
Ratcliff1972 , \citename Berger, Pietra, and Pietra1996 ] ).

Another method of the quadruple approach is to employ
‘transformation-based error-driven learning’ [ \citename Brill1995 ] ,
as proposed in [ \citename Brill and Resnik1994 ] . This method learns
and uses IF-THEN type rules, where the IF parts represent conditions
like ( @xmath is ‘with’) and ( @xmath is ‘see’), and the THEN parts
represent transformations from (attach to @xmath ) to (attach to @xmath
), and vice-versa. The first rule is always a default decision, and all
the other rules indicate transformations (changes of attachment sites)
subject to various IF conditions.

#### 5.2 The combined approach

Although the use of lexical knowledge can effectively resolve
ambiguities, it still has limitation. It is preferable, therefore, to
utilize other kind of knowledge in disambiguation, especially when a
decision cannot be made solely on the basis of lexical knowledge.

The following two facts suggest that syntactic knowledge should also be
used for the purposes of disambiguation. First, interpretations are
obtained through syntactic parsing. Second, psycholinguistists observe
that there are certain syntactic principles in human’s language
interpretation. For example, in English a phrase on the right tends to
be attached to the nearest phrase on the left, - referred to as the
‘right association principle’ [ \citename Kimball1973 ] . (See also [
\citename Ford, Bresnan, and Kaplan1982 , \citename Frazier and
Fodor1979 , \citename Hobbs and Bear1990 , \citename Whittemore,
Ferrara, and Brunner1990 ] ).

We are thus led to the problem of how to define a probability model
which combines the use of both lexical semantic knowledge and syntactic
knowledge. One approach is to introduce probability models on the basis
of syntactic parsing. Another approach is to introduce probability
models on the basis of psycholinguistic principles [ \citename Li1996 ]
.

Many methods belonging to the former approach have been proposed. A
classical method is to employ the PCFG (Probabilistic Context Free
Grammar) model [ \citename Fujisaki et al.1989 , \citename Jelinek,
Lafferty, and Mercer1990 , \citename Lari and Young1990 ] , in which a
CFG rule having the form of

  -- -------- --
     @xmath   
  -- -------- --

is associated with a conditional probability

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

In disambiguation the likelihood of an interpretation is defined as the
product of the conditional probabilities of the rules which are applied
in the derivation of the interpretation.

The use of PCFG, in fact, resorts more to syntactic knowledge rather
than to lexical knowledge, and its performance seems to be only
moderately good [ \citename Chitrao and Grishman1990 ] . There are also
many methods proposed which more effectively make use of lexical
knowledge.

\namecite

Collins97 proposes disambiguation through use of a generative
probability model based on a lexicalized CFG (in fact, a restricted form
of HPSG [ \citename Pollard and Sag1987 ] ). (See also [ \citename
Collins1996 , \citename Schabes1992 , \citename Hogenhout and
Matsumoto1996 , \citename Den1996 , \citename Charniak1997 ] .) In
Collins’ model, each lexicalized CFG rule is defined in the form of

  -- -------- --
     @xmath   
  -- -------- --

where a capitalized symbol denotes a category, with @xmath being the
head category on the right hand site. A category is defined in the form
of @xmath , where @xmath denotes the name of the category, @xmath the
head word associated with the category, and @xmath the part-of-speech
tag assigned to the head word. Furthermore, each rule is assigned a
conditional probability @xmath (cf., ( 5 )) that is assumed to satisfy

  -- -------- --
     @xmath   
  -- -------- --

In disambiguation, the likelihood of an interpretation is defined as the
product of the conditional probabilities of the rules which are applied
in the derivation of the interpretation. While Collins has devised
several heuristic methods for estimating the probability model, further
investigation into learning methods for this model still appears
necessary.

\namecite

Magerman95 proposes a new parsing approach based on probabilistic
decision tree models [ \citename Quinlan and Rivest1989 , \citename
Yamanishi1992a ] to replace conventional context free parsing. His
method uses decision tree models to construct parse trees in a bottom-up
and left-to-right fashion. A decision might be made, for example, to
create a new parse-tree-node, and conditions for making that decision
might be, for example, the appearances of certain words and certain tags
in a node currently being focussed upon and in its neighbor nodes.
Magerman has also devised an efficient algorithm for finding the parse
tree (interpretation) with the highest likelihood value. The advantages
of this method are its effective use of contextual information and its
non-use of a hand-made grammar. (See also [ \citename Magerman and
Marcus1991 , \citename Magerman1994 , \citename Black et al.1993 ,
\citename Ratnaparkhi1997 , \citename Haruno, Shirai, and Ooyama1998 ] )

\namecite

Su88 propose the use of a probabilistic score function for
disambiguation in generalized LR parsing (see also [ \citename Su et
al.1989 , \citename Chang, Luo, and Su1992 , \citename Chiang, Lin, and
Su1995 , \citename Wright1990 , \citename Kita1992 , \citename Briscoe
and Carroll1993 , \citename Inui, Sornlertlamvanich, and Tanaka1998 ] ).
They first introduce a conditional probability of a category obtained
after a reduction operation and in the context of the reduced categories
and of the categories immediately left and right of those reduced
categories. The score function, then, is defined as the product of the
conditional probabilities appearing in the derivation of the
interpretation. The advantage of the use of this score function is its
context-sensitivity, which can yield more accurate results in
disambiguation.

\namecite

Alshawi94 propose for disambiguation purposes the use of a linear
combination of various preference functions based on lexical and
syntactic knowledge. They have devised a method for training the weights
of a linear combination. Specifically, they employ the minimization of a
squared-error cost function as a learning strategy and employ a
‘hill-climbing’ algorithm to iteratively adjust weights on the basis of
training data.

Additionally, some non-probabilistic approaches to structural
disambiguation have also been proposed (e.g., [ \citename Wilks1975 ,
\citename Wermter1989 , \citename Nagao1990 , \citename Kurohashi and
Nagao1994 ] ).

### 6 Word Sense Disambiguation

Word sense disambiguation is an issue closely related to the structural
disambiguation problem. For example, when analyzing the sentence “Time
flies like an arrow,” we obtain a number of ambiguous interpretations.
Resolving the sense ambiguity of the word ‘fly’ (i.e., determining
whether the word indicates ‘an insect’ or ‘the action of moving through
the air’), for example, helps resolve the structural ambiguity, and the
converse is true as well.

There have been many methods proposed to address the word sense
disambiguation problem. (A number of tasks in natural language
processing, in fact, fall into the category of word sense disambiguation
[ \citename Yarowsky1993 ] . These include homograph disambiguation in
speech synthesis, word selection in machine translation, and spelling
correction in document processing.)

A simple approach to word sense disambiguation is to employ the
conditional distribution – a ‘decision model’

  -- -------- --
     @xmath   
  -- -------- --

where random variable @xmath assumes word senses as its values, and
random variables @xmath represent pieces of evidence for disambiguation.
For example, @xmath can be the insect sense or the action sense of the
word ‘fly,’ @xmath can be the presence or absence of the word ‘time’ in
the context. Word sense disambiguation, then, can be realized as the
process of finding the sense @xmath whose conditional probability @xmath
is the largest, where @xmath are the values of the random variables
@xmath in the current context.

Since the conditional distribution has a large number of parameters,
however, it is difficult to estimate them. One solution to this
difficulty is to estimate the conditional probabilities by using Bayes’
rule and by assuming that the pieces of evidence for disambiguation are
mutually independent [ \citename Yarowsky1992 ] . Specifically, we
select a sense @xmath satisfying:

  -- -------- --
     @xmath   
  -- -------- --

Another way of estimating the conditional probability distribution is to
represent it in the form of a probabilistic decision list ⁵ ⁵ 5 A
probabilistic decision list [ \citename Yamanishi1992a ] is a kind of
conditional distribution and different from a deterministic decision
list [ \citename Rivest1987 ] , which is a kind of Boolean function. ,
as is proposed in [ \citename Yarowsky1994 ] . Since a decision list is
a sequence of IF-THEN type rules, the use of it in disambiguation turns
out to utilize only the strongest pieces of evidence. Yarowsky has also
devised a heuristic method for efficient learning of a probabilistic
decision list. The merits of this method are ease of implementation,
efficiency in processing, and clarity.

Another approach to word sense disambiguation is the use of weighted
majority learning [ \citename Littlestone1988 , \citename Littlestone
and Warmuth1994 ] . Suppose, for the sake of simplicity, that the
disambiguation decision is binary, i.e., it can be represented as 1 or
0. We can first define a linear threshold function:

  -- -------- --
     @xmath   
  -- -------- --

where feature @xmath takes on @xmath and @xmath as its values,
representing the presence and absence of a piece of evidence,
respectively, and @xmath denotes a non-negative real-valued weight. In
disambiguation, if the function exceeds a predetermined threshold @xmath
, we choose @xmath , otherwise @xmath . We can further employ a learning
algorithm called ‘winnow’ that updates the weights in an on-line (or
incremental) fashion. ⁶ ⁶ 6 Winnow is similar to the well-known
classical ‘perceptron’ algorithm, but the former uses a multiplicative
weight update scheme while the latter uses an additive weight update
scheme. \namecite Littlestone88 has shown that winnow performs much
better than perceptron when many attributes are irrelevant. This
algorithm has the advantage of being able to handle a large set of
features, and at the same time not ordinarily be affected by features
that are irrelevant to the disambiguation decision. (See [ \citename
Golding and Roth1996 ] .)

For word sense disambiguation methods, see also [ \citename Black1988 ,
\citename Brown et al.1991 , \citename Guthrie et al.1991 , \citename
Gale, Church, and Yarowsky1992 , \citename McRoy1992 , \citename
Leacock, Towell, and Voorhees1993 , \citename Yarowsky1993 , \citename
Bruce and Wiebe1994 , \citename Niwa and Nitta1994 , \citename Voorhees,
Leacock, and Towell1995 , \citename Yarowsky1995 , \citename Golding and
Schabes1996 , \citename Ng and Lee1996 , \citename Fujii et al.1996 ,
\citename Schütze1997 , \citename Schütze1998 ] .

### 7 Introduction to MDL

The Minimum Description Length principle is a strategy (criterion) for
data compression and statistical estimation, proposed by Rissanen (1978;
1983; 1984; 1986; 1989; 1996; 1997). Related strategies were also
proposed and studied independently in [ \citename Solomonoff1964 ,
\citename Wallace and Boulton1968 , \citename Schwarz1978 ] . A number
of important properties of MDL have been demonstrated by \namecite
Barron91 and \namecite Yamanishi92a.

MDL states that, for both data compression and statistical estimation,
the best probability model with respect to given data is that which
requires the shortest code length in bits for encoding the model itself
and the data observed through it. ⁷ ⁷ 7 In this thesis, I describe MDL
as a criterion for both data compression and statistical estimation.
Strictly speaking, however, it is only referred to as the ‘MDL
principle’ when used as a criterion for statistical estimation.

In this section, we will consider the basic concept of MDL and, in
particular how to calculate description length. Interested readers are
referred to [ \citename Quinlan and Rivest1989 , \citename
Yamanishi1992a , \citename Yamanishi1992b , \citename Han and
Kobayashi1994 ] for an introduction to MDL.

#### 7.1 Basics of Information Theory

##### IID process

Suppose that a data sequence (or a sequence of symbols)

  -- -------- --
     @xmath   
  -- -------- --

is independently generated according to a discrete probability
distribution

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

where random variable (information source) @xmath takes on values from a
set of symbols:

  -- -------- --
     @xmath   
  -- -------- --

Such a data generation process is generally referred to as ‘i.i.d’
(independently and identically distributed).

In order to transmit or compress the data sequence, we need to define a
code for encoding the information source @xmath , i.e., to assign to
each value of @xmath a codeword, namely a bit string. In order for the
decoder to be able to decode a codeword as soon as it comes to the end
of that codeword, the code must be one in which no codeword is a prefix
of any other codeword. Such a code is called a ‘prefix code.’

###### Theorem 1

The sufficient and necessary condition for a code to be a prefix code is
as follows,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the code length of the codeword assigned to symbol
@xmath .

This is known as Kraft’s inequality.

We define the expected (average) code length of a code for encoding the
information source @xmath as

  -- -------- --
     @xmath   
  -- -------- --

Moreover, we define the entropy of (the distribution of) @xmath as ⁸ ⁸ 8
Throughout this thesis, ‘ @xmath ’ denotes logarithm to the base 2.

  -- -------- --
     @xmath   
  -- -------- --

###### Theorem 2

The expected code length of a prefix code for encoding the information
source @xmath is greater than or equal to the entropy of @xmath , namely

  -- -------- --
     @xmath   
  -- -------- --

We can define a prefix code in which symbol @xmath is assigned a
codeword with code length

  -- -------- --
     @xmath   
  -- -------- --

according to Theorem 1 , since

  -- -------- --
     @xmath   
  -- -------- --

Such a code is on average the most efficient prefix code, according to
Theorem 2 . Hereafter, we refer to this type of code as a ‘
non-redundant code.’ (In real communication, a code length must be a
truncated integer: @xmath , ⁹ ⁹ 9 @xmath denotes the least integer not
less than @xmath . but we use here @xmath for ease of mathematical
manipulation. This is not harmful and on average the error due to it is
negligible.) When the distribution @xmath is a uniform distribution,
i.e.,

  -- -------- --
     @xmath   
  -- -------- --

the code length for encoding each symbol @xmath turns out to be

  -- -------- --
     @xmath   
  -- -------- --

##### General case

We next consider a more general case. We assume that the data sequence

  -- -------- --
     @xmath   
  -- -------- --

is generated according to a probability distribution @xmath where random
variable @xmath takes on values from @xmath . The data generation
process needs neither be i.i.d. nor even stationary (for the definition
of a stationary process, see, for example, [ \citename Cover and
Thomas1991 ] ). Again, our goal is to transmit or compress the data
sequence.

We define the expected code length for encoding a sequence of @xmath
symbols as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath represents the probability of observing the data sequence
@xmath and @xmath the code length for encoding @xmath . We further
define the entropy of (the distribution of) @xmath as

  -- -------- --
     @xmath   
  -- -------- --

We have the following theorem, widely known as Shannon’s first theorem
(cf., [ \citename Cover and Thomas1991 ] ).

###### Theorem 3

The expected code length of a prefix code for encoding a sequence of
@xmath symbols @xmath is greater than or equal to the entropy of @xmath
, namely

  -- -------- --
     @xmath   
  -- -------- --

As in the i.i.d. case, we can define a non-redundant code in which the
code length for encoding the data sequence @xmath is

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

The expected code length of the code for encoding a sequence of @xmath
symbols then becomes

  -- -------- -- -----
     @xmath      (8)
  -- -------- -- -----

Here we assume that we know in advance the distribution @xmath (in
general @xmath ). In practice, however, we usually do not know what kind
of distribution it is. We have to estimate it by using the same data
sequence @xmath and transmit first the estimated model and then the data
sequence, which leads us to the notion of two-stage coding.

#### 7.2 Two-stage code and MDL

In two-stage coding, we first introduce a class of models which includes
all of the possible models which can give rise to the data. We then
choose a prefix code and encode each model in the class. The decoder is
informed in advance as to which class has been introduced and which code
has been chosen, and thus no matter which model is transmitted, the
decoder will be able to identify it. We next calculate the total code
length for encoding each model and the data through the model, and
select the model with the shortest total code length. In actual
transmission, we transmit first the selected model and then the data
through the model. The decoder then can restore the data perfectly.

##### Model class

We first introduce a class of models, of which each consists of a
discrete model (an expression) and a parameter vector (a number of
parameters). When a discrete model is specified, the number of
parameters is also determined.

For example, the tree cut models within a thesaurus tree, to be defined
in Chapter 4, form a model class. A discrete model in this case
corresponds to a cut in the thesaurus tree. The number of free
parameters equals the number of nodes in the cut minus one.

The class of ‘linear regression models’ is also an example model class.
A discrete model is

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes a random variable, @xmath a parameter, and @xmath a
random variable based on the standard normal distribution @xmath . The
number of parameters in this model equals @xmath .

A class of models can be denoted as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath stands for a discrete model, @xmath a set of discrete
models, @xmath a parameter vector, and @xmath a parameter space
associated with @xmath .

Usually we assume that the model class we introduced contains the ‘true’
model which has given rise to the data, but it does not matter if it
does not. In such case, the best model selected from the class can be
considered an approximation of the true model. The model class we
introduce reflects our prior knowledge on the problem.

##### Total description length

We next consider how to calculate total description length.

Total description length equals the sum total of the code length for
encoding a discrete model (model description length @xmath ), the code
length for encoding parameters given the discrete model (parameter
description length @xmath ), and the code length for encoding the data
given the discrete model and the parameters (data description length
@xmath ). Note that we also sometimes refer to the model description
length as @xmath .

Our goal is to find the minimum description length of the data (in
number of bits) with respect to the model class, namely,

  -- -- --
        
  -- -- --

##### Model description length

Let us first consider how to calculate model description length @xmath .
The choice of a code for encoding discrete models is subjective; it
depends on our prior knowledge on the model class.

If the set of discrete models @xmath is finite and the probability
distribution over it is a uniform distribution, i.e.,

  -- -------- --
     @xmath   
  -- -------- --

then we need

  -- -------- --
     @xmath   
  -- -------- --

to encode each discrete model @xmath using a non-redundant code.

If @xmath is a countable set, i.e., each of its members can be assigned
a positive integer, then the ‘Elias code,’ which is usually used for
encoding integers, can be employed [ \citename Rissanen1989 ] . Letting
@xmath be the integer assigned to a discrete model @xmath , we need

  -- -------- --
     @xmath   
  -- -------- --

to encode @xmath . Here the sum includes all the positive iterates and
@xmath denotes a constant of about 2.865.

##### Parameter description length and data description length

When a discrete model @xmath is fixed, a parameter space will be
uniquely determined. The model class turns out to be

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes a parameter vector, and @xmath the parameter space.
Suppose that the dimension of the parameter space is @xmath , then
@xmath is a vector with @xmath real-valued components:

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes a transpose of @xmath .

We next consider a way of calculating the sum of the parameter
description length and the data description length through its
minimization:

  -- -------- --
     @xmath   
  -- -------- --

Since the parameter space @xmath is usually a subspace of the @xmath
-dimensional Euclidean space and has an infinite number of points
(parameter vectors), straightforwardly encoding each point in the space
takes the code length to infinity, and thus is intractable. (Recall the
fact that before transmitting an element in a set, we need encode each
element in the set.) One possible way to deal with this difficulty is to
discretize the parameter space; the process can be defined as a mapping
from the parameter space to a discretized space, depending on the data
size @xmath :

  -- -------- --
     @xmath   
  -- -------- --

A discretized parameter space consists of a finite number of elements
(cells). We can designate one point in each cell as its representative
and use only the representatives for encoding parameters. The
minimization then turns out to be

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the code length for encoding a representative
@xmath and @xmath denotes the code length for encoding the data @xmath
through that representative.

A simple way of conducting discretization is to define a cell as a micro
@xmath -dimensional rectangular solid having length @xmath on the axis
of @xmath . If the volume of the parameter space is @xmath , then we
have @xmath number of cells. If the distribution over the cells is
uniform, then we need

  -- -------- --
     @xmath   
  -- -------- --

to encode each representative @xmath using a non-redundant code.

On the other hand, since the number of parameters is fixed and the data
is given, we can estimate the parameters by employing Maximum Likelihood
Estimation (MLE), obtaining

  -- -------- --
     @xmath   
  -- -------- --

We may expect that the representative of the cell into which the maximum
likelihood estimate falls is the nearest to the true parameter vector
among all representatives. And thus, instead of conducting minimization
over all representatives, we need only consider minimization with
respect to the representative of the cell which the maximum likelihood
estimate belongs to. This representative is denoted here as @xmath . We
approximate the difference between @xmath and @xmath as

  -- -------- --
     @xmath   
  -- -------- --

Data description length using @xmath then becomes

  -- -------- --
     @xmath   
  -- -------- --

Now, we need consider only

  -- -------- --
     @xmath   
  -- -------- --

There is a trade-off relationship between the first term and the second
term. If @xmath is large, then the first term will be small, while on
the other hand the second term will be large, and vice-versa. That means
that if we discretize the parameter space loosely, we will need less
code length for encoding the parameters, but more code length for
encoding the data. On the other hand, if we discretize the parameter
space precisely, then we need less code length for encoding the data,
but more code length for encoding the parameters.

In this way of calculation (see Appendix 9.A for a derivation), we have

  -- -------- -- -----
     @xmath      (9)
  -- -------- -- -----

where @xmath indicates @xmath , a constant. The first term corresponds
to the data description length and has the same form as that in ( 7 ).
The second term corresponds to the parameter description length. An
intuitive explanation of it is that the standard deviation of the
maximum likelihood estimator of one of the parameters is of order @xmath
, ¹⁰ ¹⁰ 10 It is well known that under certain suitable conditions, when
the data size increases, the distribution of the maximum likelihood
estimator @xmath will asymptotically become the normal distribution
@xmath where @xmath denotes the true parameter vector, @xmath the Fisher
information matrix, and @xmath the data size [ \citename Fisher1956 ] .
and hence encoding the parameters using more than @xmath @xmath bits
would be wasteful for the given data size.

In this way, the sum of the two kinds of description length @xmath is
obtained for a fixed discrete model @xmath (and a fixed dimension @xmath
). For a different @xmath , the sum can also be calculated.

##### Selecting a model with minimum total description length

Finally, the minimum total description length becomes, for example,

  -- -------- --
     @xmath   
  -- -------- --

We select the model with the minimum total description length for
transmission (data compression).

#### 7.3 MDL as data compression criterion

Rissanen has proved that MDL is an optimal criterion for data
compression.

###### Theorem 4

[ \citename Rissanen1984 ] Under certain suitable conditions, the
expected code length of the two-stage code described above (with code
length ( 9 ) for encoding the data sequence @xmath ) satisfies

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the entropy of @xmath .

This theorem indicates that when we do not know the true distribution
@xmath in communication, we have to waste on average about @xmath bits
of code length (cf., ( 8 )).

###### Theorem 5

[ \citename Rissanen1984 ] Under certain suitable conditions, for any
prefix code , for some @xmath such that @xmath and @xmath such that the
volume of it @xmath satisfies @xmath , for any model with parameter
@xmath , the expected code length @xmath is bounded from below by

  -- -------- --
     @xmath   
  -- -------- --

This theorem indicates that in general, i.e., excluding some special
cases, we cannot make the average code length of a prefix code more
efficient than the quantity @xmath . The introduction of @xmath
eliminates the case in which we happen to select the true model and
achieve on average a very short code length: @xmath . Theorem 5 can be
considered as an extension of Shannon’s Theorem (Theorem 3 ).

Theorems 4 and 5 suggest that using the two-stage code above is nearly
optimal in terms of expected code length. We can, therefore, say that
encoding a data sequence @xmath in the way described in ( 9 ) is the
most efficient approach not only to encoding the data sequence, but
also, on average, to encoding a sequence of @xmath symbols.

#### 7.4 MDL as estimation criterion

The MDL principle stipulates that selecting a model having the minimum
description length is also optimal for conducting statistical estimation
that includes model selection.

##### Definition of MDL

The MDL principle can be described more formally as follows [ \citename
Rissanen1989 , \citename Barron and Cover1991 ] . For a data sequence
@xmath and for a model class @xmath , the minimum description length of
the data with respect to the class is defined as

  -- -------- -- ------
     @xmath      (10)
  -- -------- -- ------

where @xmath denotes a discretization of @xmath and where @xmath is the
code length for encoding @xmath , satisfying

  -- -------- --
     @xmath   
  -- -------- --

Note that ‘ @xmath ’ stead of ‘ @xmath ’ is used here because there are
an infinite number of points which can serve as a representative for a
cell. Furthermore, @xmath is the code length for encoding @xmath ,
satisfying

  -- -------- --
     @xmath   
  -- -------- --

For both data compression and statistical estimation, the best
probability model with respect to the given data is that which achieves
the minimum description length given in ( 10 ).

The minimum description length defined in ( 10 ) is also referred to as
the ‘stochastic complexity’ of the data relative to the model class.

##### Advantages

MDL offers many advantages as a criterion for statistical estimation,
the most important perhaps being its optimal convergency rate.

##### Consistency

The models estimated by MDL converge with probability one to the true
model when data size increases – a property referred to as ‘consistency’
[ \citename Barron and Cover1991 ] . That means that not only the
parameters themselves but also the number of them converge to those of
the true model.

##### Rate of convergence

Consistency, however, is a characteristic to be considered only when
data size is large; in practice, when data size can generally be
expected to be small, rate of convergence is a more important guide to
the performance of an estimator.

\namecite

Barron91 have verified that MDL as an estimation strategy is near
optimal in terms of the rate of convergence of its estimated models to
the true model as the data size increases. When the true model is
included in the class of models considered , the models selected by MDL
converge in probability to the true model at the rate of @xmath , where
@xmath is the number of parameters in the true model, and @xmath the
data size. This is nearly optimal.

\namecite

Yamanishi92a has derived an upper bound on the data size necessary for
learning probably approximately correctly (PAC) a model from among a
class of conditional distributions, which he calls stochastic rules with
finite partitioning. This upper bound is of order @xmath , where @xmath
denotes the number of parameters of the true model, and @xmath the
accuracy parameter for the stochastic PAC learning. For MLE, the
corresponding upper bound is of order @xmath , where @xmath denotes the
maximum of the number of parameters of a model in the model class. These
upper bounds indicate that MDL requires less data than MLE to achieve
the same accuracy in statistical learning, provided that @xmath (note
that, in general, @xmath ).

##### MDL and MLE

When the number of parameters in a probability model is fixed, and the
estimation problem involves only the estimation of parameters, MLE is
known to be satisfactory [ \citename Fisher1956 ] . Furthermore, for
such a fixed model, it is known that MLE is equivalent to MDL: given the
data @xmath , the maximum likelihood estimator @xmath is defined as one
that maximizes likelihood with respect to the data, that is,

  -- -------- -- ------
     @xmath      (11)
  -- -------- -- ------

It is easy to see that @xmath also satisfies

  -- -------- --
     @xmath   
  -- -------- --

This is, in fact, no more than the MDL estimator in this case, since
@xmath is the data description length .

When the estimation problem involves model selection, MDL’s behavior
significantly deviates from that of MLE. This is because MDL insists on
minimizing the sum total of the data description length and the model
description length, while MLE is still equivalent to minimizing the data
description length alone. We can, therefore, say that MLE is a special
case of MDL.

Note that in ( 9 ), the first term is of order @xmath and the second
term is of order @xmath , and thus the first term will dominate that
formula when the data size increases. That means that when data size is
sufficiently large, the MDL estimate will turn out to be the MLE
estimate; otherwise the MDL estimate will be different from the MLE
estimate.

##### MDL and Bayesian Estimation

In an interpretation of MDL from the viewpoint of Bayesian Estimation,
MDL is essentially equivalent to the ‘MAP estimation’ in Bayesian
terminology. Given data @xmath and a number of models, the Bayesian
(MAP) estimator @xmath is defined as one that maximizes posterior
probability, i.e.,

  -- -------- -- ------
     @xmath      (12)
  -- -------- -- ------

where @xmath denotes the prior probability of model @xmath and @xmath
the probability of observing data @xmath through @xmath . In the same
way, @xmath satisfies

  -- -------- --
     @xmath   
  -- -------- --

This is equivalent to the MDL estimator if we take @xmath to be the
model description length. Interpreting @xmath as the model description
length translates, in Bayesian Estimation, to assigning larger prior
probabilities to simpler models, since it is equivalent to assuming that
@xmath , where @xmath is the code length of model @xmath . (Note that if
we assign uniform prior probability to all models, then ( 12 ) becomes
equivalent to ( 11 ), giving the MLE estimator.)

##### MDL and MEP

The use of the Maximum Entropy Principle (MEP) has been proposed in
statistical language processing [ \citename Ratnaparkhi, Reynar, and
Roukos1994 , \citename Ratnaparkhi, Reynar, and Roukos1994 , \citename
Ratnaparkhi1997 , \citename Berger, Pietra, and Pietra1996 , \citename
Rosenfeld1996 ] ). Like MDL, MEP is also a learning criterion, one which
stipulates that from among the class of models that satisfies certain
constraints, the model which has the maximum entropy should be selected.
Selecting a model with maximum entropy is, in fact, equivalent to
selecting a model with minimum description length [ \citename
Rissanen1983 ] . Thus, MDL provides an information-theoretic
justification of MEP.

##### MDL and stochastic complexity

The sum of parameter description length and data description length
given in ( 9 ) is still a loose approximation. Recently, Rissanen has
derived this more precise formula:

  -- -------- -- ------
     @xmath      (13)
  -- -------- -- ------

where @xmath denotes the Fisher information matrix, @xmath the
determinant of matrix @xmath , and @xmath the circular constant, and
@xmath indicates @xmath . It is thus preferable to use this formula in
practice.

This formula can be obtained not only on the basis of the ‘complete
two-stage code,’ but also on that of ‘quantized maximum likelihood
code,’ and has been proposed as the new definition of stochastic
complexity [ \citename Rissanen1996 ] . (See also [ \citename Clarke and
Barron1990 ] .)

When the data generation process is i.i.d. and the distribution is a
discrete probability distribution like that in ( 6 ), the sum of
parameter description length and data description length turns out to be
[ \citename Rissanen1997 ]

  -- -------- -- ------
     @xmath      (14)
  -- -------- -- ------

where @xmath denotes the Gamma function ¹¹ ¹¹ 11 Euler’s Gamma function
is defined as @xmath . . This is because in this case, the determinant
of the Fisher information matrix becomes @xmath , and the integral of
its square root can be calculated by the Dirichlet’s integral as @xmath
.

#### 7.5 Employing MDL in NLP

Recently MDL and related techniques have become popular in natural
language processing and related fields; a number of learning methods
based on MDL have been proposed for various applications [ \citename
Ellison1991 , \citename Ellison1992 , \citename Cartwright and Brent1994
, \citename Stolcke and Omohundro1994 , \citename Brent, Murthy, and
Lundberg1995 , \citename Ristad and Thomas1995 , \citename Brent and
Cartwright1996 , \citename Grunwald1996 ] .

##### Coping with the data sparseness problem

MDL is a powerful tool for coping with the data sparseness problem, an
inherent difficulty in statistical language processing. In general, a
complicated model might be suitable for representing a problem, but it
might be difficult to learn due to the sparseness of training data. On
the other hand, a simple model might be easy to learn, but it might be
not rich enough for representing the problem. One possible way to cope
with this difficulty is to introduce a class of models with various
complexities and to employ MDL to select the model having the most
appropriate level of complexity.

An especially desirable property of MDL is that it takes data size into
consideration. Classical statistics actually assume implicitly that the
data for estimation are always sufficient. This, however, is patently
untrue in natural language. Thus, the use of MDL might yield more
reliable results in many NLP applications.

##### Employing efficient algorithms

In practice, the process of finding the optimal model in terms of MDL is
very likely to be intractable because a model class usually contains too
many models to calculate a description length for each of them. Thus,
when we have modelized a natural language acquisition problem on the
basis of a class of probability models and want to employ MDL to select
the best model, what is necessary to consider next is how to perform the
task efficiently, in other words, how to develop an efficient algorithm.

When the model class under consideration is restricted to one related to
a tree structure, for instance, the dynamic programming technique is
often applicable and the optimal model can be efficiently found.
\namecite Rissanen97, for example, has devised such an algorithm for
learning a decision tree.

Another approach is to calculate approximately the description lengths
for the probability models, by using a computational-statistic
technique, e.g., the Markov chain Monte-Carlo method, as is proposed in
[ \citename Yamanishi1996 ] .

In this thesis, I take the approach of restricting a model class to a
simpler one (i.e., reducing the number of models to consider) when doing
so is still reasonable for tackling the problem at hand.

## Chapter 3 Models for Lexical Knowledge Acquisition

  -------------------------------------------------------------------------------------------------------------------------------------------------------
  The world as we know it is our interpretation of the observable facts in the light of theories that we ourselves invent. - Immanuel Kant (paraphrase)
  -------------------------------------------------------------------------------------------------------------------------------------------------------

In this chapter, I define probability models for each subproblem of the
lexical semantic knowledge acquisition problem: (1) the hard case slot
model and the soft case slot model; (2) the word-based case frame model,
the class-based case frame model, and the slot-based case frame model;
and (3) the hard co-occurrence model and the soft co-occurrence model.
These are respectively the probability models for (1) case slot
generalization, (2) case dependency learning, and (3) word clustering.

### 1 Case Slot Model

##### Hard case slot model

We can assume that case slot data for a case slot for a verb like that
shown in Table 2 are generated according to a conditional probability
distribution, which specifies the conditional probability of a noun
given the verb and the case slot. I call such a distribution a ‘case
slot model.’

When the conditional probability of a noun is defined as that of the
noun class to which the noun belongs, divided by the size of the noun
class, I call the case slot model a ‘hard-clustering-based case slot
model,’ or simply a ‘hard case slot model.’

Suppose that @xmath is the set of nouns, @xmath is the set of verbs, and
@xmath is the set of slot names. A partition @xmath of @xmath is defined
as a set satisfying @xmath , ¹ ¹ 1 @xmath denotes the power set of a set
@xmath ; if, for example, @xmath , then @xmath . @xmath and @xmath . An
element @xmath in @xmath is referred to as a ‘class.’ A hard case slot
model with respect to a partition @xmath is defined as a conditional
probability distribution:

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

where random variable @xmath assumes a value from @xmath , random
variable @xmath from @xmath , and random variable @xmath from @xmath ,
and where @xmath is satisfied. ² ² 2 Rigorously, a hard case slot model
with respect to a noun partition @xmath should be represented as

@xmath

We can formalize the case slot generalization problem as that of
estimating a hard case slot model. The problem, then, turns out to be
that of selecting a model, from a class of hard case slot models, which
is most likely to have given rise to the case slot data.

This formalization of case slot generalization will make it possible to
deal with the data sparseness problem, an inherent difficulty in a
statistical approach to natural language processing. Since many words in
natural language are synonymous, it is natural to classify them into the
same word class and employ class-based probability models. A class-based
model usually has far fewer parameters than a word-based model, and thus
the use of it can help handle the data sparseness problem. An important
characteristic of the approach taken here is that it automatically
conducts the optimization of word clustering by means of statistical
model selection. That is to say, neither the number of word classes nor
the way of word classification are determined in advance, but are
determined automatically on the basis of the input data.

The uniform distribution assumption in the hard case slot model seems to
be necessary for dealing with the data sparseness problem. If we were to
assume that the distribution of words (nouns) within a class is a
word-based distribution, then the number of parameters would not be
reduced and the data sparseness problem would still prevail.

Under the uniform distribution assumption, generalization turns out to
be the process of finding the best configuration of classes such that
the words in each class are equally likely to be the value of the slot
in question. (Words belonging to a single word class should be similar
in terms of likelihood; they do not necessarily have to be synonyms.)
Conversely, if we take the generalization to be such a process, then
viewing it as statistical estimation of a hard case slot model seems to
be quite appropriate, because the class of hard case slot models
contains all of the possible models for the purposes of generalization.
The word-based case slot model (i.e., one in which each word forms its
own word class) is a (discrete) hard case slot model, and any grouping
of words (nouns) leads to one (discrete) hard case slot model.

##### Soft case slot model

Note that in the hard case slot model a word (noun) is assumed to belong
to a single class. In practice, however, many words have sense
ambiguities and a word can belong to several different classes, e.g.,
‘bird’ is a member of both @xmath animal @xmath and @xmath meat @xmath .
It is also possible to extend the hard case slot model so that each word
probabilistically belongs to several different classes, which would
allow us to resolve both syntactic and word sense ambiguities at the
same time. Such a model can be defined in the form of a ‘finite mixture
model,’ which is a linear combination of the word probability
distributions within individual word (noun) classes. I call such a model
a ‘soft-clustering-based case slot model,’ or simply a ‘soft case slot
model.’

First, a covering @xmath of the noun set @xmath is defined as a set
satisfying @xmath , @xmath . An element @xmath in @xmath is referred to
as a ‘class.’ A soft case slot model with respect to a covering @xmath
is defined as a conditional probability distribution:

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

where random variable @xmath denotes a noun, random variable @xmath a
verb, and random variable @xmath a slot name. We can also formalize the
case slot generalization problem as that of estimating a soft case slot
model.

If we assume, in a soft case slot model, that a word can only belong to
a single class alone and that the distribution within a class is a
uniform distribution, then the soft case slot model will become a hard
case slot model.

##### Numbers of parameters

Table 1 shows the numbers of parameters in a word-based case slot model
( 1 ), a hard case slot model ( 1 ), and a soft case slot model ( 2 ).
Here @xmath denotes the size of the set of nouns, @xmath the partition
in the hard case slot model, and @xmath the covering in the soft case
slot model.

The number of parameters in a hard case slot models is generally smaller
than that in a soft case slot model. Furthermore, the number of
parameters in a soft case slot model is generally smaller than that in a
word-based case slot model (note that the parameters @xmath is common to
each soft case slot model). As a result, hard case slot models require
less data for parameter estimation than soft case slot models, and soft
case slot models less data than word-based case slot models. That is to
say, hard and soft case slot models are more useful than word-based
models, given the fact that usually the size of data for training is
small.

Unfortunately, currently available data sizes are still insufficient for
the accurate estimating of a soft case slot model. (Appendix 9.B shows a
method for learning a soft case slot model.) (See [ \citename Li and
Yamanishi1997 ] for a method of using a finite mixture model in document
classification, for which more data are generally available.)

In this thesis, I address only the issue of estimating a hard case slot
model. With regard to the word-sense ambiguity problem, one can employ
an existing word-sense disambiguation technique (cf., Chapter2) in
pre-processing, and use the disambiguated word senses as virtual words
in the subsequent learning process.

### 2 Case Frame Model

We can assume that case frame data like that in Table 1 are generated
according to a multi-dimensional discrete joint probability distribution
in which random variables represent case slots. I call such a
distribution a ‘case frame model.’ We can formalize the case dependency
learning problem as that of estimating a case frame model. The
dependencies between case slots are represented as probabilistic
dependencies between random variables. (Recall that random variables
@xmath are mutually independent, if for any @xmath , and any @xmath ,
@xmath ; otherwise, they are mutually dependent.)

The case frame model is the joint probability distribution of type,

  -- -------- --
     @xmath   
  -- -------- --

where index @xmath stands for the verb, and each of the random variables
@xmath represents a case slot.

In this thesis, ‘case slots’ refers to surface case slots, but they can
also be deep case slots. Furthermore, obligatory cases and optional
cases are uniformly treated. The possible case slots can vary from verb
to verb. They can also be a predetermined set for all of the verbs, with
most of the slots corresponding to (English) prepositions.

The case frame model can be further classified into three types of
probability models according to the type of value each random variable
@xmath assumes. When @xmath assumes a word or a special symbol @xmath as
its value, the corresponding model is referred to as a ‘word-based case
frame model.’ Here @xmath indicates the absence of the case slot in
question. When @xmath assumes a word-class (such as @xmath person @xmath
or @xmath company @xmath ) or @xmath as its value, the corresponding
model is referred to as a ‘class-based case frame model.’ When @xmath
takes on @xmath or @xmath as its value, the model is called a
‘slot-based case frame model.’ Here @xmath indicates the presence of the
case slot in question, and @xmath the absence of it. For example, the
data in Table 2 could have been generated by a word-based model, the
data in Table 3 by a class-based model, where @xmath denotes a word
class, and the data in Table 4 by a slot-based model. Suppose, for
simplicity, that there are only 4 possible case slots corresponding,
respectively, to subject, direct object, ‘from’ phrase, and ‘to’ phrase.
Then,

  -- -------- --
     @xmath   
  -- -------- --

is specified by a word-based case frame model. In contrast,

  -- -------- --
     @xmath   
  -- -------- --

is specified by a class-based case frame model, where @xmath and @xmath
denote word classes. Finally,

  -- -------- --
     @xmath   
  -- -------- --

is specified by a slot-based case frame model. One can also define a
combined model in which, for example, some random variables assume word
classes and 0 as their values while others assume 1 and 0.

Note that since in general

  -- -------- --
     @xmath   
  -- -------- --

one should not use here the joint probability @xmath as the probability
of the case frame ‘(fly (arg1 1)(arg2 1)).’

In learning and using of the case frame models, it is also assumed that
word sense ambiguities have been resolved in pre-processing.

One may argue that when the ambiguities of a verb are resolved, there
would not exist case dependencies at all (cf., ‘fly’ in sentences of ( 2
)). Sense ambiguities, however, are generally difficult to define
precisely. I think that it is preferable not to resolve them until doing
so is necessary in a particular application. That is to say, I think
that, in general, case dependencies do exist and the development of a
method for learning them is needed.

##### Numbers of parameters

Table 5 shows the numbers of parameters in a word-based case frame
model, a class-based case frame model, and a slot-based case frame
model, where @xmath denotes the number of random variables, @xmath the
size of the set of nouns, and @xmath the maximum number of classes in
any slot.

### 3 Co-occurrence Model

##### Hard co-occurrence model

We can assume that co-occurrence data over a set of nouns and a set of
verbs like that in Figure 3 are generated according to a joint
probability distribution that specifies the co-occurrence probabilities
of noun verb pairs. I call such a distribution a ‘co-occurrence model.’

I call the co-occurrence model a ‘hard-clustering-based co-occurrence
model,’ or simply a ‘hard co-occurrence model,’ when the joint
probability of a noun verb pair can be defined as the product of the
joint probability of the noun class and the verb class to which the noun
and the verb respectively belong, the conditional probability of the
noun given its noun class, and the conditional probability of the verb
given its verb class.

Suppose that @xmath is the set of nouns, and @xmath is the set of verbs.
A partition @xmath of @xmath is defined as a set which satisfies @xmath
, @xmath and @xmath . A partition @xmath of @xmath is defined as a set
which satisfies @xmath , @xmath and @xmath . Each element in a partition
forms a ‘class’ of words. I define a hard co-occurrence model with
respect to a noun partition @xmath and a verb partition @xmath as a
joint probability distribution of type:

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

where random variable @xmath denotes a noun and random variable @xmath a
verb and where @xmath and @xmath are satisfied. ³ ³ 3 Rigorously, a hard
co-occurrence model with respect to a noun partition @xmath and a verb
partition @xmath should be represented as

@xmath

Figure 1 shows a hard co-occurrence model, one that can give rise to the
co-occurrence data in Figure 3 .

Estimating a hard co-occurrence model means selecting, from the class of
such models, one that is most likely to have given rise to the
co-occurrence data. The selected model will contain a hard clustering of
words. We can therefore formalize the problem of word clustering as that
of estimating a hard co-occurrence model.

We can restrict the hard co-occurrence model by assuming that words
within a same class are generated with an equal probability [ \citename
Li and Abe1996 , \citename Li and Abe1997 ] , obtaining

  -- -------- --
     @xmath   
  -- -------- --

Employing such a restricted model in word clustering, however, has an
undesirable tendency to result in classifying into different classes
those words that have similar co-occurrence patterns but have different
absolute frequencies.

The hard co-occurrence model in ( 3 ) can also be considered an
extension of that proposed in [ \citename Brown et al.1992 ] . First,
dividing the equation by @xmath , we obtain

  -- -------- --
     @xmath   
  -- -------- --

Since @xmath holds, we have

  -- -------- --
     @xmath   
  -- -------- --

We can rewrite the model for word sequence predication as

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

where random variables @xmath and @xmath take on words as their values.
In this way, the hard co-occurrence model turns out to be a bigram class
model and is similar to that proposed in [ \citename Brown et al.1992 ]
(cf., Chapter 2). ⁴ ⁴ 4 Strictly speaking, the bigram class model
proposed by [ \citename Brown et al.1992 ] and the hard case slot model
defined here are different types of probability models; the former is a
conditional distribution, while the latter is a joint distribution. The
difference is that the model in ( 4 ) assumes that the configuration of
word groups for @xmath and the configuration of word groups for @xmath
can be different, while Brown et al’s model assumes that the
configurations for the two are always the same.

##### Soft co-occurrence model

The co-occurrence model can also be defined as a double mixture model,
which is a double linear combination of the word probability
distributions within individual noun classes and those within individual
verb classes. I call such a model a ‘soft-clustering-based co-occurrence
model,’ or simply ‘soft co-occurrence model.’

First, a covering @xmath of the noun set @xmath is defined as a set
which satisfies @xmath , @xmath . A covering @xmath of the verb set
@xmath is defined as a set which satisfies @xmath , @xmath . Each
element in a covering is referred to as a ‘class.’ I define a soft
co-occurrence model with respect to a noun covering @xmath and a verb
covering @xmath as a joint probability distribution of type:

  -- -------- --
     @xmath   
  -- -------- --

where random variable @xmath denotes a noun and random variable @xmath a
verb. Obviously, the soft co-occurrence model includes the hard
co-occurrence model as a special case.

If we assume that a verb class consists of a single verb alone, i.e.,
@xmath , then the soft co-occurrence model turns out to be

  -- -------- --
     @xmath   
  -- -------- --

which is equivalent to that proposed in [ \citename Pereira, Tishby, and
Lee1993 ] .

Estimating a soft co-occurrence model, thus, means selecting, from the
class of such models, one that is most likely to have given rise to the
co-occurrence data. The selected model will contain a soft clustering of
words. We can formalize the word clustering problem as that of
estimating a soft co-occurrence model.

##### Numbers of parameters

Table 6 shows the numbers of parameters in a hard co-occurrence model
and in a soft co-occurrence model. Here @xmath denotes the size of the
set of nouns, @xmath the size of the set of verbs, @xmath and @xmath are
the partitions in the hard co-occurrence model, and @xmath and @xmath
are the coverings in the soft co-occurrence model.

In this thesis, I address only the issue of estimating a hard
co-occurrence model.

### 4 Relations between Models

Table 7 summarizes the formalization I have made above.

The models described above are closely related. The soft case slot model
includes the hard case slot model, and the soft co-occurrence model
includes the hard co-occurrence model. The slot-based case frame model
will become the class-based case frame model when we granulate
slot-based case slot values into class-based slot values. The
class-based case frame model will become the word-based case frame model
when we perform further granulation. The relation between the hard case
slot model and the case frame models, that between the hard case slot
model and the hard co-occurrence model, and that between the soft case
slot model and the soft co-occurrence model are described below.

##### Hard case slot model and case frame models

The relationship between the hard case slot model and the case frame
models may be expressed by transforming the notation of the conditional
probability @xmath in the hard case slot model to

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

which is the ratio between a marginal probability in the class-based
case frame model and a marginal probability in the slot-based case frame
model.

This relation ( 5 ) implies that we can generalize case slots by using
the hard case slot model and then acquire class-based case frame
patterns by using the class-based case frame model.

##### Hard case slot model and hard co-occurrence model

If we assume that the verb set consists of a single verb alone, then the
hard co-occurrence model with respect to slot @xmath becomes

  -- -------- --
     @xmath   
  -- -------- --

If we further assume that nouns within a same noun class have an equal
probability, then we have

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

This is no more than the hard case slot model, which has a different
notation.

##### Soft case slot model and soft co-occurrence model

If we assume that the verb set consists of a single verb alone, then the
soft co-occurrence model with respect to slot @xmath becomes

  -- -------- --
     @xmath   
  -- -------- --

Suppose that @xmath is common to each slot @xmath , then we can denote
it as @xmath and have

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

This is equivalent to the soft case slot model.

### 5 Discussions

##### Generation models v.s. decision models

The models defined above are what I call ‘generation models.’ A case
frame generation model is a probability distribution that gives rise to
a case frame with a certain probability. In disambiguation, a generation
model predicts the likelihood of the occurrence of each case frame .

Alternatively, we can define what I call ‘decision models’ to perform
the disambiguation task. A decision model is a conditional distribution
which represents the conditional probabilities of disambiguation (or
parsing) decisions. For instance, the decision tree model and the
decision list model are example decision models (cf., Chapter 2). In
disambiguation, a decision model predicts the likelihood of the
correctness of each decision .

A generation model can generally be represented as a joint distribution
@xmath (or a conditional distribution @xmath ), where random variables
@xmath denote linguistic (syntactical and/or lexical) features. A
decision model can generally be represented by a conditional
distribution @xmath where random variables @xmath denote linguistic
features and random variable @xmath denotes usually a small number of
decisions.

Estimating a generation model requires merely positive examples. On the
other hand, estimating a decision model requires both positive and
negative examples.

A case frame generation model can be used for purposes other than
structural disambiguation. A decision model, on the other hand, is
defined specifically for the purpose of disambiguation.

In this thesis, I investigate generation models because of their
important generality.

The case slot models are, in fact, ‘one-dimensional lexical generation
models,’ the co-occurrence models are ‘two-dimensional lexical
generation models,’ and the case frame models are ‘multi-dimensional
lexical generation models.’ Note that the case frame models are not
simply straightforward extensions of the case slot models and the
co-occurrence models; one can easily define different multi-dimensional
models as extensions of the case slot models and the co-occurrence
models (from one or two dimensions to multi-dimensions).

##### Linguistic models

The models I have so far defined can also be considered to be linguistic
models in the sense that they straightforwardly represent case frame
patterns (or selectional patterns, subcategorization patterns) proposed
in the linguistic theories of [ \citename Fillmore1968 , \citename Katz
and Fodor1963 , \citename Pollard and Sag1987 ] . In other words, they
are generally intelligible to humans, because they contain descriptions
of language usage.

##### Probability distributions v.s. probabilistic measures

An alternative to defining probability distributions for lexical
knowledge acquisition, and consequently for disambiguation, is to define
probabilistic measures (e.g., the association ratio, the selectional
association measure). Calculating these measures in a theoretically
sound way can be difficult, however, and needs further investigation.

The methods commonly employed to calculate the association ratio measure
(cf., Chapter 2) are based on heuristics. For example, it is calculated
as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath denote, respectively, the Laplace estimates of
the probabilities @xmath and @xmath . Here, each of the two estimates
can only be calculated with a certain degree of precision which depends
on the size of training data. Any small inaccuracies in the two may be
greatly magnified when they are calculated as a ratio, and this will
lead to an extremely unreliable estimate of @xmath (note that
association ratio is an unbounded measure). Since training data is
always insufficient, this phenomenon may occur very frequently.
Unfortunately, a theoretically sound method of calculation has yet to be
developed.

Similarly, a theoretically sound method for calculating the selectional
association measure also has yet to be developed. (See [ \citename Abe
and Li1996 ] for a heuristic method for learning a similar measure on
the basis of the MDL principle.) In this thesis I employ probability
distributions rather than probabilistic measures.

### 6 Disambiguation Methods

The models proposed above can be independently used for disambiguation
purposes, they can also be combined into a single natural language
analysis system. In this section, I first describe how they can be
independently used and then how they can be combined.

##### Using case frame models

Suppose for example that in the analysis of the sentence

  -- -- --
        
  -- -- --

the following alternative interpretations are obtained.

  -- -- --
        
  -- -- --

  -- -------- --
     @xmath   
  -- -------- --

We wish to select the more appropriate of the two interpretations.
Suppose for simplicity that there are four possible case slots for the
verb ‘fly,’ and there is only one possible case slot for the noun ‘jet.’
A disambiguation method based on word-based case frame models would
calculate the following likelihood values and select the interpretation
with higher likelihood value:

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

If the former is larger than the latter, we select the former
interpretation, otherwise we select the latter interpretation.

If we assume here that case slots are independent, then we need only
compare

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Similarly, when the models are slot-based and the case slots are assumed
to be independent, we need only compare

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

That is to say, we need noly compare

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

The method proposed by \namecite Hindle91 in fact compares the same
probabilities; they do it by means of statistical hypothesis testing.

##### Using hard case slot models

Another way of conducting disambiguation under the assumption that case
slots are independent is to employ the hard case slot model.
Specifically we compare

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

If the former is larger than the latter, we select the former
interpretation, otherwise we select the latter interpretation.

##### Using hard co-occurrence models

We can also use the hard co-occurrence model to perform the
disambiguation task, under the assumption that case slots are
independent. Specifically, we compare

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Here, @xmath is calculated on the basis of a hard co-occurrence model
over the set of nouns and the set of verbs with respect to the ‘from’
slot, and @xmath on the basis of a hard co-occurrence model over the set
of nouns with respect to the ‘from’ slot.

Since the joint probabilities above are all estimated on the basis of
class-based models, the conditional probabilities are in fact calculated
on the basis of not only the co-occurrences of the related words but
also of those of similar words. That means that this disambiguation
method is similar to the similarity-based approach (cf., Chapter 2). The
difference is that the method described here is based on a probability
model, while the similarity-based approach usually is based on
heuristics.

##### A combined method

Let us next consider a method based on combination of the above models.

We first employ the hard co-occurrence model to construct a thesaurus
for each case slot (we can, however, construct only thesauruses for
which there are enough co-occurrence data with respect to the
corresponding case slots). We next employ the hard case slot model to
generalize values of case slots into word classes (word classes used in
a hard case slot model can be either from a hand-made thesaurus or from
an automatically constructed thesaurus; cf., Chapter 4). Finally, we
employ the class-based case frame model to learn class-based case frame
patterns.

In disambiguation, we refer to the case frame patterns, calculate
likelihood values for the ambiguous case frames, and select the most
likely case frame as output.

With regard to the above example, we can calculate and compare the
following likelihood values:

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

assuming that there are only three case slots: arg1, arg2 and ‘from’ for
the verb ‘fly,’ and there is one case slot: ‘from’ for the noun ‘jet.’
Here @xmath denotes a word class. We make the pp-attachment decision as
follows: if @xmath , we attach the phrase ‘from Tokyo’ to ‘fly;’ if
@xmath , we attach it to ‘jet;’ otherwise we make no decision.

Unfortunately, it is still difficult to attain high performance with
this method at the current stage of statistical language processing,
since the corpus data currently available is far less than that
necessary to estimate accurately the class-based case frame models.

### 7 Summary

I have proposed the soft/hard case slot model for case slot
generalization, the word-based/class-based/slot-based case frame model
for case dependency learning, and the soft/hard co-occurrence model for
word clustering. In Chapter 4, I will describe a method for learning the
hard case slot model, i.e., generalizing case slots; in Chapter 5, a
method for learning the case frame model, i.e., learning case
dependencies; and in Chapter 6, a method for learning the hard
co-occurrence model, i.e., conducting word clustering. In Chapter 7, I
will describe a disambiguation method, which is based on the learning
methods proposed in Chapters 4 and 6. (See Figure 1 .)

## Chapter 4 Case Slot Generalization

  ----------------------------------------------------------------------------
  Make everything as simple as possible - but not simpler. - Albert Einstein
  ----------------------------------------------------------------------------

In this chapter, I describe one method for learning the hard case slot
model, i.e., generalizing case slots.

### 1 Tree Cut Model

As described in Chapter 3, we can formalize the case slot generalization
problem into that of estimating a conditional probability distribution
referred to as a ‘hard case slot model.’ The problem thus turns to be
that of selecting the best model from among all possible hard case slot
models. Since the number of partitions for a set of nouns is very large,
the number of such models is very large, too. The problem of estimating
a hard case slot model, therefore, is most likely intractable. (The
number of partitions for a set of nouns is @xmath , where @xmath is the
size of the set of nouns (cf., [ \citename Knuth1973 ] ), and is roughly
of order @xmath .)

To deal with this difficulty, I take the approach of restricting the
class of case slot models. I reduce the number of partitions necessary
for consideration by using a thesaurus, following a similar proposal
given in [ \citename Resnik1992 ] . Specifically, I restrict attention
to those partitions that exist within the thesaurus in the form of a
cut. Here by ‘thesaurus’ is meant a rooted tree in which each leaf node
stands for a noun, while each internal node represents a noun class, and
a directed link represents set inclusion (cf., Figure 1 ). A ‘cut’ in a
tree is any set of nodes in the tree that can represent a partition of
the given set of nouns. For example, in the thesaurus of Figure 1 ,
there are five cuts: [ANIMAL],[BIRD, INSECT], [BIRD, bug, bee, insect],
[swallow, crow, eagle, bird, INSECT], and [swallow, crow, eagle, bird,
bug, bee, insect].

The class of ‘tree cut models’ with respect to a fixed thesaurus tree is
then obtained by restricting the partitions in the definition of a hard
case slot model to be those that are present as a cut in that thesaurus
tree. The number of models, then, is drastically reduced, and is of
order @xmath when the thesaurus tree is a complete @xmath -ary tree,
because the number of cuts in a complete @xmath -ary tree is of that
order (see Appendix 9.C ). Here, @xmath denotes the number of leaf
nodes, i.e., the size of the set of nouns.

A tree cut model @xmath can be represented by a pair consisting of a
tree cut @xmath (i.e., a discrete model), and a probability parameter
vector @xmath of the same length, that is,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath and @xmath are

  -- -------- --
     @xmath   
  -- -------- --

where @xmath forms a cut in the thesaurus tree and where @xmath is
satisfied. Hereafter, for simplicity I sometimes write @xmath for @xmath
, where @xmath .

If we employ MLE for parameter estimation, we can obtain five tree cut
models from the case slot data in Figure 1 ; Figures 2 - 4 show three of
these. For example, @xmath @xmath , @xmath shown in Figure 3 is one such
tree cut model. Recall that @xmath defines a conditional probability
distribution @xmath in the following way: for any noun that is in the
tree cut, such as ‘bee,’ the probability is given as explicitly
specified by the model, i.e., @xmath ; for any class in the tree cut,
the probability is distributed uniformly to all nouns included in it.
For example, since there are four nouns that fall under the class BIRD,
and ‘swallow’ is one of them, the probability of ‘swallow’ is thus given
by @xmath . Note that the probabilities assigned to the nouns under BIRD
are smoothed , even if the nouns have different observed frequencies.

In this way, the problem of generalizing the values of a case slot has
been formalized into that of estimating a model from the class of tree
cut models for some fixed thesaurus tree.

### 2 MDL as Strategy

The question now becomes what strategy (criterion) we should employ to
select the best tree cut model. I propose to adopt the MDL principle.

In our current problem, a model nearer the root of the thesaurus tree,
such as that of Figure 4 , generally tends to be simpler (in terms of
the number of parameters), but also tends to have a poorer fit to the
data. By way of contrast, a model nearer the leaves of the thesaurus
tree, such as that in Figure 2 , tends to be more complex, but also
tends to have a better fit to the data. Table 1 shows the number of free
parameters and the ‘KL divergence’ between the empirical distribution
(namely, the word-based distribution estimated by MLE) of the data shown
in Figure 2 and each of the five tree cut models. ¹ ¹ 1 The KL
divergence (also known as ‘relative entropy’) is a measure of the
‘distance’ between two probability distributions, and is defined as
@xmath where @xmath and @xmath represent, respectively, probabilities in
discrete distributions @xmath and @xmath [ \citename Cover and
Thomas1991 ] . In the table, we can see that there is a trade-off
between the simplicity of a model and the goodness of its fit to the
data. The use of MDL can balance the trade-off relationship.

Let us consider how to calculate description length for the current
problem, where the notations are slightly different from those in
Chapter 2. Suppose that @xmath denotes a sample (or data), which is a
multi-set of examples, each of which is an occurrence of a noun at a
slot @xmath for a verb @xmath (i.e., duplication is allowed). Further
suppose that @xmath denotes the size of @xmath , and @xmath indicates
the inclusion of @xmath in @xmath . For example, the column labeled
‘slot value’ in Table 3 represents a sample @xmath for the arg1 slot for
‘fly,’ and in this case @xmath .

Given a sample @xmath and a tree cut @xmath , we can employ MLE to
estimate the parameters of the corresponding tree cut model @xmath ,
where @xmath denotes the estimated parameters.

The total description length @xmath of the tree cut model @xmath and the
data @xmath observed through @xmath may be computed as the sum of model
description length @xmath , parameter description length @xmath , and
data description length @xmath , i.e.,

  -- -------- --
     @xmath   
  -- -------- --

Model description length @xmath , here, may be calculated as ² ² 2
Throughout this thesis, ‘ @xmath ’ denotes the logarithm to base 2.

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the set of all cuts in the thesaurus tree @xmath .
From the viewpoint of Bayesian Estimation, this corresponds to assuming
that each tree cut model to be equally likely a priori .

Parameter description length @xmath may be calculated by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the sample size and @xmath denotes the number of
free parameters in the tree cut model, i.e., @xmath equals the number of
nodes in @xmath minus one.

Finally, data description length @xmath may be calculated as

  -- -------- --
     @xmath   
  -- -------- --

where for simplicity I write @xmath for @xmath . Recall that @xmath is
obtained by MLE, i.e.,

  -- -------- --
     @xmath   
  -- -------- --

for each @xmath , where for each @xmath

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the frequency of nouns in class @xmath in data
@xmath .

With the description length defined in the above manner, we wish to
select a model with the minimum description length, and then output it
as the result of generalization. Since every tree cut has an equal
@xmath , technically we need only calculate and compare @xmath . In the
discussion which follows, I sometimes use @xmath for @xmath , where
@xmath is the tree cut of @xmath , for the sake of simplicity.

The description lengths of the data in Figure 1 for the tree cut models
with respect to the thesaurus tree in Figure 1 are shown in Table 3 .
(Table 2 shows how the description length is calculated for the model
with tree cut [BIRD, bug, bee, insect].) These figures indicate that
according to MDL, the model in Figure 4 is the best model. Thus, given
the data in Table 3 as input, we are able to obtain the generalization
result shown in Table 4 .

Let us next consider some justifications for calculating description
lengths in the above ways.

For the model description length @xmath , I assumed the length to be
equal for all the discrete tree cut models. We could, alternatively,
have assigned larger code lengths to models nearer the root node and
smaller code lengths to models nearer the leaf nodes. I chose not to do
so for the following reasons: (1) in general, when we have no
information about a class of models, it is optimal to assume, on the
basis of the ‘minmax strategy’ in Bayesian Estimation, that each model
has equal prior probability (i.e., to assume ‘equal ignorance’); (2)
when the data size is large enough, the model description length, which
is only of order @xmath , will be negligible compared to the parameter
description length, which is of order @xmath ; (3) this way of
calculating the model description length is compatible with the
dynamic-programming-based learning algorithm described below.

With regard to the calculation of parameter description length @xmath ,
we should note that the use of the looser form ( 9 ) rather than the
more precise form ( 14 ) is done out of similar consideration of
compatibility with the dynamic programming technique.

### 3 Algorithm

In generalizing the values of a case slot using MDL, if computation time
were of no concern, one could in principle calculate the description
length for every possible tree cut model and output a model with the
minimum description length as a generalization result, But since the
number of cuts in a thesaurus tree is usually exponential (cf., Appendix
9.C ), it is impractical to do so. Nonetheless, we were able to devise a
simple and efficient algorithm, based on dynamic programming, which is
guaranteed to find a model with the minimum description length.

The algorithm, which we call ‘Find-MDL,’ recursively finds the optimal
submodel for each child subtree of a given (sub)tree and follows one of
two possible courses of action: (1) it either combines these optimal
submodels and returns this combination as output, or (2) it collapses
all these optimal submodels into the (sub)model containing the root node
of the given (sub)tree. Find-MDL simply chooses the course of action
which will result in the shorter description length (cf., Figure 5 ).
Note that for simplicity I describe Find-MDL as outputting a tree cut,
rather than a tree cut model.

Note in the above algorithm that the parameter description length is
calculated as @xmath , where @xmath is the number of nodes in the
current cut, both when @xmath is the entire tree and when it is a proper
subtree. This contrasts with the fact that the number of free parameters
is @xmath for the former, while it is @xmath for the latter. For the
purpose of finding a tree cut model with the minimum description length,
however, this distinction can be ignored (cf., Appendix 9.D ).

Figure 6 illustrates how the algorithm works. In the recursive
application of Find-MDL on the subtree rooted at AIRPLANE, the if-clause
on line 9 is true since @xmath , @xmath , and hence @xmath is returned.
Similarly, in the application of Find-MDL on the subtree rooted at
ARTIFACT, the same if-clause is false since @xmath , @xmath , and hence
@xmath is returned.

Concerning the above algorithm, the following proposition holds:

###### Proposition 1

The algorithm Find-MDL terminates in time @xmath , where @xmath denotes
the number of leaf nodes in the thesaurus tree @xmath , and it outputs a
tree cut model of @xmath with the minimum description length (with
respect to the coding scheme described in Section 2 ).

See Appendix 9.D for a proof of the proposition.

### 4 Advantages

##### Coping with the data sparseness problem

Using the MDL-based method described above, we can generalize the values
of a case slot. The probability of a noun being the value of a slot can
then be represented as a conditional probability estimated (smoothed)
from a class-based model on the basis of the MDL principle.

The advantage of this method over the word-based method described in
Chapter 2 lies in its ability to cope with the data sparseness problem.
Formalizing this problem as a statistical estimation problem that
includes model selection enables us to select models with various
complexities, while employing MDL enables us to select, on the basis of
training data, a model with the most appropriate level of complexity.

##### Generalization

The case slot generalization problem can also be restricted to that of
generalizing individual nouns present in case slot data into classes of
nouns present in a given thesaurus. For example, given the thesaurus in
Figure 1 and frequency data in Figure 1 , we would like our system to
judge that the class ‘BIRD’ and the noun ‘bee’ can be the value of the
arg1 slot for the verb ‘fly.’ The problem of deciding whether to stop
generalizing at ‘BIRD’ and ‘bee’ or to continue generalizing further to
‘ANIMAL’ has been addressed by a number of researchers (cf., [ \citename
Webster and Marcus1989 , \citename Velardi, Pazienza, and Fasolo1991 ,
\citename Nomiyama1992 ] ). The MDL-based method described above
provides a disciplined way to realize this on the basis of data
compression and statistical estimation.

The MDL-based method, in fact, conducts generalization in the following
way. When the differences between the frequencies of the words in a
class are not large enough (relative to the entire data size and the
number of the words), it generalizes them into the class. When the
differences are especially noticeable (relative to the entire data size
and the number of the words), on the other hand, it stops generalization
at that level.

As described in Chapter 3, the class of hard case slot models contains
all of the possible models for generalization, if we view the
generalization process as that of finding the best configuration of
words such that the words in each class are equally likely to the value
of a case slot. And thus if we could estimate the best model from the
class of hard case slot models on the basis of MDL, we would be able to
obtain the most appropriate generalization result. When we make use of a
thesaurus (hand-made or automatically constructed) to restrict the model
class, the generalization result will inevitablely be affected by the
thesaurus used, and the tree cut model selected may be a loose
approximation of the best model. Because MDL achieves a balanced
trade-off between model simplicity and data fit, we may expect that the
model it selects will represent a reasonable compromise.

##### Coping with extraction noise

Avoiding the influence of noise in case slot data is another problem
that needs consideration in case slot generalization. For example,
suppose that the case slot data on the noun ‘car’ in Figure 6 is noise.
In such case, the MDL-based method tends to generalize a noun to a class
at quite high a level, since the differences between the frequency of
the noun and those of its neighbors are not high (e.g., @xmath and
@xmath ). The probabilities of the generalized classes will, however, be
small. If we discard those classes in the obtained tree cut that have
small probabilities, we will still acquire reliable generalization
results. That is to say, the proposed method is robust against noise.

### 5 Experimental Results

#### 5.1 Experiment 1: qualitative evaluation

I have applied the MDL-based generalization method to a data corpus and
inspected the obtained tree cut models to see if they agree with human
intuition. In the experiments, I used existing techniques (cf., [
\citename Manning1992 , \citename Smadja1993 ] ) to extract case slot
data from the tagged texts of the Wall Street Journal corpus (ACL/DCI
CD-ROM1) consisting of 126,084 sentences. I then applied the method to
generalize the slot values.

Table 5 shows some example case slot data for the arg2 slot for the verb
‘eat.’ There were some extraction errors present in the data, but I
chose not to remove them because extraction errors are such a generally
common occurrence that a realistic evaluation should include them.

When generalizing, I used the noun taxonomy of WordNet (version1.4) [
\citename Miller1995 ] as the thesaurus. The noun taxonomy of WordNet is
structured as a directed acyclic graph (DAG), and each of its nodes
stands for a word sense (a concept), often containing several words
having the same word sense. WordNet thus deviates from the notion of a
thesaurus as defined in Section 1 – a tree in which each leaf node
stands for a noun, and each internal node stands for a class of nouns;
we need to take a few measures to deal with this.

First, each subgraph having multiple parents is copied so that the
WordNet is transformed into a tree structure ³ ³ 3 In fact, there are
only few nodes in WordNet, which have multiple parent nodes, i.e., the
structure of WordNet approximates that of a tree. and the algorithm
Find-MDL can be applied. Next, the issue of word sense ambiguity is
heuristically addressed by equally dividing the observed frequency of a
noun between all the nodes containing that noun. Finally, the highest
nodes actually containing the values of the slot are used to form the
‘staring cut’ from which to begin generalization and the frequencies of
all the nodes below to a node in the starting cut are added to that
node. Since word senses of nouns that occur in natural language tend to
concentrate in the middle of a taxonomy, ⁴ ⁴ 4 Cognitive scientists have
observed that concepts in the middle of a taxonomy tend to be more
important with respect to learning, recognition, and memory, and their
linguistic expressions occur more frequently in natural language – a
phenomenon known as ‘basic level primacy.’ (cf., [ \citename Lakoff1987
] ) a starting cut given by this method usually falls around the middle
of the thesaurus.

Figure 7 indicates the starting cut and the resulting cut in WordNet for
the arg2 slot for ‘eat’ with respect to the data in Table 5 , where
@xmath denotes a node in WordNet. The starting cut consists of those
nodes @xmath , @xmath ,etc. which are the highest nodes containing the
values of the arg2 slot for ‘eat.’ Since @xmath has significantly more
frequencies than its neighbors @xmath and @xmath , MDL has the
generalization stop there. By way of contrast, because the nodes under
@xmath have relatively small differences in their frequencies, they are
generalized to the node @xmath . The same is true of the nodes under
@xmath . Since @xmath has a much higher frequency than its neighbors
@xmath and @xmath , generalization does not proceed any higher. All of
these results seem to agree with human intuition, indicating that the
method results in an appropriate level of generalization.

Table 6 shows generalization results for the arg2 slot for ‘eat’ and
three other arbitrarily selected verbs, where classes are sorted in
descending order with respect to probability values. (Classes with
probabilities less than @xmath have been discarded due to space
limitations.) Despite the fact that the employed extraction method is
not noise-free, and word sense ambiguities remain after extraction, the
generalization results seem to agree with intuition to a satisfactory
degree. (With regard to noise, at least, this is not too surprising
since the noisy portion usually has a small probability and thus tends
to be discarded.)

Table 7 shows the computation time required (on a SPARC ‘Ultra 1’ work
station, not including that for loading WordNet) to obtain the results
shown in Table 6 . Even though the noun taxonomy of WordNet is a large
thesaurus containing approximately 50,000 nodes, the MDL-based method
still manages to generalize case slots efficiently with it. The table
also shows the average number of levels generalized for each slot, i.e.,
the average number of links between a node in the starting cut and its
ancestor node in the resulting cut. (For example, the number of levels
generalized for @xmath is one in Figure 7 .) One can see that a
significant amount of generalization is performed by the method – the
resulting tree cut is on average about 5 levels higher than the starting
cut.

#### 5.2 Experiment 2: pp-attachment disambiguation

Case slot patterns obtained by the method can be used in various tasks
in natural language processing. Here, I test the effectiveness of the
use of the patterns in pp-attachment disambiguation.

In the experiments described below, I compare the performance of the
proposed method, referred to as ‘MDL,’ against the methods proposed by [
\citename Hindle and Rooth1991 ] , [ \citename Resnik1993b ] , and [
\citename Brill and Resnik1994 ] , referred to respectively as ‘LA,’
‘SA,’ and ‘TEL.’

##### Data set

As a data set, I used the bracketed data of the Wall Street Journal
corpus (Penn Tree Bank 1) [ \citename Marcus, Santorini, and
Marcinkiewicz1993 ] . First I randomly selected one of the 26
directories of the WSJ files as test data and what remained as training
data. I repeated this process ten times and obtained ten sets of data
consisting of different training and test data. I used these ten data
sets to conduct cross validation , as described below.

From the test data in each data set, I extracted @xmath quadruples using
the extraction tool provided by the Penn Tree Bank called ‘tgrep.’ At
the same time, I obtained the answer for the pp-attachment for each
quadruple. I did not double-check to confirm whether or not the answers
were actually correct. From the training data of each data set, I then
extracted @xmath and @xmath doubles, and @xmath and @xmath triples using
tools I developed. I also extracted quadruples from the training data as
before. I then applied 12 heuristic rules to further preprocess the
data; this processing included (1) changing the inflected form of a word
to its stem form, (2) replacing numerals with the word ‘number,’ (3)
replacing integers between @xmath and @xmath with the word ‘year,’ (4)
replacing ‘co.,’ ‘ltd.’ with the word ‘company,’ (5) etc. After
preprocessing some minor errors still remained, but I did not attempt to
remove them because of lacking a good method to do so automatically.
Table 8 shows the number of different types of data obtained in the
above process.

##### Experimental procedure

I first compared the accuracy and coverage for MDL, SA and LA.

For MDL, @xmath is generalized on the basis of two sets of triples
@xmath and @xmath that are given as training data for each data set,
with WordNet being used as the thesaurus in the same manner as it was in
Experiment 1. When disambiguating, rather than comparing @xmath and
@xmath I compare @xmath and @xmath , where @xmath and @xmath are classes
in the output tree cut models dominating @xmath ⁵ ⁵ 5 Recall that a node
in WordNet represents a word sense and not a word, @xmath can belong to
several different classes in the thesaurus. In fact, I compared @xmath
and @xmath . ; because I empirically found that to do so gives a
slightly better result. For SA, I employ a basic application (also using
WordNet) in which @xmath is generalized given @xmath and @xmath triples.
For disambiguation I compare @xmath and @xmath (defined in ( 2 ) in
Chapter 2)). For LA, I estimate @xmath and @xmath from the training data
of each data set and compare them for disambiguation.

I then evaluated the results achieved by the three methods in terms of
accuracy and coverage. Here ‘coverage’ refers to the percentage of test
data by which a disambiguation method can reach a decision, and
‘accuracy’ refers to the proportion of correct decisions among all
decisions made.

Figure 8 shows the accuracy-coverage curves for the three methods. In
plotting these curves, I first compare the respective values for the two
possible attachments. If the difference between the two values exceeds a
certain threshold, I make the decision to attach at the higher-value
site. The threshold here was set successively to @xmath , @xmath ,
@xmath , @xmath , @xmath , @xmath , @xmath ,and @xmath for each of the
three methods. When the difference between the two values is less than
the threshold, no decision is made. These curves were obtained by
averaging over the ten data sets. Figure 8 shows that, with respect to
accuracy-coverage curves, MDL outperforms both SA and LA throughout,
while SA is better than LA.

I also implemented the method proposed by [ \citename Hindle and
Rooth1991 ] which makes disambiguation judgements using t-scores (cf.,
Chapter 2). Figure 8 shows the result as ‘LA.t,’ where the threshold for
the t-score is @xmath (at a significance level of 90 percent.)

Next, I tested the method of applying a default rule after applying each
method. That is, attaching @xmath to @xmath for the part of the test
data for which no decision was made by the method in question.
(Interestingly, over the data set as a whole it is more favorable to
attach @xmath to @xmath , but for what remains after applying LA, SA,
and MDL, it turns out to be more favorable to attach @xmath to @xmath .)
I refer to these combined methods as MDL+Default, SA+Default,
LA+Default, and LA.t+Default. Table 9 shows the results, again averaged
over the ten data sets.

Finally, I used transformation-based error-driven learning (TEL) to
acquire transformation rules for each data set and applied the obtained
rules to disambiguate the test data (cf., Chapter 2). The average number
of obtained rules for a data set was @xmath . Table 9 shows
disambiguation results averaged over the ten data sets.

From Table 9 , we see that TEL performs the best, edging out the second
place MDL+Default by a tiny margin, and followed by LA+Default, and
SA+Default. I discuss these results below.

##### MDL and SA

Experimental results show that the accuracy and coverage of MDL appear
to be somewhat better than those of SA. Table 10 shows example
generalization results for MDL (with classes with probability less than
@xmath discarded) and SA. Note that MDL tends to select a tree cut model
closer to the root of the thesaurus. This is probably the key reason
that MDL has a wider coverage than SA for the same degree of accuracy.
One may be concerned that MDL may be ‘over-generalizing’ here, but as
shown in Figure 8 , this does not seem to degrade its disambiguation
accuracy.

Another problem which must be dealt with concerning SA is how to
increase the reliability of estimation. Since SA actually uses the ratio
between two probability estimates, namely @xmath , when one of the
estimates is unreliably estimated, the ratio may be lead astray. For
instance, the high estimated value shown in Table 10 for @xmath
drop,bead,pearl @xmath at ‘protect against’ is rather odd, and arises
because the estimate of @xmath is unreliable (very small). This problem
apparently costs SA a non-negligible drop in the disambiguation
accuracy.

##### MDL and LA

LA makes its disambiguation decision completely ignoring @xmath . As [
\citename Resnik1993b ] pointed out, if we hope to improve
disambiguation performance with increasing training data, we need a
richer model, such as those used in MDL and SA. I found that @xmath of
the quadruples in the entire test data were such that they shared the
same @xmath but had different @xmath , and their pp-attachment sites
went both ways in the same data, i.e., both to @xmath and to @xmath .
Clearly, for these examples, the pp-attachment site cannot be reliably
determined without knowing @xmath . Table 11 shows some of these
examples. (I have adopted the attachment sites given in the Penn Tree
Bank, without correcting apparently wrong judgements.)

##### MDL and TEL

TEL seems to perform slightly better than MDL. We can, however, develop
a more sophisticated MDL method which outperforms TEL, as may be seen in
Chapter 7.

### 6 Summary

I have proposed a method for generalizing case slots. The method has the
following merits: (1) it is theoretically sound; (2) it is
computationally efficient; (3) it is robust against noise. One of the
disadvantages of the method is that its performance depends on the
structure of the particular thesaurus used. This, however, is a problem
commonly shared by any generalization method which uses a thesaurus as
prior knowledge.

The approach of applying MDL to estimate a tree cut model in an existing
thesaurus is not limited to just the problem of generalizing values of a
case slot. It is potentially useful in other natural language processing
tasks, such as estimating n-gram models (cf., [ \citename Brown et
al.1992 , \citename Stolcke and Segal1994 , \citename Pereira and
Singer1995 , \citename Rosenfeld1996 , \citename Ristad and Thomas1995 ,
\citename Saul and Pereira1997 ] ) or semantic tagging (cf., [ \citename
Cucchiarelli and Velardi1997 ] ).

## Chapter 5 Case Dependency Learning

  ---------------------------------------------------------------------------------------------------------------------------------------------
  The concept of the mutual independence of events is the most essential sprout in the development of probability theory. - Andrei Kolmogorov
  ---------------------------------------------------------------------------------------------------------------------------------------------

In this chapter, I describe one method for learning the case frame
model, i.e., learning dependencies between case frame slots.

### 1 Dependency Forest Model

As described in Chapter 3, we can view the problem of learning
dependencies between case slots for a given verb as that of learning a
multi-dimensional discrete joint probability distribution referred to as
a ‘case frame model.’ The number of parameters in a joint distribution
will be exponential, however, if we allow interdependencies among all of
the variables (even the slot-based case frame model has @xmath
parameters, where @xmath is the number of random variables ), and thus
their accurate estimation may not be feasible in practice. It is often
assumed implicitly in natural language processing that case slots
(random variables) are mutually independent .

Although assuming that random variables are mutually independent would
drastically reduce the number of parameters (e.g., under the
independence assumption, the number of parameters in a slot-based model
becomes @xmath ). As illustrated in ( 2 ) in Chapter 1, this assumption
is not necessarily valid in practice.

What seems to be true in practice is that some case slots are in fact
dependent on one another, but that the overwhelming majority of them are
mutually independent, due partly to the fact that usually only a few
case slots are obligatory; the others are optional. (Optional case slots
are not necessarily independent, but if two optional case slots are
randomly selected, it is very likely that they are independent of one
another.) Thus the target joint distribution is likely to be
approximatable as the product of lower order component distributions,
and thus has in fact a reasonably small number of parameters. We are
thus lead to the approach of approximating the target joint distribution
by a simplified distribution based on corpus data.

In general, any n-dimensional discrete joint distribution can be written
as

  -- -------- --
     @xmath   
  -- -------- --

for a permutation ( @xmath ) of @xmath , letting @xmath denote @xmath .

A plausible assumption regarding the dependencies between random
variables is that each variable directly depends on at most one other
variable. This is one of the simplest assumptions that can be made to
relax the independence assumption. For example, if the joint
distribution @xmath over 3 random variables @xmath can be written
(approximated) as follows, it (approximately) satisfies such an
assumption:

  -- -------- -- -----
     @xmath      (1)
  -- -------- -- -----

I call such a distribution a ‘dependency forest model.’

A dependency forest model can be represented by a dependency forest
(i.e., a set of dependency trees), whose nodes represent random
variables (each labeled with a number of parameters), and whose directed
links represent the dependencies that exist between these random
variables. A dependency forest model is thus a restricted form of a
Bayesian network [ \citename Pearl1988 ] . Graph (5) in Figure 1
represents the dependency forest model defined in ( 1 ). Table 1 shows
the parameters associated with each node in the graph, assuming that the
dependency forest model is slot-based. When a distribution can be
represented by a single dependency tree, I call it a ‘dependency tree
model.’

It is not difficult to see that disregarding the actual values of the
probability parameters, we will have 16 and only 16 dependency forest
models (i.e., 16 dependency forests) as approximations of the joint
distribution @xmath , Since some of them are equivalent with each other,
they can be further reduced into 7 equivalent classes of dependency
forest models. Figure 1 shows the 7 equivalent classes and their
members. (It is easy to verify that the dependency tree models based on
a ‘labeled free tree’ are equivalent to one another (cf., Appendix 9.E
). Here a ‘labeled free tree’ refers to a tree in which each node is
uniquely associated with a label and in which any node can be the root [
\citename Knuth1973 ] .)

### 2 Algorithm

Now we turn to the problem of how to select the best dependency forest
model from among all possible ones to approximate a target joint
distribution based on the data. This problem has already been
investigated in the area of machine learning and related fields. One
classical method is Chow & Liu’s algorithm for estimating a
multi-dimensional discrete joint distribution as a dependency tree
model, in a way which is both efficient and theoretically sound [
\citename Chow and Liu1968 ] . ¹ ¹ 1 In general, learning a Baysian
network is an intractable task [ \citename Cooper and Herskovits1992 ] .
More recently, Suzuki has extended their algorithm, on the basis of the
MDL principle, so that it estimates the target joint distribution as a
dependency forest model [ \citename Suzuki1993 ] , and Suzuki’s is the
algorithm I employ here.

Suzuki’s algorithm first calculates the statistic @xmath between all
node pairs. The statistic @xmath between node @xmath and @xmath is
defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the empirical mutual information between random
variables @xmath and @xmath ; @xmath and @xmath denote, respectively,
the number of possible values assumed by @xmath and @xmath ; and @xmath
the input data size. The empirical mutual information between random
variables @xmath and @xmath is defined as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the maximum likelihood estimate of probability
@xmath . Furthermore, @xmath is assumed to be satisfied.

The algorithm then sorts the node pairs in descending order with respect
to @xmath . It then puts a link between the node pair with the largest
@xmath value, provided that this value is larger than zero. It repeats
this process until no node pair is left unprocessed, provided that
adding that link will not create a loop in the current dependency graph.
Figure 2 shows the algorithm. Note that the dependency forest that is
output by the algorithm may not be uniquely determined.

Concerning the above algorithm, the following proposition holds:

###### Proposition 2

The algorithm outputs a dependency forest model with the minimum
description length.

See Appendix 9.F for a proof of the proposition.

It is easy to see that the number of parameters in a dependency forest
model is of the order @xmath , where @xmath is the maximum of all @xmath
, and @xmath is the number of random variables. If we employ the ‘quick
sort algorithm’ to perform line 4, average case time complexity of the
algorithm will be only of the order @xmath , and worst case time
complexity will be only of the order @xmath .

Let us now consider an example of how the algorithm works. Suppose that
the input data is as given in Table 3 and there are 4 nodes (random
variables) @xmath , @xmath , @xmath , and @xmath . Table 2 shows the
statistic @xmath for all node pairs. The dependency forest shown in
Figure 3 has been constructed on the basis of the values given in Table
2 . The dependency forest indicates that there is dependency between the
‘to’ slot and the arg2 slot, and between the ‘to’ slot and the ‘from’
slot.

As previously noted, the algorithm is based on the MDL principle. In the
current problem, a simple model means a model with fewer dependencies,
and thus MDL provides a theoretically sound way to learn only those
dependencies that are statistically significant in the given data. As
mentioned in Chapter 2, an especially interesting feature of MDL is that
it incorporates the input data size in its model selection criterion.
This is reflected, in this case, in the derivation of the threshold
@xmath . Note that when we do not have enough data (i.e., @xmath is too
small), the thresholds will be large and few nodes will be linked,
resulting in a simpler model in which most of the random variables are
judged to be mutually independent. This is reasonable since with a small
data size most random variables cannot be determined to be dependent
with any significance.

Since the number of dependency forest models for a fixed number of
random variables @xmath is of order @xmath (the number of dependency
tree models is of order @xmath [ \citename Knuth1973 ] ), it would be
impossible to calculate description length straightforwardly for all of
them. Suzuki’s algorithm effectively utilizes the tree structures of the
models and efficiently calculates description lengths by doing it
locally (as does Chow & Liu’s algorithm).

### 3 Experimental Results

I have experimentally tested the performance of the proposed method of
learning dependencies between case slots. Most specifically, I have
tested to see how effective the dependencies acquired by the proposed
method are when used in disambiguation experiments. In this section, I
describe the procedures and the results of those experiments.

#### 3.1 Experiment 1: slot-based model

In the first experiment, I tried to learn slot-based dependencies. As
training data, I used the entire bracketed data of the Wall Street
Journal corpus (Penn Tree Bank). I extracted case frame data from the
corpus using heuristic rules. There were @xmath verbs for which more
than @xmath case frame instances were extracted from the corpus. Table 3
shows the most frequent verbs and the corresponding numbers of case
frames. In the experiment, I only considered the 12 most frequently
occurring case slots (shown in Table 4 ) and ignored others.

##### Example case frame patterns

I acquired slot-based case frame patterns for the 354 verbs. There were
on average @xmath dependency links acquired for each of these 354 verbs.
As an example, Figure 4 shows the case frame patterns (dependency forest
model) obtained for the verb ‘buy.’ There are four dependencies in this
model; one indicates that, for example, the arg2 slot is dependent on
the arg1 slot.

I found that there were some verbs whose arg2 slot is dependent on a
preposition (hereafter, @xmath for short) slot. Table 5 shows the @xmath
verbs having the largest values of @xmath , sorted in descending order
of these values. The dependencies found by the method seem to agree with
human intuition.

Furthermore, I found that there were some verbs having preposition slots
that depend on each other (I refer to these as @xmath and @xmath for
short). Table 6 shows the @xmath verbs having the largest values of
@xmath , sorted in descending order. Again, the dependencies found by
the method seem to agree with human intuition.

##### Perplexity reduction

I also evaluated the acquired case frame patterns (slot-based models)
for all of the @xmath verbs in terms of reduction of the ‘test data
perplexity.’ ² ² 2 The test data perplexity is a measure of testing how
well an estimated probability model predicts future data, and is defined
as @xmath , where @xmath denotes the estimated model, @xmath the
empirical distribution of the test data (cf., [ \citename Bahl, Jelinek,
and Mercer1983 ] ). It is roughly the case that the smaller perplexity a
model has, the closer to the true model it is.

I conducted the evaluation through a ten-fold cross validation. That is,
to acquire case frame patterns for the verb, I used nine tenths of the
case frames for each verb as training data, saving what remained for use
as test data, and then calculated the test data perplexity. I repeated
this process ten times and calculated average perplexity. I also
calculated average perplexity for ‘independent models’ which were
acquired based on the assumption that each case slot is independent.

Experimental results indicate that for some verbs the use of the
dependency forest model results in less perplexity than does use of the
independent model. For @xmath of the @xmath ( @xmath ) verbs, perplexity
reduction exceeded @xmath , while average perplexity reduction overall
was @xmath . Table 7 shows the @xmath verbs having the largest
perplexity reductions. Table 8 shows perplexity reductions for @xmath
randomly selected verbs. There were a small number of verbs showing
perplexity increases with the worst case being @xmath . It seems safe to
say that the dependency forest model is more suitable for representing
the ‘true’ model of case frames than the independent model, at least for
@xmath of the @xmath verbs.

#### 3.2 Experiment 2: slot-based disambiguation

To evaluate the effectiveness of the use of dependency knowledge in
natural language processing, I conducted a pp-attachment disambiguation
experiment. Such disambiguation would be, for example, to determine
which word, ‘fly’ or ‘jet,’ the phrase ‘from Tokyo’ should be attached
to in the sentence “She will fly a jet from Tokyo.” A straightforward
way of disambiguation would be to compare the following likelihood
values, based on slot-based models,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

assuming that there are only two case slots: arg2 and ‘from’ for the
verb ‘fly,’ and there is one case slot: ‘from’ for the noun ‘jet.’ In
fact, we need only compare

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

or equivalently,

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Obviously, if we assume that the case slots are independent, then we
need only compare @xmath and @xmath . This is equivalent to the method
proposed by [ \citename Hindle and Rooth1991 ] . Their method actually
compares the two probabilities by means of hypothesis testing.

It is here that we first employ the proposed dependency learning method
to judge if slots @xmath and @xmath with respect to verb ‘fly’ are
mutually dependent; if they are dependent, we make a disambiguation
decision based on the t-score between @xmath and @xmath ; otherwise, we
consider the two slots independent and make a decision based on the
t-score between @xmath and @xmath . I refer to this method as ‘DepenLA.’

In the experiment, I first randomly selected the files under one
directory for a portion of the WSJ corpus, a portion containing roughly
one @xmath th of the entire bracketed corpus data, and extracted @xmath
quadruples (e.g., (fly, jet, from, Tokyo)) as test data. I then
extracted case frames from the remaining bracketed corpus data as I did
in Experiment 1 and used them as training data. I repeated this process
ten times and obtained ten data sets consisting of different training
data and test data. In each training data set, there were roughly @xmath
case frames on average for verbs and roughly @xmath case frames for
nouns. On average, there were @xmath quadruples in each test data set.

I used these ten data sets to conduct disambiguation through cross
validation. I used the training data to acquire dependency forest
models, which I then used to perform disambiguation on the test data on
the basis of DepenLA. I also tested the method of LA. I set the
threshold for the t-score to @xmath . For both LA and DepenLA, there
were still some quadruples remaining whose attachment sites could not be
determined. In such cases, I made a default decision, i.e., forcibly
attached @xmath to @xmath , because I empirically found that, at least
for our data set for what remained after applying LA and DepenLA, it is
more likely for @xmath to go with @xmath . Tab. 9 summarizes the
results, which are evaluated in terms of disambiguation accuracy,
averaged over the ten trials.

I found that as a whole DepenLA+Default only slightly improves
LA+Default. I further found, however, that for about @xmath of the data
in which the dependencies are strong (i.e., @xmath or @xmath ),
DepenLA+Default significantly improves LA+Default. That is to say that
when significant dependencies between case slots are found, the
disambiguation results can be improved by using dependency knowledge.
These results to some extent agree with the perplexity reduction results
obtained in Experiment 1.

#### 3.3 Experiment 3: class-based model

I also used the @xmath verbs in Experiment 1 to acquire case frame
patterns as class-based dependency forest models. Again, I considered
only the 12 slots listed in Table 4 . I generalized the values of the
case slots within these case frames using the method proposed in Chapter
4 to obtain class-based case frame data like those presented in Table 3
. ³ ³ 3 Since a node in WordNet represents a word sense and not a word,
a word can belong to several different classes (nodes) in an output tree
cut model. I have heuristically replaced a word @xmath with the word
class @xmath such that @xmath is satisfied. I used these data as input
to the learning algorithm.

On average, there was only a @xmath dependency link found in the
patterns for a verb. That is, very few case slots were determined to be
dependent in the case frame patterns. This is because the number of
parameters in a class based model was larger than the size of the data
we had available.

The experimental results indicate that it is often valid in practice to
assume that class-based case slots (and also word-based case slots) are
mutually independent, when the data size available is at the level of
what is provided by Penn Tree Bank. For this reason, I did not conduct
disambiguation experiments using the class-based dependency forest
models.

I believe that the proposed method provides a theoretically sound and
effective tool for detecting whether there exists a statistically
significant dependency between case slots in given data; this decision
has up to now been based simply on human intuition.

#### 3.4 Experiment 4: simulation

In order to test how large a data size is required to estimate a
dependency forest model, I conducted the following experiment. I defined
an artificial model in the form of a dependency forest model and
generated data on the basis of its distribution. I then used the
obtained data to estimate a model, and evaluated the estimated model by
measuring the KL divergence between the estimated model and the true
model. I also checked the number of dependency links in the obtained
model. I repeatedly generated data and observed the ‘learning curve,’
namely the relationship between the data size used in estimation and the
number of links in the estimated model, and the relationship between the
data size and the KL divergence separating the estimated and the true
model. I defined two other artificial models and conducted the same
experiments. Figures 5 and 6 show the results of these experiments for
the three artificial models averaged over @xmath trials. The number of
parameters in Model 1, Model 2, and Model 3 are @xmath , @xmath , and
@xmath respectively, and the number of links in them @xmath , @xmath ,
and @xmath . Note that the KL divergences between the estimated models
and the true models converge to @xmath , as expected. Also note that the
numbers of links in the estimated models converge to the correct value
(1, 3, and 5) in each of the three examples.

These simulation results verify the consistency property of MDL (i.e.,
the numbers of parameters in the selected models converge in probability
to that of the true model as the data size increases), which is crucial
for the goal of learning dependencies. Thus we can be confident that the
dependencies between case slots can be accurately learned when there are
enough data, as long as the ‘true’ model exists as a dependency forest
model.

We also see that to estimate a model accurately the data size required
is as large as @xmath to @xmath times the number of parameters. For
example, for the KL divergence to go to below @xmath , we need more than
@xmath examples, which is roughly @xmath to @xmath times the number of
parameters.

Note that in Experiment 3, I considered 12 slots, and for each slot
there were roughly 10 classes as its values; thus a class-based model
tended to have about @xmath parameters. The corpus data available to us
was insufficient for accurate learning of the dependencies between case
slots for most verbs (cf., Table 3 ).

### 4 Summary

I conclude this chapter with the following remarks.

1.  The primary contribution of the research reported in this chapter is
    the proposed method of learning dependencies between case slots,
    which is theoretically sound and efficient.

2.  For slot-based models, some case slots are found to be dependent.
    Experimental results demonstrate that by using the knowledge of
    dependency, when dependency does exist, we can significantly improve
    pp-attachment disambiguation results.

3.  For class-based models, most case slots are judged independent with
    the data size currently available in the Penn Tree Bank. This
    empirical finding indicates that it is often valid to assume that
    case slots in a class-based model are mutually independent.

The method of using a dependency forest model is not limited to just the
problem of learning dependencies between case slots. It is potentially
useful in other natural language processing tasks, such as word sense
disambiguation (cf., ( [ \citename Bruce and Wiebe1994 ] )).

## Chapter 6 Word Clustering

  ------------------------------------------------------------------------------------------------------------------------------------------------------------
  We may add that objects can be classified, and can become similar or dissimilar, only in this way - by being related to needs and interests. - Karl Popper
  ------------------------------------------------------------------------------------------------------------------------------------------------------------

In this chapter, I describe one method for learning the hard
co-occurrence model, i.e., clustering of words on the basis of
co-occurrence data. This method is a natural extension of that proposed
by Brown et al (cf., Chapter 2), and it overcomes the drawbacks of their
method while retaining its merits.

### 1 Parameter Estimation

As described in Chapter 3, we can view the problem of clustering words
(constructing a thesaurus) on the basis of co-occurrence data as that of
estimating a hard co-occurrence model.

The fixing of partitions determines a discrete hard co-occurrence model
and the number of parameters. We can estimate the values of the
parameters on the basis of co-occurrence data by employing Maximum
Likelihood Estimation (MLE). For given co-occurrence data

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes a noun, and @xmath a verb. The maximum likelihood
estimates of the parameters are defined as the values that maximize the
following likelihood function with respect to the data:

  -- -------- --
     @xmath   
  -- -------- --

It is easy to verify that we can estimate the parameters as

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

so as to maximize the likelihood function, under the conditions that the
sum of the joint probabilities over noun classes and verb classes equals
one, and that the sum of the conditional probabilities over words in
each class equals one. Here, @xmath denotes the entire data size, @xmath
the frequency of word pairs in class pair @xmath , @xmath the frequency
of noun @xmath , @xmath that of @xmath , @xmath the frequency of words
in class @xmath , and @xmath that in @xmath .

### 2 MDL as Strategy

I again adopt the MDL principle as a strategy for statistical
estimation.

Data description length may be calculated as

  -- -------- --
     @xmath   
  -- -------- --

Model description length may be calculated, here, as

  -- -------- --
     @xmath   
  -- -------- --

where @xmath denotes the number of free parameters in the model, and
@xmath the data size. We in fact implicitly assume here that the
description length for encoding the discrete model is equal for all
models and view only the description length for encoding the parameters
as the model description length. Note that there are alternative ways of
calculating the model description length. Here, for efficiency in
clustering, I use the simplest formulation.

If computation time were of no concern, we could in principle calculate
the total description length for each model and select the optimal model
in terms of MDL. However, since the number of hard co-occurrence models
is of order @xmath (cf., Chapter 4), where @xmath and @xmath denote the
sizes of the set of nouns and the set of verbs respectively, it would be
infeasible to do so. We therefore need to devise an efficient algorithm
that will heuristically perform this task.

### 3 Algorithm

The algorithm that we have devised, denoted here as ‘2D-Clustering,’
iteratively selects a suboptimal MDL model from among a class of hard
co-occurrence models. These models include the current model and those
which can be obtained from the current model by merging a noun (or verb)
class pair. The minimum description length criterion can be reformalized
in terms of (empirical) mutual information. The algorithm can be
formulated as one which calculates, in each iteration, the reduction of
mutual information which would result from merging any noun (or verb)
class pair. It would perform the merge having the least mutual
information reduction, provided that the least mutual information
reduction is below a threshold, which will vary depending on the data
size and the number of classes in the current situation .

2D-Clustering @xmath
@xmath is input co-occurrence data. @xmath and @xmath are positive
integers.

1.  Initialize the set of noun classes @xmath and the set of verb
    classes @xmath as:

      -- -------- --
         @xmath   
      -- -------- --

      -- -------- --
         @xmath   
      -- -------- --

    @xmath and @xmath denote the set of nouns and the set of verbs,
    respectively.

2.  Repeat the following procedure:

    1.  execute Merge @xmath to update @xmath ,

    2.  execute Merge @xmath to update @xmath ,

    3.  if @xmath and @xmath are unchanged, go to Step 3.

3.  Construct and output a thesaurus of nouns based on the history of
    @xmath , and one for verbs based on the history of @xmath .

For the sake of simplicity, let us next consider only the procedure for
Merge as it is applied to the set of noun classes while the set of verb
classes is fixed.

Merge @xmath

1.  For each class pair in @xmath , calculate the reduction in mutual
    information which would result from merging them. (Details of such a
    calculation are given below.) Discard those class pairs whose mutual
    information reduction is not less than the threshold of

      -- -------- -- -----
         @xmath      (1)
      -- -------- -- -----

    where @xmath denotes total data size, @xmath the number of free
    parameters in the model before the merge, and @xmath the number of
    free parameters in the model after the merge. Sort the remaining
    class pairs in ascending order with respect to mutual information
    reduction.

2.  Merge the first @xmath class pairs in the sorted list.

3.  Output current @xmath .

For improved efficiency, the algorithm performs a maximum of @xmath
merges at step 2, which will result in the output of an at most @xmath
-ary tree. Note that, strictly speaking, once we perform one merge, the
model will change and there will no longer be any guarantee that the
remaining merges continue to be justifiable from the viewpoint of MDL.

Next, let us consider why the criterion formalized in terms of
description length can be reformalized in terms of mutual information.
Let @xmath refer to the pre-merge model, @xmath to the post-merge model.
According to MDL, @xmath should be that model which has the least
increase in data description length

  -- -------- --
     @xmath   
  -- -------- --

and that at the same time satisfies

  -- -------- --
     @xmath   
  -- -------- --

since the decrease in model description length equals

  -- -------- --
     @xmath   
  -- -------- --

and the decrease in model description length is common to each merge.

In addition, suppose that @xmath is obtained by merging two noun classes
@xmath and @xmath in @xmath to a noun class @xmath . We in fact need
only calculate the difference in description lengths with respect to
these classes, i.e.,

  -- -------- --
     @xmath   
  -- -------- --

Since

  -- -------- --
     @xmath   
  -- -------- --

holds, we also have

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

and

  -- -------- --
     @xmath   
  -- -------- --

Hence,

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

The quantity @xmath is equivalent to the data size times the empirical
mutual information reduction. We can, therefore, say that in the current
context a clustering with the least data description length increase is
equivalent to that with the least mutual information decrease.

Note further that in ( 2 ), since @xmath is unchanged before and after
the merge, it can be canceled out. Replacing the probabilities with
their maximum likelihood estimates, we obtain

  -- -------- --
     @xmath   
  -- -------- --

We need calculate only this quantity for each possible merge at Step 1
in Merge.

In an implementation of the algorithm, we first load the co-occurrence
data into a matrix, with nouns corresponding to rows, verbs to columns.
When merging a noun class in row @xmath and that in row @xmath ( @xmath
), for each @xmath , we add @xmath and @xmath , obtaining @xmath ; then
write @xmath on row @xmath ; move @xmath to row @xmath . This reduces
the matrix by one row.

With the above implementation, the worst case time complexity of the
algorithm turns out to be @xmath , where @xmath denotes the size of the
set of nouns, and @xmath that of verbs. If we can merge @xmath and
@xmath classes at each step, the algorithm will become slightly more
efficient, with a time complexity of @xmath .

The method proposed in this chapter is an extension of that proposed by
Brown et al. Their method iteratively merges the word class pair having
the least reduction in mutual information until the number of word
classes created equals a certain designated number. This method is based
on MLE, but it only employs MLE locally .

In general, MLE is not able to select the best model from a class of
models having different numbers of parameters because MLE will always
suggest selecting the model having the largest number of parameters,
which would have a better fit to the given data. In Brown et al’s case,
MLE is used to iteratively select the model with the maximum likelihood
from a class of models that have the same number of parameters . Such a
model class is repeatedly obtained by merging any word class pair in the
current situation. The number of word classes within the models in the
final model class, therefore, has to be designated in advance. There is,
however, no guarantee at all the designated number will be optimal.

The method proposed here resolves this problem by employing MDL. This is
reflected in use of the threshold ( 1 ) in clustering, which will result
in automatic selection of the optimal number of word classes to be
created.

### 4 Experimental Results

#### 4.1 Experiment 1: qualitative evaluation

In this experiment, I used heuristic rules to extract verbs and their
arg2 slot values (direct objects) from the tagged texts of the WSJ
corpus (ACL/DCI CD-ROM1) which consists of 126,084 sentences.

I then constructed a number of thesauruses based on these data, using
the method proposed in this chapter. Figure 1 shows a part of a
thesaurus for 100 randomly selected nouns, that serve as direct objects
of 20 randomly selected verbs. The thesaurus seems to agree with human
intuition to some degree. The words ‘stock,’ ‘security,’ and ‘bond’ are
classified together, for example, despite the fact that their absolute
frequencies are quite different (272, 59, and 79, respectively). The
results seem to demonstrate one desirable feature of the proposed
method: it classifies words solely on the basis of the similarities in
co-occurrence data and is not affected by the absolute frequencies of
the words.

#### 4.2 Experiment 2: compound noun disambiguation

I tested the effectiveness of the clustering method by using the
acquired word classes in compound noun disambiguation. This would
determine, for example, the word ‘base’ or ‘system’ to which ‘data’
should be attached in the compound noun triple (data, base, system).

To conduct compound noun disambiguation, we can use here the
probabilities

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

If the former is larger, we attach ‘data’ to ‘base;’ if the latter is
larger we attach it to ‘system;’ otherwise, we make no decision.

I first randomly selected 1000 nouns from the corpus, and extracted from
the corpus compound noun doubles (e.g., (data, base)) containing the
nouns as training data and compound noun triples containing the nouns as
test data. There were 8604 training data and 299 test data. I also
labeled the test data with disambiguation ‘answers.’

I conducted clustering on the nouns in the left position in the training
data, and also on the nouns in the right position, by using,
respectively, both the method proposed in this chapter, denoted as
‘2D-Clustering,’ and Brown et al’s, denoted as ‘Brown.’ I actually
implemented an extended version of their method, which separately
conducts clustering for nouns on the left and those on the right (which
should only improve the performance).

I conducted structural disambiguation on the test data, using the
probabilities like those in ( 3 ) and ( 4 ), estimated on the basis of
2D-Clustering and Brown, respectively. I also tested the method of using
probabilities estimated based on word occurrences, denoted here as
‘Word-based.’

Figure 2 shows the results in terms of accuracy and coverage, where
‘coverage’ refers to the percentage of test data for which the
disambiguation method was able to make a decision. Since for Brown the
number of word classes finally created has to be designed in advance, I
tried a number of alternatives and obtained results for them (for
2D-Clustering, the optimal number of word classes is automatically
selected). We see that, for Brown, when the number of word classes
finally to be created is small, though the coverage will be large, the
accuracy will deteriorate dramatically, indicating that in word
clustering it is preferable to introduce a mechanism to automatically
determine the final number of word classes.

Table 1 shows final results for the above methods combined with
‘Default’ in which we attach the first noun to the neighboring noun when
a decision cannot be made by an individual method.

We can see here that 2D-Clustering performs the best. These results
demonstrate one desirable aspect of 2D-Clustering: its ability to
automatically select the most appropriate level of clustering, i.e., it
results in neither over-generalization nor under-generalization. (The
final result of 2D-Clustering is still not completely satisfactory,
however. I think that this is partly due to insufficient training data.)

#### 4.3 Experiment 3: pp-attachment disambiguation

I tested the effectiveness of the proposed method by using the acquired
classes in pp-attachment disambiguation involving quadruples ( @xmath ).

As described in Chapter 3, in disambiguation of (eat, ice-cream, with,
spoon), we can perform disambiguation by comparing the probabilities

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

If the former is larger, we attach ‘with spoon’ to ‘eat;’ if the latter
is larger we attach it to ‘ice-cream;’ otherwise, we make no decision.

I used the ten sets used in Experiment 2 in Chapter 4, and conducted
experiments through ‘ten-fold cross validation,’ i.e., all of the
experimental results reported below were obtained from averages taken
over ten trials.

I conducted word clustering by using the method proposed in this
chapter, denoted as ‘2D-Clustering,’ and the method proposed in [
\citename Brown et al.1992 ] , denoted as ‘Brown.’ In accord with the
proposal offered by [ \citename Tokunaga, Iwayama, and Tanaka1995 ] ,
for both methods, I separately conducted clustering with respect to each
of the 10 most frequently occurring prepositions (e.g., ‘for,’ ‘with,’
etc). I did not cluster words with respect to rarely occurring
prepositions. I then performed disambiguation by using probabilities
estimated based on 2D-Clustering and Brown. I also tested the method of
using the probabilities estimated based on word co-occurrences, denoted
here as ‘Word-based.’

Next, rather than using the co-occurrence probabilities estimated by
2D-Clustering, I only used the noun thesauruses constructed by
2D-Clustering, and applied the method of estimating the best tree cut
models within the thesauruses in order to estimate conditional
probabilities like those in ( 5 ) and ( 6 ). I call this method
‘1D-Thesaurus.’

Table 2 shows the results for all these methods in terms of coverage and
accuracy. It also shows the results obtained in the experiment described
in Chapter2, denoted here as ‘WordNet.’

I then enhanced each of these methods by using a default decision of
attaching @xmath to @xmath when a decision cannot be made. This is
indicated as ‘Default.’ Table 3 shows the results of these experiments.

We can make a number of observations from these results. (1)
2D-Clustering achieves broader coverage than does 1D-Thesaurus. This is
because, in order to estimate the probabilities for disambiguation, the
former exploits more information than the latter. (2) For Brown, I show
here only its best result, which happens to be the same as the result
for 2D-Clustering, but in order to obtain this result I had to take the
trouble of conducting a number of tests to find the best level of
clustering. For 2D-Clustering, this needed to be done only once and
could be done automatically. (3) 2D-Clustering outperforms WordNet in
term of accuracy, but not in terms of coverage. This seems reasonable,
since an automatically constructed thesaurus is more domain dependent
and therefore captures the domain dependent features better, thus
helping achieve higher accuracy. On the other hand, with the relatively
small size of training data we had available, its coverage is smaller
than that of a general purpose hand-made thesaurus. The result indicates
that it makes sense to combine the use of automatically constructed
thesauruses with that of a hand-made thesaurus. I will describe such a
method and the experimental results with regard to it in Chapter 7.

### 5 Summary

I have described in this chapter a method of clustering words. That is a
natural extension of Brown et al’s method. Experimental results indicate
that it is superior to theirs.

The proposed clustering algorithm, 2D-Clustering, can be used in
practice so long as the data size is at the level of the current Penn
Tree Bank. It is still relatively computationally demanding, however,
and the important work of improving its efficiency remains to be
performed.

The method proposed in this chapter is not limited to word clustering;
it can be applied to other tasks in natural language processing and
related fields, such as, document classification (cf., [ \citename
Iwayama and Tokunaga1995 ] ).

## Chapter 7 Structural Disambiguation

  -------------------------------------------------------------------------------------------------------------------------------------
  To have good fruit you must have a healthy tree; if you have a poor tree you will have bad fruit. - The Gospel according to Matthew
  -------------------------------------------------------------------------------------------------------------------------------------

In this chapter, I propose a practical method for pp-attachment
disambiguation. This method combines the use of the hard co-occurrence
model with that of the tree cut model.

### 1 Procedure

Let us consider here the problem of structural disambiguation, in
particular, the problem of resolving pp-attachment ambiguities involving
quadruples ( @xmath ), such as (eat, ice-cream, with, spoon).

As described in Chapter 6, we can resolve such an ambiguity by using
probabilities estimated on the basis of hard co-occurrence models. I
denote them as

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Further, as described in Chapter 4, we can also resolve the ambiguity by
using probabilities estimated on the basis of tree cut models with
respect to a hand-made thesaurus , denoted as

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

Both methods are a class-based approach to disambiguation, and thus can
help to handle the data sparseness problem. The former method is based
on corpus data and thus can capture domain specific features and achieve
higher accuracy. At the same time, since corpus data is never
sufficiently large, coverage is bound to be less than satisfactory. By
way of contrast, the latter method is based on human-defined knowledge
and thus can bring about broader coverage. At the same time, since the
knowledge used is not domain-specific, accuracy might be expected to be
less than satisfactory. Since both methods have pros and cons, it would
seem be better to combine the two, and I propose here a back-off method
to do so.

In disambiguation, we first use probabilities estimated based on hard
co-occurrence models; if the probabilities are equal (particularly both
of them are 0), we use probabilities estimated based on tree cut models
with respect to a hand-made thesaurus; if the probabilities are still
equal, we make a default decision. Figure 1 shows the procedure of this
method.

### 2 An Analysis System

Let us consider this disambiguation method in more general terms. The
natural language analysis system that implements the method operates on
the basis of two processes: a learning process and an analysis process.

During the learning process, the system takes natural language sentences
as input and acquires lexical semantic knowledge. First, the POS
(part-of-speech) tagging module uses a probabilistic tagger (cf.,
Chapter 2) to assign the most likely POS tag to each word in the input
sentences. The word sense disambiguation module then employs a
probabilistic model (cf., Chapter 2) to resolve word sense ambiguities.
Next, the case frame extracting module employs a heuristic method (cf.,
Chapter 2) to extract case frame instances. Finally, the learning module
acquires lexical semantic knowledge (case frame patterns) on the basis
of the case frame instances.

During the analysis process, the system takes a sentence as input and
outputs a most likely interpretation (or several most likely
interpretations). The POS tagging module assigns the most likely tag to
each word in the input sentence, as is in the case of learning. The word
sense disambiguation module then resolves word sense ambiguities, as is
in the case of learning. The parsing module then analyzes the sentence.
When ambiguity arises, the structural disambiguation module refers to
the acquired knowledge, calculates the likelihood values of the
ambiguous interpretations (case frames) and selects the most likely
interpretation as the analysis result.

Figure 2 shows an outline of the system. Note that while for simplicity
the parsing process and the disambiguation process are separated into
two modules, they can (and usually should) be unified into one module.
Furthermore, for simplicity some other knowledge necessary for natural
language analysis, e.g., a grammar, has also been omitted from the
figure.

The learning module consists of two submodules: a thesaurus construction
submodule, and a case slot generalization submodule. The thesaurus
construction submodule employs the hard co-occurrence model to calculate
probabilities. The case slot generalization submodule then employs the
tree cut model to calculate probabilities.

The structural disambiguation module refers to the probabilities, and
calculates likelihood for each interpretation. The likelihood values
based on the hard co-occurrence model for the two interpretations of the
sentence ( 1 ) are calculated as follows

  -- -------- --
     @xmath   
  -- -------- --

  -- -------- --
     @xmath   
  -- -------- --

The likelihood values based on the tree cut model can be calculated
analogously. Finally, the disambiguation module selects the most likely
interpretation on the basis of a back-off procedure like that described
in Section 1.

Note that in its current state of development, the disambiguation module
is still unable to exploit syntactic knowledge. As described in Chapter
2, disambiguation decisions may not be made solely on the basis of
lexical knowledge; it is necessary to utilize syntactic knowledge as
well. Further study is needed to determine how to define a unified model
which combines both lexical knowledge and syntactic knowledge. In terms
of syntactic factors, we need to consider psycholinguistic principles,
e.g., the ‘right association principle.’ I have found in my study that
using a probability model embodying these principles helps improve
disambiguation results [ \citename Li1996 ] . Another syntactic factor
we need to take into consideration is the likelihood of the phrase
structure of an interpretation (cf., [ \citename Charniak1997 ,
\citename Collins1997 , \citename Shirai et al.1998 ] ).

### 3 Experimental Results

I tested the proposed disambiguation method by using the data used in
Chapters 4 and 6. Table 1 shows the results; here the method is denoted
as ‘2D-Clustering+WordNet+Default.’ Table 1 also shows the results of
WordNet+Default and TEL which were described in Chapter 4, and the
result of 2D-Clustering+Default which was described in Chapter 6. We see
that the disambiguation method proposed in this chapter performs the
best of four.

Table 2 shows the disambiguation results reported in other studies.
Since the data sets used in the respective studies were different, a
straightforward comparison of the various results would have little
significance, we may say that the method proposed in this chapter
appears to perform relatively well with respect to other
state-of-the-art methods.

## Chapter 8 Conclusions

  -----------------------------------------------------------------------------------
  If all I know is a fraction, then my only fear is of losing the thread. - Lao Tzu
  -----------------------------------------------------------------------------------

### 1 Summary

The problem of acquiring lexical semantic knowledge is an important
issue in natural language processing, especially with regard to
structural disambiguation. The approach I have adopted here to this
problem has the following characteristics: (1) dividing the problem into
three subproblems: case slot generalization, case dependency learning,
and word clustering, (2) viewing each subproblem as that of statistical
estimation and defining probability models (probability distributions)
for each subproblem, (3) adopting MDL as a learning strategy, (4)
employing efficient learning algorithms, and (5) viewing the
disambiguation problem as that of statistical prediction.

Major contributions of this thesis include: (1) formalization of the
lexical knowledge acquisition problem, (2) development of a number of
learning methods for lexical knowledge acquisition, and (3) development
of a high-performance disambiguation method.

Table 1 shows the models I have proposed, and Table 2 shows the
algorithms I have employed. The overall accuracy achieved by the
pp-attachment disambiguation method is @xmath , which is better than
that of state-of-the-art methods.

### 2 Open Problems

Lexical semantic knowledge acquisition and structural disambiguation are
difficult tasks. Although I think that the investigations reported in
this thesis represent some significant progress, further research on
this problem is clearly still needed.

Other issues not investigated in this thesis and some possible solutions
include:

  More complicated models  

    In the discussions so far, I have restricted the class of hard case
    slot models to that of tree cut models for an existing thesaurus
    tree . Under this restriction, we can employ an efficient
    dynamic-programming-based learning algorithm which can provablely
    find the optimal MDL model. In practice, however, the structure of a
    thesaurus may be a directed acyclic graph (DAG) and
    straightforwardly extending the algorithm to a DAG may no longer
    guarantee that the optimal model will be found. The question now is
    whether there exist sub-optimal algorithms for more complicated
    model classes. The same problem arises in case dependency learning,
    for which I have restricted the class of case frame models to that
    of dependency forest models. It would be more appropriate, however,
    to restrict the class to, for example, the class of normal Bayesian
    Networks. How to learn such a complicated model, then, needs further
    investigation.

  Unified model  

    I have divided the problem of learning lexical knowledge into three
    subproblems for easy examination. It would be more appropriate to
    define a single unified model. How to define such a model, as well
    as how to learn it, are issues for future research. (See [ \citename
    Miyata, Utsuro, and Matsumoto1997 , \citename Utsuro and
    Matsumoto1997 ] for some recent progress on this issue; see also
    discussions in Chapter 3.)

  Combination with extraction  

    We have seen that the amount of data currently available is
    generally far less than that necessary for accurate learning, and
    the problem of how to collect sufficient data may be expected to
    continue to be a crucial issue. One solution might be to employ
    bootstrapping, i.e., to conduct extraction and generalization,
    iteratively. How to combine the two processes needs further
    examination.

  Combination with word sense disambiguation  

    I have not addressed the word sense ambiguity problem in this
    thesis, simply proposing to conduct word sense disambiguation in
    pre-processing. (See [ \citename McCarthy1997 ] for her proposal on
    word sense disambiguation.) In order to improve the disambiguation
    results, however, it would be better to employ the soft case slot
    model to perform structural and word sense disambiguation at the
    same time. How to effectively learn such a model requires further
    work.

  Soft clustering  

    I have formalized the problem of constructing a thesaurus into that
    of learning a double mixture model. How to efficiently learn such a
    model is still an open problem.

  Parsing model  

    The use of lexical knowledge alone in disambiguation might result in
    the resolving of most of the ambiguities in sentence parsing, but
    not all of them. As has been described, one solution to the problem
    might be to define a unified model combining both lexical knowledge
    and syntactic knowledge. The problem still requires further work.
