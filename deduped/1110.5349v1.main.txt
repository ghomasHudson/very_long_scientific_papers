## Thesis Overview

Chapter 1 presents the theory of elementary particles and their
interactions (the Standard Model) in general and the spontaneous
symmetry breaking through the Higgs mechanism in particular. Possible
mechanisms to achieve the spontaneous symmetry breaking in theories
beyond the Standard Model are also briefly discussed.

Chapter 2 reviews the direct Standard Model Higgs boson searches at the
LEP, Tevatron and LHC particle accelerators and the indirect constraints
on the Higgs boson mass from fits of precision electroweak data to the
Standard Model theory predictions. The direct Higgs boson search
presented in this thesis, as well as its motivation, are also
introduced.

Chapter 3 presents the experimental infrastructure used for this
analysis, especially the Fermilab particle accelerator complex,
including the Tevatron accelerator, and the Collider Detector at
Fermilab, especially its tracking, calorimeter and muon subdetector
systems.

Chapter 4 presents the high level object reconstruction, such as tracks,
primary vertices, calorimeter clusters, charged lepton candidates
(electron, muon, isolated track), missing transverse energy, jets. The
algorithms used in this analysis that identify jets originating in
bottom quarks are also described.

Chapter 5 introduces the signal and background processes used by this
analysis, describes how an event is simulated using a Monte Carlo
generator and enumerates the generators used to simulate every relevant
physics process.

Chapter 6 details the online (trigger) and offline (analysis) event
selection. The analysis uses electron or muon triggers for the tight
charged lepton categories and the missing transverse energy plus jets
for the isolated track category. The baseline event selection, the
several @xmath -tagging categories and the non- @xmath (QCD) background
veto are also described.

Chapter 7 presents the computation of the predicted number of @xmath and
@xmath signal events, and enumerates various sources of systematic
uncertainties for the event yield calculation.

Chapter 8 presents the complex methodology used to compute the estimated
event yield for each background process, as well as the tables for
background, signal and data event yields.

Chapter 9 describes the artificial neural network (ANN) used as a final
discriminant in this analysis. After an introduction to ANNs, the
variables used in the ANN training are enumerated, an overtraining check
is presented and the ANN output shapes are overlaid for different signal
and background processes. The second part of the chapter describes a
second ANN used in this analysis to correct the jet energies to their
true parton-level energies.

Chapter 10 presents the final result of this analysis, namely the ratio
of the 95% credibility level upper limits on the Standard Model Higgs
boson cross section times branching ratio to the SM predicted values.

Chapter 11 concludes this dissertation with a review of the analysis,
its methodology, our original contributions and the results achieved.
Future plans and possible improvements are also discussed.

Appendix A describes in detail one of our major contributions to the
analysis, namely the parameterization of each of the three
missing-transverse-energy-plus-jets triggers and the measurement of the
trigger prescale.

Appendix B presents the details of another major contribution of ours to
the analysis, namely the novel method to combine any number of triggers
in order to maximize the event yield while avoiding the correlations
between triggers.

Appendix C describes the general structure, features and advantages of
the data analysis software package used for this analysis, for which I
was one of the three main developers.

Appendix D enumerates the control plots relevant to proving that the
analysis uses variables for which the background modelling in Monte
Carlo simulated events agrees very well with the data measurements.

The results presented in this dissertation have undergone multiple
stages of intensive review within the Collider Detector at Fermilab
collaboration. As such, the findings of this study may be disseminated
publicly. They have already been presented in a talk at the New
Perspectives conference on May 31 2011 and in a poster at the Fermilab
Users Meeting on June 01 2011, both at Fermilab, USA.

## Original Contributions

This dissertation presents an experimental search for the Standard Model
Higgs boson produced in association with a @xmath boson in
proton-antiproton collisions at the Tevatron accelerator at Fermilab and
recorded with the Collider Detector at Fermilab. The goal is to observe
or exclude the Standard Model Higgs boson and thus confirm or refute the
Higgs Mechanism. Research in experimental particle physics is a
collaborative effort and this @xmath search makes no exception. This
work could not have been possible without the contribution of the
approximately 600 members of the Collider Detector at Fermilab (CDF)
collaboration in general and without that of the approximately two dozen
members of the @xmath CDF group in particular. In the sections below, I
will emphasize my own original contributions to the @xmath search at CDF
and present the sections in the dissertations that explain these in more
detail, as well as the CDF published results that employ my work. Also,
the images employed in the thesis must be assumed to be my own
contribution, unless mentioned differently in the caption by a reference
or credit statement.

### A New Loose Charged Lepton Channel: “Isolated Track”

I improved the sensitivity of the @xmath search by making possible the
introduction of a novel looser criterion to reconstruct charged lepton
candidates. High-transverse-momentum good quality tracks isolated from
other activity in the tracking chamber that are not required to match a
calorimeter cluster or a muon chamber deposit are called “isolated
tracks” or “ISOTRK” candidates. These loose charged lepton candidates
form an orthogonal sample to the tight charged lepton candidates (which
we call “TIGHT” candidates) and reconstruct also real charged leptons
that arrive in the non-instrumented regions of the detector and would
otherwise have been lost (Chapter 4 and Chapter 6 ). Thus, the number of
@xmath boson candidate events is increased, which in turn increases the
expected number of @xmath signal candidate events in our analysis
(Chapter 7 ). Although the signal over background ratio is worse for the
ISOTRK category than for the TIGHT lepton category (Chapter 8 ), the
expected upper limit on the cross section times branching ratio for the
rare @xmath signal is improved for all Higgs boson mass points when both
categories are used in the limit calculation (Chapter 10 ). The final
result of this dissertation is the Higgs boson upper limit with TIGHT
and ISOTRK alone and with TIGHT and ISOTRK combined.

The @xmath analysis presented in this thesis is not the first one at
CDF. I continuously improved the trigger parametrization and therefore
the signal yield for the ISOTRK category, applying it to the @xmath
searches that used 2.7 @xmath , 4.3 @xmath and 5.7 @xmath of integrated
luminosity in different years. The @xmath search presented in this
dissertation introduces for the first time a novel method to combine
different triggers. This method is subsequently applied to two
previously used triggers and one new trigger to arrive at an updated set
of results in a sample of 5.7 @xmath of integrated luminosity.

### Three “MET + Jets” Trigger Parameterization

Since the CDF detector does not have a dedicated-“isolated track”
trigger, we use triggers based on the event information orthogonal to
the charged lepton information, i.e. missing transverse energy (MET) and
jets. We have three such MET-based triggers at CDF. In order for these
to be usable for analyses, one needs to parametrize for each trigger the
efficiency turnon curves for each trigger level, compute the trigger
prescale and identify the jet selection needed so that the trigger
efficiency is flat with respect to jet information. These measurements
are done in data samples. This complex parametrization is used to
compute for each Monte-Carlo-simulated event a trigger efficiency using
event information. The simulated events are weighted by the trigger
efficiency. One of my important contributions to the @xmath search was
determining the MET-based trigger parametrization in new datasets and
continuously improving the parametrization methodology.

My parametrization of a first MET-based trigger was used for the ISOTRK
charged lepton channel of the @xmath analysis with 2.7 @xmath that is
described in detail in the PhD dissertation of Jason Slaunwhite from the
summer of 2008 [ 1 ] . A Physical Review D paper draft based on this
analysis is currently under the second and last review of the CDF
collaboration. Our analysis is a @xmath analysis using an artificial
neural network. There is also another @xmath analysis using matrix
element computations as inputs to a multivariate technique involving a
boosted decision tree approach. The two analyses using the same event
selection and an integrated luminosity of 2.7 @xmath were combined in
the summer of 2008. The combination increased the @xmath search
sensitivity by 10% over the best of the two analyses. In this
combination, both analyses used the ISOTRK charged lepton channel and
the trigger parametrization I had measured. The result was presented at
the summer conferences of 2008 and published in a Physical Review
Letters paper [ 2 ] .

I improved the methodology for trigger parametrization and two MET-based
triggers were used for the ISOTRK charged lepton category for the @xmath
analysis with 4.3 @xmath that is described in detail in the PhD
dissertation of Yoshikazu Nagai from the summer of 2009 [ 3 ] . The
combination of the two triggers avoided correlations between triggers
since the events were divided in two orthogonal kinematic regions based
on jet information. Each trigger was used for all the events in a given
kinematic region and ignored for the events in the other kinematic
region. The result was one of the inputs for the limit calculation for
the CDF combination of August 2009 presented in Figure 2.9 and for the
Tevatron combination of November 2009 presented in the bottom part of
Figure 2.10 . The results were presented at the summer conferences of
2009.

I used the same methodology to parametrize the trigger efficiency turnon
curves using data collected with a total integrated luminosity of 5.7
@xmath . These were used to update the @xmath analysis, whose result was
again one of the inputs for the limit calculation for the CDF
combination of July 2010 presented in Figure 2.8 and for the Tevatron
combination of July 2010 presented in the top part of Figure 2.10 . The
results were presented at the summer conferences of 2010. A Physical
Review D paper draft based on this analysis is currently under
development.

I later parametrized a third additional MET-based trigger that did not
exist from the beginning of Run II of the Tevatron, but was introduced
after about 2.7 @xmath had already been collected, thanks to the effort
of the CDF Higgs Trigger Task Force. For each of the three MET-based
triggers, I measured the prescale and the necessary jet variable
selection so that the parametrization is done as a function of missing
energy only. A detailed description of the MET-based trigger
parametrization is given in Appendix A .

For three triggers, it is more complex to divide the jet kinematic phase
space in orthogonal regions and study which trigger is on average more
efficient for each region. It is in this context that I introduced a
novel method to combine any number of triggers in order to maximize the
event yield and yet not have an “OR” between the triggers in order to
avoid trigger correlations and thus systematic uncertainty estimation
difficulties.

### Novel Method to Combine Triggers

The novel method I introduced generalizes the previous method by
considering each individual event as an infinitesimally small kinematic
region. Just as before, only one trigger is assigned to all the events
in this kinematic region. A study is performed to check which trigger is
more efficient in this region. However, since the region is formed of
only one event, the study consists simply of comparing the trigger
efficiencies of the three triggers for this event and assigning to the
event only the trigger with the largest efficiency. For data events, it
is checked if the event has fired the chosen trigger. If it does, then
the event is kept. If it does not, the event is rejected, without
checking if the event has fired other triggers, which makes sure
correlations between triggers are avoided. For a Monte-Carlo simulated
event, the trigger is always assumed to fire and the event is assigned a
weight equal to the efficiency of the chosen trigger.

Although the event yield is slightly smaller than in the case when we
take a simple “OR” between triggers, a reduction in the systematic
uncertainty of the event yield is expected due to avoiding correlations
between the triggers. It also becomes easier to evaluate the systematic
uncertainty. To achieve this, I also parametrized the trigger efficiency
turnon curves in several bins of several variables. A detailed
description of the novel method to combine triggers is given in Appendix
B .

I also developed a software package called ABCDF, which is easily
portable in any CDF analysis (and in fact in any high-energy-physics
data analysis framework). ABCDF will take as input all the relevant
information from a given event and return the trigger efficiency (event
weight due to the trigger parametrization). The method works with an
unlimited number of triggers, so other triggers can in principle be
added in order to increase the signal acceptance.

The analysis presented in this thesis uses the same integrated
luminosity as of the summer of 2010, but introduces for the first time
the third MET-based trigger and the novel methodology to combine
triggers, which improves the signal acceptance and the analysis
sensitivity even more. As soon as this PhD dissertation is submitted, I
will continue to work on the @xmath analysis as a postdoctoral URA
Visiting Scholar Fellow in order to make use of the latest available
integrated luminosity and add further improvements to be part of the
Summer 2011 CDF and Tevatron combinations and to be shown at the Summer
2011 conferences.

### Main Author of a New Data Analysis Framework

As part of the subgroup that searches for the Higgs boson produced in
association with a W boson and decaying into a bottom quark pair (WH
group), I am one of the main authors that developed a new data analysis
software framework, called WHAM, that performs several analyses that use
the signature of exactly one charged lepton plus missing transverse
energy plus a number of tight jets, such as @xmath , @xmath and
Technicolor searches and single-top measurements. WHAM allows all these
analyses to share common tools in order to produce a larger number of
scientific results with less manpower. WHAM will play a crucial role
from now until the end of the CDF analysis effort, when the CDF
collaboration manpower decreases. Since I integrated my ABCDF software
package fully into WHAM, most of these ongoing analyses are benefiting
from the ISOTRK channel or from other loose muon channels that make use
of the three MET-based triggers combined with the novel method. For
these reasons, I will be a main author for the subsequent publications
of these analyses in the @xmath group at CDF. WHAM is described in
detail in Appendix C .

The work on WHAM has delayed my thesis submission by about a year. Yet,
it was time well spent and a huge investment in the analysis power in
the lepton plus jet signature at CDF, as well as in my coding and data
analysis skills.

### “Isolated Track” Scale Factor Measurement

As for all the charged lepton categories, also ISOTRK candidates have a
different reconstruction efficiency in Monte Carlo simulated events and
in data events. A postdoctoral researcher (Nils Krumnack) built a code
to measure these efficiencies for each jet multiplicity and compute
their ratio, which is also known as the “scale factor” between data and
Monte Carlo (Subsubsection 4.4.4 ). For the past two years I have
maintained this code and updated the scale factor value and its control
plots for new data periods being processed for the collaboration.

## Chapter 1 The Standard Model and the Higgs Mechanism

### 1.1 Elementary Particle Physics

Humans have always asked themselves how the world works. They looked
around them and observed a large diversity of plants, animals and
minerals. They wanted to know what all these are made of and how the
constituent elements stayed together. In other words, they wanted to
know what are the fundamental ingredients of matter and what is the
recipe used by Nature to produce out of the fundamental ingredients all
the diversity of things, both alive and inert, that exist.

Elementary particle physics ¹ ¹ 1 Elementary particle physics is also
called particle physics, subatomic physics or high energy physics. is a
domain of physics that uses the scientific method to describe the
fundamental building blocks of matter and the elementary interactions
between them. The current elementary particle physics theory is called
the Standard Model (SM). Before we examine the Standard Model in more
detail, let us have a short incursion into the history of the human
attempts to answer these fundamental questions about the Universe. Let
us examine briefly the advance of science that lead to the advent of
elementary particle physics.

#### 1.1.1 Short History of Quest for Ingredients of Matter

At first, humans attributed supernatural causes to everyday phenomena.
Their explanations touched on superstition and religion.

However, about 2,500 years ago, people living in Ancient Greece and
impassioned about understanding the Universe, who called themselves
philosophers, started to search for natural causes for natural
phenomena. Therefore, they created science and created the first schools
where science was studied and promoted. Two schools of thought in
Ancient Greece proposed two different answers to what the fundamental
ingredients of the Universe were. One proposed that everything is made
of various combinations of four fundamental elements: earth, water, air
and fire ² ² 2 This line of thought was first mentioned in the work of
the Greek philosopher Empedocles around 450 BC. . Another one proposed
that everything is made up of many types of indivisible spheres called
atoms ³ ³ 3 This school of thought was introduced by the Ancient Greece
philosophers Leucippus, Democritus and Epicurus. . Both were closer to
philosophy than to scientific theories and both had no idea of what the
underlying laws of the Universe could be.

The next big step happened around 500 years ago in Western Europe when
ancient science evolved into modern science. Modern science used both
experiments and mathematical formalism to advance knowledge of natural
phenomena. Modern science made the jump from simple observation of the
world to performing reproducible experiments. The advancement of
mathematics allowed for a mathematical formulation of scientific ideas.
Scientific theories used mathematics to make numerical predictions about
observable quantities in nature. Scientific experiments measured these
quantities. If there was an agreement, the theory was potentially
confirmed. Otherwise, the theory was contradicted. It is experiment that
is the supreme judge of truth in the scientific method ⁴ ⁴ 4 However, it
is possible for the outcomes of scientific experiments to agree with
theories that are incorrect. .

Modern science was indeed successful. First ⁵ ⁵ 5 The generation of
chemists lead by the French chemist Antoine-Laurent de Lavoisier (1743 -
1794) founded modern chemistry by discovering the chemical elements. ,
it showed that the fundamental four elements of Ancient Greece were not
fundamental after all, but rather made of other elements. Tens of such
elements were discovered and studied by a scientific field called
chemistry. Therefore, fundamental elements of the Universe were thought
to be these chemical elements, but no fundamental interaction was known
between these. Next ⁶ ⁶ 6 The British chemist John Dalton (1766 - 1844)
discovered the atoms. , chemists showed that each chemical element is
made of a certain type of atom, an idea also originating in Ancient
Greece, but whose experimental demonstration came only in the 19 @xmath
century.

Then, another field of science called physics showed that even atoms are
not elementary, but rather they have a structure. Inside atoms there are
elementary particles called electrons that have a negative electric
charge. It is the electric interaction between electrons that keeps
atoms together in molecules and molecules together in inorganic matter
or cells of living organisms. Therefore, for the first time, a recipe of
the Universe was proposed: the electric force. Physics pushed more and
discovered inside an atom also a nucleus positively charged. The same
electric force now explained the structure and stability of atoms.

Later ⁷ ⁷ 7 The British theoretical physicist James Clerk Maxwell (1831
- 1879) built in 1865 the theory of electromagnetism that described the
electric force, the magnetic force and light. it was shown that the
electric force and the magnetic force are two different aspects of one
fundamental force: the electromagnetic force. Next ⁸ ⁸ 8 The New
Zealander experimental physicist Ernest Rutherford (1871 - 1937)
discovered the nucleus in 1910 and the proton in 1919. His student,
British experimental physicist James Chadwick (1891 - 1974), discovered
the neutron in 1932. , physics showed that even nuclei have a structure,
as they are made of protons and neutrons. A new sub domain of physics
was formed: nuclear physics. In order to explain how protons and
neutrons are kept together inside a nucleus, a new interaction was
introduced, namely the strong nuclear interactions. In order to explain
why certain types of atoms are unstable and decay into other type of
atoms plus some radiation, a third interaction was introduced, namely
the weak nuclear interaction. It was the second time that fundamental
recipes of the Universe were introduced. At that stage the fundamental
ingredients of the Universe were the proton, the neutron and the
electron and the fundamental interactions of the Universe the
electromagnetic force, the gravitational force, the strong nuclear force
and the weak nuclear force.

But physicists did not stop there. They showed that not even protons and
neutrons are elementary particles. Instead, they are each formed by
three main quarks and a multitude of other quarks and antiquarks, all
kept together by the exchange of new elementary particles, gluons. A new
sub domain of physics is thus formed: particle physics. This is the
domain of science discussed in this thesis.

#### 1.1.2 Current Paradigm in Particle Physics

The current paradigm in particle physics is expressed in the theory of
the Standard Model of particle physics. The SM describes that matter is
formed by six types of quarks and six types of leptons. These elementary
particles interact with each other through exchanges of other elementary
particles called gauge bosons. There are four fundamental interactions
in nature, but the gravitational force is not described by the SM
because of the very small strength of the gravitational force at the
energies currently available in our particle accelerators. The SM
describes the strong force mediated by gluons, the weak force mediated
by the @xmath , @xmath and @xmath bosons and the electromagnetic force
mediated by the photon ( @xmath ).

However, cosmological experimental data in the last decade has shown
that the ordinary matter described by the SM represents in fact only
about 4 @xmath of the matter-energy content of the Universe, while a
currently unknown type of matter forms about 22 @xmath (dark matter) and
an unknown type of energy forms about 74 @xmath (dark energy).

#### 1.1.3 Accelerator and Cosmic-Ray Particle Physics

Particle physics also has two sub domains: accelerator-based particle
physics and cosmic-ray particle physics. In accelerator-based particle
physics, subatomic particles such as protons, antiprotons, electrons
and/or positrons are produced and accelerated to very high energies, up
to velocities very close to the speed of light in vacuum. These
accelerated beams of particles are collided either head-on with other
beams of particles (collider particle physics) or with a fixed target
material (fixed-target particle physics). In these collisions, the large
kinetic energy of incoming particles is transformed into mass of new
elementary particles, which decay into other elementary particles that
are recorded by large particle detectors that surround the collision
region.

Cosmic-ray particle physics studies collisions of very energetic cosmic
subatomic particles (neutrons, light nuclei) with protons and neutrons
from the atoms from Earth’s atmosphere. These collisions produce a
shower of particles that propagate in the atmosphere and are detected by
ground-based particle detectors. The advantage of cosmic-ray collisions
is that they are sometimes a lot more energetic than the current
particle accelerators on Earth can offer. The disadvantage is that the
experimental conditions are not reproducible and the data rates are low.

Some of the first subatomic particles were discovered in cosmic-ray
collisions: the positron, the muon, the @xmath meson. Then
accelerator-based particle physics started to dominate the advancement
of particle physics. In this thesis we will be discussing collider
particle physics.

#### 1.1.4 Elementary Particles and Interactions

Elementary particles ⁹ ⁹ 9 Elementary particles are also called
fundamental particles. come in two types: fermions and bosons [ 4 ] .
Fermions have semi-integer spins and obey the statistics of Fermi-Dirac.
Bosons have integer spins and obey the statistics of Bose-Einstein.
Matter elementary particles are fermions and they also come in two
categories: leptons and quarks. Leptons and quarks interact with each
other through the exchange of force carrier particles, which are bosons.

There are six types of leptons grouped in three weak isospin doublets.
Each pair contains a negatively charged lepton and a neutrally charged
lepton generically called neutrino. The first doublet is formed by an
electron ( @xmath ) and an electron neutrino ( @xmath ). The second
doublet is formed by a muon ( @xmath ) and a muon neutrino ( @xmath ).
The third doublet is formed by a tau lepton ( @xmath ) and a tau
neutrino ( @xmath ). Leptons interact only through the electromagnetic
and the weak forces.

There are also six types of quarks grouped in three pairs as well.
Unlike the leptons, quarks have fractional electric charge. However, the
electric charge difference between the members of the pair is also a
unit of one. Each pair contains a quark with electric charge equal to
@xmath and another quark with electric charge equal to @xmath , where
@xmath is the electric charge of the electron. The first pair is formed
by the quarks up ( @xmath ) and down ( @xmath ). The second pair is
formed by the quarks charm ( @xmath ) and strange ( @xmath ). The third
pair is formed by the quarks top ( @xmath ) and bottom ( @xmath ).
Quarks interact through all the three elementary interactions, including
the strong force which is specific only to them. Each quark possesses a
quantum number called ‘‘color’’ that can have the values of red, green
or blue ¹⁰ ¹⁰ 10 These names have nothing to do with the colours from
everyday life. They originate in an analogy with light from everyday
life, where the colours red, green and blue produce the colour white
when combined. The colour white is considered neutral. If for the
electric force there is a need of two different charges to produce a
neutral object, the strong force needs three. This is why they bear the
names of the three colours that need to be combined to produce a neutral
colour (white). .

The three families of leptons and quarks respectively are also called
generations. On average, the particles of the second generations are
more massive than the ones from the first generation and less massive
than the ones in the third generation. It is currently believed that
when the Universe was still relatively small and hot, all these
elementary particles were produced. However, as the Universe expanded
and cooled, the particles from the third generation decayed into
particles of inferior generations and as the Universe expanded and
cooled even more, the particles of the second generation decayed into
particles of the first generation. Therefore, at current energies in the
Universe, all matter is made up of particles from the first generation:
the up and down quarks form protons and neutrons, electrons are part of
the atoms and electronic neutrinos are emitted in nuclear radioactive
decay.

The electromagnetic interaction is mediated by a photon ( @xmath ). The
electroweak interaction is mediated by the @xmath , @xmath and @xmath
bosons. The strong interaction is mediated by eight type of gluons. Each
gluon has a combination of a color and an anticolor.

In Table 1.1 we present a summary of the elementary particles of the
Standard Model and their properties.

#### 1.1.5 Antiparticles

For almost every elementary particle there is an antiparticle with the
same mass, spin, lifetime and decay width, but with opposite electric
charge and other quantum numbers. The particles that are their own
antiparticles are the photon and the @xmath boson. Experiments have not
yet shown if the neutrinos are the same particle as the antineutrinos,
or if they are different particles, as the Standard Model assumes.

Unless otherwise explicitly mentioned, throughout this thesis all
statements referring to particles are also valid for their corresponding
antiparticles.

### 1.2 Particle Physics Theories

#### 1.2.1 Global Gauge Theories

The Standard Model is a local quantum field theory with a local gauge
symmetry. The equations of motion are deduced using a least action
principle.

Historically speaking, a theory with a global gauge symmetry was first
proposed by Paul Adrien Maurice Dirac who described the equations of
motion of an electron with electric charge and spin that did not
interact with other particles (free particles). The Dirac free
Lagrangian describes in fact all free fermions fields:

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

In equation 1.1 , @xmath represents a Dirac field of mass @xmath and
@xmath are Dirac’s matrices. This equation is invariant under a global
U(1) transformation described by

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

where @xmath is the electric charge, @xmath is a space-time 4-vector and
@xmath is a parameter that does not vary with spatial coordinates. There
is a theorem called Noether’s theorem [ 5 ] that states that for every
symmetry there is a conserved quantity. The U(1) symmetry results in a
conserved 4-vector current @xmath :

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

It means that the charge Q is conserved in time, i.e. the integral over
the space coordinates for @xmath .

#### 1.2.2 Local Gauge Theories

As real elementary particles interact with one another, interactions
need to be introduced in the free Lagrangian from equation 1.1 . This is
obtained by using a parameter @xmath that is space dependent. The U(1)
transformation becomes a local U(1) transformation.

  -- -------- -- -------
     @xmath      (1.4)
  -- -------- -- -------

Adding simply this local gauge transformation in the free Lagrangian
does not keep the Lagrangian invariant. In order to do so, the partial
derivative is also transformed into a covariant derivative @xmath that
is defined by the following equation:

  -- -------- -- -------
     @xmath      (1.5)
  -- -------- -- -------

where @xmath is a new 4-vector field that is transformed like this:

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

It results that

  -- -------- -- -------
     @xmath      (1.7)
  -- -------- -- -------

By replacing all these in the Dirac Lagrangian formula in equation 1.1 ,
we obtain the Lagrangian for the interacting fermions. This theory is
called Quantum ElectroDynamics (QED):

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

where @xmath is the covariant kinetic term of @xmath .

The QED theory describes the electromagnetic interaction. If we apply
the Euler-Lagrange least action principle [ 5 ] on 1.8 then we obtain
the equation of motion for a field @xmath undergoing electromagnetic
interaction through the exchange of a massless vector field @xmath :

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

Indeed, this Lagrangian does not have any mass term of the type @xmath ,
as this would break the Lorentz invariance. Therefore the vector field
@xmath is massless. This is in complete agreement with the fact that the
gauge boson mediator for the electromagnetic interaction, the photon, is
massless. This is due to the fact that the electromagnetic force’s range
of action is infinite.

#### 1.2.3 Standard Model Theory

Local gauge theories are also used to describe the other fundamental
interactions of elementary particles. The weak interaction has already
been unified theoretically with the electromagnetic interaction
described above. The new fundamental interaction called electroweak is
described by the Standard Model.

The groups @xmath describe the electroweak interaction which is
spontaneously broken into the weak interaction described by the V-A
theory and the electromagnetic interaction described by Quantum
Electrodynamics (QED). SU(2) is a non-Abelian group of spin algebra
(weak isospin group) and it has three gauge vector fields.

A simplified model with two half-integer-spin massless fermions @xmath
and @xmath can explain the electroweak interaction such as the @xmath ,
where @xmath is the electric charge. The V-A currents explain the weak
interactions of leptons using a left-handed doublet field and two
right-handed singlet fields.

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

with:

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (1.12)
  -- -------- -- --------

It is helpful to define @xmath as the third component of the weak
isospin from the @xmath group and Y the hypercharge from the @xmath
group. Then the electric charge Q, @xmath and @xmath are connected by
the Gell-Mann-Nishijima equation:

  -- -------- -- --------
     @xmath      (1.13)
  -- -------- -- --------

We can see that the left-handed doublet with @xmath and @xmath describes
a charged lepton @xmath and a neutrino @xmath . However, the
right-handed singlet with @xmath , @xmath contains only the charged
lepton. This means that there is no right-handed component of the
neutrino.

In order to express the Lagrangian for the full electroweak interaction
with three generations of massless lepton pairs, we start with the free
field Lagrangian:

  -- -------- -- --------
     @xmath      (1.14)
  -- -------- -- --------

Introducing now in the free Lagrangian a @xmath gauge transformation
given by

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

and a covariant derivative given by

  -- -------- -- --------
     @xmath      (1.16)
  -- -------- -- --------

we obtain the following interaction Lagrangian:

  -- -------- -- --------
     @xmath      (1.17)
  -- -------- -- --------

In equation 1.16 there are three vector bosons ( @xmath ) from the
@xmath generators, one ( @xmath ) from the @xmath generator, and four
coupling constants ( @xmath and @xmath , where @xmath ).

After algebraic calculations, the electroweak interaction Lagrangian
1.17 can be decomposed in a charge current term ( @xmath ) and a neutral
current term ( @xmath ):

  -- -------- -- --------
     @xmath      (1.18)
  -- -------- -- --------

The charge current term contains only terms from the left doublet field

  -- -------- -- --------
     @xmath      (1.19)
  -- -------- -- --------

We recognize in this expression that @xmath is a linear combination of
@xmath and @xmath . This Lagrangian describes the fermion interactions
mediated by the @xmath and @xmath bosons.

Since the neutral current term of the electroweak interaction Lagrangian
contains a linear combination of the neutral vector fields @xmath and
@xmath , it can be also written as the sum of two terms:

  -- -------- -- --------
     @xmath      (1.20)
  -- -------- -- --------

The first term can be written as

  -- -------- -- --------
     @xmath      (1.21)
  -- -------- -- --------

and the second term can be written as

  -- -------- -- --------
     @xmath      (1.22)
  -- -------- -- --------

In the equations 1.21 and 1.22 @xmath is the Weinberg angle. We then
write the following four equations:

  -- -------- -- --------
     @xmath      (1.23)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (1.24)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (1.25)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (1.26)
  -- -------- -- --------

These equations represent the fundamentals of the Standard Model, to
which we have to add the equations that explain the gluon exchange
interactions between the quarks. The equations above explain the fermion
interaction, as the coupling constants appear in these equations.
However, in these equations all fermions and bosons are massless. We
know from experiment that is not the case. In section 1.3 we will
discuss how a spontaneous symmetry breaking and the Higgs mechanism
generate the mass for leptons and gauge bosons without breaking the
gauge invariance of the electroweak interaction Lagrangian.

### 1.3 Spontaneous Symmetry Breaking

Systems with infinite degrees of freedom and a Lagrangian invariant
under a group @xmath of transformations can have non symmetric states
through spontaneous symmetry breaking, or a Higgs mechanism.

Starting with the electrodynamics Lagrangian we can imagine a toy Higgs
mechanism [ 6 ] [ 7 ] [ 8 ] :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --

where @xmath is the scalar field of electromagnetic interaction via the
Abelian gauge field @xmath and where @xmath , @xmath . It can be checked
easily that equation 1.3 is invariant for the following gauge
transformations:

  -- -------- -- --------
     @xmath      (1.28)
  -- -------- -- --------

The equations of motion produced using this Lagrangian have solutions
that correspond to the minimal energy, thus to the vacuum expectation
values in the lowest order perturbation theory. Since @xmath , besides
the trivial solution @xmath there exists a set of degenerate solutions
with @xmath due to the underlying gauge symmetry @xmath (see Fig. 1.1 ).
We are allowed to choose such that @xmath is real. It implies that the
lowest energy state is @xmath .

Then at first order we can write

  -- -------- -- --------
     @xmath      (1.29)
  -- -------- -- --------

Now we can substitute equation 1.29 into equation 1.3 and arrange the
Lagrangian in powers of @xmath :

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --

We can now find physical meaning in each of these lines. The first line
describes a massive vector field with mass @xmath (the previously
massless gauge boson acquired mass). The second line describes the
interaction term between the massive vector field and the neutral scalar
field with coupling strengths @xmath and @xmath . The third line
corresponds to a free scalar particle called the Higgs particle with a
mass @xmath . The fourth line describes the self interaction of the
scalar field.

The effect of the spontaneous symmetry breaking is that the initial four
degrees of freedom in equation 1.3 (two in the initial complex scalar
field and two in the massless vector field) transformed into four other
degrees of freedom (a real neutral scalar particle and a massive gauge
boson).

### 1.4 The Higgs Mechanism and the Higgs Boson

In a similar way, the Higgs mechanism can be applied to equation 1.10 to
allow the @xmath and @xmath bosons to acquire mass. Two complex scalar
fields introduced to break spontaneously the symmetry of the gauge
groups @xmath form an isodoublet with respect to the @xmath group:

  -- -------- -- --------
     @xmath      (1.31)
  -- -------- -- --------

where the charged (neutral) component of the doublet is @xmath ( @xmath
), respectively. This creates a Higgs field potential where @xmath and
@xmath :

  -- -------- -- --------
     @xmath      (1.32)
  -- -------- -- --------

From equation 1.29 we know that the neutral scalar field @xmath has a
vacuum expectation value of @xmath . Therefore, at first order we can
rewrite equation 1.31 to

  -- -------- -- --------
     @xmath      (1.33)
  -- -------- -- --------

In this equation there is an explicit @xmath gauge freedom, as three of
the four components of the field @xmath are gone and only one real
scalar field @xmath remains, with @xmath .

By putting together all the previous equations we obtain the SM part of
the Lagrangian that produces the mass of the @xmath and @xmath bosons:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
                       @xmath   
                       @xmath   
                       @xmath   
  -- -------- -------- -------- --

Equation 1.4 tells us that the @xmath bosons have acquired mass:

  -- -------- -- --------
     @xmath      (1.35)
  -- -------- -- --------

and that also the @xmath bosons have acquired mass:

  -- -------- -- --------
     @xmath      (1.36)
  -- -------- -- --------

We can see that some Standard Model parameters are now constrained by
theory, such as:

  -- -------- -- --------
     @xmath      (1.37)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (1.38)
  -- -------- -- --------

However, the mass of the Higgs boson ( @xmath , sometimes written as
@xmath as well) is not constrained by theory and has to be measured by
experiments once the Higgs boson is observed experimentally.

If a Yukawa coupling is added such as

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                       @xmath   
  -- -------- -------- -------- --

where @xmath represents a fermion and @xmath represents the
corresponding antifermion, @xmath represents a fermion field and @xmath
an antifermion field, the Higgs Mechanism will produce a term to the SM
Lagrangian that will give mass both to fermions and antifermions, both
to leptons and quarks:

  -- -------- -- --------
     @xmath      (1.40)
  -- -------- -- --------

Here the constant @xmath is also not constrained by theory and is
deduced from the measured fermion masses. The mass of a fermion is equal
with the mass of the corresponding antifermion.

### 1.5 Physics Beyond the Standard Model

The Standard Model of elementary particles and their interactions is the
most precise physics theory ever developed. No experiment has shown
convincing contradiction with the SM. However, physicists believe that
the SM is a low-energy approximation of a higher-energy theory [ 10 ] [
11 ] , much the same way that classical physics is a particular case of
quantum mechanics. There are various aspects that are not explained yet
by the Standard Model: the disappearance of antimatter shortly after Big
Bang, the nature of dark matter, the mass of neutrinos, why there are
three generations of elementary particles, etc. In addition, in order to
have a Higgs boson mass at the electroweak scale, there has to be some
mechanism that cancels the radiative corrections of the Higgs spectrum.
Elementary particle physicists perform experiments at the Tevatron, the
LHC and other particle physics laboratories around the world to test for
the validity of the Standard Model, with a hope that new phenomena will
be observed. A new theory that would describe correctly these supposedly
new phenomena will be called a theory of physics beyond the Standard
Model (BSM).

#### 1.5.1 Supersymmetry

A popular candidate theory BSM is the theory of supersymmetry (SUSY). It
predicts that every SM elementary particle has a corresponding partner
that has not yet been observed experimentally. This theory introduces a
symmetry between fermions and bosons, predicting that every SM fermion
has a bosonic partner and every SM boson has a fermionic partner. This
allows for the cancellations of Higgs radiative corrections that would
otherwise require a very precise fine tuning that physicists find hard
to accept. The supersymmetric partners of SM elementary particles are
called “superpartners”. For example, the superpartner of the top quark,
the gluon, the @xmath and @xmath bosons are called stop ( @xmath ),
gluino ( @xmath ), gauginos ( @xmath ) and gaugino ( @xmath ),
respectively.

If supersymmetry exists, it is a broken symmetry. If it were unbroken,
the superpartners would have exactly the same mass as their SM partners
and they would have been already observed experimentally. Therefore, if
the superpartners exist, they must be more massive than their SM
counterparts.

The minimal extension to the Standard Model as a supersymmetry is called
the Minimal Supersymmetric Standard Model (MSSM). In MSSM there would
not be just one Higgs boson, as predicted by the Standard Model, but 5
new particles that play the role of the Higgs boson, three neutral and
two electrically charged: @xmath , @xmath , @xmath , @xmath and @xmath [
12 ] . The lightest of the neutral Higgs particles ( @xmath ) has very
similar properties to those of the Standard Model Higgs boson. This is
why if a Standard Model Higgs boson is discovered at the Tevatron or the
LHC, precise measurements of its properties would be necessary to check
if it is really a Standard Model Higgs boson or a SUSY one. All SUSY
models predict the existence of at least two Higgs bosons.

Although LHC experiments will not have the Tevatron’s sensitivity for
low mass Standard Model Higgs search within approximately 1-2 years, the
LHC experiments will be able to improve upon the Tevatron’s results for
MSSM Higgs boson searches with 1 @xmath of integrated luminosity at
@xmath that is collected right until the projected major shutdown to
prepare the LHC for @xmath collisions.

#### 1.5.2 Dynamic Electroweak Symmetry Breaking

The Standard Model and its supersymmetric extension explains the
spontaneous symmetry breaking by the introduction of a Higgs mechanism
and the prediction of one scalar (spinless) elementary particle
recognized as a Higgs boson. However, they do not explain why there
should be in nature a scalar field with a non-zero vacuum expectation
value. For this reason, a new theory called “Technicolor” providing a
dynamic reason for the electroweak symmetry breaking was created in 1979
by Weinberg [ 13 ] and Susskind [ 14 ] . Back then, the most massive
known fermion was the bottom quark, with a mass of approximately 5
@xmath [ 4 ] . The electroweak theory had been developed and it had
predicted the existence of the @xmath ( @xmath ) gauge boson with a mass
of approximately 80 (90) @xmath , although they had not yet been
observed experimentally. Since the largest fermion mass was negligible
with respect to the gauge boson masses, the Technicolor theory explained
the mass of the gauge bosons and not the mass of the fermions. The
Technicolor theory is very similar to the QCD theory and introduces the
dynamic spontaneous symmetry breaking in a similar way as the
spontaneous chiral symmetry breaking in QCD. Therefore, the Technicolor
theory introduces a new strong interaction, a new gauge group and
predicts the existence of new elementary particles called techniquarks.

Since the Technicolor theory could not predict the mass of Standard
Model fermions, it was an incomplete theory. The theory was extended and
models called Extended Technicolor were introduced in order to explain
also the fermion masses. However, precise experimental measurements
revealed that the predictions of the theory were refuted for quarks as
massive as the charm quark. The theory was therefore further refined and
Walking Technicolor and Multi-Scale Technicolor emerged. Some
combinations between Supersymmetry and Technicolor theories also
appeared.

When in 1995 the top quark was discovered and was found to have the
unexpected large mass of approximately 175 @xmath [ 4 ] , which is also
the value of the weak scale @xmath , where @xmath is the Fermi constant,
it was suggested that the top quark could play a special role in the
spontaneous symmetry breaking in theories beyond the Standard Model. A
theory called Topcolor was created, but since it predicted a top quark
mass of 220 @xmath , the theory was immediately refuted. However, when
Topcolor and Technicolor theories are combined, a new theory called
Topcolor Assisted Technicolor was created, where the top quark is the
first of the predicted techniquarks. This theory explains the mass of
all gauge bosons and fermions, including the heavy top quark. The
predictions of this theory can be checked at the Tevatron or the LHC.

In conclusion, there are a variety of dynamic spontaneous symmetry
breaking theories that have evolved with time. A well written summary of
such theories can be read in Reference [ 15 ] .

### 1.6 Summary

This chapter started by introducing elementary particle physics, the
subdomain of physics that studies the elementary particles, the smallest
building blocks of matter, and the elementary interactions or forces
that allow the elementary particles to form bound states and thus create
the structure of matter we see around us. We presented the two
experimental methods to study elementary particles: accelerator particle
physics and cosmic-ray particle physics. We continued by presenting the
advancement of particle physics theories from global gauge theories to
local gauge theories. We then presented the current theory of particle
physics, a local gauge theory called the Standard Model of elementary
particles and their interactions. The Standard Model has been confirmed
by all the numerous experiments in elementary particle physics ever
conducted. However, the theory by itself does not allow elementary
particles to acquire mass. We know experimentally that elementary
particles have masses, though, and if they had none, the Universe would
be very different than it is today and we would not exist.

In 1964 a new theory was proposed by the theorists Higgs, Englert,
Brout, Guralnik, Hagen and Kibble, which was later called the Higgs
mechanism. The theory explained in an elegant way the spontaneous
symmetry breaking of the electro-weak interaction into the
electromagnetic interaction and the weak interaction, which is
equivalent to splitting an elementary particle into the photon of zero
mass and the @xmath boson of non zero mass, which is equivalent to
introducing a mechanism to allow the @xmath boson to acquire mass. The
Higgs mechanism predicts the existence of a scalar field that pervades
the entire Universe, the Higgs field. Each elementary particle couples
to the Higgs field with a strength proportional to its mass. The Higgs
mechanism is a falsifiable theory, since it predicts the existence of a
new elementary particle, the Higgs boson, which is described uniquely by
its mass. This thesis will present an experimental search for the
existence of the Standard Model Higgs boson and therefore an
experimental test of the Higgs Mechanism and the Standard Model of
particle physics. We concluded the chapter by discussing several
spontaneous symmetry breaking explanations in theories beyond the
Standard Model, such as the Supersymmetry or Technicolor theories.

In the next chapter we will present the summary of the direct and
indirect searches for the Standard Model Higgs boson that have been
performed until now at particle accelerators around the world, namely
the LEP, the Tevatron and the LHC accelerators. We will also introduce
and motivate the Higgs boson direct search presented in this
dissertation.

## Chapter 2 Standard Model Higgs Boson Experimental Searches

The Standard Model Higgs boson has not yet been observed experimentally.
However, experimental limits on the Standard Model Higgs boson mass have
been set both by direct and indirect searches. In this chapter we will
introduce the direct searches at the LEP and Tevatron accelerators, as
well as the prospects for the LHC accelerator. We will also present the
indirect electroweak fits for the Higgs boson. Finally we will present
our @xmath search at CDF at the Tevatron and present our motivation for
choosing this search. In this dissertation, by a Higgs boson we mean a
Standard Model Higgs boson.

### 2.1 Direct Searches at the LEP Accelerator

The Large Electron Positron Collider (LEP) collided electrons and
positrons between 1989 and 2000 at centre-of-mass energies @xmath
between 189 and 209 GeV [ 16 ] . A total of 2461 pb @xmath of integrated
luminosity of collision data was analyzed by each of the four detector
collaborations at LEP: OPAL, ALEPH, L3 and DELPHI. They looked for Higgs
boson production in association with a Z boson, where the Higgs boson
decayed to a pair of bottom-antibottom quarks and the Z boson decayed
leptonically ( @xmath , with @xmath or @xmath and @xmath ), or where the
Higgs boson decayed to a @xmath lepton pair and the Z boson decayed to a
generic quark pair ( @xmath , with @xmath and @xmath ).

The results from each experiment and channel were combined by the LEP
Electroweak Working Group and a Standard Model Higgs boson was excluded
at 95 @xmath confidence level (CL) ¹ ¹ 1 All CDF and Tevatron results
presented in this thesis use a Bayesian statistical treatment in setting
confidence levels denoted with CL. All other results presented from
other experiments use a frequentist approach with confidence level also
denoted with CL. [ 4 ] for a mass less than 114.4 @xmath , as we can see
in Figure 2.1 .

### 2.2 Electroweak Indirect Fits

Although the Higgs boson mass cannot be predicted by theory, it can be
constrained through fits on electroweak parameters that are measured by
experiment. Radiative corrections due to the Higgs boson loops to the
mass of the @xmath and @xmath bosons or the top quark, as shown in
Figure 2.2 , depend on the Higgs boson mass. Inversely, precise
measurements of these masses put an indirect constraint on the Higgs
boson mass.

The contribution of Higgs boson mass to the mass of gauge bosons can be
given by taking into account Feynman diagrams of radiative corrections,
such as those in Figure 2.2 , and is given by the following formula:

  -- -------- -- -------
     @xmath      (2.1)
     @xmath      (2.2)
  -- -------- -- -------

where @xmath is the Fermi coupling constant, @xmath is the Weinberg
angle, @xmath , @xmath , @xmath and @xmath are, respectively, the masses
of top quark, @xmath and @xmath bosons and Higgs boson.

Figure 2.3 [ 18 ] shows predictions of Higgs boson mass as a function of
the @xmath boson mass @xmath and the top quark mass @xmath or vice versa
. The dashed (solid) line represents indirect constraints on @xmath and
@xmath from LEP-I and SLD experiments (LEP-II and Tevatron-II)
experiments. Both contours are plotted at 68% CL. The green contour
represents the allowed phase space for @xmath and @xmath as a function
of the Higgs boson mass. We can see that the two contours agree and they
suggest a low mass Higgs boson, just beyond the direct lower mass limit
set by the LEP experiments. The arrow labelled as @xmath shows the
global variation if @xmath is changed by one standard deviation. This
variation gives an additional uncertainty to the SM band shown in the
figure.

Figure 2.4 conveys the same message in a different manner. The plot
presents the quality of the Standard Model fit ( @xmath ) as a function
of the Higgs boson mass. The Higgs mass preferred by the fit is the one
that minimizes @xmath . The latest fit is produced by the LEP
Electroweak Working Group [ 19 ] using @xmath [ 20 ] and @xmath [ 21 ] :

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

and the 95% confidence level upper limit is

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

We see that the direct preferred Higgs boson mass is excluded by the
direct searches at LEP experiments. If this exclusion is taken into
account and a new fit is performed, then the 95% confidence level values
increases up to 185 @xmath [ 19 ] .

### 2.3 Direct Searches at the Tevatron

The Tevatron accelerator at the Fermi National Accelerator Laboratory
(FNAL) in Batavia, Illinois (in the suburbs of Chicago), USA, collides
protons and antiprotons at a centre-of-mass energy of 1.96 TeV. For
almost two decades, the collisions at the Tevatron were both the most
energetic and with the highest instantaneous luminosity in the world.
However, in March 2010 the Large Hadron Collider (LHC) at Le Centre
Européen de Recherche Nucléaire (CERN) in Geneva, Switzerland, has
broken Tevatron’s record for centre-of-mass energy. As of March 2010,
the LHC collides protons and protons at a centre-of-mass energy of 7
TeV, with a predicted 14 TeV energy in about five years. As of May 2011,
the LHC has demonstrated instantaneous @xmath luminosities in excess of
@xmath , more than double the highest @xmath luminosity achieved by the
Tevatron. In this dissertation we present a Standard Model Higgs boson
search at the Tevatron.

#### 2.3.1 Higgs Boson Production at the Tevatron

There are various processes predicted by the Standard Model through
which a Higgs boson can be produced at the Tevatron accelerator. The
cross sections of these processes vary with the centre-of-mass energy of
collisions.

At the Tevatron, the relative cross sections can be seen in Figure 2.5 .
The most likely is gluon-gluon fusion. Next in line and about ten times
less likely is an associated production of a @xmath boson and a Higgs
boson. Next comes the associated production between a @xmath boson and a
Higgs boson and is about twice less likely than the previous process.
These are followed by vector-boson fusion and the associated production
between a top-quark pair and Higgs bosons, which have almost negligible
cross sections.

#### 2.3.2 Higgs Boson Decay

The Standard Model predicts the Higgs boson decay modes and their
branching ratios independent of the way the Higgs boson was produced ² ²
2 Be it at the LHC in @xmath collisions at @xmath or at the Tevatron in
@xmath at @xmath , the Higgs boson decays in the same way, depending
only on its mass. However, the production cross section increases with
the centre-of-mass energy. and depend only on the Higgs boson mass, as
we can see in Figure 2.6 .

Higgs boson masses up to 114.4 @xmath have been excluded at 95% CL by
direct searches at experiments at the LEP accelerator. Also, all Higgs
boson masses from 186 @xmath upward have been excluded also at 95% CL by
indirect electroweak fits. The remaining interval of possible Higgs
boson masses is divided into values below and above 135 @xmath . If in
the interval 114.4 @xmath -135 @xmath (also called the “low mass
range”), the Higgs boson decays predominantly to a pair of
bottom-antibottom quarks ( @xmath ). On the other hand, if in the
interval 135 @xmath -186 @xmath (also called the “high mass range”), the
Higgs boson decays predominantly to a pair of @xmath bosons ( @xmath ) ³
³ 3 Since both @xmath bosons cannot be on shell for a Higgs boson mass
below approximately 160 @xmath , one of the @xmath bosons will be a
virtual particle, hence the notation with “*”. .

A @xmath boson can decay leptonically to a charged lepton and its
corresponding neutrino ( @xmath , or @xmath or @xmath ) or hadronically
to a quark-antiquark pair ( @xmath ).

#### 2.3.3 Low Mass Direct Searches at the Tevatron

At the Tevatron, the quark pair production through QCD processes has a
cross section of about ten orders of magnitude larger than the Higgs
boson processes. Quarks hadronize very quickly and they are observed in
particle detectors as collimated streams of subatomic particles called
jets. The key therefore is that at least one @xmath or @xmath boson
originating from a Higgs boson event is observed in the detector through
its leptonic decay products.

Since an event with a Higgs boson produced through gluon fusion that
subsequently decays to a bottom-quark pair ( @xmath ) does not present
any charged lepton, it is practically impossible to observe a low mass
Higgs boson produced through gluon fusion due to the higher backgrounds.

However, the process next in line in terms of cross section (WH
associated production) can be observed when the @xmath boson is
reconstructed through its leptonic decay: @xmath . The charged lepton
(an electron or a muon) is reconstructed in the detector and the
neutrino is seen as missing transverse energy. Therefore, the compromise
is that a ten times smaller signal cross section process brings the
advantage of a high QCD background rejection due to the presence in the
Higgs boson event of a @xmath boson that is reconstructed through its
leptonic decay.

A similar channel is the @xmath associated production. The @xmath boson
is also reconstructed through leptonic decay, either to a pair of
charged leptons ( @xmath ) or to a pair of neutrinos ( @xmath ). The
sensitivity of this channel is similar but smaller than that of the
@xmath channel.

It is also possible to search for a Higgs boson produced in gluon fusion
that decays to a pair of tau leptons ( @xmath ). A tau lepton decays
either leptonically or hadronically. By asking for one tau lepton to
decay leptonically and another one to decay hadronically this channel is
also somewhat sensitive at the Tevatron, although less than the
associated production modes.

#### 2.3.4 High Mass Direct Searches at the Tevatron

If in the high mass range, a Higgs boson decays to a pair of @xmath
bosons. Reconstructing both @xmath bosons reduces significantly the QCD
and electroweak backgrounds. This search is the most sensitive at the
Tevatron in the high mass range.

The most recent high mass direct search from CDF was performed using an
integrated luminosity of 7.2 @xmath and is presented in the top part of
Figure 2.7 . When combined to the high mass analyses at DZero using up
to an integrated luminosity of 8.2 @xmath , a Tevatron high mass Higgs
boson combination result is achieved, which is presented at the bottom
part of Figure 2.7 . These results were made public on 7 March 2011 and
were presented at the conferences of Winter 2011 [ 23 ] .

#### 2.3.5 This Analysis: SM @xmath Search at CDF II

Our motivation for choosing a Standard Model Higgs boson search for this
PhD thesis, although a SUSY Higgs search would have been possible as
well ⁴ ⁴ 4 There are other analyses at CDF and also at the DZero, ATLAS
and CMS detectors searching for a SUSY Higgs boson. , is that we feel
that priority should be given to the effort of discovering or excluding
a Standard Model Higgs boson. The Tevatron accelerator was chosen
because it was the only particle accelerator in the world to have been
and still be sensitive to Standard Model Higgs boson on the timescale of
the thesis. Given that electroweak indirect fits favour a light Higgs
over a more massive one, this thesis performs a low mass Higgs boson
search. The most sensitive low mass Higgs boson search at the Tevatron
is the associated production @xmath , where the @xmath bosons decay
leptonically and the Higgs bosons decay to a pair of bottom quarks. The
leading-order Feynman diagram of the @xmath process is presented in
Figure 5.1 . This is the analysis presented in this dissertation.

#### 2.3.6 Combination of all CDF SM Higgs Analyses

The @xmath search is one of the most sensitive low mass Standard Model
Higgs boson modes at CDF, but not the only one. CDF combined in July
2010 all Higgs searches, both at low and high mass, to obtain an
improved CDF Standard Model upper limit on the cross section [ 24 ] .
The individual analyses expected ⁵ ⁵ 5 Expected limits measure the
sensitivity of the analysis and are computed as the median of a
distribution of many pseudo-experiments using pseudo-data, as explained
in Section 10.5 . and observed upper limits are presented in the top
part of Figure 2.8 . My work contributed directly to the @xmath search
using exactly two tight jets (in red) and therefore contributed also to
the combined CDF result, which is also presented in detail in the bottom
part of Figure 2.8 .

The CDF Higgs combination from July 2010 improves and supersedes the
same result from August 2009 [ 25 ] , to which my work also contributed
directly as part of the @xmath search (in red). The CDF Higgs
combination from August 2009 is presented in Figure 2.9 .

#### 2.3.7 Combination of all Tevatron SM Higgs Analyses

In order to achieve a higher sensitivity for the Standard Model Higgs
boson search, all the low and high mass search channels, both from CDF
and DZero, are combined to create one Tevatron cross section times
branching ratio plot. The latest Tevatron combination dates from July
2010 and can be seen in the top part of Figure 2.10 . It excludes at 95%
CL a SM Higgs boson with a mass in the range 158-175 @xmath [ 26 ] . In
the low mass region, the limit is about 2 times the SM prediction. This
result improves upon and supersedes the previous Higgs Tevatron
combination result from November 2009, which had excluded the interval
@xmath to @xmath [ 27 ] and which can be seen in the bottom part of
Figure 2.10 . My work contributed directly to both of these combined
Higgs searches.

### 2.4 Expected Higgs Boson Production at the LHC

The only other particle accelerator in the world that can search for a
Standard Model Higgs boson is the Large Hadron Collider (LHC) at the
CERN laboratory in Europe. At the moment, the LHC is colliding protons
and protons at centre-of-mass energy of @xmath . The current run is
scheduled to end in 2012, with a “technical stop” at the end of 2011. At
least @xmath of integrated luminosity is expected to be collected during
this run by each LHC experiment. This run will be followed by a shutdown
lasting about two years when the LHC will be upgraded to be able to run
at the design centre-of-mass energy of @xmath .

Various processes have different cross sections as a function of the
colliding particles and the centre-of-mass energy. Due to similar
behaviour for background processes and different behaviour for signal
processes, as seen in Figure 2.11 , the most sensitive channels for a
Higgs boson discovery at the LHC are different than those at the
Tevatron. A compilation of the most up-to-date cross section values for
SM Higgs boson can be found in Reference [ 22 ] .

The ATLAS [ 28 ] and CMS [ 29 ] experiments have made public their first
searches for the Standard Model Higgs boson for the winter conferences
of 2011. However, none of those results were as sensitive as the
Tevatron combination result from the summer of 2010.

### 2.5 Summary

In this chapter we have presented the history of direct searches for the
Standard Model Higgs boson at the LEP, Tevatron and LHC accelerators, as
well as the constraints on the Higgs boson mass from fits between
precise experimental electroweak measurements and the Standard Model
prediction. Since the Standard Model Higgs boson has not yet been
observed experimentally and since it is uniquely described by its mass,
experimental particle physicists have refuted at 95% CL the existence of
the Higgs boson if its mass is in a certain mass range. For example, in
2000 the combination of several direct searches from four collaborations
at the LEP accelerator at CERN have excluded at 95% CL Higgs bosons with
masses smaller than 114.5 @xmath . In the summer of 2010, the
combination of several direct searches from the CDF and DZero
collaborations at the Tevatron accelerator at Fermilab have excluded at
95% CL Higgs boson masses in the range of 158-175 @xmath . There were
also indirect fits from measurements of the masses of the @xmath boson
and top quark to the Standard Model predictions that exclude at 95% CL
Higgs boson masses with values larger than 185 @xmath .

At the winter conferences of 2011 the first searches for the Standard
Model Higgs boson from the ATLAS and CMS collaborations at the LHC
accelerator at CERN have been made public, but they are not yet more
sensitive than the Tevatron results. However, the situation is expected
to change in 2012, when the LHC experiments will take the lead from the
Tevatron once more and in a few extra years will observe or exclude the
Standard Model Higgs boson over its entire available mass range.

This dissertation presents a direct search for the Standard Model Higgs
boson by the Collider Detector at Fermilab collaboration at the Tevatron
accelerator at Fermilab. The allowed Higgs mass interval may be divided
into a low mass region and a high mass region, depending on the most
probable decay channel. Since indirect electroweak fits suggest a Higgs
mass in the low mass region, we chose to perform the most sensitive
search at the Tevatron for a low mass Higgs, which is the associated
production between a @xmath boson and a Higgs boson. The leptonic decay
of the @xmath bosons allows us to reconstruct in the detector the
electron or muon candidate and thus increase considerably the signal
over background ratio and thus the sensitivity of the analysis.

In the next chapter we will present the experimental infrastructure used
to perform the @xmath search.

## Chapter 3 Experimental Infrastructure

The Higgs boson search presented in this thesis is performed using the
Collider Detector at Fermilab (CDF) that surrounds one of the two
collision points at the Tevatron accelerator hosted at Fermilab,
Batavia, IL, USA. In this chapter we will present the experimental
infrastructure of our analysis, namely the accelerator complex at
Fermilab and the CDF detector.

### 3.1 The Fermi National Accelerator Laboratory

The Fermi National Accelerator Laboratory (also known as Fermilab or
FNAL) was founded by Robert W. Wilson (1914-2000) to be the largest
particle physics laboratory in the USA ¹ ¹ 1 Besides the scientific
aspect, Fermilab offers many other things to the US in general and its
local community in particular. Fermilab owns a very large surface of
land where the prairie was recovered as it used to be a couple of
hundred years ago. In addition, a bison herd is raised in a large farm
at Fermilab. Fermilab also hosts numerous lakes where a number of
migratory birds take refuge, especially the Canada geese. The public is
allowed to walk or bike in the natural environment at Fermilab. Fermilab
is also a strong supporter of science communication to the general
public. Its second director, Leon Lederman, created a science outreach
centre at Fermilab. In addition, there are Saturday morning physics
lectures and other public lectures from the latest developments on
science and science policy. I had the chance to spend many months at
Fermilab and I took great pleasure of enjoying all these various
opportunities that Fermilab has to offer. . As of January 2011, Fermilab
remains the only national particle physics laboratory in the US, as
other former particle physics laboratories have switched to related
fields of science ² ² 2 For example, the Brookhaven National Laboratory
hosts the RHIC accelerator that performs heavy-ion collisions and
studies the quark-gluon plasma that existed shortly after the Big Bang,
and the Stanford Linear Accelerator Center (SLAC) now uses its electron
accelerator to produce free electron lasers. .

Fermilab hosts a multi-stage complex accelerator complex, of which the
Tevatron is just the most energetic accelerator. The Tevatron performs
proton-antiproton collisions that are recorded by two general purpose
detectors, CDF and DZero. These collisions are used for fundamental
particle physics research. Fermilab also hosts a series of fixed target
experiments, as beams of protons are extracted from the Tevatron and
collided with fixed targets. In this process secondary beams of mesons,
muons or neutrinos are produced.

### 3.2 Fermilab Accelerator Complex

The Tevatron is a proton-antiproton storage ring where @xmath collisions
are made to occur at a centre-of-mass energy @xmath . Its first
collisions were achieved in 1986 and since then a series of improvements
allowed for many increases in collision energy and instantaneous
luminosity. The first physics run (Run I) used @xmath and took place
between 1992 and 1996. Between 1997 and 2001, both the accelerator
complex and the particle detectors were improved dramatically. While the
Tevatron accelerated only 6 bunches of protons or antiprotons in Run I,
it accelerates 36 in Run II. Also, while the time interval between two
consecutive bunch crossings was 3500 ns in Run I, it decreases to 396 ns
in Run II.

Run II started in 2001 and as of January 2011 is still in progress.
Initially the Tevatron was scheduled to shut down in 2009, when the LHC
was supposed to start its physics program at @xmath and higher
instantaneous luminosity. However, delays in the LHC construction and an
incident that stopped the LHC for an extra year motivated prolonging Run
II at the Tevatron until September 2011. Furthermore, as the LHC plans
to run about 2 years at @xmath and then have a one year and a half major
shutdown to prepare the LHC for @xmath has prompted Fermilab to seek
approval and financing for a Run III for the Tevatron from September
2011 through August 2014.

The main motivation is that the Tevatron has exhibited excellent
performance and is running smoothly, breaking instantaneous luminosity
records every month. The Fermilab Accelerator Complex is very well
understood after about 25 years of usage. Run II was initially designed
to collect 2 @xmath of integrated luminosity and over 10 @xmath have
already been delivered, with an expected 12 @xmath before the end of Run
II in 2011.

Acceleration of protons and antiprotons to collision energies is
realized by a complex of eight accelerators, two linear
(Cockcroft-Walton and Linac) and six circulating synchrotrons (Booster,
Main Injector, Debuncher, Accumulator, Recycler and Tevatron). This huge
accelerator complex consumes 30 MW of electric power and stretches over
9 km.

Proton-antiproton collisions take place at two points around the
Tevatron storage ring, in two buildings called BZero and DZero. A
general purpose particle detector surrounds each collision point: the
Collider Detector at Fermilab (CDF) in the BZero building and the DZero
experiment in the DZero building.

The accelerator complex at Fermilab [ 30 ] and the CDF and DZero
experiments are shown schematically in figure 3.1 and from a bird’s eye
view in figure 3.2 . The proton and antiproton sources and the various
acceleration stages are described in the sections below.

#### 3.2.1 Proton Source

First, protons have to be produced. A strong electric field ionizes
hydrogen atoms at room temperature (0.04 eV/atom) and sends protons and
electrons in opposite directions. The protons fall on and stick to a
cesium surface. The work needed to free an electron from a cesium
surface is smaller than in the case of any other atom, since cesium is
the most reactive atom. A falling atom may collide with a group
consisting of a proton and two electrons that are temporally together on
the cesium surface. The group is thus freed from the surface and it
forms a hybrid negative hydrogen ion ( @xmath ). Thanks to the same
electric field, a continuous @xmath beam of about 25 keV is collected.

The @xmath beam is accelerated by a Cockcroft-Walton accelerator to an
energy of 750 keV by a constant electric field. The acceleration voltage
is limited by the fact that at high voltages the air creates sparks.

The @xmath beam is subsequently accelerated to 400 MeV by a 130 m long
linear accelerator called the Linac. The Linac uses alternating current
and resonant frequency cavity technology. The continuous beam is
therefore bunched up. When outside a cavity, a bunch is accelerated by
an electric field. When inside a cavity, a bunch does not see the
electric field now in the opposite direction and therefore is not
decelerated. As @xmath particles acquire momentum, cavities and gaps are
longer to provide constant acceleration. A typical bunch contains @xmath
particles. A typical pulse contains 4,000 bunches, a total of @xmath
particles and a typical pulse length corresponds to 20 ms. While in the
Linac, a particle is accelerated by an electric field of 3MV/m. The beam
power is 18 MW when the pulsed hybrid @xmath ion beam exits the Linac.

The @xmath beam is injected into a 475 m circumference circular
synchrotron accelerator called the Booster. After the first bending
magnet in the Booster, the beam passes through a carbon foil, after
which we are left with a @xmath (proton) beam. Since the 20 ms pulse
length for the Linac is much larger than the 2.2 ms Booster
circumference, the @xmath pulse is present for several rotations of the
new proton beam inside the Booster. The fact that the incoming pulse is
made of @xmath instead of protons allows the merging of the two beams
inside the Booster. The choice of @xmath for the linear accelerator is
therefore not driven by the acceleration process, but by the complex
engineering process of transferring the beam from the linear accelerator
to a circular accelerator. The same process happens also at the
Brookhaven accelerator complex [ 31 ] .

The Booster synchrotron accelerates charged particles thanks to a
resonant frequency cavity. As their momentum increases, particles are
kept at a constant radius by a corresponding increase of the magnetic
field. The proton beam is accelerated every turn by a 500 kV voltage
drop. After completing 16,000 turns in 33 ms, the beam has 8 GeV, exits
the Booster and enters the Main Injector synchrotron accelerator. The
Main Injector injects 150 GeV protons into the Tevatron synchrotron
accelerator and 120 GeV protons into the antiproton production complex.

#### 3.2.2 Antiproton Source

Antiproton production occurs in the antiproton source. The bunched beam
of 120 GeV protons from the Main injector smashes on a 7 cm nickel
target every 1.5 s. Particles created in the forward direction are
recovered through a lithium lens. A pulsed magnet acting as a
charge-mass spectrometer selects only antiprotons. The antiproton beam
is pulsed, which means the beam exhibits a large energy spread and a
small time spread. To be debunched, the beams passed into another
synchrotron accelerator, called the Debuncher. Low (high) energy
antiprotons follow the interior (exterior) path, arrive at different
times at the resonance frequency cavity. As they see different phases,
low (high) energy antiprotons are accelerated (decelerated). After about
100 ms, the antiproton beam is almost continuous, having a small energy
spread and a large time spread. After 1.5 seconds in the Debuncher, the
beam is injected into a circulating synchrotron called the Accumulator.
A new pulsed antiproton beam is then inserted into the Debuncher. It
takes 1 million 120 GeV protons to hit the nickel target for 20
antiprotons at 8 GeV each to be injected into the Accumulator.

The Accumulator uses stochastic cooling to accumulate antiprotons while
keeping them at the desired (very small) longitudinal (transverse)
momentum for hours, even days. The Accumulator has a shape of a triangle
with rounded corners. Stochastic cooling [ 32 ] transforms particles
from a hot state, with large spreads in energy, to a cooler state, with
smaller spreads in energy, thanks to a feedback technique using pickups
and kickers. The triangular shape of the Accumulator is driven by the
necessity to have several 16-meter-long straight sections to accommodate
the pickups and kickers for the stochastic cooling [ 33 ] . Van der Meer
received in 1984 the Nobel Prize for inventing stochastic cooling.

The continuous beam of 8 GeV antiprotons from the Accumulator is
injected in the Main Injector. The Main Injector replaced in 1998 the
Main Ring situated in the same tunnel as the Tevatron. This represents
one of the major upgrades from Run I to Run II. The Main Injector
accelerates both protons and antiprotons in the same ring, using the
same magnetic field for ensuring the circular trajectory for these
particles. 150 GeV antiprotons are sent in the Tevatron accelerator
where they are accelerated to 980 GeV and collided with the proton beam.
When a store ends, almost 75% of the antiprotons survive. Since creating
antiprotons is such a hard task, surviving antiprotons are recuperated
in another synchrotron accelerator, called the Recycler.

The Recycler sits just above the Main Injector and acts as a
fixed-energy storage ring thanks to its permanent magnets and stochastic
cooling. The Recycler receives antiprotons both from the Accumulator and
from the Tevatron at the end of a store. The Recycler acts as an
antiproton storage ring until the Tevatron is ready to accept
antiprotons in a new store.

#### 3.2.3 Tevatron

When built in 1983, the Tevatron was the first superconducting
synchrotron accelerator. The Tevatron’s 1000 superconducting
electromagnets can produce a magnetic field as large as 4.2 Tesla.
Electromagnet coils are made of 8 mm niobium-titanium alloy wire. One
coil contains about 70,000 km of wire. A dipole magnet is about 6.4 m
long. Once per turn, particles receive a kick in energy of about 650 kV
from a resonance frequency cavity. In about 20 seconds the magnetic
field increases gradually from 0.66 Tesla to 3.54 Tesla, while the beam
energies increase gradually from 150 GeV to 800 GeV. Meanwhile, the
beams turn around the 1 km radius circular accelerator 1 million times.
When the beams arrive at 980 GeV, an electric current of more than 4 kA
flows through the electromagnet and creates a magnetic field of 4.2
Tesla. For comparison, the superconducting magnets at the LHC will run
at 8.4 Tesla when the beam energy will be 7 TeV. Superconducting
electromagnet coils kept at liquid helium temperature (4.3 K) have no
resistance and therefore dissipate no energy through the Joule effect.
Significantly larger currents are able to flow though these coils in
order to produce very large magnetic fields. Tevatron’s cryogenic system
is one of the world’s largest, along with HERA’s and LHC’s. If it
absorbs 23 kW of power, it can still maintain the liquid helium
temperature. The system can deliver 1000 litres/hour of liquid helium at
4.2 K.

Table 3.1 summarizes the acceleration characteristics of the different
stages of the Fermilab @xmath Accelerator Complex. In this table, @xmath
expresses the speed of the particle as a fraction of the speed of light
in vacuum and @xmath is the relativistic factor. Also, for highly
relativistic particles, kinetic and total energies can be approximated
to be the same.

#### 3.2.4 Collisions and Luminosity

##### Store

When 36 new bunches of protons and 36 new bunches of antiprotons enter
the Tevatron, it is said that a new store starts. A typical bunch length
is 0.43 meters. Both beams have an average energy per accelerated
particle of @xmath . A proton bunch contains typically @xmath protons.
An antiproton bunch contains typically @xmath antiprotons. Since
antiprotons are antimatter, they annihilate with regular matter. This is
why antiprotons are accumulated about one order of magnitude less than
protons. As the two beams collide head on at a rate of 2.5 million times
per second, @xmath scatterings occur at a certain rate per unit of area,
which is described by the instantaneous luminosity.

##### Luminosity

The Tevatron’s instantaneous luminosity is given by:

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

The first fraction represents quantities that cannot be easily changed
after the experiment is started, such as f, the beam revolution
frequency at the Tevatron, which is set by the radius and the speed of
light c; E, the beam energy set by the physics goals of the experiment;
@xmath , the beam emittance at injection set by getting the beam into
the Tevatron. The second fraction presents quantities that can be
changed easily during the period of taking data, such as @xmath , the
number of proton or antiproton bunches found at one time in the
Tevatron; @xmath , the strength of the final focus; @xmath ( @xmath ),
the number of protons (antiprotons) per bunch.

However, the instantaneous luminosity delivered by the Tevatron is not
calculated by experiments using this formula. It is rather measured by a
special detector apparatus described in section 3.3.2 .

As the store’s duration increases, instantaneous luminosity decreases
exponentially, in the first few hours due to the intra-beam scattering
and later due to antiproton depletion. Instantaneous luminosity is
expected to reach 50% in 7 hours and to reach 1/ @xmath in 12 hours.
Typically after 24 hours a store is ended as the proton and antiproton
bunches are evacuated from the Tevatron. Subsequently new bunches are
inserted in the Tevatron and a new store starts.

Table 3.2 compares various parameters of Run I and Run II of Tevatron,
especially in terms of luminosity [ 34 ] .

A typical integrated luminosity per week is @xmath . Figure 3.3 shows
the weekly and integrated luminosity delivered by the Tevatron since the
start of Run II. The analysis presented in this thesis uses an
integrated luminosity of 5.7 @xmath . A dataset of about 8.0 @xmath is
currently under preparation to be shown at the Summer conferences of
2011.

##### Quench

Stores may end prematurely when the beam is lost in a process called
quenching. Quenches may happen when a beam hits a superconducting
magnet. The magnet is locally not superconducting any more and releases
energy by the Joule effect [ 35 ] . Soon the whole magnet warms up and
is no longer superconducting. Physicists then need to wait for the whole
magnet to be cooled down to liquid helium temperature before inserting a
new store in the Tevatron.

##### Number of Collisions

Accelerator based particle physics examines final-state particles
created by the initial beam particle collisions. The most interesting
processes often have very rare signatures. Counting experiments count
the number of events where the particular signature appears; any excess
above the estimated background is considered as signal. Per unit time, a
signature occurs in a number of events proportional to the physical
probability of occurrence (cross section @xmath ) and to the beam
collision conditions in the accelerator complex (instantaneous
luminosity @xmath ). However, not all events are reconstructed and
identified by the particle physics detector. The experimental efficiency
( @xmath ) measures the percentage of events that are seen correctly by
the detector. Therefore, the observed number of events per unit of time
is given by

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

Integrating the instantaneous luminosity in time provides the integrated
luminosity, which gives the total number of observed events:

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

Since the physical cross section of many processes increases with the
centre-of-mass energy ( @xmath ), particle physicists try to build
accelerators with larger and larger @xmath . Furthermore, particle
physicists try to build better detectors with reconstruction
efficiencies for various particles very close to one.

### 3.3 The Collider Detector at Fermilab

The Collider Detector at Fermilab II (CDF II or simply CDF) is a general
purpose subatomic particle detector that surrounds one of the two @xmath
collision points of the Tevatron accelerator ³ ³ 3 The other general
purpose detector at the Tevatron is DZero. There is a friendly
competition between CDF and DZero, with each detector collaboration
thriving to achieve a physics result first, but with other collaboration
needed to confirm the first result before it is fully accepted by the
particle physics community. Moreover, only together CDF and DZero have a
chance to exclude or discover the Standard Model Higgs boson. Therefore,
it is customary that, once or twice a year, CDF and DZero combine their
Higgs boson search in what is called a “Tevatron combination”. and is
described in detail in the Technical Design Report of CDF II Detector [
36 ] and in the CDF paper [ 37 ] . During Tevatron Run I there was an
older version of CDF, CDF I [ 38 ] . The detector underwent a major
upgrade in preparation for Run II of Tevatron, thus creating CDF II.

The CDF detector has a cylindrical symmetry (i.e. both azimuthal and
forward-backward) around the proton-antiproton beam pipe. CDF is formed
of three main subdetector systems. The first and innermost one is the
tracking system. It is made of a set of silicon strips and an open-cell
drift chamber inside a solenoid-produced magnetic field. This system
detects momenta of electrically charged particles and displacements of
secondary particle vertices with respect to primary vertices ⁴ ⁴ 4 The
primary vertex is the position of the primary hard-scattering
interaction. . Next comes the calorimeter system, which measures
energies for both electrically charged and electrically neutral
particles as they produce a shower of secondary particles through
interaction with dense matter ⁵ ⁵ 5 The energies of all the secondary
particles from the shower are measured as they are absorbed by the
calorimeter system. They are all added to obtain the energy of the
initial particle. . Finally, outside the calorimeter system is located
the muon subdetector, which measures the momenta of muon candidates ⁶ ⁶
6 Muons are minimum ionizing particles and leave only very faint energy
deposits in the calorimeter system. This is why we need a dedicated muon
detector system on the outer part of the CDF. .

Besides the three main subdetector systems, CDF has a subdetector system
called Time of Flight (TOF) that measures the mass of low-momentum
charged particles, as well as an instantaneous luminosity counter called
the Cherenkov Luminosity Counter (CLC) that uses Cherenkov light
emission to measure the number of @xmath collisions per second inside
CDF.

The CDF detector is built around the ‘‘beam pipe’’ , which is a vacuum
pipe with a diameter of 2.2 cm through which the proton and antiproton
beams circle around the Tevatron. The pipe is made of beryllium because
the material provides the lowest particle interaction cross section
possible ⁷ ⁷ 7 The atomic number Z of beryllium is 4. Typically the
cross section of interaction of a subatomic particle with a material
increases with Z. while also possessing good mechanical properties.
Proton beams circulate clockwise as seen from the top and enter CDF from
the west side, while antiproton beams circulate counterclockwise and
enter CDF from the east side.

This section will cover in detail the different subdetectors of CDF II
briefly described above. In figure 3.4 we can see a diagram of CDF II
with one quadrant taken out so that we can clearly see the interior. The
various subdetector systems are mentioned in the diagram. Figure 3.5
shows an elevation view of the CDF detector.

#### 3.3.1 The CDF Coordinate System

The coordinate system used by CDF reflects the cylindrical symmetry of
the detector. However, depending on the situation, a cylindrical or
adapted spherical coordinate system is used.

The z axis lies along the beamline, with the positive @xmath side in the
direction the protons travel (from west to east). A longitudinal plane
is parallel to the z axis, while a transverse plane is perpendicular to
the z axis. Although the Cartesian coordinates @xmath and @xmath are not
typically used, for the sake of completeness we should note that the
@xmath direction is horizontal towards north and the @xmath direction is
vertical and upward.

The radial coordinate @xmath is the radial distance from the beamline
and is expressed by the formula:

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

The azimuthal angle is noted @xmath , represents the angle made around
the beamline and is expressed by the formula:

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

The polar angle, denoted @xmath , represents the angle made with respect
to the beamline and is expressed by the formula:

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

However, since @xmath is not a Lorentz-invariant quantity ⁸ ⁸ 8 Protons
and antiprotons are extended objects travelling along the @xmath axis
with an energy of 980 GeV. However, not all elementary constituents of
protons (generically called partons, such as valance quarks, sea quarks
and gluons) have the same momentum along the z axis and the number of
particles per unit of @xmath angle ( @xmath )is not Lorentz invariant. ,
a derived quantity called rapidity ( @xmath ) is Lorentz invariant and
is defined by the formula

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

where E is the energy and @xmath is the longitudinal momentum of a
particle described.

Moreover, since in collider physics the particles involved have total
energies much larger than their rest energies, we can use the
approximation that @xmath . As such, the rapidity is approximated by a
new quantity called pseudorapidity ( @xmath ), which is not Lorentz
invariant, but is approximately Lorentz invariant, and is defined by

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

The advantage of the @xmath quantity is that it does not depend on the
mass of the particle and therefore is a pure geometrical quantity and
can be used to describe the trajectory of all elementary particles. The
cylindrical coordinate system ( @xmath , @xmath , @xmath ) is used to
describe the geometry of the detector, while the adapted spherical
coordinates ( @xmath , @xmath ) are used to describe the direction of a
particle inside the detector.

A quantity @xmath is used to measure the distance between two directions
of particles inside the detector (in the @xmath - @xmath plane):

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

For a particle with energy @xmath and momentum @xmath , we define
transverse energy as @xmath and transverse momentum as @xmath .

#### 3.3.2 The Cherenkov Luminosity Counter

Although Tevatron accelerator personnel measure the instantaneous
luminosity that the Tevatron is delivering, only a fraction of it,
albeit one close to unity, is recorded by particle detectors. This is
why each detector has its own subdetector that measures instantaneous
luminosity as well.

The general formula of instantaneous luminosity is:

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

where @xmath is the number of events (the number of hard (inelastic)
scattering interactions) recorded per unit time, @xmath the
instantaneous luminosity of the beam, @xmath the cross section of the
given event and @xmath is the efficiency of observing that event in the
detector after all selection requirements (the fraction of signal events
that pass the selection requirements). In this particular case, @xmath
represents the average number of inelastic @xmath scattering
interactions per bunch crossing ( @xmath ) times the number of bunch
crossings per second ( @xmath ), or the rate of bunch crossings):

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

By plugging equation 3.11 into equation 3.10 we obtain the following
equation:

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

and we can find an expression for the instantaneous luminosity @xmath :

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

where @xmath is the number of bunch crossings per second at the Tevatron
Run II ( @xmath ), and @xmath is the cross section of @xmath inelastic
scattering, which has been determined experimentally [ 39 ] .

Only @xmath and @xmath are not known. For this purpose, CDF uses long
conical gaseous Cherenkov counters to measure the average inelastic
interactions per bunch crossing ( @xmath ) and the efficiency to detect
them ( @xmath ). The name of this detector subsystem is Cherenkov
Luminosity Counter (CLC) [ 40 ] [ 41 ] . It has been designed especially
for CDF II in order to cope with increased Tevatron instantaneous
luminosity (on the order of @xmath ) and decreased time interval between
consecutive bunch crossings (396 ns).

This subdetector system is made of two modules located in the forward
regions of CDF (both on west and east sides), immediately after the beam
pipe, in a 3 \degree gap between the beam pipe and the rest of the
detector systems, corresponding to a @xmath region of @xmath , as we can
see in Figure 3.6 .

Each CLC module is formed of 3 concentric layers of 18 Cherenkov
counters each. Each Cherenkov counter is long and conical, stretching
from the interaction region towards one of the forward regions. The
Cherenkov cones in the two outer layers (one interior layer) have a
length of about 180 (160) cm. Being conical, their diameter increases
from 2 cm in the interaction region to 6 cm in the forward region. At
the latter end there is a conical mirror that collects light into a 2.5
cm diameter photomultiplier tube (PMT). The light is produced by
particles created in @xmath collisions that travel at large @xmath
pseudorapidities, thus travelling only inside one cone and emitting
Cherenkov light as they travel through the gaseous environment in the
cone. Each PMT has a 1 mm thick concave-convex quartz window to collect
the ultraviolet light of the Cherenkov spectra with a gain of @xmath .
The gaseous environment is isobutane at atmospheric pressure (1 atm),
with the possibility to increase the pressure up to 2 atm if there is a
need to increase the yield of Cherenkov light. In this environment,
particles emit Cherenkov light with an angle of 3.1 \degree if they have
a momentum larger than @xmath (electrons) and @xmath (pions).

The conical geometry and orientation were chosen so that particles
produced in @xmath collisions close to the centre of the CDF detector
can travel a large distance inside of the CLC, producing an important
light yield (several hundred photo-electrons), while the particles
produced by beam halo or secondary particles travel at smaller @xmath ,
travel a smaller distance in the CLC and have a smaller light yield.

By requiring a certain minimum light yield threshold in each channel,
the background from beam halo and secondary particles is rejected and
only the signal of particles produced in @xmath interactions is
measured. Also, each module has an excellent time resolution of less
than 100 ps. This allows to ask for coincidence hits in the two modules
in the forward and rear regions of CDF with respect to the @xmath axis,
which improves the signal and background separation.

Finally the CLC subdetector measures continuously the parameter @xmath ,
while studies on the CLC have measured the efficiency @xmath . Using
formula 3.13 CDF measures the instantaneous luminosity it records. The
uncertainty on the instantaneous luminosity measurement is 5.9% [ 42 ]
and is quoted as a systematic uncertainty on any measurement at CDF,
including the Higgs boson search described in this thesis.

#### 3.3.3 The Tracking Systems

The CDF detector has two tracking systems that provide very precise
measurements of charged particle momenta. Both follow the cylindrical
geometry of CDF and are embedded in a 1.4 T solenoidal magnetic field.
Inside the magnetic field, charged particles follow helical
trajectories, whose parameters ⁹ ⁹ 9 The tracking helical parameters are
usually expressed by five non independent variables: @xmath , @xmath (or
@xmath ), curvature, @xmath and @xmath (the impact parameter, or the
distance of minimum approach between a track and the beam axis). are
measured by a silicon strip vertex detector and a cylindrical open-cell
drift chamber.

The closest to the beam pipe is the silicon detector, which is made up
of three subdetectors: Layer 00 (L00), the Silicon Vertex Detector
(SVX-II) and the Intermediate Silicon Layers (ISL). It allows precise
measurements of the @xmath coordinate for the primary interaction
vertex, impact parameters and @xmath for tracks, as well as
identification of secondary vertices in jets originating from @xmath
-hadrons. Next comes the drift chamber, which is called the Central
Outer Tracker (COT). A schematic view of the tracking systems in CDF is
presented in Figure 3.7 .

##### The Solenoid

The superconducting solenoid is 5 m long, is made of Nb-Ti stabilized
with Al, is operated at a current of about 5650 A, and generates a
uniform 1.4 T magnetic field parallel to the @xmath axis and pointing in
the direction of the proton beam. The Lorentz force will bend the
trajectories of charged particles. Measuring their curvatures by the
tracking systems allows the precise measurements of the momenta of
charged particles.

##### The Silicon Vertex Detector

The physics principle of a silicon detector is a reversed-biased
semiconductor p-n junction. When a charged particle passes through, it
deposits energy in the detector material through ionization. This
creates electron-hole pairs. Electrons drift towards the anode and holes
drift toward the cathode. The energy deposited by the incoming particle
is estimated by the amount of charge deposited in the detector, which is
to first order proportional to the path length traversed in the material
by the charged particle.

CDF uses a strip geometry for its silicon microdetectors. This allows to
reconstruct the position of the particle as it travels through the
detectors. The distance between two silicon microstrips is about 60
@xmath m. A single incoming charged particle will typically deposit
energy in more than one microstrip, forming a charge deposition called a
“cluster”. CDF employs two types of silicon microstrip detectors: single
and double-sided. The single microstrip detectors have only the p side
of the junction segmented into strips that are parallel to the @xmath
axis, thus allowing @xmath - @xmath position measurements. The double
sided microstrip detectors have in addition the n side segmented in
strips at an (stereo) angle with respect to the p sides (that are
parallel to the @xmath axis), thus allowing the measurement of the
@xmath position as well.

###### Layer 00

The first layer of the silicon detector, Layer 00 (L00) [ 43 ] , is
mounted directly on the beam pipe between 1.35 cm and 1.62 cm of radius.
It has two overlapping hexagonal structures that can be seen in red in
Figure 3.8 . These detectors have to be very resistant to radiation from
the beam pipe and have only single-sided microstrips. Although they
provide only @xmath - @xmath measurements, they improve the spatial
resolution up to 15 @xmath m per hit, whereas the resolution is 20
@xmath m per hit without them.

###### Svx-Ii

Next comes the second silicon vertex detector, the SVX-II [ 44 ] . It
extends from a radius of 2.5 cm to a radius of 11 cm and it has a @xmath
coverage of @xmath cm. SVX-II is formed of five concentric layers of
double-sided silicon microstrip detectors, which allow for a 3D position
measurement with a spacial resolution of about 20 @xmath m. The first
two layers can be seen in black in Figure 3.8 and all the five layers
can be seen in black in figure 3.9 .

###### Intermediate Silicon Layers

Further out in radius is the last silicon detector, the Intermediate
Silicon Layers (ISL) [ 45 ] . ISL is made of three double-sided detector
layers. Going from small radius outwards, there is a layer with @xmath
at @xmath cm, then a layer with @xmath at @xmath cm, followed finally by
a layer with @xmath at @xmath cm. The three layers can be seen in green
in Figure 3.9 .

Tracks in the central region are reconstructed using both silicon and
COT information. However, only silicon information is used to
reconstruct tracks in the forward region, up to @xmath .

##### The Central Outer Tracker

Besides the silicon detector, the tracking system contains a cylindrical
open-cell drift chamber called the Central Outer Tracker (COT) [ 46 ] .
The COT has a cylindrical geometry, extends from a radius of 43.4 cm up
to one of 132.2 and has a length of 3.1 m. The COT is made up of eight
subdetectors called “superlayers”, as can be seen in Figure 3.10 . Four
superlayers have their sense wires parallel to the @xmath axis (axial
superlayers) and therefore can measure trajectories in the @xmath -
@xmath plane. The other four superlayers have their sense wires at a 2
\degree angle with respect to the @xmath axis (stereo superlayers),
which allows for measurements along the @xmath axis.

The 30,240 sense wires in the COT were divided approximately equally
between the axial and stereo superlayers. Particles originating from the
primary interaction vertex that have @xmath pass through all the eight
COT superlayers, but those with @xmath pass only through four
superlayers. The COT is very precise for measurements in the @xmath -
@xmath plane (transverse momentum, @xmath ), but less so for
measurements along the @xmath axis (longitudinal momentum, @xmath ).

The gas filling the drift cells that constitute the COT is formed by an
Argon-Ethane gas mixture and Isopropyl alcohol in the proportions
49.5:49.5:1. The motivation for this choice is that of having a constant
drift velocity along the cell width. This produces a drift velocity of
100 @xmath m/ns and a maximum drift time of 177 ns, which allows for all
the electric charges produced by ionization in the drift chamber by the
incoming particles to drift away before the next bunch crossing takes
places (every 396 ns). The electric field increases exponentially with
decreasing distance to the sense wire. This produces an avalanche of
electrons very close to the sense wire, which amplifies naturally the
measured current and allows for a better measurement.

Next the electric currents read by the sense wires are processed by an
ASDQ chip [ 47 ] . The ASDQ amplifies the signal, analyses its shape and
height and allows for the measurement of the deposited charge, which is
used to measure the ionization along the trail of the charged particle (
@xmath ), a quantity that is very helpful in discriminating between
different types of particles. Next the pulses are digitized by Time to
Digital (TDC) boards in the CDF collision hall.

Furthermore, pattern recognition algorithms (tracking algorithms)
reconstruct helical trajectories of particles in the COT. The position
resolution for a track is about 140 @xmath m and the track @xmath
resolution measured using cosmic rays varies with the track @xmath :

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

#### 3.3.4 Time of Flight

Another improvement of CDF during the upgrade to Tevatron Run II was the
addition of a new subdetector aimed at improving particle identification
at low momentum. A Time Of Flight (TOF) system [ 48 ] was built outside
the solenoid magnet of the tracking system at a radius of approximately
1.5 m. The TOF measures the time interval particles travel from the
primary vertex of @xmath collisions to the TOF system. This allows the
separation of low momentum protons, kaons and pions.

The TOF system consists of 216 bars of scintillating material
approximately 3 m in length and with a cross section of @xmath . These
bars are arranged in a cylindrical geometry around the tracking system.
The TOF coverage in @xmath is @xmath . The scintillating material was
chosen to be Zircon 408, because it has a short rise time and a long
attenuation length (308 cm).

The particle separation principle is the following. All particles are
produced at the same time from the same primary interaction. As they
pass through the scintillating material, they deposit small flashes of
visible light that are detected by photomultiplier tubes (PMT) attached
to both ends of each scintillating bar. Next a preamplifier circuit
mounted directly on the PMT processes the light signal. Then the readout
electronics digitizes both the amplitude and time for the light signal.
The time interval is digitized by a Time to Digital Converter (TDC) only
when the signal reaches a fixed discrimination threshold. Moreover, the
TDC corrects for the effect that a larger amplitude signal reaches the
threshold first (time walk effect). If a particle crosses the
scintillating bar just in front of a PMT, the time resolution is about
110 ps. If the particle crosses the scintillating bar farther away from
a PMT, the resolution worsens [ 49 ] .

At its best performance (110 ps resolution) the TOF can distinguish
charged pions and kaons with momenta @xmath with a precision of two
standard deviations. This information is complementary to the particle
separation due to the @xmath effect measured by the COT. Figure 3.11
shows the effect of TOF and COT superimposed in particle separation.

#### 3.3.5 The Calorimeters

Besides the trajectory and momentum of a given particle, one needs also
to determine its energy in order to reconstruct an event fully. This
role is played by the calorimeter systems.

The calorimeter detectors in CDF II are “sampling” calorimeters. They
have a sandwich structure, where dense absorbing material alternates
with scintillating material. As particles pass through the absorbing
material, they develop showers of secondary particles. These secondary
particles emit light as they pass through the scintillating material.
The light is detected and measured by photomultiplier tubes (PMT). By
adding up the energies measured by all the PMT in the calorimeter
systems, the energy of the initial particle is measured.

Each calorimeter is made of two parts. The innermost part is an
electromagnetic calorimeter (EMCAL), which measures the energies of
electromagnetic objects (electrons, positrons and photons). EMCAL has a
large number of radiation lengths @xmath and a small number of
interaction lengths. The outermost part is the hadronic calorimeter
(HADCAL), which measures the energy of hadrons, such as pions, mesons,
@xmath hadrons. HADCAL has a large number of interaction lengths.
Electromagnetic particles start to shower immediately as they enter the
calorimeter and most of their shower is contained in the EMCAL. On the
other hand, hadronic objects deposit very little energy in the EMCAL and
most of the energy in the HADCAL. This structure for the calorimeters
allows to distinguish between the electromagnetic objects and hadrons.

Each calorimeter has a projective geometry, meaning that it is divided
in @xmath and @xmath towers that point towards the interaction region.
That way a particle entering a calorimeter tower tends to spend most of
its trajectory in that same tower. The calorimeter systems have a @xmath
coverage in @xmath and @xmath in pseudorapidity.

The calorimeter system is made up of three parts spanning different
geometric regions: the central calorimeter in the barrel region (both
electromagnetic, CEM, and hadronic, CHA), the plug calorimeter in the
forward region (both electromagnetic, PEM, and hadronic, PHA) and the
end wall calorimeter in between the central and plug calorimeters (only
hadronic, WHA).

In Figure 3.12 we can see a schematic view of the calorimeter detectors
in one of the CDF barrels.

##### Central Electromagnetic Calorimeter (CEM)

The CEM [ 51 ] covers the region @xmath . The CEM has 24 towers in
@xmath and 10 towers in @xmath . It is a sampling calorimeter with lead
as the absorbing material alternated with scintillating material. It has
18 radiation lengths ( @xmath ). The energy resolution of the CEM is

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

where the notation @xmath represents the sum in quadrature of the
constant and stochastic terms.

In Figure 3.13 we can see a schematic view of a single wedge from the
CEM calorimeter.

##### Central Electromagnetic Shower Maximum Detector (CES)

The goal of CES [ 51 ] is to improve the position measurement of
electromagnetic showers in the CEM. To achieve this goal, CES is a
proportional chamber with wire and strip readout located inside the CEM
at the position where on average the showers created by electrons,
positrons and photons have a maximum number of particles (at a depth of
6 @xmath ) in the CEM. This is also called a shower maximum and it is
reflected in the name of the detector. For an electromagnetic object
with an energy of 50 GeV, the position resolution achieved by the CES is
0.2 cm.

##### Plug Electromagnetic Calorimeter (PEM)

The PEM covers the region @xmath and PEM is formed by 24 (48) towers in
@xmath for the inner (outer) groups and 12 towers in @xmath for all
groups. As for the CEM, it is also a lead-scintillator sampling
calorimeter. Its thickness is slightly larger (21 @xmath ). The energy
resolution of the PEM is

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

##### Plug Electromagnetic Shower Maximum Detector (PES)

Similar to CEM, PES [ 52 ] measures precisely the position of the
maximum of electromagnetic showers in the PEM. Also as for the CEM, the
PES is located at a depth of 6 @xmath radiation lengths inside the PEM.
The PES is made of two layers of 5 mm wide scintillator strips and each
layer is at a 45 \degree angle relative to the other.

The properties of the electromagnetic calorimeters at CDF are summarized
in Table 3.3 .

##### Central Hadronic Calorimeter (CHA)

The CHA [ 53 ] covers the region @xmath and the CHA uses iron as a
showering material that alternates with scintillator material. The CHA
is segmented in 24 towers in @xmath and 8 towers in @xmath . The CHA is
located radially just outward of CEM. Each CHA tower is made of 32
layers, with a total of 4.7 interaction lengths ( @xmath ). The energy
resolution of CHA is

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

##### Plug Hadronic Calorimeter (PHA)

The PHA [ 54 ] covers the region @xmath . It is constituted from 23
layers of sandwiched iron (as absorbing material) and scintillating
material. The energy resolution for the PHA is:

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

In Figure 3.14 we can see a schematic view of one of the two forward
(plug) calorimeters (PEM and PHA).

##### Wall Hadronic Calorimeter (WHA)

The WHA [ 53 ] covers the region @xmath . It extends the @xmath coverage
between the central and plug hadronic calorimeters. It is also an
iron-scintillator sampling calorimeter. There are 15 5.0 cm thick iron
layers alternating with 1.0 cm thick scintillator. The energy resolution
of WHA is

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

#### 3.3.6 The Muon Chambers

Although nearly all particles are absorbed by the calorimeter system,
muons pass through the calorimeters as minimum ionizing particles and
then exit the calorimeter system ¹⁰ ¹⁰ 10 Neutrinos are the only known
particles that leave the detector completely undetected. . The outermost
subdetector of CDF is a muon system. It is made out of single wire drift
chambers and scintillator counters for fast timing, located radially
just outside the calorimeter system.

There are various muon subsystems as follows: the Central Muon Detector
(CMU), the Central Muon uPgrade Detector (CMP), the Central Scintillator
uPgrade (CSP), the Central Muon eXtension Detector (CMX), the Central
Scintillator eXtension (CSX), the Toroid Scintillator Upgrade (TSU), the
Barrel Muon Upgrade (BMU) and the Barrel Scintillator Upgrade (BSU).

The CMU, CMP and CSP systems cover an @xmath range of @xmath , the CMX
and CSX systems cover an @xmath range of @xmath and the TSU, BMU and BSU
subsystems cover an @xmath range of @xmath . The muon subsystems can be
seen in Figure 3.15 .

The innermost muon system is CMU [ 55 ] . It was built for CDF I and is
located just outside the CHA calorimeter, at a radius of 350 cm and
arranged in 12.6 \degree wedges in @xmath . Each wedge has three layers
(stacks) and each stack has four rectangular drift chambers. Each drift
chamber is filled with the same gaseous composition as the COT
(argon-ethane-alcohol 49.5:49.5:1) and has a 50 @xmath m sense wire in
the middle of the cell, parallel to the @xmath axis.

The CMU is followed by another muon system, the CMP [ 56 ] . The CMP has
rectangular geometry and is formed of four layers of drift chambers of
the same constitution as above. In preparation for Tevatron Run II, a 60
cm thick layer of steel was added. The @xmath threshold for the CMU
(CMP) is 1.4 (2.2) @xmath .

On the outer side of CMP there is the CSP [ 56 ] , formed of a single
rectangular layer of scintillating material with a wave guide to
transport the light to a PMT. The CSP fast response is used in
triggering.

The CMX muon system is located at each edge between the CDF barrel and
forward regions. It has a conical geometry with drift chambers similar
to the CMP. Also, it has a scintillating system called the CSX, similar
to the CSP. The CMX system covers 360 \degree with 15 wedges in @xmath .
Each wedge is formed of eight layers of drift chambers in the radial
direction.

Various properties of the muon subsystems are summarized in Table 3.4 .

Based on the timing information provided by the individual drift
chambers, short “tracks” of ionization in the drift chambers (called
“stubs”) are reconstructed in the muon chambers. At CDF a muon candidate
is formed when a track reconstructed by the COT is matched to a stub in
the muon system. In the reconstruction process, a @xmath in the @xmath
coordinate ( @xmath ) is computed for the match between a COT track and
a muon stub. To ensure good quality of muon candidates, an upper limit
cut is set on @xmath .

#### 3.3.7 The Trigger System and Data Acquisition

The trigger system is indispensable at each collider physics experiment
because the collision rate is much larger than the maximum rate of data
events stored for analysis. At the Tevatron the theoretical bunch
crossing (collision) rate is 2.5 MHz for 36 bunches of protons and 36
bunches of antiprotons. In practice, there are 1.7 MHz @xmath collisions
per second. Even so, this rate is a lot larger than the 50 Hz rate to
save data events ¹¹ ¹¹ 11 In CDF, an event is defined as all the
recorded particle activity in the detector during a bunch crossing. On
average, more than one @xmath hard scattering happens during an event,
with multiple primary vertices present in the event (pile-up). to
magnetic tape ¹² ¹² 12 One might be surprised that in the era of CDs,
DVDs and USB storage keys, collision data is saved on old fashioned
magnetic tapes. The motivation comes from the fact that the latter
perserve the data for more years and with lower costs than the former. .
Also, on average a data event needs about 250 kB of data storage. Even
if the rate to copy these to magnetic tape would be sufficient, there
would not be enough storage space to save all these events. Nor would it
be easy to analyze all these events afterwards.

There are three essential conditions that the CDF trigger system has to
meet. First, the trigger has to be quick enough to make a decision about
an event (whether to save it or reject it) before the next event comes
in (in other words, there should be zero dead time). Second, the
Tevatron collider imposes that a new event comes in every 396 ns. Third,
the magnetic tape can only save about 50 events per second.

In conclusion, the goal of the CDF trigger system is to analyze, every
second, 1.7 million events and decide also every second which 50 of
these events will be saved to tape.

Figure 3.16 represents a diagram of the three trigger levels in the CDF
trigger system.

##### CDF Trigger Levels

There are three trigger levels at CDF, and each needs a certain amount
of time to reach a decision whether to reject an event or send it to the
next trigger level.

The first trigger level (L1) uses hardware-based custom electronics to
try to reconstruct physics objects using only a subset of CDF
information. L1 makes a decision by object count and energy values. The
second trigger level (L2) also uses custom designed hardware to
reconstruct better physics objects. This information is passed to
programmable processors to make decisions. The third trigger level (L3)
uses all CDF information and a PC farm of about 500 CPUs to do a full
event reconstruction and decide if the event is kept.

##### Trigger Paths

A trigger path represents a sequence of requirements that an event has
to pass at L1, L2 and then at L3. About 100 trigger paths are
implemented by the CDF II trigger system. An event will be saved to tape
if it passes the requirements of at least one trigger path.

##### L1 Trigger Level

L1 reduces the event rate from 1.7 MHz to about 40 KHz, thus discarding
about 98% of events. In order to have enough time to analyze each event,
L1 uses a pipeline and 42 buffers so that L1 has 2 @xmath s to analyze
each event.

The input for L1 comes from the tracking, calorimeter and muon systems.
The decision to send an event to L2 is taken using the number and energy
values of electron, muon and jet candidates, as well as the value of
missing transverse energy ¹³ ¹³ 13 As neutrinos escape the detector
without being detected, the energy they carry appears as missing energy
in the event. Since the partons in protons or antiprotons have a well
measured energy in the transverse plane but a distribution of energy
along the @xmath axis, we can only say that the vector sum of energies
should be zero in the transverse plane, but we are not able to say the
same for the @xmath axis. This is why we refer only to missing
transverse energy. Also note that this happens at all hadron colliders,
LHC included, and that at lepton colliders, such as the PEP-II collider
(for the BABAR experiment) and the KEKB collider (for the Belle
experiment), the energy is well measured in all directions and they can
refer to missing energy, not only missing transverse energy. , or the
kinematic properties of a pair of tracks.

A subsystem called eXtremly Fast Tracker (XFT) [ 57 ] reconstructs at L1
high- @xmath tracks ( @xmath ) using COT information. XFT uses a
digitized readout of ionization (hits) in the COT. The hits from COT’s
superlayers are combined into segments. Pattern recognition algorithms
group segments into tracks that cross the entire COT. The tracks
reconstructed by the XFT are combined with information about energy
deposits in calorimeter (muon) systems to produce L1 electron (muon)
candidates.

Energies of jets, electron and photon candidates, as well as missing
transverse energy and sums of jet energies in an event are approximated
using clusters of energy in the calorimeter systems.

The only muon system used at L1 is CMU.

##### L2 Trigger Level

Events accepted by L1 are sent to L2, where the event rate is reduced
from 40 KHz to about 500, thus discarding about 99% of events passed to
L2. L2 uses 4 buffers in order to have enough time to analyze each event
(about 20 @xmath s).

At L2 a better event reconstruction is performed. The tracking is
improved by taking into account the silicon tracking information as
well. Also, better track reconstruction and calorimeter clustering ¹⁴ ¹⁴
14 Jets are reconstructed at L2 with the help of L2 cluster finder
(L2CAL), which starts from a seed of 3 GeV calorimeter tower and adds
adjacent towers with energies larger than 1 GeV. for jet finding
algorithms are used. The reconstruction of electron and photon
candidates is improved by taking into account as well the information
from the central calorimeter shower maximum subdetector (CES) ¹⁵ ¹⁵ 15
This way the resolution for electron and photon showers is better than
the cluster location. This information is combined with the tracking
information to reconstruct better electron candidates. . Also, secondary
vertices ¹⁶ ¹⁶ 16 Jets originating from a @xmath quark contain a
secondary vertex displaced by about 3 mm from the primary @xmath
interaction vertex due to the fact that @xmath quarks live longer than
other quarks before decaying. The decay products appear emerging from
the same vertex. This is called secondary vertex. are reconstructed at
L2 inside certain jets using the Silicon Vertex Tracker (SVT) trigger
subsystem [ 58 ] ¹⁷ ¹⁷ 17 The information from SVXII subdetector is
combined with the L1 XFT reconstructed track to reconstruct both a more
precise track and reconstruct a secondary vertex inside a jet that
originates from a @xmath quark. . Also, all muon systems are used to
combine hits in the muon chambers with the L1 XFT reconstructed track to
produce L2 muon candidates.

Figure 3.17 represents a block diagram of the L1 and L2 trigger levels
at CDF.

##### L3 Trigger Level

If an event is accepted at L2, it is sent to L3, where a full detector
readout is done and a full reconstruction is done using the computer
farm. If an event passes L3 requirements then it is sent to the Consumer
Server/Logger (CSL) that is the final component of the CDF data
acquisition.

##### Consumer Server/Logger

The CSL categorizes events by trigger path, writes to disk those that
pass at least one of the trigger paths and sends a fraction of these
events to online processors for online monitoring of data quality.
Figure 3.18 represents a diagram of Consumer server/Logger.

As a graduate student on CDF, I did three one-week online data quality
monitoring shifts ¹⁸ ¹⁸ 18 At CDF these shifts are called “Consumer
Operator shifts“. and one three-month online data acquisition and
detector control shift ¹⁹ ¹⁹ 19 At CDF these shifts are called “ACE
shifts“. .

### 3.4 Summary

In this chapter we have presented the experimental infrastructure used
for the @xmath associated production direct search presented in this
thesis. We started by introducing the US national particle physics
laboratory, Fermilab. We then presented in detail the Fermilab
accelerator complex that accelerates and collides protons and
antiprotons at a centre-of-mass energy of @xmath with the help of a
chain of particle accelerators formed by the proton source, the
Cockwroft-Walton, Linac, Booster, Debuncher, Recycler, Main Injector and
the Tevatron accelerator. We continued with the detailed description of
the Collider Detector at Fermilab, the apparatus that records the
elementary particles produced in the proton-antiprotons collisions
delivered by the Tevatron accelerator. We first introduced the CDF
coordinate system and the Cherenkov Luminosity Counter. We then
described the several subdetectors structured as layers of CDF which
measure several properties of the elementary particles. The first layer
is formed by the tracking system, which measures precisely the momenta
of elementary particles. The second layer is the calorimeter system,
which measures precisely the energy of elementary particles. The third
layer is dedicated to the muons. From a total of 2.5 million collisions
per second, only about 100 collisions per second are chosen to be stored
by the trigger system, which is deployed by way of three levels.

In the next chapter we will present how the several detector systems are
used in order to reconstruct the elementary particles used in our @xmath
analysis.

## Chapter 4 Object Identification

The energy deposits in the CDF subdetectors are digitized and
transformed into electronic signals that are then reconstructed to
high-level objects such as primary vertices, tracks, calorimeter
clusters and muon stubs. Physics objects such as electrons, muons, jet
candidates and missing transverse energy are reconstructed by applying
cuts, or selection criteria, on high level objects. In this analysis we
present a @xmath search. The final state contains therefore a charged
lepton (an electron or a muon ¹ ¹ 1 We do not consider the tau lepton
final state directly. However, tau leptons that decay leptonically to
electrons or muons plus neutrinos contribute to our signal, background
and data selection. ), missing transverse energy due to the undetected
neutrino, and two jets originating from bottom ( @xmath ) quarks.

A primary vertex represents the reconstructed position of the primary
@xmath collision that produced primary particles. In the case of our
signal, these are the @xmath boson and the Higgs boson.

A track represents the reconstructed trajectory of a charged particle in
the tracking systems from the particle’s electromagnetic interactions in
these detector systems.

A calorimeter cluster is a collection of adjacent calorimeter towers
where energy is deposited due to an incoming particle, either
electrically charged or electrically neutral.

A muon stub is a collection of energy deposits in adjacent muon chambers
in the muon systems.

This chapter presents the reconstruction techniques for the basic
physics objects used in this analysis, such as tracks, primary vertices
and calorimeter clusters. Next comes the reconstruction of high-level
physics objects such as electron, muon and jet candidates, as well as
missing transverse energy, which are used by the event selection.
Finally, two algorithms used to identify jets originating from @xmath
quarks are described.

### 4.1 Track Identification

Tracks are reconstructed at CDF using information from the tracking
systems. They are used in primary vertex reconstruction, charged lepton
identification and identification of jets originating from @xmath
quarks. A track represents a reconstructed three dimensional helical
trajectory of a charged particle passing through the solenoidal magnetic
field in the tracking systems and is described by the following
parameters.

The half-curvature of the trajectory ( @xmath ) is defined as

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

where @xmath is the electric charge of the track, and @xmath is the
radius of the circle formed by the projection of the helical trajectory
on the transverse plane. @xmath carries the same sign as the electrical
charge of the particle and is inversely proportional to the transverse
momentum ( @xmath ) of the track.

@xmath is the cotangent of the polar angle of the trajectory at the
closest approach to the primary interaction vertex.

The impact parameter ( @xmath ) is the minimal distance in the
transverse plane between the trajectory and primary interaction vertex.
It is given by the expression

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

where @xmath and @xmath are the coordinates in the transverse plane of
the centre of the circle of the helix.

@xmath is the azimuthal angle of the trajectory at the closest approach
to the primary interaction vertex.

@xmath is the @xmath coordinate position of the trajectory at the
closest approach to the primary interaction vertex.

#### 4.1.1 Tracking Algorithms

There are three tracking algorithms that we use in this analysis: COT
stand-alone tracking, Outside-In (OI) tracking and Inside-Out (IO)
tracking.

##### COT stand-alone tracking

The COT stand-alone tracking algorithm uses only COT information (with
no information from the silicon detectors). Electromagnetic interactions
inside the COT cells or silicon detectors are called hits. First, hits
in each superlayer are fit together to reconstruct short tracks. Then,
short tracks from all the superlayers are fit together to form a COT
stand-alone track. The details of this latter fit are the following.
Since axial and stereo superlayers alternate, first a fit is performed
where only the axial superlayers are considered in the order from the
most outer one to the most inner one. Then, stereo layers are added and
a new fit is performed. The final COT stand-alone track needs to have
hits in at least 2 axial and 2 stereo superlayers. This tracking
algorithm is used in the central region of the detector, corresponding
to @xmath .

##### Outside-In tracking

The Outside-In (OI) tracking algorithm starts with a COT stand-alone
track (called a seed track) and adds high-resolution hits from silicon
detector information. First the axial silicon hits are added to the COT
stand-alone track. Then, for each silicon wafer, every hit on a stereo
silicon strip is added to a different copy of the current track. After
the last silicon wafer has been processed, there are a multitude of
track candidates. The Outside-In track is chosen to be that with the
largest number of silicon hits and with the lowest @xmath over the
number of degrees of freedom. This tracking algorithm is used in the
central region of the detector ( @xmath ).

##### Inside-Out tracking

The Inside-Out (IO) tracking algorithm is needed for tracking in the
forward regions of CDF. The algorithm starts with hits in at least three
layers of the silicon detector. Then hits in the COT that have not
already been used by the COT stand-alone and OI algorithms are added to
produce the final IO track.

### 4.2 Primary Vertex Identification

CDF uses two main algorithms to reconstruct primary interaction vertices
(PV). The locus of all PVs represents the beamline, or the luminous
region of the detector.

#### 4.2.1 Primary Vertex Reconstruction Algorithms

The ZVertexFinder algorithm [ 59 ] takes as an input a set of tracks
passing minimum quality requirements based on the number of silicon and
COT hits. The algorithm computes an error weighted average ( @xmath ) of
@xmath coordinates of these tracks, which is given by

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

The algorithm outputs a collection of PVs characterized by their own
quality, track multiplicity, @xmath position, @xmath position error and
@xmath . However, PVs output by the ZVertexFinder algorithm present no
@xmath and @xmath position information. Each reconstructed PV
corresponds either to a hard scattering or to an underlying event of a
hard scattering. It may also happen that a physical PV gets
reconstructed into two PVs due to tracking resolution. The PV transverse
momentum @xmath is defined as the sum of the transverse momenta of its
tracks @xmath and conveys the information of how energetic a PV is.
Typical @xmath PV candidates have @xmath on the order of 100 GeV. The PV
quality conveys the information of how well the PVs are reconstructed.
PV quality is based on the track multiplicity, as shown in Table 4.1 .

The PV with the best chances to be the PV of the interaction triggered
on is considered the event PV. The CDF collaboration used to use a
run-averaged beamline position as an event PV. CDF developed in 2003 an
algorithm called the PrimeVertexFinder [ 60 ] [ 61 ] that reconstructs a
3D event PV on an event-by-event basis. This algorithm allowed CDF to
improve the efficiency of identifying jets as originating from a bottom
quark (b-tagging) for shorter secondary vertex displacements and to
reduce the systematic uncertainties due to the run-dependent beam
position variation. PrimeVertexFinder takes as an input a set of good
quality tracks in good agreement ( @xmath ) with a seed vertex (usually
the beamline position or one of the PVs output by ZVertexFinder). These
tracks are reconstructed to a new 3D PV and checked if they are still in
good agreement with the new PV. Tracks with @xmath are rejected. The
remaining tracks are reconstructed to a new 3D PV. The procedure is
iterated until all remaining tracks have a @xmath with respect to the
latest PV. The last 3D PV becomes the event PV. 3D position information
is crucial for b-tagging techniques that use information about the
bottom quark lifetime.

A PV position is represented by @xmath . A typical longitudinal width is
@xmath cm. A typical transverse width is circular, smaller at the centre
of the detector, @xmath and larger at the extremities, @xmath . Typical
@xmath and @xmath are very small, on the order of tens of microns. Event
PV reconstruction is trusted only in the luminous region ( @xmath cm).
Events with the event PV outside the luminous region are rejected
(luminous cut).

#### 4.2.2 Primary Vertex Definition Studies

In my Master of Science thesis [ 30 ] I compared two possible
definitions for the primary interaction vertex for high- @xmath events
with the charged lepton+missing transverse energy+jets signature (top
quark pair production) at CDF. The analysis presented in this PhD thesis
also uses a signature of a charge lepton + missing transverse energy +
jets (the @xmath search). The only difference is that there are two jets
in our case and there are four jets in that for @xmath . The two
possible primary vertex definitions were:

-   The primary vertex with the closest @xmath coordinate to the @xmath
    coordinate of the charged lepton (electron or muon) reconstructed
    track.

-   The primary vertex with the largest transverse momentum of all the
    primary vertices with the @xmath coordinate within 5 cm of the
    @xmath coordinate of the charged lepton (electron or muon)
    reconstructed track.

The study concluded that both definitions are equally efficient and
therefore confirmed that the definition used by CDF was the best
possible one.

#### 4.2.3 Primary Vertex Reconstruction at L1 Trigger Level Study

During my first year of PhD studies I performed a study within the Higgs
Trigger Task Force (HTTF) [ 62 ] at CDF. The goal of HTTF was to design
new triggers to increase the acceptance of Higgs boson events. As part
of that effort, I performed a study to evaluate if the triggers would
benefit from the ability to know at L1 Trigger Level if the primary
interaction vertex was in the east or west side of CDF. If the answer
were yes, then a hardware based hit count would have been implemented to
evaluate in which half of CDF more particle activity was recorded.
However, the study showed that this information would not have changed
the trigger efficiency significantly and therefore primary vertex
identification was not introduced at L1. However, other efforts were
proven to be worthwhile. For example, a new missing transverse energy +
jets trigger was designed, and we currently use this in our analysis.

### 4.3 Calorimeter Clustering Algorithm

High transverse momentum electrons, photons and jets leave energy
deposits in the calorimeter systems in adjacent calorimeter towers.
Together they form a calorimeter cluster, which is reconstructed using a
clustering algorithm. The first step is finding a seed cluster that has
an energy deposit larger than a certain threshold value. Then
neighbouring clusters with energy deposits larger than another lower
threshold are also added to the cluster. The total energy of the cluster
is the sum of the energy deposits in each calorimeter tower. The
position of the cluster is computed as an energy-weighted average of the
positions of each tower in the cluster. Then, in order to improve the
precision for the cluster position, the calorimeter cluster is matched
with a cluster in the shower maximum detector. The latter type of
cluster is built with a similar algorithm, but one that is optimized to
achieve a better position resolution.

### 4.4 Charged Lepton Identification

In this section we will discuss charged lepton identification. Except
when otherwise mentioned, a process described for a particle is also
true for its antiparticle. In this analysis we use electron, muon and
isolated track candidates.

#### 4.4.1 Electron Identification

An electron typically deposits most of its energy in the electromagnetic
calorimeters. The basic selection for an electron candidate is a high
@xmath track, isolated from other activity in the tracking systems,
which is matched to an electromagnetic calorimeter cluster. The
isolation requirement ensures that the charged lepton candidate
originates in the primary interaction vertex and therefore is the
daughter particle of the @xmath boson, and does not originate in a
@xmath hadron semi-leptonic decay, as in the case of a jet originating
in a @xmath quark. Further selection criteria summarized in Table 4.2
are required to reconstruct the tight central electron candidates in the
region @xmath (CEM calorimeter) and therefore noted in this thesis as
CEM.

Here is the explanation of the notations from Table 4.2 :

-   The transverse energy of the calorimeter cluster is @xmath . The
    trigger requires @xmath , but the analysis requires @xmath to make
    sure that the trigger is fully efficient.

-   @xmath is the ratio between the energy deposited in the hadronic
    calorimeters (CHA, WHA or PHA) and the energy deposited in the
    electromagnetic calorimeters (CEM). This ratio is very small for
    electron candidates, on the order of 5%.

-   Isolation is defined as the ratio between energy deposited in all
    the additional towers located in a cone of radius @xmath around the
    calorimeter cluster and the energy of the calorimeter cluster
    itself. Isolation is required to be smaller than 0.1, which means
    that the calorimeter cluster is required to be isolated from other
    significant energy deposits in the calorimeter.

-   Track @xmath is the @xmath coordinate position where the isolated
    track intersects the beamline.

-   Track @xmath is the transverse momentum of the electron candidate
    that is explicitly measured by the track curvature.

-   COT Axial (Stereo) Segments is the number of axial (stereo) COT
    layers that have at least 5 hits each associated with the track.

-   @xmath is a quantity that measures how well the theoretical electron
    shower profile matches the distribution of energy in the calorimeter
    cluster.

-   E/p is the ratio between the energy of the calorimeter cluster and
    the track momentum.

-   @xmath is the @xmath of the fit of the shower-maximum profile
    measured by the shower maximum detector (CES or PES) with respect to
    the electron test beam data.

-   @xmath is the signed difference in @xmath coordinate between the
    track and the calorimeter cluster when the track is extrapolated to
    the position of the shower maximum.

-   Q is the measured electric charge of the electron candidate
    (negative for electron and positive for positron).

-   @xmath is the absolute value in the @xmath coordinate between the
    position of the calorimeter cluster and the position of the track
    that is extrapolated to the position of the shower maximum.

#### 4.4.2 Muon Identification

A muon behaves like a minimum ionizing particle due to its rest mass,
which is about 200 times larger than that of the electron [ 4 ] .
Therefore muons deposit very little energy in the calorimeter systems.
The outer layer of the CDF detector is instrumented by muon chambers.
Various types of muon candidates are reconstructed that bear the name of
the muon detector that records them. Ionization deposits from a muon
candidate in a given muon detector constitute a “stub”.

The basic selection for a muon candidate is a well reconstructed high-
@xmath track isolated from other activity in the detector that is
matched to a muon stub. Other selection cuts required to refine the
selection above are summarized in Table 4.3 for two types of tight muon
candidates (CMUP and CMX).

CMUP muon candidates are reconstructed in the region @xmath and a track
is required to match stubs in both CMU and CMP muon detectors. Some very
energetic hadrons are able to deposit some of their energy outside the
calorimeter systems, in the muon detectors, and therefore fake the muon
signature. The “fake muon” fraction in the collected sample is decreased
considerably by requiring both CMU and CMP stubs.

CMX muon candidates are reconstructed in the region @xmath and require a
stub in the CMX muon detector.

#### 4.4.3 Isolated Track Identification

Our original contribution to the @xmath analysis is the introduction of
a new charged lepton reconstruction method based on a high- @xmath track
isolated from energy deposits in the tracking systems and with no
requirements about energy deposits in the calorimeter or muon detectors.
We call these “isolated tracks”. The isolation requirement is necessary
in order to ensure that the track corresponds to a charged lepton
produced in a decay of a @xmath boson and not part of a jet of hadrons
that originate in quarks. The fact that the isolated track is not
required to match a calorimeter cluster or a muon stub allows to recover
real charged leptons that arrive in non-instrumented regions of the
calorimeter or muon detectors, as seen in Figure 7.1 . Thus, the signal
acceptance of the @xmath search is increased and this has the potential
to increase the @xmath search sensitivity.

The first analysis that used isolated tracks at CDF [ 63 ] was a top
quark cross section measurement in top quark pair production where each
top quark decays to one @xmath boson and a @xmath quark and where each
@xmath boson decays leptonically to a charged lepton and a corresponding
neutrino, using an integrated luminosity of @xmath . In this analysis
one charged lepton was either an electron or a muon candidate and the
second charged lepton was an isolated track candidate. The events used
in this analysis were selected using an electron-inclusive trigger or a
muon-inclusive trigger.

In our analysis we have exactly one charged lepton candidate and in this
channel the charged lepton is an isolated track candidate. However, we
do not have an isolated track trigger at CDF. This is why we use three
triggers with requirements on MET and jets, but not on charged leptons.
One of my original contributions consisted in parameterizing the trigger
efficiency turnon curves and measuring the systematic uncertainty of
this procedure.

My work contributed directly to the neural network @xmath searches that
used one (two) MET-based triggers in @xmath ( @xmath ), as described in
the Ph.D. thesis [ 1 ] ( [ 3 ] ) and in their corresponding publications
and CDF and Tevatron combinations. As my work evolved in time, this
thesis presents for the first time the addition of a third MET-based
trigger and a novel method to combine an unlimited number of triggers
short of having an “OR” between triggers, as described in detail in
Subsection 6.1.4 .

In order to reconstruct isolated tracks, we select on an event by event
basis a set of good quality tracks with criteria that meet the
requirements detailed in Table 4.4 .

Next, for each good quality track we define and compute a quantity
called “track isolation” as

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

where @xmath is the transverse momentum of the specific track we analyze
(candidate) and @xmath is the sum of the transverse momenta of all good
quality tracks within a cone radius of 0.4 of the candidate track. Given
this definition, a track is fully isolated if it has a track isolation
of 1.0. However, very seldom a track is fully isolated in a hadronic
collision. In this analysis we consider a track to be isolated if it has
an isolation larger than 0.9, which means that at least 90% of the
@xmath in the vicinity of the track corresponds to the track itself.

From the sample of good quality tracks with isolation larger than 0.9 we
select the sample of “isolated track” candidates by requiring further
tighter track reconstruction criteria, as described Table 4.5 .

The purity for isolated tracks is about 80%, whereas the purity for
TIGHT charged lepton candidates is about 90%, as shown in Figure 8.1 .

#### 4.4.4 Charged Lepton Reconstruction Scale Factor

Our detector is not as efficient to reconstruct charged lepton
candidates in Monte-Carlo-simulated events as in real data events. For
this reason we correct each simulated event by a scale factor for its
specific charged lepton reconstructed category, which is defined as the
ratio between the reconstruction efficiencies in data and simulated
events.

##### Scale Factor Measurement for Isolated Tracks

We studied @xmath Monte Carlo simulated events and concluded that ISOTRK
charged lepton candidates are muon candidates in 85% of cases, electron
candidates in 7% of cases and tau lepton candidates in 8% of cases [ 1 ]
. We measure the scale factor for muon ISOTRK charged leptons and
correct the systematic uncertainty on that value for the fact that in
15% of the cases ISOTRK events are not muon candidates.

We select a sample of events where a @xmath boson decays to a
muon-antimuon pair and use the generic method called “tag and probe”,
which is also used to measure the scale factor for the reconstruction of
CMUP muons in Reference [ 64 ] . We select events with a well
reconstructed tight muon (CMUP or CMX), which is considered the tag leg,
and a high- @xmath track isolated from other activity in the detector,
which is called the probe leg. We ask further selection requirements to
improve the purity of the @xmath boson sample: the invariant mass of the
tag and prob legs should be in the @xmath boson mass window ( @xmath );
the absolute value of the @xmath position between the two candidates has
to be smaller than 4 cm; the legs must have opposite electric charges;
for data events, the tag charged lepton must fire the CMUP or the CMX
muon trigger; the event must pass the cosmic veto to ensure it is not
produced by cosmic rays ² ² 2 Cosmic rays that reach the CDF detector
situated 10 meters underground and shielded with thick concrete walls
are mostly muons. Such a muon from cosmic rays is typically very
energetic and therefore its trajectory is not curved much by the
solenoid magnetic field. Thus, it will produce an almost straight track
in the tracking systems. On the other hand, the software reconstruction
is looking for tracks starting from the centre of the detector and will
reconstruct this one muon as two back-to-back muon candidates. ; the
probe charged lepton must have @xmath and be matched to a muon stub.
After these cuts, we have a very pure @xmath boson sample and we are
confident that also the probe muon candidate is a real muon. We measure
the efficiency that this probe muon candidate is indeed reconstructed as
an ISOTRK charged lepton. We divide the efficiencies of data and Monte
Carlo simulated event to obtain the ISOTRK reconstruction scale factor
for each jet bin. For events with exactly two tight jets, as in our main
analysis, we obtain the average value of @xmath . Figures 4.1 , 4.2 and
4.3 show the simulated and data efficiencies and scale factor as a
function of the ISOTRK @xmath , @xmath and @xmath .

However, we do not quote a systematic uncertainty of 1%. We take into
account that in 15% of cases the ISOTRK charged lepton is either an
electron or a tau lepton candidate. We assign a 25% uncertainty for
these cases [ 1 ] [ 3 ] . The total ISOTRK scale factor systematic
uncertainty is computed as a weighted average, namely @xmath .
Therefore, the ISOTRK scale factor for our analysis is @xmath .

The code for this procedure was written by a postdoctoral researcher
(Nils Krumnack). For the past two years, I maintained the code after he
left the collaboration and I used it to measure the ISOTRK scale factors
for the @xmath analyses of 2009, 2010 and 2011.

##### Scale Factor Measurement for Tight Charged Leptons

We use a very similar method to measure the scale factor for the
reconstruction of tight muon candidates, CMUP and CMX. For the dataset
of 5.7 @xmath used in this analysis, we measure for CMUP candidates a
scale factor of @xmath and for CMX candidates a scale factor of @xmath .

A similar method with the exception that a @xmath boson sample decaying
to electron pairs is selected is used to measure the scale factor for
the CEM tight electron to be @xmath .

### 4.5 Missing Transverse Energy Identification

Neutrinos are the only subatomic particles that leave the detector
completely undetected. Their momentum and energy appears to be missing.
Since the longitudinal energies of the colliding partons are unknown and
not necessarily equal, we can only say that the total transverse energy
of the @xmath collision is zero. Therefore we observe a missing
transverse energy (MET or @xmath ) due to the neutrino.

The vector missing transverse energy ( @xmath ) is the opposite of the
vector sum of all energy deposits in calorimeter towers, with the @xmath
position of the neutrino closest to the beamline considered to be the
@xmath vertex position. The missing transverse energy is the absolute
value of the vector missing transverse energy ( @xmath ). From its
definition we see that @xmath is formed by missing transverse energy
from all undetected particles, such as one or more neutrinos, but also
subatomic particles predicted by theories beyond the Standard Model,
such as the lightest supersymmetric particle or a signature of particles
travelling in extra spatial dimensions. In our analysis, real @xmath is
produced by only one neutrino from the @xmath boson decay.

Real missing transverse energy can also be faked by mismeasured energy
deposits in the calorimeter, especially due to jets. Typically jet
energies are underestimated, as seen in the next section, which
translates into an overestimation of the @xmath . Events that produce
jets in pure QCD processes tend to have fake @xmath as we will see in
the background chapter.

At trigger level, @xmath is computed using only calorimeter information
and assuming the primary interaction vertex is located in the centre of
the detector. As shown in subsection 4.2.3 ,I performed a study in the
context of the Higgs Trigger Task Force that showed that improving the
primary vertex position measurement at L1 trigger level by noting in
which side of the detector it is located does not improve significantly
the offline @xmath vertex position reconstruction. This study confirmed
this approach to be optimal at trigger level. However, the offline
reconstructed @xmath is corrected for the true @xmath vertex position,
for the corrected jet energies, and by subtracting the momenta of
minimum ionizing high- @xmath muons and adding back the transverse
energy of the calorimeter towers crossed by the muon. These corrections
can be summarized in the following equation:

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

### 4.6 Jet Identification

A jet consists of a collimated shower of secondary particles produced in
the hadronization of a quark or gluon produced in the primary @xmath
interaction. There are various possible algorithms to reconstruct jets
from calorimeter towers. The algorithm used in this analysis is called
JETCLU [ 65 ] . JETCLU is a cone algorithm that searches for towers with
energy deposits within a cone of radius @xmath in the @xmath - @xmath
plane around the seed cluster. If a charged lepton is reconstructed
within this cone, its energy is neglected in the calculation of the jet
energy.

A diagram of the jet production at CDF, passing from parton jet to
particle jet and then to calorimeter jet, is shown in Figure 4.4 .

The energy of the jet is corrected for various effects [ 66 ] :

-   Relative Energy Corrections take into account the fact that the
    detector is not uniform in @xmath , since the plug and central
    calorimeters have different geometries and because there are cracks
    between calorimeter towers. Central calorimeters are better
    calibrated than the plug ones. This is why plug calorimeters are
    corrected with respect to the central calorimeters using Pythia
    Monte Carlo (MC) simulated events and di-jet data events. In our
    analysis, both data and MC events are corrected with respect to
    @xmath to make sure there is a uniform jet energy response across
    the detector.

-   Multiple Interaction Corrections take into account the effect that
    typically there are more than one @xmath interactions in a bunch
    crossing. For an instantaneous luminosity on the order of @xmath the
    average number of primary interactions is three. This is why CDF
    uses algorithms to select the correct primary interaction vertex.
    But also the energy of the jets is corrected for the effect that
    energy deposits from particles produced in @xmath interactions other
    than the one that interests us happen in the same calorimeter
    cluster as the one for the selected jet. These corrections are
    derived from minimum bias data events and are parametrized as a
    function of the number of primary vertices in the bunch crossing
    (event).

-   Absolute Energy Corrections take into account the effect of the non
    linearities and the uninstrumented regions of the detector. These
    corrections map the hadron-level jet after its hadronization from a
    quark or gluon to the calorimeter level-jet.

-   Underlying Event Corrections subtract the energy deposited in the
    calorimeter towers by the underlying event. The underlying event is
    represented by soft energy depositions due to the spectator quarks
    and gluons in the protons and antiprotons.

-   Out-of-Cone Corrections add the energy deposited in calorimeter
    towers that have not been reconstructed by the JETCLU algorithm to
    be part of the jet.

The corrected transverse jet energy can be summarized by the following
equation:

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

where the correction factors are @xmath (the scale factor that makes the
jet energy measurement uniform for various @xmath , @xmath (the
correction factor for multiple @xmath interactions per bunch crossings),
@xmath (the absolute energy correction determined by matching the parton
energy to the jet energy), @xmath (the scale factor that corrects for
the underlying event) and @xmath (the out-of-cone correction for the
energy of the initial parton that is not reconstructed in the jet cone).

The corrections and their systematic uncertainty can be seen in Figure
4.5 . The transverse jet energy resolution [ 66 ] is given approximately
by

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

In our analysis we classify events by the number of tight jets. Tight
jets are jets with the following tight selection criteria: @xmath and
@xmath . Events can also have loose jets. Loose jets are jets with
looser selection criteria and they are exclusive to the tight jets.
Loose jets have @xmath and @xmath or @xmath and @xmath .

### 4.7 @xmath-jet Identification

In this analysis we search for an associated production of a @xmath
boson and a Higgs boson, where the latter decays to a pair of
bottom-antibottom quarks ( @xmath pair). Each quark hadronizes and is
seen in the detector as a jet. An essential point of our analysis is to
identify for a given jet if it is produced by a @xmath quark or not.
After hadronization, @xmath hadrons (mesons or baryons) travel for a
relatively long lifetime, on the order of a few picoseconds, before
decaying. Therefore, we can see in the silicon detectors a secondary
vertex displaced by about three millimetres from the primary @xmath
interaction vertex. The tracks originating in the secondary displaced
vertex have on average larger values for the @xmath parameter.

In this analysis we use two @xmath -tagging algorithms: Secondary Vertex
Tagger ( \secvtx ) and Jet Probability Tagger ( \jetprob ).

#### 4.7.1 Secondary Vertex Algorithm

The Secondary Vertex algorithm ( \secvtx ) [ 67 ] reconstructs secondary
vertices displaced with respect to the primary interaction vertex.
\secvtx operates not on an event by event basis, but on a jet by jet
basis, which means that one event can have two or more jets identified
by \secvtx as originating from a @xmath hadron, as shown in Figure 4.6 .

\secvtx

starts by looking at well reconstructed tracks by the Inside-Out
tracking algorithm in the cone of radius 0.4 around the calorimeter
cluster. That means that silicon tracks are required first and then a
match to a COT track is required. In order to reject poorly
reconstructed tracks, all tracks used by \secvtx must have @xmath ,
their impact parameter @xmath is corrected for the primary interaction
vertex and meets the criterion of @xmath , and the distance in the
@xmath coordinate between the track and the primary vertex should be
less than 5 cm ( @xmath ). In addition, all tracks have to pass a
certain number of hits in the silicon detector and the COT and track fit
have @xmath criteria.

Once all the tracks inside the jet cone are reconstructed, \secvtx has a
two pass approach.

First, \secvtx tries to fit three loosely defined tracks to a common
secondary vertex. These tracks have to have @xmath , @xmath and at least
one of the tracks needs to have @xmath .

If no secondary vertex is found in the first pass, then in the second
pass only two tracks are required, but they have to pass tighter
selection criteria of @xmath and @xmath .

If a secondary vertex is found by the first or second pass, then its two
dimensional decay length is measured with respect to the primary vertex
@xmath , together with the uncertainty @xmath . From these we obtain the
decay length significance @xmath . @xmath is positive (negative) when
the tracks emerging the secondary vertex are heading in the same
(opposite) direction as the jet. If @xmath the jet is \secvtx tagged,
with a positive tag if @xmath and negative tag if @xmath .

A positive \secvtx tag means that the jet has been identified as
originating from a @xmath quark. A negative \secvtx tag means that the
tracks have not been properly identified and that the primary vertex
lies in front of the jet. The negatively tagged jets are unlikely to be
produced by a @xmath hadron and are used to estimate the percentage of
positively tagged jets that actually originate from light quarks (such
as @xmath , @xmath , @xmath and @xmath ), also called the mistag rate.
The mistag rate is parameterized as a function of various variables as
seen in Figure 4.7 .

The \secvtx algorithm is tuned to accept only a very low mistag rate, on
the order of 1-2%, and this translates to a trade off in efficiency of
only 40% as seen in Figure 4.8 . What it means is that 60% of jets
originating from a @xmath quark will not be positively tagged by \secvtx
.

Also, out of the remaining positively tagged jets, about half originate
from the @xmath quark, which is also relatively massive and long lived.
In fact, in general @xmath -taggers are actually heavy flavour taggers,
where the flavour quarks are the @xmath and @xmath quarks.

Furthermore, the efficiency for tagging a jet with \secvtx is different
between data and Monte Carlo simulated events. We define a @xmath
-tagging efficiency scale factor on a jet per jet basis for \secvtx
defined as the ratio between the efficiencies for data and simulated
events. We selected a jet sample enriched in jets originating in @xmath
quarks and we measure a @xmath -tagging scale factor of @xmath .

#### 4.7.2 Jet Probability Algorithm

Another jet by jet basis @xmath -tagging algorithm employed in this
analysis is Jet Probability ( \jetprob ) [ 68 ] [ 69 ] . \jetprob looks
at the distribution of impact parameters for the tracks reconstructed in
the cone of the jet to estimate a probability that the ensemble of all
these tracks is consistent with originating from the primary interaction
vertex.

The impact parameter of a track is considered positive (negative) if the
angle between the track and the jet it belongs to is smaller (larger)
than 90 degrees. More precisely, an impact parameter in the @xmath -
@xmath plane is positive (negative) if @xmath ( @xmath ), where @xmath
is the angle between the direction of the jet and the distance of
closest approach between the track and the primary vertex, as seen in
Figure 4.9 .

Jets originating from light quarks ( @xmath , @xmath , @xmath ) and
gluons typically decay very close to the primary interaction vertex and
their tracks should appear to emerge from the primary interaction
vertex. However, due to finite tracking resolution, these reconstructed
tracks have non zero impact parameter values, as seen in the top of
Figure 4.9 . A track has equal chances to have a positive or negative
impact parameter. Therefore, the distribution of impact parameter values
for tracks from light flavour jets is symmetric around zero, as seen in
the top of Figure 4.10 .

However, jets originating from heavy quarks ( @xmath and @xmath ) will
have tracks that emerge from a secondary displaced vertex, which
translates to impact parameter values that are on average positive and
larger in absolute values, as seen in the right hand side of Figure 4.9
. Therefore, the distribution of impact parameter values for tracks from
heavy flavour jets is elongated toward positive values, as seen in the
bottom of Figure 4.10 .

The \jetprob algorithm uses only good quality tracks that have @xmath ,
@xmath , more than 3 hits in the silicon detector, more than 20 COT
axial segment hits and more than 17 COT stereo segment hits, as well as
less than 5 cm in the @xmath coordinate between the track and the
primary interaction vertex.

The first step is to quantify a probability for every track with a
positive impact parameter that it originates from the primary
interaction vertex (we note @xmath the probability of the k @xmath such
track). Tracks with negative impact parameters are only used to quantify
the uncertainty on the impact parameters, which depends on tracking
detector resolution, beam spot size and multiple scattering. The impact
parameter significance @xmath is defined as the ratio of the impact
parameter and its uncertainty

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

The individual track probability is parametrized as a function of its
impact parameter significance:

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

The next step is to quantify a probability on a jet by jet basis (
@xmath ) that the jet assumed to be made up of N well reconstructed
tracks with positive impact parameter is consistent with originating
from a primary interaction vertex (i.e. that the jet is originating from
a light flavour quark or gluon):

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

By the definition of @xmath and all the things explained above, we can
deduce that the distribution of @xmath is uniformly distributed between
0 and 1 for light flavour jets and is peaking at 0 for heavy flavour
jets, as seen in Figure 4.11 . In this analysis we ask for @xmath where
the \jetprob @xmath -tagging efficiency is approximately 33% and the
\jetprob @xmath -tagging scale factor is @xmath . A mistag matrix is
also measured for \jetprob .

### 4.8 Summary

This chapter presented the reconstruction techniques for the physics
objects used in this analysis. First the basic physics objects such as
tracks, primary vertices and calorimeter clusters were introduced.
Later, we described the reconstruction of high-level physics objects
such as electrons, muons, isolated tracks and jet candidates, as well as
missing transverse energy, which are used by the event selection.
Finally, two algorithms used to identify jets originating from @xmath
quarks were described. We also present the scale factors for charged
lepton identification and @xmath -tagging algorithms. All these physics
objects are used to reconstruct all the event, background and data
events for the @xmath search presented in this thesis.

Since all signals and all but one background processes are simulated
using Monte Carlo generators, in the next chapter we present how an
event is generated in a Monte Carlo simulation and what are the
particularities of simulation for each physics process.

## Chapter 5 Monte Carlo Simulated Events

In this chapter we describe the Monte Carlo simulated events used as
signal and background. In this analysis, the signal is represented by
the associated production of a Standard Model Higgs boson and a @xmath
boson ( @xmath process). We also consider a small contribution to the
signal represented by the associated production of the Higgs boson and a
@xmath boson, where the @xmath boson decays to a pair of charged leptons
and one of the charged leptons is not reconstructed in the detector (
@xmath process). The following processes have identical or very similar
final state signatures and therefore can mimic the signal to constitute
background processes for this analysis: @xmath +jets, top quark pair (
@xmath ), single top, @xmath +jets, diboson and non-W (QCD) production.
We use Monte Carlo generators to simulate signal and background
processes, except for the non-W (QCD) background, which is estimated
using a data sample.

### 5.1 Monte Carlo Simulation

Simulation is needed to predict the distribution of signal and
background events in the data event sample, as well as signal
efficiencies and scale factors between data and Monte Carlo simulated
events.

A typical particle physics event has the following steps from initial
@xmath interaction up to detection of final state particles in CDF:

-   Parton Distribution Functions. In a @xmath collision there is
    actually a parton-parton collision that takes place, whereas the
    other partons are spectator partons. Parton is a generic term used
    for a constituent of a proton or neutron and represents physically
    either a quark or a gluon ¹ ¹ 1 Historically speaking, first came
    evidence of a structure in protons and neutrons and only decades
    later came the confirmation of quark and gluon existence. . A parton
    has a certain probability to carry a certain fraction of the total
    momentum of the proton or antiproton. This probability is called a
    parton distribution function. They are measured in particle physics
    experiments and then are used as inputs for theory or Monte Carlo
    simulated events.

-   As a proton and antiproton approach each other at momenta of @xmath
    , a shower of partons appears from each parton in the proton and
    antiproton. This is called generation of initial state partons.

-   As two of these partons collide, they transfer momentum to each
    other and they can even change flavour. This process is called the
    hard scattering event.

-   Just as the incoming partons were branching, the final state partons
    may also branch. This produces the final state partons.

-   Because the strong force described by QCD, which mediates the
    interaction between partons, does not allow the existence of neutral
    coloured particles, an outgoing parton will transform part of its
    energy to produce the mass of new partons that together form neutral
    hadrons travelling in the same direction as the initial parton. This
    process is called fragmentation.

-   Most hadrons produced are unstable and decay to other particles,
    thus producing the final state particles that deposit energy in the
    detector.

In following subsections we will describe the various tools used for
Monte Carlo simulated events, both for signal and background processes:
event generators, parton shower and hadronization generators, detector
simulation.

#### 5.1.1 Event Generators

The signal Monte Carlo simulated events are generated with \pythia v6.2
[ 70 ] , a general-purpose event generator. The hard parton scattering
processes are computed at leading order matrix elements. \pythia uses
the parton distribution functions (PDF) provided by CTEQ5L [ 71 ] . In
this manner, a full particle physics event is generated, with parton
shower and hadronization included.

The @xmath +jets and @xmath +jets background events are simulated using
\alpgen [ 72 ] . It is an event generator specialized in electroweak
bosons ( @xmath and @xmath bosons) produced in association with a
desired number of jets coming from either quarks or gluons. \pythia is
also used to generate simulated events for the following background
processes: diboson ( @xmath , @xmath and @xmath ) production and top
quark pair ( @xmath ) production. The single-top background events are
simulated using \madevent [ 73 ] as event generator. As it produces
events at parton level, it is sensitive to the top quark polarization
that plays a role in the distribution of kinematic quantities in the
events. The top quark mass is assumed to be 172.5 @xmath when modelling
@xmath and single top production. The pure QCD (non-W) background events
are not simulated at all. Instead, the contribution of this background
process is measured directly from data events.

#### 5.1.2 Parton Showering and Hadronization

Irrespective of the event generator used, all simulated events use
\pythia to model the parton showering, gluon radiation and then
hadronization. The parton showering process allows for initial and final
state gluon radiation. These gluons then decay to quark pairs and thus
increase the number of jets detected in CDF. Beside the initial hard
scattering events, more particles may be detected in CDF due to effects
of multiple interactions and beam remnants. Once these particles are
produced, they are all passed to the hadronization stage of the
simulation. In the case of @xmath collisions at CDF, the hadronization
takes place at small @xmath and large @xmath and therefore perturbation
theory calculations cannot be used. Instead, phenomenological models
that depend on the Monte Carlo generator are employed.

The table 5.1 summarizes the event generators used for different
background and signal processes used in this analysis.

#### 5.1.3 Detector Simulation

Once the final state particles are generated, their propagation through
the detector (their interaction with matter in the detector) is
simulated using \geant [ 74 ] . Interaction with the silicon detectors
is simulated using an unrestricted Landau distribution and a simple
ionizing particle path length geometrical model. Interaction with the
tracking chamber system is simulated using \garfield [ 75 ] , whose
parameters are tuned to match CDF COT data [ 46 ] . Interaction with the
calorimeter system uses the \gflash [ 76 ] package. No special
parametrization is used for interaction with the muon system. A detailed
description of the CDF simulation can be seen in Reference [ 77 ] .

#### 5.1.4 Monte Carlo Validation

Since the number of events due to background processes is estimated
using Monte Carlo simulated events, it is essential to validate that the
Monte Carlo simulation models the data correctly. This is why we compare
background estimation and data distributions for all the kinematic
quantities used in this analysis, but also for other quantities as well.

### 5.2 Signal Samples

In this analysis the signal process is the associated production of a
@xmath boson and a Higgs boson, where a @xmath boson decays leptonically
to a charged lepton and a neutrino and the Higgs boson decays to a pair
of bottom-antibottom quarks. The leading-order Feynman diagram of the
@xmath process is presented in Figure 5.1 .

The signature of this process consists of a reconstructed charged lepton
candidate (an electron or a muon directly and a tau candidate indirectly
through its decay to an electron or a muon), missing transverse energy
(due to the fact that the neutrino escapes the detector without being
detected) and two jets (due to the bottom and antibottom quarks). This
final state signature is often called ”lepton + jets”, but it would be
more correct to call it “charged lepton + missing transverse energy +
jets”.

Since the same signature is obtained also for the associated production
of a Standard Model Higgs boson and a @xmath boson, where the @xmath
boson decays to a pair of charged leptons and one of them is not
reconstructed in the detector, we also consider this small contribution
to the signal of the analysis (the @xmath channel).

Since the mass is unknown for the Standard Model Higgs boson, we have
produced several @xmath and @xmath samples assuming several values of
Higgs boson masses that start at @xmath , end at @xmath and increment in
steps of @xmath . We have chosen this mass range because we are looking
for a Higgs boson in the ”low mass” region. The Monte Carlo event
generator is \pythia , which treats the Higgs boson as a resonance. The
production cross section and decay branching fraction depend on the
assumed Higgs boson mass and include all the latest higher order QCD and
electroweak corrections [ 78 ] , as illustrated in Table 5.2 .

The Monte Carlo samples model a different number of primary interaction
vertices per event in order to simulate the relatively low, medium and
high instantaneous luminosity that the Tevatron accelerator provided
during its current Run II.

### 5.3 Background Samples

In general there are two types of background: reducible and irreducible.
The reducible backgrounds can be reduced given enough data sets and/or
improved analysis techniques. They typically have different signatures
than the signal processes or they have the same signature but are made
up of one or more objects incorrectly reconstructed (fake objects) ² ² 2
As examples of fake object reconstruction we quote a real electron being
reconstructed as a jet and vice versa, or mismeasured jet, photon and
electron energies that appear to produce missing transverse energy in
the event. or by one particle not being reconstructed at all. The
irreducible backgrounds have the same final states and therefore, even
if all high level objects are correctly reconstructed, those backgrounds
will not go away. All one can do is measure them correctly.

In this analysis we use data or Monte Carlo simulated events to measure
the contribution of the background processes to our sample. Each of them
will be described in more detail below.

#### 5.3.1 Top Quark Pair Production

The first reducible background is due to top quark pair production (
@xmath ). A top quark decays almost 100% of the time to a @xmath boson
and a bottom quark. For this analysis, the background process happens
when one @xmath boson decays leptonically and some of the second @xmath
boson decay daughter particles products are not reconstructed properly.
Since in principle better detector and analysis techniques can reduce
the fraction of events reconstructed incorrectly, this background is
reducible. The leading order Feynman diagrams for the @xmath production
and decay are illustrated in Figure 5.2 , where the process mimics our
@xmath signal if one of the charged leptons (process in the left
diagram), or two of the jets (process in the right diagram) are not
reconstructed.

#### 5.3.2 @xmath Boson + Jets Production

The second reducible background is the associated production of a @xmath
boson and a gluon, where the @xmath boson decays to two charged leptons
(and one is not reconstructed at all) and the gluon decays to a @xmath
pair (and the mismeasured jet energies produce a fake missing transverse
energy). The leading order Feynman diagram for this process is shown in
the Figure 5.3 (a).

#### 5.3.3 Non-W (QCD) Multi-jet Production

The third reducible background of this analysis is the multijet
production described by the QCD theory, where one jet fakes an electron
candidate and the mismeasured energy of all the jets fakes the missing
transverse energy. Since semi-leptonic decays of hadrons containing
@xmath or @xmath quarks produce muons, a jet could in principle also
fake a muon candidate. However, we reduce most of these cases by
requiring the isolation requirement on the charged lepton candidates.
Thus a fake @xmath boson is reconstructed in the event, whereas the
physical process contains none, as seen in the leading order of the
non-W (QCD) background is illustrated in the Figure 5.3 (b). It is the
only background for which the event yield and kinematic shapes are
estimated using a data sample, and not one of Monte Carlo simulated
events. As we analyze an increasingly larger integrated luminosity, we
gain a better understanding of the process where jets fake real
electrons and the jet energy resolution. This is why the non-W (QCD) is
a reducible background.

#### 5.3.4 @xmath Boson + Jets Production

There are several @xmath +jets processes. Just as in the case of the
signal, these processes present a real @xmath bosons and two jets. The
jets may originate from light flavour partons (up, down, strange quarks
and any type of gluons) or from heavy flavour partons (charm and bottom
quarks). For the signal process, both jets originate from bottom quarks.
After we employ algorithms to identify jets that originate from bottom
quarks ( @xmath -tagging algorithms), the signal over background ratio
increases.

However, these algorithms are not perfect. On one side, light flavour
jets may be incorrectly identified as heavy flavour jets and thus become
a background for our signal, which we denote @xmath + Light Flavour
jets, or shortly, W+LF or Mistags. Such processes are shown in Figure
5.4 (c) where two jets are produced from two gluons and also in Figure
5.4 (a) if we replace the two bottom quarks with two light quarks. On
another side, these algorithms also identify charm quarks as bottom
quarks. In other words, although they are denoted @xmath -tagging
algorithms, in reality they tag heavy flavour quarks, with about half
tagged jets originating either from a bottom or charm quark. Such a
process is presented in Figure 5.4 (b) and is called @xmath + cj. The
process of Figure 5.4 (a) where we replace the two bottom quarks with
two charm quarks is called Wcc. In this thesis we add presented these
processed together under the common name of Wcc.

Since the @xmath -tagging algorithms may be improved if more time is
available and calibrated better in Monte Carlo and data simulated events
as we collected a data sample that corresponds to a larger integrated
luminosity, these W+LF and Wcc processes are reducible backgrounds for
our @xmath signal.

The first irreducible background is the associated production of a
@xmath boson and two jets that originate from bottom quarks, as seen in
5.4 (a). It is called irreducible since even if we had perfect
reconstruction algorithms, the process has exactly the same final state
as our signal. In order to reduce an irreducible background event
prediction, one has to look at subtle kinematic differences between the
two processes, as just reconstructing correctly the final state is not
enough any more. We call this process Wbb background. We also use the
denomination of @xmath + Heavy Flavour, W+HF, for the sum of Wcc and
Wbb.

For each process, the @xmath boson decay is produced leptonically to
either electron, muon or tau leptons plus neutrinos. Also, each of these
processes is simulated with various numbers of extra generic partons
(W+HF, namely @xmath , @xmath , @xmath ). All the subsamples have to be
added in order to obtain the generic W+HF background event yield. Some
extra jets can be produced by \pythia to account for the initial state
radiation (ISR) and final state radiation (FSR). In order not to count
incorrectly the number of jets in these events, we use the MLM
prescription [ 72 ] .

Each of these samples is simulated using \alpgen for matrix element
generation and \pythia for parton showering. After a jet-based flavour
overlap removal algorithm [ 80 ] is applied to both W+LF and W+HF
samples, they are all added together, weighting each sample by its
production cross section.

#### 5.3.5 Single Top Quark Production

The second irreducible background process for this analysis is the
single top production, where a top quark is produced by the electroweak
force in association with a bottom quark (s-channel, which has the exact
final state as our @xmath signal and therefore is an irreducible
background) and in association with a bottom quark and a generic quark,
where the generic quark escapes detection (t-channel, in principle a
reducible background due to the presence of the generic jet). The
s-channel (t-channel) leading order Feynman diagrams are illustrated in
the left (right) side of Figure 5.5 . The single top production has been
observed (discovered) experimentally at CDF [ 79 ] .

Single top events are generated with \madevent and the parton showering
is done with \pythia .

#### 5.3.6 Diboson Production

The electroweak diboson production processes are the associated
production of two @xmath bosons ( @xmath ), two Z bosons ( @xmath ) or
the associated production of a @xmath boson and a @xmath boson ( @xmath
). The @xmath and @xmath processes constitute an irreducible background
when one @xmath boson decays leptonically and the second @xmath boson or
the @xmath boson decay hadronically to two quarks. The @xmath process
constitutes a reducible background when one @xmath boson decays
leptonically to a pair of charged leptons, one of which is not
reconstructed at all, and the second @xmath boson decays hadronically.
For these processes, both the matrix elements and the parton showering
are simulated using \pythia . The Feynman diagrams of these processes
can be seen in Figure 5.6 . The non-resonant production of diboson
processes predicted by the Standard Model have been observed at the
Tevatron [ 81 ] [ 82 ] [ 83 ] .

### 5.4 Summary

We started this chapter by describing a three-step methodology to
simulate @xmath collisions that produce elementary particles that are
recorded by the CDF detector. The first step is to model the @xmath
interaction using Monte Carlo event generators, such as \pythia ,
\alpgen and \madevent . If quarks are produced, they hadronize and form
a shower, which is modelled by \pythia . Finally, the propagation of
such showers and other particles through the CDF detector is modelled
using \geant .

In the second part of the chapter, we described the signal processes for
the @xmath search. The main signal process is the @xmath associated
production, where the @xmath boson decays leptonically and the Higgs
boson decays to a @xmath pair. The second signal process is the @xmath
associated production, where the Higgs boson decays to a @xmath pair and
the @xmath boson decays to two charged leptons, but one of them is not
reconstructed in the CDF detector. The second process is only a small
contribution to the main @xmath process. We presented the Feynman
diagrams for the signal processes, as well as their cross sections and
branching ratios for the Higgs boson masses studies in this analysis.

We continued by presenting all the main background processes to our
@xmath search. We divided the backgrounds into two main categories. The
reducible backgrounds (top-quark-pair production, @xmath
-boson-plus-jets production, Non- @xmath -QCD-multi-jet production) are
those processes that can be separated from the signal in principle, with
datasets with large integrated luminosity and with better analysis
techniques. The irreducible background processes ( @xmath
-boson-plus-jets production, diboson production, single-top-quark
production) are those processes that cannot be separated further from
the signal even with larger datasets and improved analysis techniques
because they contain the same final state as the signal. We also
presented the Feynman diagrams for the background processes.

In the next chapter we will present the event selection, both online (at
the trigger level) and offline (at analysis level) to select from the
many @xmath collisions stored by the CDF those that have the signature
presented in this chapter, namely one charged lepton, missing transverse
energy and two jets that originate from bottom quarks.

## Chapter 6 Event Selection

In this analysis we select candidate events consistent with the
signature of one charged lepton (electron or muon), missing transverse
energy and two tight jets (at least one required to be @xmath -tagged by
the \secvtx algorithm). We analyze a data sample that corresponds to an
integrated luminosity of @xmath .

In this chapter we will describe the online and offline event selection.
The online selection is performed by the three levels of the trigger
system that selects, every second, about 100 bunch crossings out of the
about 2 million bunch crossings that take place per second. The total
information recorded by the CDF detector during a bunch crossing is
called an event. These selected events are saved on magnetic tape and
later are analyzed in detail in the offline analysis.

### 6.1 Online Event Selection

Events for the tight lepton categories are selected using the charged
lepton information. Events with CEM charged leptons are required to pass
the high transverse momentum CEM trigger. Events with CMUP charged
leptons are required to pass the trigger that asks for ionization both
in the CMU and CMP detector. Events with CMX charged leptons have to
pass the trigger requirements of CMX ionization deposits.

One of our contributions to this analysis is the addition of a loose
charged lepton category that uses at trigger level the orthogonal
information to the charged lepton, namely the missing transverse energy
and jets. We use three missing transverse energy + jets triggers to
select isolated track events, thereby increasing the signal acceptance.

Each trigger has three levels of selections, each more stringent than
the previous. The details of selection for all these triggers at all the
trigger levels are presented in detail below.

#### 6.1.1 CEM Trigger

The trigger used for selecting tight central electron candidate events
(CEM) is called “ELECTRON_CENTRAL_18”. At L1, it requires a track with
@xmath , a calorimeter tower with @xmath and the ratio between the
energy deposited in the hadronic calorimeter to that in the
electromagnetic calorimeter @xmath . At L2, it requires a calorimeter
cluster with @xmath matched to a track of @xmath . At L3, it requires an
electron candidate with @xmath matched to a track of @xmath . We note
that as we move up the trigger levels, object reconstruction becomes
more sophisticated and the selection cuts are tighter ¹ ¹ 1 We see that
only towers and tracks are required at L1, but clusters reconstructed
around towers and matching between clusters and tracks are asked for at
L2 and full electron reconstruction is performed at L3. We also note
that the cut values increase as we move from L1 to L3. . This is a
general feature of triggers.

We use a data sample of @xmath boson events decaying leptonically to an
electron and a neutrino to measure the efficiency of this trigger. The
efficiency of a trigger is the percentage of signal Monte Carlo
simulated events that meet all of the trigger requirements. We find an
average efficiency of @xmath for the CEM trigger.

#### 6.1.2 CMUP Trigger

The trigger used for selecting tight central muon candidate events
(CMUP) has the name of “MUON_CMUP18”. At L1, it requires a track with
@xmath that is matched to ionization in both the CMU and CMP muon
chamber that is consistent a muon candidate with @xmath . At L2, it
requires a track with @xmath and that the calorimeter cluster in the
direction of the track and muon stubs have ionization deposits
consistent with a minimum ionizing particle. At L3, it requires a fully
reconstructed COT track with @xmath that, if extrapolated to the CMU
(CMP) detector, matches hits in this detector within @xmath ( @xmath ).

We measure the efficiency of the CMUP trigger by collecting a data
sample of @xmath bosons that decay to a muon-antimuon pair, where one
muon passes the trigger requirements and the second muon is checked if
it passes the trigger selection or not. We find a CMUP trigger
efficiency of @xmath .

#### 6.1.3 CMX Trigger

The trigger used for selecting another category of tight central muon
candidate events, more forward than the CMUP muons, is called
“MUON_CMX18_DPS”. At L1, it requires a track with @xmath that is matched
to ionization in the CMX muon chamber. At L2 and L3, the criteria are
identical to that of CMUP trigger, only that they refer to the CMX
detector.

In a similar way to CMUP trigger, we measure a trigger efficiency of
@xmath for the CMX trigger.

#### 6.1.4 Triggers for ISOTRK

Our main original contribution to this analysis is the use of three
missing transverse energy + jets triggers that are used to select ISOTRK
events by using information orthogonal to the charged lepton ones,
namely the missing transverse energy and the jets. In order to use these
three MET-based triggers, we had to parameterize the trigger efficiency
turnon curve as a function of trigger MET for each of the triggers and
each trigger level. We also introduced a novel method to combine the
three different triggers in order to maximize the event yield and yet do
not have a logical “OR” between the triggers in order to avoid
correlations and measure easier and correctly the systematic uncertainty
related to the combined trigger efficiency. Due to the long description
necessary for this work, we present it in detail in Appendix A and
Appendix B .

### 6.2 Offline Event Selection

In this section we will describe the baseline offline event selection,
followed by the description of the @xmath -tagging categories used and
the QCD veto used for every charged lepton category.

#### 6.2.1 Baseline Event Selection

The offline event selection makes sure selected events pass a series of
criteria that help discriminate the @xmath signal against a large
background.

-   Events must fire the trigger specific to their charged lepton
    category. There is a special treatment for ISOTRK channel which,
    constitutes one of our original contributions to this analysis.

-   The @xmath coordinate of the primary interaction vertex must be
    within 60 cm of the centre of the detector, the so-called fiducial
    region.

-   One and only one high transverse momentum isolated charged lepton
    candidate must be reconstructed in the event. We require muon
    (electron) candidates to have @xmath ( @xmath ). We note that the
    trigger requirement was of @xmath (( @xmath ), but we require higher
    values in the offline selection to make sure that the event is in
    the plateau region of an possible trigger turnon.

-   The @xmath coordinate difference between the charged lepton track
    and the primary interaction vertex has to be smaller than 5 cm (
    @xmath cm).

-   Photon conversion events are vetoed. High energy electrons can emit
    photons primarily as they pass through the detector due to
    bremsstrahlung radiation. As these photons are also very energetic,
    they decay to electron-positron pairs. Events with such secondary
    electrons are identified and removed thanks to their two tracks with
    a small opening angle emerging from a vertex far away from the
    primary interaction vertex.

-   Cosmic ray events are vetoed, as discussed in the muon
    identification subsection 4.4.2 .

-   Events are vetoed if the invariant mass between the reconstructed
    charged lepton and any other track with opposite charge in the event
    falls in the @xmath boson mass window ( @xmath ).

-   The event must present large missing transverse energy @xmath for
    all charged lepton categories. The missing transverse energy is
    corrected for the position of the primary interaction vertex, for
    jet energies and the presence of muons, as described in detail in
    the object reconstruction chapter.

-   The event must contain exactly two tight jets reconstructed using a
    JETCLU algorithm with a radius @xmath . Each jet is required to have
    @xmath and be in the central region of the detector ( @xmath ) ² ² 2
    There are two other WH analyses at CDF [ 17 ] [ 84 ] that studied in
    addition the sample of events with exactly three tight jets. The
    third jet may appear from the underlying event, the initial state
    radiation or the final state radiation. .

-   Each event is required to pass some selection criteria designed to
    reject a significant part of the non-W (QCD) background events (QCD
    veto). The QCD veto is specific to every charged lepton category and
    will be discussed below.

-   Each event must have at least one jet that is @xmath -tagged using
    \secvtx algorithm in order to discriminate against events with light
    flavour jets. The detailed tag categories are descried below.

#### 6.2.2 @xmath-tagging Categories

Identifying jets that originate in @xmath quarks is essential for this
analysis, helping discriminate against the W+LF background. Since for
our @xmath signal events both jets originate from a @xmath quark, we ask
all events to have at least one jet tagged by \secvtx . Since \secvtx is
more efficient than \jetprob and we want to maximize the number of
events that have two tagged jets, we use the following @xmath -tagging
categories:

-    \secvtx
    tight + \secvtx tight (SVTSVT): Both jets in the event are tagged
    with \secvtx at the tight operating point.
-    \secvtx
    tight + \jetprob 5% (SVTJP05): One jet in the event is tagged by
    \secvtx at the tight operating point and another one is not tagged
    by \secvtx , but is tagged by \jetprob algorithm at 5% operating
    point. A jet is considered @xmath -tagged by \jetprob if it has a
    probability of less than 5% to emerge from a primary interaction
    vertex.
-    \secvtx
    tight (SVTnoJP05): One jet in the event is tagged by \secvtx at the
    tight operating point and another one is neither tagged by \secvtx ,
    nor by \jetprob .

By the construction of the @xmath -tagging categories, we can see that
they are orthogonal. Our analysis is divided in several channels based
on the charged lepton category and the @xmath -tagging categories.

#### 6.2.3 QCD Veto

In order to evaluate this background event yield, we select events based
on a non-W model specific for each charged lepton category in the
analysis. These selection criteria are very close to the charged lepton
ones, with one or two cuts required to fail. Therefore samples of
enriched non-W events are selected, which give the shape for the non-W
background for different kinematic quantities. The normalization of
these shapes (the exact yield of background events) is calculated later
on using a fit in data for missing transverse energy distributions, as
we will see in detail in the background estimation chapter.

For the tight central electron (CEM) channel of the analysis, the non-W
model is called “anti-electron” and it tries to model the jets that are
incorrectly reconstructed as an electron candidate. The electron trigger
is required to have fired as well as passing all but two of the
following kinematic quantities: @xmath , @xmath , @xmath , @xmath and
@xmath .

For the tight forward electron (PHX) channel of the analysis, the non-W
model is called “jet-electron”. It selects jet candidate events with a
transverse energy @xmath , @xmath and at least four tracks in the jet.
This makes sure that no real electrons are selected in this sample.

For the central muon (CMUP and CMX) channels of the analysis, the non-W
model is called “non-isolated muons”. It selects muon candidates that
pass all the selection criteria but fail the “isolation” one. That means
that the charged lepton track is required to be surrounded by other
activity in the COT, which is typical for jets.

The same criteria are required for the central loose muon candidates
(ISOTRK) channel of this analysis, where the non-W model is called
“non-isolated loose muons”. We select loose muon candidates that pass
all the selection criteria but fail the “isolation” cut.

In order to reduce the non-W (QCD) background in data samples before
asking any @xmath -tag requirement (Pretag samples) and in the SVTnoJP05
samples, we apply further selection cuts that are generically called
“QCD veto”. The QCD veto will not be applied for the SVTSVT and SVTJP05
tagging categories. The QCD veto applied is specific for each charged
lepton category.

We first define @xmath as the reconstructed transverse mass of the W
boson:

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

We then define @xmath as the azimuthal ( @xmath ) angle between the
missing transverse energy vector ( @xmath ) and the second most
energetic jet in the event (also called the second-leading jet) and
@xmath as the missing transverse energy significance given by the
following formula:

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where @xmath is the jet energy correction factor and @xmath is the
azimuthal angle difference between the uncorrected and corrected missing
transverse energy directions.

For the CEM category we require that:

-   @xmath

-   @xmath

-   @xmath

For the CMUP, CMX and ISOTRKcategories we require that:

-   @xmath .

#### 6.2.4 Further Event Selection for ISOTRK

##### Ensuring Exactly One Charged Lepton

For ISOTRK category we require a high transverse momentum isolated track
in the central region of the detector ( @xmath ), as it was described in
Subsection 4.4.3 . Since this is a loose charged lepton category, we
apply further cuts to ensure that the isolated track is genuinely a
charged lepton produced in a @xmath boson decay and that the event is
not counted already in another charged lepton category, since the
signature of our analysis contains exactly one charged lepton. Therefore
we apply the following veto cuts:

-   Tight Lepton Veto: If the event has a reconstructed CEM, CMUP or CMX
    lepton candidate, that event can not be an ISOTRK event.

-   Tight Jet Veto: If an isolated track is within a cone radius @xmath
    of a tight jet, then most likely the track belongs to a particle
    produced in a quark decay in a secondary vertex. We reject the event
    to avoid that the track does not originate from the primary
    interaction vertex.

-   Two or More Isolated Track Veto: If two or more isolated tracks have
    been reconstructed in the event, we veto the event.

##### Ensuring Trigger-Specific Jet Requirements

Also, the jet selection is specific for each of the three MET-based
triggers. As described in Appendix B , in my new method of combining
three MET-based triggers, we compute on an event by event basis the
trigger efficiency for each trigger and then we set it to zero if the
event fails the jet selection specific to that trigger. Then we choose
the largest of the weights and we require that it is strictly larger
than zero, thus ensuring that the jet selection is passed for at least
one of the triggers. We weight simulated events by this final weight.
For data events, we check that the trigger that gave the largest weight
fires for the particular event. If it does, the event is kept. If it
does not, the event is rejected, even if it could fire other triggers.
We do not even check if other triggers fire and it is this specific
handling of triggers that allows us to select the maximum signal
acceptance without having correlations (OR) between the triggers.

### 6.3 Summary

In this chapter we have presented the online (trigger) and offline event
selection. We started by describing in detail the charged-lepton
inclusive triggers used in this analysis, namely the CEM, CMUP and CMX
triggers. Our original contribution is the isolated track charged lepton
category, for which we introduce a novel method to combine three
different MET-plus-jets-based triggers. We continued by describing the
baseline event selection for all charged lepton categories. We then
introduced the three different @xmath -tagging categories employed in
this analysis with the help of two different @xmath -tagging algorithms.
We then presented the selection used to remove a large fraction of the
Non-W (QCD) background, which is specific to each charged lepton
category. The novel charged lepton category we introduced needs further
specific event selection, such as vetoing events with two or more
charged leptons or isolated tracks and ensuring the event passes a jet
selection specific to the chosen MET-based trigger.

The event selection criteria described in this chapter are applied both
to data and Monte-Carlo-simulated events. In the next chapter we will
present the methodology and the result for the calculation of signal
event yield prediction. In the following chapter we will present the
background estimation method.

## Chapter 7 Higgs Boson Signal Estimation

In this chapter we use Monte Carlo calculations to estimate the number
of @xmath ( @xmath ) signal events in our sample and the systematic
uncertainty on this number.

### 7.1 Signal Prediction Estimation

We calculate the number of predicted signal events from by the main
signal channel @xmath using Equation 7.1 . The supplementary
contribution from @xmath is calculated using Equation 7.2 .

  -- -------- -- -------
     @xmath      (7.1)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (7.2)
  -- -------- -- -------

In these equations, @xmath represents the @xmath production cross
section in @xmath collisions at a centre-of-mass energy of @xmath .
These values are a function of the Higgs boson mass and are presented in
Table 5.2 . The @xmath is @xmath [ 4 ] and represents the branching
ratio of a @xmath boson leptonic decay and is equal to the sum of the
branching ratio of a @xmath boson decay to an electron and an electron
neutrino plus the branching ratio of a @xmath boson decay to a muon and
a muon neutrino plus the branching ratio of a @xmath boson decay to a
tau lepton and a tau neutrino. The @xmath is @xmath [ 4 ] and represents
the branching ratio of a @xmath boson decay charged leptons and is equal
to the sum of the branching ratio of a @xmath boson decay to an
electron-antielectron pair plus the branching ratio of a @xmath boson
decay to a muon-antimuon pair plus the branching ratio of a @xmath boson
decay to a tau-antitau pair. The @xmath is the branching ratio of the
Higgs boson decay to a @xmath pair. These values are a function of the
Higgs boson mass and are presented in Table 5.2 . The @xmath is the
integrated luminosity. For this analysis, it has a value of @xmath
@xmath . The @xmath is the efficiency of the signal selection and is
given by equation

  -- -------- -- -------
     @xmath      (7.3)
  -- -------- -- -------

In Equation 7.3 , @xmath is the efficiency of the cut that requires the
primary vertex position to be situated within 60 cm of the centre of the
detector ( @xmath cm). The @xmath is the efficiency of the requirement
that the event fires the required trigger. It is measured in data for
each trigger and therefore is specific to each charged lepton category.
The @xmath is the ratio of the lepton identification efficiencies for
data and for simulated signal events, also called lepton identification
scale factor. The @xmath the fraction of signal events, after the
requirement of @xmath cm, that pass all the other kinematic selections
of the analysis. For the various @xmath -tagging categories, this term
also takes into account the @xmath -tagging scale factor between data
and signal simulated events.

For a Higgs boson mass of @xmath , the computed @xmath and @xmath signal
event predictions for each charged lepton and @xmath -tagging category
are presented in Table 7.1 . My original contribution to this analysis,
the ISOTRK charged lepton category, increases the @xmath ( @xmath )
signal prediction by 33% (66%) over the TIGHT charged lepton category
alone.

This increase is easily understood from Figure 7.1 , which represents
the @xmath - @xmath distribution of a sample of Monte Carlo simulated
events for the @xmath signal, where the Higgs must have a mass of 115
@xmath , after the full event selection in the Pretag sample. My
original contribution to this analysis, the addition of ISOTRK charged
lepton candidates, fills the gaps, as we see in red. This increases the
number of selected events containing charged leptons and thus the signal
event prediction. Unlike the case of muon candidates, the distribution
for electron candidates is smooth. This is why ISOTRK candidates are
mostly muon candidates. The calorimeter detectors still have very small
non instrumented regions, such as between the wedges of the calorimeter
towers. A detailed study [ 1 ] identified that ISOTRK candidates are
muon candidates in 85% of cases, electron candidates in 6% of cases and
tau candidates in 7% of cases.

### 7.2 Systematic Uncertainty on Signal Event Prediction

In this section we describe the various contributions to the systematic
uncertainty we quote on the @xmath and @xmath signal event prediction.

#### 7.2.1 Trigger

The systematic uncertainty on the trigger used is measured by selecting
data events with an orthogonal trigger and then asking the fraction of
events that fire our trigger of interest. This process is also done for
each charged lepton category. The systematic uncertainties are measured
to be @xmath for the TIGHT charged leptons. For the ISOTRK charged
lepton the analysis employs the new trigger parametrization described in
Appendix A and the novel method to combine triggers described in
Appendix B , with the method to compute systematic uncertainty described
in Section A.5 . We measured a trigger systematic uncertainty for the
ISOTRK category of 3%.

#### 7.2.2 Lepton Identification

The lepton identification systematic uncertainty is measured by
comparing a data sample highly enriched in @xmath boson events with a
@xmath boson sample of Monte Carlo simulated events using \pythia as
event generator. @xmath boson decay to charged leptons are used to
evaluate the lepton identification systematic uncertainty for each
charged lepton category. Details are presented in Subsection 4.4.4 .

#### 7.2.3 Initial and Final State Radiation

The systematic uncertainty due to the effect of the initial state
radiation (ISR) and final state radiation (FSR) is calculated by
changing in the Monte Carlo simulation the parameters related to ISR and
FSR to their half and double values. The systematic uncertainty is
quoted as half of the difference between the signal event prediction
with the two changes [ 3 ] .

#### 7.2.4 Parton Distribution Functions

Another systematic uncertainty source is the fact that the parton
distribution functions (PDFs) are not perfectly known, neither for the
protons, nor the antiprotons. We first compute three systematic
uncertainties that we use to compute the final PDF systematic
uncertainty.

The PDFs for the simulations used in this analysis use CTEQ5L [ 71 ] ,
which is parameterized using 20 eigenvectors. We weight the nominal
Monte Carlo simulated events for each of the 20 eigenvectors and for
each we compute a signal event prediction. The first PDF systematic
uncertainty is quoted as the quadrature sum between the differences
between the nominal and weighted event prediction.

We also compute the signal event prediction using MRST72 [ 85 ] as PDF
generator. The absolute value of the difference between the CTEQ5L and
MRST72 signal prediction is quoted as the second PDF systematic
uncertainty.

In addition we compute the signal event prediction using PDFs that are
generated using different values of the coupling constant of the strong
force, i.e. different QCD energy scales. We use MRST72 ( @xmath ) and
MRST75 ( @xmath ). The absolute value of the difference between the
CTEQ72 and MRST75 signal predictions is quoted as the third PDF
systematic uncertainty.

The final PDF systematic uncertainty is computed by adding in quadrature
the maximum between the first and the second with the third PDF
systematic uncertainty [ 3 ] .

#### 7.2.5 Jet Energy Scale

First we compute the nominal @xmath signal event prediction using a
Higgs boson mass of @xmath . Then, for the same Higgs boson mass, we
scale the jet energy scale [ 66 ] up and down by one standard deviation.
We compute the signal event prediction for these two cases. We take the
largest deviation from the nominal signal predictions as the jet energy
scale rate systematic uncertainty.

In this analysis we also consider one jet energy scale shape systematic.
We scale the jet energies up and down by one standard deviation and then
we propagate this to all reconstructed variables, including the final
analysis discriminant, described in detail in Chapter 9 and denoted the
BNN output. Both the central and the alternate plus and minus BNN shapes
are used in the limit calculation, as described in Section 10.3 . The
top plots in Figure 7.2 ( 7.3 ) presents the BNN shapes for JES zero,
JES plus, JES minus for each of the @xmath -tagging categories (SVTSVT,
SVTJP05 and SVTnoJP05, as described in Subsection 6.2.2 and the TIGHT
(ISOTRK) charged lepton category. The bottom plots in the same figures
represent the ratio of the JES plus and JES minus to JES Zero.

#### 7.2.6 @xmath-tagging Scale Factor

The systematic uncertainty on the @xmath -tagging scale factor between
data and Monte Carlo simulated events is computed in Section 4.7 .

#### 7.2.7 Integrated Luminosity

The integrated luminosity of 5.7 @xmath used in this analysis has been
measured with the Cherenkov Luminosity Counter, which as been described
in detail in 3.3.2 . The measurement has a systematic uncertainty of 6%
[ 42 ] .

#### 7.2.8 Systematic Uncertainty Values

The computed values for the total systematic uncertainties are presented
in Table 7.2 for tight central charged lepton categories (CEM, CMUP and
CMX) and in Table 7.3 for the loose central charged lepton category
(ISOTRK).

### 7.3 Summary

The first part of this chapter has presented the calculation of event
yield predictions for the @xmath and @xmath signal processes, which is
the multiplication of the integrated luminosity with the signal cross
section, decay branching ratios and efficiencies for the various
selection cuts. For the novel charged lepton category we introduced, we
explained how real charged leptons that go towards non-instrumented
regions of the detector are recovered and thus increase the signal event
yield.

In the second part of the chapter we presented the signal systematic
uncertainties that we take into account in this analysis, namely the
trigger, lepton identification, initial and final state radiation,
parton distribution functions, jet energy scale, @xmath -tagging scale
factor and integrated luminosity.

## Chapter 8 Background Estimation

Our background estimation method assumes that we have correctly
identified all the processes that mimic our @xmath signal: top quark
pair, single top (both s- and t-channel), diboson ( @xmath , @xmath and
@xmath ), @xmath +jets (both @xmath + Heavy Flavour (HF) jets and @xmath
+ Light Flavour (LF) jets), @xmath +jets (both W+LF and W+HF) and non-W
(QCD) production. In Chapter 5 we described in detail our signal and
background processes, especially their Feynman diagrams, signatures and
Monte Carlo generators used to simulate the events. In this chapter we
will describe how data and Monte Carlo generated events are used to
compute the contribution of each background type in our data sample.

We recall that we call the “pretag” sample the events that pass all the
event selection requirements, but are not required to pass any @xmath
-tagging information. The @xmath -tagging categories (SVTSVT, SVTJP05,
SVTnoJP05) are orthogonal to each other and each is a sub-sample of the
pretag sample. The Pretag sample is dominated by background events, with
very little signal contribution. This is why we use it as a “control
region” or “sideband” to check that the background modelling is correct
and in agreement with the data events. Table 8.1 gives the observed
number of events for the Pretag data sample. Also, we use the pretag
sample to measure the fraction of the data events that are non-W (QCD)
and @xmath +jets (both LF and HF) events, which is then extrapolated to
the “tagged” categories signal region, as we will discuss in the next
sections.

### 8.1 Top Quark and Other Electroweak Backgrounds

We first compute the expected background events in our data sample
background processes that are well modelled at tree level by Monte Carlo
simulations (top quark pair production, single top, diboson and @xmath
+jets). We use the same procedure and formulae as those used for the
signal acceptance calculation, described in detail in Chapter 7 and
summarized by the following formula:

  -- -------- -- -------
     @xmath      (8.1)
  -- -------- -- -------

We denote the estimated number of events due to electroweak processes
(diboson or @xmath +jets) as @xmath and the estimated number of events
due to top quark processes (top quark pair and single top) as @xmath .
They will be used in the background estimation of QCD and @xmath +jets
processes, as described below.

We use the cross sections and branching ratios specific for each
background process, as seen in Table 8.2 .

The efficiency term @xmath multiplies all the individual efficiencies,
except the @xmath -tagging scale factor. The efficiency term @xmath is
the @xmath -tagging scale factor for the event. For the pretag sample,
@xmath .

We measure the systematic uncertainty on the background normalization
values due to the @xmath -tagging efficiency by varying the scale factor
and mistag probabilities within one standard deviation of their values
and then reproducing this entire procedure.

### 8.2 Pretag: @xmath+Jets and QCD Events Faking a W boson

As described in detail in Subsection 6.2.3 , we developed a QCD template
(modelling) for each of the charged lepton categories ¹ ¹ 1 Each charged
lepton category is susceptible to different kinds of faking due to the
different reconstruction criteria. by selecting events that fail one or
two relevant cuts for charged lepton selection. These samples are
enriched in fake @xmath boson candidates and therefore represent a model
for the non-W (QCD) processes. However, the QCD background is the most
poorly predicted and the least understood, which requires us to assign a
large systematic uncertainty on its normalization estimate.

We fit the @xmath distribution in pretag data to a sum of @xmath
background shapes. In the fit the electroweak ( @xmath ) and top (
@xmath ) normalization values are fixed, but the normalization of QCD (
@xmath ) and @xmath +jets ( @xmath ) are allowed to float. The fit is
performed in the range @xmath using the @xmath distributions of the QCD
and W+jets samples, which are obtained after removing the @xmath from
the standard event selection. The normalization of the QCD and W+jets
samples are then fixed by the fit.

We define and extract from the fit the fraction of QCD events ( @xmath )
from the total number of events in the pretag data sample after the
standard selection @xmath cut is applied - equation 8.2 ), which we call
@xmath :

  -- -------- -- -------
     @xmath      (8.2)
  -- -------- -- -------

Then the number of QCD events in the pretag sample ( @xmath ) is given
by:

  -- -------- -- -------
     @xmath      (8.3)
  -- -------- -- -------

We are careful to check how the QCD normalization changes when we modify
the histogram binning, the @xmath fit interval, the @xmath cut for
definition of @xmath , as well as the non-W models used for CEM, CMUP,
CMX, ISOTRK and PHX. As a conclusion of these studies, we assign a
conservative 40% systematic uncertainty on the QCD normalization.
Despite this very large uncertainty, the total number of QCD events is
relatively small in the final sample, thanks to the @xmath cut we apply
in the final analysis. This permits the analysis to remain sensitive to
the @xmath process.

At this stage we have estimated the number of electroweak, top quark and
QCD processes in the pretag data set. The remaining events in the sample
are therefore @xmath +jets, as resulting from the following formula:

  -- -------- -- -------
     @xmath      (8.4)
  -- -------- -- -------

The @xmath +jets sample is divided in two: @xmath +heavy flavour (W+HF)
and @xmath +light flavour (W+LF).

The W+LF background contains light flavour jets that are incorrectly
tagged to originate from a heavy flavour quark ( @xmath or @xmath ) by
the @xmath -tagging algorithm. This phenomenon is called “mistag”, as
discussed in Section 4.7 .

### 8.3 Tag: @xmath+Jets and QCD Events Faking a W boson

Now we reproduce the procedure described above for each of the @xmath
-tagged categories. The background templates are all weighed by @xmath ,
as described in the section above.

#### 8.3.1 QCD Background

For a given category the QCD normalization is given by Equation 8.5 (to
be compared with Equation 8.3 for pretag) and the @xmath +jets
normalization is given by equation 8.6 (to be compared with Equation 8.4
for pretag).

  -- -------- -- -------
     @xmath      (8.5)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (8.6)
  -- -------- -- -------

#### 8.3.2 W+hf

The normalization of W+HF in a tagged sample ( @xmath ) is computed
starting from the normalization of @xmath +jets in the pretag sample,
that is multiplied by the fraction of pretag events with jets matched to
heavy flavour quarks ( @xmath ), by the scale factor between data and
Monte Carlo for the heavy flavour fraction ( @xmath ) and by the @xmath
-tagging efficiency:

  -- -------- -- -------
     @xmath      (8.7)
  -- -------- -- -------

The heavy flavour fraction @xmath is measured from Monte Carlo simulated
events of all the processes that produce one and only one real @xmath
boson. This quantity does not agree with the data prediction exactly and
their ratio is represented by the scale factor @xmath , which acts as a
correction for the heavy flavour fraction @xmath that is applied to
Monte Carlo simulated events from our analysis. The @xmath factor is
measured in the 1-jet bin of the analysis, that is a “sideband” and not
a signal region and has the largest statistics of all the jet bins. We
then assume that K has the same value across all the jet bins.

#### 8.3.3 W+lf

The normalization of W+LF in a tagged sample ( @xmath ) is computed
starting from the normalization of @xmath +jets in the pretag sample,
that is multiplied by the fraction of pretag events that is not matched
to heavy flavour ( @xmath ) and by the overall fake tag rate ( @xmath ),
also called the mistag rate:

  -- -------- -- -------
     @xmath      (8.8)
  -- -------- -- -------

The mistag rate is measured for each @xmath -tagging algorithm in
generic light jet data samples. The reason that sometimes light flavour
jets are tagged as heavy flavour jets is due to finite tracking
resolution. We measure the mistag rate using the negatively tagged jets,
as explained in Section 4.7 . In the end we produce a function that
inputs various jet quantities and outputs the mistag probability for
that jet. We call this function a mistag matrix. The mistag matrix for
\secvtx is a function of jet @xmath , jet @xmath , number of interaction
vertices in the event, jet track multiplicity and the scalar sum of all
the transverse energy in the event. The mistag matrix for \jetprob uses
the same information and, in addition, uses the @xmath coordinate of the
primary interaction vertex.

Once we obtain the mistag rate on a jet-per-jet basis, we need to do the
same thing for the entire event. We add the mistag jet rates to obtain
an event mistag rate. We sum all the event mistag probabilities to
obtain the total mistag probability, @xmath .

We measure the systematic uncertainty on the normalization of the W+LF
background by fluctuating the per-jet tag rates by one standard
deviation and then reproducing the entire procedure.

### 8.4 Background Fits and Event Counts

Figures 8.1 through 8.4 show the results fitting the @xmath distribution
in the Pretag and SVTSVT, SVTJP05 and SVTnoJP05 tag regions for each
charged lepton category. We note that we fit separately for CEM, CMUP,
CMX charged lepton categories and only later add the templates and event
counts. We set a default 40% systematic uncertainty on the QCD (non-W)
background normalization. Due to low statistics, for ISOTRK SVTSVT and
SVTJP05 we use a 100% systematic uncertainty.

Table 8.3 shows the event counts for each of the six analysis channels
given by the two charged lepton categories (TIGHT and ISOTRK) and the
three @xmath -tagging categories (SVTSVT, SVTJP05 and SVTnoJP05). The
event counts are estimated using Monte-Carlo simulated samples for the
signal and background processes and the data sample for the real
observed events.

### 8.5 Summary

This chapter has presented the methodology to calculate the event yield
prediction for the various background processes. Since any blind
analysis compares a control sample with an analysis sample, we define
the control sample as the Pretag sample and the signal samples as each
of the three @xmath -tagging categories. Although the signal samples are
included in the Pretag samples, since the Pretag sample is much larger
this is a very good approximation to the orthogonality of the control
sample and the pretag sample. We then presented the computation of the
top quark and electroweak background event yield by using the same
methodology used for the signal processes. It is only the @xmath +jets
and the QCD background yields that are determined from a missing
transverse energy fit to the data distribution. A slightly more complex
procedure is used for the @xmath -tagging category than for the Pretag
sample. We presented the fit plots in order to demonstrate the quality
of the fits. We concluded with the presentation of the table with the
event yield for the signal and background processes, as well as the
measured event counts, for each charged lepton and @xmath -tagging
category, as well as the uncertainty on these values due to the
systematic uncertainty. In all categories, the background prediction and
data agree within the systematic uncertainty for the background.

Since we do not see a signal excess, we employ multivariate techniques
to separate the signal and background even more. We detail these in the
following chapter.

## Chapter 9 Neural Network Discriminant

The event selection is optimized to separate as much as possible the
signal and background processes. Requiring @xmath -tagging separates
them even more. For a given @xmath -tagging category, a @xmath search
based on a counting experiment would be a search for a bump in the
distribution of the invariant mass of the two jets (dijet invariant
mass). However, since the expected signal is about two orders of
magnitude smaller than the background prediction, as seen in Chapter 8 ,
such a bump would not be visible. The main reason why a counting
experiment @xmath analysis is not sensitive enough is that the dijet
invariant mass describes just the Higgs candidate system and ignores any
information about the reconstructed @xmath boson candidate (charged
lepton and missing transverse energy), as well as any correlation
between the @xmath and Higgs systems.

For improved sensitivity, in this analysis we use a multivariate
technique. In general, a multivariate technique uses several kinematic
distributions to discriminate further the signal events from the
background events. The multivariate technique chosen for this search is
an artificial neural network.

### 9.1 Artificial Neural Networks Overview

Artificial Neural Networks (ANN) are multivariate technique functions
that are produced through an iterative training process. An ANN is
formed of several interconnected nodes. Each connection is weighted by a
sigmoid function. The first layer contains input nodes, the last layer
contains the output nodes, and the rest of the nodes are organized into
intermediate hidden layers. Using a feed-forward process, an ANN allows
the information to flow from the input nodes to the output nodes. For a
given event, the input nodes receive values of certain kinematic
quantities and the output nodes give the neural network output values
for that event.

### 9.2 Neural Network Structure

In this analysis we use as a final discriminant the output of an ANN
with only one hidden intermediate layer and with only one node in the
output layer, as seen in Figure 9.1 . The ANN is basically a function
that takes various quantities from the event as an input and returns
only one number.

If there are @xmath nodes in the input layer, every node @xmath in the
hidden layer is described by a sigmoid function that depends on the
neural network input values @xmath :

  -- -------- -- -------
     @xmath      (9.1)
  -- -------- -- -------

The weights @xmath are determined by training the artificial neural
network. Once trained, the ANN output from the only node in the third
layer is computed using a linear combination between the hidden layer
values:

  -- -------- -- -------
     @xmath      (9.2)
  -- -------- -- -------

where the weights @xmath are also determined by training the neural
network.

### 9.3 Neural Network Training

Before the training starts, all the weights ( @xmath and @xmath ) have
some initial values. At the end of the training process, these weights
would be such that the ANN output is as close as possible to the chosen
target value ( @xmath ), which means we optimize the quantity @xmath :

  -- -------- -- -------
     @xmath      (9.3)
  -- -------- -- -------

In this analysis, we choose for the ANN a target value of 1 for signal
samples and 0 for background samples. Therefore we use signal and
background samples for the training process. We divide these samples in
half. One half is used for training and the other half is used for
validation of the training on independent dataset samples.

For each event from the training sample, we minimize the quantity @xmath
. We then use back propagation to change the values of the weights (
@xmath and @xmath ) with an amount proportional to the ANN output to the
target. The training is finished when new events would not change any
more these weights in a significant way.

This is not the first search that uses an ANN at CDF. It is now a
standard multivariate technique that is used in high energy physics
experiments.

There are many types of ANN. In this search we use a Bayesian Neural
Network (BNN) algorithm [ 86 ] [ 87 ] . The advantage of BNN over other
artificial neural network algorithms is that it is less prone to over
training because of the Bayesian statistical interpretation where each
weight is considered as a posterior probability in Bayes’ theorem.

For this analysis, we employ distinct BNN discriminant functions which
were optimized for one of the three @xmath -tagging categories: SVTSVT,
SVTJP05 and SVTnoJP05.

### 9.4 Neural Network Training Check

Once training is done, we perform a check for overtraining. Overtraining
may happen when the neural network ”learns” the training sample instead
of “generalizing” a model from the training sample. In overtraining the
smallest statistical fluctuation is believed to be part of the real
model. For this reason, an overtrained neural network has very little
predictive power. If overtrained, the neural network output distribution
for an independent test sample will not be the same as the one for the
training sample.

We check that our neural networks are not overtrained by comparing the
training sample shape to that for a test sample which was not used in
training. Figure 9.2 shows examples of an overtraining check for a Higgs
mass of 115 @xmath . We conclude that our BNN discriminant is not
overtrained, since the response of the training sample is in good
agreement with that of the test sample.

### 9.5 Neural Network Inputs

Each BNN is optimized independently to separate the @xmath signal from
the various background processes (W+HF, W+LF, top quark pair, single top
s-channel production). We use distinct BNN input quantities for each
tagging category.

For the SVTSVT @xmath -tagging category, seven kinematic quantities are
used as inputs. The first one is the invariant mass of the two jets in
the event ( @xmath ). This quantity is computed after both jets have
their energies corrected using another type of artificial neural
network, as described in Section 9.8 . The second quantity is the scalar
sum of the transverse momenta of the charged lepton and the jets, from
which the missing transverse energy is subtracted ( @xmath imbalance).
The third one is the invariant mass of the charged lepton, missing
transverse energy and one of the two jets, where we choose the jet that
produces the largest invariant mass ( @xmath ). The fourth one is the
charge of the charged lepton multiplied by its @xmath coordinate (
@xmath ). The fifth one is the scalar sum of the transverse energy of
the loose jets of the events ( @xmath ). The loose jets are orthogonal
to the tight jets and have @xmath and @xmath . The tight jets are
explicitly not included in this summation. The sixth one is the
transverse momentum of the reconstructed @xmath boson candidate,
computed as the vector sum of the transverse momentum of the charged
lepton and missing transverse energy ( @xmath ). The seventh one is the
scalar sum of the transverse energies of all the objects in the event,
such as jets, charged lepton and missing transverse energy ( @xmath ).

For the SVTJP05 @xmath -tagging category, we also use seven kinematic
quantities as inputs. Five of them are the same as for the SVTSVT
category, namely @xmath , @xmath , @xmath , @xmath and @xmath . Instead
of @xmath we use @xmath , which means we pick the jet that minimizes the
quantity. We also use missing transverse energy ( @xmath ) instead of
@xmath imbalance. Both changes are motivated by the fact that for each
@xmath -tagging category we have tested a large number of input
parameter combinations and we have chosen the inputs that give the
largest sensitivity in that category.

For the SVTnoJP05 @xmath -tagging category, we also use seven kinematic
quantities as inputs, namely @xmath , @xmath , @xmath , @xmath , @xmath
, @xmath and @xmath imbalance.

### 9.6 Background Modelling Check

We plot the distribution of each of the inputs and outputs for BNN to
check that the total background prediction agrees with the observed data
distribution, as seen in Figures 9.3 9.4 and in those of Appendix D .
This procedure cross checks that our various background processes are
modelled well. As all distributions agree in normalization and shape
with the data distributions, we are safe to use the BNN output in the
final limit calculation.

### 9.7 Neural Network Output

From a physics point of view, the BNN takes as an input an entire event
(from which it selects the information required by the input nodes) and
gives as an output only one value, which represents the probability the
event is signal or background. The BNN is trained for signal to peak at
output values of 1 and background at output values of 0. The training
sample is obtained using the same event selection described in Chapter 6
. The training is done independently for Higgs boson masses between 100
and 150 @xmath with increments of 5 @xmath . Indeed, this is confirmed
in the normalized-to-unit-area BNN output distributions for signal and
background for the various @xmath -tagging categories and the TIGHT and
ISOTRK charged lepton categories, where the Higgs boson is assumed to
have a mass of @xmath , which can be seen in Figure 9.5 .

### 9.8 Neural Network for Jet Energy Correction

All quantities in the analysis are computed using the jet energies
corrected for various effects, as described in Chapter 4 . However, as
the dijet invariant mass is the most sensitive observable to distinguish
between the @xmath signal and various types of background, we make an
extra effort to reduce its uncertainty. This improves directly the
@xmath search sensitivity.

We have designed a method that corrects the jet energies once more,
depending on the information if they have been @xmath -tagged or not [
88 ] . Each correction is implemented on a per-jet basis. Typically the
jet energies measured by the calorimeter are underestimated and even
more so for jets originating in @xmath quarks due to the semi-leptonic
decays of the @xmath quark producing a muon that is a minimum ionizing
particle in the calorimeter. We add vertex and tracking information
about the jet in order to improve the jet energy resolution.

To achieve this goal, we use a multivariate regression technique in the
form of a second artificial neural network algorithm. All neural network
algorithms provided by ROOT have been tested and the best results were
obtained with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method [ 89 ]
.

We train three BFGS neural networks, one for jets tagged by \secvtx ,
one for jets not tagged by \secvtx but tagged by \jetprob , and one for
jets that are neither tagged by \secvtx nor by \jetprob . We use only
@xmath signal samples, that are divided in two. One half is used as
training sample and another one as test sample. We do not use background
samples because we are focused on improving the dijet mass resolution
for signal. This quantity would anyway have a broad distribution for
background processes. The input nodes use quantities that describe the
jet and will be enumerated below. There is only one output value and its
target value has a specific value for each jet taken as an input by the
neural network. In principle this method works for any type of jet, but
it must be trained for the specific type of jet.

#### 9.8.1 BFGS Inputs

For the neural network trained on \secvtx -tagged jets, we use the
following nine quantities as inputs: jet @xmath ; jet @xmath ;
uncorrected jet @xmath ; jet transverse mass; jet decay length ( @xmath
); uncertainty on the jet decay length ( @xmath ); sum of transverse
momenta of tracks originating from the secondary vertex identified by
the \secvtx algorithm; the maximum @xmath of tracks inside the jet; the
scalar sum of transverse momenta of tracks inside the jet. For the jets
not tagged by \secvtx , we use the same input variables except @xmath
and @xmath .

#### 9.8.2 BFGS Output

The neural network is designed to have as an output ( @xmath ) the extra
correction factor by which the standard-corrected jet transverse
energies ( @xmath ) have to be multiplied in order to achieve the newly
corrected values ( @xmath ).

  -- -------- -- -------
     @xmath      (9.4)
  -- -------- -- -------

For each jet, the target value ( @xmath ) for @xmath is represented by
the ratio between the Monte Carlo generator level transverse energy (
@xmath ) and the standard-corrected transverse energy ( @xmath ).

  -- -------- -- -------
     @xmath      (9.5)
  -- -------- -- -------

Following this procedure, after the training, @xmath will be closer to
@xmath than @xmath .

#### 9.8.3 Background Modelling Check

Also for this second neutral network type used for @xmath -jet energy
corrections, we check that the data reproduce well the background
modelling for all the kinematic distributions of the inputs and output
of these two jet energy correction neural networks. For SVTSVT events,
we sum up the histograms for each of the two tight jets in the events so
that we double the statistical power. Figure 9.6 shows the input and the
output of the neural network based SVT-correction, on a jet by jet
basis, proving that this correction works both for Mote-Carlo-simulated
and data events.

#### 9.8.4 Dijet Invariant Mass Resolution

We now compute the dijet invariant mass using @xmath instead of @xmath
for all the samples used in our analysis: Pretag, SVTSVT, SVTJP05 and
SVTnoJP05. This specific correction improves the resolution of the dijet
invariant mass from about 15% to about 11% in the SVTSVT category and
from about 17% to about 14% in the SVTnoJP05 category across all the
Higgs boson mass range, as we can see Figure 9.7 .

### 9.9 Summary

In conclusion, in this chapter we have presented two artificial neural
networks that we use in our analysis. One corrects the energy of the
jets based on the information of whether the jet is tagged or not by the
@xmath -taggers used in our analysis. The corrected energies are used to
compute the dijet invariant mass variable, which in turn is the main
input variable to the final analysis discriminant, another artificial
neural network (BNN) trained to separate the signal and the various
background processes. In the next chapter we will present how the BNN
output is used in order to compute an upper limit on the Higgs boson
cross section times branching ratio.

## Chapter 10 Upper Limits on Higgs Boson Production

In this thesis we present a search for the existence of the Standard
Model Higgs boson. In the absence of an observation, as a final result
we present an upper limit on the Higgs boson cross section times
branching ratio at 95% credibility level (CL), as is typical in Bayesian
statistical inference in experimental particle physics. We compute the
upper limit using the BNN neural network output distributions for
signal, background and data processes. Since the BNN distributions are
binned, we use a binned likelihood technique. In Section 10.1 we
introduce an unbinned Bayesian likelihood technique, i.e. the treatment
of one specific bin in the distribution, which is equivalent to a simple
counting experiment. In Section 10.2 we explain how each independent
systematic uncertainty is taken into account as a nuisance parameter
that is convolved with the likelihood. In Section 10.3 we present how
the method is generalized to a given number of bins for one analysis
channel, i.e. a charged lepton and @xmath -tagging category pair. In
Section 10.4 we explain how combining all analysis channels is
equivalent to adding new bins to only one distribution. In Section 10.5
we explain how we use pseudo-experiments to compute the expected limit
(the sensitivity of the analysis) and read data to compute the observed
limit (our result). In Section 10.6 we present the expected and observed
upper limits in the @xmath search in the TIGHT, ISOTRK, and TIGHT+ISOTRK
channels, with all @xmath -tagging categories combined.

### 10.1 Bayesian Upper Limit Calculation

One a bin-by-bin basis, we denote with @xmath the expected number of
events and with @xmath the measured number of events. Our goal is to
evaluate the probability that we expect @xmath events when we measure
@xmath events, given that the probability that we measure @xmath events
given we expect @xmath events is described by the Poisson distribution
expressed by Formula 10.1 . This process is called statistical
inference.

  -- -------- -- --------
     @xmath      (10.1)
  -- -------- -- --------

There are two principal methodologies for statistical inference. The
frequentist approach assumes no prior knowledge about the generic
probabilities @xmath and @xmath and introduces a test statistic variable
that is used to test if the data distribution agrees more with the
background-plus-signal hypothesis, or with the background-only
hypothesis. The frequentist approach is used to set upper limits on the
Higgs boson at the DZero experiment at Fermilab. At CDF we use the
Bayesian approach, where prior information is taken into account as
well. We start with Bayes’ theorem:

  -- -------- -- --------
     @xmath      (10.2)
  -- -------- -- --------

where @xmath is the posterior probability on the expected number of
events @xmath , i.e. the probability distribution of @xmath after the
experiment is performed; @xmath is the probability to measure n events
given the expected number of events @xmath , has a Poisson distribution
and is given by formula 10.3 ; @xmath is the prior probability on the
expected number of events @xmath , i.e. the probability distribution of
@xmath before the experiment is even performed; @xmath is the
probability of distribution for the observed number of events n, which
is evaluated as a normalization constant given by the condition that the
sum of all probabilities should be exactly 1, as shown in Equation 10.4
.

  -- -------- -- --------
     @xmath      (10.3)
  -- -------- -- --------

  -- -------- -- --------
     @xmath      (10.4)
  -- -------- -- --------

By combining Equations 10.2 and 10.4 we obtain

  -- -------- -- --------
     @xmath      (10.5)
  -- -------- -- --------

which leads to the formula for @xmath

  -- -------- -- --------
     @xmath      (10.6)
  -- -------- -- --------

A decision has to be made about what prior information should be assumed
about the expected number of events distribution. We assume a flat prior
with @xmath in order not to bias us towards a certain distribution of
the signal and yet help us deal with systematic uncertainties. Given
that @xmath is also a constant with respect to @xmath we define @xmath
as a constant that depends only on @xmath , and given by the formula

  -- -------- -- --------
     @xmath      (10.7)
  -- -------- -- --------

Combining Equations 10.2 and 10.7 , we can express the posterior
probability

  -- -------- -- --------
     @xmath      (10.8)
  -- -------- -- --------

In our analysis, the number of expected events @xmath is represented by
the sum of the expected number of background events ( @xmath ) and the
expected number of signal events ( @xmath ), as seen in equation

  -- -------- -- --------
     @xmath      (10.9)
  -- -------- -- --------

Also, the observed number of events @xmath is the number of measured
data events ( @xmath ), as in

  -- -------- -- ---------
     @xmath      (10.10)
  -- -------- -- ---------

By combining Equations 10.8 , 10.9 and 10.10 , the equation for the
likelihood becomes

  -- -------- -- ---------
     @xmath      (10.11)
  -- -------- -- ---------

In our analysis, we want to set an upper limit on the value of the
number of Higgs signal events @xmath , given not only the measured
number of data events @xmath , but also the number of expected
background events @xmath . All the predictions of the Standard Model
have been confirmed, except the existence of the Higgs boson. This is
why we are sure, within uncertainties, of the existence of the
background processes and of their predicted number of events @xmath . We
then reinterpret the posterior probability as the posterior probability
of having @xmath expected signal events given @xmath expected background
events and @xmath measured data events and we note it as @xmath . After
the change of variable from @xmath to @xmath , Equation 10.11 becomes

  -- -------- -- ---------
     @xmath      (10.12)
  -- -------- -- ---------

We denote with @xmath the number of signal events predicted by the
Standard Model. We express the true value for the signal predicted
events @xmath by introducing the ratio @xmath as @xmath . We also denote
that the total background event prediction is formed of several
background processes. We therefore note @xmath , where k is the index of
a given background sample. The expected value is now

  -- -------- -- ---------
     @xmath      (10.13)
  -- -------- -- ---------

After a change of variable from @xmath to @xmath , Equation 10.12
becomes

  -- -------- -- ---------
     @xmath      (10.14)
  -- -------- -- ---------

By definition, the minimum value that @xmath can take is zero. This case
corresponds to the background-only hypothesis. Any positive value of
@xmath corresponds to the background-plus-signal hypothesis. In this
analysis, we set a 95% credibility level upper limit on @xmath , as is
typical in particle physics experimental searches for new particles (the
Standard Model Higgs boson search or new elementary particles predicted
by theories beyond the Standard Model). In other words, we want to find
the upper value @xmath for the ratio @xmath between the true number of
expected events @xmath and the Standard Model prediction @xmath , such
that the probability that @xmath lies in the interval [0, @xmath ] is
0.95. Therefore, we have to solve for @xmath in the equation

  -- -------- -- ---------
     @xmath      (10.15)
  -- -------- -- ---------

By combining Equations 10.14 and 10.15 , we have to solve for @xmath in
the equation

  -- -------- -- ---------
     @xmath      (10.16)
  -- -------- -- ---------

Similarly with equation 10.5 , the constant term @xmath is given by the
condition

  -- -------- -- ---------
     @xmath      (10.17)
  -- -------- -- ---------

to be

  -- -------- -- ---------
     @xmath      (10.18)
  -- -------- -- ---------

By combining Equations 10.15 and 10.18 we obtain the equation for @xmath
:

  -- -------- -- ---------
     @xmath      (10.19)
  -- -------- -- ---------

Equation 10.19 cannot be solved analytically, so numerical integrators
have to be used. The strategy is to consider increasingly higher values
of @xmath , compute the integral and stop immediately after the value of
0.95 is reached.

In conclusion, this is the Bayesian approach to computing the upper
limit @xmath for an analysis with only one bin in our final discriminant
(for a simple counting experiment) and without taking any systematic
uncertainties into account. In the following sections we will present
how to take into account the systematic uncertainties, several bins and
several analysis channels.

### 10.2 Taking Systematic Uncertainties Into Account

The procedure described in the previous section does not take into
account the systematic uncertainties on the Standard Model predicted
events for several background channels ( @xmath ) and for the Higgs
boson process ( @xmath ).

There are two types of systematic uncertainties. Rate systematic
uncertainties apply to all the bins in the distribution. Shape
systematic uncertainties are rate systematic uncertainties applied on a
bin-by-bin basis. In this section we will describe how rate systematic
uncertainties are introduced in a one-bin analysis. Shape systematics
for binned discriminants will be described in the following section. In
our analysis we employ only symmetric systematic uncertainties
characterized by the standard deviation @xmath .

For each independent systematic uncertainty we introduce a nuisance
parameter @xmath as a coefficient to the expected number of events for a
particular process. Most systematic uncertainties affect multiple
physics processes. If we consider @xmath and @xmath the sets of nuisance
parameters that apply to the signal and to the @xmath background
process, respectively, then we can express the expected number of events
@xmath as

  -- -------- -- ---------
     @xmath      (10.20)
  -- -------- -- ---------

We model each nuisance parameter @xmath as a truncated Gaussian
distribution. We recall that a Gaussian distribution of a variable
@xmath with a mean @xmath and a variance @xmath is given by

  -- -------- -- ---------
     @xmath      (10.21)
  -- -------- -- ---------

Since nuisance parameters are used as coefficients for the predicted
number of events for each background and signal process, we set @xmath
to the value of the chosen systematic uncertainty for that particular
process expressed as a ratio of the absolute systematic uncertainty and
the absolute expected number of events. For example, if a systematic
uncertainty is 3%, we set @xmath . This implies that the expected value
@xmath is set to 1.0. Therefore, nuisance parameters @xmath are
described by the following Gaussian distribution:

  -- -------- -- ---------
     @xmath      (10.22)
  -- -------- -- ---------

We truncate the Gaussian distribution for each nuisance parameter in
order to keep only those values that make physical sense, such as
positive values.

As per the instructions in the statistics section of the Particle Data
Group review [ 4 ] , we take the systematic uncertainties into account
by convolving each nuisance parameter into the likelihood function,
integrating over the nuisance parameters and then reproducing the
reasoning in the previous section using the new likelihood. The
likelihood is a function of the nuisance parameters @xmath since it is a
function of the expected number of events @xmath which is a function of
the nuisance parameters, as described in Equation 10.20 . The likelihood
is given by

  -- -------- -- ---------
     @xmath      (10.23)
  -- -------- -- ---------

The new likelihood is a convolution of the old likelihood from Equation
10.23 with all nuisance parameters modelled by Gaussian distributions
and is given by

  -- -------- -- ---------
     @xmath      (10.24)
  -- -------- -- ---------

Each integral is performed between a lower range, chosen such that the
quantity is positive, and infinity. This process is called integrating
out the nuisance parameters and the result is that the new likelihood
does not depend any more on the nuisance parameters. Since such an
integral over many parameters is very difficult to compute with normal
Monte Carlo methods, a Markov Chain Monte Carlo (MCMC) [ 4 ] method is
used in our analysis.

Since the likelihood depends now only on @xmath , @xmath and @xmath , as
if the systematic uncertainties did not exist, we have reduced our
problem to the simpler problem solved in the previous section. The upper
limit @xmath is computed by yet another integration given by the generic
formula

  -- -------- -- ---------
     @xmath      (10.25)
  -- -------- -- ---------

### 10.3 Taking All The Bins Into Account

In the previous two sections we have described the Bayesian statistical
inference of an upper limit assuming there is only one bin in our chosen
distribution, as is the case in a simple counting experiment. We
increase the sensitivity of the analysis if we take advantage of the
shapes of the distributions, i.e. we consider each bin of the BNN output
distribution separately. On a bin-by-bin basis the signal over
background ratio (S/B) changes. By construction of our BNN output, the
lower S/B is achieved for low values of BNN and higher S/B is achieved
for high values of BNN.

Rate systematics such as described above apply the same way to all the
bins. However, shape systematics must now be taken into account as well.
A shape change is actually a change in values on a bin-by-bin basis. A
shape systematic can be understood as a rate systematic that is
bin-specific. For each shape systematic we introduce a new nuisance
parameter modelled by a Gaussian distribution. We integrate all the rate
and shape nuisance parameters in the new binned likelihood in order to
obtain the analysis likelihood. The only difference with respect to the
previous section is the integration range for the shape nuisance
parameters. For rate systematics, they were the value where the
parameter became positive and infinity. For a shape systematic, the
value is computed from the templates for the shape upper fluctuation,
lower fluctuation and central value. In our analysis we use only one
shape systematic, namely the one due to jet energy scale for the jet
energy.

The likelihood of the @xmath bin is given by

  -- -------- -- ---------
     @xmath      (10.26)
  -- -------- -- ---------

where @xmath represents both the rate and shape nuisance parameters for
the @xmath bin. Since all bins are statistically independent, the
likelihood for all the binned BNN distribution is given by the product
of the likelihoods for each bin, namely

  -- -------- -- ---------
     @xmath      (10.27)
  -- -------- -- ---------

We have now reduced the problem to that of a one bin distribution. We
integrate out the nuisance parameters as in Equation 10.24 and we
compute the upper limit as in Equation 10.19 .

### 10.4 Taking Into Account All Analysis Channels

In the previous section we have presented the limit calculation for one
analysis channel with a binned distribution. However, our analysis has
six independent channels, each channel given by a pair of charged lepton
and @xmath -tagging categories. In this analysis we have two charged
lepton (TIGHT and ISOTRK) and three @xmath -tagging categories (SVTSVT,
SVTJP05 and SVTnoJP05).

We first perform the analysis in each category separately, which means
computing a likelihood for each category. Since all channels are
statistically independent, we combine all these categories by
multiplying all these likelihoods together, which is equivalent to
considering all the bins in the discriminant output from the six
channels juxtaposed in only one histogram. We have reduced the problem
of multiple channels to the simpler problem of only one channel and now
we proceed as in the previous section.

### 10.5 Expected Limits and Observed Limits

Before computing the limits using the real data distribution (an unblind
analysis) we evaluate the sensitivity of our analysis to a @xmath (plus
@xmath ) signal (a blind analysis). We simulate the number of data
events (pseudo-data events) by picking randomly according to a Poisson
probability density function a real expected number of events in the
interval generated by the background prediction allowed to fluctuate
smoothly in its one sigma interval. Therefore, we do not use data, but
pseudo-data. We do not use real events, but pseudo-events. Each
pseudo-event is characterized by a different random number so that the
value @xmath is specific for each event. Also, in the pseudo-events we
assume there is no signal at all ( @xmath =0). The median of the
distribution of upper limits is considered the median expected limit and
it characterizes the sensitivity of our analysis. To ensure enough
pseudo-experiment statistics to compute the lower and upper one and two
standard deviation bounds on the median expected limit, and yet limit
the CPU power consumption, we perform 3000 pseudo-experiments ¹ ¹ 1 The
computation of 3000 pseudo-experiments for each mass point, for each
charged lepton category and b-tagging category pair, and for all the
categories combined takes on the order of 3 days using the parallel
computing facilities of the CDF collaboration. .

Contrary to the expected limit that uses pseudo-data in many
pseudo-experiments, the observed limit (the real result of our analysis)
uses real data in only one experiment. Both upper limits using
pseudo-experiments and the real experiment use the methodology described
in the sections above. When we present our expected and observed limits,
we check that the observed limit is within the two standard deviation
interval around the median expected value for each Higgs boson mass
point.

### 10.6 WH Neural Network Upper Limits

At CDF a binned likelihood technique such as described in the previous
sections is implemented in the MCLIMIT [ 90 ] [ 91 ] package that we
used for our analysis as well. We search for a Higgs signal excess in
our BNN neural network output distributions. Since we find no evidence
for such an excess, we set upper limits on the @xmath production cross
section times the branching ratio: @xmath . We present the upper limits
as ratios (normalized) to the Standard Model predicted ones (x SM) for
TIGHT, ISOTRK and TIGHT combined with ISOTRK, each with all @xmath
-tagging categories combined in Table 10.1 and in Figure 10.1 .

### 10.7 Impact of Our Original Contribution

Since my original contribution to the @xmath analysis is the addition of
the ISOTRK charged lepton category with respect to the TIGHT charged
lepton category, it is shown in Table 10.2 for each Higgs boson mass
point the expected and observed upper limits for the TIGHT category only
and the TIGHT combined with ISOTRK, as well as the percentage by which
the limits become smaller and thus better. In Figure 10.2 it is shown in
black the expected and observed limits for the TIGHT charged lepton
category only and in red the ones from the combination of the TIGHT and
ISOTRK categories. The improvement both for the expected limits and
observed limits visible in Table 10.2 is now also visible in a visual
form.

### 10.8 Summary

In this chapter we have presented the computation methodology and
results for the upper limit on the Higgs boson cross section times
branching ratio as a function of the Higgs boson mass for the TIGHT,
ISOTRK and TIGHT+ISOTRK cases, when all @xmath -tagging categories are
combined. In the first part of the chapter we introduce the methodology
of the limit calculation for a one-bin analysis without and with
systematic uncertainties. We then presented the methodology for a
multi-bin analysis. Finally, we presented how several channels are
combined, such as our @xmath -tagging categories. We then introduced the
pseudo-experiments, which use data simulation by allowing the background
prediction to fluctuate within its uncertainty, in order to measure the
sensitivity of the analysis. We then presented the real measurement
using real data. In the last part of the chapter we presented both in
plots and tables that adding the ISOTRK charged lepton category on the
top of the TIGHT charged lepton category improves both the expected and
observed upper limits significantly.

## Chapter 11 Conclusions and Discussions

### 11.1 Summary

In this dissertation we have presented an experimental test of the
current theory of particle physics and their interactions, the Standard
Model (SM). All the predictions of the SM have been observed
experimentally, except one, namely the existence of a new elementary
particle called the Higgs boson. If the particle is discovered, the SM
is confirmed experimentally. If the particle is excluded, then the SM is
refuted, which means that the true model of nature is not the SM, but
some other theory beyond the SM. The mass of the SM Higgs boson is
unconstrained by the theory, but direct and indirect experimental
searches constrain it at 95% confidence level between 114.4 and 158 or
between 175 and 185 @xmath . The preferred Higgs boson mass from SM
indirect fits is towards the lower edge of the allowed mass ranges. We
therefore performed an experimental search of the Higgs boson that was
most sensitive to possible low Higgs boson masses.

We study a sample of @xmath collisions at the Tevatron at the
centre-of-mass energy @xmath that corresponds to an integrated
luminosity of 5.7 @xmath collected by the Collider Detector at Fermilab.
There are many ways a Higgs boson is hypothetically produced at the
Tevatron, but independently of the production mode, once produced, a
Higgs boson would decay the same way, depending just on its mass. For
masses below 135 @xmath , the Higgs boson is expected to decay
predominantly to a @xmath pair, whereas for higher mass it decays
predominantly to a @xmath pair. Given our preference for a low mass
Higgs boson, we choose the most promising channel to identify a Higgs
boson that decays to @xmath pairs: the associated production between the
Higgs boson and the @xmath boson, where the @xmath boson decays
leptonically. Our analysis channel is therefore @xmath .

We select events consistent with a signature of a high- @xmath charged
lepton (electron or muon) candidate, large @xmath and exactly two jets.
In order to improve the signal over background ratio, we require that at
least one of the jets is identified to originate in a @xmath quark. We
use a sample of events without this requirement as a Pretag control
sample. In order to discriminate further between signal and background
events, we employ an artificial neural network. Finally, using a
Bayesian statistical inference technique, we compute expected and upper
limits on the cross section times branching ratio with respect to the SM
prediction for Higgs masses between 100 and 150 @xmath .

The main charged lepton categories at CDF are electron and muon
candidates with stringent reconstruction criteria. An electron (muon)
candidate is typically a high- @xmath isolated track that is matched to
a calorimeter cluster (muon stub). We reconstruct tight electron (muon)
candidates using an electron(muon)-inclusive trigger. We add together
all the @xmath events selected using tight charged leptons into the
TIGHT sample.

Our detector has uninstrumented regions both at the calorimeter level,
such as the small space between calorimeter towers, and at the muon
detector level, where the eta-phi coverage is not uniform. We introduce
a novel charged-lepton category with looser reconstruction criteria,
namely a high- @xmath isolated track that is freed from the requirement
to match a calorimeter cluster or a muon stub. Such charged-lepton
candidates recover also real charged leptons that would have been
otherwise lost in the non-instrumented regions of the detector. We call
the @xmath sample collected in this way the ISOTRK sample. We make sure
that the ISOTRK and TIGHT samples are orthogonal.

As there is no ISOTRK-dedicated trigger at CDF, we use triggers that
make use of the orthogonal information in the event, namely the @xmath
and the jets. We have three such MET-based triggers at CDF. We
parameterized at each of the three trigger levels the trigger efficiency
turnon curves as a function of trigger @xmath and identified the
appropriate jet kinematic selection so that the efficiency is flat with
respect to jets and only varies with respect to trigger @xmath . We also
measured the prescale of one of the triggers that is prescaled. Since
not all triggers were used for all the runs in our dataset, we measured
the fraction of luminosity where each of the possible combinations of
the triggers were used.

There are many possible ways to combine the triggers and indeed in the
@xmath search our contribution on this topic was a work in progress. As
such, the @xmath analysis using 2.7 @xmath from the summer of 2008 used
only one MET-based trigger. The @xmath analyses using 4.3 @xmath from
the summer of 2009 and using @xmath from the summer of 2010 used two
different MET-based triggers. The ISOTRK channel was an original
contribution to these analyses. These results went into the CDF and
Tevatron combinations from those years and were presented at the major
summer conferences.

In this latest analysis we use the same integrated luminosity as the
@xmath analysis of the summer of 2010. The motivation is that we focused
on two main issues. The @xmath group at CDF of which I am an active
member decided to develop a new data analysis software framework called
WHAM, completely independent of the one used for the 2010 analysis. This
new framework is more modular, more flexible and dedicated to the single
charged lepton plus missing transverse energy plus jets. Several
analyses such as @xmath , @xmath , single top, technicolor, @xmath are
currently produced in this framework. WHAM allows the sharing of almost
all the tools between the analyses and as such avoids redundancy, allows
an analysis improvement to be instantly propagated to the other related
analyses, and allows greater scrutiny of a common piece of code and thus
identify bugs easier. I am one of the three main authors of the software
framework. My first task was therefore to reproduce in the new framework
the 2010 @xmath analysis in the TIGHT and ISOTRK category, while my
second task was to improve the MET-plus-jets parameterization in order
to improve the ISOTRK category, the results of which are embodied in
this dissertation.

Our final result is represented by the expected and observed 95% CL
upper limits on the Higgs boson cross section times branching ratio with
respect to the SM when all categories are combined. The expected upper
limits vary between 2.73 @xmath SM and 31.2 @xmath SM for a mass range
of 100-150 @xmath . The improvement in sensitivity due to addition of
the ISOTRK charged lepton category is between 16 and 19% for the entire
mass range. The observed upper limits vary between 2.39 @xmath SM and
31.1 @xmath SM for the entire mass range. Since the upper limit set by
the LEP experiments is 114.4 @xmath , it is interesting to note that for
a 115 @xmath Higgs boson, we compute an expected upper limit of 3.79
@xmath SM and an observed upper limit of 5.08 @xmath SM.

### 11.2 Future Prospects

This section presents the future prospects and potential improvements of
the @xmath search at CDF, of the Higgs combined searches at the Tevatron
and of the original method to combine an unlimited numbers of
MET-plus-jets triggers that I introduce in this analysis.

#### 11.2.1 @xmath search at CDF

By the time this thesis is submitted, there are about two more months
remaining until the summer conferences of 2011. I will lead the @xmath
effort through the internal review process and a possible subsequent
publication for approximately 7 months thanks to a Universities Research
Association ¹ ¹ 1 Universities Research Association (URA) is the
association of universities that manages Fermilab. McGill University and
the University of Toronto are the only universities in Canada that are
members of the URA. Visiting Scholar Postdoctoral Fellowship and a grant
for travel to Fermilab. It is both the goal of the CDF experiment and my
personal goal to improve this analysis with the newly available datasets
and as many of the following potential improvements in the analysis
technique: add a forward electron charged lepton category; migrate some
of the current ISOTRK events in a separate loose muon category and use
triggers dedicated to them; replace the current @xmath -tagging
algorithms with a newer one that has been produced by the CDF
collaboration. These improvements are expected to sharpen the analysis
sensitivity significantly more than just the addition of more integrated
luminosity would allow us to, as seen in the following section.

#### 11.2.2 Higgs search at the Tevatron

The cross section times branching ratio sensitivity of Higgs searches at
the Tevatron typically improve continuously due to two reasons: using
larger integrated luminosity data sets, as Tevatron accelerator performs
excellently and is scheduled to run until 30 September 2011; improving
the analysis techniques. If analysis improvements are ignored, the
sensitivity scales as @xmath . Over the past few years, CDF has managed
to improve always the expected sensitivity more than just could be
achieved by using larger data sets, as can be seen in Figure 11.1 for a
Higgs boson mass of @xmath (top) and @xmath (bottom), where twice larger
datasets than CDF only are assumed, simulating at first order an
expected Tevatron combination between the CDF and DZero experiments,
that have almost similar data sets and analysis sensitivity. Also,
Figure 11.1 suggests that with about 9 @xmath of integrated luminosity
collected until the expected end of Run-II at the Tevatron in summer
2011, if the potential analysis improvements identified are implemented,
then the Tevatron combination would approach a Standard Model
sensitivity at both the low and high Higgs boson mass.

#### 11.2.3 Trigger Combination Method

Our original method to combine triggers is already in use by other CDF
analyses due to the fact that the method is implemented by a user
friendly software package called ABCDF that we designed. The @xmath and
@xmath will use the three triggers combined by the novel method in the
searches they prepare for the summer of 2011. The method has potential
applications in searches of physics beyond the Standard Model, such as
supersymmetry, that have as key signatures large missing transverse
energy and jets. As such, it is already applied at other CDF analyses
and can be applied to other analyses at DZero at the Tevatron or at
ATLAS and CMS at the Large Hadron Collider at CERN. The method has the
advantage that it incorporates an unlimited number of triggers and each
trigger is allowed to have its own jet selection, prescale and interval
of applicability.

#### 11.2.4 Higgs searches at the LHC

Both ATLAS and CMS experiments at the Large Hadron Collider have started
to present results on SM Higgs searches. As of April 2011, none of these
searches has become more sensitive than those of CDF and DZero at the
Tevatron. However, the LHC has broken the instantaneous luminosity
record of the Tevatron and as more data is rapidly collected by the
well-performing ATLAS and CMS detectors, they will surpass in
sensitivity finally the Tevatron searches. For now, the jury is still
out and the friendly competition between the Tevatron and LHC is
ongoing. Being in experimental particle physics is indeed living in
interesting times.

### 11.3 Conclusions

We have presented a @xmath search at CDF and we introduced a novel
charged lepton category that improved the sensitivity of the search by
16-19% across a Higgs boson mass of 100-150 @xmath . In the process we
developed a novel method to combine an unlimited number of MET-plus-jets
triggers, which is already being used by other CDF analysis and has the
potential to be useful for other experiments as well, since triggers
based on this signature are key to some physics beyond the Standard
Model scenarios.

## Appendix A MET-based Trigger Parametrization

Events that do not present a tight charged lepton, but have a high-
@xmath track isolated from other activity in the tracking system are
called isolated track (ISOTRK) events. This new charged lepton category
has looser reconstruction criteria than the tight charged leptons and
represents my original contribution to the @xmath analysis. At CDF the
tight charged lepton events are collected using dedicated inclusive
electron or muon triggers. However, there is no trigger dedicated to
ISOTRK events. For this reason, we use triggers based on the orthogonal
information to the charged lepton, namely the missing transverse energy
(MET) and jets. We call them generically MET-based triggers. At CDF we
have three such triggers. We denote them with MET2J, MET45 and METDI.
For Monte Carlo simulated events we have to model the trigger selection
by applying on an event-by-event basis a weight that represents the
trigger efficiency. This chapter describes the parametrization that we
measured for these MET-based triggers used in the analysis for the
ISOTRK category. The novel method we introduced to combine these three
MET-based triggers is described in Appendix B .

### a.1 Three MET-based Triggers at CDF

There are three trigger levels at CDF, which we denote L1, L2 and L3. At
each trigger level, quantities are reconstructed more correctly, using
successively greater computing resources, than at the previous trigger
level. As a general rule, trigger requirements become more stringent as
the trigger level is higher. The offline event selection is then even
tighter. In this section we will describe the trigger requirements for
each of the three MET-based triggers employed in this analysis.

#### a.1.1 MET + 2 Jets Trigger

The MET + 2 jets trigger (MET2J) has been active since the beginning of
Run II at the Tevatron. A data event fires the L1 of MET2J if it has a
MET larger than 28 GeV ( @xmath ), in which case it is sent
automatically to be studied by L2. In order to pass the L2 requirements,
a data event must have @xmath and at least two jets, one of them with a
transverse energy @xmath and reconstructed in the central region of the
detector ( @xmath ) and the other jet with @xmath and @xmath . Not all
events that pass requirements at L2 are sent to be analyzed by the L3,
which means that this trigger is prescaled. The prescale is done in an
automatic way as a function of the instantaneous luminosity. We measure
the prescale of this trigger, as seen in Section A.6 . Events that reach
L3 and meet the requirement of @xmath fire the full MET2J trigger and
are saved on tape to be used in our analysis. Since there are real data
events that do not meet these trigger criteria, they are not stored and
therefore not used in the analysis. For this reason, not all Monte Carlo
simulated events should be used, or rather they should all be used to
preserve the statistics, but they should be weighted to simulate the
trigger selection.

In time, there were four major versions of the MET2J trigger used at
CDF. The trigger evolved as the instantaneous luminosity increased and
as the requirements of the specific physics groups changed. For example,
previous versions required a @xmath at L1, did not require that one of
the jets be central at L2, and there was no prescale at L2. For a given
run, only one version of the trigger was used. In this analysis we
parameterize the trigger efficiency averaged out over the several
historical versions, as if there were only one version. We make sure our
offline requirements are tighter than the most recent and stringent
requirements of the MET2J trigger.

#### a.1.2 MET Trigger

The MET-only trigger has requirements only on @xmath , but not on jets.
This trigger has also been in existence since the beginning of Run II.
It comes in two historical versions. The first version was used for the
first 2.3 @xmath of the integrated luminosity and required at L1 @xmath
, at L2 @xmath and @xmath . The second version started being used
afterwards for the remaining of 3.4 @xmath of the integrated luminosity
used in this analysis. The physics desire was to decrease the @xmath
value in order to select more events from rare processes, such as Higgs
or physics beyond the Standard Model. As such, it is required at L1 that
@xmath , at L2 @xmath and at L3 @xmath . Just as in the case of the
MET2J trigger, we parametrize the trigger as if it has only one version
which we call MET45. This trigger was never prescaled.

One caveat is that somewhere in the early data taking there was a bug in
the MET45 trigger and although the event fired and the information was
stored, the information is not to be trusted. Therefore, in the run
range 178637-192363, which approximates 3% of the total integrated
luminosity used in this analysis, we treat data events as if the trigger
MET45 was not defined and therefore could not have fired. We have to
simulate this in Monte Carlo events as well. The novel method we
introduce to combine triggers takes this into account easily, as seen in
Appendix B .

#### a.1.3 MET + Dijet Trigger

The third and last MET-based trigger at CDF is the MET + dijet trigger.
As its name suggests, it is very similar to the MET2J trigger and in
order to avoid confusion it is denoted in this thesis as METDI. This
trigger was first introduced when about 2.4 @xmath of integrated
luminosity have already been collected and has never been changed since.
This trigger was designed by the Higgs Trigger Task Force and was
optimized for the Higgs boson search. This trigger was never prescaled.
Since this trigger was applied only in about 42% of the integrated
luminosity, we also have to simulate that in Monte Carlo events. The
novel method we implemented does that easily, as described in Appendix B
.

### a.2 Variable Choice for Trigger Parametrization

We want to parametrize the trigger efficiency turnon curves as a
function of only one variable that is common for all the three triggers
and apply cuts so that we are in the plateau regions with respect to all
the other variables. For the MET-based triggers, the naturally quantity
for the parametrization is the missing transverse energy and the trigger
specific selections are based on the kinematic distributions of the two
jets in the event.

Since the parametrization is performed using an offline data sample and
since we also apply the parametrization in the analysis online, we need
to choose one @xmath quantity computed offline that is as close as
possible to the @xmath quantities used at trigger level. As discussed in
Section 4.5 , the fully corrected @xmath on which we apply a cut at 20
GeV for all events in our analysis is raw @xmath corrected for the
position of the primary interaction vertex, for the jet energies in the
event and for the energy deposited by the muon in the calorimeter (which
is relevant in the case of ISOTRK charged lepton events, which are muons
in 85% of the time). From a physics point of view, @xmath represents the
missing transverse energy due to the neutrino in the final state.
However, these corrections are not performed at trigger level and
therefore the physical meaning of the trigger @xmath is the missing
transverse energy of the @xmath boson (and not of the neutrino!).

Ideally we should choose raw @xmath for our parametrization. However,
studies have shown that this variable is not well modelled in the
control sample (Pretag) of the analysis. Therefore we correct this
quantity for the position of the primary interaction vertex and the
energy of the jets, but not the energy of the muon. Its physics meaning
remains the missing transverse energy of the @xmath boson, but it is now
modelled better. We denote this quantity trigMET and we use it for the
trigger turnon curve efficiency parametrization.

### a.3 Trigger-Specific Jet Selection

The next step is to identify for each trigger the specific jet cuts that
allow for the remaining data events that the turnon curve
parametrization is indeed flat in any jet quantities and depends only on
trigMeET. All events in our analysis must have exactly two jets with
@xmath and @xmath .

The jet selection specific for the MET2J trigger is the following: both
jets need to have @xmath , one of them must be in the central region of
the detector ( @xmath ), while the second jet must have @xmath , and
also the two jets have to specially separated ( @xmath ).

The MET45 trigger does not have any jet requirements at trigger level.
Therefore, for this trigger the jet selection is the same as for the
non-ISOTRK charged lepton categories, namely exactly two jets with
@xmath and @xmath .

For the METDI trigger we studied that the optimum specific jet selection
requires the most energetic jet to have @xmath , the second most
energetic jet to have @xmath and both jets to have @xmath .

We can already see that each trigger must be applied only in its
specific jet kinematic region, which is equivalent to assuming the
trigger is not defined in the other kinematic regions. The method we
introduce in Chapter B also takes this easily into account.

### a.4 Parametrization for MET-based Triggers

Since we use these MET-based triggers for the ISOTRK charged lepton
category and previous studies have shown that ISOTRK candidates are in
85% of cases muon candidates, we measure the trigger turnon curves using
a data sample collected with a muon inclusive trigger. We require
exactly one reconstructed CMUP muon candidate which fires the CMUP
trigger. We compute trigger efficiency turnon curves for each of the
three MET-based triggers and for each of the three trigger levels in
such a way that none of these turnon curves takes into account the
prescale of the trigger, if any. In the following paragraph we describe
the procedure for one generic case.

We select the subset of events that pass the jet selections specific for
this trigger. For these events we fill a histogram for the variable
trigMET. This is the denominator histogram. We then fill another
histogram for the same variable, but only for the events that also fired
the chosen MET-based trigger. This is the numerator histogram. Since the
CMUP and the MET-based trigger are uncorrelated for all practical
purposes, we divide the numerator and denominator histograms to obtain
the efficiency turnon histogram for the chosen trigger. We fit the
efficiency histogram to a sigmoid function with four parameters and as a
function of trigMET, given by

  -- -------- -- -------
     @xmath      (A.1)
  -- -------- -- -------

where @xmath represents the highest plateau efficiency, @xmath
represents the central value of the turnon region (and is measured in
@xmath ), @xmath represents the width of the turnon region (and is also
measured in @xmath ) and @xmath represents the lowest efficiency value.
The fit returns the four parameters which uniquely defines the
efficiency as a function of trigMET only for a given trigger and trigger
level.

In order to ensure that the turnon curves do not include the effect of
the eventual trigger prescale (the method is general, although we know
that only the trigger MET2J is prescaled only at L2), for L2 we include
in the denominator the requirement that the event was sent from L1 to L2
and for L3 that the event was sent from L2 to L3.

The nine turnon curves are presented in the Figure A.1 (MET2J trigger),
the Figure A.2 (MET45 trigger), and the Figure A.3 (METDI trigger).

### a.5 Parametrization for Systematic Uncertainty Evaluation

We repeat the procedure above in bins of the following kinematic
quantities: @xmath , @xmath and @xmath of both jets, absolute value of
the @xmath , @xmath , @xmath , @xmath , as well as fully corrected
@xmath of the analysis and the fraction of total luminosity that
corresponds to each run. The number of bins are chosen automatically by
the code in order to have a minimum specified number of events in the
turnon region of the distribution so that the fit is performed
correctly.

Whereas the standard trigger weight is the value of the central turnon
curve for the event trigMET, the systematic weight that corresponds to
the variable @xmath is given by the same trigMET applied to a different
turnon curve specific to the bin of the particular event @xmath . The
same is repeated for all systematic uncertainties. The total weighted
average is compared between the central turnon curve and each of the
systematic values and the largest percentage difference is quoted as the
systematic uncertainty of the analysis. This procedure is general and
works to all trigger combination methods, including the one we introduce
in this analysis and we present in the next chapter.

### a.6 Prescales for MET-based Triggers

Since the MET45 and METDI triggers are unprescaled, their prescales are
@xmath . However, the trigger MET2J is prescaled at L2 for a large
fraction of the integrated luminosity. We computed the total integrated
luminosity for MET2J and we divided it to the total integrated of
another trigger (which requires a jet with an energy larger than 100
@xmath ) which has also been continuously active since the beginning of
Run II and was never prescaled. Therefore we computed the prescale for
MET2J to be @xmath .

### a.7 Integrated Luminosities for MET-based Triggers

Not all triggers were defined at the same time. In order to properly
simulate that in Monte Carlo events, we measured the fraction of
integrated luminosity for each combination of MET-based triggers were
defined, as shown in Table A.1 .

We have now all the information needed about triggers in order to
combine them in our analysis. We introduce a novel trigger combination
technique that we present in the next chapter.

## Appendix B Novel Method to Combine Triggers

Once the MET-based triggers are parametrized, there are many ways they
can be combined in an analysis. Our trigger parameterizations were used
for the @xmath analyses for the summers of 2008, 2009 and 2010, which
were presented in two Ph.D. theses, CDF and Tevatron Higgs combinations,
one PRL paper and two PRD drafts under preparation, as presented in
detail in the section entitled ”Original Contributions”.

### b.1 The Method for the 2.7 @xmath @xmath search

The analysis for the summer of 2008 used 2.7 @xmath of integrated
luminosity. It used only one MET-based trigger, namely MET2J, and
therefore there were no complications. For both Monte-Carlo-simulated
and data events only the same trigger would be used. For data events,
the event is checked if it has fired the trigger. If it has, the event
is kept and is given a weight of exactly 1.0. If not, the event is
rejected, which is equivalent to receiving a weight of exactly 0.0.

As a side note, this analysis used a simplified and less precise trigger
parameterization than the one we described in Appendix A . The analysis
used only one trigger efficiency turnon curve measured across all
trigger levels and which included the trigger prescale. The trigger
weight was applied to all Monte-Carlo-simulated events without
exception.

### b.2 The Method for the 4.3 and 5.7 @xmath @xmath searches

The analysis for the summer of 2009 (2010) used 4.3 (5.7) @xmath of
integrated luminosity. Both analyses used two MET-based triggers, namely
MET2J and MET45. Since MET2J could be applied only for a subset of the
jet kinematic phase space where MET45 could be applied and since
efficiency studies suggest that MET2J is more efficient than MET45, it
was decided that for events with two jets with @xmath , one jet central
with @xmath and the other jet with @xmath , and with non overlapping
jets with @xmath , only the MET2J would be used, whereas for the
remaining phase space up to two jets with @xmath and @xmath only MET45
would be used. This method is very clean and brings no complications,
since for a kinematic region, both the Monte-Carlo-simulated and data
events will use the same trigger.

For a given event, the jet kinematic region was checked. If the event
was in the tight kinematic region and it was a data event, then the
event was checked if it fired the MET2J trigger. If it did, the event
was kept. Otherwise, it was rejected. If the event was simulated, then a
weight given by the multiplication of the weights given by the L1, L2,
L3 and turnon curves for MET2J and by the prescale of MET2J was used for
that event. If the event was not in the tight jet kinematic region, but
it was in the looser kinematic region, then the procedure described
above was done for the MET45 trigger, with the caveat that if the data
event was in the particular run range where the MET45 trigger had a bug,
then the event was rejected even if the trigger fired. But how to model
that correctly in Monte Carlo simulation? Since the effect was small, on
the order of 2.6% (Table A.1 ), the effect was ignored and included in
the systematic uncertainty of the procedure.

The method described in the previous section does not use a local “OR”
between the MET2J and MET45 triggers in order to avoid the correlations
between the triggers. The price to pay is a smaller event acceptance.
The advantage is that systematic uncertainty is easier to calculate
correctly and is smaller than in the case of correlated triggers. The
main feature of the method described in the previous section is that in
the kinematic phase space of jet selection, a trigger is assigned to one
region and another trigger is assigned to another region. We stress that
this is done before the data events are checked if the trigger fire. The
choice of kinematic regions and the chosen trigger for each kinematic
region is based on an a priori study. In the case of the particular
method described in the previous section, the turnon curves for the
MET2J (Figure A.1 ) and the MET45 (Figure A.1 ) triggers were compared
and it was observed that the MET2J trigger has a turnon region on
smaller trigMET values than MET45 and therefore has a potential larger
efficiency than MET45. In the kinematic region that was not tight, but
it was loose, only MET45 trigger could be used, so there was no dilemma.
For the tight jet region, though, since correlation between triggers was
to be avoided, only one trigger had to be chosen and based on the
reasoning above this was MET2J.

This procedure increased the signal acceptance over MET2J only, but it
was clear that does not provide the maximum signal acceptance. For
example, for some events in the tight jet region it is possible that the
trigger weight is larger for MET45 than for MET2J, or that the event was
not selected by the MET2J stream due to a prescaling or other effect,
but was selected by the MET45 trigger. Why not divide this kinematic
region further into smaller kinematic regions and for each of them
reevaluate if to use the MET2J or the MET45 triggers? But how should
this division be done? And how to take into account the fact that for
2.6% of data events the trigger MET45 was not defined? And what about
using also the METDI trigger, which is defined only for about 60.8% of
the integrated luminosity (Table A.1 )? It seems there would be very
many kinematic regions and there would be a very complicated
bookkeeping.

In the next section we propose a general method that solves all these
problems for any number of triggers combining. We apply this method to
optimize the signal acceptance for our @xmath analysis.

### b.3 The Novel Method for Combining Triggers

The solution we propose in order to maximize the event selection while
using only one trigger per kinematic region with minimal bookkeeping is
that of considering the largest possible number of kinematic regions,
i.e. the number of events in that particular Monte-Carlo-simulated or
data sample. Our idea is to consider each event as an independent
kinematic region. Just as in the case of the method above, we study a
priori which is the most efficient trigger in the kinematic region,
choose that trigger and ignore the other triggers. Since the kinematic
region means that unique event, it means we choose the trigger that has
a priory the largest probability to fire that event. Below we will go
into the details of how such a probability is computed for every trigger
for the particular event. Once they are computed, though, we choose the
trigger as the one that has the largest a priory probability. If all
probabilities are strictly zero, then the event is rejected both if it
is a data event or a MC event. This is equivalent to a study done for a
particular kinematic region and choosing that all events in that
kinematic region are assigned to only one trigger, such as MET2J was
preferred to MET45 in the tight jet region of the previous method. For a
data event, the chosen trigger is checked if it fired the event. If it
has, the event is kept, or equivalently is assigned a weight of strictly
1.0. If the event did not fire the trigger, the event is thrown away, or
equivalently is assigned a weight of strictly 0.0, and the other
triggers are not checked at all. Ignoring the other triggers is the main
feature that allows the orthogonality between triggers. It is crucial
that the trigger is chosen before the trigger is checked if it fired.
For a Monte Carlo event it is assumed automatically that the trigger has
fired the event and the probability of the chosen trigger is returned as
the trigger weight.

The a priori probability that an event fires a particular trigger is
given by the product between the weight at L1 for that particular
trigger, the weight at L2 for that particular trigger, the weight at L3
for that particular trigger, the prescale for that particular trigger.

  -- -------- -- -------
     @xmath      (B.1)
  -- -------- -- -------

where @xmath , @xmath , @xmath are weights of that particular trigger at
L1, L2 and L3, respectively, are given by Formula A.1 , and vary as a
function of the event trigMET; @xmath represents the average prescale of
the evaluated trigger, with the same value for all events, given by
Formula B.2 ; @xmath represents the jet selection and is 1 if the jet
selection for that particular trigger is met by the event and zero if it
is not, given by Formula B.4 ; @xmath represents the condition if the
chosen trigger is defined for the event and is 1 if this happens and 0
if it does not.

  -- -------- -- -------
     @xmath      (B.2)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (B.3)
  -- -------- -- -------

  -- -------- -- -------
     @xmath      (B.4)
  -- -------- -- -------

For data events, @xmath is evaluated easily as each event contains the
information if the trigger is defined or not. For MET45 we also consider
the trigger is not defined for the 2.6% of the integrated luminosity
where the trigger has a bug. As seen in Table A.1 , for 0.13% of the
integrated luminosity no MET-based trigger is defined, for 2.53% of the
integrated luminosity only MET2J is defined, for 36.52% of the
integrated luminosity only MET2J and MET45 are defined and for 60.81% of
the integrated luminosity all three MET-based triggers are defined. The
simulation of this in Monte Carlo was the really tricky part with the
previous methods.

In our approach, for every Monte Carlo simulated event a random number
is chosen from a uniform distribution between 0 and 1. It represents the
probability that the event falls in any of the integrated luminosity
intervals based on which it is decided which triggers are defined or not
for the MC event. If the random number is in the interval
[0.0000-0.0013] then all triggers are assumed to be undefined and TD=0.0
for all triggers. For the interval [0.0013-0.0266] TD=1.0 for MET2J and
TD=0.0 for MET45 and METDI. For the interval [0.0266-0.3918] TD=1.0 for
MET2J and MET45 and TD=0.0 for METDI. For the interval [0.3918,1.000]
TD=1.0 for all three MET-based triggers.

Formula B.1 ensures that each trigger is allowed in the competition with
the other triggers to decide which is more likely to fire for the
particular event only if the trigger is meaningful for that particular
event, i.e. if the event passes the jet selection specific and necessary
for the trigger to be considered and if the trigger was actually defined
for that particular run to which the event belongs. If this does not
happen, the trigger ends up with a Probability of zero, which makes sure
the trigger does not win over the other triggers. If the probability is
different than zero, than it is the probability given by the turnon
curves at each of the three trigger values as well as the prescale of
that trigger.

We implemented this method into a software package called ABCDF ¹ ¹ 1
The name of the package comprises the author’s initials followed by
those of the Collider Detector at Fermilab experiment that allows for a
user-friendly inclusion of the method into an analysis. The ABCDF
package will take in the event information and return a probability
between 0 and 1 for Monte Carlo simulated events and either 0 or 1 for
data events. A selection cut of probability strictly larger than 0.001
is necessary to make sure the data events for which the chosen trigger
did not fire are rejected from the analysis, as they would have a weight
of strictly 0.0. ABCDF is part of the CDF software repository and can be
used by any analysis. In fact, it is being used already by several
analysis at CDF for the three MET-based triggers.

The method is a general one and can be used with any number of MET-based
triggers. Each trigger would come with its own specific kinematic
selection. Information other than jet information can be included in the
kinematic selection. This method was used in the analysis presented in
this dissertation and is currently being used by other analyses at CDF.
It also has potential applications at other experiments, as MET+jet
triggers are used for new physics searches such as supersymmetry at the
LHC experiments. The method can be used on other trigger types and even
the parametrization could be done as a function of another variable for
each different trigger, i.e. @xmath could be computed by different
formulae specific for each trigger.

The total Monte Carlo event count is the count of events that pass the
event selection weighted by the total Probability of the chosen trigger,
i.e. the largest Probability amongst the available triggers. The
systematic uncertainty is calculated easily by considering other turnon
curves in bins of several kinematic quantities enumerated in Section A.5
which changes on an event by event basis the @xmath values and may
change not only the Probability of each trigger, but also the chosen
trigger. The largest percentage difference for all systematic variations
with respect to the standard event count is typically considered to
constitute a sensible systematic uncertainty.

An assumption of our method is that the triggers are very efficient. The
assumption is a very good one in the case of our analysis. However, if
the triggers were very inefficient, an OR method would provide a
significantly higher even yield. One would have to model correctly the
correlation between triggers and compute the systematic uncertainty for
the OR trigger combination.

## Appendix C WHAM package in CDF

### c.1 Introduction

Associated WH production search Analysis Modules (WHAM) is a new data
analysis framework for the CDF collaboration. It builds on a previous
framework developed by the top quark study group inside CDF and adds
functionality through its improved modular structure. WHAM performs all
the analysis stages from data and Monte Carlo ntuples produced by the
CDF production group up to the final measurement, such as limits or
cross sections. I am one of the main contributors to WHAM code
development and validation. Also, WHAM has been used to produce the
results presented in this thesis.

### c.2 Motivation

The main motivation of WHAM is to perform a combination analysis between
two WH searches inside CDF: the one using as a discriminant an
artificial neural network (WHNN, the search presented in this thesis)
and the one using a boosted decision tree and matrix elements (WHME).
Studies inside CDF has shown that there is no 100 @xmath correlations
between the two discriminants and therefore more information could be
extracted by combining the two searches. This can be achieved if a
superdiscriminant is built that takes as inputs the event by event basis
values for discriminants of each analyses. CDF latest combination
between WHNN and WHME showed a 10 @xmath increase in sensitivity up to
the best performing analysis [ 2 ] .

The key words here are ”on an event by event basis”. That means we need
to make sure both analysis have the exact event selection. Also we have
to make sure both analyses reconstruct the same way various kinematic
quantities such as jet energies, missing transverse energies, dijet
invariant mass and have the same values for these on an event by event
basis. So far each analysis used its own framework, its own definition
of loose charged leptons and its own way for corrected various kinematic
quantities for various effects. This makes a combination between WHNN
and WHME very time and resource consuming. The combination cited above
is the only one achieved so far.

The goal of WHAM is to allow an easy combination of these two analyses
at any desired moment. This will allow CDF collaboration to present
combined and therefore improved results for the WH search at each
conference cycle.

But that is not all. WHAM allows very easily to perform searches that
have the same or very close signature as the WH one: Technicolour and
@xmath searches, as well as @xmath and single top measurements. So far
these analyses have had their own framework.

The fact that all these analyses could be done in a common framework
allows that a given improvement by one collaborator once integrated in
WHAM and validated for one analysis can be used automatically and thus
help improve all the other analyses as well.

One last point is that WHAM does all the steps from original data and
Monte Carlo simulations up to the final result with a minimal number of
actions from a user. In the context as the number of CDF collaborators
is decreasing, WHAM is very helpful in last years of CDF data taking and
data analyzing because one postdoctoral student will be able to update
very easily all these analyses with new data.

### c.3 Implementation

The WHAM code is contained in various folders with suggestive names and
where code with very specific goal is placed: setup, inputs, commands,
modules and results.

The folder ”setup” allows for the easy setup of the entire analysis
framework.

The folder ”inputs” contains all the inputs needed for the analyses:

-   Lists of files with events to be processed, either data or Monte
    Carlo signal or background simulated events;

-   Lists of data runs with good quality data;

-   Text files with information needed for the analysis such as analysis
    cuts, tasks to be performed, luminosities and data-simulation scale
    factors. This allows to change input parameters in the analysis
    without recompiling the code. Also, these input parameters are
    always saved with the results and they can be retrieved if in doubt
    on the input parameters of a given result.

The folder ”modules” contains in a modular way most of the code that
performs tasks in WHAM:

-   The folder ”dep” contains the dependencies files with extension .o
    and .d produced during the compilation of code;

-   The folder ”shlib” contains the shared objects with extension .so
    produced during the compilation of code

-   The folder ”inc” is the include directory that contains symbolic
    links to all the packages inside the ”modules” folder;

-   The folder ”external” contains all the packages already existing in
    the CDF software archive that WHAM uses, such as high level object
    reconstruction code, @xmath -tagging algorithm and their mistag
    matrices, background calculation methods, limit calculation method,
    manipulating .root files, boost ¹ ¹ 1 Boost provides free
    peer-reviewed portable C++ source libraries at http://www.boost.org
    . , NKRoot ² ² 2 NKRoot is a collection of tools for ROOT file
    manipulation needed for particle physics data analysis, developed by
    Nils Krumnack while a postdoc on CDF and still being developed as he
    is a researcher on ATLAS. The package is available for free to
    anyone at http://www-cdf.fnal.gov/~nils/root/ . and ABCDF ³ ³ 3
    ABCDF is the software package I developed in order to model the
    trigger simulation for missing energy + jets triggers and it is
    placed in the external package as it can be used by other CDF
    frameworks and analyses as well. .

The folder ”native” contains all the packages produced only for WHAM,
such as event reconstruction, various computations, such as the
discriminant output on an event by event basis, making analysis trees
from the original data or simulated events from cdf production group,
making histogram root files that are used for background estimation.

The folder ”commands” contains various tasks such as submitting
computing jobs to the CDF’s Central Analysis Farm (CAF), the limit
calculation code, various data analysis ROOT macros and various scripts
that read log files and compute things with that information or merge
existing root files.

The folder ”results” contains all the results of tasks from ”modules”.
Here we store the ROOT trees produced with WHAM using CAF that are used
then for background estimation and limit calculation, the limit results,
text files with enumerations of events that pass our event selection and
their kinematic properties (event dumps), histogram files used for
background calculation, various smaller trees for various studies such
as signal acceptance improvement or jet energy resolution improvement.

### c.4 Future Plans

At the moment that this thesis is submitted, only the WHNN analysis has
been performed and validated using the WHAM framework. Other analyses
are in progress, such as single top measurements and technicolour and
@xmath searches.

## Appendix D Control Plots

This appendix presents a selection of relevant plots for our analysis.
In each plot, all backgrounds, properly normalized, are stacked. Data
points are overlaid in order to show the good modelling of the
backgrounds. The two signal processes are also overlaid after having
been multiplied by a factor in order to be visible on the plots. Only
the most sensitive analysis channel (TIGHT SVTSVT) is shown in order not
to make the thesis too long. Since the Pretag category is a control
sample for each analysis channel, also the TIGHT Pretag category is
shown.

For each category, a collection of plots relevant to the kinematic
distribution of the event is shown, such as the transverse energy,
@xmath and @xmath of the two jets and the charged lepton, as well as the
@xmath , the transverse mass of the @xmath boson, the @xmath between the
@xmath and each jet and the charged lepton, followed by the @xmath
between the two jets. Also for each category a collection of plots
showing the variables used as input to the BNN final discriminant, as
well as the BNN output for a Higgs mass of 115 @xmath , are shown.

## Appendix E Glossary

-   @xmath -tagging  - The process of identifying if a jet originates in
    a bottom quark

-    ABCDF  - The software package I wrote for trigger combination

-    ALPGEN  - Monte Carlo event generator

-    ANN  - Artificial Neural Network

-    BNN  - The final analysis discriminant, the output of an artificial
    neural network

-    BSM  - Theories beyond the Standard Model theory

-    CDF  - Collider Detector at Fermilab

-    CEM  - Central Electromagnetic Calorimeter

-    CES  - Central Electromagnetic Shower Maximum Detector

-    CHA  - Central Hadronic Calorimeter

-    CL  - Confidence Level (in the frequentist approach) and
    Credibility Level (in the Bayesian approach)

-    CLC  - Cherenkov Luminosity Counter

-    CMP  - Central Muon uPgrade Detector

-    CMU  - Central Muon Detector

-    CMUP  - Muon candidates with hits both in the CMU and CMP detectors

-    CMX  - Central Muon eXtension Detector

-    COT  - Central Outer Tracker, the drift chamber used for tracking

-    CSL  - Consumer Server/Logger

-    DiTop  - The background sample of top quark pair production

-    FNAL  - Fermilab National Accelerator Laboratory

-    FSR  - Final State Radiation

-    ISL  - Intermediate Silicon Layers, the third subdetector of the
    silicon detector

-    ISOTRK  - The central loose charged lepton category (mainly loose
    muon candidates)

-    ISR  - Initial State Radiation

-    JES  - Jet Energy Scale, one of the most important sources of
    systematic uncertainty for this analysis, as well as the only shape
    systematics

-    JetProb  - Jet Probability @xmath -tagging algorithm

-    L00  - Layer 00, the first subdetector of the silicon detector

-    L1  - The first trigger level

-    L2  - The second trigger level

-    L3  - The third trigger level

-    LHC  - The Large Hadron Collider

-    MADEVENT  - Monte Carlo event generator

-    MC  - Monte Carlo simulation

-    MET  - Missing transverse energy

-    MET2J  - The first of the MET-based triggers

-    MET45  - The second of the MET-based triggers

-    METDI  - The third of the MET-based triggers

-    Obs  - The number of data events observed

-    PDF  - Parton Distribution Function

-    PEM  - Plug Electromagnetic Calorimeter

-    PES  - Plug Electromagnetic Shower Maximum Detector

-    PHA  - Plug Hadronic Calorimeter

-    PMT  - Photomultiplier Tube

-    Pretag  - The sample of events that pass all the event selection
    requirements, before any @xmath -tagging requirement is applied

-    PYTHIA  - Monte Carlo event generator and parton showering program

-    QCD  - The background sample of pure QCD production faking a @xmath
    boson production

-    SecVtx  - Secondary Vertex @xmath -tagging algorithm

-    SM  - The Standard Model of elementary particles and their
    interactions

-    STopS  - The background sample of single top production in the s
    channel

-    STopT  - The background sample of single top production in the t
    channel

-    SUSY  - The principle of supersymmetry, which lead to several
    theories beyond the Standard Model

-    SVT  - Secondary vertex reconstruction at the L2 trigger level

-    SVTJP05  - One @xmath -tagging category, where one jet is tagged by
    the Secondary Vertex algorithm and the other jet by the Jet
    Probability algorithm

-    SVTnoJP05  - One @xmath -tagging category, where one jet is tagged
    by the Secondary Vertex algorithm and the other jet is not tagged by
    the Jet Probability algorithm

-    SVTSVT  - One @xmath -tagging category, where both jets are tagged
    by the Secondary Vertex algorithm

-    SVX-II  - Silicon Vertex Detector, the second subdetector of the
    silicon detector

-    Technicolor  - A family of theories beyond the Standard Model

-    TIGHT  - The central tight charged lepton categories (central
    electrons and central muons)

-    TOF  - Time of Flight

-    Wbb  - The background sample of @xmath boson + @xmath

-    Wcc  - The background sample of @xmath boson + @xmath and @xmath
    boson + @xmath

-    WH  - The main signal process for the Higgs boson search described
    in this thesis

-    WH115  - The signal sample of @xmath associated production when the
    Higgs boson has a mass of 115 @xmath

-    WHA  - Wall Hadronic Calorimeter

-    WHAM  - WH Analysis Modules, the data analysis framework of which I
    am coauthor

-    Wlf  - The background sample of @xmath boson + light flavour jets
    incorrectly tagged as heavy flavour (mistags)

-    WW  - The background sample of @xmath

-    WZ  - The background sample of @xmath

-    XFT  - eXtremely Fast Tracker - track reconstruction at L1 trigger
    level

-    ZH  - The second signal process for the Higgs boson search
    described in this thesis

-    ZH115  - The signal sample of @xmath associated production when the
    Higgs boson has a mass of 115 @xmath

-    Zjets  - The background sample of @xmath boson + jets

-    ZZ  - The background sample of @xmath
