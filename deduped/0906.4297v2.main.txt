##### Acknowledgements. First and foremost, I would like to thank my
graduate advisor, Ingrid Daubechies, for directing me towards
interesting and challenging problems, and for giving me the confidence
to attack them. You are truly an inspiration, and I feel so proud and
fortunate to have had the opportunity to work with you.
I would also like to thank the signal processing community as a whole
for being friendly and warm to young people in the field. I have had
many wonderful interactions and established nice collaborations as a
result. In particular, I would like to thank Massimo Fornasier for being
a great colleague and friend. Working with you is so much fun! Your
attitude is infectious, and I can only hope to pass it along to others.
I am also particularly thankful to Robert Calderbank, Sinan Gunturk,
Albert Cohen, Thao Nguyen, and Ozgur Yilmaz for enlightening discussions
that strengthened this thesis immensely.
I am thankful to the graduate students in PACM for making me feel part
of a family. I have yet to encounter another department that gets along
as well as we do. I am thankful to Arie for helping me simplify several
of the arguments in this thesis, but more importantly for being by my
side during the dark times, and for yelling at me when I take work too
seriously. And finally, to my amazing family: your love and support
inspires and strengthens me every day. \dedication To my father, for
introducing me to the strange and wonderful world of mathematics. . “But
what did the bird see in the clear stream below? His own image; no
longer a dark, gray bird, ugly and disagreeable to look at, but a
graceful and beautiful swan.”

- The Ugly Duckling

### 0.1 Overview

The first part of the thesis is concerned with the acquisition of
real-valued, continuous-time signals, the likes of which we unknowingly
process every day: speech, music, video, and wireless communications are
all produced in this form, for example. At the same time, it is becoming
increasingly efficient to store and manipulate information in digital
format. This is partly due to the greater robustness of digital signals
versus analog signals with respect to noise and fluctuation: any slight
variation in an analog signal can cause a considerable amount of
distortion, but for digital sequences can be safely rounded off to the
nearest element in the discrete alphabet. The design and implementation
of fast, stable, and accurate algorithms for conversion between the
continuous and discrete domain is therefore crucial.
The process of analog-to-digital conversion usually consists of two
parts: sampling and quantization . Sampling consists of converting the
continuous-time signal of interest @xmath into a discrete-time signal
@xmath . According to the Shannon - Nyquist interpolation formula, this
operation is invertible if the analog signal of interest is uniformly
bounded and absolutely integrable, @xmath , and bandlimited , meaning
that its Fourier transform @xmath vanishes outside a bounded interval
@xmath . The importance of this result lies in the fact that many
signals of practical engineering interest are well-approximated by
bandlimited functions. For example, speech signals can be modelled as
bandlimited functions whose bandwidth @xmath is @xmath KHz, while audio
signals are well modelled as bandlimited functions whose bandwidth is
@xmath KHz [ 59 ] .
Specifically, @xmath -bandlimited functions @xmath can be recovered from
sufficiently dense evenly-spaced samples according to the following
reconstruction formula, which holds for @xmath :

  -- -- -- -----
           (1)
  -- -- -- -----

here, the reconstruction filter @xmath can be any function satisfying
@xmath , @xmath for @xmath , and @xmath for @xmath . At critical
sampling rate @xmath , or Nyquist rate , these restrictions determine
the Fourier transform @xmath as the indicator function for the interval
@xmath , yielding the sinc filter, @xmath sinc @xmath . As sinc @xmath
is not absolutely summable, sampling at the critical rate @xmath renders
the reconstruction ( 1.1 ) unstable in the presence of inevitable
additive error on the samples; if noisy input @xmath is instead
observed, and @xmath , then the sign pattern of the noise sequence
@xmath can be chosen adversarily so that the resulting series @xmath
does not even converge [ 31 ] .
In practice, reconstruction is stabilized by oversampling at a fixed
rate corresponding to @xmath . In this setting, one has the freedom to
design @xmath so that its Fourier transform @xmath is smooth, in which
case @xmath will have fast decay, and only those samples @xmath for
which @xmath is sufficiently small will contribute significantly towards
the reconstruction of @xmath .

Quantization in analog to digital conversion

The second step in analog to digital conversion is quantization, whereby
the real-valued sequence @xmath is encoded in a bitstream @xmath , with
@xmath bits @xmath allocated to each sample @xmath . From these bits,
the original function @xmath can be reconstructed in the analog domain,
albeit imperfectly, by replacing each sample @xmath in ( 1.1 ) by a
function of the @xmath -bit sequence @xmath . For example, this sequence
could be taken to be the first @xmath bits in the binary expansion of
@xmath - such pulse code modulation quantization schemes are often
employed in practice. In Chapter 1, we will study a related quantization
scheme, the @xmath -encoder, whereby expansions of @xmath in base @xmath
are considered in place of binary expansions (where @xmath ). Note that
quantization, unlike sampling, is no longer an invertible operation.
However, in any reasonable quantization scheme, the accuracy of the
approximation to @xmath from the quantization sequence @xmath will
increase as as function of @xmath , the number of bits spent per Nyquist
interval. That is, oversampling allows for more accurate as well as more
stable reconstruction, but at the expense of additional sampling
resources. The balance between accuracy, stability, and sampling
efficiency in signal acquisition will be a recurring theme throughout
this thesis.
In Chapters 1 and 2, we focus on the design of stable quantization
schemes, assuming that we may spend many bits per Nyquist interval in
acquiring samples @xmath . This is the case for audio signal processing,
where signals of interest have reasonable bandwidth @xmath KHz, and
sampling at a rate of e.g. @xmath bits per Nyquist interval corresponds
to using fewer than @xmath million bits per second. In determining the
stability of a particular quantization scheme to nonidealities in its
implementation, of particular importance is its sensitivity to
imperfections in the quantizer element @xmath that is necessary for
conversion from the analog to digital representation. @xmath is usually
taken to be the signum function @xmath , the behavior of which an actual
quantizer, having finite precision and subject to thermal fluctuations,
can only approximate. Actual quantizer elements usually come with a
pre-assigned tolerance @xmath : for input with magnitude below this
tolerance, @xmath , the output @xmath is not reliable. We will say that
a quantization scheme is robust with respect to quantization error if,
for some @xmath , the worst approximation error produced by the
quantization scheme can be made arbitrarily small by allowing a
sufficiently large bit budget, even if the quantizer used in its
implementation is imprecise to within a tolerance @xmath . Such
robustness can be achieved if the quantization sequence @xmath
constitutes a redundant representation of the original @xmath , so that
an incorrect bit @xmath caused by quantization error can be redeemed by
an appropriate choice of subsequent bits. Of course, any quantization
scheme of interest will require additional components in its
implementation, such as adders, multipliers, integrators, and so forth;
robustness with respect to imperfections in all these elements must be
taken into account in evaluating the reconstruction accuracy of a
particular scheme.
In Chapter 1, we investigate robustness properties of the @xmath
-encoder, a quantization scheme that was recently designed to be robust
with respect to quantization errors, while also having asymptotically
optimal reconstruction accuracy. We show that the @xmath -encoder is
also robust with respect to the other components in its implementation,
securing the validity of its reconstruction accuracy. Interestingly,
such robustness is achieved by enforcing a slight shift in the quantizer
element, @xmath ; that is, we take advantage of the freedom to choose an
imperfect quantizer, offered by redundancy, in order to ensure
additional robustness guarantees.
In Chapter 2, we focus on a different notion of stability in the design
of quantization schemes. Sigma Delta ( @xmath ) quantization, and in
particular one-bit @xmath modulation, is a coarse quantization method,
as each sample value @xmath in ( 1.1 ) is replaced by a single bit
@xmath that depends on all previous @xmath . Because of their robustness
properties and ease of implementation, @xmath schemes are widely popular
in practice, despite suffering from an entirely different kind of
problem: in audio signal processing, periodic oscillatory patterns in
the bit output @xmath often occur, producing idle tone components in
response to stretches of zero input @xmath , or even small amplitude
sinusoidal inputs. Such tones are audible to the listener, and are so
pronounced in low-order @xmath quantizers that such schemes are often
avoided in audio applications for more complex higher order @xmath
schemes [ 51 ] . As one of the contributions of this thesis, we
introduce a second-order @xmath scheme that eliminates such
periodicities by damping the internal variables of the system once the
input @xmath is identically zero. This approach is in line with the
philosophy of Chapter 1: to eliminate the idle tones, we exploit the
freedom to induce a small amount of leakage to the system without
sacrificing accuracy, as offered by redundancy of the @xmath
quantization output.

Beyond Nyquist sampling

The second part of the thesis concerns compressed sensing , a fast
emerging area of applied mathematics that represents a new approach to
traditional sampling paradigms. At the core of this field is the
remarkable fact that a large class of underdetermined linear systems
@xmath are invertible, and can be inverted efficiently, subject to the
constraint that @xmath is sufficiently sparse , or has sufficiently
small nonzero support @xmath . In particular, a matrix @xmath with unit
normed columns is said to have the ( @xmath )- restricted isometry
property , or satisfy @xmath -RIP for short, if all singular values of
any @xmath column submatrix of @xmath lie in the interval @xmath for a
given constant @xmath . For @xmath -RIP matrices, a @xmath -sparse
vector @xmath can be recovered from its lower-dimensional image @xmath
as the minimal @xmath norm solution,

  -- -------- -- -----
     @xmath      (2)
  -- -------- -- -----

That is, @xmath -RIP for the matrix @xmath furnishes an equivalence
between the minimal @xmath norm and minimal @xmath norm solution from
the affine space @xmath if a @xmath -sparse solution exists; while
finding the former solution represents an NP-hard problem in general,
the latter solution can be recovered efficiently using linear
programming methods. Even more can be said: for vectors @xmath
satisfying @xmath that are approximately but not exactly @xmath -sparse,
the error @xmath incurred by the approximation @xmath using @xmath -RIP
matrix @xmath is still small, thanks to the following stability result:

  -- -------- -- -----
     @xmath      (3)
  -- -------- -- -----

In words, the error @xmath is bounded by a fraction of the best possible
approximation error between @xmath and the set of @xmath -sparse vectors
in the metric of @xmath [ 15 ] .
Constructing @xmath -RIP matrices With high probability, an @xmath
random matrix whose coefficients are drawn as independent and identical
realizations of a Gaussian or Bernoulli random variable will satisfy
@xmath -RIP of optimal order,

  -- -------- -- -----
     @xmath      (4)
  -- -------- -- -----

While no deterministic constructions of RIP matrices have been found of
this order, many matrices with a smaller degree of randomness satisfy
the restricted isometry property to almost optimal order. Most notably,
with high probability an @xmath matrix obtained by selecting @xmath rows
at random from the @xmath discrete Fourier matrix satisfies @xmath -RIP
of order @xmath [ 66 ] . This particular result has an interesting
interpretation that connects back to the first two chapters: signals
that are well-approximated as elements from the class of sparse
trigonometric polynomials ,

  -- -------- -- -----
     @xmath      (5)
  -- -------- -- -----

can be efficiently reconstructed from only @xmath samples; when @xmath
is small, this represents an exponentially smaller number than the
@xmath samples required for the Shannon-Whittaker reconstruction formula
( 1.1 ) to hold. This is significant in applications such as radar,
navigation, and satellite communications, where signals of interest
often have a sparse frequency representation, while measurements are at
the same time expensive to implement.

Cross validation in compressed sensing

From the reconstruction formula ( 3 ) and order relation ( 4 ), the
quality of the approximation @xmath to an unknown signal @xmath in the
affine space @xmath depends on the approximability of @xmath is by its
@xmath largest coordinates. In the literature, a known and fixed value
for @xmath is assumed to well-approximate all signals in the class of
interest, while more realistically, @xmath may be a parameter of the
unknown input. Natural images, for instance, are generally
well-approximated by only a few discrete Wavelet basis elements ;
however, certain heavily textured images, such as those containing hair
or sand, are not particularly compressible in such bases. In statistics,
parameter selection and noise estimation are commonly achieved through
cross validation , whereby the available data is separated into
independent testing and training sets. In Chapter 3, we show that cross
validation incorporates naturally into the compressed sensing paradigm,
as the random measurements often used for compressed sensing are the
same measurements that provide almost-isometric lower dimensional
embeddings of generic point sets, as guaranteed by the
Johnson-Lindenstrauss lemma. More precisely, we take a set of @xmath
measurements of the unknown @xmath , and use @xmath of these
measurements, @xmath , in a compressed sensing decoding algorithm to
return a sequence @xmath of candidate approximations to @xmath . The
remaining @xmath measurements, @xmath , are then used to identify from
among this sequence a ‘best’ approximation @xmath , along with an
estimate of the sparsity level of @xmath . The proposed method for error
estimation in compressed sensing is extremely cheap : approximation
errors of up to @xmath distinct approximations @xmath can be estimated
with high accuracy at the expense of only @xmath samples. Whereas in
Chapters 1 and 2 we exploited redundancy of representation afforded by
oversampling to provide stability results for quantization schemes, we
now exploit the stability of random measurements to validate the
assumption that the representation at hand is redundant.

Compressed sensing and free-discontinuity problems

Chapter 4 explores the connection between minimization problems arising
in compressed sensing and those corresponding to free-discontinuity
problems , which describe situations where the solution of interest is
defined by a function and a lower dimensional set consisting of the
discontinuities of the function. Such situation arise, for instance, in
crack detection from fracture mechanics [ 65 ] or in certain digital
image segmentation problems [ 41 ] . The best-known example of a
free-discontinuity problem is that of minimizing the so-called
Mumford-Shah functional [ 22 ] , which is defined by

  -- -------- --
     @xmath   
  -- -------- --

here, the set @xmath is a bounded open subset of @xmath , the constants
@xmath are fixed, @xmath , and @xmath denotes the @xmath -dimensional
Hausdorff measure. In the context of visual analysis, the function
@xmath is a given noisy image that is to be approximated by the
minimizing function @xmath ; the set @xmath is simultaneously is used in
order to segment the image into connected components.
The Mumford-Shah functional @xmath is not easily handled, theoretically
or numerically speaking. This difficulty has given rise to the
development of approximation methods for the Mumford-Shah functional and
its minimizers where sets are no longer involved [ 22 ] . In the
one-dimensional setting, minimizers of @xmath can be approximated in a
strong sense by minimizers of a discrete functional which, reformulated
in terms of the discrete derivative, and generalized to incorporate
inverse problems, takes the form of a selective least squares problem,

  -- -------- -- -----
     @xmath      (6)
  -- -------- -- -----

Note that the unknown discrete derivative vector, assumed to be ‘small’
and smoothly varying except on the discontinuity set of the solution,
should be well-approximated by the minimizer of @xmath . However,
despite successful numerical results [ 24 ] observed for such selective
least squares problems, no rigorous results on the existence of
minimizers, let alone on the convergence of several proposed algorithms
to such minimizers, are currently available in the literature.
In Chapter 4, we establish a connection between the selective least
squares problem ( 6 ) and the @xmath minimization problem from
compressed sensing,

  -- -------- -- -----
     @xmath      (7)
  -- -------- -- -----

using the notion of gamma convergence. Moreover, we derive an iterative
thresholding algorithm for finding solutions to the selective least
squares functional @xmath , motivated by the recent application of such
algorithms for minimizing the @xmath regularized functional ( 7 ). Our
algorithm is shown to converge to local minimizers @xmath of the
functional @xmath that are characterized by certain fixed point
conditions, including the following gap property: for each @xmath ,
either @xmath or @xmath . We show in addition that any global minimizer
of @xmath must satisfy such fixed point conditions, giving mathematical
justification to the observation that minimizers of @xmath tend to be
“cartoon”-like, or segmented into regions of small gradient, separated
by edges corresponding to large gradient. Moreover, we show that
minimizers are restricted to regions where the non-convex functional
@xmath is locally strictly convex. Thus, global minimizers are always
isolated, although not necessarily unique, whereas local minimizers may
constitute a continuum of unstable equilibria. These observations sheds
light on fundamental properties, virtues, and limitations, of
regularization by means of the Mumford-Shah functional, and provide a
rigorous justification of the numerical results appearing in the
literature.

Contributions of this thesis

Chapter 1. The @xmath -encoder and golden ratio encoder were recently
introduced as quantization schemes in analog to digital conversion that
are simultaneously robust with respect to quantizer imperfections in
their analog implementation and efficient in their bit-budget use.
However, it was not clear at first [ 34 ] that these schemes were also
robust with respect to imprecisions in the multiplier element. Using a
result in analytic number theory concerning the roots of power series
with coefficients in @xmath , we show that these schemes are indeed
robust as such, and the underlying value of @xmath can be reconstructed
as the unique root on @xmath of a power series obtained from the
quantization output of test input @xmath .

Chapter 2. Sigma Delta ( @xmath ) modulation is a course quantization
method in analog to digital conversion that is widely used for its
simplicity of implementation and robustness properties; however, a
persistent problem in @xmath modulation is the occurrence of undesirable
idle tones arising from oscillatory patterns in the bit output. Such
oscillations are omnipresent, and automatically arise along stretches of
zero input, the likes of which are unavoidable in audio signal
processing. Responding to a question left open in [ 79 ] , we introduce
a family of quiet second-order 2-bit asymmetrically damped @xmath
schemes whose quantization output becomes constant at the onset of zero
input, effectively eliminating the idle tones that arise in this
context.

Chapter 3. The emerging area of compressed sensing boasts efficient
sensing techniques based on the phenomenon that an @xmath dimensional
real-valued vector can be reconstructed efficiently to within a factor
of its best @xmath -term approximation error by taking only @xmath
measurements. However, the quality of such a reconstruction is not
assured if the underlying sparsity level of the signal is unknown. We
show that sharp bounds on the errors between a signal and @xmath
approximations to the signal (corresponding to a different sparsity
hypotheses, say) can be achieved by reserving only @xmath measurements
of the total @xmath measurements for this purpose. This error estimation
technique is reminiscent of cross validation in statistics and learning
theory, and theoretical justification in the context of compressed
sensing is provided by the Johnson Lindenstrauss lemma.

Chapter 4. Inspired by recent thresholding techniques in compressed
sensing, we develop an algorithm for minimizing a discrete version of
the Mumford Shah (MS) functional arising in image segmentation [ 21 ] .
Although regularization methods involving similar nonconvex functionals
work well in practice [ 63 ] , ours is the first proven to converge to
local minimizers of the discrete MS functional. The proposed algorithm
can be adapted to a more general class of nonconvex functionals that
includes the @xmath -regularized functional from compressed sensing.
Finally, we show that the @xmath regularized functional can be viewed as
the limit of a sequence of free-discontinuity type problems,
illuminating an intimate connection between the two problems.

## Chapter 1 Robust quantization in analog to digital conversion

### 1.1 Analog to digital conversion: an introduction

Digital signals are omnipresent; one important reason is that
transmission and storage for digital signals is much more robust with
respect to noise and fluctuation than for analog signals. Because many
signals of interest to us are produced in analog form, a transition from
analog to digital is therefore necessary.
Analog-to-digital (A/D) conversion consists of two parts: sampling and
quantization . The process of sampling, followed by quantization, can be
schematically represented by

@xmath

where @xmath is a finite sequence of digits from a finite (and usually
binary) alphabet. It is reasonable to assume that the real-valued analog
signal of interest, @xmath , is uniformly bounded and absolutely
integrable, @xmath , and is bandlimited ; i.e., its Fourier transform
@xmath vanishes outside some bounded interval @xmath . Under this
assumption, the sampling step in A/D conversion is an invertible
operation, as @xmath can be reconstructed from the samples @xmath as
long as @xmath , via the reconstruction formula

  -- -- -- -------
           (1.1)
  -- -- -- -------

Above, @xmath can be any function such that @xmath for @xmath and @xmath
for @xmath . If @xmath is smooth, then @xmath will have fast decay and
the reconstruction @xmath will be almost entirely local . By scaling and
normalizing appropriately, we can reduce any bandlimited function in
@xmath to an element of the class @xmath , in which case the
reconstruction formula @xmath reduces to

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

The second step in A/D conversion is quantization , in which sample
values @xmath are replaced by finite bitstreams @xmath and associated
function @xmath , so that the original function can be reconstructed at
a later time by the approximation

  -- -------- -- -------
     @xmath      (1.3)
  -- -------- -- -------

Quantization, unlike sampling, is not in general an invertible
operation. That is, if @xmath is the operator which maps functions in
@xmath to bitstreams (the encoding operator) associated with a
particular quantization scheme, and @xmath is the decoding operator
which maps bitstreams back to functions in @xmath , then typically
@xmath . In order to measure the performance of a particular
quantization scheme, one considers the distortion of the scheme, given
by @xmath , where @xmath is the norm of interest; e.g.,the @xmath norm
on @xmath . In any reasonable quantization scheme, the distortion will
decrease as the number of bits per unit interval (the so-called bit
budget ) increases; schemes whose distortion decreases faster as a
function of the bit budget are generally considered superior
quantization schemes.
If the relation between the distortion @xmath and the bit budget @xmath
were the only important measure associated with a quantization scheme,
then the widely-used pulse code modulation (PCM) quantization algorithm
would always be preferred in practice. Given a function @xmath in @xmath
, a @xmath -bit PCM algorithm simply replaces each sample value @xmath
with @xmath bits: one bit for its sign, followed by the first @xmath
bits of the binary expansion of @xmath . One can show that for signals
@xmath in @xmath , and for fixed @xmath , the distortion @xmath for an
@xmath -bit PCM, when, for instance, @xmath is the @xmath norm on @xmath
.

Sigma Delta ( @xmath ) quantization

Another popular quantization algorithm, @xmath modulation , has
distortion that decays only like an inverse polynomial in the bit budget
@xmath . One-bit @xmath modulation replaces each sample @xmath in @xmath
by a single bit, @xmath (In multibit @xmath schemes, the coefficients
@xmath replacing the @xmath can assume a larger range of discrete
values, and thus require several bits). In contrast to PCM, the @xmath
in @xmath depend not only on @xmath , but on all previous @xmath ,
@xmath . Consider first-order, one-bit @xmath , where the @xmath are
generated recursively by the scheme,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.4)
  -- -------- -------- -------- -- -------

The quantizer @xmath just returns the sign of its argument, which can be
taken to be either @xmath or @xmath when @xmath . Assuming @xmath (which
is true of functions @xmath ), and initializing @xmath , a simple
inductive argument guarantees that @xmath for all @xmath [ 31 ] . Using
this, along with the observation that @xmath for one-bit quantization
schemes, we arrive at an error estimate @xmath for the first-order
@xmath scheme @xmath . More generally, the @xmath order analog of the
recursion @xmath equates @xmath to the @xmath order (as opposed to first
order) difference of a bounded sequence @xmath , and has corresponding
decay @xmath ; in particular, the second-order one-bit @xmath
quantization scheme can be recast in the form

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.5)
  -- -------- -------- -------- -- -------

The function @xmath can be any map that guarantees the boundedness of
the state variables @xmath ; for example, the linear function @xmath
suffices for an appropriate range of @xmath [ 79 ] .

Robustness for @xmath quantization

Clearly, the @xmath decay of PCM will outperform the @xmath decay of a
@xmath -th order one-bit @xmath scheme, for any order @xmath . Yet,
@xmath schemes of as low as second order are preferred in practice over
high order PCM quantization schemes. This can be partly understood by
the fact that @xmath schemes are less sensitive than PCM schemes to
inevitable errors in the analog components of their implementation. The
notion of of @xmath schemes to quantization error, which was first
studied in [ 31 ] , is connected to how well @xmath schemes exploit the
redundancy of the representation @xmath afforded by oversampling @xmath
. By redundancy, we refer to the fact that the functions @xmath form a
frame ¹ ¹ 1 Technically, the shifts of @xmath are not a frame for the
space of @xmath -bandlimited functions because these functions don’t
live in this space. But this is only a technicality due to the
definition of a frame; the function @xmath lives in the larger Hilbert
space of @xmath -bandlimited functions, or simply @xmath for that
matter, and its shifts satisfy the frame property for the smaller space
of @xmath -bandlimited functions. for the Hilbert space of @xmath
-bandlimited functions, and their frame redundancy increases with @xmath
. One source of quantization error incurred in both @xmath and PCM
quantization comes from the quantizer function itself: the function
@xmath that is used in both the @xmath schemes and in the recursive
algorithms used to generate binary expansions in PCM cannot be built to
have infinite precision. In fact, quantizer elements for A/D circuits
generally come with a prescribed tolerance @xmath for which the output
@xmath of such quantizers should not be trusted once @xmath (and of
course, quantizers with lower tolerance are more expensive). As the
binary expansion of almost every real number is unique, an incorrect bit
assignment, and especially an incorrect initial bit, in the truncated
binary expansion of a sample @xmath will cause an error in the resulting
approximation that obviates the possibility of @xmath decay. @xmath
schemes, on the other hand, keep track of prior samples @xmath , @xmath
, in such a way that errors caused by imprecise quantizers can be
corrected later, and the @xmath maintained, by exploiting the redundant
information in the samples @xmath .
This discussion brings us to the question: Is it possible to have the
best of both worlds ? That is, can one design a quantization scheme that
has exponential reconstruction guarantees of PCM while also being robust
with respect to quantization imperfections like @xmath ? This question
was answered affirmatively in [ 33 ] , with the introduction of the
@xmath -encoder.

### 1.2 On the robustness of beta-encoders and golden ratio encoders

Beta-encoders are similar to PCM in that they replace each sampled
function value @xmath with a truncated series expansion in a base @xmath
, where @xmath , and with binary coefficients. Clearly, if @xmath , then
this algorithm coincides with PCM. However, whereas the binary expansion
of almost every real number is unique, for every @xmath , there exist a
continuum of different @xmath expansions of almost every @xmath in
@xmath (see ). It is precisely this redundancy that gives beta-encoders
the freedom to correct errors caused by imprecise quantizers shared by
@xmath schemes. Whereas in @xmath , a higher degree of robustness is
achieved via finer sampling, beta-encoders are made more robust by
choosing a smaller value of @xmath as the base for expansion.
Although beta-encoders as discussed in are robust with respect to
quantizer imperfections, these encoders are not as robust with respect
to imprecisions in other components of their circuit implementation.
Typically, beta-encoders require a multiplier in which real numbers are
multiplied by @xmath . Like all analog circuit components, this
multiplier will be imprecise; that is, although a known value @xmath may
be set in the circuit implementation of the encoder, thermal
fluctuations and other physical limitations will have the effect of
changing the true multiplier to an unknown value @xmath within an
interval of the pre-set value @xmath . The true value @xmath will vary
from device to device, and will also change slowly in time within a
single device. This variability, left unaccounted for, disqualifies the
beta-encoder as a viable quantization method since the value of @xmath
must be known with exponential precision in order to reconstruct a good
approximation to the original signal from the recovered bit streams.
We overcome this potential limitation of the beta-encoder by introducing
a method for recovering @xmath from the encoded bitstreams of a real
number @xmath and its negative, @xmath . Our method incorporates the
techniques used in , but our analysis is simplified using a
transversality condition, as defined in , for power series with
coefficients in @xmath . As the value of @xmath can fluctuate within an
interval @xmath over time, our recovery technique can be repeated at
regular intervals during quantization (e.g., after the quantization of
every 10 samples).
The golden ratio encoder (GRE) was proposed in as a quantizer that
shares the same robustness and exponential rate-distortion properties as
beta-encoders, but that does not require an explicit multiplier in its
circuit implementation. GRE functions like a beta-encoder in that it
produces beta-expansions of real numbers; however, in GRE, @xmath is
fixed at the value of the golden ratio, @xmath . The relation @xmath
characterizing the golden ratio permits elimination of the multiplier
from the encoding algorithm. Even though GRE does not require a precise
multiplier, component imperfections such as integrator leakage in the
implementation of GRE may still cause the true value of @xmath to be
slightly larger than @xmath ; in practice it is reasonable to assume
@xmath . Our method for recovering @xmath in general beta-encoders can
be easily extended to recovering @xmath in the golden ratio encoder.
Our work in this section will be organized as follows:

1.  In sections @xmath and @xmath , we review relevant background on
    beta-encoders and golden ratio encoders, respectively.

2.  In section @xmath , we introduce a more realistic model of the
    golden ratio encoder that takes into account the effects of
    integrator leak in the delay elements of the circuit. We show that
    the output of this revised model still correspond to truncated
    beta-expansions of the input, but in an unknown base @xmath that
    differs from the pre-set value.

3.  Section @xmath describes a way to recover the unknown value of
    @xmath up to arbitrary precision using the bit streams of a ‘test’
    number @xmath , and @xmath . The recovery scheme reduces to finding
    the root of a polynomial with coefficients in @xmath .

4.  Section @xmath extends the recovery procedure of the previous
    section to the setting of beta-encoders having leakage in the
    (single) delay element of their implementation. We show that the
    analysis in this case is completely analogous to that of Section
    @xmath .

#### 1.2.1 The beta-encoder

In this section, we summarize certain properties of beta-encoders (or
@xmath -encoders) with error correction, from the perspective of
encoders which produce beta expansions with coefficients in @xmath . For
more details on beta-encoders, we refer the reader to .
We start from the observation that given @xmath , every real number
@xmath admits a sequence @xmath , with @xmath , such that

  -- -------- -- -------
     @xmath      (1.6)
  -- -------- -- -------

Under the transformation @xmath , @xmath is equivalent to the
observation that every real number @xmath admits a beta-expansion in
base @xmath , with coefficients @xmath . Accordingly, all of the results
that follow in this section have straightforward analogs in terms of
@xmath -beta-expansions; see for more details.
One way to compute a sequence @xmath that satisfies @xmath is to run the
iteration

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.7)
  -- -------- -------- -------- -- -------

where the quantizer @xmath is simply the sign-function,

  -- -------- -- -------
     @xmath      (1.8)
  -- -------- -- -------

For @xmath , the expansion @xmath is unique for almost every @xmath ;
however, for @xmath , there exist uncountably many expansions of the
type @xmath for any @xmath (see ). Because of this redundancy in
representation, beta encoders are robust with respect to quantization
error, while PCM schemes are not. We now explain in more detail what we
mean by quantization error. The quantizer @xmath in @xmath is an
idealized quantizer; in practice, one has to deal with quantizers that
only approximate this ideal behavior. A more realistic model is obtained
by replacing @xmath in @xmath with a ‘flaky’ version @xmath , for which
we know only that

  -- -------- -- -------
     @xmath      (1.9)
  -- -------- -- -------

In practice, @xmath is a quantity that is not known exactly, but over
the magnitude of which we have some control, e.g. @xmath for a known
@xmath . This value @xmath is called the tolerance of the quantizer. We
shall call a quantization scheme robust with respect to quantization
error if, for some @xmath , the worst approximation error produced by
the quantization scheme can be made arbitrarily small by allowing a
sufficiently large bit budget, even if the quantizer used in its
implementation is imprecise to within a tolerance @xmath . According to
this definition, the naive @xmath -binary expansion is not robust. More
specifically, suppose that a flaky quantizer @xmath is used in @xmath to
compute the base-2 expansion of a number @xmath which is sufficiently
small that @xmath . Since @xmath is within the flaky zone for @xmath ,
if @xmath is assigned incorrectly; i.e., if @xmath differs from the sign
of @xmath , then no matter how the remaining bits are assigned, the
difference between @xmath and the number represented by the computed
bits will be at least @xmath . This is not the case if @xmath , as shown
by the following whose proof can be found in :

###### Theorem 1.2.1.

Let @xmath and @xmath . Suppose that in the beta-encoding of @xmath ,
the procedure @xmath is followed, but the quantizer @xmath is used
instead of the ideal @xmath at each occurence, with @xmath satisfying
@xmath . Denote by @xmath the bit sequence produced by applying this
encoding to the number @xmath . If @xmath satisfies

@xmath

then

  -- -------- -- --------
     @xmath      (1.10)
  -- -------- -- --------

with @xmath .

For a given tolerance @xmath , running the recursion @xmath with a
quantizer @xmath of tolerance @xmath and a value of @xmath in @xmath
produces bitstreams @xmath corresponding to a beta-expansion of the
input @xmath in base @xmath ; however, the precise value of @xmath must
be known in order recover @xmath from such a beta-expansion. As
mentioned in the introduction and detailed in the following section,
component imperfections may cause the circuit to behave as if a
different value of @xmath is used, and this value will possibly be
changing slowly over time within a known range, @xmath . In , a method
is proposed whereby an exponentially precise approximation @xmath to the
value of @xmath at any given time can be encoded and transmitted to the
decoder without actually physically measuring its value, via the encoded
bitstreams of a real number @xmath and @xmath . This decoding method can
be repeated at regular time intervals during the quantization procedure,
to account for the possible time-variance of @xmath . That an
exponentially precise approximation @xmath to @xmath is sufficient to
reconstruct subsequent samples @xmath with exponential precision is the
content of the following theorem, which is essentially a restatement of
Theorem 5 in .

###### Theorem 1.2.2 (Daubechies, Yilmaz).

Consider @xmath and @xmath , or @xmath and @xmath . Suppose @xmath is
such that @xmath . Suppose @xmath is such that @xmath for some fixed
@xmath . Then @xmath satisfies

  -- -------- -- --------
     @xmath      (1.11)
  -- -------- -- --------

where @xmath is a constant which depends only on @xmath and @xmath .

Although the approach proposed in for estimating @xmath from bitstreams
overcomes potential approximation error caused by imprecise multipliers
in the circuit implementation of the beta-encoder, new robustness issues
are nevertheless introduced. Typically, one cannot ensure that the
reference level 1 in @xmath is known with high precision. To circumvent
this problem, the authors consider other heuristics whereby @xmath is
recovered from clever averaging of multiple pairs @xmath and @xmath .
These heuristics do not require that the reference level 1 in @xmath be
precise; however, these approaches become quite complicated in and of
themselves, and any sort of analytical analysis of their performance
becomes quite difficult. As one of the contributions of the present
work, we present an approach for recovering @xmath that is inspired by
the approach in [ 34 ] but does not require a precise reference level,
yet still allows for exponentially precise approximations to @xmath .

#### 1.2.2 The golden ratio encoder (GRE)

As shown in the previous section, beta-encoders are robust with respect
to imperfect quantizer elements, and their approximation error decays
exponentially with the number of bits @xmath . To attain this
exponential precision, @xmath must be measured with high precision,
which is quite complicated in practice. These complications motivated
the invention of the golden ratio encoder (GRE) of , which has the same
robustness and rate-distortion properties as beta-encoders, but uses an
additional delay element in place of precise multiplier in its
implementation. That is, if one implements the recursion @xmath with
@xmath , then using the relation @xmath , one obtains the recursion
formula @xmath . If the term @xmath in this formula is removed, then the
resulting recursion @xmath should look familiar; indeed, with initial
conditions @xmath , this recursion generates the Fibonacci numbers
@xmath , and it is well-known that the sequence @xmath as @xmath . If
@xmath is instead replaced by a single bit taking values in @xmath ,
then we are led to the following scheme:

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.12)
  -- -------- -------- -------- -- --------

In this paper, we will consider quantizers @xmath in @xmath of the form
@xmath , where

  -- -------- -- --------
     @xmath      (1.13)
  -- -------- -- --------

along with their flaky analogs,

  -- -------- -- --------
     @xmath      (1.14)
  -- -------- -- --------

In , the authors consider the recursion formula @xmath implemented with
flaky @xmath -quantizers of the form @xmath . For the simplicity of
presentation, we will consider only the @xmath -quantizers @xmath , but
many of our results extend straightforwardly to quantizers of the type
@xmath .
The following theorem was proved in ; it shows that as long as @xmath
and @xmath are such that the state sequence @xmath remains bounded, a
golden ratio encoder (corresponding to the recursion @xmath ) will
produce a bitstream @xmath corresponding to a beta-expansion of @xmath
in base @xmath , just as does the beta-encoder from which the GRE was
derived.

###### Theorem 1.2.3 (Daubechies, Güntürk, Wang, Yilmaz).

Consider the recursion @xmath . Suppose the 1-bit quantizer @xmath which
outputs bits @xmath in @xmath is of the type @xmath such that the state
sequence @xmath with @xmath and @xmath is bounded. Then

  -- -------- -- --------
     @xmath      (1.15)
  -- -------- -- --------

Here @xmath is the golden ratio.

###### Proof.

Note that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
                                
              @xmath   @xmath   
                       @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The second to last equality uses the relation @xmath , and the last
equality is obtained by setting @xmath and @xmath . Since the state
sequence @xmath is bounded, it follows that @xmath . Thus,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

Although the implementation of GRE requires 2 more adders and one more
delay element than the implementation of the beta-encoder, the
multiplier element @xmath in GRE does not have to be precise (see
section 6), whereas imprecisions in the multiplier element @xmath of the
beta-encoder result in beta-expansions of the input @xmath in a
different base @xmath .

#### 1.2.3 GRE: A revised scheme incorporating integrator leak

In modeling the golden ratio encoder by the system @xmath , we are
assuming that the delay elements used in its circuit implementation are
ideal. A more realistic model would take into account the effect of
integrator leak, which is inevitable in any practical circuit
implementation (see for more details). After one clock time, the stored
input in the first delay is reduced to @xmath times its original value,
while the stored input in the second delay is replaced by @xmath times
its original value. In virtually all circuits of interest, no more than
@xmath percent of the stored input is leaked at each timestep; that is,
we can safely assume that @xmath and @xmath are parameters in the
interval [.9,1]. The precise values of these parameters may change in
time; however, as virtually all practical A/D converters produce over
1000 bits per second (and some can produce over 1 billion bits per
second), we may safely assume that @xmath and @xmath are constant
throughout the quantization of at least every 10 samples.
Fixing an input value @xmath , we arrive at the following revised
description of the golden ratio encoder (revised GRE):

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.16)
  -- -------- -------- -------- -- --------

Obviously, @xmath corresponds to the original model @xmath . It is
reasonable to assume in practice that @xmath , and in virtually all
cases @xmath .
We will show that the revised scheme @xmath still produces
beta-expansions of the input @xmath , but in a slightly different base
@xmath , which increases away from @xmath as the parameters @xmath and
@xmath decrease. Key in the proof of Theorem was the use of the relation
@xmath to reduce @xmath to the sum of the input @xmath , and a remainder
term that becomes arbitrarily small with increasing @xmath .
Accordingly, the relation @xmath gives @xmath , where @xmath goes to
@xmath as @xmath goes to infinity.

###### Theorem 1.2.4.

Suppose the 1-bit quantizer @xmath in @xmath of type @xmath is such that
the state sequence @xmath with @xmath and @xmath is bounded. Consider
@xmath . Then

@xmath

where @xmath .

###### Proof.

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
                       @xmath   
                       @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The last equality is obtained by setting @xmath and @xmath . Since the
@xmath are bounded, it follows as in the proof of @xmath that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

∎

Theorem implies that if @xmath is known, then the revised GRE scheme
@xmath still gives exponential approximations to the input signal @xmath
, provided that the @xmath are indeed bounded. The following theorem
gives an explicit range for the parameters @xmath which results in
bounded sequences @xmath when the input @xmath , independent of the
values of the leakage parameters @xmath in the set @xmath . This
parameter range is only slightly more restrictive than that derived in
for the ideal GRE scheme @xmath ; that is, the admissable parameter
range for @xmath and @xmath is essentially robust with respect to
leakage in the delay elements of the GRE circuit implementation.

###### Theorem 1.2.5.

Let @xmath , and @xmath . Suppose that the GRE scheme @xmath is
followed, and the quantizer @xmath is used, with @xmath possibly varying
at each occurrence, but always satisfying @xmath for some fixed
tolerance @xmath . If the parameter @xmath takes values in the interval
@xmath , then the resulting state sequence @xmath is bounded.

We leave the proof of Theorem to the appendix. This result in some sense
parallels Theorem in that an admissable range for the ”multiplication”
parameter ( @xmath in GRE, @xmath in beta-encoders) is specified for a
given quantizer tolerance @xmath ; however, we stress that the specific
value of @xmath is needed in order to recover the input from the
bitstream @xmath in beta-encoders. In contrast, a GRE encoder can be
built with a multiplier @xmath set at any value within the range @xmath
, and as long as this multiplier element has enough precision that the
true value of @xmath will not stray from this interval, then the
resulting bitstreams @xmath will always represent a series expansion of
the input @xmath in base @xmath of Theorem , which does not depend on
@xmath . The base @xmath does however depend on the leakage parameters
@xmath and @xmath , which are not known a priori to the decoder and can
also vary in time from input to input: as discussed earlier, the only
information available to the decoder a priori is that @xmath in
virtually all cases of interest, and @xmath , or @xmath in most cases of
interest. In the next section, we show that the upper bound of @xmath is
sufficiently small that the value of @xmath can be recovered with
exponential precision from the encoded bitstreams of a pair of real
numbers @xmath . In this sense, GRE encoders are robust with respect to
leakage in the delay elements, imprecisions in the multiplier @xmath ,
and quantization error.

#### 1.2.4 Determining @xmath up to exponential precision

##### Approximating @xmath using an encoded bitstream for x = 0

Recall that by Theorem , exponentially precise approximations @xmath to
the root @xmath in Theorem are sufficient in order to reconstruct
subsequent input @xmath whose bit streams are expansions in root @xmath
with exponential precision. In this section, we present a method to
approximate @xmath with such precision using the only information at our
disposal at the decoding end of the quantization scheme: the encoded
bitstreams of real numbers @xmath . More precisely, we will be able to
recover the value @xmath using only a single bitstream corresponding to
a beta-expansion of the number 0. It is easy to adapt this method to
slow variations of @xmath in time, as one can repeat the following
procedure at regular time intervals during quantization, and update the
value of @xmath accordingly.
The analysis that follows will rely on the following theorem by Peres
and Solomyak :

###### Theorem 1.2.6 (Peres-Solomyak).

[ @xmath -transversality] Consider the intervals @xmath . If @xmath ,
then for any @xmath of the form

  -- -------- -- --------
     @xmath      (1.17)
  -- -------- -- --------

and any @xmath , there exists a @xmath such that if @xmath then @xmath .
Furthermore, as @xmath increases to @xmath , @xmath decreases to 0.

Theorem has the following straightforward corollary:

###### Corollary 1.2.7.

If @xmath is a polynomial or power series belonging to the class @xmath
given by

  -- -------- -- --------
     @xmath      (1.18)
  -- -------- -- --------

then @xmath can have no more than one root on the interval @xmath .
Furthermore, if such a root exists, then this root must be simple.

In , Peres and Solomyak used Theorem to show that the distribution
@xmath of the random series @xmath is absolutely continuous for a.e.
@xmath . The estimates in Theorem are obtained by computing the smallest
double zero of a larger class of power series @xmath . The specific
upper bound @xmath for which @xmath -transversality holds on the
interval @xmath is the tightest bound that can be reached using their
method of proof, but the true upper bound cannot be much larger; in , a
power series @xmath belonging to the class @xmath in @xmath is
constructed which has a double zero at @xmath (i.e., @xmath and @xmath
), and having a double root obviously contradicts @xmath
-transversality.
We now show how to use Theorem to recover @xmath from a bitstream @xmath
produced from the GRE scheme @xmath corresponding to input @xmath . We
assume that the parameters @xmath of the quantizer @xmath used in this
implementation are within the range provided by Theorem . Note that such
a bitstream @xmath is not unique if @xmath , or if more than one pair
@xmath correspond to the same value of @xmath . Nevertheless, Theorem
implies that @xmath , so that @xmath is a root of the power series
@xmath . Suppose we know that @xmath , or that @xmath . Since @xmath
belongs to the class @xmath of Corollary @xmath , @xmath must
necessarily be the smallest positive root of @xmath . In reality one
does not have access to the entire bitsream @xmath , but only a finite
sequence @xmath . It is natural to ask whether we can approximate the
first root of @xmath by the first root of the polynomials @xmath . Since
the @xmath still belong to the class @xmath of Corollary @xmath , @xmath
has at most one zero on the interval @xmath . The following theorem
shows that, if it is known a priori that @xmath for some @xmath , then
for @xmath sufficiently large, @xmath is guaranteed to have a root
@xmath in @xmath , and @xmath decreases exponentially as @xmath
increases.

###### Theorem 1.2.8.

Suppose that for some @xmath , it is known that @xmath . Let @xmath be
such that @xmath -transversality holds on the interval @xmath . Let
@xmath be the smallest integer such that @xmath . Then for @xmath ,

1.  @xmath has a unique root @xmath in @xmath

2.  @xmath , where @xmath .

###### Proof.

Without loss of generality, we assume @xmath , and divide the proof into
2 cases: (1) @xmath , and (2) @xmath . The proof is the same for @xmath
, except that the cases are reversed.
Case (1) : Suppose @xmath . In this case, many of the restrictions in
the theorem are not necessary; in fact, the theorem holds here for all
@xmath and @xmath . @xmath has opposite signs at @xmath and @xmath , so
@xmath must have at least one root @xmath in between. Moreover, this
root is unique by Theorem . To prove part (b) of the theorem, observe
that Theorem implies that if @xmath at some @xmath , then @xmath in the
interval @xmath . In particular, @xmath and @xmath for @xmath . By the
Mean Value Theorem, @xmath for some @xmath , so that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The inequality @xmath follows from Theorem .
Case (2) : If @xmath , then Theorem implies that the first positive root
of @xmath , if it exists, must be greater than @xmath . Whereas in Case
(1), @xmath was guaranteed to have a root @xmath for all @xmath , in
this case @xmath might not even have a root in @xmath ; for example, the
first positive root of the polynomial @xmath occurs at @xmath . However,
if @xmath is sufficiently large, @xmath will have a root in @xmath .
Precisely, let @xmath , where @xmath is the smallest integer such that
@xmath . Then @xmath . Since @xmath , Theorem gives that @xmath for
@xmath , and

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

We have shown that @xmath and @xmath , so @xmath is guaranteed to have a
root @xmath in the interval @xmath , which is in fact the unique root of
@xmath in @xmath . Furthermore, the discrepency between @xmath and
@xmath becomes exponentially small as @xmath increases, as

@xmath .

∎

###### Remark 1.2.9.

In it is shown that @xmath -transversality holds on @xmath with @xmath .
This interval corresponds to @xmath in Theorem . If @xmath is known a
priori to be less than @xmath , then @xmath , and @xmath of Theorem
@xmath satisfies

@xmath .

Of course, even if we know @xmath , @xmath will be too large to solve
for @xmath analytically. However, if we can approximate @xmath by @xmath
with a precision of @xmath , then @xmath will also be of order @xmath ,
so that the estimates @xmath are still exponentially accurate
approximations to the input signal.
Indeed, any @xmath satisfying @xmath provides such an exponential
approximation to @xmath . Since @xmath , it follows that @xmath , and so
@xmath on the interval between @xmath and @xmath . Thus,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (1.19)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath .

##### Approximating @xmath with beta-expansions of @xmath

The method for approximating the value of @xmath in the previous section
requires a bitstream @xmath corresponding to running the GRE recursion
@xmath with specific input @xmath . This assumes that the reference
value @xmath can be measured with high precision, which is an
impractical constraint. We can try to adapt the argument using
bitstreams of an encoded pair @xmath as follows. Let @xmath and @xmath
be bitstreams corresponding to @xmath and @xmath , respectively. Define
@xmath . Put @xmath , and consider the sequence @xmath defined by @xmath
. Since @xmath , it follows that @xmath , and by Theorem , we have that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (1.20)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

so that

  -- -------- -- --------
     @xmath      (1.21)
  -- -------- -- --------

where the constant @xmath .

Equation @xmath , along with the fact that the polynomials @xmath are of
the form @xmath , allows us to apply Theorem to conclude that for @xmath
sufficiently large, the first positive root @xmath of @xmath becomes
exponentially close to @xmath . However, note that the encoding of
@xmath is not equivalent to the encoding of @xmath . The value of @xmath
used to define the sequence @xmath can be arbitrarily large . In fact,
if an ideal quantizer @xmath is used, then the bitstreams @xmath and
@xmath are uniquely defined by @xmath , so that @xmath . Thus, this
method for recovering @xmath actually requires the use of a flaky
quantizer @xmath . To this end, one could intentially implement GRE with
a quantizer which toggles close to, but not exactly at zero. One could
alternatively send not only a single pair of bitstreams @xmath , but
multiple pairs of bitstreams @xmath corresponding to several pairs
@xmath , to increase the chance of having a pair that has @xmath for
relatively small @xmath .
Figure 1.2 plots several instances of @xmath , and @xmath ,
corresponding to @xmath . The quantizer @xmath is used, with @xmath and
@xmath . These values of @xmath and @xmath generate bounded sequences
@xmath for all @xmath by Theorem . Figure 1.3 plots several instances of
the same polynomials, but with root @xmath .
As shown in Figure 1.1 , numerical evidence suggests that 10 iterations
of Newton’s method starting from @xmath will compute an approximation to
@xmath , the first root of @xmath , with the desired exponential
precision. The figure plots @xmath versus the error @xmath , where
@xmath is the approximation to @xmath obtained via a 10- step Newton
Method, starting from @xmath . More precisely, for each @xmath , we ran
@xmath different trials, with @xmath and @xmath picked randomly from the
intervals @xmath and @xmath respectively, and independently for each
trial. The worst case approximation error of the 100 trials is plotted
in each case. The quantizer used is @xmath with @xmath , and @xmath
picked randomly in the interval @xmath , independently for each trial.
Again, Theorem shows that these values of @xmath and @xmath generate
bounded sequences @xmath for all @xmath .

#### 1.2.5 The beta-encoder revisited

Even though our analysis of the previous section was motivated by leaky
GRE encoders, it can be applied to general beta-encoders to recover the
value of @xmath at any time during quantization. From the last section,
we have:

###### Theorem 1.2.10.

Let @xmath be a power series belonging to the class @xmath . Suppose
that @xmath has a root at @xmath , where @xmath for some @xmath . Let
@xmath be such that @xmath -transversality holds on the interval @xmath
. Let @xmath be the smallest integer such that @xmath . Then for @xmath
,

1.   The polynomials @xmath have a unique root @xmath in @xmath

2.   Any @xmath which satisfies @xmath also satisfies @xmath , where
    @xmath .

This theorem applies to beta encoders, corresponding to implementing the
recursion @xmath with flaky quantizer @xmath defined by @xmath , and
with @xmath known a priori to be contained in an interval @xmath . If
@xmath , or @xmath , then we can recover @xmath from either a bitstream
corresponding to 0, or a pair of bitstreams @xmath , using Theorem . Of
course we should not consider only the scheme @xmath , but rather a
revised scheme which accounts for integrator leak on the (single)
integrator used in the beta-encoder implementation. The revised
beta-encoding scheme, with slightly different initial conditions,
becomes

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.22)
  -- -------- -------- -------- -- --------

where @xmath is an unknown parameter in @xmath . As long as @xmath , we
still have that @xmath where @xmath ; furthermore, we can use Theorem to
recover @xmath in @xmath , although the specific values of @xmath and
@xmath cannot be distinguished, just as the specific values of @xmath
and @xmath in the expression for @xmath could not be distinguished in
GRE.

##### Remarks

Figure 1.1 suggests that the first positive roots of the @xmath serve as
exponentially precise approximations to @xmath for values of @xmath
greater than @xmath ; Figure 1.3 suggests that the first positive root
of @xmath will approximate values of @xmath up to @xmath . Furthermore,
these figures suggest that the constants @xmath and @xmath of @xmath in
the exponential convergence of these roots to @xmath can be made much
sharper, even for larger values of @xmath . This should not be
surprising, considering that nowhere in the analysis of the previous
section did we exploit the specific structure of beta-expansions
obtained via the particular recursions @xmath and @xmath , such as the
fact that such sequences @xmath cannot contain infinite strings of
successive 1’s or -1’s. It is precisely power series with such infinite
strings that are the ‘extremal cases’ which force the bound of @xmath in
Theorem . It is difficult to provide more refined estimates for the
constants @xmath and @xmath of @xmath in general, but in the idealized
setting where beta-expansions of @xmath are available via the ideal GRE
scheme @xmath without leakage, or via the beta-encoding @xmath with
@xmath , the beta-expansions of @xmath have a very special structure:

###### Proposition 1.2.11.

Consider the ideal GRE recursion @xmath with input @xmath and @xmath ,
or the beta-encoder recursion @xmath with @xmath , @xmath , and @xmath .
As long as @xmath in @xmath , or @xmath in @xmath , then for each @xmath
, @xmath is equal to @xmath or @xmath , and @xmath .

The proof is straightforward, and we omit the details.
Proposition can be used to prove directly that @xmath must be the first
positive root of the polynomials @xmath , when @xmath in either @xmath
or @xmath . Indeed, @xmath can be factored as follows:

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (1.23)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath is a polynomial with random coefficients of the form @xmath
.
@xmath clearly has a root at @xmath , and this root must be the only
root of @xmath on @xmath , since on this interval @xmath is bounded away
from @xmath by @xmath . We can also obtain a lower bound on @xmath by
@xmath . Note that this bound holds also for the infinite sum @xmath ,
and that the bound of @xmath is much sharper than the bound on @xmath
given by Theorem ; e.g., @xmath , and @xmath (see ). Similar bounds on
the derivatives @xmath and @xmath corresponding to beta-expansions of
@xmath in a base @xmath close to @xmath should hold, leading to sharper
estimates on the constants @xmath and @xmath of @xmath in the case where
beta-expansions of @xmath are available.

#### 1.2.6 Closing remarks

We have shown that golden ratio encoders are robust with respect to
leakage in the delay elements of their circuit implementation. Although
such leakage may change the base @xmath in the reconstruction formula
@xmath , we have shown that exponentially precise approximations @xmath
to @xmath can be obtained from the bitstreams of a pair @xmath , and
such approximations @xmath are sufficient to reconstruct subsequent
input @xmath by @xmath .
Our method can be extended to recover the base @xmath in general
beta-encoders, as long as @xmath is known a priori to be sufficiently
large; e.g. @xmath . This method is similar to the method proposed in
for recovering @xmath in beta-encoders when @xmath -quantizers are used,
except that our method does not require a fixed reference level, which
is difficult to measure with high precision in practice.

#### 1.2.7 Appendix: proof of Theorem

In this section we prove Theorem , which provides a range within which
the ”flakiness” parameter @xmath and the ”amplifier” parameter @xmath in
the quantizer @xmath can vary from iteration to iteration, without
changing the fact that the sequences @xmath produced by the scheme
@xmath will be bounded. The derived range for @xmath and @xmath is
independent of the specific values of the parameters @xmath in @xmath .
The techniques of this section are borrowed in large part from those
used in to prove a similar result for the ideal GRE scheme @xmath ;
i.e., taking @xmath in @xmath . As is done in , we first observe that
the recursion formula @xmath corresponding to the leaky GRE is
equivalent to the following piecewise affine discrete dynamical system
on @xmath :

  -- -------- -- --------
     @xmath      (1.24)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (1.25)
  -- -------- -- --------

where @xmath and @xmath . We will construct a class of subsets @xmath of
@xmath for which @xmath , where @xmath is the disk of radius @xmath
centered at the origin; i.e. @xmath . Note that these sets are not only
positively invariant sets of the map @xmath , but have the additional
property that if @xmath , the image @xmath may be perturbed at any time
within a radius of @xmath (for example, by additive noise), and the the
resulting sequence @xmath will still remain bounded within @xmath for
all time @xmath .
We refer the reader to Figure 1.4 as we detail the construction of the
sets @xmath . Rectangles @xmath and @xmath in Figure 1.4 are designed so
that their respective images under the affine maps @xmath and @xmath
(defined below) are both equal to the dashed rectangle @xmath .

  -- -------- -- --------
     @xmath      
     @xmath      (1.26)
  -- -------- -- --------

That is, rectangles @xmath and @xmath will satsify @xmath . More
specifically, @xmath , @xmath , and so on. Since the rectangle @xmath is
contained within the union of @xmath and @xmath , @xmath is a positively
invariant set for any map @xmath satisfying

  -- -------- -- --------
     @xmath      (1.27)
  -- -------- -- --------

where @xmath . In particular, if the parameters @xmath and @xmath in the
map @xmath are chosen such that the intersection of @xmath and the strip
@xmath is a subset of @xmath (recall @xmath and @xmath , then @xmath is
of the form @xmath . Indeed, @xmath is the region of the plane in which
the quantizer @xmath operates in flaky mode. This geometric setup is
clarified with a figure, provided in Figure 1.5 .

It remains to verify the existence of at least one solution to the setup
in Figure 1.4 for each @xmath . This can be done, following the lead of
, in terms of the parameters defined in Figure 1.4 .
Note that the matrix

  -- -------- -- --------
     @xmath      (1.28)
  -- -------- -- --------

in @xmath has as eigenvalues @xmath and @xmath . In particular, when
@xmath , we have that @xmath and @xmath . The eigenvalues @xmath and
@xmath have respective normalized eigenvectors

@xmath and @xmath ,

where @xmath and @xmath . It follows that the affine map @xmath acts as
an expansion by a factor of @xmath along @xmath and a reflection
followed by contraction by a factor of @xmath along @xmath , followed by
a vertical translation of +1. @xmath is the same as @xmath except with a
vertical translation of -1 instead of +1. After some straightforward
algebraic calculations, the mapping relations described above imply that
the parameters in Figure 1.4 are given by

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.29)
  -- -------- -------- -------- -- --------

The existence of the overlapping region @xmath is equivalent to the
condition @xmath which in turn is equivalent to the condition @xmath .
This expression is minimized over the range @xmath when @xmath , in
which case this constraint simplifies to @xmath .
We are interested primarily in quantizing numbers in @xmath , so that
this is the range of interest for the input @xmath ; for @xmath we
simply take @xmath . The set @xmath is equivalent to the condition that
the line @xmath which passes through the points @xmath and @xmath in
Figure 1.4 have y-intercept @xmath for all @xmath . This line has slope
@xmath , and passes through the point @xmath , so that its y-intercept
is given by @xmath . Rearranging terms, this implies that @xmath if and
only if

  -- -------- -- --------
     @xmath      (1.30)
  -- -------- -- --------

The right hand side of the above inequality is bounded below over the
range @xmath by @xmath , so that indeed @xmath is satisfied independent
of @xmath , for all admissable @xmath .
In practice, the ”flaky” parameter @xmath of the quantizer @xmath is not
known exactly; we will however be given a tolerance @xmath for which it
is known that @xmath . It follows that for each @xmath , we would like a
range @xmath for the ”amplifier” @xmath such that the GRE scheme @xmath
, implemented with quantizer @xmath , produces bounded sequences @xmath
for all values of @xmath . Now, it will be easier to derive all of the
following results for @xmath and @xmath , and return to the original
parameters @xmath afterwards. For a particular choice of @xmath , it is
not hard to derive an admissable range for @xmath in terms of the
eigenvalues @xmath and @xmath for fixed @xmath . We will take @xmath in
the following analysis for the sake of simplicity. First of all, the
tolerance @xmath must be admissable, i.e. the coordinate @xmath should
lie within the region @xmath . Indeed, for @xmath , @xmath can vary
within a small neighborhood of @xmath , as long as the line with slope
@xmath which passes through @xmath remains bounded above in the region
@xmath by the line passing through @xmath and @xmath . In other words,
the admissable range for @xmath is obtained from the constraint @xmath ,
where @xmath is the slope of the line through the points @xmath and
@xmath is the slope of the line through the points @xmath in Figure 1.5
. Rewriting @xmath and @xmath in terms of the eigenvalues @xmath and
@xmath via the relations @xmath , we have that

  -- -------- -- --------
     @xmath      (1.31)
  -- -------- -- --------

where

  -- -- -------- -------- --
                 @xmath   
        @xmath   @xmath   
  -- -- -------- -------- --

and

  -- -------- -- --------
     @xmath      (1.32)
  -- -------- -- --------

It is not hard to show that for any fixed @xmath , the function @xmath
increases as a function of @xmath over the range @xmath , as long as
@xmath . It follows that the minimum of @xmath over the rectangle @xmath
occurs along the edge corresponding to @xmath . But @xmath decreases as
a function of @xmath over the interval @xmath , so that the minimum of
@xmath over the entire rectangle @xmath occurs at @xmath , giving the
following uniform lower bound on @xmath for @xmath :

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (1.33)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

We proceed in the same fashion to derive a uniform upper bound on @xmath
over @xmath , except that we analyze the numerator and the denominator
in the expression for @xmath separately. The numerator @xmath increases
as a function of @xmath over the range @xmath for fixed @xmath , so that
@xmath achieves its maximum over the rectangle @xmath along the edge
corresponding to @xmath . @xmath increases as a function of @xmath over
the range @xmath , so that @xmath achieves its maximum over @xmath at
@xmath . A similar analysis shows that the denominator @xmath is
minimized over @xmath at @xmath . It follows that for @xmath , @xmath is
bounded above by

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (1.34)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Finally, we note that @xmath is admissable for all @xmath if and only if
@xmath for this value of @xmath and for all coresponding @xmath and
@xmath . Thus an admissable range for @xmath is obtained by equating the
lower bound of @xmath in @xmath and the upper bound of @xmath in @xmath
; namely, @xmath .
In terms of the original parameters @xmath and @xmath ,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (1.35)
  -- -------- -------- -------- -- --------

represents a uniformly admissible range for @xmath over the interval
@xmath .

## Chapter 2 Quiet quantization in analog to digital conversion

In the last chapter, we studied robustness properties of the @xmath
-encoder, a quantization method in analog to digital conversion that is
both robust with respect to quantizer imperfections like @xmath schemes,
and also achieves optimal reconstruction guarantees like pulse code
modulation (PCM). While we were able to show that @xmath -encoders are
also robust with respect to other component imperfections, the @xmath
-encoder is not robust with respect to additive noise; that is, if the
discrete input @xmath to the @xmath -encoder ( 1.7 ) represent noisy
signal samples @xmath , then unavoidably

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

and the situation is similar for the golden ratio encoder. That is, no
matter how many bits are spent per Nyquist interval, the reconstruction
error of the beta-encoder and golden ratio encoder is limited by the
level of noise on the samples @xmath .
On the other hand, @xmath schemes are robust with respect to additive
noise. That is, the @xmath reconstruction accuracy of the @xmath th
order @xmath scheme is preserved under additive noise, as facilitated by
the incorporation of all previous samples @xmath , @xmath , in
determining the quantization output @xmath . However, @xmath schemes
suffer from an entirely different kind of instability in the context of
audio signal processing, corresponding to a mechanical rather than
mathematical reconstruction inaccuracy: at the onset of periodicities in
the bit output @xmath , the filters used in the reconstruction of the
analog signal by @xmath quantizers are such that periodic oscillatory
patterns in the quantization output @xmath generate low-level idle tones
that are not present in the signal. These tones are particularly
intolerable to the listener when no signal is present, such as between
successive audio tracks. As idle tone components are most prominent in
low-order, 1-bit @xmath schemes [ 5 ] , it is desirable to adapt these
schemes in such a way as to eliminate the possibility of periodic
behavior of the output @xmath for vanishing input @xmath ; we shall
refer to such as quiet @xmath quantizers , in line with the following
definition:

###### Definition 2.0.1 (Quiet quantization).

A quantization scheme is said to be quiet if the onset of identically
zero input @xmath after some finite time @xmath induces constant bit
output @xmath after some later time @xmath .

In the first order @xmath model @xmath , which, starting from initial
state @xmath , follows the recursion

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.2)
  -- -------- -------- -------- -- -------

one can eliminate periodicities in the output @xmath at instantiation of
small input @xmath by simply replacing the standard 2-level quantizer
@xmath by a tri-level quantizer @xmath :

  -- -------- -- -------
     @xmath      (2.3)
  -- -------- -- -------

It is straightforward that the first-order @xmath scheme retains its
first-order approximation accuracy when implemented with the tri-level
quantizer @xmath , and is also quiet, in the sense of Definition 2.0.1 ;
however, first order schemes are rarely used in practice, and we would
thus like a similar result for higher order schemes. Unfortunately, the
second-order @xmath scheme,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.4)
  -- -------- -------- -------- -- -------

is not quiet, as shown in [ 79 ] , even when implemented with tri-level
quantizer @xmath and with linear rule @xmath . In fact, for most initial
states @xmath at the onset of zero input @xmath , the sequence @xmath
will become oscillatory. An exception to this rule occurs for the
initial condition @xmath , in which case @xmath and @xmath persist as
long as @xmath remains zero. That is, the origin is a fixed point of the
zero-input discrete map @xmath produced by ( 2.4 ) with tri-level
quantizer and zero input @xmath , but is not an attractive fixed point.
Recall that an attractive fixed point for a discrete map @xmath is a
fixed point @xmath having the property that for any value of @xmath in
the domain that is close enough to @xmath , the orbit of @xmath ,

@xmath

converges to @xmath . Such a fixed point is said to be a global
attracting fixed point if @xmath for any value of @xmath in the domain.
In order to modify the second-order scheme ( 2.4 ) so that the origin is
an attractive fixed point for the corresponding zero-input map, the
state sequence @xmath must be made to decay . One way to induce decay is
by applying a contraction @xmath before each iteration of ( 2.4 ),
resulting in the modification

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.5)
  -- -------- -------- -------- -- -------

This so-called finite-memory scheme was first studied in [ 79 ] . As the
accuracy of @xmath methods rely on ‘keeping track’ of the previous input
@xmath through the sequence @xmath , it is not clear that the
finite-memory scheme, which loses a fraction of its ‘memory’ at each
step, should still be second-order. However, as long as @xmath is
sufficiently small, second-order accuracy is maintained, as proven in [
79 ] . For completeness, we include this result below, starting with the
analogous result for the first-order finite-memory scheme,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.6)
  -- -------- -------- -------- -- -------

We note that the following result does not depend on the choice of
quantizer, and requires only that the sequence @xmath be uniformly
bounded.

###### Lemma 2.0.2 (Yilmaz).

Suppose @xmath , let @xmath be bandlimited with supp @xmath and @xmath ,
and let @xmath be a function satisfying @xmath for @xmath , @xmath for
@xmath , and @xmath . Suppose that the leakage factor @xmath , and
assume that the sequence @xmath generated by the finite-memory scheme (
2.6 ) is bounded. If @xmath is the output of the first-order finite
@xmath -quantizer ( 2.6 ), then

  -- -------- -- -------
     @xmath      (2.7)
  -- -------- -- -------

where @xmath , and @xmath

###### Proof.

We have @xmath . Therefore,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Using the bound @xmath ,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

∎

The analogous result for the second-order scheme follows the same
argument, and we state the result without proof.

###### Lemma 2.0.3 (Yilmaz).

Let @xmath , and @xmath be as in Lemma 2.0.2 . Assume that the sequence
@xmath generated by ( 2.5 ) is bounded. If @xmath is the output of the
second-order leaky @xmath -quantizer given in ( 2.5 ), then

  -- -------- -- -------
     @xmath      (2.8)
  -- -------- -- -------

where @xmath and @xmath are as before.

It is important to note that Lemma 2.0.3 is useful only in the finite
regime, even though it is asymptotic statement. Indeed, the oversampling
ratio @xmath is not taken to be arbitrarily large in practice as limited
by the resolution of analog-to-digital technologies; in most A/D
converters, @xmath is set. For this range of @xmath , the assumption in
Lemma 2.0.3 that the damping factor @xmath can be prescribed up to
resolution @xmath is certainly reasonable.
Lemma 2.0.3 rests on the assumption that the initial input @xmath can be
prescribed so that the sequence @xmath remains uniformly bounded. We can
ensure such stability by constructing positively invariant sets for the
discrete map @xmath corresponding to the finite-memory scheme ( 2.5 ).
Recall that a set @xmath is positively invariant for the discrete map
@xmath if @xmath implies that @xmath . The following theorem, borrowed
again from [ 79 ] , gives explicit constructions of such positively
invariant sets for the finite-memory map ( 2.5 ) with linear rule @xmath
:

###### Theorem 2.0.4 (Yilmaz).

Fix @xmath , and constant @xmath satisfying

  -- -------- --
     @xmath   
  -- -------- --

Consider the following functions,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

and the subset of points in @xmath lying between the graphs of @xmath
and @xmath :

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.11)
  -- -------- -------- -------- -- --------

Finally, suppose that @xmath in the linear rule @xmath is set to a value
in the range,

  -- -------- --
     @xmath   
  -- -------- --

If @xmath is obtained from @xmath via the second-order recursion ( 2.5 )
with input @xmath satisfying @xmath , quantizer @xmath , @xmath , and
linear rule @xmath , then @xmath .

A qualitative depiction of the invariant region @xmath is shown in
Figure below, as generated by admissible parameter values @xmath ,
@xmath , and @xmath .

#### 2.0.1 The revised scheme: Quiet @xmath quantization

Henceforth, we will assume that the oversampling factor @xmath is fixed,
and that the damping factor @xmath in the finite-memory scheme ( 2.5 )
satisfies the requirements of Lemma 2.0.3 for the finite-memory scheme
to be second-order. We further assume that the @xmath bound @xmath is
fixed, and that @xmath is in the admissible range, as specified by
Theorem 2.0.4 , for the finite-memory @xmath scheme ( 2.5 ) to be
stable.
Unfortunately, the finite-memory scheme ( 2.5 ) implemented with
tri-level quantizer @xmath is not quiet for @xmath sufficiently close to
@xmath , as a range of initial conditions @xmath at the onset of zero
input still lead to oscillatory behavior of the output. In Figure 2.2 ,
we indicate whether or not the iterates @xmath generated by the
finite-memory scheme with zero input @xmath remain oscillatory, for a
set of perturbations @xmath and initial conditions @xmath .

Nevertheless, we show in the following sections that a simple
modification to the finite-memory scheme ( 2.5 ) will ensure quietness,
in the sense of Definition 2.0.1 . The key idea is to apply a damping
factor @xmath as in the finite second order scheme @xmath , but not at
every iteration; only at iterations @xmath for which @xmath (or the
symmetric, when @xmath ). The situation @xmath must keep reccurring if
the system is to remain stable, and the application of an asymmetric
damping forces the state variables @xmath , and in turn the variables
@xmath , to zero once @xmath .

More precisely, we will consider the following asymmetrically damped
modification to ( 2.5 ), which, starting from initial @xmath , can be
implemented according to the recursion

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.12)
  -- -------- -------- -------- -- --------

with 4-level quantizer,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.13)
  -- -------- -------- -------- -- --------

Although we will take @xmath for the theoretical analysis of the next
section, simulation results suggest that quietness persists for a much
more general class of 4-level quantizers that includes the symmetric
quantizer @xmath .
That the asymmetric scheme ( 2.12 ) maintains second order accuracy for
@xmath is an immediate consequence of Proposition 2.0.3 . Furthermore,
the asymmetric scheme is stable and inherits the positively invariant
sets @xmath constructed in Proposition 2.0.4 . It remains to prove that
( 2.12 ) is quiet. Our accomplishments are summarized in the following
theorem.

###### Theorem 2.0.5 (Main theorem).

The asymmetrically-damped second-order @xmath scheme ( 2.12 ) is quiet
when implemented with 4-level quantizer @xmath . That is, if the input
@xmath to ( 2.12 ) becomes identically equal to zero after some time
@xmath , then the quantization output @xmath becomes constant after a
finite number of additional iterations @xmath .

We prove Theorem 3.17 by showing that @xmath is a global attracting
fixed point of the zero-input map governing @xmath in the asymmetric
scheme ( 2.12 ). As such, we need to develop a better understanding for
this dynamical system.

###### Observation 2.0.6.

The discrete map @xmath produced by the asymmetrically-damped @xmath
scheme ( 2.12 ) with zero input @xmath and 4-level quantizer @xmath
corresponds to the orbit of a piecewise-affine dynamical system,

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

and

  -- -------- --
     @xmath   
  -- -------- --

The origin @xmath is clearly a fixed point of the map @xmath . We will
show that @xmath is a global attracting fixed point, or that @xmath for
any initial condition @xmath , in two steps:

1.  In Section @xmath , we construct a positively invariant set @xmath
    for the map @xmath having the property that all orbits initialized
    in @xmath converge to the origin.

2.  In Section @xmath , we show that @xmath is also a trapping set for
    @xmath so that all orbits eventually become trapped in @xmath .

It is not clear that the proof need necessarily be split into two steps;
however, numerical results such as those in Figure 2.5 suggest a marked
change in the behavior of sequences @xmath upon entering the invariant
set @xmath .

#### 2.0.2 An invariant set @xmath and convergence to the origin

We begin by constructing a positively invariant set for the map @xmath .
We refer the reader to Figure 2.3 for reference.

###### Proposition 2.0.7.

The union @xmath of the affine regions

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

is a positively invariant set for the map @xmath . Moreover, the bit
sequence @xmath as in ( 2.12 ) associated to a sequence @xmath contained
in @xmath has the following alternating structure:

-    If @xmath then @xmath ,

-    If @xmath then @xmath .

Finally, @xmath if and only if @xmath and @xmath , while @xmath if and
only if @xmath and @xmath .

###### Proof.

First, as @xmath is the union of two convex sets, each containing the
origin, @xmath implies @xmath for @xmath . As such, if @xmath is
positively invariant for the map @xmath , then @xmath must also be
positively invariant for the full map @xmath . In order to show that
@xmath is positively invariant for @xmath , or that @xmath implies
@xmath , we can assume without loss that that @xmath by symmetry of
@xmath and @xmath . Note that the bit @xmath in ( 2.12 ) associated to a
point @xmath in the asymmetric scheme ( 2.12 ) is restricted to @xmath .
We split the proof into two cases, according to whether @xmath or @xmath
.

1.   Case 1 : @xmath , or @xmath : In this case, @xmath . Since @xmath ,
    @xmath if @xmath , or if @xmath if @xmath . We can expand @xmath as

      -- -------- --
         @xmath   
      -- -------- --

    and so indeed

      -- -------- --
         @xmath   
      -- -------- --

    while also

      -- -------- --
         @xmath   
      -- -------- --

    Thus, @xmath and has associated bit @xmath .

2.   Case 2: @xmath , or @xmath : Within this region, @xmath . Since
    @xmath , @xmath if @xmath , or if @xmath . Proceeding as before,

      -- -------- --
         @xmath   
      -- -------- --

    while at the same time

      -- -------- --
         @xmath   
      -- -------- --

    Thus, @xmath and has associated bit @xmath .

The proposition follows by incorporating the symmetric result for
initial conditions @xmath . ∎

The alternating pattern of the bit sequence @xmath associated to
iterates @xmath will be key in the following lemma. In particular, this
constraint on the bit sequence, in combination with the imbalance
induced by damping @xmath only in the positive half-space @xmath ,
forces the @xmath , and in turn the @xmath , to zero.

###### Lemma 2.0.8.

Suppose that @xmath . Then the subsequence @xmath from @xmath consisting
of indices @xmath for which @xmath satisfies @xmath as @xmath .

###### Proof.

First note that the event @xmath must keep recurring, for if not, or if
@xmath for all @xmath , then @xmath for all @xmath according to
Proposition 2.0.7 , and

  -- -------- --
     @xmath   
  -- -------- --

diverges. The index set @xmath is then an infinite subset of the natural
numbers, so that @xmath represents an infinite subsequence of @xmath .
As @xmath in passing from @xmath to @xmath , @xmath while both @xmath
and @xmath remain in @xmath , and @xmath in passing back from @xmath to
@xmath , it follows that @xmath contracts along the index set @xmath .
The subsequence @xmath thus converges to zero as @xmath . ∎

Note that convergence of the subsequence @xmath in Lemma 2.0.8 does not
guarantee convergence of the full sequence @xmath , as the residual
sequence @xmath over the index set @xmath forms a subsequence satisfying
@xmath as @xmath , if @xmath is an infinite set. However, the following
proposition ensures that this situation cannot occur.

###### Proposition 2.0.9.

If @xmath , then the full state sequence @xmath satisfies @xmath as
@xmath .

###### Proof.

Using the aforementioned results, we assume without loss that @xmath ,
and that @xmath for arbitrarily small @xmath .

1.  Consider the situation where @xmath for all @xmath . In this case,
    @xmath for all @xmath , @xmath , and @xmath is defined recursively
    according to

      -- -------- -------- -------- -------- --------
         @xmath   @xmath   @xmath            (2.16)
                           @xmath   @xmath   
                           @xmath   @xmath   
                           @xmath   @xmath   
                           @xmath   @xmath   
      -- -------- -------- -------- -------- --------

    where we use in the last equality that @xmath . In this case, then,
    @xmath .

2.  Suppose that @xmath , in which case @xmath , and @xmath , yielding

      -- -------- -------- -------- -------- --------
         @xmath   @xmath   @xmath            (2.17)
                           @xmath   @xmath   
                           @xmath   @xmath   
      -- -------- -------- -------- -------- --------

    the last inequality requiring that @xmath , which is satisfied as
    @xmath was arbitrary. This case, then, reduces to the first case.

3.  It remains only to consider sequences @xmath for which @xmath
    implies @xmath , or @xmath . In terms of Figure 2.3 , such sequences
    avoid the lower parallel section of the set @xmath . The trajectory
    of such @xmath under the map @xmath can then be described as
    follows:

    1.  If @xmath , then
        @xmath ,
        @xmath ,
        @xmath

        Else if @xmath ,

    2.  If @xmath , then
        @xmath ,
        @xmath ,
        @xmath

    3.  If @xmath then
        @xmath ,
        @xmath ,
        @xmath

    Since @xmath cannot occur in successive iterations according to the
    alternating nature of @xmath , we arrive at the period-2 inequality

      -- -------- -- --------
         @xmath      (2.18)
      -- -------- -- --------

    indicating that the iterates @xmath diverge. This case, in
    conclusion, cannot occur.

∎

#### 2.0.3 The invariant set @xmath as a global trapping set for @xmath

We prove in this section that the positively invariant set @xmath is
also a global trapping set for @xmath ; this result in combination with
the results of the last section guarantee that the origin is a global
fixed point for @xmath .
To be precise,

###### Definition 2.0.10.

A global trapping set of a discrete mapping @xmath is any set @xmath
such that

1.  @xmath (that is, @xmath is a positively invariant set for @xmath ),

2.   for any @xmath , there exists @xmath such that @xmath .

The construction of global trapping sets for the full second-order
@xmath scheme was recently developed by Sidong [ 80 ] . We will follow
the approach of that work and show that @xmath is a global trapping set
for the map @xmath using a Lyapunov function argument. In the context of
discrete dynamical systems, a Lyapunov function refers loosely to a
nonnegative convex energy functional @xmath that contracts along the
action of the discrete map, @xmath . If @xmath contracts as such for all
@xmath , and @xmath only if @xmath , then all orbits @xmath must
converge to the global fixed point @xmath . More generally, if @xmath
decreases along orbits @xmath while the iterates @xmath remain outside
an invariant set @xmath , then @xmath can be shown to be a globally
attracting set, as follows:

###### Lemma 2.0.11.

Let @xmath be a positively invariant set for the discrete map @xmath .
Suppose there exists a nonnegative function @xmath and a parameter
@xmath for which, if @xmath is not in @xmath , then either @xmath or
@xmath after some finite time @xmath . It follows that @xmath is a
global trapping set for @xmath .

###### Proof.

Suppose @xmath is such that @xmath for all @xmath , and denote @xmath .
From the stated hypotheses, @xmath after some finite time @xmath , and,
by induction, @xmath after a finite time @xmath for any positive integer
@xmath . But then eventually @xmath , which is impossible since @xmath
by assumption. ∎

Lemma ( 2.0.11 ) has the following implication for the map @xmath and
invariant set @xmath under consideration.

###### Proposition 2.0.12.

Consider the positively invariant set @xmath as constructed in
Proposition ( 2.0.7 ). @xmath is a global trapping set for @xmath if
there exists a nonnegative and convex function @xmath and a parameter
@xmath for which the following sets are contained in @xmath :

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.19)
  -- -------- -------- -------- -- --------

###### Proof.

We verify that the conditions for Lemma ( 2.0.11 ) hold for invariant
set @xmath and parameter @xmath . The proof is split into two cases,
according to whether or not @xmath or @xmath .

1.   Case 1: Suppose first that @xmath . On this half-space, @xmath acts
    as a contraction. If @xmath , but @xmath , then @xmath by positive
    invariance of @xmath for @xmath . If alternatively @xmath , then

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

2.   Case 2 : Suppose now that @xmath for all @xmath . Under this
    assumption, the quantization output is restricted to @xmath for all
    @xmath , so that @xmath forms a monotonically nondecreasing
    sequence. If @xmath for all @xmath , then @xmath diverges, which is
    impossible since @xmath belongs to a bounded set. If alternatively
    @xmath at some index @xmath , then @xmath for all @xmath by
    monotonicity and, still, @xmath . The case @xmath for all @xmath
    cannot happen since @xmath by assumption. It follows that this case
    is impossible.

We can conclude that after a finite number of iterations @xmath , either
@xmath or @xmath . The result follows by application of Lemma 2.0.11 . ∎

Following the approach in [ 80 ] , we consider the following Lyapunov
function for the map @xmath :

  -- -------- -- --------
     @xmath      (2.20)
  -- -------- -- --------

Note that @xmath is a convex function on @xmath . Letting @xmath and
@xmath , it is easily verified that @xmath . The motivation for this
choice of Lyapunov function is that @xmath and @xmath are the unique
functional solutions satisfying the relations

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.21)
  -- -------- -------- -------- -- --------

It is straightforward that the positively invariant set @xmath contains
the set @xmath for sufficiently small @xmath . In order to apply
Proposition ( 2.0.12 ), it remains only to verify that @xmath also
contains the set @xmath .

###### Proposition 2.0.13.

Consider the map @xmath as defined in ( 2.0.6 ), the convex function
@xmath , and the set @xmath , with

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.22)
  -- -------- -------- -------- -- --------

The set @xmath is contained in the positively invariant set @xmath (as
shown in Figure 2.4 ). Moreover, @xmath contains the set @xmath .

We defer the proof of Proposition 2.0.13 , which amounts to a
straightforward case by case analysis, to Appendix 1.
The main result of this chapter, Theorem 3.17 , follows from Proposition
2.0.12 and Lemma 2.0.9 .

###### Proof of Theorem 3.17.

By Proposition ( 2.0.12 ), @xmath is a global trapping set for @xmath ,
the discrete map governing @xmath in the asymmetrically-damped scheme (
2.12 ) with 4-level quantizer @xmath and zero input @xmath . In other
words, for any initial condition @xmath , the iterates @xmath become
trapped in the set @xmath after a finite number of iterations. From
Lemma 2.0.9 , it follows that @xmath ; once the iterates @xmath become
trapped in the positive quadrant, the bit output @xmath is constant. ∎

### 2.1 Future Directions

We have introduced a @xmath -bit second order @xmath scheme that is
guaranteed to be quiet : periodicities in the bit output are eliminated
when the input vanishes. It remains to analyze the stability of such
quietness in the face of unavoidable component imprecisions. Although a
full analysis is beyond the scope of the current presentation, we
outline a few key issues below.

1.   Randomness helps. Any introduction of randomness to a quantization
    scheme will only increase the aperiodicity of the bit output; this
    includes zero-mean additive noise on the samples @xmath and unbiased
    ‘flakiness’ in the quantizer element. Quietness for the
    asymmetrically-perturbed scheme ( 2.12 ) is threatened more from
    biased imprecisions, such as those resulting from an offset in the
    4-level quantizer, @xmath . Numerical simulations suggest that
    quietness of the asymmetric scheme nevertheless persists in the face
    of such offsets: the state sequence @xmath still converges at the
    onset of zero input, but the iterates @xmath approach a nonzero
    limit @xmath that grows with the magnitude of the shift; if @xmath
    is small enough, then quietness is still attained. More generally,
    quietness of the asymetrically-damped scheme appears to persist for
    a much more general class of quantizers than the the particular
    4-level quantizer @xmath that was needed for the theoretical
    analysis of the previous section; indeed, indistinguishable results
    are observed by implementing ( 2.12 ) with the far simpler symmetric
    4-level quantizer,

      -- -------- -- --------
         @xmath      (2.23)
      -- -------- -- --------

2.   Other robustness issues.

    1.   Imprecisions in @xmath : The damping factor @xmath does not
        have to be constant from iteration to iteration nor known
        precisely; all of the analysis of the previous section holds for
        @xmath varying at each iteration, as long as the sequence
        remains bounded from below as to maintain second-order accuracy
        of the scheme ( 2.12 ), and is not chosen adversarialy to
        converge @xmath .

    2.   Imprecisions in @xmath : We have until now assumed that the
        parameter @xmath is fixed throughout the iterations. However,
        numerical evidence suggests that quietness does not require that
        @xmath be fixed in the expression @xmath , as long as @xmath is
        within the range of stability, as stated in Theorem ( 2.0.4 ).

    3.   Leaky integrators. More realistically, one should incorporate
        integrator leakage into the proposed model ( 2.12 ), analogous
        to the modification studied for the golden ratio encoder in the
        last chapter. That is, the asymmetric scheme, being second
        order, requires two delay elements in its implementation to hold
        each of the states @xmath and @xmath over one iteration. After
        one clock time, the stored input in the first delay is reduced
        to @xmath times its original value, while the stored input in
        the second delay is replaced by @xmath times its original value;
        we can incorporate such leakage into the asymmetric model ( 2.12
        ) as follows:

          -- -------- -------- -------- -- --------
             @xmath   @xmath   @xmath      
             @xmath   @xmath   @xmath      
             @xmath   @xmath   @xmath      (2.24)
          -- -------- -------- -------- -- --------

        Above, @xmath in most circuits on interest; the specific leakage
        factors within this window are generally unknown and may vary
        slowly in time. When @xmath , the positively invariant set
        @xmath is still a trapping set for the revised scheme; however,
        once in this set, it is not immediately clear how to modify the
        analysis of section @xmath to prove that @xmath . Numerical
        studies indicate that quietness persists in the face of such
        leakage, requiring only that the perturbations remain
        asymmetric, or that each of @xmath and @xmath is strictly
        smaller than either of @xmath and @xmath .

3.   Hybrid Chaotic-Quiet @xmath Quantization

    While the asymmetry of the perturbation @xmath in ( 2.12 ) is key
    for inducing quietness, it is not clear that this perturbation need
    be nonnegative. So-called chaotic @xmath quantization [ 51 ] has
    been proposed as a means to eliminate idle tones in @xmath
    quantization, not just at the onset of zero input, but at the onset
    of any constant, or DC input sequence. In the context of 1-bit,
    second-order @xmath quantization, chaotic @xmath quantization
    corresponds to expanding , rather than contracting, by a small
    factor @xmath at each iteration:

      -- -------- -------- -------- -- --------
         @xmath   @xmath   @xmath      
         @xmath   @xmath   @xmath      
         @xmath   @xmath   @xmath      (2.25)
      -- -------- -------- -------- -- --------

    This modification is termed chaotic because it has the effect of
    generating aperiodic output at the onset of DC input; however, a
    complete stability analysis of the scheme ( 2.25 ) remains an open
    problem. Numerical studies, such as those in Figure 2.6 , indicate
    that the chaotic scheme outperforms the asymmetrically-damped scheme
    ( 2.12 ) in generating aperiodic output for general DC input,
    although the asymmetric scheme shows a marked improvement over the
    fully damped tri-level scheme. While the chaotic scheme ( 2.25 ) is
    not quiet in the sense of Definition ( 2.0.1 ), we expect that
    quietness can be achieved by introducing asymmetry into the model (
    2.25 ). Specifically, the hybrid scheme,

      -- -------- -------- -------- -- --------
         @xmath   @xmath   @xmath      
         @xmath   @xmath   @xmath      
         @xmath   @xmath   @xmath      (2.26)
      -- -------- -------- -------- -- --------

    with @xmath taking values in @xmath so that either damping @xmath or
    expansion @xmath is applied at each iteration, appears to inherit
    the aperiodic orbit structure of its parent chaotic map ( 2.25 ),
    and also quietness induced by its asymmetry. We hope to study the
    stability and aperiodicity for the chaotic maps ( 2.25 ), ( 2.26 )
    in the future.

### 2.2 Appendix : proof of Proposition 2.0.13

Let @xmath and let @xmath . Suppose that @xmath is such that @xmath .
Our first aim is to show that @xmath in this situation.

1.   Case 1: If @xmath , then @xmath , while @xmath ,so

      -- -------- --
         @xmath   
      -- -------- --

    But since @xmath , we know that @xmath , so @xmath , and @xmath
    holds in this case.

2.   Case 2: If @xmath , then @xmath , while the expression for @xmath
    remains unchanged, so

      -- -------- -- --------
         @xmath      (2.27)
      -- -------- -- --------

    We split this case into two subcases:

    1.   Case 2(a) : If, on the other hand, @xmath , then @xmath , and (
        2.27 ) simplifies to

          -- -------- --
             @xmath   
          -- -------- --

        But since @xmath by assumption, the result holds in this
        subcase.

    2.   Case 2(b) : If @xmath , then

          -- -------- -------- -------- -------- --------
             @xmath   @xmath   @xmath            (2.28)
                               @xmath   @xmath   
          -- -------- -------- -------- -------- --------

        But of course, the condition @xmath holds throughout @xmath .

We have shown thus far that @xmath if @xmath and @xmath . It remains to
show that @xmath if @xmath and @xmath . By inspection of Figure 1, the
intersection of the regions @xmath and @xmath consists of two disjoint
sets: @xmath , and @xmath

1.   Case 1: @xmath : As @xmath , the restriction @xmath trivially
    holds, and so @xmath , and

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    which is satisfied by assumption.

2.   Case 2: @xmath : @xmath is contained in the quadrant @xmath , and
    so @xmath , @xmath , and

      -- -------- --
         @xmath   
      -- -------- --

    which again is satisfied by assumption.

By symmetry of the set @xmath and the map @xmath , we have also that
@xmath if @xmath .

## Chapter 3 Cross validation in compressed sensing

### 3.1 Compressed sensing: Redefining traditional notions of sampling

Compressed Sensing (CS) is a fast developing area in applied
mathematics, motivated by the reality that most data we store and
transmit contains far less information than its dimension suggests. For
example, a one-dimensional slice through the pixels in a typical
grayscale image will contain segments of smoothly varying intensity,
with sharp changes between adjacent pixels appearing only at edges in
the image. If a large data vector contains only @xmath nonzero entries,
or is k-sparse , it is common practice to temporarily store the entire
vector, possibly with the intent to go back and replace this vector with
a smaller dimensional vector encoding the location and magnitude of its
@xmath significant coefficients. In compressed sensing, one instead
collects fewer fixed linear measurements of the data to start with,
sufficient in number to recover the location and numerical value of the
@xmath nonzero coordinates at a later time. Finding ”good” linear
measurements, as well as fast, accurate, and simple algorithms for
recovering the original data from these measurements, are the twofold
goals of Compressed Sensing research today.
In the sequel, we restrict our focus to signals that can be represented
as real-valued vectors @xmath . This signal model works well for digital
images, which are naturally sparse: for example, a one-dimensional slice
through the pixels in a typical grayscale image will contain segments of
smoothly varying intensity, with sharp changes between adjacent pixels
appearing only at edges in the image. Often this sparsity in information
translates into a sparse (or approximately sparse) representation of the
data with respect to some standard basis; for the image example, the
basis would be a wavelet of curvelet basis. For such @xmath dimensional
data vectors that are well approximated by a @xmath -sparse vector (or a
vector that contains at most @xmath nonzero entries), it is common
practice to temporarily store the entire vector, possibly with the
intent to go back and replace this vector with a smaller dimensional
vector encoding the location and magnitude of its @xmath significant
coefficients. In compressed sensing, one instead collects fewer fixed
linear measurements of the data to start with, sufficient in number to
recover the location and numerical value of the @xmath nonzero
coordinates at a later time. Finding ‘good’ linear measurements, as well
as fast, accurate, and simple algorithms for recovering the original
data from these measurements, are the twofold goals of compressed
sensing research today.
Review of basic CS setup . The data of interest is taken to be a
real-valued vector @xmath that is unknown , but from which we are
allowed up to @xmath linear measurements, in the form of inner products
of @xmath with @xmath vectors @xmath of our choosing. Letting @xmath
denote the @xmath matrix whose @xmath th row is the vector @xmath , this
is equivalent to saying that we have the freedom to choose and store an
@xmath matrix @xmath , along with the @xmath -dimensional measurement
vector @xmath . Of course, since @xmath maps vectors in @xmath to
vectors in a smaller dimensional space @xmath , the matrix @xmath is not
invertible, and we thus have no hope of being able to reconstruct an
arbitrary @xmath dimensional vector @xmath from such measurements.
However, if the otherwise unknown vector @xmath is specified to be
@xmath -sparse, and @xmath is fairly small compared with @xmath , then
there do exist matrices @xmath for which @xmath uniquely determines
@xmath , and allows recovery of @xmath using fast and simple algorithms.
It was the interpretation of this phenomenon given by Candes and Tao , ,
and Donoho , that gave rise to compressed sensing. In particular, these
authors define classes of matrices that possess this property. One
particularly elegant characterization of this class is via the
Restricted Isometry Property (RIP) . A matrix @xmath with unit normed
columns is said to be @xmath -RIP if all singular values of any @xmath
column submatrix of @xmath lie in the interval @xmath for a given
constant @xmath . For a fixed value of @xmath , an @xmath matrix @xmath
whose entries @xmath are independent realizations of a Gaussian or
Bernoulli random variable satisfies @xmath -RIP of level

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

with probability @xmath . Also with high probability, an @xmath matrix
obtained by selecting @xmath rows at random from the @xmath discrete
Fourier matrix satisfies @xmath -RIP of the same order as @xmath up to
an additional @xmath factor [ 66 ] . In fact, the order of @xmath given
by @xmath is optimal given @xmath and @xmath , as shown in using
classical results on Gelfand widths of @xmath unit balls in @xmath . To
date, there exist no deterministic constructions of RIP matrices of this
order.
Recovering or approximating x. As shown in , the following approximation
results hold for matrices @xmath that satisfy @xmath -RIP with constant
@xmath :

1.  If @xmath is @xmath -sparse, then @xmath can be reconstructed from
    @xmath and the measurement vector @xmath as the solution to the
    following @xmath minimization problem:

      -- -------- -- -------
         @xmath      (3.2)
      -- -------- -- -------

2.  If @xmath is not @xmath -sparse, the error between @xmath and the
    approximation @xmath is still bounded by

      -- -------- -- -------
         @xmath      (3.3)
      -- -------- -- -------

    where @xmath , and @xmath denotes the best possible approximation
    error in the metric of @xmath between @xmath and the set of @xmath
    -sparse signals in @xmath . The approximation error @xmath is
    realized by the @xmath -sparse vector @xmath that corresponds to the
    vector @xmath with all but the @xmath largest entries set to zero,
    independent of the @xmath norm in the approximation @xmath .

This immediately suggests to use the @xmath -minimizer @xmath as a means
to recover or approximate an unknown @xmath with sparsity constraint.
Several other decoding algorithms are used as alternatives to @xmath
minimization for recovering a sparse vector @xmath from its image @xmath
, not because they offer better accuracy ( @xmath minimization gives
optimal approximation bounds when @xmath satisfies RIP), but because
they can be faster and easier to implement. For a comprehensive survey
on compressed sensing decoding algorithms, we refer the reader to [ 57 ]
.

### 3.2 Estimating the accuracy of compressed sensing estimates

According to the bound @xmath , the quality of a compressed sensing
estimate @xmath depends on how well @xmath can be approximated by a
@xmath -sparse vector, where the value of @xmath is determined by the
number of rows @xmath composing @xmath . While @xmath is assumed to be
known and fixed in the compressed sensing literature, no such bound is
guaranteed for real-world signal models such as vectors @xmath
corresponding to wavelet coefficient sequences of discrete
photograph-like images. Thus, the quality of a compressed sensing
estimate @xmath in general is not guaranteed.
If the error @xmath incurred by a particular approximation @xmath were
observed to be large, then decoding could be repeated using more
measurements, perhaps at increasing measurement levels @xmath , until
the error @xmath corresponding to @xmath measurements were observed to
be sufficiently small. Of course, the errors @xmath and @xmath are
typically not known, as @xmath is unknown. Our main observation in this
chapter is that one can apply the Johnson-Lindenstrauss lemma to the set
of @xmath points,

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

In particular, @xmath measurements of @xmath , provided by @xmath , when
@xmath is, e.g. a Gaussian or Bernoulli random matrix, are sufficient to
guarantee that with high probability,

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

and

  -- -------- -- -------
     @xmath      (3.6)
  -- -------- -- -------

for any @xmath compressed sensing estimates. The equivalences @xmath and
@xmath allow the measurable quantities @xmath and @xmath to function as
proxies for the unknown quantities @xmath and @xmath ; these proxies can
be used to

1.  provide tight numerical upper and lower bounds on the errors @xmath
    and @xmath at up to @xmath compressed sensing estimates @xmath ,

2.  provide estimates of the underlying @xmath -term approximations
    @xmath of @xmath for up to @xmath different values of @xmath , and

3.  return from among a sequence of estimates @xmath with different
    initialization parameters, an estimate @xmath having error @xmath
    that does not exceed a small multiplicative factor of the best
    possible error in the metric of @xmath between @xmath and an element
    from the sequence at hand.

More precisely, all CS decoding algorithms require as input a parameter
@xmath corresponding to the number of rows in @xmath ; some compressed
sensing decoding algorithms (such as greedy algorithms) require also a
parameter @xmath indicating the sparsity level of @xmath , and other
algorithms require as input a bound @xmath on the expected amount of
energy in @xmath outside of its significant coefficients. All CS
decoding algorithms can be symbolically represented by functions of the
form @xmath , and we will give examples where each of the parameters
@xmath , @xmath , and @xmath can be optimized over a sequence of
estimates @xmath indexed by increasing hypotheses on each of the
parameters @xmath , @xmath , and @xmath .
The method we propose for parameter selection and error estimation in
compressed sensing is reminiscent of cross validation , which is a
technique used in statistics and learning theory whereby a data set is
separated into a training/estimation set and a test/cross validation
set, and the test set is used to prevent overfitting on the training set
by estimating underlying noise parameters. Indeed, we take a set of
@xmath measurements of the unknown @xmath , and use @xmath of these
measurements, @xmath , in a compressed sensing decoding algorithm to
return a sequence @xmath of candidate approximations to @xmath . The
remaining @xmath measurements, @xmath , are then used to identify from
among this sequence a ‘best’ approximation @xmath , along with an
estimate of the sparsity level of @xmath . Although the application of
cross validation in compressed sensing has been previously proposed by
Boufounos, Duarte, and Baraniuk in , the context in which it is studied
there is different from that of the present paper (we will discuss this
difference further in the last section), and in their application one
cannot immediately apply the mathematical justification of the Johnson
Lindenstrauss lemma that we present below.

### 3.3 Preliminary notation

Throughout the paper, we will be dealing with large dimensional vectors
that have few nonzero coefficients. We use the notation @xmath to
indicate that a vector @xmath @xmath has exactly @xmath nonzero
coordinates.
We will sometimes use the notation @xmath as shorthand for the
multiplicative relation

  -- -------- -- -------
     @xmath      (3.7)
  -- -------- -- -------

that can be worded as “the quantity @xmath approximates the quantity
@xmath to within a multiplicative factor of @xmath ”. Note that the
relation @xmath is not symmetric. Properties of the relation @xmath are
listed below; we omit the proofs, which amount to a string of simple
inequalities.

###### Lemma 3.3.1.

Fix @xmath .

1.   If @xmath satisfy @xmath , then @xmath .

2.   If @xmath satisfy @xmath and @xmath , then @xmath for parameter
    @xmath .

3.   If @xmath and @xmath are sequences in @xmath , and @xmath for each
    @xmath , then @xmath .

### 3.4 Mathematical foundations

The Johnson Lindenstrauss (JL) lemma, in its original form, states that
any set of @xmath points in high dimensional Euclidean space can be
embedded into @xmath dimensions, without distorting the distance between
any two points by more than a factor of @xmath . In the same paper, it
was shown that a random orthogonal projection would provide such an
embedding with positive probability. Following several simplifications
to the original proof , , , it is now understood that Gaussian random
matrices, among other purely random matrix constructions, can substitute
for the random projection in the original proof of Johnson and
Lindenstrauss. Of the several versions of the lemma now appearing in the
literature, the following variant presented in Matousek is most
applicable to the current presentation.

###### Lemma 3.4.1 (Johnson-Lindenstrauss Lemma).

Fix an accuracy parameter @xmath , a confidence parameter @xmath , and
an integer @xmath .
Let @xmath be a random @xmath matrix whose entries @xmath are
independent realizations of a random variable R that satisfies:

1.   Var @xmath (so that the columns of @xmath have expected @xmath norm
    1)

2.  @xmath ,

3.   For some fixed @xmath and for all @xmath ,

      -- -------- -- -------
         @xmath      (3.8)
      -- -------- -- -------

Then for a predetermined @xmath ,

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

is satisfied with probability exceeding @xmath .

The constant @xmath bounding @xmath in Lemma grows with the parameter
@xmath specific to the construction of @xmath @xmath . Gaussian and
Bernoulli random variables @xmath will satisfy the concentration
inequality @xmath for a relatively small parameter @xmath (as can be
verified directly), and for these matrices one can take @xmath in Lemma
.
The Johnson Lindenstrauss lemma can be made intuitive with a few
observations. Since @xmath and @xmath , the random variable @xmath
equals @xmath in expected value; that is,

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

Additionally, @xmath inherits from the random variable @xmath a nice
concentration inequality:

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (3.11)
  -- -------- -------- -------- -- --------

The first inequality above is at the heart of the JL lemma; its proof
can be found in . The second inequality follows using that @xmath and
@xmath by construction. A bound similar to @xmath holds for @xmath as
well, and combining these two bounds gives desired result @xmath .
For fixed @xmath , a random matrix @xmath constructed according to Lemma
fails to satisfy the concentration bound @xmath with probability at most
@xmath . Applying Boole’s inequality, @xmath then fails to satisfy the
stated concentration on any of @xmath predetermined points @xmath ,
@xmath , with probability at most @xmath . In fact, a specific value of
@xmath may be imposed for fixed @xmath by setting @xmath . These
observations are summarized in the following corollary to Lemma .

###### Corollary 3.4.2.

Fix an accuracy parameter @xmath , a confidence parameter @xmath , and
fix a set of @xmath points @xmath . Set @xmath , and fix an integer
@xmath . If @xmath is a @xmath matrix constructed according to Lemma ,
then with probability @xmath , the bound

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

obtains for each @xmath .

### 3.5 Cross validation in compressed sensing

We return to the situation where we would like to approximate a vector
@xmath with an assumed sparsity constraint using @xmath linear
measurements @xmath and @xmath matrix @xmath of our choosing. Continuing
the discussion in Section 1, we will not reconstruct @xmath in the
standard way by @xmath for fixed values of the input parameters, but
instead separate the @xmath matrix @xmath into an @xmath implementation
matrix @xmath and an @xmath cross validation matrix @xmath , and
separate the measurements @xmath accordingly into @xmath and @xmath . We
use the implementation matrix @xmath and corresponding measurements
@xmath as input into the decoding algorithm to obtain a sequence of
possible estimates @xmath corresponding to increasing one of the input
parameters @xmath , @xmath , or @xmath . We reserve the cross validation
matrix @xmath and measurements @xmath to estimate each of the error
terms @xmath in terms of the computable @xmath . We will also estimate
the unknown oracle error ,

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

corresponding to the best possible approximation to @xmath in the metric
of @xmath from the sequence @xmath , using the computable cross
validation error ,

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

Our main result, which follows from Corollary @xmath , details how the
number of cross validation measurements @xmath should be chosen in terms
of the desired accuracy @xmath of estimation, confidence level @xmath in
the prediction, and number @xmath of estimates @xmath to be measured.

###### Theorem 3.5.1.

For a given accuracy @xmath , confidence @xmath , and number @xmath of
estimates @xmath , it suffices to allocate @xmath rows to a cross
validation matrix @xmath of Gaussian or Bernoulli type, normalized
according to Lemma 3.4.2 and independent of the estimates @xmath , to
obtain with probability greater than or equal to @xmath , and for each
@xmath , the bounds

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

and

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
                                
  -- -------- -------- -------- --

and also

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

###### Proof.

.

-   The bounds in ( 3.15 ) are obtained by application of Lemma to the
    @xmath points @xmath , and rearranging the resulting bounds
    according to Lemma part @xmath . The bound ( 3.17 ) follows from the
    bounds ( 3.15 ) and part (3) of Lemma .

-   The bounds in ( LABEL:2a ) are obtained by application of Lemma to
    the @xmath points @xmath , and regrouping the resulting bounds
    according to part (2) of Lemma .

∎

###### Remark 3.5.2.

The measurements making up the cross validation matrix @xmath must be
independent of the measurements comprising the rows of the
implementation matrix @xmath . This comes from the requirement in Lemma
that the matrix @xmath be independent of the points @xmath . This
requirement is crucial, as observed when @xmath solves the @xmath
minimization problem

  -- -------- -- --------
     @xmath      (3.18)
  -- -------- -- --------

in which case the constraint @xmath clearly precludes the rows of @xmath
from giving any information about the error @xmath .

###### Remark 3.5.3.

If the full compressed sensing matrix @xmath [ @xmath ; @xmath ] is
itself of Gaussian or Bernoulli type, then one can obtain a more
accurate approximation to the unknown quantities @xmath by using all of
the measurements @xmath in the estimates @xmath .

###### Remark 3.5.4.

Proposition 3.17 should be applied with a different level of care
depending on what information about the sequence @xmath is sought. If
the minimizer @xmath is sufficient for one’s purposes, then the precise
normalization of @xmath in Proposition 3.17 is not important. The
normalization doesn’t matter either for estimating the normalized
quantities @xmath . On the other hand, if one is using cross validation
to obtain estimates for the quantities @xmath , then normalization is
absolutely crucial, and one must observe the normalization factor given
by Lemma that depends on the number of rows @xmath allocated to the
cross validation matrix @xmath .

### 3.6 Applications

#### 3.6.1 Estimation of the best @xmath-term approximation error

We have already seen that if the @xmath matrix @xmath satisfies @xmath
-RIP with parameter @xmath , and @xmath is returned as the solution to
the @xmath minimization problem @xmath , then the error between @xmath
and the approximation @xmath is bounded by

  -- -------- -- --------
     @xmath      (3.19)
  -- -------- -- --------

Several other decoding algorithms in addition to @xmath minimization
enjoy the reconstruction guarantee @xmath under similar bounds on @xmath
, such as the Iteratively Reweighted Least Squares algorithm (IRLS) ,
and the greedy algorithms CoSAMP and Subspace Pursuit . It has recently
been shown [ 78 ] [ 38 ] that if the bound ( 3.19 ) is obtained, and if
@xmath lies in the null space of @xmath (as is the case for the decoding
algorithms just mentioned), then if @xmath is a Gaussian or a Bernoulli
random matrix, the error @xmath also satisfies a bound, with high
probability on @xmath , with respect to the @xmath residual, namely

  -- -------- -- --------
     @xmath      (3.20)
  -- -------- -- --------

for a reasonable constant @xmath depending on the RIP constant @xmath of
@xmath . In the event that @xmath is obtained, a cross validation
estimate @xmath can be used to lower bound the residual @xmath , with
high probability, according to

  -- -------- -- --------
     @xmath      (3.21)
  -- -------- -- --------

with @xmath rows reserved for the matrix @xmath @xmath . At this point,
we will use Corollary 3.2 of , where it is proved that if the bound
@xmath holds for @xmath with constant @xmath , then the same bound will
hold for

  -- -------- -- --------
     @xmath      (3.22)
  -- -------- -- --------

the best @xmath -sparse approximation to @xmath , with constant @xmath .
Thus, we may assume without loss of generality that @xmath is @xmath
-sparse, in which case @xmath also provides an upper bound on the
residual @xmath by

  -- -------- -- --------
     @xmath      (3.23)
  -- -------- -- --------

With almost no effort then, cross validation can be incorporated into
many decoding algorithms to obtain tight upper and lower bounds on the
unknown @xmath -sparse approximation error @xmath of @xmath . More
generally, the allocation of @xmath measurements to the cross validation
matrix @xmath is sufficient to estimate the errors @xmath or the
normalized approximation errors @xmath at @xmath sparsity levels @xmath
by decoding @xmath times, adding @xmath measurements to the
implementation matrix @xmath at each repetition. Recall that the
quantities @xmath and @xmath are related according to @xmath .

#### 3.6.2 Choice of the number of measurements @xmath

Photograph-like images have wavelet or curvelet coefficient sequences
@xmath that are compressible , having entries that obey a power law
decay

  -- -------- -- --------
     @xmath      (3.24)
  -- -------- -- --------

where @xmath denotes the @xmath th largest coefficient of @xmath in
absolute value, the parameter @xmath indicates the level of
compressibility of the underlying image, and @xmath is a constant that
depends only on @xmath and the normalization of @xmath . From the
definition @xmath , compressible signals are immediately seen to satisfy

  -- -------- -- --------
     @xmath      (3.25)
  -- -------- -- --------

so that the solution @xmath to the @xmath minimization problem @xmath
using an @xmath matrix @xmath of optimal RIP order @xmath satisfies

  -- -------- -- --------
     @xmath      (3.26)
  -- -------- -- --------

The number of measurements @xmath needed to obtain an estimate @xmath
satisfying @xmath for a predetermined threshold @xmath will vary
according to the compressibility of the image at hand. Armed with a
total of @xmath measurements, the following decoding method that
adaptively chooses the number of measurements for a given signal @xmath
presents a more democratic alternative to standard compressed sensing
decoding structure:

#### 3.6.3 Choice of regularization parameter in homotopy-type
algorithms

Certain compressed sensing decoding algorithms iterate through a
sequence of intermediate estimates @xmath that could be potential
optimal solutions to @xmath under certain reconstruction parameter
choices. This is the case for greedy and homotopy-continuation based
algorithms. In this section, we study the application of cross
validation to the intermediate estimates of decoding algorithms of
homotopy-continuation type.
LASSO is the name coined in for the problem of minimizing of the
following convex program:

  -- -------- -- --------
     @xmath      (3.28)
  -- -------- -- --------

The two terms in the LASSO optimization problem @xmath enforce data
fidelity and sparsity, respectively, as balanced by the regularization
parameter @xmath . In general, choosing an appropriate value for @xmath
in @xmath is a hard problem; when @xmath is an underdetermined matrix,
as is the case in compressed sensing, the function @xmath is unknown to
the user but is seen empirically to have a minimum at a value of @xmath
in the interval @xmath that depends on the unknown noise level and/or
and compressibility level of @xmath .
The homotopy continuation algorithm [ 53 ] , which can be viewed as the
appropriate variant of LARS [ 53 ] , is one of many algorithms for
solving the LASSO problem @xmath at a predetermined value of @xmath ; it
proceeds by first initializing @xmath to a value sufficiently large to
ensure that the @xmath penalization term in @xmath completely dominates
the minimization problem and @xmath trivially. The homotopy continuation
algorithm goes on to generate @xmath for decreasing @xmath until the
desired level for @xmath is reached. If @xmath , then the homotopy
method traces through the entire solution path @xmath for @xmath before
reaching the final algorithm output @xmath corresponding to the @xmath
minimizer @xmath .
From the non-smooth optimality conditions for the convex functional
@xmath , it can be shown that the solution path @xmath is a
piecewise-affine function of @xmath [ 53 ] , with “kinks” possible only
at a finite number of points @xmath . Proposition 3.17 suggests a method
whereby an appropriate value of @xmath can be chosen from among a
subsequence of the kinks @xmath by solving the minimization problem
@xmath for appropriate cross validation matrix @xmath . Moreover, since
the solution @xmath for @xmath is restricted to lie in the
two-dimensional subspace spanned by @xmath and @xmath , one can combine
the Johnson Lindenstrauss Lemma with a covering argument analogous to
that used to derive the RIP property for Gaussian and Bernoulli random
matrices in , to cross validate the entire continuum of solutions @xmath
between @xmath . More precisely, the following bound holds under the
conditions outlined in Theorem @xmath , with the exception that @xmath
(as opposed to @xmath ) measurements are reserved to @xmath :

  -- -------- -- --------
     @xmath      (3.29)
  -- -------- -- --------

Unfortunately, the bound @xmath is not strong enough to provably
evaluate the entire solution path @xmath for @xmath , because the best
upper bound on the number of kinks on a generic LASSO solution path can
be very large. One can prove that this number is bounded by @xmath , by
observing that if @xmath and @xmath have the same sign pattern, then
@xmath also has the same sign pattern for @xmath . Applying Proposition
3.17 to @xmath points @xmath , this suggests that @xmath rows would need
to be allocated to a cross validation matrix @xmath in order for
Proposition 3.17 and the corollary @xmath to apply to the entire
solution path, which clearly defeats the compressed sensing purpose.
However, whenever the matrix @xmath is an @xmath compressed sensing
matrix of random Gaussian, Bernoulli, or partial Fourier construction,
it is observed empirically that the number of kinks along a homotopy
solution path behaves like @xmath for generic vectors @xmath used to
generate the path, see Figure 3.1 . This suggests, at least
heuristically, that the allocation of @xmath out of @xmath compressed
sensing measurements of this type suffices to ensure that the error
@xmath for the solution @xmath will be within a small multiplicative
factor of the best possible error in the metric of @xmath obtainable by
any approximant @xmath along the solution curve @xmath . At the value of
@xmath corresponding to @xmath , the LASSO solution @xmath can be
computed using all @xmath measurements @xmath as a final approximation
to @xmath .

The Dantzig selector (DS) [ 19 ] refers to a minimization problem that
is similar in form to the LASSO problem:

  -- -------- -- --------
     @xmath      (3.30)
  -- -------- -- --------

The difference between the DS @xmath and LASSO @xmath is the choice of
norm ( @xmath versus @xmath ) on the fidelity-promoting term.
Homotopy-continuation based algorithms have also been developed to solve
the minimization problem @xmath by tracing through the solution path
@xmath for @xmath . As the minimization problem @xmath can be
reformulated as a linear program, its solution path @xmath is seen to be
a piecewise constant function of @xmath , in contrast to the LASSO
solution path. In practice, the total number of breakpoints @xmath in
the domain @xmath is observed to be on the same order of magnitude as
@xmath when the @xmath matrix @xmath satisfies RIP [ 50 ] ; thus, the
procedure just described to cross validate the LASSO solution path can
be adapted to cross validate the solution path of @xmath as well.
Thus far we have not discussed the possibility of using cross validation
as a stopping criterion for homotopy-type decoding algorithms. Along the
LARS homotopy curve @xmath , most of the breakpoints @xmath appear only
near the end of the curve in a very small neighborhood of @xmath . These
breakpoints incur only miniscule changes in the error @xmath even though
they account for most of the computational expense of the LARS decoding
algorithm. Therefore, it would be interesting to adapt such algorithms,
perhaps using cross validation, to stop once @xmath is reached for which
the error @xmath is sensed to be sufficiently small.

#### 3.6.4 Choice of sparsity parameter in greedy-type algorithms

Greedy compressed sensing decoding algorithms also iterate through a
sequence of intermediate estimates @xmath that could be potential
optimal solutions to @xmath under certain reconstruction parameter
choices. Orthogonal Matching Pursuit (OMP), which can be viewed as the
prototypical greedy algorithm in compressed sensing, picks columns from
the implementation matrix @xmath one at a time in a greedy fashion
until, after @xmath iterations, the @xmath -sparse vector @xmath , a
linear combination of the @xmath columns of @xmath chosen in the
successive iteration steps, is returned as an approximation to @xmath .
The OMP algorithm is listed in Table 2. Although we will not describe
the algorithm in full detail, a comprehensive study of OMP can be found
in .

Note in particular that OMP requires as input a parameter @xmath
corresponding to the expected sparsity level for @xmath . Such input is
typical among greedy algorithms in compressive sensing (in particular,
we refer the reader to , , and ). As shown in , OMP will recover with
high probability a vector @xmath having at most @xmath nonzero
coordinates from its image @xmath if @xmath is a (known) @xmath Gaussian
or Bernoulli matrix with high probability. Over the more general class
of vectors @xmath that can be decomposed into a @xmath -sparse vector
@xmath (with @xmath presumably less than or equal to @xmath ) and
additive noise vector @xmath , we might expect an intermediate estimate
@xmath to be a better estimate to @xmath than the final OMP output
@xmath , at least when @xmath . Assuming that the signal @xmath admits a
decomposition of the form @xmath , the sequence of intermediate
estimates @xmath of an OMP algorithm can be cross validated in order to
estimate the noise level and recover a better approximation to @xmath .
We will study this particular application of cross validation in more
detail below.

### 3.7 Orthogonal matching pursuit: a case study

As detailed in Table 2, a single index @xmath is added to a set @xmath
estimated as the @xmath most significant coefficients of @xmath at each
iteration @xmath of OMP; following the selection of @xmath , an estimate
@xmath to @xmath is determined by the least squares solution,

  -- -------- -- --------
     @xmath      (3.31)
  -- -------- -- --------

among the subspace of vectors @xmath @xmath having nonzero coordinates
in the index set @xmath . OMP continues as such, adding a single index
@xmath to the set @xmath at iteration @xmath , until @xmath at which
point the algorithm terminates and returns the @xmath -sparse vector
@xmath as approximation to @xmath .
Suppose @xmath has only @xmath significant coordinates. If @xmath could
be specified beforehand, then the estimate @xmath at iteration @xmath of
OMP would be returned as an approximation to @xmath . However, the
sparsity @xmath is not known in advance, and @xmath will instead be an
upper bound on @xmath . As the estimate @xmath in OMP can be then
identified with the hypothesis that @xmath has @xmath significant
coordinates, the application of cross-validation as described in the
previous section applies in a very natural way to OMP. In particular, we
expect @xmath and @xmath of Proposition 3.17 to be close to the estimate
@xmath at index @xmath corresponding to the true sparsity of @xmath ;
furthermore, in the case that @xmath is significantly less than @xmath ,
we expect the cross validation estimate @xmath to be a better
approximation to @xmath than the OMP-returned estimate @xmath . We will
put this intuition to the test in the following numerical experiment.

#### 3.7.1 Experimental setup

We initialize a signal @xmath of length @xmath and sparsity level @xmath
as

  -- -------- -- --------
     @xmath      (3.32)
  -- -------- -- --------

Noise is then added to @xmath in the form of a Gaussian random variable
@xmath distributed according to

  -- -------- -- --------
     @xmath      (3.33)
  -- -------- -- --------

and the resulting vector @xmath is renormalized to satisfy @xmath . This
yields an expected noise level of

  -- -------- -- --------
     @xmath      (3.34)
  -- -------- -- --------

We fix the input @xmath in Table 2, and assume we have a total number of
compressed sensing measurements @xmath . A number @xmath of these @xmath
measurements are allotted to cross validation, while the remaining
@xmath measurements are allocated as input to the OMP algorithm in Table
2. This experiment aims to numerically verify Proposition 3.17 ; to this
end, we specify a confidence @xmath , and solve for the accuracy @xmath
according to the relation @xmath ; that is,

  -- -------- -- --------
     @xmath      (3.35)
  -- -------- -- --------

Note that the specification @xmath corresponds to setting the constant
@xmath in Proposition 3.17 . Although @xmath is needed for the proof of
the Johnson Lindenstrauss lemma at present, we find that in practice
@xmath already upper bounds the optimal constant needed for Proposition
3.17 for Gaussian and Bernoulli random ensembles.
A single (properly normalized) Gaussian @xmath measurement matrix @xmath
is generated (recall that @xmath = @xmath - @xmath ) , and this matrix
and the measurements @xmath are provided as input to the OMP algorithm;
the resulting sequence of estimates @xmath is stored. The final estimate
@xmath from this sequence is the returned OMP estimate @xmath to @xmath
. The error @xmath is greater than or equal to the oracle error of the
sequence, @xmath .
With the sequence @xmath at hand, we consider @xmath realizations @xmath
of an @xmath cross validation matrix having the same componentwise
distribution as @xmath , but normalized to have variance @xmath
according to Theorem . The cross validation error

  -- -------- -- --------
     @xmath      (3.36)
  -- -------- -- --------

is measured at each realization @xmath ; we plot the average @xmath of
these @xmath values and intervals centered at @xmath having length equal
to twice the empirical standard deviation. Note that we are effectively
testing @xmath trials of OMP-CV, the algorithm which modifies OMP to
incorporate cross validation so that @xmath are output instead of @xmath
.
At the specified value of @xmath , Proposition 3.17 part @xmath (with
constant @xmath ) implies that

  -- -- -- --------
           (3.37)
  -- -- -- --------

should obtain on at least @xmath of the @xmath estimates @xmath ; in
other words, at least @xmath of the @xmath discrepancies @xmath should
be bounded by

  -- -------- -- --------
     @xmath      (3.38)
  -- -------- -- --------

Using the relation @xmath between @xmath and @xmath , this bound becomes
tighter as the number @xmath of CV measurements increases; however, at
the same time, the oracle error @xmath increases with @xmath for fixed
@xmath as fewer measurements @xmath are input to OMP. An ideal number
@xmath of CV measurements should not be too large or too small; Figure 1
suggests that setting aside just enough measurements @xmath such that
@xmath is satisfied in @xmath serves as a good heuristic to choose the
number of cross validation measurements (in Figure 1, @xmath is
satisfied by taking only @xmath measurements).
We indicate the theoretical bound @xmath with dark gray in Figure 1,
which is compared to the interval in light gray of the @xmath values of
@xmath that are closest to @xmath in actuality.
This experiment is run for several values of @xmath within the interval
@xmath ; the results are plotted in Figure 1(a), with the particular
range @xmath blown up in Figure 1(b).
We have also carried out this experiment with a smaller noise variance;
i.e. @xmath is subject to additive noise

  -- -------- -- --------
     @xmath      (3.39)
  -- -------- -- --------

The signal @xmath is again renormalized to satisfy @xmath ; it now has
an expected noise level of

  -- -------- -- --------
     @xmath      (3.40)
  -- -------- -- --------

The results of this experiment are plotted in Figure 1(c).

#### 3.7.2 Experimental results

1.  We remind the reader that the cross-validation estimates @xmath are
    observable to the user, while the values of @xmath , @xmath , along
    with the noise level @xmath , are not available to the user.
    Nevertheless, @xmath can serve as a proxy for @xmath according to
    @xmath , and this is verified by the plots in Figure 1. @xmath can
    also provide an upper bound on @xmath , as is detailed in Section
    5.1.

2.  The theoretical bound @xmath is seen to be tight, when compared with
    the observed concentration bounds in Figure 1.

3.  With high probability, the estimate @xmath using @xmath out of the
    alloted @xmath measurements will be a better estimate of @xmath than
    the OMP estimate: @xmath . With overwhelming probability , the
    estimate @xmath will result in error @xmath . We note that the
    estimates @xmath and @xmath correspond to accuracy parameters @xmath
    and @xmath in ( 3.35 ), indicating that @xmath is a good heuristic
    to determine when enough CV measurements have been reserved.

4.  The OMP-CV estimate @xmath will have more pronounced improvement
    over the OMP estimate @xmath when there is larger discrepancy
    between the true sparsity @xmath of @xmath and the upper bound
    @xmath used by OMP (in Figure (1), @xmath and @xmath ). In contrast,
    OMP-CV will not outperform OMP in approximation accuracy when @xmath
    is close to @xmath ; however, the multiplicative relation @xmath
    guarantees that OMP-CV will not underperform OMP, either.

### 3.8 Beyond compressed sensing

The Compressed Sensing setup can be viewed within the more general class
of underdetermined linear inverse problems , in which @xmath is to be
reconstructed from a known @xmath underdetermined matrix @xmath and
lower dimensional vector @xmath using a decoding algorithm @xmath ; in
this broader context, @xmath is given to the user, but not necessarily
specified by the user as in compressed sensing. In many cases, a prior
assumption of sparsity is imposed on @xmath , and an iterative decoding
algorithm for solving the LASSO problem @xmath will be used to
reconstruct @xmath from @xmath [ 30 ] . If it is possible to take on the
order of @xmath additional measurements of @xmath by an @xmath matrix
@xmath satisfying the conditions of Lemma , then all of the analysis
presented in this paper applies to this more general setting. In
particular, the error @xmath at up to @xmath successive approximations
@xmath of the decoding algorithm @xmath may be bounded from below and
above using the quantities @xmath , and the final approximation @xmath
to @xmath can be chosen from among the entire sequence of estimates
@xmath as outlined in Proposition 3.17 ; an earlier estimate @xmath may
approximate @xmath better than a final estimate @xmath which contains
the artifacts of parameter overfitting occurring at later stages of
iteration.

### 3.9 Extensions

We have presented an alternative approach to compressed sensing in which
a certain number @xmath of the @xmath allowed measurements of a signal
@xmath @xmath are reserved to track the error in decoding by the
remaining @xmath measurements, allowing us to choose a best
approximation to @xmath in the metric of @xmath out of a sequence of
@xmath estimates @xmath , and estimate the error between @xmath and its
best approximation by a @xmath -sparse vector, again with respect to the
metric of @xmath . We detailed how the number @xmath of such
measurements should be chosen in terms of desired accuracy @xmath of
estimation, confidence level @xmath in the prediction, and number @xmath
of decoding iterations to be measured; in general, @xmath measurements
suffice. Several important issues remain unresolved; we mention only a
few below.

1.   Noisy measurements. Consider the noisy measurement model,

      -- -------- -- --------
         @xmath      (3.41)
      -- -------- -- --------

    where @xmath , @xmath , and @xmath is a Gaussian random variable
    that accounts for both noise and quantization error on the
    measurements @xmath . Because measurement noise and quantization
    error are unavoidable in any real-world sensing device, any proposed
    compressed sensing technique should extend to the model @xmath .
    Cross validation is studied in in the context of noisy measurements,
    assuming that @xmath is truly sparse and that @xmath is Gaussian
    noise. The experimental results in indicate that cross validation
    works well in the presence of noise, and Proposition 3.17 provides
    intuition as to why. Suppose that @xmath is @xmath -sparse, so that
    the best possible reconstruction to @xmath using @xmath noisy
    measurements @xmath of the @xmath total noisy measurements @xmath is
    bounded by the @xmath norm of @xmath :

      -- -------- -- --------
         @xmath      (3.42)
      -- -------- -- --------

    Alternatively, the noise vector @xmath corresponding to the
    remaining @xmath cross validation measurements has exponentially
    smaller expected @xmath norm @xmath , assuming that @xmath . It
    follows by application of the triangle inequality,

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    whence the approximation error @xmath dominates both lower and upper
    bounds on the noisy cross validation error.

2.   Other cross validation techniques. The cross validation technique
    promoted in this paper corresponds in particular to the technique of
    holdout cross validation in statistics, where a data set is
    partitioned into a single training and cross validation set (as a
    rule of thumb, the cross validation set is usually taken to be less
    than or equal to a third of the size of the training set; in the the
    current paper, we have shown that the Johnson Lindenstrauss lemma
    provides a theoretical justification of how many, or, more
    precisely, how few, cross validation measurements are needed in the
    context of compressed sensing). Other forms of cross validation,
    such as repeated random subsampling cross validation or K-fold cross
    validation, remain to be analyzed in the context of compressed
    sensing. The former technique corresponds to repeated application of
    holdout cross validation, with @xmath cross validation measurements
    out of the total @xmath measurements chosen by random selection at
    each application. The results are then averaged (or otherwise
    combined) to produce a single estimation. The latter technique,
    @xmath -fold cross validation, also corresponds to repeated
    application of holdout cross validation. In this case, the @xmath
    measurements are partitioned into @xmath subsets of equal size
    @xmath , and cross-validation is repeated exactly @xmath times with
    each of the @xmath subsets of measurements used exactly once as the
    validation set. The @xmath results are again combined to produce a
    single estimation. Although Proposition does not directly apply to
    these cross validation models, the experimental results of Section 6
    suggest that, equiped with an @xmath matrix satisfying the
    requirements of Lemma , the application of @xmath fold cross
    validation to subsets of the measurements of size @xmath just large
    enough that @xmath in Proposition 3.17 for fixed accuracy @xmath and
    constant @xmath can be combined to accurately approximate the
    underlying signal with near certainty.

3.   Cross validation in different reconstruction metrics. We have only
    considered cross validation over the metric of @xmath . However, the
    error @xmath , or root mean squared error , is just one of several
    metrics used in image processing for analyzing the quality of a
    reconstruction @xmath to a (known) image @xmath . In fact, the
    @xmath reconstruction error @xmath has been argued to outperform the
    root mean squared error as an indicator of reconstruction quality .
    Unfortunately, Proposition 3.17 cannot be extended to the metric of
    @xmath , as there exists no @xmath analog of the Johnson
    Lindenstrauss Lemma [ 25 ] . However, it remains to understand the
    extent to which cross validation in compressed sensing can be
    applied over a broader class of image reconstruction metrics,
    perhaps using more refined techniques than those considered in this
    paper.

4.   Cross validation with more general CS matrix ensembles. Many more
    compressed sensing matrices in addition to those satisfying the
    requirements of Lemma can be used for cross validation purposes, in
    the sense that a concentration bound similar to ( 3.4.1 ) will hold
    for most input @xmath . Indeed, it has been recently shown [ 39 ]
    that such random partial Fourier matrices, if multiplied on the
    right by an @xmath diagonal matrix of independent and identically
    distributed Bernoulli random variables, will satisfy the
    concentration bound @xmath with slightly larger number of
    measurements @xmath .

## Chapter 4 Free discontinuity problems meet iterative thresholding

### 4.1 Introduction

Free-discontinuity problems describe situations where the solution of
interest is defined by a function and a lower dimensional set consisting
of the discontinuities of the function. Hence, the derivative of the
solution is assumed to be a ‘small’ function almost everywhere except on
sets where it concentrates as a singular measure. This is the case, for
instance, in crack detection from fracture mechanics or in certain
digital image segmentation problems. If we discretize such situations
for numerical purposes, the free-discontinuity problem in the discrete
setting can be re-formulated as that of finding a derivative vector with
small components at all but a few entries that exceed a certain
threshold. This problem is similar to those encountered in Compressed
sensing, where vectors with a small number of dominating components in
absolute value are recovered from a few given linear measurements via
the minimization of related energy functionals. Several iterative
thresholding algorithms that intertwine gradient-type iterations with
thresholding steps have been designed to recover sparse solutions in
this setting. It is natural to wonder if and/or how such algorithms can
be used towards solving discrete free-discontinuity problems. The
current chapter explores this connection, and, by establishing an
iterative thresholding algorithm for discrete free-discontinuity
problems, provides new insights on properties of minimizing solutions
thereof.

#### 4.1.1 Free-discontinuity problems: the Mumford-Shah functional

The terminology ‘free-discontinuity problem’ was introduced by De Giorgi
[ 36 ] to indicate a class of variational problems that consist in the
minimization of a functional, involving both volume and surface
energies, depending on a closed set @xmath , and a function @xmath on
@xmath usually smooth outside of @xmath . In particular,

-   @xmath is not fixed a priori and is an unknown of the problem;

-   @xmath is not a boundary in general, but a free-surface inside the
    domain of the problem.

The best-known example of a free-discontinuity problem is the one
modelled by the so-called Mumford-Shah functional [ 56 ] , which is
defined by

  -- -------- --
     @xmath   
  -- -------- --

The set @xmath is a bounded open subset of @xmath , @xmath are fixed
constants, and @xmath . Here @xmath denotes the @xmath -dimensional
Hausdorff measure. Throughout this paper, the dimension of the
underlying Euclidean space @xmath will always be @xmath or @xmath . In
the context of visual analysis, @xmath is a given noisy image that we
want to approximate by the minimizing function @xmath ; the set @xmath
is simultaneously used in order to segment the image into connected
components. For a broad overview on free-discontinuity problems, their
analysis, and applications, we refer the reader to [ 3 ] .

If the set @xmath were fixed, then the minimization of @xmath with
respect to @xmath would be a relatively simple problem, equivalent to
solving the following system of equations:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath is the outward-pointing normal vector at any @xmath .
Therefore the relevant unknown in free-discontinuity problems is the set
@xmath . Ensuring the existence of minimizers @xmath of @xmath is a
challenging problem because there is no topology on the closed sets that
ensures

-   compactness of minimizing sequences and

-   lower semicontinuity of the Hausdorff measure.

Indeed, it is well-known, by the direct method of calculus of variations
[ 28 , Chapter 1] , that the two previous conditions ensure the
existence of minimizers. However, the problem becomes more manageable if
we restrict our domain to functions @xmath , and make the identification
@xmath where @xmath is the well-defined discontinuity set of @xmath . In
this case, we need to work only with a topology on the space @xmath of
bounded variation, and no set topology is anymore required.
Unfortunately the space @xmath is ‘too large’; it contains Cantor-like
functions whose approximate gradient vanishes, @xmath , almost
everywhere, and whose discontinuity set has measure zero, @xmath . As
these functions are dense in @xmath , the problem is trivialized; see [
3 ] for details.
Nevertheless, it is possible to give a meaningful formulation of the
functional @xmath if we exclude such functions and restrict @xmath to
the space @xmath constituted of @xmath -functions with vanishing Cantor
part. If we assume again @xmath , the solution can be recast as the
minimization of

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

The existence of minimizers in @xmath for the functional ( 4.1 ) was
established by Ambrosio on the basis of his fundamental compactness
theorem in [ 2 ] , see also [ 3 , Theorem 4.7 and Theorem 4.8] .

#### 4.1.2 @xmath-convergence approximation to free-discontinuity
problems

The discontinuity set @xmath of a @xmath -function @xmath is not an
object that can be easily handled, especially numerically. This
difficulty gave rise to the development of approximation methods for the
Mumford-Shah functional and its minimizers where sets are no longer
involved, and instead substituted by suitable indicator functions . In
order to understand the theoretical basis for these approximations, we
need to introduce the notion of @xmath -convergence, which is today
considered one of the most successful notions of ‘variational
convergence’; we state only the definition of @xmath -convergence below,
but refer the reader to [ 28 , 13 ] for a broad introduction.

###### Definition 4.1.1.

Let @xmath be a metric space ¹ ¹ 1 Observe that by [ 28 , Proposition
8.7] suitable bounded sets @xmath endowed with the weak topology induced
by a larger Banach space are indeed metrizable, so this condition is not
that restrictive. and let @xmath be functions for @xmath . We say that
@xmath @xmath -converges to @xmath if the following two conditions are
satisfied:

-    for any sequence @xmath converging to @xmath ,

      -- -------- --
         @xmath   
      -- -------- --

-    for any @xmath , there exists a sequence @xmath converging to
    @xmath such that

      -- -------- --
         @xmath   
      -- -------- --

One important consequence of Definition 4.1.1 is that if a sequence of
functionals @xmath @xmath -converges to a target functional @xmath ,
then the corresponding minimizers of @xmath also converge to minimizers
of @xmath , see [ 28 , Corollary 7.30] .
We define now

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

over the domain @xmath , along with the related functional

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

Note that at the minimizer @xmath of @xmath , the function @xmath tends
to indicate the discontinuity set @xmath of the functional ( 4.1 ) as
@xmath . In [ 4 ] Ambrosio and Tortorelli proved the following @xmath
-approximation result:

###### Theorem 4.1.2 (Ambrosio-Tortorelli ’90).

For any infinitesimal sequence @xmath , the functional @xmath @xmath
-converges in @xmath to the functional

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

#### 4.1.3 Discrete approximation

In fact, the Mumford-Shah functional is the continuous version of a
previous discrete formulation of the image segmentation problem proposed
by Geman and Geman in [ 47 ] ; see also the work of Blake and Zisserman
in [ 9 ] . Let us recall this discrete approach. For simplicity let
@xmath (as for image processing problems), @xmath , and let @xmath ,
@xmath be a discrete function defined on @xmath , for @xmath . Define
@xmath to be the truncated quadratic potential, and

  -- -------- -------- -------- -------- -------
     @xmath   @xmath   @xmath            (4.5)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- -------

Chambolle [ 20 , 21 ] gave formal clarification as to how the discrete
functional @xmath approximates the continuous functional @xmath of
Ambrosio: discrete sequences can be interpolated by piecewise linear
functions in such a way as to allow for discontinuities when the
discrete finite differences of the sampling values are large enough. On
the basis of this identification of discrete functions on @xmath and
functions defined on the ‘continuous domain’ @xmath , we have the
following result:

###### Theorem 4.1.3 (Chambolle ’95).

The functional @xmath @xmath -converges in @xmath (the space of
Borel-measurable functions, which is metrizable, see [ 21 ] for details)
to

  -- -------- --
     @xmath   
  -- -------- --

as @xmath , where @xmath is the so-called ‘cab-driver’ measure defined
below.

Basically @xmath measures the length of a curve only through its
projections along horizontal and vertical axes; for a regular @xmath
curve @xmath , with @xmath , we have

  -- -------- --
     @xmath   
  -- -------- --

The reason this anisotropic (or, direction dependent) measure appears,
in place of the Hausdorff measure in the Mumford-Shah functional, is due
to the approximation of derivatives by finite differences defined on a
‘rigid’ squared geometry. A discretization of derivatives based on
meshes adapted to the morphology of the discontinuity indeed leads to
precise approximations of the Mumford-Shah functional [ 22 , 12 ] .

#### 4.1.4 Free-discontinuity problems and discrete derivatives

In the literature, several methods have been proposed to numerically
approximate minimizers of the Mumford-Shah functional [ 8 , 12 , 20 , 21
, 54 ] . In particular, a relaxation algorithm, based essentially on
alternated minimization of a finite element approximation of the
Ambrosio and Tortorelli functional ( 4.3 ), leads to iterated solutions
of suitable elliptic PDEs, where the differential part includes the
auxiliary variable @xmath which encodes and indicates information about
the discontinuity set. These implementations are basically finite
dimensional approximations to the following algorithm: Starting with
@xmath , iterate

  -- -------- --
     @xmath   
  -- -------- --

However, neither has a proof of convergence of this iterative process to
its stationary points been explicitly provided in the literature, nor
have the properties of such stationary points been investigated,
especially in case of genuine inverse problems (see the discussion in
Subsection 4.1.4 ).

In this paper, we take a different approach and investigate how
minimization of the @xmath -approximating discrete functionals ( 4.5 )
can be implemented efficiently by iterative thresholding on the discrete
derivatives. Unlike the aforementioned approach, we will be able to
provide a rigorous proof of convergence to stationary points, which
coincide with local minimizers of the discrete Mumford-Shah functional.
Moreover, we are able to characterize stability properties of such
stationary points, and demonstrate the stability of global minimizers of
the discrete Mumford Shah functional.

Let us recall: the solutions @xmath of a free-discontinuity problem are
supposed to be smooth out of a minimal ipersurface @xmath . This means
that the distributional derivative of @xmath is a ‘small function’
everywhere except on @xmath where it coincides with a singular measure.
In the discrete approximation @xmath , the vector of finite differences
@xmath corresponds to a piecewise constant function that is small
everywhere except for a few locations, corresponding to @xmath , that
approximate the discontinuity set @xmath . So, in terms of derivatives,
solutions of @xmath are vectors having only few large entries. In the
next section, we clarify how we can indeed work with just derivatives
and forget the primal problem.

##### The 1-D case

Let us assume for simplicity that the dimension @xmath , the domain
@xmath , and the parameters @xmath . Denote by @xmath a discrete
function defined on @xmath , for @xmath ; note that the vector @xmath
for @xmath . In this setting, the discrete functional @xmath reduces to

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where we recall that @xmath . Since no geometrical anisotropy is now
involved ( @xmath ), it is possible to show that this discrete
functional @xmath -converges precisely to the corresponding Mumford-Shah
functional on intervals [ 20 ] .
For @xmath we define the discrete derivative as the matrix @xmath that
maps @xmath into @xmath , given by

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

It is not too difficult to show that

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the pseudo-inverse matrix of @xmath (in the
Moore-Penrose sense; note that @xmath maps @xmath into @xmath and is an
injective operator) and @xmath is a constant vector which depends on
@xmath , and the values of its entries coincide with the mean value
@xmath of @xmath . Therefore, any vector @xmath is uniquely identified
by the pair @xmath .

Since constant vectors comprise the null space of @xmath , the
orthogonality relation @xmath holds for any vector @xmath and any
constant vector @xmath . Here the scalar product @xmath is the standard
Euclidean scalar product, which induces the Euclidean norm @xmath .
Using this orthogonality property, we have that

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

Hence, with a slight abuse of notation, we can reformulate the original
problem in terms of derivatives, and mean values, by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath and @xmath . Of course at the minimizer @xmath we have
@xmath , since this term in @xmath does not depend on @xmath .
Therefore, @xmath does not play any role in the minimization and can be
neglected. Once the minimal derivative vector @xmath is computed, we can
assemble the minimal @xmath by incorporating the mean value of @xmath as
follows:

  -- -------- --
     @xmath   
  -- -------- --

##### The 2-D case, discrete Schwartz conditions, and constrained
optimization

Let us assume now @xmath , and again @xmath . Denote @xmath , @xmath , a
discrete function defined on @xmath , @xmath , and

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

In two dimensions, we have to consider the derivative matrix @xmath that
maps the vector @xmath to the vector composed of the finite differences
in the horizontal and vertical directions @xmath and @xmath
respectively, given by

  -- -------- --
     @xmath   
  -- -------- --

Note that its range @xmath is a @xmath -dimensional subspace because
@xmath for constant vectors @xmath . Again, we have the
differentiation-integration formula, given by

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the pseudo-inverse matrix of @xmath (in the
Moore-Penrose sense); note that @xmath maps @xmath injectively into
@xmath . Also, @xmath is a constant vector that depends on @xmath , and
the values of its entries coincide with the mean value @xmath of @xmath
.
Proceeding as before and again with a slight abuse of notation, we can
reformulate the original discrete functional @xmath in terms of
derivatives, and mean values, by

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

where @xmath , and @xmath . Of course @xmath is again assumed at the
minimizer @xmath , since this latter term in @xmath does not depend on
@xmath . However, in order to minimize only over vectors in @xmath that
are derivatives of vectors in @xmath , we must minimize @xmath subject
to the constraint @xmath .

The @xmath linearly independent constraints @xmath are equivalent to the
discrete Schwartz constraints ² ² 2 These discrete conditions correspond
to the well-known Schwartz mixed derivative theorem for which @xmath for
any @xmath . ,

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

that establish the equivalence of the length of the paths from @xmath to
@xmath , whether one moves in vertical first and then in horizontal
direction or in horizontal first and then in vertical direction (see
Figure 4.1 ).
In short, we arrive at the following constrained optimization problem:

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

for @xmath and @xmath . Once the minimal derivative vector @xmath is
computed, we can assemble the minimal @xmath by incorporating the mean
value of @xmath as follows:

  -- -------- --
     @xmath   
  -- -------- --

##### Regularization of inverse problems by means of the Mumford-Shah
constraint

The Mumford-Shah regularization term

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

has been used frequently in inverse problems for image processing [ 41 ,
62 ] , such as inpainting and tomographic inversion. Despite the
successful numerical results observed in the aforementioned papers for
the minimization of functionals of the type

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

where @xmath is a bounded operator which is not boundedly invertible, no
rigorous results on existence of minimizers are currently available in
the literature. Indeed, the Ambrosio compactness theorem [ 2 ] used for
the proof of the case @xmath does not apply in general. A few attempts
towards using the regularization @xmath for inverse problems in fracture
detection appear in the work of Rondi [ 63 , 64 , 65 ] , although
restrictive technical assumptions on the admissible discontinuities of
the solutions are required.

As one of the contributions to this paper, we show that discretizations
of regularized functionals of the type ( 4.10 ) always have minimizers
(see Theorem 4.2.2 ). More precisely, these discretizations correspond
to functionals of the form,

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

and we prove that such functionals admit minimizers. Note that the
discrete Mumford-Shah approximation @xmath can be written in this form.
We go on to show that such minimizers can be characterized by certain
fixed point conditions, see Theorem 4.6.1 and Theorem 4.6.2 . As a
consequence of these achievements we can prove that global minimizers
are always isolated, although not necessarily unique, whereas local
minimizers may constitute a continuum of unstable equilibria. Hence, our
analysis will shed light on fundamental properties, virtues, and
limitations, of regularization by means of the Mumford-Shah functional
@xmath , and provide a rigorous justification of the numerical results
appearing in the literature.
It is useful to show how the discrete functional ( 4.11 ) can be still
expressed in terms of the sole derivatives for general @xmath . As done
before in the case @xmath , and with the now usual identification @xmath
, we can rewrite the functional in terms of derivatives and mean value
as follows:

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.12)
  -- -------- -------- -------- -- --------

Note that in general we cannot anymore split orthogonally the
discrepancy @xmath into a sum of two terms which depend only on
derivatives @xmath and mean value @xmath respectively. Nevertheless, for
fixed @xmath , it is straightforward to show that @xmath depends on
@xmath via an affine map. Indeed we can compute

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the constant vector with entries identically @xmath .
Here we assume that @xmath , that is a necessary condition in order to
be able to identify the mean value of minimizers (a similar condition is
required anytime we deal with regularization functionals which depend on
the sole derivatives, see, e.g., [ 23 , 73 ] ). By substituting this
expression for @xmath into ( 4.12 ), it is clear that the minimization
of functionals @xmath can be reformulated, in terms of the sole
derivatives, as constrained minimization problems of the form @xmath .

### 4.2 Existence of minimizers for a class of discrete
free-discontinuity problems

In light of the observations above, we can transform the problem of the
minimization of functionals of the type ( 4.10 ), by means of
discretization first and then reduction to sole derivatives, into the
(possibly, but not necessarily) constrained minimization problem:

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

Our first result ensures the existence of minimizers for the constrained
optimization problem ( 4.13 ):

###### Proposition 4.2.1.

Assume @xmath , and fix linear operators @xmath and @xmath , which are
identified in the following with their matrices with respect to the
canonical bases. We also fix @xmath . The constrained minimization
problem

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

has minimizers @xmath .

###### Proof.

We begin by noting that @xmath is well-defined and finite, since @xmath
is bounded from below. It remains to show that there exists a vector
@xmath that satisfies @xmath . Towards this goal, consider the following
partition @xmath of @xmath indexed by the subsets @xmath of the index
set @xmath , as follows:

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

The minimization of @xmath subject to @xmath and constrained to the
closure of the subset @xmath can be reformulated as a quadratic
optimization problem, for which the classical Frank-Wolfe theorem
guarantees the existence of a minimizer @xmath . Now, since @xmath , the
minimal value of @xmath subject to @xmath and over all of @xmath is just
the minimal value from the finite set @xmath ; that is,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
  -- -------- -------- -------- --

and @xmath ∎

In fact, Proposition 4.2.1 extends to a much larger class of
free-discontinuity type minimization problems; by the same reasoning as
before, we arrive at the more general result:

###### Theorem 4.2.2.

The constrained minimization problem

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

has minimizers @xmath for any real-valued parameter @xmath .

The Frank-Wolfe theorem, which guarantees the existence of minimizers
for quadratic programs with bounded objective function, does not apply
to the general case @xmath where the objective function @xmath is not
necessarily quadratic. Nevertheless, with the following generalization
for the Frank-Wolfe theorem, Theorem 4.2.2 follows directly from a
similar argument as for Proposition 4.2.1 .

###### Proposition 4.2.3.

Suppose @xmath is an @xmath positive semidefinite matrix, and suppose
@xmath and @xmath are @xmath vectors. Suppose also that @xmath is a
nonempty convex polyhedral subset of @xmath . The convex optimization
problem

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

admits minimizers for any real parameter @xmath , as long as the
objective function is bounded from below.

For ease of presentation, we reserve the proof of Proposition 4.2.3 to
the Appendix.

From the proof of Theorem 4.2.2 , one could in principle obtain a
minimizer for @xmath by computing a minimizer @xmath for each subset
@xmath using a quadratic program solver , and then minimizing @xmath
over the finite set of points @xmath . Unfortunately, this algorithm is
computationally infeasible as the number of subsets of the index set
@xmath grows exponentially with the dimension @xmath of the underlying
space.

Indeed, the minimization problem ( 4.16 ) is NP hard, as the known
NP-complete problem SUBSET-SUM can be reduced to this problem. A
complete discussion about the NP-hardness of ( 4.16 ) can be found in [
1 ] .

### 4.3 An iterative thresholding algorithm for 1-D free-discontinuity
inverse problems

#### 4.3.1 Overview of the algorithm

In this section, we introduce an algorithm that is guaranteed to
converge to a local minimizer of the real-valued functional @xmath
having the form

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

subject to the conditions:

-   @xmath and @xmath are countable sets of indices, and @xmath is a
    bounded linear operator, which is in the following identified with
    its matrix associated to the canonical basis;

-   the operator @xmath has spectral norm @xmath . Note that this
    requirement is easily met by an appropriate scaling for the
    functional, i.e., we may have to consider instead

      -- -------- --
         @xmath   
      -- -------- --

    This modification leads to minor changes in the analysis that
    follows (see also Subsection 4.7.2 ), and throughout this paper we
    assume, without loss of generality, that @xmath ;

-   the parameter @xmath is in the range @xmath . In case the index set
    @xmath is finite , only the restriction @xmath is necessary.

We note that the scaled 1D discrete Mumford-Shah functional @xmath is
clearly a functional of the form ( 4.18 ) having @xmath , index set
@xmath , parameter @xmath , and operator @xmath . As shown in the
Appendix, the operators @xmath satisfy the uniform bound @xmath ,
independent of dimension, so a scaling factor is not needed in this
case.

In the following, we will not minimize @xmath directly. Instead, we
propose a majorization-minimization algorithm for finding solutions to
@xmath , motivated by the recent application of such algorithms for
minimizing energy functionals arising in sparse signal recovery and
image denoising [ 10 , 30 ] . More precisely, consider the following
surrogate objective function,

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

The surrogate functional @xmath satisfies @xmath everywhere, with
equality if and only if @xmath , and is such that the sequence

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

obtained by successive minimizations of @xmath in @xmath for fixed
@xmath results in a nonincreasing sequence of the original functional
@xmath (see Lemmas 4.3.1 and 4.3.2 ). We will study the implementation
and the convergence properties of the iteration ( 4.20 ) as follows:

-   in Section @xmath , we review the standard properties of
    majorization-minimization iterations,

-   in Section @xmath , we explicitly compute @xmath -global minimizers
    of the surrogate functional @xmath , for @xmath fixed;

-   in Section @xmath we discuss connections between the resulting
    thresholding functions and thresholding functions used in compressed
    sensing,

-   in Sections @xmath , @xmath , and @xmath , we show that the sequence
    @xmath defined by ( 4.20 ) will converge to a stationary value
    @xmath , starting from any initial value @xmath for which @xmath ,

-   in Section @xmath , we show that such stationary values @xmath are
    also local minimizers of the original functional @xmath that satisfy
    a certain fixed point condition, and

-   in Section @xmath , it is shown that any global minimizer of @xmath
    is among the set of possible fixed points @xmath of the iteration
    @xmath .

By means of the thresholding algorithm, we also show that global
minimizers of the functional @xmath are isolated, and moreover possess a
certain segmentation property that is also shared by fixed points of the
algorithm.

#### 4.3.2 Preliminary lemmas

The lemmas in this section are standard when using surrogate functionals
(see and ), and concern general real-valued surrogate functionals of the
form

  -- -------- -- --------
     @xmath      (4.21)
  -- -------- -- --------

The lemmas in this section hold independent of the specific form of the
functional @xmath , but do rely on the restriction that @xmath .

###### Lemma 4.3.1.

If the real-valued functionals @xmath and @xmath satisfy the relation
@xmath and the sequence @xmath defined by @xmath is initialized in such
a way that @xmath , then the sequences @xmath and @xmath are
non-increasing as long as @xmath .

###### Proof.

Since @xmath , also @xmath , and so the operator @xmath is a
well-defined positive operator whose spectrum is contained within a
closed interval @xmath that is bounded away from zero @xmath . We can
then rewrite @xmath as @xmath , from which it follows that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.22)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where the second inequality follows from @xmath being a minimizer of
@xmath . ∎

From Lemma 4.3.1 we obtain the following corollary:

###### Lemma 4.3.2.

As long as the conditions of Lemma are satisfied, one can choose @xmath
sufficiently large such that for all @xmath , @xmath , i.e.,

  -- -------- --
     @xmath   
  -- -------- --

###### Proof.

From Lemma 4.3.1 , it follows that @xmath is a nonincreasing sequence,
therefore it converges, and @xmath for @xmath . The lemma follows from (
4.22 ), and the estimates

  -- -- --
        
  -- -- --

∎

#### 4.3.3 The surrogate functional @xmath, its explicit minimization,
and a new thresholding operator

It is not immediately clear that the surrogate functional @xmath in
@xmath is any easier to manage than its parent functional @xmath .
However, expanding the squared terms on the right hand side of @xmath ,
@xmath can be equivalently expressed as

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where the term @xmath depends only on @xmath , @xmath and @xmath .
Indeed, unlike the original functional @xmath , the surrogate functional
@xmath decouples in the variables @xmath , due to the cancellation of
terms involving @xmath . Because of this decoupling, global @xmath
-minimizers of @xmath , for @xmath fixed, can be computed component-wise
according to

  -- -------- -- --------
     @xmath      (4.23)
  -- -------- -- --------

One can solve @xmath explicitly when e.g. @xmath , @xmath , and @xmath ;
in the general case @xmath , we have the following result:

###### Proposition 4.3.3 (Minimizers of @xmath for @xmath fixed).

.

1.   If @xmath , the minimization problem @xmath can be solved
    component-wise by

      -- -------- -- --------
         @xmath      (4.24)
      -- -------- -- --------

    where @xmath is the ‘thresholding function’ ,

      -- -------- -- --------
         @xmath      (4.25)
      -- -------- -- --------

    Here, @xmath is the inverse of the function @xmath , and @xmath is
    the unique positive value at which

      -- -------- -- --------
         @xmath      (4.26)
      -- -------- -- --------

2.   When @xmath , the general form @xmath still holds, but we have to
    consider two cases:

    1.   If @xmath , the thresholding function @xmath satisfies

          -- -------- -- --------
             @xmath      (4.27)
          -- -------- -- --------

    2.   If, on the other hand, @xmath , the function @xmath satisfies

          -- -------- -- --------
             @xmath      (4.28)
          -- -------- -- --------

In all cases, the function @xmath is continuous except at @xmath , where
@xmath has a jump-discontinuity of size @xmath if @xmath . In
particular, it holds that @xmath while @xmath .

We leave the proof of Proposition 4.3.3 to the Appendix.

###### Remark 4.3.4.

In the particular case @xmath corresponding to classical Mumford-Shah
regularization ( 4.13 ), the thresholding function @xmath has a
particularly simple explicit form:

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

In addition to @xmath and @xmath , the thresholding operator @xmath
corresponding to @xmath can also be computed explicitly, by solving for
the positive root of a suitable polynomial of third degree. In Figure
below, we plot @xmath , and @xmath with parameter @xmath . For general
noninteger values of @xmath , @xmath cannot be solved in closed form.
However, recall the following general properties of @xmath :

-   @xmath is an odd function,

-   @xmath , and

-   @xmath once @xmath .

In fact, we can effectively pre compute @xmath by numerically solving
for the value of @xmath on a discrete set @xmath of points @xmath . At
@xmath , one just needs to solve the real equation

  -- -------- -- --------
     @xmath      (4.30)
  -- -------- -- --------

which can be computed effortlessly via a root-finding procedure such as
Newton’s method: while @xmath satisfies @xmath , set @xmath ; once this
constraint is violated, set @xmath .

### 4.4 A connection to compressive sensing

When @xmath and @xmath , we know from Theorem 4.3.3 that the iterative
algorithm

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

reduces to the component-wise thresholding

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.32)
  -- -------- -------- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

This thresholding function @xmath is referred to as hard-thresholding in
the area of sparse recovery, and the iteration ( 4.32 ) generated by
successive applications of hard thresholding has been previously studied
[ 10 ] . In particular, the iteration ( 4.32 ) was shown in [ 10 ] to
correspond to successive minimization in @xmath for fixed @xmath of the
surrogate functional @xmath corresponding to the @xmath regularized
functional,

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

Here, the @xmath quasi-norm @xmath is defined component-wise by

  -- -------- --
     @xmath   
  -- -------- --

It is not a coincidence that hard thresholding comes out from applying
iterative thresholding to both the @xmath regularized problem ( 4.34 ),
and free-discontinuity problem,

  -- -------- --
     @xmath   
  -- -------- --

Indeed, consider the more general class of free-discontinuity-type
functionals,

  -- -------- -- --------
     @xmath      (4.35)
  -- -------- -- --------

which have an additional degree of freedom in the scaling parameter
@xmath that is present in the discrete MS functional ( 4.5 ), but which
was omitted in the previous section to ease computations.
The free-discontinuity functional @xmath is related to the @xmath
regularized functional @xmath as follows:

###### Proposition 4.4.1.

For any sequence @xmath satisfying @xmath , the functional sequence
@xmath @xmath -converges to @xmath in the metric of @xmath .

###### Proof.

We have to verify the conditions for gamma convergence as established in
Definition( 4.1.1 ). Note that we can write

  -- -------- --
     @xmath   
  -- -------- --

so that @xmath convergence of the full functional sequence @xmath is
established if we can show @xmath convergence of the scalar function
@xmath to @xmath .

-   Suppose that @xmath . Our aim is to show that @xmath :

    -   If @xmath , then @xmath for @xmath sufficiently large, and
        @xmath .

    -   If on the other hand @xmath , then @xmath

-   On the other hand, it is easily seen that @xmath for any @xmath ,
    since

      -- -------- --
         @xmath   
      -- -------- --

The desired result is established, according to ( 4.1.1 ). ∎

###### Remark 4.4.2.

We showed that @xmath @xmath converges to @xmath for sequences @xmath .
More generally, @xmath @xmath converges to @xmath for any @xmath . It is
not hard to show that the thresholding functions @xmath corresponding to
@xmath , which can be derived following the proof of Proposition 4.3.3 ,
converge to the hard thresholding function @xmath corresponding to
@xmath .

To ease presentation, we again take @xmath in the sequel, although all
of the following analysis extends to the case where @xmath is a free
parameter. Because a convergence analysis of the iteration @xmath
corresponding to hard thresholding has been studied already , we omit
the case @xmath and @xmath in the sequel.

### 4.5 Convergence of the iterative thresholding algorithm (4.20)

#### 4.5.1 Fixation of the discontinuity set

We prove now that the sequence @xmath defined by

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.36)
  -- -------- -------- -------- -- --------

or equivalently, according to Proposition , component-wise by

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.37)
  -- -------- -------- -------- -- --------

will converge, granted that @xmath and @xmath . To ease notation, we
define the operator @xmath by its component-wise action,

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

so that the iteration @xmath can be written more concisely in operator
notation as

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

We omit the dependence of @xmath on the parameters @xmath , and the
function @xmath for continuity of presentation. At the core of the
convergence proof is the fact that the ‘discontinuity set’, indicated
below by @xmath , of @xmath must eventually fix during the iteration (
4.37 ), at which point the ‘free-discontinuity’ problem is transformed
into a simpler ‘fixed-discontinuity’ problem.

###### Lemma 4.5.1 (Fixation of the index set @xmath).

Fix @xmath , and @xmath . Consider the iteration

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

and the time-dependent partition of the index set @xmath into ‘small’
set

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.41)
  -- -------- -------- -------- -- --------

and ’large’ set

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.42)
  -- -------- -------- -------- -- --------

where @xmath is the position of the jump discontinuity of the
thresholding function, as defined in Proposition . For @xmath
sufficiently large, this partition fixes during the iteration @xmath ;
that is, there exists a set @xmath such that for all @xmath , @xmath and
@xmath .

###### Proof.

By discontinuity of the thresholding operator @xmath , each sequence
component

  -- -------- -- --------
     @xmath      (4.43)
  -- -------- -- --------

satisfies

1.  @xmath , if @xmath , or

2.  @xmath , if @xmath .

Thus, @xmath if @xmath , or vice versa if @xmath . At the same time,
Lemma implies

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

once @xmath , and @xmath can be taken arbitrarily small. In particular,
@xmath implies that @xmath and @xmath must be fixed once @xmath and
@xmath . ∎

After fixation of the index set @xmath , @xmath and @xmath is an
operator having component-wise action, for @xmath ,

  -- -------- -------- -- -- --------
     @xmath   @xmath         (4.45)
  -- -------- -------- -- -- --------

Here, as in Proposition , the function @xmath is the inverse of the
function @xmath . Again, for ease of presentation, we omit the
dependence of @xmath on the parameters @xmath , and @xmath . For @xmath
the description is similar, and in general, one easily verifies the
equivalence

  -- -- -- --------
           (4.46)
  -- -- -- --------

where @xmath is a surrogate for the convex functional,

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

That is, fixation of the index set @xmath implies that the sequence
@xmath has become constrained to a subset of @xmath on which the map
@xmath agrees with a map @xmath , associated to the convex functional
@xmath . As we will see, this implies that the nonconvex functional
@xmath behaves locally like a convex functional in neighborhoods of
fixed points @xmath , including the global minimizers of @xmath .

#### 4.5.2 On the nonexpansiveness and convergence for @xmath injective

Given that @xmath after a finite number of iterations, we can use
well-known tools from convex analysis to prove that the sequence @xmath
converges. If the operator @xmath is invertible, or, equivalently, if
the operator @xmath maps onto its range and has a trivial null space –
as, for example, does the discrete pseudoinverse @xmath in the 1D
Mumford-Shah approximation – then the mapping @xmath has the nice
property of being a contraction mapping, so that a direct application of
the Banach fixed point theorem ensures exponential convergence of the
sequence @xmath after fixation of the index sets.

###### Theorem 4.5.2.

Suppose @xmath maps onto @xmath and has a trivial null space. Let @xmath
be a lower bound on the spectrum of @xmath . Then the sequence

  -- -------- -- --------
     @xmath      (4.48)
  -- -------- -- --------

as defined in @xmath , is guaranteed to converge in norm. In particular,
after a finite number of iterations @xmath , this mapping takes the form

  -- -------- -- --------
     @xmath      (4.49)
  -- -------- -- --------

and the sequence @xmath converges to the unique fixed point @xmath of
the map @xmath . Moreover, after fixation of of the index set @xmath ,
the rate of convergence becomes exponential:

  -- -- -- --------
           (4.50)
  -- -- -- --------

The proof of Theorem 4.5.2 is deferred to the Appendix.

#### 4.5.3 Convergence for general operators @xmath

Unfortunately, if @xmath is not invertible (that is, if @xmath belongs
to its nonnegative spectrum), then the map @xmath is not necessarily a
contraction, and we can no longer apply the Banach fixed point theorem
to prove convergence of the sequence @xmath . However, as long as @xmath
, we observe by following the proof of Theorem ( 4.5.2 ) that @xmath is
still non-expansive , meaning that for all @xmath , @xmath . The
following Opial’s theorem , here reported adjusted to our notations and
context, gives sufficient conditions under which non-expansive maps
admit convergent successive iterations:

###### Theorem 4.5.3 (Opial’s Theorem).

Let the mapping @xmath from @xmath to @xmath satisfy the following
conditions:

1.  @xmath is asymptotically regular: for all @xmath , @xmath for @xmath
    ;

2.  @xmath is non-expansive: for all @xmath , @xmath ;

3.   the set @xmath of the fixed points of @xmath in @xmath is not
    empty.

Then, for all @xmath , the sequence @xmath converges weakly to a fixed
point in @xmath .

In fact, we already know that @xmath is asymptotically regular, in
addition to being nonexpansive - this follows by application of Lemma
and Lemma to the functional @xmath . Thus, in order to apply Opial’s
theorem, it remains only to show that @xmath has a fixed point; that is,
that there exists a point @xmath for which

  -- -------- --
     @xmath   
  -- -------- --

In more detail, we must prove the existence of a vector @xmath
satisfying

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.51)
  -- -------- -------- -------- -- --------

The following lemma gives a simple yet useful characterization of points
satisfying the fixed point relation @xmath :

###### Lemma 4.5.4.

Suppose @xmath . A vector @xmath satisfies the fixed point relation
@xmath if and only if

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

Alternatively, if @xmath and @xmath , @xmath is satisfied if and only if

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

where in @xmath , the index set @xmath is split into

-   @xmath , and

-   @xmath .

Again, recall the notation @xmath , and observe that the fixed point
relation ( 4.52 ) has a very simple expression when @xmath . The proof
of Lemma 4.5.4 is given in the Appendix.
The fixed point characterization of Lemma will be crucial in the
following theorem that ensures the existence of a fixed point @xmath .
We remind the reader that until now, all of the results of Section
@xmath remain valid in the infinite-dimensional setting @xmath . From
this point on, however, certain results will only hold in finite
dimensions; for clarity, we will account each such situation explicitly.

###### Proposition 4.5.5.

In finite dimensions @xmath , then there exist (global) minimizers of
the convex functional,

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

for all @xmath , and any minimizer @xmath of @xmath satisfies the fixed
point relation @xmath . Restricted to the range @xmath , the statement
is true also in the limit @xmath .

###### Proof.

In the finite-dimensional setting, minimizers necessarily exist for all
@xmath according to Proposition 4.2.3 . We now consider the general
case. Consider the unique decomposition @xmath into a vector @xmath
supported on @xmath and another @xmath supported on @xmath , i.e., the
vectors @xmath and @xmath . Let @xmath and @xmath denote the orthogonal
projections onto the subspaces @xmath and @xmath , respectively.
Consider the operators @xmath and @xmath ; note that clearly @xmath is
satisfied. The functional ( 4.54 ) can be re-written with this
decomposition according to

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

where @xmath is the @xmath -norm on vectors supported on @xmath .
Let @xmath be the orthogonal projection onto the range of @xmath in
@xmath (not to be confused with @xmath , which operates on the space
@xmath ) and let @xmath be the orthogonal projection in @xmath onto the
orthogonal complement of the range of @xmath . Then, fixing @xmath , the
vector @xmath is the solution to the minimization problem

  -- -------- -- --------
     @xmath      (4.56)
  -- -------- -- --------

so that minimizers of the functional @xmath defined by

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.57)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

with @xmath , and @xmath , will yield minimizers of @xmath . Functionals
of the form @xmath were studied in ; there, it is shown that as long as
@xmath , @xmath has minimizers, and any minimizer @xmath can be
characterized by the fixed point relation

  -- -------- -- --------
     @xmath      (4.58)
  -- -------- -- --------

(recall that @xmath is the inverse of the function @xmath ).
In the finite-dimensional setting @xmath , the Euler-Lagrange equations
corresponding to minimizers of the convex functional @xmath as in ( 4.57
) imply the same fixed point relation ( 4.58 ) also, for all @xmath .
By Lemma , the characterization ( 4.58 ) is equivalent to the condition

-   @xmath :

      -- -------- -- --------
         @xmath      (4.59)
      -- -------- -- --------

-   @xmath :

      -- -------- -- --------
         @xmath      (4.60)
      -- -------- -- --------

Making the identification @xmath and @xmath , and rewriting @xmath , and
@xmath , the relations @xmath and @xmath imply the full fixed point
characterization in Lemma 4.5.4 . ∎

###### Remark 4.5.6.

The restriction @xmath that is necessary for the results of this paper
in the infinite dimensional setting @xmath was only used in the proof of
Theorem , where it comes from and is needed there to prove the existence
of minimizers of functionals @xmath of the form @xmath . If that proof
can be extended to functionals of the form @xmath for general @xmath ,
then the restriction @xmath can be dropped in the current paper. For
instance, if we additionally require that @xmath is a bounded operator
from @xmath to @xmath for @xmath then the existence of minimizers would
be guaranteed also for @xmath and @xmath . In this case we could
consider a minimizing sequence @xmath of @xmath , which is necessarily
bounded in @xmath . Therefore, there exists a subsequence @xmath which
weakly converges in @xmath to a point @xmath . This also implies the
weak convergence of the sequence @xmath in @xmath ; note that @xmath ,
for @xmath . By Fatou’s lemma we obtain @xmath and @xmath is a minimizer
of @xmath . However, we still require that @xmath for the proof of
Proposition and for the results of the next section to hold.

Combining the results from this section, we obtain:

###### Theorem 4.5.7.

Suppose @xmath . Starting from any @xmath satisfying @xmath , the
sequence @xmath defined by @xmath as in ( 4.38 ) will converge weakly to
a vector @xmath that satisfies the fixed point condition,

1.  @xmath , if @xmath

2.  @xmath , for @xmath , if @xmath , and

3.  1.   If @xmath :

          -- -------- -- --------
             @xmath      (4.61)
          -- -------- -- --------

    2.   If @xmath and @xmath :

          -- -------- -- --------
             @xmath      (4.62)
          -- -------- -- --------

If the index set @xmath is finite dimensional, the theorem holds for all
@xmath .

###### Proof.

By Lemma , the map @xmath becomes equivalent to a map of the form @xmath
after a finite number of iterations @xmath . By Lemma and Proposition ,
the subset @xmath separates @xmath in the sense that, for all @xmath ,

-   @xmath , if @xmath ,

-   @xmath , if @xmath .

That the sequence @xmath converges to a fixed point of the map @xmath
follows from Opial’s theorem applied to the map @xmath :

1.  the asymptotic regularity of @xmath is a consequence of Lemmas and ;

2.  the nonexpansiveness of @xmath follows from the proof of Theorem (
    4.5.2 ), and

3.  Theorem guarantees that the set of fixed points of @xmath in @xmath
    is nonempty.

The limit @xmath of the sequence @xmath will satisfy the fixed point
conditions of Lemma 4.5.4 . Since weak convergence implies
component-wise convergence, it follows for all @xmath that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.63)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

and the respective lower bound @xmath holds analogously for @xmath . ∎

### 4.6 On minimizers of @xmath

We are now in a position to explore the relationship between limit
vectors @xmath of the iterative thresholding algorithm ( 4.38 ) and
minimizers of the free-discontinuity functional @xmath ( 4.18 ). As a
first but important result in this direction,

###### Theorem 4.6.1.

A point @xmath satisfying the fixed point relation of Theorem is a local
minimizer of the functional @xmath defined in @xmath .

The proof of Theorem 4.6.1 is omitted at present but can be found in the
Appendix. This result should not be surprising, however. Due to the
separation of the entries of any fixed point @xmath , such that @xmath
for @xmath and @xmath , we have also @xmath and @xmath for all @xmath ,
where @xmath is a ball around an equilibrium point @xmath of radius
@xmath sufficiently small. On this neighborhood @xmath of @xmath , the
functional @xmath is convex. Since @xmath is obtained as the limit of a
sequence @xmath in @xmath for which the sequence @xmath is
nonincreasing, one would expect that @xmath minimizes @xmath within this
neighborhood.
More surprising is that global minimizers of @xmath are also fixed
points, as shown in the following theorem. Even though the existence of
such minimizers is only guaranteed in the finite-dimensional setting
(see Proposition 4.2.3 ), the following result is not restricted as
such.

###### Theorem 4.6.2 (Global minimizers of @xmath are fixed points
@xmath).

Any global minimizer @xmath of @xmath satisfies the fixed point
condition of the map @xmath that is given in Theorem 4.5.7 .

The proof of Theorem 4.6.2 is rather long and we defer it to the
Appendix. We reiterate once more that on a ball @xmath around an
equilibrium point @xmath of radius @xmath sufficiently small, the
functional @xmath is convex; following the proof of Theorem 4.6.2 , we
see that @xmath is in fact strictly convex whenever @xmath is a global
minimizer, since the restriction of @xmath to the subspace @xmath of
vectors with support in @xmath must be an injective operator in this
case. Hence a global minimizer is necessarily an isolated minimizer,
whereas we cannot ensure the same property for local minimizers if
@xmath has a nontrivial null-space; in this case, local minimizers may
form continuous sets, as it is shown in the bottom-right box of Figure
4.3 . We conclude the following remark.

###### Corollary 4.6.3.

Minimizers of @xmath are isolated.

### 4.7 Numerical Experiments

#### 4.7.1 Dynamical systems, stability, and equilibria

Iterative thresholding algorithms have a natural interpretation as
discrete-time dynamical systems with nonsmooth right-hand-side, and can
be associated to continuous dynamical systems of the type:

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

The study of the existence, uniqueness, stability, and long-time
behavior of these ODE’s is of fundamental interest in order to clarify
also the stability properties of iterative thresholding algorithms.
Indeed, other than soft-thresholding iterations [ 30 ] , the
corresponding right-hand-side is not Lipschitz continuous and can even
be discontinuous, as is the case for free-discontinuity problems. In [
14 , 42 ] conditions are established for the existence, uniqueness, and
continuous dependence on the initial data (at finite time) of solutions
of dynamical systems with discontinuous right-hand-side. However, very
little is known about long-time properties of such dynamical systems and
about the nature of their equilibrium points.

For several continuous thresholding functions, such as the ones
introduced in [ 30 , 45 , 44 ] , one can easily show, for instance by
means of @xmath -convergence arguments, that equilibrium points depend
continuously on the parameters of the thresholding, see, e.g., [ 44 ,
Theorem 5.1] . Nevertheless, for discontinuous thresholding functions
@xmath such as those studied in this paper, sudden bifurcation phenomena
and instabilities do appear in general. Figure 4.3 shows that multiple
equilibrium points can exist for these thresholding operators and their
number may depend discontinuously on the thresholding shape parameters.
Moreover, as established in Theorem 4.6.2 , global minimizers of @xmath
are always stable equilibria and isolated points, while local minimizers
can be unstable equilibria and form a continuous set, as shown in the
bottom-right box of Figure 4.3 .

#### 4.7.2 Denoising and segmentation of 1-D signals and digital images

In this subsection, we are concerned with numerical experiments in the
use of an iterative thresholding algorithm for the minimization of

  -- -------- -- --------
     @xmath      (4.64)
  -- -------- -- --------

modelling problems of denoising and segmentation.

Note that we introduced an additional regularization parameter @xmath
which has the sole effect of modifying the thresholding function @xmath
as follows

  -- -------- -- --------
     @xmath      (4.65)
  -- -------- -- --------

This thresholding function can be again easily computed by means of an
argument similar to the proof of Proposition 4.3.3 . In Figure
LABEL:1Dsignal we show the results of applications of the iterative
thresholding algorithm ( 4.37 ). In Figure 4.5 we show a comparison of
the use of the thresholding @xmath and the soft-thresholding @xmath (see
its definition in ( 4.104 )); the former promotes the minimization of
the Mumford-Shah constraint @xmath and piecewise smooth solutions,
whereas the latter promotes the minimization of a total variation
constraint [ 67 ] , which is also well-known to produce (almost)
piecewise constant solutions with a perhaps unwanted ‘staircase effect’;
see also [ 23 , Section 4] for details.

#### 4.7.3 Inverse problems

As already mentioned in Subsection 4.1.4 the Mumford-Shah term @xmath is
also used for regularizing inverse problems involving operators @xmath
which are not boundedly invertible. In this section we present a
numerical experiment on the use of the algorithm ( 4.37 ) for 1D
interpolation (Figure 4.6 ). In this case the operator @xmath is a
multiplier by a characteristic function of a subdomain, i.e., @xmath ,
for @xmath ; see [ 41 ] for other numerical examples previously obtained
with the Mumford-Shah regularization.

In Figure 4.6 we show the reconstruction of the noiseless signal of
Figure LABEL:1Dsignal provided information only out of the interval
@xmath which has to be restored. On the left boxes we show the results
due to algorithm ( 4.37 ) and on the left ones the solution computed by
iterative soft-thresholding. In the former the solution is again
piecewise smooth and in the latter a (almost) piecewise constant
solution is instead produced.

### 4.8 Appendix

#### 4.8.1 Proof of Proposition 4.2.3

First, we recall Weierstrass’ Theorem, which is used in the proof of
Proposition below.

###### Theorem 4.8.1 (Weierstrass’ Theorem).

The set of minima of a convex function @xmath over a subset @xmath is
nonempty and compact if @xmath is closed, @xmath is lower semicontinuous
over @xmath , and the function @xmath , given by

  -- -------- -- --------
     @xmath      (4.66)
  -- -------- -- --------

is coercive , i.e., for every sequence @xmath s.t. @xmath , we have
@xmath .

The following two lemmas will be helpful in the proof of Proposition
4.2.3 .

###### Lemma 4.8.2.

Let @xmath be a convex function defined on @xmath having the general
form @xmath , for some @xmath . Fix @xmath and @xmath in @xmath . If
@xmath is bounded above and below on the ray @xmath , then @xmath is
constant on the line @xmath .

###### Proof.

Let @xmath , and note that @xmath is convex because @xmath is convex.
Moreover, @xmath has the general form @xmath where @xmath is a
polynomial in @xmath of order at most @xmath . Without loss of
generality, suppose @xmath for all values of @xmath . Then there exists
a sequence of points @xmath , @xmath for @xmath , for which @xmath is a
convergent sequence; let us denote the limit of this sequence by @xmath
.

1.   Case 1: @xmath . To repeat,

      -- -------- -- --------
         @xmath      (4.67)
      -- -------- -- --------

    Since @xmath , it follows that all coefficients in @xmath of degree
    2 must vanish. In turn, then, @xmath , has the implication that for
    each @xmath , one of the coefficients @xmath or @xmath must vanish
    as well. Following in the same manner, we conclude that all linear
    coefficients in @xmath also vanish, leaving only the possibility
    that @xmath is a constant function.

2.   Case 2: @xmath : The proof in this case is identical to that of the
    previous case, and as such we leave the details to the reader.

∎

###### Lemma 4.8.3.

Suppose @xmath is a convex function defined on @xmath that is bounded
from below, and has the property that if @xmath is bounded above on a
ray @xmath , then @xmath is constant on the line @xmath . Then if @xmath
is constant on the line @xmath , @xmath is also constant on any parallel
line @xmath .

###### Proof.

Let @xmath which by assumption is a constant function @xmath , and let
@xmath . Fix @xmath , and let @xmath be the point @xmath , i.e. @xmath .
By convexity of @xmath , we have that

  -- -------- -- --------
     @xmath      (4.68)
  -- -------- -- --------

for a constant @xmath . It follows that @xmath is bounded above by
@xmath on the ray @xmath , from which it follows, by assumption, that
@xmath is constant on the line @xmath . ∎

We now prove Proposition 4.2.3 . Choosing @xmath , we define the
(nonempty) set

  -- -------- -- --------
     @xmath      (4.69)
  -- -------- -- --------

Obviously, the set @xmath is convex and closed. By assumption, @xmath is
bounded from below on @xmath and hence on @xmath . Therefore, if @xmath
is bounded, then Weierstrass’ Theorem yields the desired result.
Thus, we may assume that @xmath is unbounded. Then, the convexity of
@xmath implies that @xmath contains a ray @xmath . Denote by @xmath a
set of @xmath rays in @xmath corresponding to linearly independent
vectors @xmath , so that any ray in @xmath can be expressed as a linear
combination of the @xmath . By definition of @xmath and by the
assumption, @xmath is bounded on @xmath , hence, @xmath is constant on
each of the the lines @xmath , according to Lemma ( 4.8.2 ). From Lemma
( 4.8.3 ), it follows that @xmath is constant along each line @xmath for
arbitrary @xmath , from which we deduce that @xmath is constant along
any line @xmath for arbitrary @xmath . Thus, we project @xmath onto the
subspace of @xmath that is orthogonal to @xmath ; call this subspace
@xmath .
From the foregoing arguments, we have

  -- -------- -- --------
     @xmath      (4.70)
  -- -------- -- --------

As @xmath is still a convex polyhedral set, and by construction @xmath
contains no rays, Weierstrass’ Theorem yields the desired result.

#### 4.8.2 On uniform boundedness of @xmath

The aim of the second part of the appendix is to prove the uniform bound
@xmath eluded to in Section 3.1. Again, @xmath denotes the spectral norm
of the matrix @xmath , and @xmath is the pseudo-inverse of the discrete
derivative matrix @xmath as given by ( 4.6 ), with the identification
@xmath . From the expression for @xmath , and the knowledge that @xmath
is the identity operator and @xmath is self-adjoint, the @xmath matrix
@xmath is identified as follows:

  -- -------- -- --------
     @xmath      (4.71)
  -- -------- -- --------

It is well-known that the spectral norm of an @xmath matrix can be
bounded by the more manageable entry-wise Frobenius norm, according to

  -- -------- -- --------
     @xmath      (4.72)
  -- -------- -- --------

As such, we need only to bound the sum of the squares of the entries of
@xmath . The sum @xmath over entries in the first row of @xmath is given
by @xmath , using the familiar formula @xmath . The analogous sum over
entries in the @xmath row of @xmath is seen inductively to satisfy
@xmath . The total sum @xmath is then @xmath , and we arrive at the
desired uniform bound:

  -- -------- -- --------
     @xmath      (4.73)
  -- -------- -- --------

#### 4.8.3 Proof of Proposition 4.3.3

In order to help the reading of the current proof, as well as the proofs
of Theorem 4.5.7 and Theorem 4.6.2 in later appendices, we report in
Table 1 the notation of the functions used in the proof of Proposition
4.3.3 for the definition of @xmath .

Consider the functions

  -- -------- -- --------
     @xmath      (4.74)
  -- -------- -- --------

and

  -- -------- -- --------
     @xmath      (4.75)
  -- -------- -- --------

The proof reduces to solving for

  -- -------- -- --------
     @xmath      (4.76)
  -- -------- -- --------

as a function of @xmath . Since @xmath , the function @xmath will be
odd, and since also @xmath , we can, without loss of generality,
restrict the domain of interest to @xmath . On this domain, @xmath is
nonnegative, since @xmath when @xmath and @xmath . Hence, we can
restrict the minimization of @xmath to @xmath .

It will be convenient to split the proof into two cases: @xmath and
@xmath .

1.  We first analyze the case @xmath .
    Note that

      -- -------- -------- -------- -------- --------
         @xmath   @xmath   @xmath            (4.77)
                           @xmath   @xmath   
      -- -------- -------- -------- -------- --------

    so that the minimization @xmath naturally splits into the following
    two cases:

    1.  If @xmath , the minimizer has to be searched in @xmath , hence

          -- -------- -- --------
             @xmath      (4.78)
          -- -------- -- --------

        where @xmath is the functional inverse of the increasing, and
        continuous function

          -- -------- -- --------
             @xmath      (4.79)
          -- -------- -- --------

    2.  On the other hand, if @xmath , the minimizer has to be searched
        in @xmath , hence

          -- -------- -------- -------- --
             @xmath   @xmath   @xmath   
          -- -------- -------- -------- --

    By implicit differentiation of the functional relation @xmath , it
    is clear that the functions @xmath and @xmath are strictly
    increasing functions in @xmath . Indeed, we have the bounds

      -- -------- --
         @xmath   
      -- -------- --

    and

      -- -------- -------- -------- --
         @xmath   @xmath   @xmath   
                  @xmath   @xmath   
                  @xmath   @xmath   
      -- -------- -------- -------- --

    since @xmath , and

      -- -------- -- --------
         @xmath      (4.80)
      -- -------- -- --------

    Also observe that @xmath , and @xmath . This leads us to immediately
    conclude that

    -   If @xmath , then @xmath (from @xmath ).

    -   If @xmath , then @xmath , so that @xmath .

    -   Since @xmath while @xmath , the intermediate value theorem
        implies that there exists a unique value @xmath lying strictly
        within the interval @xmath at which

          -- -------- -- --------
             @xmath      (4.81)
          -- -------- -- --------

        and

          -- -------- -------- -------- -- --------
             @xmath   @xmath   @xmath      (4.82)
          -- -------- -------- -------- -- --------

        At @xmath , @xmath is not uniquely defined and is realized at
        @xmath and at @xmath . In this case, we identify @xmath for the
        sequel; as will be made clear, this will not cause problems in
        the ensuing analysis. Finally, note that

    -   At @xmath , the function @xmath has a discontinuity @xmath that
        is strictly positive, as long as @xmath . Indeed, on the one
        hand, we know that @xmath , on the other hand, @xmath . This
        follows because @xmath , and

          -- -------- --
             @xmath   
          -- -------- --

2.  The analysis of the case @xmath is left to the reader since it
    follows a similar argument as for @xmath .

#### 4.8.4 Proof of Theorem 4.5.2

We assume that the operator @xmath is nonnegative, so that its spectrum
lies within an interval @xmath with @xmath , and the operator @xmath has
norm @xmath . In particular, if @xmath is invertible, then the
inequality @xmath is strict, and so @xmath .
We wish to show that the map @xmath with component-wise action

  -- -------- -------- -- -- --------
     @xmath   @xmath         (4.83)
  -- -------- -------- -- -- --------

is a contraction. To this end, let @xmath be arbitrary vectors in @xmath
.

1.  If the index @xmath , then

      -- -------- --
         @xmath   
      -- -------- --

2.  If the index @xmath , then we split the analysis in two cases @xmath
    and @xmath :

    1.  for @xmath , we have

          -- -------- -------- -------- -------- --------
             @xmath   @xmath   @xmath            (4.84)
                               @xmath   @xmath   
                               @xmath   @xmath   
          -- -------- -------- -------- -------- --------

        where the second equality is an application of the mean value
        theorem, which is valid since @xmath is differentiable. The
        final inequality above follows from implicit differentiation of
        the relation

          -- -------- --
             @xmath   
          -- -------- --

        and the observation that @xmath (see the proof of Proposition
        4.3.3 );

    2.  for @xmath , by analyzing all cases, we get also that

          -- -------- -------- -------- -- --------
             @xmath   @xmath   @xmath      (4.85)
          -- -------- -------- -------- -- --------

Together, we have

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.86)
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

As @xmath is a contraction, we arrive at the stated result by
application of the Banach Fixed Point Theorem.

#### 4.8.5 Proof of Lemma 4.5.4

If @xmath , then @xmath , which is satisfied if and only if @xmath as
stated. It remains to analyze the case @xmath , and, again, we split the
argument in the cases @xmath and @xmath .

1.  First suppose @xmath . Using the notation @xmath , the fixed point
    characterization @xmath translates to

      -- -------- --
         @xmath   
      -- -------- --

    But of course @xmath is the unique value at which @xmath , and so
    this implies that

      -- -------- -- --------
         @xmath      (4.87)
      -- -------- -- --------

    and, by reversing operations, the relation @xmath in turn implies
    the fixed point condition @xmath .

2.  The case @xmath , which is similar, is left to the reader.

#### 4.8.6 Proof of Theorem 4.6.1

The proof will be much simplified by the following lemma which
characterizes vectors such as @xmath that satisfy the fixed point
relations ( 4.61 ) or ( 4.62 ):

###### Lemma 4.8.4.

If @xmath and @xmath are such that

  -- -------- -- --------
     @xmath      (4.88)
  -- -------- -- --------

then @xmath .

###### Proof.

For any @xmath and @xmath , the following holds because @xmath :

  -- -- -- --------
           (4.89)
  -- -- -- --------

If in addition @xmath and @xmath satisfy @xmath , then the desired
result is achieved by virtue of the equality @xmath . ∎

Let us show now the proof of Theorem 4.6.1 . By Lemma , it suffices to
show that at a fixed point @xmath defined by ( 4.61 ) or ( 4.62 ), any
perturbation @xmath with norm @xmath will satisfy

  -- -------- -- --------
     @xmath      (4.90)
  -- -------- -- --------

After expanding the left-hand-side above, the inequality @xmath is seen
to be equivalent to

  -- -------- -- --------
     @xmath      (4.91)
  -- -------- -- --------

At this point, it is convenient to consider the summation over @xmath
and @xmath separately.
By Lemma , the first summand above vanishes over @xmath and

1.  if @xmath , then @xmath ;

2.  if @xmath , then @xmath .

With respect to the second summation, observe from Proposition that for
all @xmath , @xmath for @xmath , so that this summation vanishes over
@xmath for any perturbation @xmath satisfying the component-wise
inequality @xmath . Similarly, @xmath for @xmath , so that for any
perturbation @xmath satisfying component-wise @xmath , we have that

  -- -------- -- --------
     @xmath      (4.92)
  -- -------- -- --------

The desired result follows if we can show that

1.  @xmath : @xmath , for all @xmath

2.  @xmath :

    1.  @xmath for all @xmath , and

    2.  @xmath , for all @xmath .

The inequality in @xmath follows directly from Lemma ; by symmetry,
@xmath and @xmath follow if, for any @xmath ,

  -- -------- -- --------
     @xmath      (4.93)
  -- -------- -- --------

When @xmath , the right-hand-side is identically zero and the result
holds. When @xmath , differentiating the right-hand-side gives that
@xmath has a local minimum at @xmath , at which @xmath , and, at the
endpoint, @xmath

#### 4.8.7 Proof of Theorem 4.6.2

Suppose that @xmath is a minimizer of the functional @xmath . Consider
the partition of the index set @xmath into @xmath and @xmath , and note
that @xmath , or else @xmath would not be finite. As in the proof of
Theorem ( 4.5.5 ), consider the unique decomposition @xmath into a
vector @xmath supported on @xmath and another @xmath supported on @xmath
. Again, let @xmath and @xmath denote the orthogonal projections onto
the subspaces @xmath and @xmath , respectively, and consider the
operators @xmath and @xmath .
By minimality of @xmath , if we fix @xmath , the vector @xmath satisfies
@xmath , where

  -- -------- -- --------
     @xmath      (4.94)
  -- -------- -- --------

Since all coefficients in @xmath have absolute value @xmath , the vector
@xmath also minimizes the functional

  -- -------- -- --------
     @xmath      (4.95)
  -- -------- -- --------

or, else, the vector @xmath minimizing @xmath would satisfy @xmath ,
contradicting the minimality of @xmath . In fact, @xmath must be the
unique vector minimizing @xmath . For, if another vector @xmath also
minimized @xmath , then the operator @xmath would have a nontrivial null
space containing the span of some nonzero vector @xmath , so that all
vectors in the affine space @xmath would be minimal solutions for @xmath
. In this case, we would have also the freedom of choosing from this
affine subspace a vector @xmath having one coefficient @xmath satisfying
@xmath . But such a vector @xmath satisfies @xmath , contradicting the
minimality of @xmath .
It follows that the operator @xmath must have trivial null space, and
@xmath is the unique minimal least squares solution to @xmath ,
well-known to be explicitly given by

  -- -------- -- --------
     @xmath      (4.96)
  -- -------- -- --------

so that @xmath is the unique orthogonal projection of @xmath onto the
range of @xmath . Actually @xmath is the orthogonal projection onto the
range of @xmath , due to the non-triviality of the null space of @xmath
. Therefore we have @xmath . It easily follows that

  -- -------- -- --------
     @xmath      (4.97)
  -- -------- -- --------

or, in other words,

  -- -------- -- --------
     @xmath      (4.98)
  -- -------- -- --------

Now, on the other hand, by observing that any optimal variable @xmath
for fixed @xmath depends on @xmath via the relationship @xmath , we
easily infer that the vector @xmath minimizes

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.99)
  -- -------- -------- -------- -- --------

where @xmath denotes the orthogonal projection operator onto the
orthogonal complement of the range of @xmath .
Consider the convex functional,

  -- -------- -- ---------
     @xmath      (4.100)
  -- -------- -- ---------

and note that @xmath , while at the same time @xmath by virtue of the
fact that @xmath . For @xmath it follows that @xmath is also a minimizer
of @xmath , and so satisfies the Euler-Lagrange equations ,

  -- -------- -- ---------
     @xmath      (4.101)
  -- -------- -- ---------

which imply the fixed point conditions

  -- -------- -- ---------
     @xmath      (4.102)
  -- -------- -- ---------

For @xmath one uses results from [ 30 ] to conclude that

  -- -------- -- ---------
     @xmath      (4.103)
  -- -------- -- ---------

where @xmath is the so-called soft-thresholding , defined component-wise
@xmath , where

  -- -------- -- ---------
     @xmath      (4.104)
  -- -------- -- ---------

(Actually, [ 30 , Proposition 3.10] only states that any fixed point of
( 4.103 ) is a minimizer of ( 4.100 ); nevertheless the converse also
holds, see [ 43 , Remarks (1), pag. 2515] .) The fixed-point condition (
4.103 ) implies

  -- -------- -- ---------
     @xmath      (4.105)
  -- -------- -- ---------

It remains to verify that

-   @xmath , if @xmath , and

-   @xmath , for @xmath , and @xmath , for @xmath , if @xmath .

We show these conditions for @xmath only, as the case @xmath is proved
with an analogous argument.

1.  We first show that @xmath if @xmath . From the first part of the
    proof, we know that at a minimizer @xmath , the functional @xmath
    can be written as

      -- -------- -- ---------
         @xmath      (4.106)
      -- -------- -- ---------

    Note that at this point we make explicit use of the finite
    cardinality of @xmath . Fix @xmath and any perturbation @xmath ,
    @xmath , along the coordinate @xmath (here, @xmath is the @xmath
    vector of the canonical basis). Consider the rank-one operator
    @xmath , where we use @xmath to denote the orthogonal projection
    onto the one-dimensional subspace spanned by @xmath . Observe that
    @xmath . Since @xmath is orthogonal to the argument @xmath under the
    @xmath penalty in @xmath , the minimality condition @xmath can be
    written as

      -- -- -- -------- -------- ---------
               @xmath            (4.107)
               @xmath   @xmath   
                        @xmath   
      -- -- -- -------- -------- ---------

    which is equivalent to the condition that

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (4.108)
      -- -------- -------- -------- -- ---------

    hold for all @xmath . Now, since @xmath , it follows that @xmath ,
    and @xmath implies that

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (4.109)
      -- -------- -------- -------- -- ---------

    holds for all @xmath , or, after the change of variables @xmath ,
    that

      -- -------- -- ---------
         @xmath      (4.110)
      -- -------- -- ---------

    holds for all @xmath . In particular, the inequality @xmath must
    hold at the value @xmath that minimizes the right-hand-side. But we
    already know from Proposition that such a minimizer @xmath is of the
    form:

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (4.111)
      -- -------- -------- -------- -- ---------

    Now, suppose @xmath (We know that @xmath , so then @xmath ). From
    the proof of Proposition we know that the function @xmath is
    increasing, so then @xmath . Since also @xmath is strictly
    increasing, it follows that @xmath . In the last inequality we used
    ( 4.80 ). (See also Table 1 for recalling the notations used here.)
    But this is a contradiction to the minimality condition, @xmath ,
    and so we must conclude that @xmath .

2.  We now show that @xmath , if @xmath . Recall that for @xmath , the
    coefficient @xmath satisfies the fixed point condition,

      -- -------- -- ---------
         @xmath      (4.112)
      -- -------- -- ---------

    Fix @xmath , and consider as before any perturbation @xmath along
    the coordinate @xmath , @xmath . Let @xmath be the rank-one operator
    as defined before. Then, the minimality condition @xmath is easily
    seen to be equivalent to

      -- -------- -------- -------- -------- ---------
         @xmath   @xmath   @xmath            (4.113)
                                    @xmath   
                           @xmath   @xmath   
                                    @xmath   
                           @xmath   @xmath   
                                    @xmath   
      -- -------- -------- -------- -------- ---------

    and the final equality follows directly from the fixed point
    condition @xmath . Now the chain of inequalities @xmath implies the
    minimality condition

      -- -------- -------- -------- -------- ---------
         @xmath   @xmath   @xmath            (4.114)
                           @xmath   @xmath   
      -- -------- -------- -------- -------- ---------

    or, again using the change of variables @xmath , the inequality

      -- -------- -- ---------
         @xmath      (4.115)
      -- -------- -- ---------

    Again, the inequality @xmath should hold for all @xmath by the
    minimality of @xmath . Minimizers @xmath of the right-hand-side of
    @xmath also are minimizers of

      -- -------- -- ---------
         @xmath      (4.116)
      -- -------- -- ---------

    which we know to have the form

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (4.117)
      -- -------- -------- -------- -- ---------

    But @xmath , so the above reduces to

      -- -------- -------- -------- -- ---------
         @xmath   @xmath   @xmath      (4.118)
      -- -------- -------- -------- -- ---------

    As before, the proof proceeds by contradiction. Suppose that @xmath
    , so that @xmath and @xmath . Note that, by recalling @xmath , we
    have

      -- -------- -- ---------
         @xmath      (4.119)
      -- -------- -- ---------

    Plugging @xmath into the right-hand-side of @xmath , noting that
    @xmath so that @xmath , and rearranging, yields the inequality

      -- -------- -- ---------
         @xmath      (4.120)
      -- -------- -- ---------

    But this contradicts the assumption that the expression in @xmath be
    larger than @xmath .