In settings where only unlabelled speech data is available, zero-resource
speech technology needs to be developed without transcriptions, pronunciation
dictionaries, or language modelling text. There are two central problems in
zero-resource speech processing: (i) finding frame-level feature
representations which make it easier to discriminate between linguistic units
(phones or words), and (ii) segmenting and clustering unlabelled speech into
meaningful units. In this thesis, we argue that a combination of top-down and
bottom-up modelling is advantageous in tackling these two problems.
  To address the problem of frame-level representation learning, we present the
correspondence autoencoder (cAE), a neural network trained with weak top-down
supervision from an unsupervised term discovery system. By combining this
top-down supervision with unsupervised bottom-up initialization, the cAE yields
much more discriminative features than previous approaches. We then present our
unsupervised segmental Bayesian model that segments and clusters unlabelled
speech into hypothesized words. By imposing a consistent top-down segmentation
while also using bottom-up knowledge from detected syllable boundaries, our
system outperforms several others on multi-speaker conversational English and
Xitsonga speech data. Finally, we show that the clusters discovered by the
segmental Bayesian model can be made less speaker- and gender-specific by using
features from the cAE instead of traditional acoustic features.
  In summary, the different models and systems presented in this thesis show
that both top-down and bottom-up modelling can improve representation learning,
segmentation and clustering of unlabelled speech data.