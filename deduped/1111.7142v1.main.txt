# Acknowledgments

I would like to thank my supervisor, John Wheater, for the significant
time he has spent helping me and the numerous interesting projects he
suggested. This thesis would not exist without his help. I would also
like to give a very large thank you to Stefan Zohren who has provided
significant encouragement and motivation in the final year of my DPhil.
Both Stefan and John have been of invaluable help during the writing of
this thesis, both suggesting numerous improvements and raising
interesting questions.

I would like to acknowledge the support of STFC studentship
PfPA/S/S/2006/04507 in providing me with the funding to undertake this
research.

I would like to thank the senior tutors at Christ Church college, Axel
Kuhn, Derek Stacey and Alan Merchant for providing me with the
opportunity to teach and partake in admissions. This was a life-saver;
especially when funds were short! My enjoyment of my time at Oxford was
also greatly enhanced by my office mates, Sesh, Shaun, Seung Joo and
Tom, who I thank for providing constant interesting conversation,
whether related to physics or not.

My family have also been a source of encouragement throughout my DPhil
and have always been there to offer support. Finally, I want to thank
Lauren for being there throughout it all and her endless willingness to
lend an ear. It is to my family and Lauren to which I dedicate this
thesis.

###### Contents

-    Abstract
-    Acknowledgments
-    1 Introduction
    -    1.1 Thesis Outline
-    2 Discrete Approaches to 2D Quantum Gravity
    -    2.1 Dynamical Triangulation
        -    2.1.1 Random Graphs and Matrix Models
        -    2.1.2 Solving the Matrix Model: Loop equations
        -    2.1.3 The Continuum Limit
        -    2.1.4 Combinatorial Interpretation of the Loop Equation
    -    2.2 Causal Dynamical Triangulation
-    3 The Dimension of Spacetime
    -    3.1 The Hausdorff Dimension @xmath
        -    3.1.1 Computing @xmath for DT and CDT
    -    3.2 The Spectral Dimension @xmath
        -    3.2.1 Dimensional Reduction
        -    3.2.2 The Spectral Dimension in CDT
-    4 A Toy Model of Dimensional Reduction
    -    4.1 Combs and Walks
        -    4.1.1 Definitions
        -    4.1.2 Random Walks
        -    4.1.3 Two point functions
        -    4.1.4 Spectral dimension and the continuum limit
    -    4.2 A simple comb
    -    4.3 Combs with Power Law Measures
    -    4.4 Multiple Scales
    -    4.5 Generic Distributions
    -    4.6 Discussion
-    5 DT with Matter, String Theory and Branes
    -    5.1 Adding Matter using Matrix Models
        -    5.1.1 The Continuum Limit and Multicritical Points
    -    5.2 Conformal Field Theory and Minimal Models
    -    5.3 Boundary States of Minimal Models
    -    5.4 String Theory
        -    5.4.1 Liouville Theory and Non-Critical String Theory
        -    5.4.2 Wave-functions and Correlation functions in Liouville
            Theory
        -    5.4.3 Minimal String Theory
        -    5.4.4 FZZT Branes and the Seiberg-Shih Relation
        -    5.4.5 Cylinder Amplitudes and the Seiberg-Shih Relation
-    6 FZZT Branes in the @xmath Minimal String at Higher Genus
    -    6.1 Testing the Seiberg-Shih relations using a Matrix Model
        -    6.1.1 Loop equations for SX Matrix Model
        -    6.1.2 Disc, Cylinders and Disc-with-Handles.
        -    6.1.3 Dual Branes
    -    6.2 The Seiberg-Shih Relation and Local Operators
        -    6.2.1 Extracting Local Operators From Loops
    -    6.3 Discussion
-    7 Summary
-    A Mellin Transforms and Asymptotics
    -    A.1 Standard generating functions
    -    A.2 Bump functions
    -    A.3 Asymptotic Series and Dirichlet Series
-    B Liouville Cylinder Amplitudes
-    C Loop equations for Disc with Handle

###### List of Figures

-    2.1 A triangulation (grey) together with its dual graph (black).
-    2.2 Move 1 relates an unrestricted triangulation (left), with a
    mark denoted by the white circle, to another unrestricted
    triangulation with less triangles.
-    2.4 A causal triangulation.
-    2.5 A triangulation of a baby universe. If we successively remove
    triangles, then the final link between the baby universe and the
    parent will be a double link.
-    2.7 A typical geometry in CDT [ 72 ] .
-    3.1 By drawing a baby universe in the lattice we see how this gives
    rise to a fractal structure [ 45 ] .
-    3.2 A causal triangulation drawn in the plane. Note we have added
    an extra vertex with links to all vertices in the inner most spatial
    slice [ 16 ] .
-    4.1 A comb.
-    4.2 An illustration of the strip @xmath in which @xmath satisfies
    property (1) given given in the text. The poles of @xmath are
    denoted by crosses and the horizontal strips in which the growth
    condition holds are indicated by the dark grey regions.
-    5.1 The worldsheet corresponding to the interaction of two strings
    may be transformed via a conformal transformation to a worldsheet of
    equivalent topology but with all asymptotic string states
    transformed into local operators.

] ] ]

## Chapter 1 Introduction

Perhaps the most ambitious question that one may try to answer is “What
is reality?”, where by reality we mean the sensation of being alive, of
experience. A priori there are many possibilities; a subscriber to
idealism would believe one’s experiences are entirely generated by one’s
mind. If, on the other hand, one subscribes to materialism, then what
constitutes reality is a representation, generated by one’s brain, of an
independent physical universe. The quest to understand reality then
becomes a matter of understanding the true nature of this physical
universe. The 20th century saw many advancements in our understanding of
the universe in this respect. The development of quantum mechanics
fundamentally altered what is considered the true nature of the matter
and forces, while the introduction of general relativity, which
describes the dynamics of spacetime and how this manifests itself as
gravity, banished the perception of space and time as being a static
background stage on which dynamical processes took place.

However, the revolution in our view of the universe caused by quantum
mechanics and general relativity was only half completed. General
relativity assumes that all matter residing in spacetime is classical.
However, we know such matter is fundamentally quantum mechanical and so
we must, at the very least, construct a theory of gravity in which the
matter is allowed to assume its true quantum mechanical form.

There have been attempts to pursue the most conservative line of attack;
keep the dynamics of spacetime as they are in general relativity, or at
least classical, but modify how matter couples to gravity in order to
allow for quantum mechanical rather than classical sources. However,
there are a number of compelling arguments to suggest this does not
produce a consistent theory [ 46 ] . A more natural possibility is that
gravity itself is quantum mechanical in nature. This avoids the
aforementioned problems of coupling classical and quantum mechanical
systems in addition to the philosophical appeal of all phenomenon in the
universe being fundamentally quantum. Such a theory and the attempts to
construct it are known as quantum gravity.

The are further compelling reasons that suggest a quantum mechanical
formulation of general relativity is necessary. Most prominently,
general relativity inevitably forms singularities in the form of black
holes or the initial big bang singularity [ 47 ] , at such times the
effect of quantum mechanics is expected to become important at the very
least in the matter sector and most likely in the gravitational sector
too.

Given that quantum mechanics provides the recipe of quantisation to
produce a quantum mechanical theory starting with a classical one, the
obvious way to construct quantum gravity is to apply this procedure to
the theory of general relativity. In doing so one is immediately faced
with a problem; what degrees of freedom should be quantised? The answer
depends on what one considers to be the fundamental quantity whose
dynamics general relativity describes. Furthermore, once we have decided
upon the form the fundamental degrees of freedom take we must also
choose a method of quantisation. For our purposes there two possible
methods of quantisation, canonical and path-integral, and each may be
applied to the theory resulting from our choice of degrees of freedom.
As one might imagine this leads to a large number of different
approaches to quantum gravity; all of which have been pursued at one
point or another. We now consider some common answers to the above
choices.

The spacetime metric @xmath is fundamental. In this approach we quantise
the metric of spacetime. In a canonical approach this involves rewriting
general relativity in terms of a spatial metric together with functions
that describe how the spatial metric evolves between each spatial
hypersurface. The resulting theory may be quantised leading to the
Wheeler-de-Witt equation [ 47 , 48 ] . A path integral approach to the
same problem requires one to make sense of integrals of the following
form,

  -- -------- -- -------
     @xmath      (1.1)
  -- -------- -- -------

where

  -- -------- -- -------
     @xmath      (1.2)
  -- -------- -- -------

where @xmath is a constant known as the cosmological constant and @xmath
is a constant satisfying @xmath , where @xmath is Newton’s constant.
Additionally one must decide exactly which geometries the integral sums
over.

The perturbation to flat space, @xmath defined by @xmath , is
fundamental. In this view, the vacuum of general relativity is Minkowski
space and we quantise the perturbations around it. This reduces the
problem of quantum gravity to a problem of quantising a spin-2 quantum
field theory in flat space. One can approach this using canonical or
path-integral methods, however in both cases one finds that the
resulting theory is perturbatively non-renormalisable [ 1 ] . Although
this would appear to rule out treating general relativity as anything
other than an effective theory one suggestion due to Weinberg is that
the coupling of the theory may in fact run to a non-Gaussian fixed point
in the UV [ 2 ] . This would render the theory UV complete while still
appearing non-renormalisable at the level of perturbation theory. A
second possibility arises in the program of string theory in which all
fundamental objects are treated as quantised strings. In this approach
the spectrum of the string contains a spin-2 state and the S-matrix for
the scattering of this state reduces to that of the S-matrix obtained
for graviton scattering when treating @xmath as fundamental [ 49 , 51 ]
.

No variables in general relativity are fundamental. A more radical
possibility is that general relativity doesn’t contain the correct
microscopic degrees of freedom at all. If this is the case then general
relativity only becomes a description of spacetime in the long distance
limit and attempting to quantise general relativity would be as
misguided as attempting to understand the short distance regime of water
by quantising the Navier-Stokes equation! Historically the problem with
this possibility is that it was very hard to know where to begin as one
must invent a theory in which spacetime and gravity appear as low energy
concepts; in effect solving the problem of quantum gravity in a moment
of brilliant insight. However, the advent of string theory has changed
this situation. String theory provides a way to construct order by order
a perturbative expansion in which each term is finite and correctly
reproduces the low energy scattering of gravitons found in the
linearised theory. Furthermore, this is achieved by quantising a theory
which makes no assumptions about the classical dynamics of spacetime.
The main problem however is the question of how to define the theory
from which the perturbative series of various string theories arise. The
solution to this problem has been tentatively named M-theory [ 50 ] ,
however a full definition of it has yet to be given.

### 1.1 Thesis Outline

As one can see, the field of quantum gravity is vast and one can only
hope, in a thesis such as this, to consider a small piece of it. In this
thesis we will be particularly interested in the dimension of spacetime
and the allowed boundary conditions of two dimensional quantum gravity.
Quantum gravity in two dimensions is special for a number of reasons,
one being that a number of the approaches listed above coincide. In
particular the approach in which the metric @xmath is fundamental
coincides with the string theory approach and so by studying one of
these theories we may study the other.

In chapter 2 we introduce the theory of pure two dimensional quantum
gravity in the metric-is-fundamental approach. We review how this theory
may be obtained as a scaling limit of a discrete theory known as
Dynamical Triangulation (DT) and introduce the matrix model and
combinatorial techniques that may be used to compute certain observables
such as the disc-function and string susceptibility. We also introduce
Causal Dynamical Triangulation (CDT) models and review their relation to
DT.

In chapter 3 we consider the final observable not considered in chapter
2 ; the dimension of spacetime. In a theory of quantum gravity one
expects that spacetime at very short distances is violently fluctuating.
Furthermore, in string theory one might expect to also begin to probe
compact extra dimensions. Both of these effects would lead the dimension
of spacetime to deviate from its infrared value. We consider this
question in the context of the approach in which the metric is
fundamental. We argue that a-priori it is not necessarily equal to the
dimension of the underlying discretisation and introduce the concept of
the Hausdorff dimension as a tool to characterise fractal structures. We
review the calculation showing that the Hausdorff dimension of DT is
four and then argue that the Hausdorff dimension in CDT is two. We then
argue that the Hausdorff dimension is inadequate to fully distinguish
between smooth and fractal spaces and so we introduce a second
definition of dimension, known as the spectral dimension, which is based
on the properties of the Laplacian operator on the fractal structure. We
argue that the spectral dimension can be obtained by considering random
walks on the triangulation. Finally we review the recently observed
phenomenon of dimensional reduction, in which the spectral dimension of
the UV is lower then IR. By considering random graphs derived from CDT
we then begin the search for a simple random graph model in which we
might capture the phenomenon of dimensional reduction.

Chapter 4 consists mainly of work published in [ 66 ] . We briefly
introduce random combs and review some known results. We give a
definition of the spectral dimension and then explain how this
definition can be extended to show different spectral dimensions at long
and short distance scales. We then introduce a simple model which we
prove does in fact exhibit a spectral dimension that is different in the
UV and IR. We generalise these results to combs in which teeth of any
length may appear with a probability governed by a power law and examine
the possibility of intermediate scales in which the spectral dimension
differs from both its UV and IR values. Finally we analyse the case of a
comb in which the tooth lengths are controlled by an arbitrary
probability distribution and show that continuum limits exist in which
the short distance spectral dimension is one while the long distance
spectral dimension can assume values in one-to-one correspondence with
the positions of the real poles of the Dirichlet series generating
function for the probability distribution.

In chapter 5 we first review how matter coupled to gravity may be
realised using matrix models. We introduce the Ising matrix model and
also consider multicritical points. We then move on to quickly review
minimal models, string theory, Liouville theory and minimal string
theory. The notion of a boundary condition in string theory is promoted
to a dynamical object known as a brane. The different boundary
conditions then correspond to different types of brane. If one wants to
obtain a non-perturbative description of string theory then the spectrum
of branes is important. We pursue the question, that arises due to a
conjecture of Seiberg and Shih [ 17 ] , of how many distinct branes
exist in @xmath minimal string. We review the fact that this conjecture
fails for cylinder amplitudes as found in [ 36 ] and conjecture a way in
which it could be fixed. The final portion of this chapter appears in [
67 ] .

Chatper 6 consists of the remainder of the work in [ 67 ] in which we
apply matrix model techniques to the question of whether the conjecture
of the previous chapter holds. In particular we consider the matrix
model which describes a @xmath minimal string theory in the presence of
a boundary magnetic field. We show how a general class of matrix models,
which includes the one describing the @xmath string may be solved, and
by varying the boundary magnetic field we reproduce the conformal
boundary states of the @xmath model. Using our general solution to this
model we then compute the disc-with-handle amplitude for all conformal
boundary conditions and compute their deviation from the Seiberg-Shih
relation. We argue that these results show that our previous conjecture
does not hold. This makes it very difficult to see how the Seiberg-Shih
relations could possibly be true. Finally we consider a different
approach to testing the Seiberg-Shih relation based on expanding
boundary states in terms of local operators. We find that the expansion
in local operators appears to be valid for only certain boundary types.
We support this by reproducing a recent calculation of Belavin in which
the one-point function on the torus was computed.

## Chapter 2 Discrete Approaches to 2D Quantum Gravity

In the previous chapter we saw that there were a variety of approaches
to quantum gravity which could be classified according to which degrees
of freedom were considered to be fundamental. In two dimensions a number
of these approaches coincide; in particular string theory and approaches
in which the metric is considered fundamental. We will therefore
restrict our attention to these in the remainder of this thesis. In this
chapter we will consider the simplest case of the metric-is-fundamental
approach, in which no matter is present and the only dynamical quantity
is that of gravity itself. We first review why in two dimensions some of
the problems of higher dimensional theories are alleviated before going
on to review the few simple observables that exist in this theory. We
then motivate a different approach to computing the path integral based
on a discretistion of the contributing geometries and show that there
exists a scaling limit which allows us to compute some of the
observables.

In the metric-is-fundamental approach we must evaluate the partition
function,

  -- -------- -- -------
     @xmath      (2.1)
  -- -------- -- -------

where we have divided by the volume of the group of diffeomorphisms of
the spacetime manifold, since this is a gauge symmetry in general
relativity. The usual strategy to evaluate path integrals is to Wick
rotate to a Euclidean theory. This has the effect of converting the
oscillatory measure to one which is exponentially damped. An immediate
problem arises in the case of gravity, in that the resulting Euclidean
action is not bounded from below and so it appears the integral does not
exist. A further problem is that it is unclear that a Wick rotation from
the Euclidean theory back to Lorentzian theory even exists.

Thankfully, when the spacetime @xmath is two dimensional some of these
problems are alleviated. Due to the Gauss-Bonnet theorem the curvature
term in the action is entirely topological and so the problem of the
unboundedness of the action is no longer present. However, restricting
our attention to two dimensions does not solve the problem of what level
of causality violations to allow i.e. how to Wick rotate between the two
theories. We will come back to this point later in this chapter. For now
we will put no causality constraint on the geometries appearing in the
integral and hence we shall sum over all Euclidean geometries.
Application of the Gauss-Bonnet theorem to ( 1.2 ) then gives,

  -- -------- -- -------
     @xmath      (2.2)
  -- -------- -- -------

where @xmath is the Euler characteristic of @xmath and is given by
@xmath , where @xmath , the genus, is the number of handles of @xmath .
In order to perform this integral we must define the measure @xmath .
This is a difficult technical problem whose solution results in a theory
in which the overall scale factor of the measure acquires dynamics. The
theory describing the dynamics of the scale factor is itself a difficult
theory to solve. We therefore postpone our discussion of this approach
until Chapter 5 . However, it is still worth considering the
observables, within the continuum framework, that one could measure in a
theory of pure quantum gravity. There are a number of observables that
have been commonly considered in the literature. These consist of,

-   Integrated correlation functions of local operators are the most
    obvious observables, although these also proved the most difficult
    to compute. In the most general case, when the manifold @xmath has
    @xmath boundaries, the action given in ( 2.2 ) must be extended to
    include a boundary action,

      -- -------- -- -------
         @xmath      (2.3)
      -- -------- -- -------

    where @xmath is metric induced on the boundary curve and @xmath are
    boundary cosmological constants and @xmath , where @xmath is the
    number of boundaries. By utilising the symmetries of the theory any
    integrated correlation function has the form @xmath [ 41 , 52 ] .
    The quantity @xmath is known as the string susceptibility [ 42 ] and
    is an observable that has been used to compare different approaches
    to two dimensional quantum gravity.

-   The Hartle-Hawking wave function. This is the partition function for
    the appearance of a universe “from nothing” and evolving into some
    prescribed state. Geometrically this corresponds to summing over all
    geometries with a disc topology with a fixed boundary condition.
    Throughout this thesis we will refer to this quantity as the
    disc-function. More generally one can consider multi-loop amplitudes
    in which the spacetime manifold has a number of boundary loops; a
    disc would be a 1-loop amplitude and a 2-loop amplitude is often
    known as a cylinder amplitude;

-   The dimension of spacetime. Living inside a particular universe one
    may probe the dimension of the spacetime by performing scattering
    experiments or even simply measuring the electro-static force
    between two charges. If there exist changes to the dimension of
    spacetime on the length scales such experiments probe, then we
    should be able to measure them. This observable may seem trivial as
    it appears we set the dimension spacetime in our theory at the
    outset, however, when probing the short distance scale of spacetime
    we expect that spacetime itself will become less smooth due to
    quantum gravity effects (such as microscopic black holes, possibly
    wormholes etc). This could easily alter the effective dimension of
    spacetime at these scales even if the underlying theory is of fixed
    dimension [ 12 ] .

In order to compute these observables we will consider an alternative
method of computing the above integral in which observables are
calculated in a different, but related, theory with a far simpler
measure. This will come at the expense of having a non-trivial
dictionary relating the observables in the original theory to the one
with a simple measure. We will review such an approach in the next
section.

### 2.1 Dynamical Triangulation

The related theory which we will consider is motivated by considering
lattice regularisations of flat space quantum field theories. If we take
the lattice regularised theory as our starting point then one can obtain
a continuum theory, corresponding to the original quantum field theory,
by taking the lattice spacing to zero while also appropriately scaling
the other parameters in the theory. In the case of gravity there exists
a number of approaches to defining a lattice regularisation, however we
shall utilise only one here. It is known as Dynamical Triangulations
(DT) and it has a long history which is reviewed in [ 42 , 53 ] . In
this case the spacetime geometry is encoded in the lattice, which is
canonically composed of triangles. The lattice links themselves are
always of fixed length ¹ ¹ 1 Links of fixed length is in contrast with
the earlier approach of Regge calculus in which the link lengths were
integrated over while keeping the triangulation fixed. . For an example
of a triangulation see fig 2.1 . The benefit of this discrete model is
that if we wish to compute a partition function, then rather than having
to integrate over geometries we must sum over the triangulations.
Because the latter is a sum, the issue of defining an integration
measure on the space of geometries does not arise. If we are able to
compute the partition function of the DT model then the hope is that
there exists a continuum limit in which we obtain the original partition
function ( 2.2 ).

In order to compute the partition function we will set each link to be
of length @xmath meaning each triangle has an area proportional to
@xmath . The discretised version of the integral ( 2.2 ) is then,

  -- -------- -- -------
     @xmath      (2.4)
  -- -------- -- -------

where @xmath is the set of triangulations of genus @xmath , @xmath is
the number of triangles in the triangulation @xmath , @xmath is the
number of triangulations of a genus @xmath manifold containing @xmath
triangles and @xmath . From the final equality we see that the
evaluation of the partition function has been reduced to computing the
generating function for @xmath , which is a graph counting problem.
However, the graphs we must count are rather complicated; in particular
the order of each vertex is unbounded and there is a non-local
restriction that each face of the graph is triangular. Both of these
problems may be solved in a simple way by considering the dual graph of
the triangulation. The dual graph is constructed by associating a vertex
to each face of the triangulation and a link joining any two vertices
whose corresponding faces share an edge. The resulting dual graphs have
the simple property that each vertex is of order three with no non-local
restrictions. Furthermore since each graph has a unique dual we need
only count the dual graphs. The dual graph construction is illustrated
in fig 2.1 .

From the figure it is amusing to note that dual graph looks very much
like a Feynman diagram composed of many vertices. In fact there exists a
very useful class of tools, known as matrix models, which exploit this
fact and are able to compute generating functions for the graph counting
function @xmath for a variety of graphs, including those with labels.
Recall that in quantum field theory the perturbative computation of the
partition function is organised such that the contribution of each
process to the overall amplitude is encoded in its Feynman graph; in
particular the overall power of the coupling associated to each graph is
given by the number of vertices it contains. If one were to find a way
of computing the partition function then the perturbative expansion
could be recovered by expanding the partition function around zero
coupling. If the quantum field theory was sufficiently simple then one
could use the resulting expansion to extract information about the
number of graphs contributing at each order of the coupling.

#### 2.1.1 Random Graphs and Matrix Models

If we are to develop a quantum field theory that will solve our graph
counting problem then it must also have the property that its
perturbative expansion is organised by the topology of the graph. This
requirement restricts the possible candidates to matrix valued field
theories as such theories will produce a perturbative series, in the
inverse size of the matrix, in which only graphs of a particular
topology contribute at a given order.

We must also make the theory simple enough that one can extract the
information concerning the graphs. The simplest matrix valued theory is
one in which the spacetime dimension is zero; such a theory is known as
a matrix model. An example of such a model, which also has the property
that it produces graphs in which all vertices have order four is,

  -- -------- -- -------
     @xmath      (2.5)
  -- -------- -- -------

where the metric field @xmath is an @xmath hermitian matrix and the
measure @xmath is defined to be the flat Lebesgue measure,

  -- -------- -- -------
     @xmath      (2.6)
  -- -------- -- -------

If we treat the quartic term in the action as a perturbation to the free
theory then we may construct a Feynman graph expansion for @xmath . The
contribution to the total amplitude from a given graph is @xmath , where
@xmath is the genus of the graph and @xmath is the number of vertices it
contains. We therefore see that the expansion of @xmath around @xmath
coupling will yield the series,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (2.7)
     @xmath   @xmath   @xmath      (2.8)
  -- -------- -------- -------- -- -------

The function that counts the number of graphs with @xmath vertices,
@xmath , differs from the one that appeared in the expression for @xmath
since the perturbative expansion includes disconnected graphs. In order
to compute @xmath we must instead consider the free energy,

  -- -------- -- -------
     @xmath      (2.9)
  -- -------- -- -------

We see that we have actually done much better than merely being able to
compute @xmath ; we have reproduced the @xmath partition function
exactly! ² ² 2 The observant reader will note that the relation of
@xmath to @xmath is not, in fact, exact, due to the above matrix model
producing not triangulations but quadrangulations. We could have
considered a matrix model with a cubic potential and so obtained an
exact equivalence with @xmath . However because the matrix model with
cubic and quartic potentials have the same continuum limit we will
continue to consider the quartic model for reasons of pedagogy; the
quartic model is simpler to solve.

#### 2.1.2 Solving the Matrix Model: Loop equations

There exist a number of approaches to computing @xmath , among them the
saddle point method [ 54 , 42 , 53 ] , orthogonal polynomials [ 42 , 53
] and loop equations [ 53 , 24 ] . It is the last of these three that we
will review in this section. In order to keep our discussion general we
will consider a generalised one hermitian-matrix model, given by

  -- -------- -- --------
     @xmath      (2.10)
  -- -------- -- --------

where @xmath is a polynomial potential. A loop equation is defined as
any equation that may be derived from an infinitesimal change in the
integration variable which preserves the domain of integration. For the
matrix model defined previously, the most arbitrary infinitesimal change
in the integration variable would be @xmath where @xmath is an arbitrary
function satisfying @xmath . In the case of ( 2.10 ) the effect of this
change of variable is,

  -- -------- -- --------
     @xmath      (2.11)
  -- -------- -- --------

where @xmath is the trace of the Jacobian matrix for the transformation
and @xmath is the change in the potential, i.e. in this case @xmath . By
equating the above equation to ( 2.10 ) and considering the order @xmath
terms we obtain,

  -- -------- -- --------
     @xmath      (2.12)
  -- -------- -- --------

A central role in the loop equation method is played by the function
defined by

  -- -------- -- --------
     @xmath      (2.13)
  -- -------- -- --------

where @xmath and @xmath is known as the resolvent. Its importance is due
to the fact that the free energy may be computed from it. Furthermore, a
very definite interpretation may attached to it. Note that the Feynman
graphs contributing to @xmath are those which have @xmath external legs.
Such graphs may be interpreted as corresponding to a discretisation of a
disc and so we see the resolvent acts as a generating function for such
amplitudes. We therefore have,

  -- -------- -- --------
     @xmath      (2.14)
  -- -------- -- --------

where @xmath denotes the number of triangulations containing @xmath
triangles and @xmath boundary links. Obviously such graphs are the ones
which contribute to the disc function observable discussed in the
previous section and therefore one might suspect that the continuum disc
function can be extracted from the resolvent. This indeed turns out to
be the case and we will compute the disc function in the next section.
Finally it is worth emphasising at this point that although the
resolvent function appears naturally in the matrix model approach, the
technique of constructing a generating function for particular classes
of amplitudes, for example in this case disc-amplitudes, will be of more
general use, as we will see when considering the combinatorial approach
to loop equations.

In order to use ( 2.12 ) to calculate @xmath we first need to specify
the change of variable we will use. We make the choice @xmath . By
analysing the effect of the change of variable on the measure ( 2.6 ),
one can show [ 24 ] that if @xmath then,

  -- -------- -- --------
     @xmath      (2.15)
  -- -------- -- --------

Using this result in ( 2.12 ) we get,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.16)
              @xmath   @xmath      (2.17)
  -- -------- -------- -------- -- --------

where @xmath is a polynomial in @xmath . We now note that the function
on the left hand side has contributions from connected and disconnected
Feynman diagrams. For such amplitudes we can in general write @xmath ,
where the @xmath stands for “connected”. We therefore get,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.18)
  -- -------- -------- -------- -- --------

This equation is the loop equation associated with the change of
variable defined by @xmath . Let us define the two-loop function as,

  -- -------- -- --------
     @xmath      (2.19)
  -- -------- -- --------

We see that since the diagrams contributing to this amplitude are
cylindrical then it goes as @xmath as @xmath . Furthermore, we expect
that both @xmath and @xmath will have a large @xmath expansion of the
form,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (2.20)
     @xmath   @xmath   @xmath      (2.21)
  -- -------- -------- -------- -- --------

If we substitute these expansions into the loop equation then we find,

  -- -------- -- --------
     @xmath      (2.22)
  -- -------- -- --------

We see that in the large @xmath limit we end up with a quadratic
equation for @xmath in which @xmath contains a number of unknown
constants. Note that this equation defines an algebraic curve in the
variables @xmath and @xmath . This is known as the spectral curve of the
matrix model.

Up to this point we have made an assumption that the partition function
for the matrix model admits an expansion in inverse powers of @xmath .
This assumption is in fact sometimes incorrect, as oscillatory terms
appear in the large @xmath expansion. The exact condition under which a
large @xmath expansion and therefore a topological expansion exists is
given in [ 24 ] to be when the spectral curve is genus zero. We see that
for the spectral curve of the matrix model considered here, the genus
zero condition is equivalent to there being a single branch cut. By
enforcing this condition we may compute the unknown constants appearing
in @xmath . The solution to ( 2.22 ) is

  -- -------- -- --------
     @xmath      (2.23)
  -- -------- -- --------

Note the discriminant of this is a polynomial in @xmath . We may
therefore enforce the genus zero condition by requiring that all but two
of the roots of the discriminant are double roots. For the particular
potential in ( 2.5 ), we therefore must be able to write,

  -- -------- -- --------
     @xmath      (2.24)
  -- -------- -- --------

where @xmath , @xmath and @xmath may be determined by requiring that the
expression in ( 2.24 ) reproduces the large @xmath expansion of ( 2.23
). Because @xmath was defined using a series valid for large @xmath we
know that there exists at least one branch of the solution which has the
property that @xmath as @xmath . If one expands ( 2.24 ) about @xmath ,
then for generic values of @xmath , @xmath and @xmath the leading order
term will be of higher order than @xmath . Requiring that the leading
order term is @xmath gives the following expressions for @xmath , @xmath
and @xmath ,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (2.25)
  -- -------- -------- -------- -- --------

#### 2.1.3 The Continuum Limit

In order to obtain a result that corresponds to a continuum theory we
must take what is known as a scaling limit of the above computation.
This corresponds to tuning the coupling of the theory to the value at
which it undergoes a second order phase transition. At such a phase
transition the average number of triangles in the triangulation diverges
and by rescaling their size along with the couplings in theory we can
derive the continuum theory which describes the model at this critical
point. We see from ( 2.4 ) that the number of triangles in the partition
will diverge when @xmath reaches the radius of convergence of ( 2.4 ).
This radius of convergence can be easily computed by considering the
quantity @xmath which can be obtained from the coefficient of @xmath in
the large @xmath expansion of @xmath . The radius of convergence for
@xmath can be read off from the position of its branch points as a
function of @xmath .

Having found the critical point of @xmath , @xmath , it remains to fix
how we scale the parameters in our theory. If we refer back to ( 2.4 )
then we expect the bare cosmological constant @xmath to be related to
the continuum cosmological constant @xmath by,

  -- -------- -- --------
     @xmath      (2.26)
  -- -------- -- --------

which implies @xmath , where @xmath . As it stands, if we were to
substitute this scaling relation in to the equation for the resolvent
then this would correspond to taking a limit in which the number of
triangles in the bulk diverges while also shrinking each triangle to
zero size. Such a limit would yield a continuous geometry in the bulk.
However, note that we have not scaled the parameter @xmath , which
controls the number of links on the boundary of the disc. Leaving the
parameter @xmath unscaled would produce geometries which, although
continuous in the bulk, have a boundary of finite size in lattice units.
From the perspective of the bulk such a boundary would appear
infinitesimal and therefore look like a local operator insertion. We
will denote the geometry which has a boundary of length @xmath in
lattice units by @xmath .

The resolvent provides one way of finding a scaling limit in which the
number of boundary links can be made to diverge. By noting that the
resolvent has a radius of convergence in @xmath determined by the
position of the branch point @xmath , we can see that by tuning @xmath
to the value @xmath , the average number of boundary links will diverge.
We therefore enforce a scaling of @xmath , where @xmath will become the
boundary cosmological constant. Inserting these scaling forms for @xmath
and @xmath into the solution for @xmath and taking @xmath , we obtain,

  -- -------- -- --------
     @xmath      (2.27)
  -- -------- -- --------

where we have introduced the dimensionless quantity @xmath .

A peculiarity of this scaling limit is that the universal quantity that
corresponds to the continuum theory amplitude is not the leading
contribution to @xmath as @xmath goes to zero. The obvious way to
identity the universal result is to compute the same quantity using
different potentials in the matrix model and then note which term in the
scaling limit remains unchanged. A way to short-cut this procedure is to
conjecture that the universal part of the above expansion corresponds to
the first term non-analytic in the bulk and boundary cosmological
constant. One can motivate this by arguing that non-analyticities only
appear in the thermodynamic limit, i.e. when the surface and boundary
are of infinite area and length in lattice units. This conjecture is
borne out by comparison to continuum calculations. We therefore have

  -- -------- -- --------
     @xmath      (2.28)
  -- -------- -- --------

where we have introduced the notation of adding a tilde to a quantity in
order to denote its continuum version. Note that this continuum quantity
is related to the term appearing in ( 2.27 ) by a wavefunction
renormalisation which removes the overall factor of @xmath .

This completes our computation of the continuum resolvent, and hence the
disc function observable, using the matrix model formulation of DT. In
particular we have shown that there exists a non-trivial limit of the
discrete equations. From this we may also extract the string
susceptibility in the case of a disc by removing the marked point on the
boundary. The mark may be removed by integrating the above expression
w.r.t @xmath to obtain, @xmath . Another question one might ask is
whether there exists a different disc function obtained by scaling
@xmath in the equation @xmath while tuning @xmath to its critical value.
This would yield a disc function in which the boundary length, rather
than boundary cosmological constant was fixed. This quantity will turn
out to be useful in the future and so it is important to understand how
it is related to the disc function. Given that the boundary cosmological
constant has dimensions of @xmath we should expect the boundary length
to scale like @xmath , we therefore define the continuum disc-function,

  -- -------- -- --------
     @xmath      (2.29)
  -- -------- -- --------

where @xmath and @xmath are chosen to give a non-trivial limit. With
this definition it is reasonably straightforward to see that @xmath is
related to @xmath by a Laplace transform,

  -- -------- -- --------
     @xmath      (2.30)
  -- -------- -- --------

Note that the above relation applies to boundaries which carry a marked
point. If we were to remove the marked point then the relationship would
become,

  -- -------- -- --------
     @xmath      (2.31)
  -- -------- -- --------

where we have used an @xmath to denote amplitudes with non-marked
boundaries.

The matrix model approach is particularly useful if one wants to compute
higher genus amplitudes such as discs with handles. This is due to the
fact that the loop equations of the matrix model produces relations
between quantities which include all genus contributions to an
amplitude. This was seen in ( 2.18 ), which was then used to derive an
equation satisfied by the genus zero contribution ( 2.22 ). Obviously,
the loops equation ( 2.18 ) may also be used to obtain expressions for
the higher genus corrections to @xmath and this will be of great
interest in later chapters.

The question of whether this continuum theory we have uncovered is at
all related to the original continuum theory ( 2.2 ), is a question we
leave until Chapter 5 . However, it is worth jumping to the punchline;
matrix model computations do indeed match the continuum formulation
whenever two quantities have been computed in both. It is therefore
worthwhile spending some time investigating what we may learn in the
discrete formulations of two dimensional quantum gravity.

#### 2.1.4 Combinatorial Interpretation of the Loop Equation

We have seen that the loop equations as obtained from the matrix model
are an efficient way to compute the disc function and that the matrix
model is particularly suited to computing higher genus amplitudes.
However, it is useful to consider a distinct method of obtaining the
loop equations which, although it is not as useful for computing the
higher genus amplitudes, gives greater understanding of the content of
the equations. This method consists of performing a more direct
combinatorial analysis of the triangulations by considering the way one
may build up a triangulation of a given size from smaller
triangulations. This eventually leads to a recursive equation for the
number of triangulations of a given size which is equivalent to the loop
equations.

Rather than trying to compute the combinatorics of the triangulations
exactly it is useful to consider a slightly generalised class of
triangulations known as unrestricted triangulations in which double
links are allowed and triangles do not necessarily have to join along an
edge. We also require one of the boundary edges to carry a mark. Such a
triangulation is shown in fig 2.3 . Following the arguments of [ 43 ] we
will see that the continuum limit of these triangulations fall into the
same universality class as the usual triangulations in DT and indeed
give rise to the same loop equations.

We can classify the graphs with @xmath triangles and @xmath boundary
edges into two sets; those in which the marked point occurs on a link
which forms a side of a triangle and those on which the marked point
resides on one of a pair of double links. All graphs in the first class
may be put into a one to one correspondence with graphs with @xmath
triangles and @xmath edges. This is achieved by the map defined by
adding double links to each side of the marked triangle which is an
unmarked boundary and then removing the marked triangle. The mark is
then transferred to the most anti-clockwise new boundary edge. This is
shown in fig 2.3 .

All graphs in the second class may be put in one to one correspondence
with pairs of triangulations which are obtained by cutting the
triangulation along the marked double link and marking the new
triangulations as shown in fig 2.3 .

This the means that the number of graphs with @xmath triangles and
@xmath edges satisfies

  -- -------- -- --------
     @xmath      (2.32)
  -- -------- -- --------

which when substituted into ( 2.14 ) and ( 2.13 ) yields the equation,

  -- -------- -- --------
     @xmath      (2.33)
  -- -------- -- --------

This equation is the same as ( 2.22 ) with a particular cubic potential.
We therefore see that the combinatorial approach has yielded the correct
loop equations. In the next chapter we will use the combinatorial
approach to obtain an expression for quantities not easily computable in
the matrix model approach, for instance the dimension of spacetime.

### 2.2 Causal Dynamical Triangulation

Moving to two dimensional gravity alleviated a number of technical
problems in the metric-is-fundamental approach, however the problem of
what level of causality violations to allow remained. In the last
section we considered the case when no causality conditions were
enforced, technically this was because we summed over all Euclidean
metrics before continuing back to a Lorentzian metric. We found that we
could compute the partition function by considering a discretised
version of the path integral. In this section we will consider the
possibility of computing the Lorentzian path integral directly by means
of triangulations that are inherently Lorentzian. This will also allow
us to explore the role causality violations play in the theory.

Following [ 4 ] we will construct a Lorentzian triangulation by
requiring all triangles composing it to be flat Minkowski space, with
one spacelike and two timelike edges. Furthermore we restrict the
topology of spacetime to be cylindrical with spacelike boundaries. We
define the set of triangulations that will be summed over by giving an
algorithm for their construction. Suppose the initial boundary is
composed of @xmath vertices joined by spacelike links, then we may
attach future pointing timelike links to each of these vertices to
obtain the set of vertices that, by joining them with spacelike links,
form the next spatial slice. Furthermore, we require that each vertex
has at least one timelike link ending on the same vertex as the
rightmost timelike link of the previous vertex. This result of this
procedure is shown in fig 2.4 .

Again the problem of computing a disc function can be solved using a
matrix model technique [ 45 ] or by the combinatorial approach. Although
results from CDT will motivate later work in this thesis we will not
have cause to compute any actual CDT observables. For this reason we
will merely review the results of these computations and refer the
interested reader to the literature for the technical details.

One can obtain a qualitative understanding of the difference between DT
and CDT by considering which geometries are excluded from the path
integral in CDT. From the construction for the causal triangulations
given above we can see that there is no point at which the spatial slice
can change topology. In terms of the combinatorial interpretation this
means that the casual triangulations contain no double links. In the
triangulations discussed in DT the double links were able to join two
disconnected triangulations to form a larger one. If one were to view
the evolution of a DT universe such a joining of two disconnected
triangulations would correspond to part of the universe budding off from
the main one to form a baby universe fig 2.6 . Together with topology
changes, baby universe production is the only type of geometry excluded
in CDT which is present in DT [ 4 ] .

The difference between DT and CDT then depends on how much difference
baby universe production makes to the resulting continuum theory. It can
be shown that the effect of baby universe production is dramatic; it
completely dominates the continuum limit of DT, with a baby universe
forming at every point in the space [ 4 ] . The resulting spacetime is
highly non-smooth and fractal like as can be seen in fig 2.6 . In
contrast in CDT there exist no baby universes in the continuum limit.
This leads to spacetimes such as the one shown in fig 2.7 .

These result suggest we should consider some method by which the
“fractalness” of space can be measured. In fact the observable we have
yet to consider, the dimension of spacetime, is perfect for this role
and will be discussed in detail in the next chapter.

## Chapter 3 The Dimension of Spacetime

In the previous chapter we introduced the matrix model method, by which
the disc function and string susceptibility of DT may be computed. Of
course there was one observable discussed in the previous chapter that
we did not analyse using the matrix model approach; the dimension of the
spacetime. This observable is in some sense the most interesting, as if
we were a being living in such a universe the dimension is a far more
accessible observable than the disc function.

Finding the dimensionality of the spacetime may seem trivial as we
defined the theory using two-dimensional building blocks; indeed we even
refer to the theory as two dimensional quantum gravity. However, we will
see that the situation is more subtle than this. To give an idea of why
this is the case one can consider the paths contributing to the
propagator of a particle in the path integral formulation of quantum
mechanics. Although all paths are built piecewise from one dimensional
lines the entire path becomes “fuzzy” as we take the continuum limit. Is
the resulting fuzzy path a one or two dimensional object? One would be
tempted to answer that it somehow has a dimension in-between these two
values. This notion of fractional dimension can be made precise in a
number of ways and in this chapter we will see how such concepts may be
applied to the quantum gravity models defined in the previous chapter.

### 3.1 The Hausdorff Dimension @xmath

One manner by which the dimensionality of a space can be defined is by
observing how the volume of a ball scales with its radius. In smooth
@xmath -dimensional space we have @xmath , which motivates the
definition of the Hausdorff dimension for a metric space. The Hausdorff
dimension is defined such that if the Hausdorff dimension of the space
is @xmath , then for all points the following holds;

  -- -------- -- -------
     @xmath      (3.1)
  -- -------- -- -------

where @xmath is the volume of the ball of radius @xmath . In the models
of quantum gravity we are considering we are not interested in the
dimensionality of a single space but rather the expectation value for
the dimensionality of spacetime. We therefore must consider,

  -- -------- -- -------
     @xmath      (3.2)
  -- -------- -- -------

The quantity @xmath in the above equation may be computed for a given
geometry i.e. metric, contributing to the path integral and the above
statement is a statement about its mean behaviour after summing over all
geometries. We will now review a number of ways that this quantity may
be computed in DT and CDT models.

#### 3.1.1 Computing @xmath for DT and CDT

We will begin by assuming that the Hausdorff dimension is not affected
by the global topology of the spacetime; this allows one to consider a
particular amplitude for which the calculation is easier. In order to
compute the Hausdorff dimension we need to understand how to compute the
average area of a spacetime of fixed topology in addition to restricting
the geometries included in the amplitude to be of a certain “radius”. To
compute the mean area we need only differentiate with respect to the
bulk cosmological constant, as this will insert the identity operator
into the bulk. To enforce a fixed “radius” requires finer control of the
computation than allowed by matrix models and therefore we must instead
turn to the combinatorial approach. In order to make the notion of
“radius” precise we define the geodesic distance between two vertices to
be the number of links on the shortest path through the graph connecting
the vertices. The distance of a point from a boundary is defined as the
minimum geodesic distance between that point and a vertex on the
boundary and the distance between two boundaries is defined if all
points on one boundary have the same geodesic distance to the other
boundary.

The canonical amplitude to consider when computing the Hausdorff
dimension is the two point correlation function on the sphere with the
restriction that the geodesic distance between the operators is fixed to
@xmath [ 44 , 56 ] . This means our computation differs slightly from
the one that would be performed if we wanted to reproduce the definition
of @xmath given in ( 3.2 ). Instead we will compute how the total volume
of the spacetime depends on the distance between two maximally separated
points as the distance is made large.

Following [ 73 , 45 ] , this amplitude may be obtained by considering
the more general amplitude, @xmath , corresponding to a cylinder
amplitude in which the geodesic distance between the entrance and exit
loops, of length @xmath and @xmath respectively, is fixed to be @xmath .
The two point amplitude may then be obtained by considering the case
when both entrance and exit loops shrink to zero size. Once we obtain
the continuum two-point function @xmath for operators separated by a
distance @xmath we can then compute the Hausdorff dimension @xmath ,
defined implicitly by,

  -- -------- -- -------
     @xmath      (3.3)
  -- -------- -- -------

where @xmath is the total volume of spacetime containing two points
separated by a distance of @xmath . For triangulations containing @xmath
triangles in which the entrance and exit loops are of length @xmath and
@xmath respectively and are separated by a distance @xmath we may derive
a recursion relation for the number, @xmath , of such triangulations by
the same combinatorial methods used for the disc function [ 44 ] . There
are two possible ways to construct such a triangulation from smaller
triangulations depending on whether we add a new triangle or double
link, as detailed in fig 2.3 and fig 2.3 . This leads to the recursion
relation,

  -- -------- -- -------
     @xmath      (3.4)
  -- -------- -- -------

where the factor of two in the final sum is due to the fact that the
exit loop may appear in either of the two disjoint triangulations being
joined. In terms of @xmath the two loop amplitude @xmath is given by,

  -- -------- -- -------
     @xmath      (3.5)
  -- -------- -- -------

Consider a graph with @xmath triangles and @xmath boundary links in the
entrance loop. If we were to add @xmath more triangles to the entrance
loop evenly along the boundary, thereby adding a new layer to the
triangulation, we would have increased the distance separating the
entrance and exit loops by one. Therefore, if we were to add only a
single triangle to the graph then we would have increased @xmath by
roughly @xmath . Combining this with ( 3.4 ), leads to,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (3.6)
              @xmath   @xmath      (3.7)
  -- -------- -------- -------- -- -------

Finally, if we now introduce the 2-loop resolvent defined as,

  -- -------- -- -------
     @xmath      (3.8)
  -- -------- -- -------

we have

  -- -------- -- -------
     @xmath      (3.9)
  -- -------- -- -------

where @xmath is the disc function as obtain using the combinatorial
approach in the last chapter. To find the appropriate initial condition
for @xmath we first consider the initial condition for @xmath . We
require @xmath , which implies @xmath . To obtain a equation for
continuum quantities we must take a scaling limit. This requires us to
decide how @xmath will scale in addition to the second boundary
parameter @xmath . One can see that in order for the initial condition
to have a non-trivial scaling limit we must set @xmath , as usual, but
for @xmath we set @xmath . This gives,

  -- -------- -- --------
     @xmath      (3.10)
  -- -------- -- --------

where we have set @xmath in order for the scaling dimension of both
sides of the equation to agree. Using this scaling behaviour for @xmath
, @xmath and @xmath in ( 3.9 ) we find that the only non-trivial limit
occurs when @xmath , resulting in the equation,

  -- -------- -- --------
     @xmath      (3.11)
  -- -------- -- --------

This equation can be solved for @xmath , which together with the initial
condition ( 3.10 ) gives,

  -- -------- -- --------
     @xmath      (3.12)
  -- -------- -- --------

where @xmath satisfies,

  -- -------- -- --------
     @xmath      (3.13)
  -- -------- -- --------

The boundaries may be shrunk to points by taking the two cosmological
constants to infinity and extracting the leading order coefficient,
giving the result,

  -- -------- -- --------
     @xmath      (3.14)
  -- -------- -- --------

where @xmath is an unimportant numerical constant. Using the above
equation in ( 3.3 ) gives @xmath ¹ ¹ 1 A naive application of ( 3.3 )
will not in fact produce @xmath due the presence of a separate length
scale in the problem; that associated with the cosmological constant. A
more transparent calculation of @xmath would involve determining the
maximal distance between points as a function of @xmath and then
eliminating @xmath from an expression of the unconstrained total volume.
In our case we simply note that from dimensional analysis @xmath
yielding the stated result for @xmath . . This is quite a striking
result as it shows the effective dimension of the space is far from what
might be expected to arise from two dimensional building blocks. We
should also emphasise that the above result does not depend on
considering a long or short distance limit and therefore that @xmath on
all scales.

One can avoid some of the above calculation and obtain the Hausdorff
dimension via more qualitative means by noting that once the scaling
dimension of the geodesic distance has been decided, then, given that we
expect a power law relation between the geodesic distance and area, we
can obtain the exponent from dimensional analysis. In the above
computation we found that @xmath and since area has dimensions of @xmath
this means @xmath . It is interesting to note that the scaling relation
@xmath indicates that each link traversed has an effective size of
@xmath . This is reminiscent of a random walk and indicates that the
internal links of the graph form a highly random lattice, such as that
shown in fig 3.1 . Intuitively, this random structure is caused by there
being baby universes everywhere on the lattice, meaning any path through
the lattice must pass through many of them. This insight allows one to
give a reasonable conjecture for the Hausdorff dimension in CDT. If one
considers the triangulations arising in CDT then due to the sliced
structure we do not have any baby universes and we expect that the
shortest path through the triangulation will scale in the same way as
the boundary length, i.e. with dimensions of @xmath . We therefore
expect that the area and geodesic distance will be related by @xmath ,
giving @xmath . This qualitative argument is confirmed by an exact
computation using combinatorial methods [ 4 , 45 ] .

### 3.2 The Spectral Dimension @xmath

We saw in the last chapter that due to baby universe production we might
intuitively expect to obtain a continuum theory from DT that describes
the dynamics of a fractal spacetime. The last section gave concrete
evidence for this in that the mean Hausdorff dimension of the continuum
spacetime was four despite the lattice theory being defined using two
dimensional simplexes. However, the fact that the Hausdorff dimension is
exactly four does raise the question of how we can be sure that the
continuum theory is indeed fractal; perhaps it is a smooth four
dimensional manifold. This question arises because the Hausdorff
dimension is in fact not that sensitive to the fractal structure of a
space. One way to see this is to note it is the same for different
graphs with greatly differing structure, for example, the square lattice
and the comb graph obtained by removing all its horizontal links except
one. The difference is due to the decreased connectivity of the comb
graph.

In this section we will introduce another method by which the dimension
of a space can be defined based on the properties of a propagating
particle. This will allow us to distinguish the DT continuum limit from
a smooth manifold.

Consider the rate at which the force of an electric charge decreases
with distance, it clearly depends on dimension. This is related to the
form the particle propagator takes in the corresponding field theory.
Both of these things are determined by the Laplacian operator of the
spacetime on which they are defined and so one would expect that the
dimension of the space could be extracted from the Laplacian. A simple
way to do this is to look at some dynamical process whose equations of
motion involves the Laplacian and look for properties of its solutions
that are dependent on the dimension of the manifold. If the dynamical
process can be defined on spaces other than manifolds then these
dimension dependent observables can be used to define the dimension for
the space.

The usual choice for the dynamical process is that of heat diffusion. We
define the heat kernel @xmath by,

  -- -------- -- --------
     @xmath      (3.15)
  -- -------- -- --------

together with the boundary condition @xmath and @xmath . On a smooth
@xmath dimensional manifold the heat kernel has the property that as
@xmath ,

  -- -------- -- --------
     @xmath      (3.16)
  -- -------- -- --------

from which we can read off the dimension of the manifold. The dimension
computed in this way is referred to as the spectral dimension of the
space.

The question of generalising the spectral dimension to less smooth
spaces now amounts to generalising the process of heat diffusion. This
is in fact why the heat diffusion process was chosen; qualitatively, the
heat kernel @xmath can be thought of as a “scaling limit” of the
probability of a random walk moving between the points @xmath and @xmath
in @xmath steps. By a scaling limit we mean a process in which the
number of steps in the walk is taken to infinity while reducing the size
of each step.

The process of a random walk is obviously something that can be
naturally defined on a graph, which allows the spectral dimension to be
defined for each triangulation contributing to the path integral. In
particular, for triangulations, we need to compute the
return-probability @xmath for the random walk to leave a given point and
return to it in @xmath steps. In the scaling limit this quantity will
become @xmath and we then can study its behaviour as @xmath to find the
spectral dimension.

Obviously the above procedure depends on giving a precise definition of
the scaling limit. In fact we may extend the definition of the spectral
dimension to discrete structures, without taking a scaling limit, by
considering the @xmath behaviour of @xmath . The reason for studying the
@xmath limit in the discrete case can be understood from considering the
process of taking the scaling limit, in which the link length of the
discrete space is reduced to zero. A walk whose length is of finite size
measured in link lengths would produce a measurement of the spectral
dimension contaminated by cutoff effects. In contrast, a short walk in
the continuum space will still correspond to a walk whose length is much
greater than the link length. We therefore expect the large @xmath
behaviour of @xmath to fall off with the same exponent as the small
@xmath behaviour of @xmath . For discrete spaces we therefore define the
spectral dimension by,

  -- -------- -- --------
     @xmath      (3.17)
  -- -------- -- --------

if the limit exists.

As we saw with the computation of the Hausdorff dimension, the matrix
model approach has difficulty computing quantities that are not
disc-functions or their generalisations. Instead, we had to augment the
computation with a combinatorial approach. This problem is in fact worse
for computing the spectral dimension; this is not surprising as the
spectral dimension relates to a much more complicated phenomenon than
the Hausdorff dimension, which simply measures volume. In DT there
exists an approach to computing the spectral dimension, based on the
continuum formulation [ 57 ] , although we will not review this approach
in detail here. The result of the computation in [ 57 ] is that the
spectral dimension in DT is two. Because the Hausdorff dimension differs
from the spectral dimension, this confirms our expectation that the
continuum limit in two dimensional DT is not describing a smooth
manifold but a fractal space.

When it comes to CDT we are mostly stuck for the moment with merely
exploring the spectral dimension numerically. The result of such
numerical simulations is that in @xmath -dimensional CDT (where @xmath
has been considered) the spectral dimension is consistent with @xmath [
68 ] . For @xmath the situation is not so problematic and there has been
progress, which we will review shortly, in computing the spectral
dimension analytically. The result of such work suggests that unlike DT,
the long distance behaviour of the resulting spacetime is that of a
smooth @xmath -dimensional manifold.

#### 3.2.1 Dimensional Reduction

In addition to providing a measure of the dimension of the space, more
information can be extracted from the analysis of a random walk by
controlling the walk length. Since the size of the space explored by a
random walk of @xmath steps goes as @xmath we can probe shorter
distances by reducing the length of the walk. We therefore can gain some
insight into the Planck scale behaviour of gravity in CDT and other
models. The effective spectral dimension as a function of the length
probe has been investigated numerically in three and four dimensional
CDT with the surprising result that although the dimension is three and
four respectively on long distance scales, on very short scales the
spectral dimension in both cases is measured to be two [ 7 ] . This
phenomenon is known as dimensional reduction.

The phenomenon of dimensional reduction of the spectral dimension is
compelling, as similar behaviour has been observed in a variety of other
approaches to quantum gravity in four dimensions such as the
non-Gaussian fixed point possibility in the perturbation-is-fundamental
program [ 8 ] and also Horava-Lifshitz gravity [ 9 , 10 ] . Other
evidence has been discussed in [ 11 , 12 ] . This is very suggestive of
dimensional reduction being a robust feature of quantum gravity. It will
be our task in the next chapter to attempt to construct a toy model in
which dimensional reduction of the spectral dimension occurs.

#### 3.2.2 The Spectral Dimension in CDT

The triangulations in DT and CDT are formulated mathematically as a set
of graphs together with a measure on that set. We will refer to a set of
graphs together with a measure as a random graph or an ensemble of
graphs. To construct a toy model exhibiting dimensional reduction we
will want to consider a random graph in which a scaling limit can be
taken. Furthermore, we will want to keep the ensemble of graphs as
closely related to the CDT graphs as possible. Before doing this it is
worth pausing to draw inspiration from CDT. We will briefly review one
attempt to derive the spectral dimension in two dimensional CDT. This
will provide clues at to what sort of graphs should be used to construct
our ensemble.

As previously discussed, matrix model and combinatorial methods are not
appropriate for computing the spectral dimension. An alternative
approach is to directly analyse the random walk. One method that proves
useful in this approach is to encode the first-return-probability for a
random walk in a generating function [ 3 , 15 , 16 ] . By deriving
sufficiently tight bounds on this generating function we can obtain an
exact value for the spectral dimension. This method has the advantage
that, when it works, it can provide stronger statements than merely
averages; it can show that a particular value for the spectral dimension
holds occurs with probability one [ 15 , 16 ] . It also has the
advantage of being entirely mathematically rigourous.

To date it has proven too difficult to carry out the above strategy
directly on the triangulation. Instead attempts have focused on
considering ensembles of simpler graphs for two reasons; firstly,
because they serve as toy examples in which to develop the necessary
techniques and secondly because a number of simpler graph ensembles can
be obtained from CDT while preserving some of its properties. In fact it
can be shown that certain simpler graphs have spectral dimensions that
bound the spectral dimension of CDT. In particular the graphs obtained
by collapsing all vertices of a given height to a single vertex have
been used to show that @xmath almost surely for CDTs [ 16 ] . On the
other hand one can relate a causal triangulation to a tree graph @xmath
by the following procedure [ 16 ] . First we recognise that casual
triangulations are planar graphs and so can be represented as lying in
the plane with the spacelike slices forming concentric circles as shown
in fig 3.3 . We denote the vertices in the @xmath th spacelike slice as
@xmath , we define @xmath as containing a single new vertex connected to
all vertices in @xmath . The algorithm then maps the triangulation into
a tree @xmath :

-   All vertices in the triangulation are in @xmath in addition to a new
    root vertex that is only linked to @xmath .

-   All links from @xmath to @xmath are in @xmath .

-   All links from a vertex in @xmath to vertices in @xmath , apart from
    the clockwise-most link, are in @xmath .

This algorithm is shown in fig 3.3 . In fact it can be shown that the
map produced by this algorithm is a bijection and furthermore preserves
the measure on the CDTs. The spectral dimension of the resulting random
tree has long been known to be @xmath and this gives a lower bound on
the spectral dimension of the CDTs. One tool to obtain results about
such trees is another random graph known as a comb. It is this random
graph that we will use in the next chapter to construct a toy example of
dimensional reduction.

## Chapter 4 A Toy Model of Dimensional Reduction

In the last chapter we saw that both DT and CDT exhibit fractal
behaviour in some regimes of the theory. However DT appears to be far
more pathological as fractal spacetimes dominate the continuum limit.
Furthermore we saw that the situation in CDT is much improved, with both
Hausdorff and spectral dimension agreeing and agreeing with the
dimension, on long distance scales, of the underlying simplexes. At
short distances however it exhibits a reduction in the spacetime
dimension; a phenomenon that has been observed in other approaches to
quantum gravity and has been given the name dimensional reduction.

In this chapter we will construct a definition for a scale dependent
spectral dimension and show that there are models which do indeed
exhibit scale dependent spectral dimensions defined in this way. In
particular we develop a simple model based on previous work on random
combs [ 3 ] . These are a family of simple geometrical models which
share some of the properties of the CDT model; instead of an ensemble of
triangulations we have an ensemble of graphs consisting of an infinite
spine with teeth of identically independently distributed length hanging
off (we define these graphs precisely in Section 2). It was shown in [ 3
] that the spectral dimension is determined by the probability
distribution for the length of the teeth. In this chapter we show that
it is possible to extend the work of [ 3 ] by taking a continuum limit
thus ensuring that the cut-off scale is much shorter than all physical
distance scales. We find that the spectral dimension is one if we take
the physical distance explored by the random walk to zero and there
exists a number of continuum limits in which the long distance spectral
dimension differs from its short distance counterpart. As a by-product
of this work we also extend some of the proofs given in [ 3 ] to a wider
class of probability distributions.

This chapter is organized as follows. In Section 4.1 we briefly review
some known results for combs and their spectral dimension and then
explain how in principle these can be extended to show different
spectral dimensions at long and short distance scales. In Section 4.2 we
introduce a simple model which we prove does in fact exhibit a spectral
dimension that is different in the UV and IR. This model forms the basis
of all later generalisations. In Section 4.3 we generalise the results
of Section 4.2 to combs in which teeth of any length may appear with a
probability governed by a power law. In Section 4.4 we examine the
possibility of intermediate scales in which the spectral dimension
differs from both its UV and IR values. In Section 4.5 we analyse the
case of a comb in which the tooth lengths are controlled by an arbitrary
probability distribution and show that continuum limits exist in which
the short distance spectral dimension is one while the long distance
spectral dimension can assume values in one-to-one correspondence with
the positions of the real poles of the Dirichlet series generating
function for the probability distribution. We then show how these
techniques can be used to extend the results of [ 3 ] . In Section 4.6
we discuss our results and possible directions for future work.

### 4.1 Combs and Walks

In this section we review some basic facts about random combs and random
walks. As much as possible we use the same notation and conventions as [
3 ] and refer to that paper for proofs omitted here.

#### 4.1.1 Definitions

We use the definition of a comb given in [ 3 ] . Consider the
nonnegative integers regarded as a graph, which we denote @xmath , so
that @xmath has the neighbours @xmath except for @xmath which only has
@xmath as a neighbour. Furthermore, let @xmath be the integers @xmath
regarded as a graph so that each integer @xmath has two neighbours
@xmath except for @xmath and @xmath which only have one neighbour,
@xmath and @xmath , respectively. A comb @xmath is an infinite rooted
tree-graph with a special subgraph @xmath called the spine which is
isomorphic to @xmath with the root at @xmath . At each vertex of @xmath
, except the root @xmath , there is attached one of the graphs @xmath or
@xmath . We adopt the convention that these linear graphs which are
glued to the spine are attached at their endpoint @xmath . The linear
graphs attached to the spine are called the teeth of the comb, see
figure 4.1 . We will find it convenient to say that a vertex on the
spine with no tooth has a tooth of length @xmath . We will denote by
@xmath the tooth attached to the vertex @xmath on @xmath , and by @xmath
the comb obtained by removing the links @xmath , the teeth @xmath and
relabelling the remaining vertices on the spine in the obvious way. The
number of nearest neighbours of a vertex @xmath will be denoted @xmath .

It is convenient to give names to some special combs which occur
frequently. We denote by @xmath the full comb in which every vertex on
the spine is attached to an infinite tooth, and by @xmath the empty comb
in which the spine has no teeth (so an infinite tooth is itself an
example of @xmath ).

Now let @xmath denote the collection of all combs and define a
probability measure @xmath on @xmath by letting the length of the teeth
be identically and independently distributed by the measure @xmath . We
will refer to the set @xmath equipped with the probability measure
@xmath as a random comb. Measurable subsets @xmath of @xmath are called
events and @xmath is the probability of the event @xmath . The measure
of the set of combs @xmath with teeth at @xmath having lengths @xmath is

  -- -------- -- -------
     @xmath      (4.1)
  -- -------- -- -------

For any @xmath -integrable function @xmath defined on @xmath we define
the expectation value ¹ ¹ 1 Since the space of combs is discrete, the
integration should be replaced by a sum. However, we will continue to
use an integral sign, with the understanding that it refers to a sum
over discrete structures.

  -- -------- -- -------
     @xmath      (4.2)
  -- -------- -- -------

We will often use the shorthand @xmath for @xmath .

#### 4.1.2 Random Walks

We consider simple random walks on the comb @xmath and count the time
@xmath in integer steps. At each time step the walker moves from its
present location at vertex @xmath to one of the neighbours of @xmath
chosen with equal probabilities @xmath . Unless otherwise stated the
walker always starts at the root at time @xmath .

The generating function for the probability @xmath that the walker is at
the root at time @xmath , having left it at @xmath , is defined by

  -- -------- -- -------
     @xmath      (4.3)
  -- -------- -- -------

and we denote by @xmath the corresponding generating function for the
probability that the walker returns to the root for the first time,
excluding the trivial walk of length 0. Since walks returning to the
root can be decomposed into walks returning for the 1st, 2nd etc time we
have

  -- -------- -- -------
     @xmath      (4.4)
  -- -------- -- -------

It is convenient to consider contributions to @xmath and @xmath from
walks which are restricted. Let @xmath denote the contribution to @xmath
from walks whose maximal distance along the spine from the root is
@xmath and define

  -- -------- -- -------
     @xmath      (4.5)
  -- -------- -- -------

which is the contribution from all walks which do not reach the point
@xmath on the spine. Similarly we define

  -- -------- -- -------
     @xmath      (4.6)
  -- -------- -- -------

Clearly @xmath can be recovered from @xmath by setting @xmath . We
define the corresponding restricted contributions to @xmath in the same
way. By decomposing walks contributing to @xmath into a step to @xmath ,
walks returning to @xmath without visiting the root, and finally a step
back to the root it is straightforward to show that

  -- -------- -- -------
     @xmath      (4.7)
  -- -------- -- -------

where we have adopted the convention that for the empty tooth, @xmath ,

  -- -------- -- -------
     @xmath      (4.8)
  -- -------- -- -------

The relation ( 4.7 ) can be used to compute the generating function
explicitly for any comb with a simple periodic structure and we list
some standard results in A.1 .

There are a number of elementary lemmas which characterise the
dependence of @xmath on the length of the teeth and the spacing between
them [ 3 ] . We state them here in a slightly generalized form which is
useful for our subsequent manipulations.

###### Lemma 1

The function @xmath is a monotonic increasing function of @xmath and
@xmath for any @xmath .

###### Lemma 2

@xmath is a decreasing function of the length, @xmath , of the tooth
@xmath for any @xmath .

###### Lemma 3

Let @xmath be the comb obtained from @xmath by swapping the teeth @xmath
and @xmath , @xmath . Then @xmath if and only if @xmath .

The proofs use ( 4.7 ) and follow those given in [ 3 ] for the case
@xmath .

An important corollary, valid for any comb, of these lemmas is that

  -- -------- -- -------
     @xmath      (4.9)
  -- -------- -- -------

which we will refer to as the trivial upper and lower bounds on @xmath .
The result follows from Lemma 2 with @xmath , which gives

  -- -------- -- --------
     @xmath      (4.10)
  -- -------- -- --------

and the explicit expressions for @xmath and @xmath given in A.1 .

#### 4.1.3 Two point functions

Two point correlation functions on the comb correspond to the
probability of a walk beginning at the root being at a particular vertex
on the spine at time @xmath . In particular, let @xmath denote the
probability that a random walk that starts at the root at time zero is
at the vertex @xmath on the spine at time @xmath having not visited the
root in the intervening period. We will refer to the generating function
for these probabilities as the two point function, @xmath , and define
it by

  -- -------- -- --------
     @xmath      (4.11)
  -- -------- -- --------

@xmath may be expressed as

  -- -------- -- --------
     @xmath      (4.12)
  -- -------- -- --------

which may be used in conjunction with Lemma 2 to obtain the bounds,

  -- -------- -- --------
     @xmath      (4.13)
  -- -------- -- --------

Now let @xmath denote the probability that a random walk that starts at
the root at time zero is at the vertex @xmath on the spine for the first
time at time @xmath having not visited the root in the intervening time.
We define the modified two point function, @xmath , by,

  -- -------- -- --------
     @xmath      (4.14)
  -- -------- -- --------

and note the following lemmas;

###### Lemma 4

The contribution @xmath to @xmath from walks whose maximal distance from
the root is @xmath or greater satisfies

  -- -------- -- --------
     @xmath      (4.15)
  -- -------- -- --------

The proof is given in section 2.4 of [ 3 ] .

###### Lemma 5

The modified two point function satisfies

  -- -------- -- --------
     @xmath      (4.16)
  -- -------- -- --------

To prove this note that

  -- -------- -- --------
     @xmath      (4.17)
  -- -------- -- --------

and use Lemma 2 .

#### 4.1.4 Spectral dimension and the continuum limit

As was discussed in the previous chapter, we may characterise the
dimension of spaces which are not manifolds by use of the spectral
dimension. For spaces obtained as the continuum limit of a graph or
ensemble of graphs the probability that a random walk returns to its
initial position generalises the heart kernel. However, given that it is
easier to work with the generating function for the return probability,
@xmath , it is more convenient to define the spectral dimension in terms
of the behaviour of @xmath implied by ( 3.17 ),

  -- -------- -- --------
     @xmath      (4.18)
  -- -------- -- --------

where by @xmath we mean that

  -- -------- -- --------
     @xmath      (4.19)
  -- -------- -- --------

where @xmath , @xmath and @xmath are positive constants. The property (
4.18 ) was adopted in [ 3 ] as the definition of spectral dimension,
assuming it exists. The spectral dimension of an ensemble average is
defined in the same way, simply replacing @xmath and @xmath by their
respective expectation values.

Our present goal is to extend the definition of the spectral dimension
to give a mathematical meaning to the notion of a scale dependent
spectral dimension as required to describe the phenomenon of dimensional
reduction. In order for the spectral dimension to be scale dependent
there must exist a length scale @xmath in the system besides the cutoff.
Furthermore, in order for this length scale to survive in the continuum,
it must be scaled as the continuum limit is taken. We will therefore
introduce a second parameter into the tooth-length probability
distribution @xmath , which defines a length scale for some structure of
the comb. One would hope that in more realistic models such a length
scale could be generated dynamically.

We now give a precise meaning to the term continuum limit. We assign the
value @xmath to the distance between adjacent vertices in the graph and
take the limit @xmath and @xmath in such a way that the scaled combs
have a finite characteristic distance scale; it is this limit to which
we give the name continuum limit and quantities which exist in this
limit we call continuum quantities. Walks much longer than @xmath will
probe different structure from walks much shorter than @xmath but
nonetheless both can be very long in units of the underlying cut-off
scale @xmath .

In the following sections we will denote dependence of a function on a
number of variables @xmath ² ² 2 In the following, the parameters @xmath
will becomes length scales related to structures in the comb. , @xmath ,
by @xmath passed as one of the function arguments. Given a random comb
ensemble specified by @xmath and the corresponding @xmath we define the
continuum limit of @xmath by,

  -- -------- -- --------
     @xmath      (4.20)
  -- -------- -- --------

where the scaling dimensions @xmath and @xmath are chosen to ensure a
non-trivial limit and the combinations @xmath are dimensionless. As we
shall see, the choice for @xmath and @xmath is unique given mild
assumptions. The function @xmath can be used to define the spectral
dimension at short and long distances.

The variable @xmath of the generating function plays the role of a
fugacity for the walk length. Indeed it is the Laplace conjugate
variable to walk length, this is completely analogous to the way in
which the variable for the resolvent in Chapter 2 plays the role of the
continuum boundary cosmological constant and is related to the boundary
length by a Laplace transform. We therefore expect @xmath to correspond
to very short continuum walks and @xmath to correspond to long continuum
walks. Let us for a moment suppose that the long and short distance
behaviour of @xmath is that of a power law and further that there exists
only one length scale @xmath . If a diffusion experiment were performed
in which the diffusion time was much greater than @xmath then an
experimenter would see the power law behaviour associated with the
@xmath limit of @xmath . Conversely, if the walk length was much less
then @xmath then an experimenter would observe the power law behaviour
of the @xmath limit of @xmath . If the exponent of the power law differs
in these two limits then the experimenter would conclude the spectral
dimension differed on short and long distance scales.

We now give an explicit definition for the long and short distance
spectral dimension and show that the above long and short distance
limits only contain contributions from long and short walks
respectively. We will assume for simplicity that there is just one scale
@xmath and that the spectral dimension in the sense of ( 3.17 ) exists
for @xmath which implies that there exists a constant @xmath such that
@xmath . Note that

  -- -------- -- --------
     @xmath      
                 
                 
     @xmath      (4.21)
  -- -------- -- --------

Since the final sum is finite it will have an analytic dependence on
@xmath . Now choose

  -- -------- -- --------
     @xmath      (4.22)
  -- -------- -- --------

and set @xmath and @xmath in ( 4.1.4 ) to get

  -- -------- -- --------
     @xmath      
                 
     @xmath      (4.23)
  -- -------- -- --------

where @xmath is a constant arising form finite sum in ( 4.1.4 ).
Provided that the limit in ( 4.20 ) exists we see that the behaviour of
@xmath as @xmath characterizes the properties of walks of continuum time
duration less than

  -- -------- -- --------
     @xmath      (4.24)
  -- -------- -- --------

and we define the spectral dimension @xmath at short distances by

  -- -------- -- --------
     @xmath      (4.25)
  -- -------- -- --------

provided this limit exists.

We can define the spectral dimension at long distances in a similar way.
First note that by ( 4.9 )

  -- -- -- --------
           (4.26)
  -- -- -- --------

so that

  -- -------- -- --------
     @xmath      (4.27)
  -- -------- -- --------

This time letting @xmath we get

  -- -------- --
     @xmath   
  -- -------- --

If the above limit exists and furthermore @xmath diverges as @xmath then
we see that in this limit @xmath only receives contributions from walks
whose length exceeds @xmath . We define the spectral dimension @xmath at
long distances to be

  -- -------- -- --------
     @xmath      (4.29)
  -- -------- -- --------

provided this limit exists.

At this point it is unclear whether there exists any examples for which
the short and long distance spectral dimensions as in ( 4.25 ) and (
4.29 ) exist and furthermore differ in value. We will now discuss some
examples based on random combs in which we realise such examples. In all
the examples given in this chapter it turns out that the exponent is
@xmath .

### 4.2 A simple comb

We now introduce a random comb whose spectral dimension differs on long
and short length scales and thus illustrates that the behaviour
described in section 4.1.4 can actually occur. This comb is defined by
the measure,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.30)
  -- -------- -------- -------- -- --------

This random comb has infinite teeth and they occur with an average
separation of @xmath . Intuitively we would expect that if a random
walker did not move further than a distance of order @xmath from its
starting position it would not see the teeth and therefore would measure
a spectral dimension of one. If however it were allowed to explore the
entire comb it would see something roughly equivalent to a full comb and
so feel a much larger spectral dimension. To prove this intuition
correct we proceed by computing upper and lower bounds for @xmath which
are uniform in @xmath and for @xmath , where the constant @xmath is
equal to one unless otherwise stated, and then take the continuum limit
to obtain bounds for @xmath .

With complete generality we may obtain a lower bound on @xmath by use of
Jensen’s inequality which takes the form,

###### Lemma 6

Let @xmath be the mean first return probability generating function of
the teeth of the comb defined by @xmath , then

  -- -------- -- --------
     @xmath      (4.31)
  -- -------- -- --------

The proof is given in [ 3 ] . For the comb ( 4.30 ) we have

  -- -------- -- --------
     @xmath      (4.32)
  -- -------- -- --------

which implies

  -- -------- -- --------
     @xmath      (4.33)
  -- -------- -- --------

Letting @xmath and @xmath gives

  -- -------- -- --------
     @xmath      (4.34)
  -- -------- -- --------

where we have introduced the dimensionless variable @xmath .

To find an upper bound on @xmath we follow [ 3 ] and use Lemmas 1 , 2
and 3 to compare a typical comb in the ensemble with the comb consisting
of a finite number of infinite teeth at regular intervals. First we
define the event

  -- -------- -- --------
     @xmath      (4.35)
  -- -------- -- --------

where @xmath is the distance between the @xmath and @xmath teeth and
then write,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.36)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Since the @xmath are independently distributed

  -- -------- -- --------
     @xmath      (4.37)
  -- -------- -- --------

Consider a comb @xmath ; then by Lemmas 1 , 2 and 3 ,

  -- -------- -- --------
     @xmath      (4.38)
  -- -------- -- --------

where @xmath is the comb obtained by removing all teeth beyond the
@xmath tooth and moving the remaining teeth so that the spacing between
each is @xmath . Now we can write

  -- -------- -- --------
     @xmath      (4.39)
  -- -------- -- --------

Since the walks contributing to @xmath do not go beyond the last tooth
we have

  -- -------- -- --------
     @xmath      (4.40)
  -- -------- -- --------

where @xmath denotes the comb consisting of infinite teeth regularly
spaced and separated by a distance @xmath . Using ( 4.40 ), Lemmas 4 and
5 we have,

  -- -------- -- --------
     @xmath      (4.41)
  -- -------- -- --------

uniformly in @xmath . @xmath and @xmath are given in Appendix A. Now set
@xmath and @xmath , where,

  -- -------- -- --------
     @xmath      (4.42)
  -- -------- -- --------

Since @xmath is manifestly a monotonic decreasing function of @xmath and
@xmath an increasing function of @xmath ,

  -- -------- -- --------
     @xmath      (4.43)
  -- -------- -- --------

where we have used ( 4.9 ) and

  -- -------- -- --------
     @xmath      (4.44)
  -- -------- -- --------

Taking the continuum limit of ( 4.43 ) and using the results of A.1 then
gives

  -- -------- -- --------
     @xmath      (4.45)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.46)
  -- -------- -- --------

It follows from ( 4.25 ), ( 4.29 ), ( 4.34 ) and ( 4.45 ) that

  -- -------- -- --------
     @xmath      (4.47)
  -- -------- -- --------

### 4.3 Combs with Power Law Measures

We now consider slightly more general combs in which the measure on the
teeth is a power law of the form,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.48)
  -- -------- -------- -------- -- --------

where @xmath is a normalisation constant and as before @xmath plays the
role of a distance scale. We consider laws in the range @xmath as it is
known that for @xmath the comb has spectral dimension @xmath in the
sense of ( 4.18 ) [ 3 ] and therefore it is not possible to get a
spectral dimension deviating from 1 on any scale.

To compute a lower bound on the return probability generating function
for the above distribution we apply Lemma 6 and reduce the problem to
computing an upper bound on @xmath . The first return generating
function @xmath for a tooth of length @xmath is recorded in ( A.5 );
bounding @xmath above by the function @xmath for @xmath and @xmath for
@xmath gives ³ ³ 3 In this particular case we could in fact compute
@xmath exactly by the Abel summation formula. However the bound we use
is good enough to give the desired result with the advantage that the
calculation can be done with elementary functions.

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.49)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

To obtain the second inequality we have applied the Abel summation
formula. We therefore have,

###### Lemma 7

For a random comb defined by the measure @xmath ,

  -- -------- -- --------
     @xmath      (4.50)
  -- -------- -- --------

where the cumulative probability function @xmath is defined by @xmath .

We will see shortly that all behaviour of the spectral dimension of the
continuum comb is encoded in the asymptotic expansion of @xmath as
@xmath goes to infinity ⁴ ⁴ 4 In general it is not obvious that this
asymptotic expansion exists due to the discontinuous nature of @xmath .
We will address this issue later when we consider generic measures. . In
the present case @xmath is trivially related to the partial sum of the
Riemann @xmath -function whose leading asymptotic behaviour is well
known and we find

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.51)
  -- -------- -------- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.52)
  -- -------- -- --------

It follows that for @xmath , where @xmath ,

  -- -------- -- --------
     @xmath      (4.53)
  -- -------- -- --------

with @xmath being constants depending only on @xmath and @xmath .
Choosing @xmath with @xmath yields a lower bound on the continuum return
generating function,

  -- -------- -- --------
     @xmath      (4.54)
  -- -------- -- --------

To obtain a comparable upper bound we need

###### Lemma 8

For any random comb and positive integers @xmath , @xmath and @xmath ,
the return probability generating function is bounded above by

  -- -------- -- --------
     @xmath      (4.55)
  -- -------- -- --------

where

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (4.56)
  -- -------- -------- -------- -- --------

and @xmath is the first return probability generating function for the
comb with teeth of length @xmath equally spaced at intervals of @xmath .

The proof is a slight modification of the upper bound argument used in
Section 4.2 . First define a long tooth to be one whose length is
greater than @xmath ; then the probability that a tooth at a particular
vertex is long is

  -- -------- -- --------
     @xmath      (4.57)
  -- -------- -- --------

Define the event

  -- -------- -- --------
     @xmath      (4.58)
  -- -------- -- --------

where now @xmath is the distance between the @xmath and @xmath long
teeth so that

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (4.59)
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

Since the @xmath are independently distributed

  -- -------- -- --------
     @xmath      (4.60)
  -- -------- -- --------

Now use Lemmas 2 and 3 in turn to note that for

  -- -------- -- --------
     @xmath      (4.61)
  -- -------- -- --------

where @xmath is the comb in which all teeth but the first @xmath long
teeth have been removed and the remaining long teeth have been arranged
so that they have length @xmath and a constant inter-tooth distance
@xmath . By the same arguments as we used in Section 4.2 to get ( 4.41 )
we obtain the bound

  -- -------- -- --------
     @xmath      (4.62)
  -- -------- -- --------

Lemma 8 then follows from ( 4.9 ), ( 4.60 ) and ( 4.62 ). We now
specialise to the power law measure ( 4.48 ) and set @xmath , @xmath and
@xmath , where

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (4.63)
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- --------

Using Lemma 8 , the scaling expressions for @xmath and @xmath given in (
A.7 ), and taking the continuum limit, gives, after a substantial amount
of algebra,

  -- -------- -- --------
     @xmath      (4.64)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (4.65)
  -- -------- -- --------

The main result of this section is

###### Theorem 1

The comb with the power law measure ( 4.48 ) for the tooth length has

  -- -------- -- --------
     @xmath      (4.66)
  -- -------- -- --------

The result follows immediately from ( 4.25 ), ( 4.29 ), ( 4.54 ) and (
4.64 ).

### 4.4 Multiple Scales

Given the results for the power law distribution it is natural to
investigate the behaviour for a random comb that has a hierarchy of
length scales. The easiest way to achieve such a comb is through a
double power law distribution,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (4.67)
  -- -------- -------- -------- -- --------

We may assume without loss of generality that the length scales @xmath
scale in the continuum limit to lengths @xmath such that @xmath and that
@xmath .

Following the procedure of previous sections a lower bound on @xmath is
obtained by using Lemma 6 and noting that @xmath for this comb is
essentially the sum of the cumulative probability functions for each
power law. This gives

  -- -------- --
     @xmath   
  -- -------- --

Choosing @xmath to scale like @xmath , where @xmath , gives a bound on
the continuum return generating function of,

  -- -------- -- --------
     @xmath      (4.69)
  -- -------- -- --------

An upper-bound on @xmath is obtained by application of Lemma 8 in which
we set @xmath , @xmath and @xmath , where

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (4.70)
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- --------

and for convenience we have introduced the function,

  -- -------- -- --------
     @xmath      (4.71)
  -- -------- -- --------

Using Lemma 8 and the scaling expressions in A.1 then gives

  -- -------- -- --------
     @xmath      
     @xmath      (4.72)
  -- -------- -- --------

where @xmath , @xmath , @xmath and we have suppressed the arguments of
@xmath in order to maintain readability.

We can now examine ( 4.69 ) and ( 4.4 ) to see what they tell us about
the behaviour of @xmath on various length scales.

-   When @xmath both upper and lower bounds of @xmath are dominated by
    the @xmath behaviour so taking the @xmath limit leads to @xmath as
    in the previous sections.

-   If @xmath then when @xmath both upper and lower bounds of @xmath are
    dominated by the @xmath behaviour so taking the @xmath limit leads
    to @xmath . There is no regime in which @xmath controls the
    behaviour.

-   If @xmath then when @xmath where

      -- -------- -- --------
         @xmath      (4.73)
      -- -------- -- --------

    both upper and lower bounds of @xmath are dominated by the @xmath
    behaviour so taking the @xmath limit leads to @xmath . However there
    is an intermediate regime @xmath where the @xmath behaviour
    dominates and @xmath lies in the envelope given by

      -- -------- -- --------
         @xmath      (4.74)
      -- -------- -- --------

    where the upper and lower bounds will have corrections suppressed by
    powers of @xmath and the upper bound will also have corrections of
    order @xmath . Both @xmath and @xmath may be chosen to make the
    corrections arbitrarily small in this scale range. The system
    therefore appears to have spectral dimension @xmath in this regime.
    This is a fairly weak statement because @xmath could in principle
    exhibit a wide variety of behaviours between its upper and lower
    bounds; this region is just a part of the crossover regime from
    @xmath to @xmath . However, as we are free to chose @xmath to be as
    large as we like compared to @xmath , this regime can exist over a
    scale range of arbitrarily large size. We therefore can force the
    leading behaviour of @xmath in this range to be as close to a power
    law with exponent @xmath as we like. This is what might be observed,
    for example, in a numerical simulation; if the difference between
    the scales @xmath and @xmath is large then there will be a
    substantial range of walk lengths in which the data will indicate a
    spectral dimension of @xmath . We will refer to a spectral dimension
    that appears in this weaker way as an apparent spectral dimension
    and denote it by @xmath rather than @xmath .

### 4.5 Generic Distributions

So far we have considered combs in which the distribution of tooth
lengths has been governed by power laws or double power laws. In this
section we extend the results of the previous sections to the case were
the form of the tooth length distribution is left arbitrary. The most
general situation is that the measure on the combs is a continuous
function of some parameters @xmath . The continuum limit of such a comb
is obtained in the usual way but with parameters @xmath scaling in a
non-trivial way; @xmath . Given a random comb with such a measure we
would like to know how many distinct continuum limits exist and for each
compute how the spectral dimension depends on the length scale.

The approach we adopt here closely mimics the arguments of the preceding
sections, indeed the main complication is technical. As we have seen the
properties of the continuum comb are controlled by the asymptotic
expansion of @xmath as @xmath goes to infinity. The main difference in
the generic case is that we may arrange matters so that the scaling
dimensions of the coefficients in the asymptotic expansion are such that
sub-leading terms appear in the continuum. For the generic case we
obviously have no way of knowing the full asymptotic expansion of @xmath
. However, we will see that for a large class of measures, the form of
the asymptotic expansion is encoded in the asymptotic expansion of a
particular generating function for @xmath .

Our first task is to introduce this generating function and relate it to
the asymptotic expansion of @xmath . To this end we introduce the notion
of a smoothed sum [ 13 ] ,

  -- -------- --
     @xmath   
  -- -------- --

where @xmath is the smooth cut-off function introduced in A.2 and @xmath
controls where the cut-off occurs. Such smoothed sums are related to
@xmath by,

  -- -------- -- --------
     @xmath      (4.76)
  -- -------- -- --------

The reason for introducing the smoothed sums is that we may use powerful
techniques from complex analysis to compute their asymptotic expansion
(see eg [ 14 ] ). The generating function to which the asymptotic
expansion of the smoothed sums is related is the Dirichlet series
generating function of @xmath ,

  -- -------- -- --------
     @xmath      (4.77)
  -- -------- -- --------

We now introduce a number of results and notations,

-   For a strip in the complex plane @xmath where @xmath , we say @xmath
    has slow growth in @xmath if for all @xmath we have @xmath for some
    @xmath as @xmath . We say @xmath has weak slow growth if the above
    property only holds for a countable number of horizontal regions
    across the strip. See figure 4.2 .

-   Define @xmath to be the set containing the triples @xmath such that
    the Laurent expansion of @xmath about the point @xmath contains the
    term @xmath where @xmath and @xmath . Here @xmath denotes the Mellin
    transformation and @xmath is introduced in A.2 . We will often find
    it useful to refer to the elements of @xmath using an index @xmath
    and denote the @xmath th element of @xmath by @xmath . Note that
    since @xmath is analytic in @xmath then the positions of the poles
    are determined only by @xmath .

-   Define the indexing sets @xmath and @xmath , such that if @xmath
    then @xmath whereas if @xmath then @xmath and for both @xmath i.e.
    they index the poles in @xmath which lie on the real line and off
    the real line respectively.

-   For a Dirichlet series with positive coefficients the abscissa of
    absolute and conditional convergence coincide. Furthermore, since
    @xmath the abscissa of convergence is less than zero.

-   Landau’s theorem: For a Dirichlet series with positive coefficients
    there exists a pole at the abscissa of convergence. A corollary of
    this is that the coefficient of the most singular term in the
    Laurent expansion about the abscissa of convergence is positive and
    furthermore it is the right-most pole of the Dirichlet series in the
    complex plane.

If @xmath is of slow growth in @xmath and no pole occurs to the right of
@xmath , then by using the expression ( A.26 ) the asymptotic expansion
of @xmath is,

  -- -------- -- --------
     @xmath      
     @xmath      (4.78)
     @xmath      
  -- -------- -- --------

where @xmath is the rectangular contour introduced in A.3 ⁵ ⁵ 5 Since we
have assumed there are no poles to the right of @xmath we may choose
@xmath , where @xmath is the constant appearing in the definition of the
contour. , @xmath are the coefficients of the Laurent expansions of
@xmath , and @xmath is a remainder function which satisfies @xmath ,
where @xmath , as @xmath goes to infinity. Note that the difference
between the coefficients @xmath and @xmath is of order @xmath , the
parameter introduced in A.2 , which can be taken to be arbitrarily small
and so @xmath and @xmath are for all purposes equal.

We now are in a position to prove the main result of this section. Note
that in the following we require the continuum comb is such that @xmath
and so has spectral dimension one on the smallest scales. It seems very
likely that this restriction could be lifted but we will not pursue such
a generalisation here.

###### Theorem 2

For a comb in which the teeth are distributed according to the measure
@xmath then if @xmath has slow growth in the strip @xmath and has no
poles on its line of convergence besides at the abscissa then continuum
limits of the comb exist with @xmath and @xmath taking values in the set
@xmath .

###### Proof.

The proof proceeds in much the same way as the proofs in the previous
sections; we first derive upper and lower bounds on @xmath and then use
these bounds to deduce the behaviour of @xmath in different scale
ranges.

We begin with the lower bound which by Jensen’s inequality amounts to
finding an upper bound on @xmath . Note that ( 4.31 ) implies that the
scaling dimension of @xmath must be greater than or equal to one in
order for @xmath and only contributes to the continuum limit if it has
scaling dimension one. From Lemma 7 we have,

  -- -------- -- --------
     @xmath      
     @xmath      (4.79)
  -- -------- -- --------

where @xmath is a constant independent of @xmath . It is important to
recall that since @xmath for all values of @xmath and @xmath , in
particular when @xmath assume their critical values, that the scaling
dimensions of the coefficients @xmath appearing in ( 4.5 ) must be
positive as otherwise @xmath would diverge if we were to set @xmath to
their critical values. Upon performing the integration in ( 4.5 ) we
will find that a given @xmath now is the coefficient for a number of
terms of increasing scaling dimension. Since we require the scaling
dimension of @xmath to be greater than or equal to one, only the term
with smallest scaling dimension can appear in the continuum limit and we
will drop any term that does not appear in the continuum limit. If we
now substitute ( 4.5 ) into ( 4.5 ) we find

  -- -------- -- --------
     @xmath      
     @xmath      (4.80)
  -- -------- -- --------

where the remainder term @xmath disappears since it cannot appear in the
scaling limit. We now suppose we may choose the critical values and
scaling dimensions of the parameters @xmath such that @xmath has a
scaling form that can be written as,

  -- -------- -- --------
     @xmath      (4.81)
  -- -------- -- --------

where @xmath , @xmath and @xmath are constants and we have included the
factor of @xmath for later convenience. From ( 4.5 ) one can see that
since we require @xmath the only terms which appear in the continuum
limit are those for which @xmath and @xmath and so it is useful to
define a restricted indexing set

  -- -------- -- --------
     @xmath      (4.82)
  -- -------- -- --------

and an equivalent one @xmath for @xmath . The continuum limit is thus,

  -- -------- -- --------
     @xmath      (4.83)
     @xmath      
  -- -------- -- --------

where @xmath . The lower bound on @xmath is then obtained from Lemma 6 .

Before deriving an upper bound on @xmath , we must analyse the
consequences of any of the oscillatory terms in the second sum appearing
in the continuum limit (i.e. if @xmath is non-empty). In the case of the
double power law measure we saw that one could obtain intermediate
behaviour in which an effective spectral dimension was measured that
differed from the UV and IR spectral dimensions. A similar phenomenon
occurs in the generic case when the various length scales of the
continuum limit are well separated. If we scale the coefficients of the
oscillatory terms such that these terms appear in the continuum limit
then we are lead to an inconsistency by the following argument,

-   Let the term associated with an oscillatory term have index @xmath .
    Consider choosing the scaling form of @xmath and @xmath such that
    @xmath but @xmath if @xmath .

-   If we were to take the scaling limit in such a scenario then the
    term that dominates the size of @xmath is the one associated with
    the length scale @xmath . This term will be oscillating around a
    mean of zero and hence must be going negative infinitely often as we
    approach the continuum.

-   Since @xmath is a probability generating function it cannot be
    negative, hence showing we have an unphysical limit.

The cause of this behaviour is that we declared by fiat that the
parameters @xmath must be scaled to give ( 4.81 ); however the
parameters must also satisfy the constraints @xmath and @xmath for all
values of the parameters and we have not ensured these constraints are
compatible with ( 4.81 ). It is sufficient for our purposes to
understand that the oscillatory terms prevent the continuum limit being
taken for certain walk lengths so they are certainly unphysical; we
therefore must scale them so that they disappear in the continuum. We
therefore have as a lower bound on @xmath ,

  -- -------- --
     @xmath   
  -- -------- --

We now consider the upper bound. Without loss of generality we may
arrange that the indexing set @xmath has the property that if @xmath
then @xmath . Furthermore define

  -- -------- -- --------
     @xmath      
     @xmath      (4.85)
     @xmath      
  -- -------- -- --------

where @xmath , i.e. @xmath is the smallest length scale with a spectral
dimension differing from one. Some modified versions of the above
quantities will also be needed,

  -- -------- -- --------
     @xmath      (4.86)
     @xmath      
  -- -------- -- --------

It is clear that @xmath and @xmath which together with Lemma 8 allows us
to conclude that the continuum limit is,

  -- -------- -- --------
     @xmath      
     @xmath      (4.87)
  -- -------- -- --------

where we have defined @xmath and arranged that no oscillatory terms
appear.

We are now in a position to prove Theorem 2 by analysing the behaviour
of ( LABEL:GeneLB ) and ( 4.5 ) for various walk lengths. We first note
that both the upper and lower bounds are controlled by the relative
sizes of the quantities @xmath . In particular the largest of these
quantities, @xmath , will determine the behaviour in a particular scale
range; if we suppose @xmath for some @xmath then the leading
contribution to both the upper and lower bound will be proportional to
@xmath . Using this we find the following behaviour,

-   On very short scales corresponding to @xmath for all @xmath then the
    lower bound of @xmath is dominated by the @xmath behaviour, which
    together with the trivial upper bound means that taking the @xmath
    limit leads to @xmath as in the previous sections.

-   On very long scales corresponding to @xmath for all @xmath then
    there will exist a length scale @xmath such that for @xmath , @xmath
    where @xmath for all @xmath . Explicitly @xmath will be given by,

      -- -------- -- --------
         @xmath      (4.88)
      -- -------- -- --------

    If we take the limit @xmath we obtain @xmath .

We see that the spectral dimension increases monotonically on successive
length scales and the spectral dimension has measured values given by
@xmath therefore proving Theorem 2 . ∎

It is interesting to note that a constraint on the crossover behaviour
exists for generic measures much as it did for the case of the double
power law measure. In particular on intermediate scales there exists
@xmath such that @xmath for @xmath and @xmath for @xmath . Hence @xmath
if @xmath and so @xmath . This means there will exist a length scale
@xmath such that for @xmath , @xmath where we have defined @xmath
implicitly by @xmath for all @xmath . Of course this does not ensure
that @xmath , indeed if the length scales @xmath are not sufficiently
separated this may not be true and we would not have a scale range in
which this term dominated. The expression for @xmath is,

  -- -------- -- --------
     @xmath      (4.89)
  -- -------- -- --------

and so we may choose @xmath independent of @xmath thereby allowing the
scale range over which this behaviour exists to be arbitrarily large.
This would result in an apparent spectral dimension of @xmath .

Finally, an interesting application of the techniques used to prove
Theorem 2 is that they allow one to analyse a wider class of combs than
the class for which the results of [ 3 ] are valid. In particular it was
proven in [ 3 ] that for a random comb the spectral dimension is @xmath
where @xmath and @xmath . This was proved subject to the assumption that
there exists @xmath such that,

  -- -------- -- --------
     @xmath      (4.90)
  -- -------- -- --------

as @xmath goes to zero.

Given the results in this section we see that @xmath may be interpreted
as the abscissa of convergence for the Dirichlet series generating
function. Furthermore, it is clear from our results that there are
distributions where the assumption ( 4.90 ) does not hold and that we
may use the techniques we have developed to analyse these cases.
Recalling that for a random comb we may compute the spectral dimension
using the relation ( 4.18 ) then we must perform a similar analysis to
that done for the continuum comb but now only scaling @xmath to zero.

Due to Landau’s theorem there always exists a pole at the abscissa.
Suppose it is of order @xmath and consider @xmath ,

  -- -------- -- --------
     @xmath      
     @xmath      (4.91)
  -- -------- -- --------

where @xmath runs over the poles on the line of convergence and @xmath
is implicitly defined in the second line. We have not included any poles
of order less than @xmath since these will not be leading order as
@xmath goes to zero and we have assumed the there are no poles of order
greater than @xmath as then the above quantity would go negative due to
the oscillating terms. Applying the same argument as we did for the
continuum comb we obtain the lower bound,

  -- -------- -- --------
     @xmath      (4.92)
  -- -------- -- --------

where @xmath is a constant. To obtain an upper bound we apply a very
similar argument as used for the continuum comb; the only difference
being that we choose,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (4.93)
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- --------

and compute the behaviour as @xmath goes to zero of @xmath , @xmath and
the size of the set @xmath in ( 4.58 ). The result is,

  -- -------- -- --------
     @xmath      (4.94)
  -- -------- -- --------

where @xmath is a constant. We see that we reproduce the result of [ 3 ]
when @xmath and there are no poles on the line of convergence. If @xmath
, the spectral dimension is the same as the @xmath case as only
logarithmic corrections are introduced. If we allow poles to appear on
the line of convergence then @xmath will have an oscillating @xmath
dependence which, if the bounds above are tight enough, would also imply
that the functional form of @xmath is not a power law and so the
spectral dimension does not exist. Whether the above bounds are tight
enough to make this conclusion we leave for future work.

### 4.6 Discussion

In this chapter we demonstrated that there exist models in which a
CDT-like scale dependent spectral dimension could be shown to exist
analytically. That we could do this was important, as all evidence for a
scale dependent spectral dimension in CDT has been numerical in nature
and therefore does not provide any understanding of the mechanism
causing the reduction in dimensionality. We hope the work begun in this
chapter may be extended to shed some light on this question.

We have given in Theorem 2 a reasonably complete classification of what
behaviours one can find on a continuum comb and indeed it is likely that
the cases not covered by the theorem do not have a well defined spectral
dimension. The behaviour for the combs covered by Theorem 2 is fairly
rich as there exists a cross-over region between the @xmath and @xmath
behaviours in which a hierarchy of apparent spectral dimensions exist.
One might expect that if one were able to extend the results of this
chapter to more CDT like models one could find examples in which the
spectral dimension again depends on the scale. Such an expectation is
not unreasonable since the proof of the bijection between
two-dimensional CDT and trees in [ 16 ] shows that any ensemble of trees
is in bijection with a CDT-like theory. It is only when the random tree
has the uniform measure that we obtain precisely CDT on the other side
of the bijection. One could therefore imagine a variation of CDT,
constructed from an ensemble of random trees with a measure that differs
from the uniform measure and with a dependence on a length scale. This
length scale could then be scaled while taking a continuum limit, as we
have done in the work here. Such a model should display a spectral
dimension that differs in the UV and IR.

One may query how closely related a random comb is to an actual model of
quantum gravity. For the case of both CDT and random trees one may show
that the evolution of the spatial volume with time is generated by a
Hamiltonian. A crucial difference when one considers random combs is
that the growth of a comb is not a Markovian process ⁶ ⁶ 6 In fact the
growth process of a comb is Markovian if the tooth length distribution
is of exponential form, however we have seen this does not lead to any
interesting scale dependent behaviour of the spectral dimension. ; the
probability of having a certain number of vertices at height @xmath from
the root may not be computed from the state of the system at height
@xmath , meaning a Hamiltonian formulation of the evolution of a
comb-like universe is not possible. An extension of the work described
in this chapter, using the techniques of [ 15 ] , to the case of random
trees would be very interesting as this would constitute an example of
CDT-like dimensional reduction for a model of quantum gravity which
admits a Hamiltonian formulation. As was briefly mentioned earlier, a
random graph that is even more closely related to the random graphs of
CDT was introduced in [ 16 ] by identifying all verticies belong to the
same spatial slice. This leads to a line like graph with multiple links
between each node. The question of whether the above phenomenon can be
extended to this case would also be interesting to pursue.

## Chapter 5 DT with Matter, String Theory and Branes

In the previous chapters we considered the emergent dimension of
spacetime in models of pure quantum gravity in addition to the disc
function and the string susceptibility. This exhausts the list of
observables that can be studied in such theories. We expect the theory
to become richer upon the addition of matter and it is this topic we
will address in this chapter. We will also discuss the relation of the
metric-is-fundamental approach, including matter, to string theory. In
fact the consideration of the string interpretation of these models will
lead to a large class of two dimensional quantum gravity models known as
@xmath minimal gravity or @xmath minimal string theory.

### 5.1 Adding Matter using Matrix Models

Of the approaches to discrete two dimensional quantum gravity considered
in the preceding chapters the one which most easily admits a
generalisation to models that describe the interaction of matter with
gravity is the matrix model approach ¹ ¹ 1 This does not mean the matrix
model approach allows any matter to be coupled. In general it is
restricted to CFTs arising from spin systems such as the Ising model.
Furthermore, transfer matrix techniques have not been pursued to the
same extent as matrix models in quantum gravity and therefore could yet
yield a description of matter interacting with gravity. . An immediate
consequence of this is that since the standard method used to compute
amplitudes in CDT is the combinatorial method, it has so far not been
possible to analytically investigate the consequences of adding matter
to CDT. There have been a number of attempts to formulate CDT as a
matrix model [ 58 , 32 , 59 ] , however these have so far been too
complicated to solve or resisted the generalisations necessary to
include matter. Hence, for the rest of this thesis we will work within
the DT approach.

We begin by considering a naive method by which one can construct a
matrix model that describes matter interacting with two dimensional
gravity. In particular we know that the Ising model on a fixed lattice
possesses a critical point at which the theory is described by a
continuum quantum field theory. Therefore if we attach a spin to each
triangular face in a triangulation and take a scaling limit in which
both the lattice and the Ising model on it become critical, we expect we
might recover the critical Ising model interacting with two dimensional
gravity. For such a model, we expect the partition function for the
discrete theory to be of the form,

  -- -------- -- -------
     @xmath      (5.1)
  -- -------- -- -------

where @xmath is the coupling between spins and @xmath is the spin on the
@xmath th triangle in the triangulation, which can take on values of
@xmath . For purposes of illustration and to make contact with graph
theory terminology it is usual to refer to the spins as “colours” so
that an assignment of spins to a triangulation becomes a “colouring” of
the graph. For a triangulation @xmath , let @xmath denote the number of
edges shared by triangles differing in colour and let @xmath be the
total number of edges. The action may then be written as,

  -- -------- -- -------
     @xmath      (5.2)
  -- -------- -- -------

which implies that the partition function, assuming the couplings are
small, may be written as,

  -- -------- -- -------
     @xmath      (5.3)
  -- -------- -- -------

where @xmath , @xmath and @xmath is the number of triangulation
containing @xmath triangles and @xmath edges separating triangles of
differing colours. It is this partition function we must realise using a
matrix model.

The key observation is that we can generate labelled graphs (i.e.
containing colours) using a matrix model by considering models which
contain more than one matrix. A matrix model containing more than one
matrix can produce Feynman diagrams containing more than one type of
“particle” species, which can then be considered as a label. As an
example, consider the matrix model defined by,

  -- -------- -- -------
     @xmath      (5.4)
  -- -------- -- -------

It is straightforward to see that this model will produce Feynman
diagrams containing propagators for the @xmath and @xmath matrices,
which will be denoted by grey and white double lines respectively.
Furthermore, due to the first term in the action, the model also
produces a mixed propagator that interpolates between grey and white.
Finally, the cubic interaction terms will produce grey and white
3-vertex interactions. The various propagators and vertices are shown in
fig 5.1(a) together with their Feynman rules. An example of a Feynman
diagram produced by this model is shown in fig 5.1(b) .

To simplify the algebra let the contribution of a colour changing
propagator be @xmath and that of a colour preserving propagator be
@xmath . The value of a Feynman diagram composed of @xmath triangles
with @xmath colour changing propagators and @xmath propagators in total
is,

  -- -------- -- -------
     @xmath      (5.5)
  -- -------- -- -------

where @xmath is the symmetry factor of the diagram. We can now compute
the free energy for the matrix model by summing over all connected
graphs and for each graph summing over @xmath . If we let @xmath and
@xmath then we obtain precisely ( 5.3 ).

#### 5.1.1 The Continuum Limit and Multicritical Points

The matrix model ( 5.4 ) belongs to a general class of two matrix models
of the form,

  -- -------- -- -------
     @xmath      (5.6)
  -- -------- -- -------

These models have been solved [ 24 ] using loop equations yielding an
equation for the resolvent @xmath ;

  -- -------- -- -------
     @xmath      (5.7)
  -- -------- -- -------

where @xmath and @xmath is a polynomial in @xmath and @xmath with
unknown coefficients. The equation is sometimes known as the master
equation. The equation for @xmath can be obtained by letting @xmath in
the master equation. If we specialise to the Ising matrix model
considered above we see that the loop equation resulting from ( 5.7 ) is
a 3rd order polynomial in @xmath . In order to compute the unknown
constants appearing in @xmath we enforce the condition that the master
equation must be a genus zero curve. We may then compute the unknown
constants by requiring that the resulting solution for the resolvent has
the correct behaviour as @xmath , i.e. there is a sheet of the resolvent
in which @xmath . In order to compute the critical point of the matrix
model we again consider the quantity @xmath and look for the value of
@xmath at which @xmath is non-analytic. In this case however we see that
since we have a second variable, @xmath , the matrix model will possess
a critical line in @xmath space rather than having a single critical
point. In the presence of a critical line there is the possibility that
there is a higher order critical point lying on the line. This is in
fact the case for the Ising matrix model.

We may qualitatively understand these critical lines and points by first
letting @xmath ; in this case the Ising spins do not interact and we
have two copies of the one matrix model considered previously. We
therefore expect that any critical point obtained by tuning @xmath so
that we hit the critical line, corresponds to pure gravity. At the
critical point however, @xmath has a very particular value and therefore
we expect it is at this point that we obtain a theory of critical Ising
matter interacting with gravity. Such higher order critical points are
known as multi-critical points.

One may wonder what the two matrix model with general potentials
corresponds to. In this case there will be a number of free parameters
contained in each potential and therefore we have the possibility of a
large number of critical hypersurfaces of decreasing dimension. By
tuning the coupling so that the model lies on some critical hypersurface
we can obtain a variety of continuum models [ 42 , 53 ] . We shall see
that in this way we may obtain a family of different field theories
known as @xmath minimal models interacting with gravity. The Ising model
corresponds in this classification to the @xmath model.

### 5.2 Conformal Field Theory and Minimal Models

One manner in which one could specify a particular quantum field theory
is to give the symmetries of the theory. One can then specify the
dynamical objects in the theory, by which we mean we specify which
representation of the symmetry group they form. These two pieces of
information largely fix the dynamics of the theory. In the usual
formulation of quantum field theory these properties are introduced
using the action and Feynman path integral, which can then be used to
construct a perturbative series that computes correlation functions.
However, in two dimensions the group of conformal transformations is
infinite dimensional. If we have a QFT invariant under conformal
transformations, this allows correlation functions to be computed
without the use of an action or path integral. We will now review this
construction following [ 60 ] .

It is often customary when discussing two dimensional field theories to
use complex coordinates, defined by @xmath and @xmath . A field @xmath
is defined to be primary if under a conformal transformation, @xmath ,
@xmath , it transforms as,

  -- -------- -- -------
     @xmath      (5.8)
  -- -------- -- -------

Fields which do not transform in this way are called secondary, of which
an important example is the stress-energy tensor. The importance of the
stress-energy tensor is that its Fourier coefficients generate conformal
transformations. In terms of complex coordinates the stress energy
tensor has two non-zero components that lie on the diagonal, we denote
them @xmath and @xmath . The Fourier expansion of the stress energy
tensor may be written as @xmath and @xmath . We will refer to @xmath and
@xmath as the generators of the conformal transformations. The algebra
for the generators of conformal transformations is,

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (5.9)
  -- -------- -------- -------- -- -------

and is known as the Virasoro algebra, the real number @xmath is known as
the central charge.

Since we work in a complexified Euclidean space the choice of how to
slice the space into surfaces of constant time is arbitrary. This is
equivalent to the freedom one has in splitting up a statistical
mechanical system into subsystems in order to define a transfer matrix.
It is useful in conformal field theory to take the surfaces of constant
time to be concentric circles centred on the origin. The Hamiltonian for
the system then generates the transformation @xmath , for @xmath . The
origin is taken to be at @xmath and @xmath is at complex infinity. This
setup is known as radial quantisation. Because the states at @xmath are
localised at the origin there is a one-to-one correspondence in
conformal field theories between local operators and states known as the
state-operator correspondence. Hence for every primary field there
exists a state known as a primary state. The primary states are the
highest weight states when constructing the highest weight
representations of the Virasoro algebra. The states obtained from the
primary state by use of the lowering ladder operators, @xmath and @xmath
where @xmath , are referred to as descendant states and they in turn
give rise to fields known as descendant fields. A primary field together
with its descendants is known as a conformal family.

A final important ingredient is the operator product algebra of the
fields. This algebra arises by considering the correlation function of a
product of local operators and studying its behaviour as two of the
operators are brought towards the same point. In general the correlation
function will diverge as the operators are brought together, however we
can characterise this divergence by use of a Laurent like series in
which the coefficients are other local operators. This series is known
as the operator product expansion (OPE). Crucially which operators
appear as coefficients in the OPE is independent of the other operators
appearing in the correlation function, i.e.,

  -- -- -- --------
           (5.10)
  -- -- -- --------

Since the OPE is independent of the other operators appearing in the
correlation function, this relation is usually written as,

  -- -------- -- --------
     @xmath      (5.11)
  -- -------- -- --------

The operator product algebra is defined as the collection of all OPEs of
the fields in the theory. It turns out that the operator product algebra
is entirely fixed by the value of the central charge together with the
3-point correlation functions of the theory.

In general the OPE of two operators will include other local operators
in the expansion besides the two original operators and their
descendants. Generically this will mean that in order to have an
operator product algebra that closes the theory must contain an infinite
number of primary fields. However, for special values for the central
charge there exist finite sets of primary fields for which the operator
algebra closes. These special conformal field theories are known as
minimal models. In particular, given two positive coprime integers
@xmath such that @xmath then the conformal field theory with central
charge

  -- -------- -- --------
     @xmath      (5.12)
  -- -------- -- --------

and whose primary fields have weights given by

  -- -------- -- --------
     @xmath      (5.13)
  -- -------- -- --------

where @xmath and @xmath are integers satisfying, @xmath and @xmath , is
known as the @xmath minimal model.

### 5.3 Boundary States of Minimal Models

To study minimal models in the presence of boundaries it is most useful
to consider the case of the minimal model on a finite length cylinder.
The boundaries of the cylinder carry boundary conditions we call @xmath
and @xmath . Using real coordinates @xmath and @xmath we will analyse
the case when the boundaries of the cylinder are curves of constant
@xmath . A necessary condition that any theory must satisfy at a
boundary is that there is no flow of energy or momentum across the
boundary. In terms of the components this requirement corresponds to
@xmath at the boundary. If we complexify the coordinates by changing to
the coordinates @xmath and @xmath , then the condition @xmath becomes
@xmath . Since the boundary is a curve of constant time, the system at
the boundary should correspond to some state in the Hilbert space. We
will refer to these boundary states as @xmath and @xmath . As was
mentioned previously, calculations in conformal field theory are usually
done in a coordinate system in which the slices on constant time form
concentric circles centred on the origin. We can transform to such a
coordinate system via the map @xmath resulting in the cylinder being
mapped to an annulus. The condition on the boundary states can then be
written as,

  -- -------- -- --------
     @xmath      (5.14)
  -- -------- -- --------

The solutions to this equation are known as the Ishibashi states and
have the explicit form,

  -- -------- -- --------
     @xmath      (5.15)
  -- -------- -- --------

where the sum is over all states in the irreducible representation of
the Virasoro algebra associated to the primary field @xmath . Note that
there is one Ishibashi state for each primary field in the theory. We
therefore appear to have a space, with dimension equal to the number of
primary fields in the theory, of boundary states. However, it turns out
that there exist only a finite number of consistent boundary states,
which are known as Cardy states and are in one-to-one correspondence
with the primary fields in the theory.

### 5.4 String Theory

The central hypothesis of string theory is that fundamental particles
may be modelled as quantum strings ² ² 2 This is unlikely to be the
correct underlying principle but it is how the subject developed
historically. . The space in which the string is embedded is known as
the target space. The simplest action for the string is known as the
Polyakov action and takes the form,

  -- -------- -- --------
     @xmath      (5.16)
  -- -------- -- --------

where @xmath is the map embedding the string in the target space and
@xmath is the metric on the worldsheet, @xmath , which is a
two-dimensional Riemannian manifold of genus @xmath . We have taken both
the worldsheet and target space to be Euclidean as then the
corresponding quantum theory is well defined. Later we will show that
the quantum theory is equivalent to a theory on a flat worldsheet and so
may be Wick rotated back to a Lorentzian theory.

The Polyakov action corresponds to a string whose excitations are purely
bosonic and is therefore known as the bosonic string. The classical
bosonic string has a number of symmetries that are manifest in the
Polyakov action;

Target space Poincare Symmetry: The action is invariant under the
transformation @xmath , where @xmath is a Lorentz transformation.

Worldsheet Diffeomorphisms: The action is invariant under worldsheet
diffeomorphisms, @xmath , where @xmath is a smooth function. Under such
transformations the embedding fields @xmath behave as scalars.

Weyl Symmetry: Finally the action is also invariant under Weyl
rescalings of the metric, @xmath , where @xmath is a smooth function of
@xmath .

To quantise the string we integrate over the embedding fields @xmath and
the worldsheet metric @xmath . We must only count each physically
equivalent configuration once and it is therefore important to decide
which symmetries of the action we consider gauge symmetries. For now we
will consider Weyl transformations and worldsheet diffeomorphism to be
gauge symmetries. This has the consequence that we must divide by the
volume of these symmetry groups in the path integral. The expression for
the partition function is then,

  -- -------- -- --------
     @xmath      (5.17)
  -- -------- -- --------

In order to perform this integral we must define the measures @xmath and
@xmath . One might already note that the partition function for the
string is that of the metric-is-fundamental approach but with the
addition of a set of scalar fields and a lack of cosmological constant.

For now we will keep the measures in the integral as formal objects and
instead work on gauge-fixing the integral. The diffeomorphism gauge
symmetry allows @xmath to be brought to the form @xmath where @xmath
depends only on a finite number of parameters @xmath , known as the
moduli. Furthermore, due to the extra Weyl symmetry in this theory the
metric may be brought to the form @xmath . Inserting the Faddeev-Popov
determinant necessary to achieve this change of variable we obtain,

  -- -------- -- --------
     @xmath      (5.18)
  -- -------- -- --------

where @xmath and @xmath are ghost fields. In order for the integrals
over @xmath and @xmath to cancel the volume of the gauge group we need
the integral,

  -- -------- -- --------
     @xmath      (5.19)
  -- -------- -- --------

to be independent of @xmath and @xmath . The action itself is invariant
under Weyl transformations and diffeomorphisms as these are classical
symmetries. Hence the only potential problems lie in the definition of
the measures @xmath , @xmath and @xmath . If these measures have a
dependence on @xmath or @xmath then we will have a Weyl or
diffeomorphism anomaly, respectively. In fact, it happens that the
measures are invariant under diffeomorphisms but the case of Weyl
transformations is more complicated and it turns out that there is
indeed a Weyl anomaly present. The Weyl anomaly can be most easily seen
by recalling that a field theory is invariant under conformal and hence
Weyl transformations if the trace of the stress energy tensor vanishes
i.e. @xmath .

If one computes the trace of stress energy tensor for the theory defined
by ( 5.19 ) one obtains @xmath , where @xmath is the number of scalar
embedding fields present and @xmath is the Riemann tensor for the
worldsheet. At this point it is easy to see how the critical dimension
of the bosonic string arises; if the number of dimensions @xmath is
twenty-six then the Weyl anomaly is not present and the integral over
@xmath trivially cancels the remaining gauge group volume. This also has
a number of important consequences. Firstly, since the geometry of the
worldsheet is entirely a gauge degree of freedom we need not worry about
which particular class of worldsheet geometries, for example Euclidean
of Lorentzian, we sum over. Furthermore, since the worldsheet metric can
be made flat everywhere, the computation of worldsheet correlation
functions, which are important in the calculation of S-matrix elements,
are reduced to correlation functions in a flat space conformal field
theory.

Having defined the partition function for the string, an ansatz for how
to compute S-matrix elements is required. If one considers a diagram for
the interaction of two strings one can note that, by utilising the
conformal symmetry of the worldsheet theory, the strings propagating
from infinity may be encoded in local worldsheet operators, which we
denote by @xmath . Hence the string interaction diagrams may be reduced
to the computation of correlation functions in the worldsheet theory.
This is shown in fig 5.1 . Furthermore, the genus of the worldsheet
determines the power of the string coupling appearing in the amplitude.
The ansatz is therefore that S-matrix elements for string states may be
computed perturbatively by computing

  -- -------- -- --------
     @xmath      (5.20)
  -- -------- -- --------

where @xmath is the @xmath -point correlation function on a surface of
genus @xmath and @xmath is the string coupling constant. Clearly this
construction fails when the string coupling becomes of order one. In
such cases one would need a non-perturbative definition of string theory
in order to proceed. Unfortunately, such a formulation is currently
incomplete.

Although, a full definition of string theory is still lacking there has
been much progress in understanding some of the crucial ingredients of
such a theory. In particular it was found that Dirichlet boundary
conditions of open strings correspond to the interaction of a string
with a dynamical object which has become known as a D-brane. An
important question in any string background then is what is the spectrum
of D-branes, as this will have consequences for the non-perturbative
physics.

Another aspect in which the above formulation is incomplete is that the
target space was taken to be flat Minkowski space of dimension @xmath .
Obviously there are other spacetimes of great importance that arise as
solutions to general relativity; namely the black hole spacetimes such
as the Schwarschild metric and time dependent solutions, relevant to
cosmology, such as the FRW metric. In particular it has proved difficult
to gain any insight into the Big Bang singularity due to the difficulty
of computing in a time dependent background.

Finally, a point related to the problem of backgrounds is that in the
most straightforward interpretation given in this section, the number of
dimensions in the target space is determined by the Weyl anomaly.
However, the Weyl anomaly is not as stringent as this; it only requires
that the overall central charge of the gauge fixed theory is zero. The
ghost sector will always arise during the gauge fixing process and comes
with central charge of @xmath , we therefore must start with a
worldsheet theory with a central charge of @xmath . There are many ways
one could arrange this besides requiring @xmath non-interacting scalar
fields to live on the worldsheet and indeed, one must modify the string
model in this section as there are clearly not 26 non-compact
dimensions! The most studied way to resolve this problem is to postulate
that some of the target space dimensions are compact; in the case of the
superstring these compact dimensions are usually taken to be a
Calabi-Yau manifold. However there is nothing preventing us from
cancelling the anomaly instead by introducing a conformal field theory
on the world sheet with no geometric interpretation at all; this would
be interpreted as degrees of freedom internal to the string. This will
form the topic of the next section.

#### 5.4.1 Liouville Theory and Non-Critical String Theory

Usually the dimension of the target space is defined to be equal to the
number of worldsheet scalar fields and we can therefore formulate string
theory in a lower number of dimensions by incorporating a CFT with a
non-geometric interpretation into the worldsheet theory. There is a well
studied CFT which has a tuneable central charge satisfying the
constraint @xmath ; it is known as Liouville theory. The action for
Liouville theory takes the form,

  -- -------- -- --------
     @xmath      (5.21)
  -- -------- -- --------

where @xmath and @xmath and @xmath are free parameters. The field @xmath
transforms as @xmath under the Weyl transform @xmath . More generally
under a conformal transformation @xmath , we have @xmath . Finally, the
metric appearing in this expression is defined to be Euclidean. This
theory may be quantised, while preserving the classical conformal
symmetry it possesses, to produce a theory with central charge @xmath
and primary fields of the form @xmath .

If we include the Liouville theory in the worldsheet theory then we are
left with a central charge of @xmath and so we still require some other
sector in order to completely cancel the Weyl anomaly. In the case when
@xmath a natural choice is to include a single scalar field. This is
known as the @xmath string and has been studied quite extensively, see [
52 ] and references therein. However, if we choose @xmath in ( 5.21 )
such that @xmath then instead of scalar fields, the natural candidate
for the matter sector is a @xmath minimal model. The resulting string
theory is known as @xmath minimal string theory. It is these string
theories that will be studied in the remainder of this thesis.

One might wonder, since the @xmath minimal string has no embedding
fields, if we have lost all geometric interpretation of the target
space. However, there is a geometric way of interpreting the @xmath
strings. The @xmath minimal models often arise as the continuum theory
describing a statistical mechanical lattice model [ 53 , 42 ] , such as
the Ising or Potts model, at its critical point. Such lattice models
assign a degree of freedom to each lattice point which can take values
in a finite set @xmath of points. If we interpret this degree of freedom
as an embedding field then these model describe the embedding of the
lattice into @xmath . Hence we can think of the @xmath minimal strings
as being strings embedded in a discrete target space.

We may go further and show that the Liouville sector of the worldsheet
theory also has a geometric interpretation. Let us turn our attention
back to ( 5.18 ) in which we have treated both the Weyl transformations
and worldsheet diffeomorphisms as gauge transformations. Because of this
it is important that there is no anomaly in either of these symmetries.
However, it is possible to construct a different theory if we drop the
interpretation of the Weyl transformations as a gauge symmetry. In this
case there is nothing preventing the appearance of a cosmological
constant term, resulting in,

  -- -------- -- --------
     @xmath      (5.22)
  -- -------- -- --------

In analysing this theory it will be useful to give a concrete definition
of the measures in the integral, which we do by following [ 61 , 62 , 63
] . To define a measure on a space of functions we must introduce a
metric in that space. For the two dimensional spacetime metric @xmath
this metric is unique if we require it to respect diffeomorphism
invariance and contain no dependence on derivatives of @xmath ; a
condition known as ultralocality. The metric then takes the form,

  -- -------- -- --------
     @xmath      (5.23)
  -- -------- -- --------

where @xmath is an arbitrary constant. Since this theory lacks a Weyl
symmetry the diffeomorphism gauge symmetry only allows @xmath to be
brought to the form @xmath . Inserting the Faddeev-Popov determinant for
this transformation we obtain,

  -- -------- -- --------
     @xmath      (5.24)
  -- -------- -- --------

We can integrate over diffeomorphisms @xmath as both the integrand and
the measures are invariant under them. However, as we have seen, the
measure for the ghosts and embedding fields still contains a dependence
on @xmath . Let us consider how this works explicitly. The metric on the
space of embedding functions @xmath is unique if we require it to
respect diffeomorphism invariance and be ultralocal. The metric then
takes the form,

  -- -------- -- --------
     @xmath      (5.25)
  -- -------- -- --------

with a similar expression for the ghosts. As can be seen, both of these
metrics have a dependence on @xmath . Using this definition for the
measures it is possible to show that the @xmath dependence of the
measures is given by,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (5.26)
  -- -------- -------- -------- -- --------

where @xmath is the Liouville action with @xmath . We therefore obtain,

  -- -------- -- --------
     @xmath      (5.27)
  -- -------- -- --------

where the original cosmological constant has been absorbed into the
cosmological constant appearing in the Liouville action. Naively, we
appear to have solved the problem of how to integrate over @xmath ,
however this conclusion is premature. The measure for @xmath is
inherited from the measure on @xmath and therefore depends on @xmath
itself. It has the form,

  -- -------- -- --------
     @xmath      (5.28)
  -- -------- -- --------

The dependence on @xmath makes it difficult to split the measure into a
product of Fourier coefficients which can easily be integrated over. In
order to complete the calculation of @xmath we must relate this @xmath
dependent measure to one that is independent of @xmath . It was
conjectured in [ 64 ] that the effect of changing from this metric to a
flat one, given by,

  -- -------- -- --------
     @xmath      (5.29)
  -- -------- -- --------

would only renormalise the constants appearing in the Liouville action.
This conjecture is borne out by comparison to the matrix model
approaches. We therefore see that although classical gravity is entirely
topological, the measure in the path integral generates dynamics for the
metric. Furthermore, the Liouville action we added to the worldsheet at
the beginning of this section can be understood as describing the
degrees of freedom internal to the worldsheet that correspond to the
scale factor of the metric. The consequence of this on the statistical
lattice model picture of the minimal models is that the Liouville field
describes the configuration of the lattice itself. We therefore see how
the program of DT naturally emerges from the path integral.

#### 5.4.2 Wave-functions and Correlation functions in Liouville Theory

We will now discuss an approximation, known as the mini-superspace ³ ³ 3
The term “superspace” in this context refers to the space of geometries
of the worldsheet rather than a supersymmetric space. approximation that
has proved useful in the study of Liouville theory. This approximation
is obtained by neglecting all oscillations of the string, leaving the
zero modes as the only dynamical quantity. Beginning with the Liouville
action we may obtain the Hamiltonian and by setting the transverse
oscillations to zero we obtain the mini-superspace Hamiltonian,

  -- -------- -- --------
     @xmath      (5.30)
  -- -------- -- --------

where @xmath is the zero mode of @xmath and @xmath is its conjugate
momentum. It will turn out to be useful to study this Hamiltonian using
the Schrodinger representation in which states in the Hilbert space
becomes functionals of the field variable. The energy eigenstates then
satisfy,

  -- -------- -- --------
     @xmath      (5.31)
  -- -------- -- --------

An interpretation of the wave function for the zero mode may be found by
consideration of the state-operator correspondence. Although discussed
previously in Section 5.2 , the state-operator correspondence can be
given a more explicit realisation using path integrals [ 49 ] . Working
in the radial quantisation picture we can associate any local operator
to a wave-functional by the following map,

  -- -------- -- --------
     @xmath      (5.32)
  -- -------- -- --------

where the path integral is taken over the interior of the unit disc and
the field @xmath assumes the value @xmath at the boundary. We therefore
expect the mini-superspace wave functions to be approximations to disc
functions in which the Liouville field has some specified value on the
boundary. In order to compare the mini-superspace wave-functions to
later quantities it is useful to note that in the mini-superspace
approximation the value of @xmath is completely fixed if we specify the
boundary length of the disc. Indeed, the two are related by @xmath and
so we may rewrite ( 5.31 ) to give,

  -- -------- -- --------
     @xmath      (5.33)
  -- -------- -- --------

where we have now dropped the subscript @xmath on the field @xmath . The
solutions to this equation are the probability distribution for the
boundary length of the disc and they have the form,

  -- -------- -- --------
     @xmath      (5.34)
  -- -------- -- --------

where @xmath is a constant. Furthermore, given the state operator
correspondence we expect the parameter @xmath to label which local state
has been inserted in the disc. It is this quantity we will compare with
exact results from Liouville and matrix model calculations.

We now turn to the problem of computing correlation functions in
Liouville theory. As was mentioned in section 5.2 , the only input
necessary to fix the operator algebra of a conformal field theory is the
three point function coefficients. We therefore need only compute the
three point correlation function. It is unnecessary for our purposes to
review the computation of the three point function in detail, however it
will be useful to consider the general approach. Historically, the
Liouville three point function was first computed in [ 19 , 20 ] using
the method of [ 69 ] , however a more efficient method was introduced
later in [ 70 ] . The method of [ 69 ] is based on the observation that
for particular combinations of operators and worldsheet topology the
correlation function may be expressed in terms of free field
correlators. Explicitly, consider the following @xmath -point
correlation function,

  -- -------- -- --------
     @xmath      (5.35)
  -- -------- -- --------

We proceed by splitting the integral into an integral over the zero mode
@xmath and the non-zero modes @xmath ,

  -- -------- -- --------
     @xmath      (5.36)
  -- -------- -- --------

where @xmath is the free field action obtained by setting @xmath in
@xmath and @xmath is the Euler characteristic of the worldsheet. We will
now focus our attention on the zero mode integration,

  -- -------- -- --------
     @xmath      (5.37)
  -- -------- -- --------

where @xmath is known as the KPZ exponent and we have introduced @xmath
. It is clear that this integral is divergent for @xmath and therefore
must be regularised in those cases. The method employed in [ 69 ] is to
compute the zero mode integral for @xmath and then analytically
continue, giving the result,

  -- -------- -- --------
     @xmath      (5.38)
  -- -------- -- --------

where the average is taken using the free action @xmath . We now make
the observation that, if we arrange the operators and topology of the
worldsheet to be such that @xmath , then the remaining correlation
function in ( 5.38 ) may be computed as it is a free field theory
correlator. One problem we must address is the prefactor of @xmath which
is infinite at precisely the points for which the correlator becomes a
free field correlator. In [ 20 ] this property was interpreted as
implying that the three point coefficients have a pole precisely when
the relation @xmath is satisfied. The result ( 5.38 ) therefore gives
the residue at these poles. Knowing the residue of the three point
function at these poles allowed [ 20 ] and [ 19 ] to guess the correct
analytic continuation of these results to all @xmath .

The above regularisation of the integral in the case @xmath was achieved
via analytic continuation. A more brutal regularisation would be to
simply cut off the @xmath integral. Unusually, the divergence of the
zero mode has both a UV and IR interpretation. With respect to the
background metric @xmath the divergence of the zero mode integral looks
like an infrared divergence which should be solved by placing the system
in a finite sized box. Alternatively, with respect to the original
metric @xmath , the divergence of the integral due to contributions from
@xmath corresponds to a divergence due to the contributions from
worldsheets in which all distances shrink to zero, which is a UV
problem. We therefore may think of placing a cutoff at @xmath as being
either a short or long distance cutoff. If we implement the cutoff in (
5.37 ), we obtain,

  -- -------- -- --------
     @xmath      (5.39)
  -- -------- -- --------

where @xmath is a cutoff dependent polynomial in @xmath . The property
that the cutoff dependent part of the above integral is polynomial in
@xmath prompted [ 71 ] to classify contributions to correlation
functions that are analytic in @xmath to be non-universal. Another way
to think of this is that since these contribution arise from zero area
worldsheets, then if they were to arise in the matrix model formulation
of the theory such contributions would feel the effects of the lattice
and hence be non-universal. Due to the exponential interaction term in
the Liouville action the theory does not probe far into the region
@xmath . Hence, in the presence of the cutoff, the volume of the
Liouville direction is @xmath .

Recall that in the presence of boundaries we could compute amplitudes
with either the boundary cosmological constant or the boundary length
fixed, with these amplitudes related to one another via the Laplace
transform ( 2.31 ). One use of the fixed boundary length amplitudes is
that one expects worldsheets of zero extent to not contribute since a
surface can not disappear if the boundary length is non-zero ⁴ ⁴ 4 We
make a distinction between vanishing worldsheets and one with zero area.
Branched polymers, as generated by the Gaussian matrix model, represent
an example of worldsheets with zero area but non-zero boundary length
and therefore have universal meaning. See [ 25 ] for further discussion.
.

Contributions to the amplitude from degenerate worldsheets could only
reappear precisely when the boundary lengths go to zero. We therefore
have two ways to determine which terms are non-universal; they are
analytic in @xmath and also they should only have support when all
boundary lengths are zero.

#### 5.4.3 Minimal String Theory

Minimal string theories are obtained by interpreting Liouville theory
coupled to a @xmath minimal model as the worldsheet theory for a string.
The requirement that the total central charge of the system is zero,
together with the expression for the central charge for the minimal
models, Liouville theory and ghosts leads to @xmath . The physical
operators of the @xmath minimal string were first given by [ 28 ] as
@xmath where,

  -- -------- -- --------
     @xmath      (5.40)
  -- -------- -- --------

and @xmath is an operator containing contributions from the ghost and
matter sector in addition to derivatives of the Liouville field. An
important physical operator is obtained by gravitationally dressing the
primary fields @xmath of the minimal model; such operators are referred
to as tachyons and take the form,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (5.41)
     @xmath   @xmath   @xmath      (5.42)
  -- -------- -------- -------- -- --------

where @xmath and @xmath are ghost fields. The expression for @xmath
arises from the requirement that the conformal dimension of the total
tachyon operator is one as this allows for the integral of the operator
over the worldsheet to be conformally invariant.

#### 5.4.4 FZZT Branes and the Seiberg-Shih Relation

Let us return briefly to consider the standard critical string. It is
well known that string theory contains other degrees of freedom which do
not appear in the perturbative string expansion. The most well known of
these are D-branes. Such objects correspond to extended membranes
thereby generalising the idea of a string. The D-branes are particularly
interesting as they do admit a description in terms of perturbative
string theory; although they do not appear in the closed string
expansion they can be described by open strings in which the boundaries
carry Dirichlet boundary conditions. If @xmath of the string embedding
coordinates carry Dirichlet boundary conditions then this means the end
of the string is confined to move in a @xmath dimensional hyperplane,
thereby giving rise to the membrane like structure of the D-brane, which
in this case would be called an @xmath -brane. Since D-branes are
described by worldsheets with boundaries the worldsheet theory is known
as a boundary conformal field theory.

Given this motivation it is natural to enquire whether the @xmath
minimal strings contain any branes. The answer to such a question would
involve classifying the possible conformally invariant boundary
conditions in the worldsheet CFT. A review of the result of such a
program for the @xmath minimal model not coupled to gravity was given in
Section 5.3 . We now consider a similar question for Liouville theory
and hence the @xmath minimal string. The Liouville theory on a manifold
with boundary is defined by the Lagrangian,

  -- -------- -- --------
     @xmath      (5.43)
  -- -------- -- --------

where @xmath is known as the boundary cosmological constant. The full
conformal bootstrap of Liouville theory on the disc was performed in [
18 ] [ 21 ] , resulting in the identification of a consistent boundary
condition for the Liouville theory; the FZZT brane. As part of the
conformal bootstrap, the bulk one-point function on a disc, i.e. the
bulk one-point function with FZZT boundary, was calculated:

  -- -------- -- --------
     @xmath      (5.44)
  -- -------- -- --------

where @xmath is defined implicitly by, @xmath and @xmath is the primary
state associated to @xmath .

In the minimal string the full FZZT brane is @xmath , where @xmath is
the @xmath Cardy boundary state of the minimal model. We will find it
convenient to follow [ 17 ] in rescaling the bulk and boundary
cosmological constant so that @xmath and @xmath . This modifies the
relation between @xmath and @xmath to @xmath .

It was noted in [ 19 ] [ 20 ] that the results of the conformal
bootstrap on the sphere are invariant under the duality transformation
@xmath , @xmath . This self duality is present in the bootstrap on the
disc if the boundary cosmological constant transforms as, @xmath , where
@xmath . The transformed boundary condition is referred to as the dual
FZZT brane and provides a physically equivalent description of the FZZT
brane. Given that all computed amplitudes exhibit this duality it is in
our opinion important for this symmetry to be maintained in any
modifications of amplitudes, such as those we consider later.

The amplitude in the minimal string for the insertion of a tachyon in
the bulk of a disc is obtained using ( 5.44 ) [ 17 ] ,

  -- -------- -- -------- -------- --------
     @xmath      @xmath            (5.45)
                          @xmath   
  -- -------- -- -------- -------- --------

where @xmath , @xmath , @xmath is a constant independent of @xmath ,
@xmath , @xmath and @xmath and @xmath is the @xmath th Chebyshev
polynomial of the second kind. This expression for the disc amplitudes
motivated Seiberg and Shih to conjecture in [ 17 ] that the construction
just outlined over counts the number of FZZT branes and that, up to BRST
null states, the following identification holds,

  -- -------- -- --------
     @xmath      (5.46)
  -- -------- -- --------

This relation, which we will refer to as the SS (for Seiberg-Shih)
relation, was originally obtained by inspection of ( 5.45 ) but was
later derived for discs using the ground ring in [ 23 ] . Essentially it
states that there is only a single FZZT brane @xmath and that all the
others are related to it by complex shifts in the boundary cosmological
constant which has somehow absorbed all the information about the matter
boundary condition.

It is worth pausing at this point to compare the above exact result for
the disc function amplitude in the presence of FZZT boundary conditions,
with the result obtained via the mini-superspace approximation. To
compare the above result ( 5.45 ) to ( 5.34 ), we must take the inverse
Laplace transform. This may be achieved by use of the identity,

  -- -------- -- --------
     @xmath      (5.47)
  -- -------- -- --------

Setting @xmath , we find the length distribution for the @xmath point
tachyon disc amplitude to be,

  -- -------- -- --------
     @xmath      (5.48)
  -- -------- -- --------

hence identifying @xmath , where @xmath is the integer defining the
operator in ( 5.40 ). We see that the exact calculation of the Liouville
disc function reproduces the mini-superspace result up to rescalings of
@xmath and the normalisation constant. This has in fact been observed to
be a generic feature of the mini-superspace approximation in this
setting; it will give the correct functional form for any amplitude.
This implies that for general disc amplitudes we have,

  -- -------- -- --------
     @xmath      (5.49)
  -- -------- -- --------

This is consistent with the application of ( 5.44 ) with @xmath .

#### 5.4.5 Cylinder Amplitudes and the Seiberg-Shih Relation

The minimal string cylinder amplitude was first computed in [ 35 ] for
the case of @xmath boundary conditions. The cylinder amplitude with
arbitrary boundary conditions was given in [ 36 ] and we have reproduced
the derivation in Appendix B . A compact expression for the general
cylinder amplitude is,

  -- -------- -- --------
     @xmath      (5.50)
  -- -------- -- --------

where @xmath is an IR cutoff and,

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
  -- -------- -- -------- --

where @xmath and @xmath . It is the purpose of this section to
investigate whether the SS relation is satisfied when applied to
cylinder amplitudes. For this purpose it is useful to define the
deviation from the SS relations for a particular amplitude @xmath as

  -- -------- -- --------
     @xmath      (5.52)
  -- -------- -- --------

where the presence of other boundaries and operator insertions is
denoted by @xmath . Furthermore, it will be useful to have at hand some
explicit formulae for amplitudes resulting from the application of (
5.50 ). For the @xmath minimal string the full set of cylinder
amplitudes are ⁵ ⁵ 5 These expressions are correct up to an unimportant
common normalisation constant and additions of numerical constants. ,

  -- -------- -- --------
     @xmath      (5.53)
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --------

where we have introduced the following notation,

  -- -------- -- --------
     @xmath      (5.54)
  -- -------- -- --------

The first check of the SS relation was performed in [ 22 ] in which it
was applied to cylinder amplitudes in which all boundaries carried
boundary states of the form @xmath . It was found that the SS relations
are not satisfied identically in this case, however it was argued that
the deviation was non-universal and therefore should be discarded. The
evidence for non-universality was two-fold, corresponding to the two
properties of non-universal terms introduced in Section 5.4.2 . Firstly
it was noted that the deviation possessed the following properties;

-   The deviation could be made independent of the bulk cosmological
    constant, and hence analytic in @xmath , by setting the IR cutoff
    @xmath in ( 5.50 ) to the volume of the Liouville direction.

As an example consider the second of the @xmath minimal string cylinder
amplitudes above,

  -- -------- -- --------
     @xmath      (5.55)
  -- -------- -- --------

Choosing @xmath equal to the volume of the Liouville direction, @xmath
where @xmath is a constant and recalling that @xmath we find,

  -- -------- -- --------
     @xmath      (5.56)
  -- -------- -- --------

Since this is independent of @xmath it was interpreted as non-universal
and therefore set to zero. For a demonstration that all cylinder
amplitudes with boundary conditions given by the states @xmath have this
property the reader is referred to the appendix of [ 22 ]

-   The inverse Laplace transform, with respect to all boundary
    cosmological constants, of the deviation minus any regularisation
    dependent parts is zero almost everywhere i.e. it is supported only
    at points.

This property was uncovered by considering ( 5.50 ). Using ( 5.47 ) in (
5.50 ) we find,

  -- -------- -- --------
     @xmath      (5.57)
  -- -------- -- --------

where @xmath . By noting that the integrand is symmetric, the integral
may be computed by closing the integral along the entire real axis by a
semicircular contour. Since the Seiberg-Shih deviation is merely a
linear combination of amplitudes its double inverse Laplace transform
with respect to each boundary cosmological constant also exists and is
given by

  -- -------- -- --------
     @xmath      (5.58)
  -- -------- -- --------

where

  -- -------- -- --------
     @xmath      (5.59)
  -- -------- -- --------

and we have assumed that the Seiberg-Shih transformation has been
applied to the @xmath boundary. An important property of @xmath is that
it is an entire function and therefore the only contributions to the
integral comes from the poles at @xmath where @xmath . In the case that
@xmath and @xmath the residues of these poles are zero and the above
expression is zero.

Although contrary to the original motivation of P2, we have allowed for
the possibility of the amplitude being non-zero at points away from
zero. We merely require the weaker condition of having point-like
support.

We now want to review the results of [ 36 ] in which all boundary
conditions on the cylinder amplitudes were considered and the
consequences of this on the SS relations. Before proceeding it is
important to note that the sum in the Seiberg-Shih relation does not
respect the symmetry of the Kac table and so for a given amplitude there
are two possible Seiberg-Shih relations that might possess P1 or P2.
However, there are cases where both possibilities lead to deviations
that do not possess either property P1 or P2.

For an example of a deviation that does not possess property P1
consider,

  -- -------- -- --------
     @xmath      (5.60)
  -- -------- -- --------

or, since @xmath ,

  -- -------- -- --------
     @xmath      (5.61)
  -- -------- -- --------

where we have suppressed the @xmath dependent term. It is clear that
since the argument of both expressions is not a homogeneous polynomial
in @xmath the dependence on @xmath cannot be factored out and placed in
a separate term. Hence the IR cutoff cannot be chosen to cancel all
dependency on the bulk cosmological constant.

An example of an amplitude not possessing P2 need only have the property
that the residues are non-zero ( 5.58 ), leading to a function with
global support. As an example consider,

  -- -------- -- --------
     @xmath      (5.62)
  -- -------- -- --------

or, since @xmath ,

  -- -------- -- --------
     @xmath      (5.63)
  -- -------- -- --------

By applying ( 5.58 ) it is now easy to show P1 and P2 are not satisfied
for ( 5.62 ) and ( 5.63 ); in both cases there exist poles when @xmath
and therefore when the integral contour is closed around them it will
result in a sum of terms containing Bessel functions of the form @xmath
which have global support in @xmath .

Having shown that the cylinder amplitudes possess deviations from the SS
relations which do not fit the non-universal classification advanced in
[ 22 ] we now wish to motivate a possible extension of the criteria by
which non-universality is judged in the hope of rescuing the SS
relations. The observation, is that, the terms that disappear in ( 5.50
) under an inverse Laplace transform are dual under the Liouville
duality to the terms that produce the troublesome poles in the above
test of P2 for arbitrary amplitudes. We therefore propose that
non-universal deviations should possess,

-   The deviation may be written in terms of contributions for which
    either the inverse Laplace transform with respect to all boundary
    cosmological constants @xmath or all their duals, @xmath , has
    point-like support.

We devote the remainder of this section to a more careful motivation of
P3. Consider the integral representation of the amplitude ( 5.50 ); it
may be computed by extending the region of integration to the entire
real line and then splitting the integrand up into terms for which the
contour may be closed in either the upper or lower half plane. Assuming
that @xmath , then upon substituting in ( B.16 ) and ( B.17 ) this
results in the expression,

  -- -------- -- --------
     @xmath      (5.64)
     @xmath      
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --------

The above expression may be understood as representing a cylinder
amplitude as a sum over disc amplitudes with various local or boundary
operators inserted. If we compute the deviation using this expression
the first sum cancels while the others remain with the replacement
@xmath . The sum in which each term is proportional to @xmath arises
from the poles in ( 5.50 ) due to the factor of @xmath in the
denominator; it is precisely these terms which vanish when we take the
double inverse Laplace transform with respect to the boundary
cosmological constant. These terms have the interpretation of some form
of descendant dual boundary length operator inserted on the boundary of
a disc. On the other hand, the sum in which each term is proportional to
@xmath arises from the poles in ( 5.50 ) due to the factor of @xmath in
the denominator; it is these terms which, under a double inverse Laplace
transform with respect to the boundary cosmological constant, become the
terms proportional to Bessel functions of the form @xmath . Similarly,
if we were instead to take the double inverse Laplace transform with
respect to the dual boundary cosmological constant, the terms
proportional to @xmath would vanish and the terms proportional to @xmath
would become terms proportional to @xmath . We therefore see that the
deviation computed from ( 5.64 ) can be understood as a sum of terms
which vanish under a double inverse Laplace transform with respect to
the boundary cosmological constant or its dual.

## Chapter 6 FZZT Branes in the @xmath Minimal String at Higher Genus

### 6.1 Testing the Seiberg-Shih relations using a Matrix Model

In the last chapter we argued that the deviations for all cylinder
amplitudes have property P3. Obviously it would be interesting to know
whether this is true for deviations of any amplitude. Unfortunately
amplitudes more complicated than the disc and cylinder have never been
computed using the Liouville approach. An alternative is to use the
matrix model formulation of minimal string theory in which the
computation of amplitudes with arbitrary numbers of boundaries and
handles is straightforward.

The disadvantage of using the matrix model is that it gives the
amplitude in a fully integrated and summed form and so it is hard to
understand the structure of the amplitude in terms of continuum concepts
such as states circulating in a loop. Furthermore, generally it is
obscure how the graph labels map to the conformal field theory degrees
of freedom and even more importantly, the matrix model appears to not
contain all the boundary states present in the minimal string. Indeed,
this was used as evidence in [ 17 ] in support of the Seiberg-Shih
relation. However, there are special cases where the relation is
manifest; we saw that the @xmath minimal string admits a formulation as
a matrix model ( 5.4 ) in which the matrices are directly related to the
spin degrees of freedom. In this model it is straight forward to
construct all conformal boundary states. This is the model we will study
in this chapter. In recent work of [ 37 , 38 ] a number of other
boundary states of the @xmath minimal string have been constructed in
the matrix model formulation and it would be interesting to extend our
results to those cases.

In this chapter we will find it useful to change the definition of the
coupling constants from those considered in ( 5.4 ). Instead we will use
the matrix model defined by,

  -- -------- -- -------
     @xmath      (6.1)
  -- -------- -- -------

The coupling between neighbouring spins is controlled by the parameter
@xmath whereas @xmath controls the cost associated with adding more
vertices to the graph. It is @xmath and @xmath we use to tune the matrix
model to its critical point which for our matrix model we do by setting,

  -- -------- -- -------
     @xmath      (6.2)
  -- -------- -- -------

where @xmath is some constant which can be determined by comparing with
the Liouville theory and the critical point is achieved by letting
@xmath ¹ ¹ 1 The constants @xmath and @xmath have the well known values
@xmath and @xmath . . We will often refer to this limit as the scaling
or continuum limit.

The conformally invariant boundary conditions of minimal CFTs are in
one-to-one correspondence with the primary fields of the theory. For a
CFT that describes the critical point of some discrete model the
boundary conditions can sometimes be understood as universality classes
of boundary conditions in the discrete model. As we approach the
critical point of a discrete model all boundary conditions in a given
class will flow to the same boundary condition in the CFT. In particular
for the @xmath minimal string its three conformal boundary conditions
are the continuum limit of the discrete configurations of boundary spins
consisting of all spin @xmath , all spin @xmath , or free spins. It is
these boundary conditions we want to implement in the matrix model.

The usual way of inserting a boundary in the matrix model is to compute
the resolvent @xmath as it is a generating function for triangulations
of discs. We will adopt the following notation for more general
quantities,

  -- -------- -- -------- --
     @xmath               
                          
                 @xmath   
                 @xmath   
  -- -------- -- -------- --

By tuning the @xmath as we take the matrix model to its critical point
we can extract continuum quantities corresponding to amplitudes with
macroscopic boundaries. By way of example, for @xmath we use ( 6.2 ) and
set,

  -- -------- -- -------
     @xmath      (6.4)
  -- -------- -- -------

where @xmath is chosen to produce a non-trivial limit and @xmath is
chosen to agree with Liouville theory. As we let @xmath , @xmath will
have an expansion in powers of @xmath ² ² 2 When taking the scaling
limit in the remainder of the chapter we will choose @xmath and @xmath .
It should be kept in mind though that by changing @xmath and @xmath we
can renormalise @xmath and @xmath . ,

  -- -------- -- -------
     @xmath      (6.5)
  -- -------- -- -------

In this expression @xmath is analytic in both arguments whereas @xmath
is defined as being the first term non-analytic in @xmath and @xmath .
It is @xmath that corresponds to the continuum quantity and so in this
case is the partition function of the continuum theory defined on a
surface with a finite sized boundary with the boundary condition
determined by which universality class the discrete quantities @xmath
belong to. This is slightly complicated by the fact that the resolvent
actually corresponds to a boundary with a marked point and therefore
must be integrated with respect to the boundary cosmological constant
before comparing to Liouville theory. This motivates introducing the
integrated quantity,

  -- -------- -- -------
     @xmath      (6.6)
  -- -------- -- -------

where we have allowed for more general amplitudes which have more than
one boundary. The generalisation to amplitudes of the form ( 6.1 ) is
obvious.

For the matrix model ( 6.1 ) it is easy to construct resolvents which
generate boundary conditions that flow to the three different boundary
conditions in the CFT:

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      
     @xmath   @xmath   @xmath      (6.7)
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- -------

The first two resolvents @xmath generate surfaces on which only @xmath
vertices appear on the boundary. Since the @xmath vertices map directly
to the spin degrees of freedom these flow to the fixed spin boundary
conditions in the continuum limit. The final resolvent generates
boundaries in which both types of vertices appear with equal weighting.
This will therefore flow to the free spin boundary condition. We can
parameterise these resolvents in the following way,

  -- -- -- -------
           (6.8)
  -- -- -- -------

This generates graphs with weighting corresponding to the Ising model on
a random lattice but with a boundary magnetic field, to which the
parameter @xmath is related. Clearly, we can choose @xmath to reproduce
the resolvents @xmath . By taking @xmath to infinity we may also obtain
the resolvent @xmath . One method to compute the resolvent @xmath is to
make the following change of variable in ( 6.1 ),

  -- -------- -- -------
     @xmath      (6.9)
  -- -------- -- -------

The partition function in these new variables takes the form,

  -- -------- -- --------
     @xmath      (6.10)
  -- -------- -- --------

where,

  -- -------- -- --------
     @xmath      (6.11)
  -- -------- -- --------

Generic values of @xmath (i.e. the Ising model on a random lattice in
the presence of a boundary magnetic field) were investigated using
matrix model techniques in [ 31 ] , in which the disc amplitudes were
calculated. The method employed in [ 31 ] to solve the matrix model is
combinatorial and so is not easily generalised to compute more
complicated amplitudes and so we will not employ it here. However, it is
clear that if we are only interested in amplitudes in which all the
boundary conditions are of the free spin form we may set @xmath in (
6.10 ) to obtain the @xmath model whose solution is well known.
Similarly if we are only interested in amplitudes in which all the
boundary conditions are of the fixed spin form we may simply use ( 6.1
). This is in fact the case for most of the results we require for the
remainder of this section and so they can be obtained form the
literature [ 24 ] [ 29 , 30 ] . For more complicated amplitudes
involving both fixed and free spin boundary conditions it is necessary
to solve the matrix model ( 6.10 ); this may be done using loop
equations and we give details of this in the next section.

#### 6.1.1 Loop equations for SX Matrix Model

The resolvents for the matrix model ( 6.10 ) can be obtained by
generating an appropriate set of loop equations. We begin by obtaining
the resolvents @xmath and @xmath . Consider the following change of
variables in the matrix model,

  -- -------- -- --------
     @xmath      (6.12)
  -- -------- -- --------

this gives the loop equation,

  -- -------- -- --------
     @xmath      (6.13)
  -- -------- -- --------

Now consider,

  -- -------- -------- -- -- --------
     @xmath   @xmath         (6.14)
     @xmath   @xmath         (6.15)
     @xmath   @xmath         (6.16)
  -- -------- -------- -- -- --------

Transformation ( 6.14 ) together with ( 6.15 ) and ( 6.13 ) gives,

  -- -------- -- --------
     @xmath      (6.17)
  -- -------- -- --------

where we have introduced the following functions,

  -- -------- -- --------
     @xmath      
     @xmath      (6.18)
     @xmath      
  -- -------- -- --------

where @xmath and @xmath .

Finally, the third transformation ( 6.15 ) gives,

  -- -------- -- -------- -- --------
     @xmath      @xmath      (6.19)
     @xmath      @xmath      
  -- -------- -- -------- -- --------

Eliminating @xmath between this and ( 6.17 ) gives,

  -- -------- -- --------
     @xmath      
     @xmath      (6.20)
     @xmath      
  -- -------- -- --------

By substituting in the large @xmath expansion for all resolvents
appearing in this expression we can obtain a recursive definition for
the genus @xmath resolvents. The genus zero resolvents satisfy:

  -- -------- -- -------- -- --------
     @xmath                  (6.21)
     @xmath      @xmath      
  -- -------- -- -------- -- --------

This is an important equation and all subsequent loop equations will be
related to this through derivative-like operations such as the loop
insertion operator. It is important to note that the LHS is polynomial
in @xmath and so if we expand the RHS in @xmath about any point, the
resulting Laurent expansion must have coefficients that are equal to a
polynomial function of @xmath . A convenient point to choose for this
expansion is infinity as the definition of the resolvent then coincides
with the Laurent expansion, whose coefficients are the yet to be
determined quantities, @xmath . Thus we produce a number of equations
containing @xmath ,

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
  -- -------- -- -------- --

where @xmath is a contour around infinity, @xmath is a polynomial in
@xmath with a finite number of unknown coefficients, @xmath is an
integer and the integral merely picks out the @xmath th coefficient of
the expansion. From the large @xmath behaviour of @xmath it is easy to
show that for @xmath the LHS is zero and for @xmath the LHS yields an
expression with no dependence on @xmath . The non-trivial cases occur
for @xmath and @xmath which generate enough equations to solve for
@xmath . The large @xmath expansion of equations obtained for @xmath
give relations among the quantities @xmath . The explicit equation
obtained for @xmath is,

  -- -------- -------- -------- --
     @xmath   @xmath   @xmath   
              @xmath   @xmath   
              @xmath   @xmath   
  -- -------- -------- -------- --

where,

  -- -------- -- --------
     @xmath      (6.24)
  -- -------- -- --------

The unknown constants in @xmath are not all independent and they may be
found in terms of the constants @xmath by expanding the RHS of ( 6.1.1 )
about @xmath and equating @xmath to the polynomial part of the Laurent
expansion. The requirement that the singular part of the expansion
vanishes again gives relations between the quantities @xmath .

To find @xmath , we again consider ( 6.21 ), however we now know @xmath
in terms of a finite number of unknown constants. On the LHS we have the
function @xmath where @xmath is a polynomial in @xmath and @xmath as
defined in ( 6.18 ), whose highest power for both @xmath and @xmath is
@xmath and so it contains @xmath unknown functions of the form @xmath
for @xmath .

We can generate a system of equations for these resolvents by again
expanding both sides of the loop equation about infinity but this time
in terms of @xmath . This results in,

  -- -------- -- --------
     @xmath      (6.25)
     @xmath      
  -- -------- -- --------

Again we may vary @xmath to generate different equations, however we now
generate non-trivial equations for all @xmath , which is enough to solve
for all unknown resolvent functions appearing in @xmath . However,
unlike @xmath for which we obtained an equation for general @xmath , we
were unable to do this for the equations for @xmath . We also note that
for the equation giving @xmath , the highest power of @xmath appearing
depends on the order of @xmath .

The approach thus outlined gives an algorithmic solution to this matrix
model with an arbitrary potential and results in algebraic equations for
all resolvents.

We now apply the above procedure to the case when the potential is (
6.11 ). There are two unknown constants in @xmath and @xmath
corresponding to @xmath and @xmath . Their values are determined by the
requirement that ( 6.1.1 ) is a genus zero curve - which, in this case,
is equivalent to the one-cut assumption.

For @xmath we can generate two equations from ( 6.25 ), for @xmath and
@xmath , containing @xmath and @xmath . The equation for @xmath
resulting from the elimination of @xmath has a number of interesting
properties. Generically it is quartic in @xmath , however it reduces to
a cubic for @xmath and it reduces to a quadratic of @xmath for @xmath .
Because of this the scaling limit for generic @xmath does not hold for
these particular values of @xmath and must be taken separately.

As was discussed before, the resolvents corresponding to the spin @xmath
and @xmath Cardy states are @xmath and @xmath respectively, which may be
computed directly from ( 6.1 ) and are clearly identical. However a
faster method, since we already have @xmath , is to set @xmath implying
@xmath .

It is easy to generate loop equations for a general 2-loop amplitudes of
the form @xmath and @xmath where @xmath is a string of @xmath and @xmath
matrices of length @xmath . We can write @xmath as, @xmath , where
@xmath is a matrix defined by,

  -- -------- -- --------
     @xmath      (6.26)
  -- -------- -- --------

and @xmath and @xmath are disjoint indexing sets. For such a product,
the notation @xmath is defined as ,

  -- -------- -- --------
     @xmath      (6.27)
  -- -------- -- --------

The loop equations for @xmath , can be generated by the following
changes of variables.

  -- -------- -- -------- -- --------
     @xmath      @xmath      (6.28)
     @xmath      @xmath      (6.29)
     @xmath      @xmath      (6.30)
     @xmath      @xmath      (6.31)
  -- -------- -- -------- -- --------

The loop equations generated by @xmath , may by obtained from the loop
equations generated by @xmath by the following procedure,

  -- -------- -- -------- -- --------
     @xmath      @xmath      
     @xmath      @xmath      
     @xmath      @xmath      (6.32)
  -- -------- -- -------- -- --------

where @xmath is any function of @xmath and @xmath . An equivalent set of
rules applies for loops equations generated by @xmath . The resulting
loop equations can then be solved by using the same method as used to
solve the one-loop amplitude equations. For example, the two loop
amplitude @xmath may be obtained by choosing @xmath in ( 6.28 ), ( 6.29
), ( 6.30 ) and ( 6.31 ). Applying the rules in ( 6.1.1 ) we get,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.33)
                                @xmath   
  -- -------- -------- -------- -------- --------

and

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (6.34)
                                @xmath   
  -- -------- -------- -------- -------- --------

By eliminating @xmath we obtain an equation relating @xmath , @xmath and
@xmath . If we arrange the resulting equation so that the LHS is
polynomial in @xmath , we may expand the RHS in @xmath about @xmath to
generate a number of equations for @xmath and @xmath . This is exactly
the same procedure we used to compute the disc amplitudes in which the
resulting expression contained a number of unknown constants, which for
the equal potential case were @xmath and @xmath . However here we get an
expression for @xmath in terms of unknown functions of the form @xmath
and @xmath . These unknown functions may be found by computing the
expansion of @xmath at @xmath and, by the symmetry of @xmath , equating
it to the expansion at @xmath . The expressions for @xmath and @xmath
then depend on a number of unknown constants of the form, @xmath , not
all of which are independent, as can be seen by considering the large
@xmath expansion of the @xmath and @xmath . In the end the independent
unknown quantities correspond to @xmath and @xmath which can be fixed by
requiring that @xmath has no singularities besides those at the branch
points of @xmath . The loop equations then give @xmath in terms of
@xmath .

In order to compute the @xmath corrections it is clear from ( 6.20 )
that @xmath , @xmath and @xmath need to be calculated. The calculation
of @xmath and @xmath , follows a similar procedure however the resulting
loop equations contain new amplitudes of the form @xmath . The
calculation of these new quantities does not pose a great difficulty;
appropriate changes of variables give loop equations very similar to the
ones presented previously. We include the necessary change of variables
in Appendix C .

Once these quantities have been calculated and substituted in ( 6.20 ),
we may again expand both sides around @xmath and then @xmath to find an
expression for @xmath and @xmath . Again this procedure introduces a new
unknown constant corresponding to the @xmath correction to @xmath which
may be determined by requiring that @xmath does not possess any poles
besides those at the branch points of @xmath .

#### 6.1.2 Disc, Cylinders and Disc-with-Handles.

To take the scaling limit of @xmath at @xmath we may use the results of
[ 24 ] or the results in the last section. The critical value of @xmath
is @xmath and the solution to the loop equation gives, in the scaling
limit,

  -- -------- -- --------
     @xmath      (6.35)
  -- -------- -- --------

For @xmath we may use [ 29 , 30 ] or solve ( 6.1.1 ). The critical value
of @xmath is at @xmath and computing the scaling limit of @xmath we get,

  -- -------- -- --------
     @xmath      (6.36)
  -- -------- -- --------

Here we run into one final technicality. For a resolvent that produces
graphs with some configuration of spins on the boundary @xmath , there
exist an entire family of resolvents which produce the same
configuration of boundary spins but with different overall weighting

  -- -------- -- --------
     @xmath      (6.37)
  -- -------- -- --------

where @xmath is a constant. Changing @xmath has the affect of
renormalising @xmath , however we want the same normalisation for all
continuum quantities and so this fixes @xmath to be @xmath for @xmath .
Therefore using ( 6.36 ) and ( 6.35 ), we have that

  -- -------- -- --------
     @xmath      (6.38)
  -- -------- -- --------

This is precisely the relation between these two disc amplitudes found
in the continuum calculation.

One may wonder what the scaling limit of @xmath is for generic values of
@xmath ; given that there are only a finite number of Cardy states the
expectation is that there should be no new non-trivial scaling limits.
For generic @xmath the critical values of @xmath occur at @xmath and
@xmath . Taking the scaling limit, for both critical values of @xmath ,
gives

  -- -------- -- --------
     @xmath      (6.39)
  -- -------- -- --------

which confirms our expectation. We have therefore demonstrated that we
can build all the boundary states of the @xmath minimal string. This
will be confirmed further by reproducing the cylinder amplitudes
shortly. The nature of the scaling limit when @xmath will be addressed
in the Section 6.1.3 .

The one case in which we are interested which cannot be found in the
literature is @xmath . The computation, using the formalism of the
preceding section, results in

  -- -------- -- --------
     @xmath      (6.40)
  -- -------- -- --------

for the scaling form of @xmath , which is in exact agreement with the
Liouville result (up to renormalisations of @xmath and @xmath ). Of
course the other cylinder amplitudes may computed in this matrix model
or using [ 24 ] [ 30 ] and we find the scaling forms to be,

  -- -------- -- --------
     @xmath      (6.41)
     @xmath      (6.42)
     @xmath      (6.43)
  -- -------- -- --------

again in agreement with the Liouville calculations. The fact that these
amplitudes agree with the Liouville calculations including terms that
were classified in [ 22 ] as non-universal suggests they really should
have a physical meaning (and shouldn’t be thrown away). This issue was
raised in [ 26 ] .

In order to test if P3 is a property of all deviations we need to
examine more complicated amplitudes. The @xmath correction to the disc
amplitude is the amplitude for a disc-with-handle. These computations
have already been done in [ 30 ] and [ 24 ] , the result being,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (6.44)
     @xmath   @xmath   @xmath      (6.45)
  -- -------- -------- -------- -- --------

If we now compute the deviations we find,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (6.46)
     @xmath   @xmath   @xmath      
  -- -------- -------- -------- -- --------

We now want to consider if @xmath possesses P3. For it to possess P3 we
require that it can be expressed as,

  -- -------- -- --------
     @xmath      (6.47)
  -- -------- -- --------

where @xmath and @xmath ³ ³ 3 We denote in the subscript the variable
the inverse Laplace transform is taken with respect to. have point-like
support. Since the definition of the deviation is a linear combination
of a number of amplitudes, the functions @xmath and @xmath will also be
a linear combination of contributions from each amplitude. Since the
theory is unitary the contributions to @xmath and @xmath from each
amplitude will be positive. From the expressions in ( 6.44 ) we see that
each amplitude goes as at most @xmath for @xmath and therefore any
contribution to @xmath or @xmath must go as at most @xmath or @xmath
respectively. We therefore may conclude that @xmath and @xmath as @xmath
and hence that their inverse Laplace transforms exist. Furthermore,
given that as @xmath ,

  -- -------- -- --------
     @xmath      (6.48)
  -- -------- -- --------

we see that @xmath and @xmath can not fall off faster than @xmath or
@xmath . However it is a well known theorem that a function with only
point-like support may be expressed as a sum of derivatives of @xmath
-functions;

  -- -------- -- --------
     @xmath      (6.49)
  -- -------- -- --------

Such a function will have a Laplace transform of

  -- -------- -- --------
     @xmath      (6.50)
  -- -------- -- --------

Given that we require that @xmath and @xmath are point-like supported
this implies @xmath and @xmath have an asymptotic behaviour as @xmath
inconsistent with ( 6.48 ) and hence the deviation can not possess P3.

#### 6.1.3 Dual Branes

In [ 17 ] the two matrix model description of the @xmath minimal string
was considered and it was shown that the continuum limit of the
resolvent for one of the matrices was the disc amplitude with a fixed
spin boundary. The continuum limit of the resolvent for the other matrix
gave the dual brane amplitude. In our matrix model ( 6.10 ) it is not
obvious if we can construct a correlation function which gives the disc
with dual brane boundary conditions. In fact it is present in the form,

  -- -------- -- --------
     @xmath      (6.51)
  -- -------- -- --------

which may be obtained from the general @xmath resolvent ( 6.8 ) by
setting @xmath . The resulting loop equation is easily solved as it is
quadratic in @xmath , however the scaling limit is obtained using a
different scaling of @xmath ,

  -- -------- -- --------
     @xmath      (6.52)
  -- -------- -- --------

which results in

  -- -------- -- --------
     @xmath      (6.53)
  -- -------- -- --------

where @xmath is a numerical constant. If we identify @xmath in this
expression with the dual boundary cosmological constant then this is
precisely the amplitude expected for the dual brane obtained from the
spin @xmath or spin @xmath boundary state. This explains the @xmath
possibility we left unexplained earlier in Section 6.1 .

### 6.2 The Seiberg-Shih Relation and Local Operators

Suppose that the boundaries considered in the previous sections could be
expressed as a sum over local operators in the theory, such as,

  -- -------- -- --------
     @xmath      (6.54)
  -- -------- -- --------

where @xmath represents some generic local operator and @xmath means
equivalence up to terms that diverge as @xmath . If this were true then
the effects of the Seiberg-Shih transformations on a boundary would
reduce to studying their effect on the above series. The expansion of a
boundary in terms of local operators was investigated in [ 25 ] in which
it was used to compute the one and two point correlation numbers of
tachyon operators on the sphere for the @xmath minimal strings. Later it
was shown [ 27 ] that such a technique could also compute the three
point function correctly for the @xmath models.

One technical point raised in [ 25 ] and [ 27 ] was that the boundary
state could only be expressed as a sum over local operators if we
included operators not in the BRST cohomology of [ 28 ] . That the
operators in the cohomology of [ 28 ] , which we refer to as
LZ-operators, are not all the physical operators of the minimal string
is easily verified by noting that the cohomology does not contain the
boundary length operator @xmath . Such an operator can be inserted on
the disc by differentiating with respect to the boundary cosmological
constant, giving the result

  -- -------- -- --------
     @xmath      (6.55)
  -- -------- -- --------

where @xmath is a constant of proportionality that only depends on
@xmath and @xmath . Indeed there exists a whole family of such operators
with Liouville charge @xmath where @xmath which we will denote by @xmath
.

In order to compute the coefficients @xmath appearing in ( 6.54 ) we
need to know the amplitude for any local operator inserted in a disc
with a @xmath FZZT boundary condition. For the tachyon operators the
amplitude is ( 5.45 ). For the non-tachyon operators we have argued
using the mini-superspace approximation and the exact conformal
bootstrap on the disc that the functional form of the one-point disc
amplitude is ( 5.49 ). Finally, we have seen in ( 6.55 ) that for the
first of the non-LZ operators the disc function has the functional form
expected from a direct application of the mini-superspace approximation
and also the exact conformal bootstrap. We therefore have,

  -- -------- -- --------
     @xmath      (6.56)
  -- -------- -- --------

If we apply the Liouville duality transformation to the above expression
we get

  -- -- -- --------
           (6.57)
  -- -- -- --------

where we have introduced the operators @xmath which have Liouville
charge @xmath . The expansion ( 6.54 ) can now be given a more concrete
form,

  -- -------- -- --------
     @xmath      (6.58)
  -- -------- -- --------

Having introduced the necessary technical results we now want to prove
the following claim,
If all the FZZT branes in the @xmath minimal string can be replaced by
local operators then the deviation from the Seiberg-Shih relations is
caused only by the non-LZ operators.
Consider the cylinder amplitude @xmath . If we replace one of the
boundaries by a sum of local operators using ( 6.58 ) then we get,

  -- -------- -- --------
     @xmath      (6.59)
  -- -------- -- --------

By comparison of ( 5.64 ) with ( 6.59 ), we find for the coefficient of
the LZ operators,

  -- -------- -- --------
     @xmath      (6.60)
  -- -------- -- --------

where @xmath is just a numerical constant, @xmath and @xmath is not a
multiple of @xmath or @xmath . Furthermore, note that the LZ operator
coefficients satisfy

  -- -------- -- --------
     @xmath      (6.61)
  -- -------- -- --------

This shows that if all boundary states admit an expansion in terms of
local operators then the Seiberg-Shih relations transform the
coefficients of the LZ-operators correctly. Hence any deviation from the
Seiberg-Shih relations must come from the non-LZ operators. However, it
is the non-LZ operators one would expect to give deviations compatible
with P3 as they correspond to boundary operators and their duals. One
might then suspect that in fact the boundary states may not be expressed
as a sum over local operators. We will now see this suspicion is borne
out by examination of the cylinder amplitudes.

If we note that for cylinder amplitudes, @xmath then together with (
6.58 ), this implies,

  -- -------- -- --------
     @xmath      (6.62)
  -- -------- -- --------

Since @xmath where @xmath form a set of linearly independent functions
then this implies

  -- -------- -- --------
     @xmath      (6.63)
  -- -------- -- --------

where @xmath is some function. We conclude that if all states admit an
expansion of the form ( 6.58 ) then cylinder amplitudes can be expressed
as,

  -- -------- -- --------
     @xmath      (6.64)
  -- -------- -- --------

We now want to see if the @xmath -independent coefficients of each term
in ( 5.64 ) has the above property. Consider the coefficients appearing
in the second sum of ( 5.64 ),

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
  -- -------- -- -------- --

where @xmath and @xmath . We want to know whether this can be written in
the form @xmath . That this is not true in general is shown by
considering the case @xmath , for which we have,

  -- -------- -- --------
     @xmath      (6.66)
     @xmath      (6.67)
  -- -------- -- --------

which is clearly incompatible with the factorisation required ( 6.64 ).
This lack of factorisation appears to be generic; for instance if we
consider the @xmath model we find that there is not even a subspace of
states which admit an expansion in terms of local operators. This can be
seen by considering the matrix,

  -- -------- --
     @xmath   
  -- -------- --

where the columns and rows correspond to @xmath . If there exist @xmath
and @xmath such that @xmath for some subset of the states, then at least
one of the cofactors of the matrix in ( 6.2 ) must be zero, which is not
the case.

The same conclusions can be reached by studying the other coefficients
associated to the non-LZ operators in ( 5.64 ). This leaves us in an
awkward position; there is no subspace of states that can be expanded in
terms of local operators unless we allow the subspace to be
one-dimensional (in which case why choose a particular boundary as being
the one that can be expanded in terms of local operators?), yet from the
results of [ 25 ] and [ 27 ] it is clear information about insertions of
operators can be extracted from loops. It is worth noting that from the
examination of cylinder amplitudes in which one of the boundaries is a
ZZ brane, it appears that ZZ branes do not couple to non-LZ states [ 36
, 39 , 40 ] and so some of the above problem might be avoided if a ZZ
brane is present.

#### 6.2.1 Extracting Local Operators From Loops

An interesting observation is that all correlation numbers were
extracted from loops with fixed spin boundary conditions. We now will
present further evidence that correlation numbers can be extracted from
the fixed spin boundary by reproducing the results of [ 26 ] for the
1-point correlation numbers on the torus in the @xmath minimal string.
This also serves as an independent check on the computation in [ 26 ] as
our methods are quite different.

A trick that makes this computation easy is that all amplitudes for the
minimal string are easily expressible in terms of @xmath , which is the
uniformising parameter of the auxiliary Reimann surface [ 17 ] or
equivalently spectral curve. If we write these amplitudes in terms of a
new variable, @xmath , defined by @xmath we may easily compute the large
@xmath expansion of the amplitudes. Since @xmath , the large @xmath
expansion will be an expansion in terms of the functions @xmath which is
exactly what is required to compute the correlation numbers. Explicitly,
if we have an amplitude @xmath then

  -- -------- -- --------
     @xmath      (6.69)
  -- -------- -- --------

but we also have

  -- -------- -- --------
     @xmath      (6.70)
  -- -------- -- --------

giving,

  -- -------- -- --------
     @xmath      (6.71)
  -- -------- -- --------

We now have the tools to extract the correlation numbers on the torus
beginning with the disc-with-handle amplitude. The calculation in [ 26 ]
of the torus one-point correlation numbers was performed for the @xmath
minimal string. The disc-with-handle amplitude for the @xmath minimal
string has been computed numerous times using matrix model techniques.
We will restrict ourselves to the case of the @xmath model in which the
disc-with-handle amplitude takes the form,

  -- -------- -- --------
     @xmath      (6.72)
  -- -------- -- --------

The boundary condition on the disc is the equivalent of our spin up
boundary as it is computed using the resolvent of @xmath where @xmath is
the only matrix appearing in the matrix model. We will now use this to
compute the torus one point correlation numbers, for tachyon operators,
by replacing the boundary with a sum of local operators as in ( 6.58 ).
That this computation produces results that match exactly the results of
[ 26 ] is evidence that we may replace this boundary by a sum of local
operators. Obviously, in order to compare results we have to renormalise
the bulk cosmological constant, boundary cosmological constant and
@xmath to be consistent with [ 26 ] . In order to avoid doing this we
note that the effect of such a renormalisation will to be to change the
amplitude by a factor dependent only on @xmath and @xmath and so will
not affect the ratio of correlation numbers. We therefore shall compare
the ratio of correlation numbers. Computing the large @xmath expansion
of ( 6.72 ) we get,

  -- -------- -- --------
     @xmath      (6.73)
  -- -------- -- --------

where the series converges if @xmath . We therefore find the torus
@xmath -point correlation number for tachyon operators in the @xmath
minimal string to be,

  -- -- -- --------
           (6.74)
  -- -- -- --------

when @xmath is odd and zero if @xmath is even. The 1-point correlation
numbers on the torus for the @xmath models were computed in [ 26 ] ,
giving,

  -- -------- -- --------
     @xmath      (6.75)
  -- -------- -- --------

for @xmath odd and zero when @xmath is even. Using standard @xmath
-function identities it is then easy to show that the above equation is
consistent with our computation ( 6.74 ).

The reproduction of the torus correlation numbers is evidence that this
procedure is correct and that the fixed spin amplitude can be used to
compute correlation numbers of local operators. Furthermore, we can
obtain evidence that the non-fixed spin boundary states do not admit an
expansion in terms of local operators by applying the above procedure to
the disc-with-handle amplitudes we computed for the @xmath model. We
have the expansions,

  -- -------- -------- -------- -- --------
     @xmath   @xmath   @xmath      (6.76)
     @xmath   @xmath   @xmath      (6.77)
  -- -------- -------- -------- -- --------

If the expansion in terms of local operators is valid for both these
boundary states then we would expect the coefficients of each term in
the large @xmath expansion to be related by a factor of @xmath as can be
seen from ( 6.60 ); this is clearly not the case. The implication of
these results is that the fixed spin boundary condition is special and
is the only one that admits an expansion in terms of local operators.

### 6.3 Discussion

In this chapter we have examined the Seiberg-Shih relations ( 5.52 ) for
cylinders and discs-with-handles. We saw in the previous chapter that
for cylinders the identification of FZZT branes is naively spoiled by
terms arising from the gravitational sector of the theory, as first seen
by [ 22 ] [ 36 ] . We found that the terms spoiling the identification
cannot always be associated with degenerate geometries of the
worldsheet, in the sense of [ 22 ] , as other geometries which are
degenerate in a dual sense are also present. This lead us to conjecture
that the terms spoiling the FZZT brane identification could always be
interpreted as arising from degenerate and dual degenerate geometries.
We checked this conjecture by computing the disc-with-handle amplitudes
for free and fixed spin boundary states in the @xmath minimal string
using matrix model techniques. We found that the deviation from the
Seiberg-Shih relations in this case could not be written in the way we
conjectured. Given that our conjecture was a very generous
interpretation of what terms might be unphysical we conclude this is
strong evidence that the FZZT brane identification conjectured in [ 17 ]
does not hold at all levels of perturbation theory.

We also considered an alternative approach to testing the Seiberg-Shih
relations by expanding boundaries in terms of local operators. If such
an expansion were possible we showed that it would lead to deviations
from the Seiberg Shih relations consistent with our conjecture. We then
gave explicit examples for cylinder amplitudes where a local operator
expansion of boundary states fails. This lead us to a paradox; how could
correlation numbers of local operators be extracted from boundary states
in [ 25 ] [ 27 ] if such boundary states cannot be expanded as local
operators? To investigate this we computed the disc-with-handle
amplitude in the @xmath model using matrix model methods and then showed
that the fixed spin boundary condition yielded an expansion from which
correlation numbers in agreement with the results of [ 26 ] could be
extracted. This lead us to conjecture that the fixed spin boundaries are
special in that they allow an expansion in terms of local operators.

There are many obvious generalisations of the work done here, the most
obvious being that it would be interesting to extend the above results
to the @xmath case. Since the Liouville techniques are not suited to
higher-genus computations such a project would have to be tackled using
matrix model methods. The main problem with this approach is that it is
difficult to represent all the states for a given minimal string in a
matrix model; in the above we chose the Ising model precisely because
there was an obvious mapping between the spin degrees of freedom and the
matrix fields. However this problem may now be less serious given recent
work [ 37 , 38 ] in which boundary states were constructed in the matrix
model formulation for @xmath minimal strings. Another strategy for
attacking the same problem may be the geometric recursion techniques
developed by Eynard et al. in [ 65 ] as this provides an efficient way
of computing many amplitudes for a given matrix model. Such methods
would also allow a relatively easy computation of the
cylinder-with-handle amplitudes; these would be interesting to study as
one could then check if quantum corrections destroy the fact, noted in [
36 , 39 , 40 ] , that FZZT-ZZ cylinder amplitudes are consistent with
the Seiberg-Shih relations.

Another generalisation would be to perform a similar investigation in
other non-critical string models. An obvious choice would be
non-critical superstrings as they perhaps have more physical relevance
in addition to being much better behaved. A second and perhaps less
obvious choice of string model would be the causal string theories
developed in [ 32 , 33 , 34 ] . Currently these models have only been
solved for a target space of zero dimensions and so such a project would
require a generalisation of the models to include worldsheet matter.
However, even in the zero dimensional case such causal string models are
better behaved and many of the odd features of the usual non-critical
string are absent due to the lack of baby-universe production; in
particular the gravity sector in these theories seems to be much weaker.
Since the terms spoiling the FZZT brane identification are due to the
gravity sector (at least for the cylinder amplitudes) might they have an
interpretation in terms of baby-universe over production and if so might
they be absent in these causal string models?

Finally, we saw that the matrix model is able to reproduce all the
boundary states found in the @xmath minimal string apart from the dual
of the free spin boundary condition. In [ 17 ] this was enough as they
claimed that there is only one brane and hence only one dual brane in
the theory. However, in light of our results there should exist other
dual branes in the theory. Are such states present in the matrix model
and if so can they be represented as some form of resolvent? This
question and the others outlined we leave to future work.

## Chapter 7 Summary

In this thesis we have reviewed how random graphs provide a very useful
tool by which observables in a variety of two dimensional quantum
gravity theories may be computed. Using these techniques we were able to
investigate conjectures related to two of these observables; the
dimension of spacetime and Hartle-Hawking wavefunctions, or more
generally spacetime containing a number of boundaries.

In Chapter 2 after introducing the approach to two dimensional quantum
gravity in which the metric is treated as the fundamental degree of
freedom we reviewed how the resulting path integral could be defined via
a lattice regularisation referred to as Dynamical Triangulation. This
lattice regularisation can naturally be interpreted as a random graph
and we reviewed how the disc-function observable could be computed in
this approach via matrix model and combinatorial methods. We then
explored the consequence of the ambiguity inherent in the
metric-is-fundamental approach resulting from the necessity of choosing
a class of metric to integrate over. In DT, the class of metrics
integrated over were the Euclidean metrics on the manifold. We reviewed
how the approach of CDT could be use to define path integrals in which
only geometries with a well-defined casual structure where included in
the integration. These theories differed from DT due to the propensity
of DT to generate baby-universes throughout the space; such behaviour
ultimately lead to a fractal geometry for the spacetime.

The question of how to investigate the fractal nature of the spacetimes
was taken up in Chapter 3 in which the concepts of Hausdorff and
spectral dimensions were introduced as ways of generalising the notion
of dimension beyond smooth manifolds. These measures of the dimension
were useful in characterising the properties of the space; we saw that
DT gave rise to fractal spacetime since @xmath whereas @xmath . For CDT
on the other hand we saw that @xmath signalling the emergence of a
smooth spacetime in the continuum limit. We then reviewed the recent
evidence obtained in a variety of approaches to quantum gravity, most
notably CDT, for the spectral dimension being lower in the UV than IR.
We referred to this phenomenon as dimensional reduction.

In Chapter 4 we constructed a toy model based on random comb graphs, in
which dimensional reduction occurs. This was useful for a number of
reasons; it shows that a scale dependent spectral dimension may be given
a rigorous definition and that models can be found in which this
rigorous definition results in a phenomenon similar to the dimensional
reduction observed in CDT and other approaches. We gave a
characterisation of possible continuum limits and the behaviour of the
spectral dimension in each case via Theorem ( 2 ). As discussed in
Section 4.6 we hope that these results may be used as the basis for
constructing more realistic models in which a similar dimensional
reduction can be observed. In particular we hope the work in [ 16 ] , in
which a type of random multigraph that shares many features with CDT
were introduced, may be extended in this direction.

In Chapter 5 we gave a brief review of minimal models, Liouville theory
and string theory. This was motivated by the desire to couple matter to
the two dimensional gravity models considered in the earlier chapters.
After showing how the matrix model techniques are easily extended to
describe DT models coupled to matter, we reviewed the equivalence
between these models and certain string theories known as @xmath minimal
string theories. We then considered the boundary states, or branes, in
these @xmath minimal strings, in particular focusing on a conjecture of
Seiberg and Shih which claimed that all branes occurring in such
theories were physically equivalent. This claim is challenged by noting
that the simple form of this conjecture fails for cylinder amplitudes.
We reviewed the suggestion that since the terms spoiling the conjecture
have particular properties, they are non-universal and therefore these
examples should not be seen as counter examples. However, for more
complicated cylinder amplitudes the spoiling terms fail to have the
properties necessary to be classed as non-universal. We introduced a
conjecture, referred to as P3, which gives a natural way, based on the
duality of Liouville theory, in which these new spoiling terms may also
be considered non-universal.

In the final Chapter 6 we considered the conjecture P3 by checking if it
held for the case of disc with handles in the @xmath model. This
required a matrix model representation of all the conformal boundary
states in this model. To accomplish this we introduced a matrix model
describing all the Ising model on a random lattice in the presence of a
boundary magnetic field. We demonstrated this matrix model could be
solved for any potential, not just the cubic one necessary for the Ising
model. Using this solution we computed the disc-with-handle amplitude
and argued that the terms spoiling the Seiberg-Shih conjecture did not
satisfy the extended conjecture. This strongly suggests that the naively
different branes in this theory are indeed different. There exist a
variety of extensions of this work as discussed in Section 6.3 .

Finally we considered an alternative approach to testing the
Seiberg-Shih conjecture by writing the boundary states as a sum of local
operators. This is a particular elegant way of approaching the problem
as it negates the need to consider individual amplitudes separately. We
found that the local operator representation of the boundary states lead
to non-trivial terms which spoil the Seiberg-Shih relation, however,
these terms appear to be of exactly the form necessary in order for
conjecture P3 to hold. We investigate this apparent contradiction and
argue that the expansion in terms of local operators only holds for the
identity brane. In doing so we verify certain recent results of Belavin
et al. [ 26 ] .

We hope that this thesis has convinced the reader of the usefulness of
random graphs in the study of low dimensional theories of quantum
gravity and furthermore that there is a great deal left to explore.

## Appendix A Mellin Transforms and Asymptotics

### a.1 Standard generating functions

We record here a number of standard results for generating functions for
random walks on combs; the details of their calculation are given in [ 3
] .

On the empty comb @xmath we have

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (A.1)
     @xmath   @xmath   @xmath      (A.2)
     @xmath   @xmath   @xmath      (A.3)
  -- -------- -------- -------- -- -------

Note that we can promote @xmath to being a continuous positive
semi-definite real variable in these expressions; @xmath is then a
strictly decreasing function of @xmath and @xmath a strictly increasing
function of @xmath . The finite line segment of length @xmath has

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (A.4)
  -- -------- -------- -------- -- -------

which it is sometimes convenient to write as

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (A.5)
  -- -------- -------- -------- -- -------

where

  -- -------- -- -------
     @xmath      (A.6)
  -- -------- -- -------

Again, @xmath can be promoted to a continuous positive semi-definite
real variable of which @xmath is a strictly increasing function. The
first return probability generating function for the comb with teeth of
length @xmath equally spaced at intervals of @xmath is given by

  -- -------- -- -------
     @xmath      (A.7)
  -- -------- -- -------

and @xmath is obtained by setting @xmath in this formula. @xmath is a
strictly decreasing function of @xmath and increasing function of @xmath
, viewed as continuous positive semi-definite real variables.

We also need the scaling limits of some of these quantities. They are

  -- -------- -------- -------- -- -------
     @xmath   @xmath   @xmath      (A.8)
  -- -------- -------- -------- -- -------

and

  -- -------- -- -------
     @xmath      
     @xmath      (A.9)
  -- -------- -- -------

### a.2 Bump functions

A function @xmath is a bump function if @xmath is smooth and has compact
support. We will now prove some properties concerning the Mellin
transformation of a bump function @xmath , @xmath , which has support on
@xmath where @xmath .

###### Lemma 9

The critical strip of the Mellin transform of the @xmath th derivative
of @xmath , @xmath , is @xmath for all @xmath .

###### Proof.

Recall that the Mellin transform is defined by, @xmath . Since @xmath
has compact support we have,

  -- -------- -- --------
     @xmath      (A.10)
  -- -------- -- --------

and since @xmath is smooth, @xmath is bounded on @xmath by some constant
@xmath , so,

  -- -------- -- --------
     @xmath      (A.11)
  -- -------- -- --------

and the RHS is finite for all @xmath since @xmath . This also shows that
@xmath is holomorphic for all @xmath . ∎

###### Lemma 10

Given @xmath , @xmath for all @xmath .

###### Proof.

Recall from the previous lemma that the critical strip of the Mellin
transform of @xmath and its derivatives coincides with @xmath . We can
therefore use the integral representation of the Mellin transform to
prove statements valid for all @xmath . By integration by parts,

  -- -------- -- --------
     @xmath      (A.12)
  -- -------- -- --------

and therefore,

  -- -------- -- --------
     @xmath      (A.13)
  -- -------- -- --------

Hence,

  -- -------- -- --------
     @xmath      (A.14)
  -- -------- -- --------

∎

Given a bump function @xmath which is always positive, has support
@xmath and is scaled such that its integral is one, we define the
cut-off function to be,

  -- -------- -- --------
     @xmath      (A.15)
  -- -------- -- --------

###### Lemma 11

The critical strip of @xmath is given be @xmath . The analytic
continuation of @xmath to all @xmath is given by

  -- -------- -- --------
     @xmath      (A.16)
  -- -------- -- --------

###### Proof.

The analytic continuation of @xmath may be obtained by applying
integration by parts to the Mellin transform of @xmath and recalling by
lemma ( 9 ) that @xmath is holomorphic everywhere. ∎

### a.3 Asymptotic Series and Dirichlet Series

Starting with

  -- -------- -- --------
     @xmath      (A.17)
  -- -------- -- --------

where @xmath is the smooth cut-off function introduced in A.2 and @xmath
controls where the cut-off occurs we take the Mellin transform of @xmath
with respect to @xmath ,

  -- -------- -------- -------- -------- --------
     @xmath   @xmath   @xmath            (A.18)
                       @xmath   @xmath   
                       @xmath   @xmath   
  -- -------- -------- -------- -------- --------

where @xmath is the Dirichlet series associated to the measure,

  -- -------- -- --------
     @xmath      (A.19)
  -- -------- -- --------

It is easy to see that the fundamental strip of @xmath is @xmath , due
to the compact support of @xmath on the positive real axis and so the
Mellin transform does indeed exist. We may now invert the Mellin
transform to obtain,

  -- -------- -- --------
     @xmath      (A.20)
  -- -------- -- --------

This may be computed by rewriting the above as,

  -- -------- -- --------
     @xmath      (A.21)
  -- -------- -- --------

where the contour @xmath is the rectangle composed of the points @xmath
with @xmath , @xmath such the contour is the right of all poles and

  -- -- -- --------
           (A.22)
  -- -- -- --------

If @xmath has slow growth in the strip @xmath then due to lemma ( 10 ),
which shows that @xmath decays faster than any polynomial as @xmath goes
to infinity, the contributions from integrating along the contours
@xmath to @xmath and from @xmath to @xmath are zero. Furthermore the
remainder term @xmath satisfies,

  -- -------- -- --------
     @xmath      (A.23)
  -- -------- -- --------

and so will only contribute terms of order @xmath to @xmath . We
therefore have,

  -- -------- -- --------
     @xmath      (A.24)
     @xmath      (A.25)
  -- -------- -- --------

where @xmath is the set of positions of the poles of @xmath and we have
used the fact that @xmath . Finally, define @xmath . By relating this
function to @xmath we may write,

  -- -------- -- --------
     @xmath      (A.26)
     @xmath      (A.27)
  -- -------- -- --------

## Appendix B Liouville Cylinder Amplitudes

The full cylinder amplitudes were given in a usable form recently by [
36 ] . We have obtained similar results independently and reproduce them
here for the purpose of completeness. The cylinder amplitude between a
@xmath and @xmath brane, @xmath , is computed by the integral

  -- -------- -- -------
     @xmath      (B.1)
  -- -------- -- -------

where @xmath is the modular parameter of the cylinder in the closed
string channel. The partition function for the ghosts is well known

  -- -------- -- -------
     @xmath      (B.2)
  -- -------- -- -------

where @xmath and @xmath denotes the Dedekind @xmath -function. The
Liouville and matter contributions are given by

  -- -------- -- -------
     @xmath      (B.3)
     @xmath      (B.4)
  -- -------- -- -------

where @xmath is the Kac table for the minimal model, @xmath is the
relevant modular @xmath -matrix, @xmath is defined in ( 5.44 ) and
@xmath and @xmath are the characters for the minimal model primary field
with Kac index @xmath and the non-degenerate Liouville primary field
respectively. The characters are given by,

  -- -------- -- -------- -- -------
     @xmath      @xmath      (B.5)
     @xmath      @xmath      (B.6)
  -- -------- -- -------- -- -------

We first concentrate on the @xmath . Recalling that the modular S-matrix
for the @xmath minimal model is

  -- -------- -- -------
     @xmath      (B.7)
  -- -------- -- -------

the explicit expression for @xmath is,

  -- -------- -- -------- -------- -------
     @xmath      @xmath            (B.8)
                          @xmath   
  -- -------- -- -------- -------- -------

where we have used the symmetry of the Kac table to rewrite the limits
on the @xmath and @xmath summation. Also, note that the quantity in the
square brackets is symmetric in @xmath and @xmath and zero if @xmath or
@xmath . This allows us to rewrite it as,

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
  -- -------- -- -------- --

where @xmath . Substituting these expressions into ( B.2 ) gives,

  -- -------- -- --------
     @xmath      
     @xmath      
     @xmath      (B.10)
  -- -------- -- --------

Focusing our attention on the integral over the moduli, we compute,

  -- -------- -- -------- -------- --------
     @xmath      @xmath            (B.11)
                          @xmath   
  -- -------- -- -------- -------- --------

which gives,

  -- -------- -- --------
     @xmath      (B.12)
  -- -------- -- --------

where @xmath is given by,

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
  -- -------- -- -------- --

where @xmath . To compute ( B.12 ) we first note that the integrand is
symmetric in @xmath and so we can extend the integral to the whole real
line. Without loss of generality we assume @xmath , and break up the
factor of @xmath in ( B.12 ) into its exponentials. For the integral
containing, @xmath we can close the integral along the real line by a
semi-circle to produce the contour @xmath in the upper-half plane, for
the other term we use a contour, @xmath in the lower-half plane.
However, the integral using the contour in the lower-half plane, by a
change of variables from @xmath to @xmath is equal to the integral using
the contour in the upper half-plane, we therefore get,

  -- -------- -- --------
     @xmath      (B.14)
  -- -------- -- --------

where @xmath is a numerical constant and

  -- -------- -- --------
     @xmath      (B.15)
  -- -------- -- --------

The poles of @xmath occur at @xmath . The residue of @xmath at @xmath
is,

  -- -------- -- -------- -------- --------
     @xmath      @xmath            (B.16)
                          @xmath   
  -- -------- -- -------- -------- --------

The residues of the @xmath are,

  -- -------- -- --------
     @xmath      (B.17)
     @xmath      
     @xmath      
     @xmath      
  -- -------- -- --------

Finally, a useful identity for computing the sums found in the
computation of the cylinder amplitudes is,

  -- -------- -- -------- --
     @xmath      @xmath   
                 @xmath   
  -- -------- -- -------- --

## Appendix C Loop equations for Disc with Handle

To complete the calculation of the disc with handle amplitude in the SX
matrix model we need a number of additional loop equations. In order to
calculate the quantities @xmath and @xmath we need,

  -- -------- -- -------- -- -------
     @xmath      @xmath      (C.1)
     @xmath      @xmath      (C.2)
     @xmath      @xmath      (C.3)
  -- -------- -- -------- -- -------

Finally the following loop equations allow for quantities of the form,
@xmath etc. to be found,

  -- -------- -- -------- -- -------
     @xmath      @xmath      (C.4)
     @xmath      @xmath      (C.5)
     @xmath      @xmath      (C.6)
  -- -------- -- -------- -- -------